[
  {
    "id": 25939,
    "package_name": "xgboost",
    "title": "Extreme Gradient Boosting",
    "description": "Extreme Gradient Boosting, which is an efficient\nimplementation of the gradient boosting framework from Chen &\nGuestrin (2016) <doi:10.1145/2939672.2939785>. This package is\nits R interface. The package includes efficient linear model\nsolver and tree learning algorithms. The package can\nautomatically do parallel computation on a single machine which\ncould be more than 10 times faster than existing gradient\nboosting packages. It supports various objective functions,\nincluding regression, classification and ranking. The package\nis made to be extensible, so that users are also allowed to\ndefine their own objectives easily.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 20.926,
    "stars": 0
  },
  {
    "id": 21807,
    "package_name": "rpart",
    "title": "Recursive Partitioning and Regression Trees",
    "description": "Recursive partitioning for classification, regression and\nsurvival trees.  An implementation of most of the functionality\nof the 1984 book by Breiman, Friedman, Olshen and Stone.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 16.6243,
    "stars": 0
  },
  {
    "id": 24815,
    "package_name": "torch",
    "title": "Tensors and Neural Networks with 'GPU' Acceleration",
    "description": "Provides functionality to define and train neural networks\nsimilar to 'PyTorch' by Paszke et al (2019)\n<doi:10.48550/arXiv.1912.01703> but written entirely in R using\nthe 'libtorch' library. Also supports low-level tensor\noperations and 'GPU' acceleration.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 16.2214,
    "stars": 0
  },
  {
    "id": 19622,
    "package_name": "phyloseq",
    "title": "Handling and analysis of high-throughput microbiome census data",
    "description": "phyloseq provides a set of classes and tools to facilitate\nthe import, storage, analysis, and graphical display of\nmicrobiome census data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 14.2327,
    "stars": 0
  },
  {
    "id": 16782,
    "package_name": "maftools",
    "title": "Summarize, Analyze and Visualize MAF Files",
    "description": "Analyze and visualize Mutation Annotation Format (MAF)\nfiles from large scale sequencing studies. This package\nprovides various functions to perform most commonly used\nanalyses in cancer genomics and to create feature rich\ncustomizable visualzations with minimal effort.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.9685,
    "stars": 0
  },
  {
    "id": 1517,
    "package_name": "DALEX",
    "title": "moDel Agnostic Language for Exploration and eXplanation",
    "description": "Any unverified black box model is the path to failure.\nOpaqueness leads to distrust. Distrust leads to ignoration.\nIgnoration leads to rejection. DALEX package xrays any model\nand helps to explore and explain its behaviour. Machine\nLearning (ML) models are widely used and have various\napplications in classification or regression. Models created\nwith boosting, bagging, stacking or similar techniques are\noften used due to their high performance. But such black-box\nmodels usually lack direct interpretability. DALEX package\ncontains various methods that help to understand the link\nbetween input variables and model output. Implemented methods\nhelp to explore the model on the level of a single instance as\nwell as a level of the whole dataset. All model explainers are\nmodel agnostic and can be compared across different models.\nDALEX package is the cornerstone for 'DrWhy.AI' universe of\npackages for visual model exploration. Find more details in\n(Biecek 2018) <https://jmlr.org/papers/v19/18-416.html>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.7571,
    "stars": 0
  },
  {
    "id": 11250,
    "package_name": "dada2",
    "title": "Accurate, high-resolution sample inference from amplicon\nsequencing data",
    "description": "The dada2 package infers exact amplicon sequence variants\n(ASVs) from high-throughput amplicon sequencing data, replacing\nthe coarser and less accurate OTU clustering approach. The\ndada2 pipeline takes as input demultiplexed fastq files, and\noutputs the sequence variants and their sample-wise abundances\nafter removing substitution and chimera errors. Taxonomic\nclassification is available via a native implementation of the\nRDP naive Bayesian classifier, and species-level assignment to\n16S rRNA gene fragments by exact matching.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.6068,
    "stars": 0
  },
  {
    "id": 24422,
    "package_name": "text",
    "title": "Analyses of Text using Transformers Models from HuggingFace,\nNatural Language Processing and Machine Learning",
    "description": "Link R with Transformers from Hugging Face to transform\ntext variables to word embeddings; where the word embeddings\nare used to statistically test the mean difference between set\nof texts, compute semantic similarity scores between texts,\npredict numerical variables, and visual statistically\nsignificant words according to various dimensions etc. For more\ninformation see <https://www.r-text.org>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.5121,
    "stars": 0
  },
  {
    "id": 17631,
    "package_name": "mlr3pipelines",
    "title": "Preprocessing Operators and Pipelines for 'mlr3'",
    "description": "Dataflow programming toolkit that enriches 'mlr3' with a\ndiverse set of pipelining operators ('PipeOps') that can be\ncomposed into graphs. Operations exist for data preprocessing,\nmodel fitting, and ensemble learning. Graphs can themselves be\ntreated as 'mlr3' 'Learners' and can therefore be resampled,\nbenchmarked, and tuned.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.0774,
    "stars": 0
  },
  {
    "id": 25507,
    "package_name": "vip",
    "title": "Variable Importance Plots",
    "description": "A general framework for constructing variable importance\nplots from various types of machine learning models in R. Aside\nfrom some standard model- specific variable importance\nmeasures, this package also provides model- agnostic approaches\nthat can be applied to any supervised learning algorithm. These\ninclude 1) an efficient permutation-based variable importance\nmeasure, 2) variable importance based on Shapley values\n(Strumbelj and Kononenko, 2014)\n<doi:10.1007/s10115-013-0679-x>, and 3) the variance-based\napproach described in Greenwell et al. (2018)\n<doi:10.48550/arXiv.1805.04755>. A variance-based method for\nquantifying the relative strength of interaction effects is\nalso included (see the previous reference for details).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.9846,
    "stars": 0
  },
  {
    "id": 7123,
    "package_name": "SingleR",
    "title": "Reference-Based Single-Cell RNA-Seq Annotation",
    "description": "Performs unbiased cell type recognition from single-cell\nRNA sequencing data, by leveraging reference transcriptomic\ndatasets of pure cell types to infer the cell of origin of each\nsingle cell independently.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.9736,
    "stars": 0
  },
  {
    "id": 19403,
    "package_name": "pdp",
    "title": "Partial Dependence Plots",
    "description": "A general framework for constructing partial dependence\n(i.e., marginal effect) plots from various types machine\nlearning models in R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.7219,
    "stars": 0
  },
  {
    "id": 17019,
    "package_name": "mboost",
    "title": "Model-Based Boosting",
    "description": "Functional gradient descent algorithm (boosting) for\noptimizing general risk functions utilizing component-wise\n(penalised) least squares estimates or regression trees as\nbase-learners for fitting generalized linear, additive and\ninteraction models to potentially high-dimensional data. Models\nand algorithms are described in <doi:10.1214/07-STS242>, a\nhands-on tutorial is available from\n<doi:10.1007/s00180-012-0382-5>. The package allows\nuser-specified loss functions and base-learners.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.5187,
    "stars": 0
  },
  {
    "id": 17640,
    "package_name": "mlr3tuning",
    "title": "Hyperparameter Optimization for 'mlr3'",
    "description": "Hyperparameter optimization package of the 'mlr3'\necosystem. It features highly configurable search spaces via\nthe 'paradox' package and finds optimal hyperparameter\nconfigurations for any 'mlr3' learner.  'mlr3tuning' works with\nseveral optimization algorithms e.g. Random Search, Iterated\nRacing, Bayesian Optimization (in 'mlr3mbo') and Hyperband (in\n'mlr3hyperband'). Moreover, it can automatically optimize\nlearners and estimate the performance of optimized models with\nnested resampling.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.3175,
    "stars": 0
  },
  {
    "id": 4424,
    "package_name": "ModelMetrics",
    "title": "Rapid Calculation of Model Metrics",
    "description": "Collection of metrics for evaluating models written in C++\nusing 'Rcpp'. Popular metrics include area under the curve, log\nloss, root mean square error, etc.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.1112,
    "stars": 0
  },
  {
    "id": 869,
    "package_name": "CAST",
    "title": "'caret' Applications for Spatial-Temporal Models",
    "description": "Supporting functionality to run 'caret' with spatial or\nspatial-temporal data. 'caret' is a frequently used package for\nmodel training and prediction using machine learning. CAST\nincludes functions to improve spatial or spatial-temporal\nmodelling tasks using 'caret'. It includes the newly suggested\n'Nearest neighbor distance matching' cross-validation to\nestimate the performance of spatial prediction models and\nallows for spatial variable selection to selects suitable\npredictor variables in view to their contribution to the\nspatial model performance. CAST further includes functionality\nto estimate the (spatial) area of applicability of prediction\nmodels. Methods are described in Meyer et al. (2018)\n<doi:10.1016/j.envsoft.2017.12.001>; Meyer et al. (2019)\n<doi:10.1016/j.ecolmodel.2019.108815>; Meyer and Pebesma (2021)\n<doi:10.1111/2041-210X.13650>; Milà et al. (2022)\n<doi:10.1111/2041-210X.13851>; Meyer and Pebesma (2022)\n<doi:10.1038/s41467-022-29838-9>; Linnenbrink et al. (2023)\n<doi:10.5194/egusphere-2023-1308>; Schumacher et al. (2024)\n<doi:10.5194/egusphere-2024-2730>. The package is described in\ndetail in Meyer et al. (2024) <doi:10.48550/arXiv.2404.06978>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.0961,
    "stars": 0
  },
  {
    "id": 17231,
    "package_name": "metagenomeSeq",
    "title": "Statistical analysis for sparse high-throughput sequencing",
    "description": "metagenomeSeq is designed to determine features (be it\nOperational Taxanomic Unit (OTU), species, etc.) that are\ndifferentially abundant between two or more groups of multiple\nsamples. metagenomeSeq is designed to address the effects of\nboth normalization and under-sampling of microbial communities\non disease association detection and the testing of feature\ncorrelations.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.0832,
    "stars": 0
  },
  {
    "id": 16212,
    "package_name": "lares",
    "title": "Lean Analytics and Robust Exploration Sidekick",
    "description": "Auxiliary package for better/faster analytics,\nvisualization, data mining, and machine learning tasks. With a\nwide variety of family functions, like Machine Learning, Data\nWrangling, Marketing Mix Modeling (Robyn), Exploratory, API,\nand Scrapper, it helps the analyst or data scientist to get\nquick and robust results, without the need of repetitive coding\nor advanced R programming skills.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.835,
    "stars": 0
  },
  {
    "id": 11467,
    "package_name": "decontam",
    "title": "Identify Contaminants in Marker-gene and Metagenomics Sequencing\nData",
    "description": "Simple statistical identification of contaminating\nsequence features in marker-gene or metagenomics data. Works on\nany kind of feature derived from environmental sequencing data\n(e.g. ASVs, OTUs, taxonomic groups, MAGs,...). Requires DNA\nquantitation data or sequenced negative control samples.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.7611,
    "stars": 0
  },
  {
    "id": 5306,
    "package_name": "PharmacoGx",
    "title": "Analysis of Large-Scale Pharmacogenomic Data",
    "description": "Contains a set of functions to perform large-scale\nanalysis of pharmaco-genomic data. These include the\nPharmacoSet object for storing the results of pharmacogenomic\nexperiments, as well as a number of functions for computing\ncommon summaries of drug-dose response and correlating them\nwith the molecular features in a cancer cell-line.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.5665,
    "stars": 0
  },
  {
    "id": 22303,
    "package_name": "scRepertoire",
    "title": "A toolkit for single-cell immune receptor profiling",
    "description": "scRepertoire is a toolkit for processing and analyzing\nsingle-cell T-cell receptor (TCR) and immunoglobulin (Ig). The\nscRepertoire framework supports use of 10x, AIRR, BD, MiXCR,\nTRUST4, and WAT3R single-cell formats. The functionality\nincludes basic clonal analyses, repertoire summaries,\ndistance-based clustering and interaction with the popular\nSeurat and SingleCellExperiment/Bioconductor R single-cell\nworkflows.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.4836,
    "stars": 0
  },
  {
    "id": 22723,
    "package_name": "shapviz",
    "title": "SHAP Visualizations",
    "description": "Visualizations for SHAP (SHapley Additive exPlanations),\nsuch as waterfall plots, force plots, various types of\nimportance plots, dependence plots, and interaction plots.\nThese plots act on a 'shapviz' object created from a matrix of\nSHAP values and a corresponding feature dataset. Wrappers for\nthe R packages 'xgboost', 'lightgbm', 'fastshap', 'shapr',\n'h2o', 'treeshap', 'DALEX', and 'kernelshap' are added for\nconvenience.  By separating visualization and computation, it\nis possible to display factor variables in graphs, even if the\nSHAP values are calculated by a model that requires numerical\nfeatures. The plots are inspired by those provided by the\n'shap' package in Python, but there is no dependency on it.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.3132,
    "stars": 0
  },
  {
    "id": 17629,
    "package_name": "mlr3misc",
    "title": "Helper Functions for 'mlr3'",
    "description": "Frequently used helper functions and assertions used in\n'mlr3' and its companion packages. Comes with helper functions\nfor functional programming, for printing, to work with\n'data.table', as well as some generally useful 'R6' classes.\nThis package also supersedes the package 'BBmisc'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.9133,
    "stars": 0
  },
  {
    "id": 1879,
    "package_name": "DirichletMultinomial",
    "title": "Dirichlet-Multinomial Mixture Model Machine Learning for\nMicrobiome Data",
    "description": "Dirichlet-multinomial mixture models can be used to\ndescribe variability in microbial metagenomic data. This\npackage is an interface to code originally made available by\nHolmes, Harris, and Quince, 2012, PLoS ONE 7(2): 1-15, as\ndiscussed further in the man page for this package,\n?DirichletMultinomial.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.8837,
    "stars": 0
  },
  {
    "id": 17512,
    "package_name": "missRanger",
    "title": "Fast Imputation of Missing Values",
    "description": "Alternative implementation of the beautiful 'MissForest'\nalgorithm used to impute mixed-type data sets by chaining\nrandom forests, introduced by Stekhoven, D.J. and Buehlmann, P.\n(2012) <doi:10.1093/bioinformatics/btr597>. Under the hood, it\nuses the lightning fast random forest package 'ranger'. Between\nthe iterative model fitting, we offer the option of using\npredictive mean matching. This firstly avoids imputation with\nvalues not already present in the original data (like a value\n0.3334 in 0-1 coded variable).  Secondly, predictive mean\nmatching tries to raise the variance in the resulting\nconditional distributions to a realistic level. This would\nallow, e.g., to do multiple imputation when repeating the call\nto missRanger(). Out-of-sample application is supported as\nwell.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.7872,
    "stars": 0
  },
  {
    "id": 18202,
    "package_name": "naivebayes",
    "title": "High Performance Implementation of the Naive Bayes Algorithm",
    "description": "In this implementation of the Naive Bayes classifier\nfollowing class conditional distributions are available:\n'Bernoulli', 'Categorical', 'Gaussian', 'Poisson',\n'Multinomial' and non-parametric representation of the class\nconditional density estimated via Kernel Density Estimation.\nImplemented classifiers handle missing data and can take\nadvantage of sparse data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.5617,
    "stars": 0
  },
  {
    "id": 19109,
    "package_name": "pRoloc",
    "title": "A unifying bioinformatics framework for spatial proteomics",
    "description": "The pRoloc package implements machine learning and\nvisualisation methods for the analysis and interogation of\nquantitiative mass spectrometry data to reliably infer protein\nsub-cellular localisation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.4417,
    "stars": 0
  },
  {
    "id": 9089,
    "package_name": "bbotk",
    "title": "Black-Box Optimization Toolkit",
    "description": "Features highly configurable search spaces via the\n'paradox' package and optimizes every user-defined objective\nfunction. The package includes several optimization algorithms\ne.g. Random Search, Iterated Racing, Bayesian Optimization (in\n'mlr3mbo') and Hyperband (in 'mlr3hyperband'). bbotk is the\nbase package of 'mlr3tuning', 'mlr3fselect' and 'miesmuschel'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.4329,
    "stars": 0
  },
  {
    "id": 19006,
    "package_name": "origami",
    "title": "Generalized Framework for Cross-Validation",
    "description": "A general framework for the application of\ncross-validation schemes to particular functions. By allowing\narbitrary lists of results, origami accommodates a range of\ncross-validation applications. This implementation was first\ndescribed by Coyle and Hejazi (2018) <doi:10.21105/joss.00512>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.3429,
    "stars": 0
  },
  {
    "id": 748,
    "package_name": "BiocNeighbors",
    "title": "Nearest Neighbor Detection for Bioconductor Packages",
    "description": "Implements exact and approximate methods for nearest\nneighbor detection, in a framework that allows them to be\neasily switched within Bioconductor packages or workflows.\nExact searches can be performed using the k-means for k-nearest\nneighbors algorithm or with vantage point trees. Approximate\nsearches can be performed using the Annoy or HNSW libraries.\nSearching on either Euclidean or Manhattan distances is\nsupported. Parallelization is achieved for all methods by using\nBiocParallel. Functions are also provided to search for all\nneighbors within a given distance.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.1288,
    "stars": 0
  },
  {
    "id": 6624,
    "package_name": "SC3",
    "title": "Single-Cell Consensus Clustering",
    "description": "A tool for unsupervised clustering and analysis of single\ncell RNA-Seq data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.1161,
    "stars": 0
  },
  {
    "id": 23712,
    "package_name": "stabs",
    "title": "Stability Selection with Error Control",
    "description": "Resampling procedures to assess the stability of selected\nvariables with additional finite sample error control for\nhigh-dimensional variable selection procedures such as Lasso or\nboosting. Both, standard stability selection (Meinshausen &\nBuhlmann, 2010, <doi:10.1111/j.1467-9868.2010.00740.x>) and\ncomplementary pairs stability selection with improved error\nbounds (Shah & Samworth, 2013,\n<doi:10.1111/j.1467-9868.2011.01034.x>) are implemented. The\npackage can be combined with arbitrary user specified variable\nselection approaches.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.0152,
    "stars": 0
  },
  {
    "id": 17657,
    "package_name": "mltools",
    "title": "Machine Learning Tools",
    "description": "A collection of machine learning helper functions,\nparticularly assisting in the Exploratory Data Analysis phase.\nMakes heavy use of the 'data.table' package for optimal speed\nand memory efficiency. Highlights include a versatile\nbin_data() function, sparsify() for converting a data.table to\nsparse matrix format with one-hot encoding, fast evaluation\nmetrics, and empirical_cdf() for calculating empirical\nMultivariate Cumulative Distribution Functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.9714,
    "stars": 0
  },
  {
    "id": 2839,
    "package_name": "GenVisR",
    "title": "Genomic Visualizations in R",
    "description": "Produce highly customizable publication quality graphics\nfor genomic data primarily at the cohort level.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.9199,
    "stars": 0
  },
  {
    "id": 17619,
    "package_name": "mlr3extralearners",
    "title": "Extra Learners For mlr3",
    "description": "Extra learners for use in mlr3.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.847,
    "stars": 0
  },
  {
    "id": 11177,
    "package_name": "cvAUC",
    "title": "Cross-Validated Area Under the ROC Curve Confidence Intervals",
    "description": "Tools for working with and evaluating cross-validated area\nunder the ROC curve (AUC) estimators.  The primary functions of\nthe package are ci.cvAUC and ci.pooled.cvAUC, which report\ncross-validated AUC and compute confidence intervals for\ncross-validated AUC estimates based on influence curves for\ni.i.d. and pooled repeated measures data, respectively.  One\nbenefit to using influence curve based confidence intervals is\nthat they require much less computation time than bootstrapping\nmethods.  The utility functions, AUC and cvAUC, are simple\nwrappers for functions from the ROCR package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.7332,
    "stars": 0
  },
  {
    "id": 23853,
    "package_name": "stochtree",
    "title": "Stochastic Tree Ensembles (XBART and BART) for Supervised\nLearning and Causal Inference",
    "description": "Flexible stochastic tree ensemble software. Robust\nimplementations of Bayesian Additive Regression Trees (BART)\nChipman, George, McCulloch (2010) <doi:10.1214/09-AOAS285> for\nsupervised learning and Bayesian Causal Forests (BCF) Hahn,\nMurray, Carvalho (2020) <doi:10.1214/19-BA1195> for causal\ninference. Enables model serialization and parallel sampling\nand provides a low-level interface for custom stochastic forest\nsamplers.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.6921,
    "stars": 0
  },
  {
    "id": 16016,
    "package_name": "kernelshap",
    "title": "Kernel SHAP",
    "description": "Efficient implementation of Kernel SHAP (Lundberg and Lee,\n2017, <doi:10.48550/arXiv.1705.07874>) permutation SHAP, and\nadditive SHAP for model interpretability.  For Kernel SHAP and\npermutation SHAP, if the number of features is too large for\nexact calculations, the algorithms iterate until the SHAP\nvalues are sufficiently precise in terms of their standard\nerrors.  The package integrates smoothly with meta-learning\npackages such as 'tidymodels', 'caret' or 'mlr3'. It supports\nmulti-output models, case weights, and parallel computations.\nVisualizations can be done using the R package 'shapviz'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.6412,
    "stars": 0
  },
  {
    "id": 20265,
    "package_name": "protr",
    "title": "Generating Various Numerical Representation Schemes for Protein\nSequences",
    "description": "Comprehensive toolkit for generating various numerical\nfeatures of protein sequences described in Xiao et al. (2015)\n<DOI:10.1093/bioinformatics/btv042>. For full functionality,\nthe software 'ncbi-blast+' is needed, see\n<https://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html>\nfor more information.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.4277,
    "stars": 0
  },
  {
    "id": 2308,
    "package_name": "FDboost",
    "title": "Boosting Functional Regression Models",
    "description": "Regression models for functional data, i.e.,\nscalar-on-function, function-on-scalar and function-on-function\nregression models, are fitted by a component-wise gradient\nboosting algorithm.  For a manual on how to use 'FDboost', see\nBrockhaus, Ruegamer, Greven (2017) <doi:10.18637/jss.v094.i10>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.4061,
    "stars": 0
  },
  {
    "id": 8803,
    "package_name": "auditor",
    "title": "Model Audit - Verification, Validation, and Error Analysis",
    "description": "Provides an easy to use unified interface for creating\nvalidation plots for any model. The 'auditor' helps to avoid\nrepetitive work consisting of writing code needed to create\nresidual plots. This visualizations allow to asses and compare\nthe goodness of fit, performance, and similarity of models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.3173,
    "stars": 0
  },
  {
    "id": 13692,
    "package_name": "gamboostLSS",
    "title": "Boosting Methods for 'GAMLSS'",
    "description": "Boosting models for fitting generalized additive models\nfor location, shape and scale ('GAMLSS') to potentially high\ndimensional data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.2416,
    "stars": 0
  },
  {
    "id": 4211,
    "package_name": "MachineShop",
    "title": "Machine Learning Models and Tools",
    "description": "Meta-package for statistical and machine learning with a\nunified interface for model fitting, prediction, performance\nassessment, and presentation of results.  Approaches for model\nfitting and prediction of numerical, categorical, or censored\ntime-to-event outcomes include traditional regression models,\nregularization methods, tree-based methods, support vector\nmachines, neural networks, ensembles, data preprocessing,\nfiltering, and model tuning and selection.  Performance metrics\nare provided for model assessment and can be estimated with\nindependent test sets, split sampling, cross-validation, or\nbootstrap resampling.  Resample estimation can be executed in\nparallel for faster processing and nested in cases of model\ntuning and selection.  Modeling results can be summarized with\ndescriptive statistics; calibration curves; variable\nimportance; partial dependence plots; confusion matrices; and\nROC, lift, and other performance curves.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.2046,
    "stars": 0
  },
  {
    "id": 9615,
    "package_name": "breakDown",
    "title": "Model Agnostic Explainers for Individual Predictions",
    "description": "Model agnostic tool for decomposition of predictions from\nblack boxes. Break Down Table shows contributions of every\nvariable to a final prediction. Break Down Plot presents\nvariable contributions in a concise graphical way. This package\nwork for binary classifiers and general regression models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.1657,
    "stars": 0
  },
  {
    "id": 8888,
    "package_name": "azuremlsdk",
    "title": "Interface to the 'Azure Machine Learning' 'SDK'",
    "description": "Interface to the 'Azure Machine Learning' Software\nDevelopment Kit ('SDK'). Data scientists can use the 'SDK' to\ntrain, deploy, automate, and manage machine learning models on\nthe 'Azure Machine Learning' service. To learn more about\n'Azure Machine Learning' visit the website:\n<https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.1631,
    "stars": 0
  },
  {
    "id": 17627,
    "package_name": "mlr3mbo",
    "title": "Flexible Bayesian Optimization",
    "description": "A modern and flexible approach to Bayesian Optimization /\nModel Based Optimization building on the 'bbotk' package.\n'mlr3mbo' is a toolbox providing both ready-to-use optimization\nalgorithms as well as their fundamental building blocks\nallowing for straightforward implementation of custom\nalgorithms. Single- and multi-objective optimization is\nsupported as well as mixed continuous, categorical and\nconditional search spaces. Moreover, using 'mlr3mbo' for\nhyperparameter optimization of machine learning models within\nthe 'mlr3' ecosystem is straightforward via 'mlr3tuning'.\nExamples of ready-to-use optimization algorithms include\nEfficient Global Optimization by Jones et al. (1998)\n<doi:10.1023/A:1008306431147>, ParEGO by Knowles (2006)\n<doi:10.1109/TEVC.2005.851274> and SMS-EGO by Ponweiser et al.\n(2008) <doi:10.1007/978-3-540-87700-4_78>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.0431,
    "stars": 0
  },
  {
    "id": 22381,
    "package_name": "scmap",
    "title": "A tool for unsupervised projection of single cell RNA-seq data",
    "description": "Single-cell RNA-seq (scRNA-seq) is widely used to\ninvestigate the composition of complex tissues since the\ntechnology allows researchers to define cell-types using\nunsupervised clustering of the transcriptome. However, due to\ndifferences in experimental methods and computational analyses,\nit is often challenging to directly compare the cells\nidentified in two different experiments. scmap is a method for\nprojecting cells from a scRNA-seq experiment on to the\ncell-types or individual cells identified in a different\nexperiment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.9396,
    "stars": 0
  },
  {
    "id": 15244,
    "package_name": "iCOBRA",
    "title": "Comparison and Visualization of Ranking and Assignment Methods",
    "description": "This package provides functions for calculation and\nvisualization of performance metrics for evaluation of ranking\nand binary classification (assignment) methods. Various types\nof performance plots can be generated programmatically. The\npackage also contains a shiny application for interactive\nexploration of results.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.9085,
    "stars": 0
  },
  {
    "id": 17623,
    "package_name": "mlr3fselect",
    "title": "Feature Selection for 'mlr3'",
    "description": "Feature selection package of the 'mlr3' ecosystem. It\nselects the optimal feature set for any 'mlr3' learner. The\npackage works with several optimization algorithms e.g. Random\nSearch, Recursive Feature Elimination, and Genetic Search.\nMoreover, it can automatically optimize learners and estimate\nthe performance of optimized feature sets with nested\nresampling.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.8883,
    "stars": 0
  },
  {
    "id": 10245,
    "package_name": "cito",
    "title": "Building and Training Neural Networks",
    "description": "The 'cito' package provides a user-friendly interface for\ntraining and interpreting deep neural networks (DNN). 'cito'\nsimplifies the fitting of DNNs by supporting the familiar\nformula syntax, hyperparameter tuning under cross-validation,\nand helps to detect and handle convergence problems.  DNNs can\nbe trained on CPU, GPU and MacOS GPUs. In addition, 'cito' has\nmany downstream functionalities such as various explainable AI\n(xAI) metrics (e.g. variable importance, partial dependence\nplots, accumulated local effect plots, and effect estimates) to\ninterpret trained DNNs. 'cito' optionally provides confidence\nintervals (and p-values) for all xAI metrics and predictions.\nAt the same time, 'cito' is computationally efficient because\nit is based on the deep learning framework 'torch'. The 'torch'\npackage is native to R, so no Python installation or other API\nis required for this package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.7616,
    "stars": 0
  },
  {
    "id": 17431,
    "package_name": "mikropml",
    "title": "User-Friendly R Package for Supervised Machine Learning\nPipelines",
    "description": "An interface to build machine learning models for\nclassification and regression problems. 'mikropml' implements\nthe ML pipeline described by Topçuoğlu et al. (2020)\n<doi:10.1128/mBio.00434-20> with reasonable default options for\ndata preprocessing, hyperparameter tuning, cross-validation,\ntesting, model evaluation, and interpretation steps.  See the\nwebsite <https://www.schlosslab.org/mikropml/> for more\ninformation, documentation, and examples.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.7246,
    "stars": 0
  },
  {
    "id": 19040,
    "package_name": "osqp",
    "title": "Quadratic Programming Solver using the 'OSQP' Library",
    "description": "Provides bindings to the 'OSQP' solver. The 'OSQP' solver\nis a numerical optimization package or solving convex quadratic\nprograms written in 'C' and based on the alternating direction\nmethod of multipliers. See <doi:10.48550/arXiv.1711.08013> for\ndetails.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.6065,
    "stars": 0
  },
  {
    "id": 1257,
    "package_name": "ClassifyR",
    "title": "A framework for cross-validated classification problems, with\napplications to differential variability and differential\ndistribution testing",
    "description": "The software formalises a framework for classification and\nsurvival model evaluation in R. There are four stages; Data\ntransformation, feature selection, model training, and\nprediction. The requirements of variable types and variable\norder are fixed, but specialised variables for functions can\nalso be provided. The framework is wrapped in a driver loop\nthat reproducibly carries out a number of cross-validation\nschemes. Functions for differential mean, differential\nvariability, and differential distribution are included.\nAdditional functions may be developed by the user, by creating\nan interface to the framework.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.5018,
    "stars": 0
  },
  {
    "id": 16324,
    "package_name": "lefser",
    "title": "R implementation of the LEfSE method for microbiome biomarker\ndiscovery",
    "description": "lefser is the R implementation of the popular microbiome\nbiomarker discovery too, LEfSe. It uses the Kruskal-Wallis\ntest, Wilcoxon-Rank Sum test, and Linear Discriminant Analysis\nto find biomarkers from two-level classes (and optional\nsub-classes).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.4241,
    "stars": 0
  },
  {
    "id": 17642,
    "package_name": "mlr3verse",
    "title": "Easily Install and Load the 'mlr3' Package Family",
    "description": "The 'mlr3' package family is a set of packages for\nmachine-learning purposes built in a modular fashion. This\nwrapper package is aimed to simplify the installation and\nloading of the core 'mlr3' packages. Get more information about\nthe 'mlr3' project at <https://mlr3book.mlr-org.com/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.262,
    "stars": 0
  },
  {
    "id": 14911,
    "package_name": "healthyR.ai",
    "title": "The Machine Learning and AI Modeling Companion to 'healthyR'",
    "description": "Hospital machine learning and ai data analysis workflow\ntools, modeling, and automations. This library provides many\nuseful tools to review common administrative hospital data.\nSome of these include predicting length of stay, and readmits.\nThe aim is to provide a simple and consistent verb framework\nthat takes the guesswork out of everything.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.2525,
    "stars": 0
  },
  {
    "id": 17639,
    "package_name": "mlr3torch",
    "title": "Deep Learning with 'mlr3'",
    "description": "Deep Learning library that extends the mlr3 framework by\nbuilding upon the 'torch' package. It allows to conveniently\nbuild, train, and evaluate deep learning models without having\nto worry about low level details. Custom architectures can be\ncreated using the graph language defined in 'mlr3pipelines'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.2525,
    "stars": 0
  },
  {
    "id": 19848,
    "package_name": "pmml",
    "title": "Generate PMML for Various Models",
    "description": "The Predictive Model Markup Language (PMML) is an\nXML-based language which provides a way for applications to\ndefine machine learning, statistical and data mining models and\nto share models between PMML compliant applications. More\ninformation about the PMML industry standard and the Data\nMining Group can be found at <http://dmg.org/>. The generated\nPMML can be imported into any PMML consuming application, such\nas Zementis Predictive Analytics products. The package isofor\n(used for anomaly detection) can be installed with\ndevtools::install_github(\"gravesee/isofor\").",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.2438,
    "stars": 0
  },
  {
    "id": 12614,
    "package_name": "escape",
    "title": "Easy single cell analysis platform for enrichment",
    "description": "A bridging R package to facilitate gene set enrichment\nanalysis (GSEA) in the context of single-cell RNA sequencing.\nUsing raw count information, Seurat objects, or\nSingleCellExperiment format, users can perform and visualize\nssGSEA, GSVA, AUCell, and UCell-based enrichment calculations\nacross individual cells. Alternatively, escape supports use of\nrank-based GSEA, such as the use of differential gene\nexpression via fgsea.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.2299,
    "stars": 0
  },
  {
    "id": 25500,
    "package_name": "vimp",
    "title": "Perform Inference on Algorithm-Agnostic Variable Importance",
    "description": "Calculate point estimates of and valid confidence\nintervals for nonparametric, algorithm-agnostic variable\nimportance measures in high and low dimensions, using flexible\nestimators of the underlying regression functions. For more\ninformation about the methods, please see Williamson et al.\n(Biometrics, 2020), Williamson et al. (JASA, 2021), and\nWilliamson and Feng (ICML, 2020).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.0701,
    "stars": 0
  },
  {
    "id": 13882,
    "package_name": "genieclust",
    "title": "Fast and Robust Hierarchical Clustering with Outlier Detection",
    "description": "The Genie algorithm (Gagolewski, 2021\n<DOI:10.1016/j.softx.2021.100722>) is a robust and\noutlier-resistant hierarchical clustering method (Gagolewski,\nBartoszuk, Cena, 2016 <DOI:10.1016/j.ins.2016.05.003>). This\npackage features its faster and more capable variant. It allows\nclustering with respect to mutual reachability distances,\nenabling it to act as an outlier detector or an alternative to\n'HDBSCAN*' that can identify a predefined number of clusters.\nThe package also features an implementation of the Gini and\nBonferroni inequality indices, external cluster validity\nmeasures (e.g., the normalised clustering accuracy, the\nadjusted Rand index, the Fowlkes-Mallows index, and normalised\nmutual information), and internal cluster validity indices\n(e.g., the Calinski-Harabasz, Davies-Bouldin, Ball-Hall,\nSilhouette, and generalised Dunn indices). The 'Python' version\nof 'genieclust' is available via 'PyPI'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.0377,
    "stars": 0
  },
  {
    "id": 3589,
    "package_name": "L0Learn",
    "title": "Fast Algorithms for Best Subset Selection",
    "description": "Highly optimized toolkit for approximately solving\nL0-regularized learning problems (a.k.a. best subset\nselection). The algorithms are based on coordinate descent and\nlocal combinatorial search. For more details, check the paper\nby Hazimeh and Mazumder (2020) <10.1287/opre.2019.1919>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.0169,
    "stars": 0
  },
  {
    "id": 22267,
    "package_name": "scDiagnostics",
    "title": "Cell type annotation diagnostics",
    "description": "The scDiagnostics package provides diagnostic plots to\nassess the quality of cell type assignments from single cell\ngene expression profiles. The implemented functionality allows\nto assess the reliability of cell type annotations, investigate\ngene expression patterns, and explore relationships between\ndifferent cell types in query and reference datasets allowing\nusers to detect potential misalignments between reference and\nquery datasets. The package also provides visualization\ncapabilities for diagnostics purposes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8274,
    "stars": 0
  },
  {
    "id": 5247,
    "package_name": "ParBayesianOptimization",
    "title": "Parallel Bayesian Optimization of Hyperparameters",
    "description": "Fast, flexible framework for implementing Bayesian\noptimization of model hyperparameters according to the methods\ndescribed in Snoek et al. <arXiv:1206.2944>. The package allows\nthe user to run scoring function in parallel, save intermediary\nresults, and tweak other aspects of the process to fully\nutilize the computing resources available to the user.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.7681,
    "stars": 0
  },
  {
    "id": 4036,
    "package_name": "MLInterfaces",
    "title": "Uniform interfaces to R machine learning procedures for data in\nBioconductor containers",
    "description": "This package provides uniform interfaces to machine\nlearning code for data in R and Bioconductor containers.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.7551,
    "stars": 0
  },
  {
    "id": 10528,
    "package_name": "cola",
    "title": "A Framework for Consensus Partitioning",
    "description": "Subgroup classification is a basic task in genomic data\nanalysis, especially for gene expression and DNA methylation\ndata analysis. It can also be used to test the agreement to\nknown clinical annotations, or to test whether there exist\nsignificant batch effects. The cola package provides a general\nframework for subgroup classification by consensus\npartitioning. It has the following features: 1. It modularizes\nthe consensus partitioning processes that various methods can\nbe easily integrated. 2. It provides rich visualizations for\ninterpreting the results. 3. It allows running multiple methods\nat the same time and provides functionalities to\nstraightforward compare results. 4. It provides a new method to\nextract features which are more efficient to separate\nsubgroups. 5. It automatically generates detailed reports for\nthe complete analysis. 6. It allows applying consensus\npartitioning in a hierarchical manner.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.662,
    "stars": 0
  },
  {
    "id": 17624,
    "package_name": "mlr3hyperband",
    "title": "Hyperband for 'mlr3'",
    "description": "Successive Halving (Jamieson and Talwalkar (2016)\n<doi:10.48550/arXiv.1502.07943>) and Hyperband (Li et al. 2018\n<doi:10.48550/arXiv.1603.06560>) optimization algorithm for the\nmlr3 ecosystem. The implementation in mlr3hyperband features\nimproved scheduling and parallelizes the evaluation of\nconfigurations. The package includes tuners for hyperparameter\noptimization in mlr3tuning and optimizers for black-box\noptimization in bbotk.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.6012,
    "stars": 0
  },
  {
    "id": 17372,
    "package_name": "miceRanger",
    "title": "Multiple Imputation by Chained Equations with Random Forests",
    "description": "Multiple Imputation has been shown to be a flexible method\nto impute missing values by Van Buuren (2007)\n<doi:10.1177/0962280206074463>. Expanding on this, random\nforests have been shown to be an accurate model by Stekhoven\nand Buhlmann <arXiv:1105.0828> to impute missing values in\ndatasets. They have the added benefits of returning out of bag\nerror and variable importance estimates, as well as being\nsimple to run in parallel.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.5925,
    "stars": 0
  },
  {
    "id": 23085,
    "package_name": "sjSDM",
    "title": "Scalable Joint Species Distribution Modeling",
    "description": "A scalable and fast method for estimating joint Species\nDistribution Models (jSDMs) for big community data, including\neDNA data. The package estimates a full (i.e. non-latent) jSDM\nwith different response distributions (including the\ntraditional multivariate probit model). The package allows to\nperform variation partitioning (VP) / ANOVA on the fitted\nmodels to separate the contribution of environmental, spatial,\nand biotic associations. In addition, the total R-squared can\nbe further partitioned per species and site to reveal the\ninternal metacommunity structure, see Leibold et al.,\n<doi:10.1111/oik.08618>. The internal structure can then be\nregressed against environmental and spatial distinctiveness,\nrichness, and traits to analyze metacommunity assembly\nprocesses.  The package includes support for accounting for\nspatial autocorrelation and the option to fit responses using\ndeep neural networks instead of a standard linear predictor. As\ndescribed in Pichler & Hartig (2021)\n<doi:10.1111/2041-210X.13687>, scalability is achieved by using\na Monte Carlo approximation of the joint likelihood implemented\nvia 'PyTorch' and 'reticulate', which can be run on CPUs or\nGPUs.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.5897,
    "stars": 0
  },
  {
    "id": 244,
    "package_name": "AlpsNMR",
    "title": "Automated spectraL Processing System for NMR",
    "description": "Reads Bruker NMR data directories both zipped and\nunzipped. It provides automated and efficient signal processing\nfor untargeted NMR metabolomics. It is able to interpolate the\nsamples, detect outliers, exclude regions, normalize, detect\npeaks, align the spectra, integrate peaks, manage metadata and\nvisualize the spectra. After spectra proccessing, it can apply\nmultivariate analysis on extracted data. Efficient plotting\nwith 1-D data is also available. Basic reading of 1D ACD/Labs\nexported JDX samples is also available.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4417,
    "stars": 0
  },
  {
    "id": 944,
    "package_name": "CHETAH",
    "title": "Fast and accurate scRNA-seq cell type identification",
    "description": "CHETAH (CHaracterization of cEll Types Aided by\nHierarchical classification) is an accurate, selective and fast\nscRNA-seq classifier. Classification is guided by a reference\ndataset, preferentially also a scRNA-seq dataset. By\nhierarchical clustering of the reference data, CHETAH creates a\nclassification tree that enables a step-wise, top-to-bottom\nclassification. Using a novel stopping rule, CHETAH classifies\nthe input cells to the cell types of the references and to\n\"intermediate types\": more general classifications that ended\nin an intermediate node of the tree.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4328,
    "stars": 0
  },
  {
    "id": 13860,
    "package_name": "genefu",
    "title": "Computation of Gene Expression-Based Signatures in Breast Cancer",
    "description": "This package contains functions implementing various tasks\nusually required by gene expression analysis, especially in\nbreast cancer studies: gene mapping between different\nmicroarray platforms, identification of molecular subtypes,\nimplementation of published gene signatures, gene selection,\nand survival analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3824,
    "stars": 0
  },
  {
    "id": 17592,
    "package_name": "mlexperiments",
    "title": "Machine Learning Experiments",
    "description": "Provides 'R6' objects to perform parallelized\nhyperparameter optimization and cross-validation.\nHyperparameter optimization can be performed with Bayesian\noptimization (via 'ParBayesianOptimization'\n<https://cran.r-project.org/package=ParBayesianOptimization>)\nand grid search. The optimized hyperparameters can be validated\nusing k-fold cross-validation. Alternatively, hyperparameter\noptimization and validation can be performed with nested\ncross-validation. While 'mlexperiments' focuses on core\nwrappers for machine learning experiments, additional learner\nalgorithms can be supplemented by inheriting from the provided\nlearner base class.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3272,
    "stars": 0
  },
  {
    "id": 16363,
    "package_name": "lfda",
    "title": "Local Fisher Discriminant Analysis",
    "description": "Functions for performing and visualizing Local Fisher\nDiscriminant Analysis(LFDA), Kernel Fisher Discriminant\nAnalysis(KLFDA), and Semi-supervised Local Fisher Discriminant\nAnalysis(SELF).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3089,
    "stars": 0
  },
  {
    "id": 16112,
    "package_name": "koinar",
    "title": "KoinaR - Remote machine learning inference using Koina",
    "description": "A client to simplify fetching predictions from the Koina\nweb service. Koina is a model repository enabling the remote\nexecution of models. Predictions are generated as a response to\nHTTP/S requests, the standard protocol used for nearly all web\ntraffic.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.239,
    "stars": 0
  },
  {
    "id": 4346,
    "package_name": "MetricsWeighted",
    "title": "Weighted Metrics and Performance Measures for Machine Learning",
    "description": "Provides weighted versions of several metrics and\nperformance measures used in machine learning, including\naverage unit deviances of the Bernoulli, Tweedie, Poisson, and\nGamma distributions, see Jorgensen B. (1997, ISBN:\n978-0412997112).  The package also contains a weighted version\nof generalized R-squared, see e.g. Cohen, J. et al. (2002,\nISBN: 978-0805822236).  Furthermore, 'dplyr' chains are\nsupported.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0972,
    "stars": 0
  },
  {
    "id": 11701,
    "package_name": "diffpriv",
    "title": "Easy Differential Privacy",
    "description": "An implementation of major general-purpose mechanisms for\nprivatizing statistics, models, and machine learners, within\nthe framework of differential privacy of Dwork et al. (2006)\n<doi:10.1007/11681878_14>. Example mechanisms include the\nLaplace mechanism for releasing numeric aggregates, and the\nexponential mechanism for releasing set elements. A sensitivity\nsampler (Rubinstein & Alda, 2017) <arXiv:1706.02562> permits\nsampling target non-private function sensitivity; combined with\nthe generic mechanisms, it permits turn-key privatization of\narbitrary programs.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.032,
    "stars": 0
  },
  {
    "id": 22256,
    "package_name": "scClassify",
    "title": "scClassify: single-cell Hierarchical Classification",
    "description": "scClassify is a multiscale classification framework for\nsingle-cell RNA-seq data based on ensemble learning and cell\ntype hierarchies, enabling sample size estimation required for\naccurate cell type classification and joint classification of\ncells using multiple references.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9877,
    "stars": 0
  },
  {
    "id": 3364,
    "package_name": "Ibex",
    "title": "Methods for BCR single-cell embedding",
    "description": "Implementation of the Ibex algorithm for single-cell\nembedding based on BCR sequences. The package includes a\nstandalone function to encode BCR sequence information by amino\nacid properties or sequence order using tensorflow-based\nautoencoder. In addition, the package interacts with\nSingleCellExperiment or Seurat data objects.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9432,
    "stars": 0
  },
  {
    "id": 13250,
    "package_name": "flashlight",
    "title": "Shed Light on Black Box Machine Learning Models",
    "description": "Shed light on black box machine learning models by the\nhelp of model performance, variable importance, global\nsurrogate models, ICE profiles, partial dependence (Friedman J.\nH. (2001) <doi:10.1214/aos/1013203451>), accumulated local\neffects (Apley D. W. (2016) <doi:10.48550/arXiv.1612.08468>),\nfurther effects plots, interaction strength, and variable\ncontribution breakdown (Gosiewska and Biecek (2019)\n<doi:10.48550/arXiv.1903.11420>).  All tools are implemented to\nwork with case weights and allow for stratified analysis.\nFurthermore, multiple flashlights can be combined and analyzed\ntogether.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9392,
    "stars": 0
  },
  {
    "id": 4103,
    "package_name": "MPRAnalyze",
    "title": "Statistical Analysis of MPRA data",
    "description": "MPRAnalyze provides statistical framework for the analysis\nof data generated by Massively Parallel Reporter Assays\n(MPRAs), used to directly measure enhancer activity. MPRAnalyze\ncan be used for quantification of enhancer activity,\nclassification of active enhancers and comparative analyses of\nenhancer activity between conditions. MPRAnalyze construct a\nnested pair of generalized linear models (GLMs) to relate the\nDNA and RNA observations, easily adjustable to various\nexperimental designs and conditions, and provides a set of\nrigorous statistical testig schemes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9243,
    "stars": 0
  },
  {
    "id": 24949,
    "package_name": "treeshap",
    "title": "Compute SHAP Values for Your Tree-Based Models Using the\n'TreeSHAP' Algorithm",
    "description": "An efficient implementation of the 'TreeSHAP' algorithm\nintroduced by Lundberg et al., (2020)\n<doi:10.1038/s42256-019-0138-9>. It is capable of calculating\nSHAP (SHapley Additive exPlanations) values for tree-based\nmodels in polynomial time.  Currently supported models include\n'gbm', 'randomForest', 'ranger', 'xgboost', 'lightgbm'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.889,
    "stars": 0
  },
  {
    "id": 12906,
    "package_name": "fairness",
    "title": "Algorithmic Fairness Metrics",
    "description": "Offers calculation, visualization and comparison of\nalgorithmic fairness metrics. Fair machine learning is an\nemerging topic with the overarching aim to critically assess\nwhether ML algorithms reinforce existing social biases. Unfair\nalgorithms can propagate such biases and produce predictions\nwith a disparate impact on various sensitive groups of\nindividuals (defined by sex, gender, ethnicity, religion,\nincome, socioeconomic status, physical or mental disabilities).\nFair algorithms possess the underlying foundation that these\ngroups should be treated similarly or have similar prediction\noutcomes. The fairness R package offers the calculation and\ncomparisons of commonly and less commonly used fairness metrics\nin population subgroups. These methods are described by Calders\nand Verwer (2010) <doi:10.1007/s10618-010-0190-x>, Chouldechova\n(2017) <doi:10.1089/big.2016.0047>, Feldman et al. (2015)\n<doi:10.1145/2783258.2783311> , Friedler et al. (2018)\n<doi:10.1145/3287560.3287589> and Zafar et al. (2017)\n<doi:10.1145/3038912.3052660>. The package also offers\nconvenient visualizations to help understand fairness metrics.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8799,
    "stars": 0
  },
  {
    "id": 17597,
    "package_name": "mllrnrs",
    "title": "R6-Based ML Learners for 'mlexperiments'",
    "description": "Enhances 'mlexperiments'\n<https://CRAN.R-project.org/package=mlexperiments> with\nadditional machine learning ('ML') learners. The package\nprovides R6-based learners for the following algorithms:\n'glmnet' <https://CRAN.R-project.org/package=glmnet>, 'ranger'\n<https://CRAN.R-project.org/package=ranger>, 'xgboost'\n<https://CRAN.R-project.org/package=xgboost>, and 'lightgbm'\n<https://CRAN.R-project.org/package=lightgbm>. These can be\nused directly with the 'mlexperiments' R package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8638,
    "stars": 0
  },
  {
    "id": 4381,
    "package_name": "MiscMetabar",
    "title": "Miscellaneous Functions for Metabarcoding Analysis",
    "description": "Facilitate the description, transformation, exploration,\nand reproducibility of metabarcoding analyses. 'MiscMetabar' is\nmainly built on top of the 'phyloseq', 'dada2' and 'targets' R\npackages. It helps to build reproducible and robust\nbioinformatics pipelines in R. 'MiscMetabar' makes ecological\nanalysis of alpha and beta-diversity easier, more reproducible\nand more powerful by integrating a large number of tools.\nImportant features are described in Taudière A. (2023)\n<doi:10.21105/joss.06038>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8351,
    "stars": 0
  },
  {
    "id": 20765,
    "package_name": "rSAFE",
    "title": "Surrogate-Assisted Feature Extraction",
    "description": "Provides a model agnostic tool for white-box model trained\non features extracted from a black-box model. For more\ninformation see: Gosiewska et al. (2020)\n<doi:10.1016/j.dss.2021.113556>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7896,
    "stars": 0
  },
  {
    "id": 7301,
    "package_name": "Statial",
    "title": "A package to identify changes in cell state relative to spatial\nassociations",
    "description": "Statial is a suite of functions for identifying changes in\ncell state. The functionality provided by Statial provides\nrobust quantification of cell type localisation which are\ninvariant to changes in tissue structure. In addition to this\nStatial uncovers changes in marker expression associated with\nvarying levels of localisation. These features can be used to\nexplore how the structure and function of different cell types\nmay be altered by the agents they are surrounded with.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6866,
    "stars": 0
  },
  {
    "id": 15461,
    "package_name": "immApex",
    "title": "Tools for Adaptive Immune Receptor Sequence-Based Machine and\nDeep Learning",
    "description": "A set of tools to for machine and deep learning in R from\namino acid and nucleotide sequences focusing on adaptive immune\nreceptors. The package includes pre-processing of sequences,\nunifying gene nomenclature usage, encoding sequences, and\ncombining models. This package will serve as the basis of\nfuture immune receptor sequence functions/packages/models\ncompatible with the scRepertoire ecosystem.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6702,
    "stars": 0
  },
  {
    "id": 16009,
    "package_name": "kerasnip",
    "title": "A Bridge Between 'keras' and 'tidymodels'",
    "description": "Provides a seamless bridge between 'keras' and the\n'tidymodels' frameworks. It allows for the dynamic creation of\n'parsnip' model specifications for 'keras' models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6391,
    "stars": 0
  },
  {
    "id": 16222,
    "package_name": "latentcor",
    "title": "Fast Computation of Latent Correlations for Mixed Data",
    "description": "The first stand-alone R package for computation of latent\ncorrelation that takes into account all variable types\n(continuous/binary/ordinal/zero-inflated), comes with an\noptimized memory footprint, and is computationally efficient,\nessentially making latent correlation estimation almost as fast\nas rank-based correlation estimation. The estimation is based\non latent copula Gaussian models. For continuous/binary types,\nsee Fan, J., Liu, H., Ning, Y., and Zou, H. (2017). For ternary\ntype, see Quan X., Booth J.G. and Wells M.T. (2018)\n<doi:10.48550/arXiv.1809.06255>. For truncated type or\nzero-inflated type, see Yoon G., Carroll R.J. and Gaynanova I.\n(2020) <doi:10.1093/biomet/asaa007>. For approximation method\nof computation, see Yoon G., Müller C.L. and Gaynanova I.\n(2021) <doi:10.1080/10618600.2021.1882468>. The latter method\nuses multi-linear interpolation originally implemented in the R\npackage <https://cran.r-project.org/package=chebpol>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6369,
    "stars": 0
  },
  {
    "id": 8062,
    "package_name": "Xeva",
    "title": "Analysis of patient-derived xenograft (PDX) data",
    "description": "The Xeva package provides efficient and powerful functions\nfor patient-drived xenograft (PDX) based pharmacogenomic data\nanalysis. This package contains a set of functions to perform\nanalysis of patient-derived xenograft data. This package was\ndeveloped by the BHKLab, for further information please see our\ndocumentation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6325,
    "stars": 0
  },
  {
    "id": 20742,
    "package_name": "rMIDAS",
    "title": "Multiple Imputation with Denoising Autoencoders",
    "description": "A tool for multiply imputing missing data using 'MIDAS', a\ndeep learning method based on denoising autoencoder neural\nnetworks. This algorithm offers significant accuracy and\nefficiency advantages over other multiple imputation\nstrategies, particularly when applied to large datasets with\ncomplex features. Alongside interfacing with 'Python' to run\nthe core algorithm, this package contains functions for\nprocessing data before and after model training, running\nimputation model diagnostics, generating multiple completed\ndatasets, and estimating regression models on these datasets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5775,
    "stars": 0
  },
  {
    "id": 11929,
    "package_name": "doubletrouble",
    "title": "Identification and classification of duplicated genes",
    "description": "doubletrouble aims to identify duplicated genes from\nwhole-genome protein sequences and classify them based on their\nmodes of duplication. The duplication modes are i. segmental\nduplication (SD); ii. tandem duplication (TD); iii. proximal\nduplication (PD); iv. transposed duplication (TRD) and; v.\ndispersed duplication (DD). Transposon-derived duplicates (TRD)\ncan be further subdivided into rTRD (retrotransposon-derived\nduplication) and dTRD (DNA transposon-derived duplication). If\nusers want a simpler classification scheme, duplicates can also\nbe classified into SD- and SSD-derived (small-scale\nduplication) gene pairs. Besides classifying gene pairs, users\ncan also classify genes, so that each gene is assigned a unique\nmode of duplication. Users can also calculate substitution\nrates per substitution site (i.e., Ka and Ks) from duplicate\npairs, find peaks in Ks distributions with Gaussian Mixture\nModels (GMMs), and classify gene pairs into age groups based on\nKs peaks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5563,
    "stars": 0
  },
  {
    "id": 16908,
    "package_name": "markeR",
    "title": "An R Toolkit for Evaluating Gene Signatures as Phenotypic\nMarkers",
    "description": "markeR is an R package that provides a modular and\nextensible framework for the systematic evaluation of gene sets\nas phenotypic markers using transcriptomic data. The package is\ndesigned to support both quantitative analyses and visual\nexploration of gene set behaviour across experimental and\nclinical phenotypes. It implements multiple methods, including\nscore-based and enrichment approaches, and also allows the\nexploration of expression behaviour of individual genes. In\naddition, users can assess the similarity of their own gene\nsets against established collections (e.g., those from MSigDB),\nfacilitating biological interpretation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5317,
    "stars": 0
  },
  {
    "id": 11553,
    "package_name": "densratio",
    "title": "Density Ratio Estimation",
    "description": "Density ratio estimation. The estimated density ratio\nfunction can be used in many applications such as anomaly\ndetection, change-point detection, covariate shift adaptation.\nThe implemented methods are uLSIF (Hido et al. (2011)\n<doi:10.1007/s10115-010-0283-2>), RuLSIF (Yamada et al. (2011)\n<doi:10.1162/NECO_a_00442>), and KLIEP (Sugiyama et al. (2007)\n<doi:10.1007/s10463-008-0197-x>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5271,
    "stars": 0
  },
  {
    "id": 23514,
    "package_name": "sperrorest",
    "title": "Perform Spatial Error Estimation and Variable Importance\nAssessment",
    "description": "Implements spatial error estimation and permutation-based\nvariable importance measures for predictive models using\nspatial cross-validation and spatial block bootstrap.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4942,
    "stars": 0
  },
  {
    "id": 11134,
    "package_name": "cubar",
    "title": "Codon Usage Bias Analysis",
    "description": "A suite of functions for rapid and flexible analysis of\ncodon usage bias. It provides in-depth analysis at the codon\nlevel, including relative synonymous codon usage (RSCU), tRNA\nweight calculations, machine learning predictions for optimal\nor preferred codons, and visualization of codon-anticodon\npairing. Additionally, it can calculate various gene- specific\ncodon indices such as codon adaptation index (CAI), effective\nnumber of codons (ENC), fraction of optimal codons (Fop), tRNA\nadaptation index (tAI), mean codon stabilization coefficients\n(CSCg), and GC contents (GC/GC3s/GC4d). It also supports both\nstandard and non-standard genetic code tables found in NCBI, as\nwell as custom genetic code tables.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4728,
    "stars": 0
  },
  {
    "id": 1434,
    "package_name": "CoreGx",
    "title": "Classes and Functions to Serve as the Basis for Other 'Gx'\nPackages",
    "description": "A collection of functions and classes which serve as the\nfoundation for our lab's suite of R packages, such as\n'PharmacoGx' and 'RadioGx'. This package was created to\nabstract shared functionality from other lab package releases\nto increase ease of maintainability and reduce code repetition\nin current and future 'Gx' suite programs. Major features\ninclude a 'CoreSet' class, from which 'RadioSet' and\n'PharmacoSet' are derived, along with get and set methods for\neach respective slot. Additional functions related to fitting\nand plotting dose response curves, quantifying statistical\ncorrelation and calculating area under the curve (AUC) or\nsurvival fraction (SF) are included. For more details please\nsee the included documentation, as well as: Smirnov, P.,\nSafikhani, Z., El-Hachem, N., Wang, D., She, A., Olsen, C.,\nFreeman, M., Selby, H., Gendoo, D., Grossman, P., Beck, A.,\nAerts, H., Lupien, M., Goldenberg, A. (2015)\n<doi:10.1093/bioinformatics/btv723>. Manem, V., Labie, M.,\nSmirnov, P., Kofia, V., Freeman, M., Koritzinksy, M., Abazeed,\nM., Haibe-Kains, B., Bratman, S. (2018) <doi:10.1101/449793>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4558,
    "stars": 0
  },
  {
    "id": 4674,
    "package_name": "NetPathMiner",
    "title": "NetPathMiner for Biological Network Construction, Path Mining\nand Visualization",
    "description": "NetPathMiner is a general framework for network path\nmining using genome-scale networks. It constructs networks from\nKGML, SBML and BioPAX files, providing three network\nrepresentations, metabolic, reaction and gene representations.\nNetPathMiner finds active paths and applies machine learning\nmethods to summarize found paths for easy interpretation. It\nalso provides static and interactive visualizations of networks\nand paths to aid manual investigation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.3345,
    "stars": 0
  },
  {
    "id": 6777,
    "package_name": "SLmetrics",
    "title": "Machine Learning Performance Evaluation on Steroids",
    "description": "Performance evaluation metrics for supervised and\nunsupervised machine learning, statistical learning and\nartificial intelligence applications. Core computations are\nimplemented in 'C++' for scalability and efficiency.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.3345,
    "stars": 0
  },
  {
    "id": 2712,
    "package_name": "GPA",
    "title": "GPA (Genetic analysis incorporating Pleiotropy and Annotation)",
    "description": "This package provides functions for fitting GPA, a\nstatistical framework to prioritize GWAS results by integrating\npleiotropy information and annotation data. In addition, it\nalso includes ShinyGPA, an interactive visualization toolkit to\ninvestigate pleiotropic architecture.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.29,
    "stars": 0
  },
  {
    "id": 15263,
    "package_name": "iNETgrate",
    "title": "Integrates DNA methylation data with gene expression in a single\ngene network",
    "description": "The iNETgrate package provides functions to build a\ncorrelation network in which nodes are genes. DNA methylation\nand gene expression data are integrated to define the\nconnections between genes. This network is used to identify\nmodules (clusters) of genes. The biological information in each\nof the resulting modules is represented by an eigengene. These\nbiological signatures can be used as features e.g., for\nclassification of patients into risk categories. The resulting\nbiological signatures are very robust and give a holistic view\nof the underlying molecular changes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.261,
    "stars": 0
  },
  {
    "id": 16840,
    "package_name": "manymodelr",
    "title": "Build and Tune Several Models",
    "description": "Frequently one needs a convenient way to build and tune\nseveral models in one go.The goal is to provide a number of\nmachine learning convenience functions. It provides the ability\nto build, tune and obtain predictions of several models in one\nfunction. The models are built using functions from 'caret'\nwith easier to read syntax. Kuhn(2014)\n<doi:10.48550/arXiv.1405.6974>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2465,
    "stars": 0
  },
  {
    "id": 21343,
    "package_name": "resemble",
    "title": "Memory-Based Learning in Spectral Chemometrics",
    "description": "Functions for dissimilarity analysis and memory-based\nlearning (MBL, a.k.a local modeling) in complex spectral data\nsets. Most of these functions are based on the methods\npresented in Ramirez-Lopez et al. (2013)\n<doi:10.1016/j.geoderma.2012.12.014>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2345,
    "stars": 0
  },
  {
    "id": 23005,
    "package_name": "simpleSeg",
    "title": "A package to perform simple cell segmentation",
    "description": "Image segmentation is the process of identifying the\nborders of individual objects (in this case cells) within an\nimage. This allows for the features of cells such as marker\nexpression and morphology to be extracted, stored and analysed.\nsimpleSeg provides functionality for user friendly, watershed\nbased segmentation on multiplexed cellular images in R based on\nthe intensity of user specified protein marker channels.\nsimpleSeg can also be used for the normalization of single cell\ndata obtained from multiple images.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2175,
    "stars": 0
  },
  {
    "id": 23928,
    "package_name": "structToolbox",
    "title": "Data processing & analysis tools for Metabolomics and other\nomics",
    "description": "An extensive set of data (pre-)processing and analysis\nmethods and tools for metabolomics and other omics, with a\nstrong emphasis on statistics and machine learning. This\ntoolbox allows the user to build extensive and standardised\nworkflows for data analysis. The methods and tools have been\nimplemented using class-based templates provided by the struct\n(Statistics in R Using Class-based Templates) package. The\ntoolbox includes pre-processing methods (e.g. signal drift and\nbatch correction, normalisation, missing value imputation and\nscaling), univariate (e.g. ttest, various forms of ANOVA,\nKruskal–Wallis test and more) and multivariate statistical\nmethods (e.g. PCA and PLS, including cross-validation and\npermutation testing) as well as machine learning methods (e.g.\nSupport Vector Machines). The STATistics Ontology (STATO) has\nbeen integrated and implemented to provide standardised\ndefinitions for the different methods, inputs and outputs.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1998,
    "stars": 0
  },
  {
    "id": 19731,
    "package_name": "planet",
    "title": "Placental DNA methylation analysis tools",
    "description": "This package contains R functions to predict biological\nvariables to from placnetal DNA methylation data generated from\ninfinium arrays. This includes inferring ethnicity/ancestry,\ngestational age, and cell composition from placental DNA\nmethylation array (450k/850k) data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1948,
    "stars": 0
  },
  {
    "id": 15125,
    "package_name": "hstats",
    "title": "Interaction Statistics",
    "description": "Fast, model-agnostic implementation of different\nH-statistics introduced by Jerome H. Friedman and Bogdan E.\nPopescu (2008) <doi:10.1214/07-AOAS148>.  These statistics\nquantify interaction strength per feature, feature pair, and\nfeature triple.  The package supports multi-output predictions\nand can account for case weights. In addition, several variants\nof the original statistics are provided. The shape of the\ninteractions can be explored through partial dependence plots\nor individual conditional expectation plots. 'DALEX'\nexplainers, meta learners ('mlr3', 'tidymodels', 'caret') and\nmost other models work out-of-the-box.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1658,
    "stars": 0
  },
  {
    "id": 16772,
    "package_name": "made4",
    "title": "Multivariate analysis of microarray data using ADE4",
    "description": "Multivariate data analysis and graphical display of\nmicroarray data. Functions include for supervised dimension\nreduction (between group analysis) and joint dimension\nreduction of 2 datasets (coinertia analysis). It contains\nfunctions that require R package ade4.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1284,
    "stars": 0
  },
  {
    "id": 13002,
    "package_name": "fastseg",
    "title": "fastseg - a fast segmentation algorithm",
    "description": "fastseg implements a very fast and efficient segmentation\nalgorithm. It has similar functionality as DNACopy (Olshen and\nVenkatraman 2004), but is considerably faster and more\nflexible. fastseg can segment data from DNA microarrays and\ndata from next generation sequencing for example to detect copy\nnumber segments. Further it can segment data from RNA\nmicroarrays like tiling arrays to identify transcripts. Most\ngenerally, it can segment data given as a matrix or as a\nvector. Various data formats can be used as input to fastseg\nlike expression set objects for microarrays or GRanges for\nsequencing data. The segmentation criterion of fastseg is based\non a statistical test in a Bayesian framework, namely the cyber\nt-test (Baldi 2001). The speed-up arises from the facts, that\nsampling is not necessary in for fastseg and that a dynamic\nprogramming approach is used for calculation of the segments'\nfirst and higher order moments.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1063,
    "stars": 0
  },
  {
    "id": 13432,
    "package_name": "formulaic",
    "title": "Dynamic Generation and Quality Checks of Formula Objects",
    "description": "Many statistical models and analyses in R are implemented\nthrough formula objects. The formulaic package creates a\nunified approach for programmatically and dynamically\ngenerating formula objects. Users may specify the outcome and\ninputs of a model directly, search for variables to include\nbased upon naming patterns, incorporate interactions, and\nidentify variables to exclude. A wide range of quality checks\nare implemented to identify issues such as misspecified\nvariables, duplication, a lack of contrast in the inputs, and a\nlarge number of levels in categorical data.  Variables that do\nnot meet these quality checks can be automatically excluded\nfrom the model.  These issues are documented and reported in a\nmanner that provides greater accountability and useful\ninformation to guide an investigation of the data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1004,
    "stars": 0
  },
  {
    "id": 19416,
    "package_name": "peco",
    "title": "A Supervised Approach for **P**r**e**dicting **c**ell Cycle\nPr**o**gression using scRNA-seq data",
    "description": "Our approach provides a way to assign continuous cell\ncycle phase using scRNA-seq data, and consequently, allows to\nidentify cyclic trend of gene expression levels along the cell\ncycle. This package provides method and training data, which\nincludes scRNA-seq data collected from 6 individual cell lines\nof induced pluripotent stem cells (iPSCs), and also continuous\ncell cycle phase derived from FUCCI fluorescence imaging data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0878,
    "stars": 0
  },
  {
    "id": 17630,
    "package_name": "mlr3oml",
    "title": "Connector Between 'mlr3' and 'OpenML'",
    "description": "Provides an interface to 'OpenML.org' to list and download\nmachine learning data, tasks and experiments. The 'OpenML'\nobjects can be automatically converted to 'mlr3' objects.  For\na more sophisticated interface with more upload options, see\nthe 'OpenML' package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.065,
    "stars": 0
  },
  {
    "id": 18848,
    "package_name": "onnx",
    "title": "R Interface to 'ONNX'",
    "description": "R Interface to 'ONNX' - Open Neural Network Exchange\n<https://onnx.ai/>. 'ONNX' provides an open source format for\nmachine learning models. It defines an extensible computation\ngraph model, as well as definitions of built-in operators and\nstandard data types.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.043,
    "stars": 0
  },
  {
    "id": 14327,
    "package_name": "gips",
    "title": "Gaussian Model Invariant by Permutation Symmetry",
    "description": "Find the permutation symmetry group such that the\ncovariance matrix of the given data is approximately invariant\nunder it. Discovering such a permutation decreases the number\nof observations needed to fit a Gaussian model, which is of\ngreat use when it is smaller than the number of variables. Even\nif that is not the case, the covariance matrix found with\n'gips' approximates the actual covariance with less statistical\nerror. The methods implemented in this package are described in\nGraczyk et al. (2022) <doi:10.1214/22-AOS2174>. Documentation\nabout 'gips' is provided via its website at\n<https://przechoj.github.io/gips/> and the paper by Chojecki,\nMorgen, Kołodziejek (2025, <doi:10.18637/jss.v112.i07>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0388,
    "stars": 0
  },
  {
    "id": 19676,
    "package_name": "pipeliner",
    "title": "Machine Learning Pipelines for R",
    "description": "A framework for defining 'pipelines' of functions for\napplying data transformations, model estimation and\ninverse-transformations, resulting in predicted value\ngeneration (or model-scoring) functions that automatically\napply the entire pipeline of functions required to go from\ninput to predicted output.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0302,
    "stars": 0
  },
  {
    "id": 18773,
    "package_name": "oem",
    "title": "Orthogonalizing EM: Penalized Regression for Big Tall Data",
    "description": "Solves penalized least squares problems for big tall data\nusing the orthogonalizing EM algorithm of Xiong et al. (2016)\n<doi:10.1080/00401706.2015.1054436>. The main fitting function\nis oem() and the functions cv.oem() and xval.oem() are for\ncross validation, the latter being an accelerated cross\nvalidation function for linear models. The big.oem() function\nallows for out of memory fitting. A description of the\nunderlying methods and code interface is described in Huling\nand Chien (2022) <doi:10.18637/jss.v104.i06>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0224,
    "stars": 0
  },
  {
    "id": 4116,
    "package_name": "MRFcov",
    "title": "Markov Random Fields with Additional Covariates",
    "description": "Approximate node interaction parameters of Markov Random\nFields graphical networks. Models can incorporate additional\ncovariates, allowing users to estimate how interactions between\nnodes in the graph are predicted to change across covariate\ngradients. The general methods implemented in this package are\ndescribed in Clark et al. (2018) <doi:10.1002/ecy.2221>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0149,
    "stars": 0
  },
  {
    "id": 5091,
    "package_name": "PLSDAbatch",
    "title": "PLSDA-batch",
    "description": "A novel framework to correct for batch effects prior to\nany downstream analysis in microbiome data based on Projection\nto Latent Structures Discriminant Analysis. The main method is\nnamed “PLSDA-batch”. It first estimates treatment and batch\nvariation with latent components, then subtracts\nbatch-associated components from the data whilst preserving\nbiological variation of interest. PLSDA-batch is highly\nsuitable for microbiome data as it is non-parametric,\nmultivariate and allows for ordination and data visualisation.\nCombined with centered log-ratio transformation for addressing\nuneven library sizes and compositional structure, PLSDA-batch\naddresses all characteristics of microbiome data that existing\ncorrection methods have ignored so far. Two other variants are\nproposed for 1/ unbalanced batch x treatment designs that are\ncommonly encountered in studies with small sample sizes, and\nfor 2/ selection of discriminative variables amongst treatment\ngroups to avoid overfitting in classification problems. These\ntwo variants have widened the scope of applicability of\nPLSDA-batch to different data settings.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0143,
    "stars": 0
  },
  {
    "id": 11317,
    "package_name": "datafsm",
    "title": "Estimating Finite State Machine Models from Data",
    "description": "Automatic generation of finite state machine models of\ndynamic decision-making that both have strong predictive power\nand are interpretable in human terms. We use an efficient model\nrepresentation and a genetic algorithm-based estimation process\nto generate simple deterministic approximations that explain\nmost of the structure of complex stochastic processes. We have\napplied the software to empirical data, and demonstrated it's\nability to recover known data-generating processes by\nsimulating data with agent-based models and correctly deriving\nthe underlying decision models for multiple agent models and\ndegrees of stochasticity.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0099,
    "stars": 0
  },
  {
    "id": 20819,
    "package_name": "rags2ridges",
    "title": "Ridge Estimation of Precision Matrices from High-Dimensional\nData",
    "description": "Proper L2-penalized maximum likelihood estimators for\nprecision matrices and supporting functions to employ these\nestimators in a graphical modeling setting. For details, see\nPeeters, Bilgrau, & van Wieringen (2022)\n<doi:10.18637/jss.v102.i04> and associated publications.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9445,
    "stars": 0
  },
  {
    "id": 11854,
    "package_name": "dml",
    "title": "Distance Metric Learning in R",
    "description": "State-of-the-art algorithms for distance metric learning,\nincluding global and local methods such as Relevant Component\nAnalysis, Discriminative Component Analysis, Local Fisher\nDiscriminant Analysis, etc. These distance metric learning\nmethods are widely applied in feature extraction,\ndimensionality reduction, clustering, classification,\ninformation retrieval, and computer vision problems.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9395,
    "stars": 0
  },
  {
    "id": 9324,
    "package_name": "bioDist",
    "title": "Different distance measures",
    "description": "A collection of software tools for calculating distance\nmeasures.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8987,
    "stars": 0
  },
  {
    "id": 1288,
    "package_name": "ClustAssess",
    "title": "Tools for Assessing Clustering",
    "description": "A set of tools for evaluating clustering robustness using\nproportion of ambiguously clustered pairs (Senbabaoglu et al.\n(2014) <doi:10.1038/srep06207>), as well as similarity across\nmethods and method stability using element-centric clustering\ncomparison (Gates et al. (2019)\n<doi:10.1038/s41598-019-44892-y>). Additionally, this package\nenables stability-based parameter assessment for graph-based\nclustering pipelines typical in single-cell data analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8785,
    "stars": 0
  },
  {
    "id": 10723,
    "package_name": "consensusOV",
    "title": "Gene expression-based subtype classification for high-grade\nserous ovarian cancer",
    "description": "This package implements four major subtype classifiers for\nhigh-grade serous (HGS) ovarian cancer as described by Helland\net al. (PLoS One, 2011), Bentink et al. (PLoS One, 2012),\nVerhaak et al. (J Clin Invest, 2013), and Konecny et al. (J\nNatl Cancer Inst, 2014). In addition, the package implements a\nconsensus classifier, which consolidates and improves on the\nrobustness of the proposed subtype classifiers, thereby\nproviding reliable stratification of patients with HGS ovarian\ntumors of clearly defined subtype.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8785,
    "stars": 0
  },
  {
    "id": 21345,
    "package_name": "reservoirnet",
    "title": "Reservoir Computing and Echo State Networks",
    "description": "A simple user-friendly library based on the 'python'\nmodule 'reservoirpy'. It provides a flexible interface to\nimplement efficient Reservoir Computing (RC) architectures with\na particular focus on Echo State Networks (ESN). Some of its\nfeatures are: offline and online training, parallel\nimplementation, sparse matrix computation, fast spectral\ninitialization, advanced learning rules (e.g. Intrinsic\nPlasticity) etc. It also makes possible to easily create\ncomplex architectures with multiple reservoirs (e.g. deep\nreservoirs), readouts, and complex feedback loops. Moreover,\ngraphical tools are included to easily explore hyperparameters.\nFinally, it includes several tutorials exploring time series\nforecasting, classification and hyperparameter tuning. For more\ninformation about 'reservoirpy', please see Trouvain et al.\n(2020) <doi:10.1007/978-3-030-61616-8_40>. This package was\ndeveloped in the framework of the University of Bordeaux’s IdEx\n\"Investments for the Future\" program / RRI PHDS.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8573,
    "stars": 0
  },
  {
    "id": 1433,
    "package_name": "Coralysis",
    "title": "Coralysis sensitive identification of imbalanced cell types and\nstates in single-cell data via multi-level integration",
    "description": "Coralysis is an R package featuring a multi-level\nintegration algorithm for sensitive integration,\nreference-mapping, and cell-state identification in single-cell\ndata. The multi-level integration algorithm is inspired by the\nprocess of assembling a puzzle - where one begins by grouping\npieces based on low-to high-level features, such as color and\nshading, before looking into shape and patterns. This approach\nprogressively blends the batch effects and separates cell types\nacross multiple rounds of divisive clustering.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8325,
    "stars": 0
  },
  {
    "id": 2012,
    "package_name": "EGSEA",
    "title": "Ensemble of Gene Set Enrichment Analyses",
    "description": "This package implements the Ensemble of Gene Set\nEnrichment Analyses (EGSEA) method for gene set testing. EGSEA\nalgorithm utilizes the analysis results of twelve prominent GSE\nalgorithms in the literature to calculate collective\nsignificance scores for each gene set.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8325,
    "stars": 0
  },
  {
    "id": 17653,
    "package_name": "mlsurvlrnrs",
    "title": "R6-Based ML Survival Learners for 'mlexperiments'",
    "description": "Enhances 'mlexperiments'\n<https://CRAN.R-project.org/package=mlexperiments> with\nadditional machine learning ('ML') learners for survival\nanalysis. The package provides R6-based survival learners for\nthe following algorithms: 'glmnet'\n<https://CRAN.R-project.org/package=glmnet>, 'ranger'\n<https://CRAN.R-project.org/package=ranger>, 'xgboost'\n<https://CRAN.R-project.org/package=xgboost>, and 'rpart'\n<https://CRAN.R-project.org/package=rpart>. These can be used\ndirectly with the 'mlexperiments' R package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8293,
    "stars": 0
  },
  {
    "id": 22351,
    "package_name": "scds",
    "title": "In-Silico Annotation of Doublets for Single Cell RNA Sequencing\nData",
    "description": "In single cell RNA sequencing (scRNA-seq) data\ncombinations of cells are sometimes considered a single cell\n(doublets). The scds package provides methods to annotate\ndoublets in scRNA-seq data computationally.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8101,
    "stars": 0
  },
  {
    "id": 21440,
    "package_name": "rgeomorphon",
    "title": "A Lightweight Implementation of the Geomorphon Algorithm",
    "description": "A lightweight implementation of the geomorphon terrain\nform classification algorithm of Jasiewicz and Stepinski (2013)\n<doi:10.1016/j.geomorph.2012.11.005> based largely on the\n'GRASS GIS' 'r.geomorphon' module. This implementation employs\na novel algorithm written in C++ and 'RcppParallel'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8011,
    "stars": 0
  },
  {
    "id": 24930,
    "package_name": "tree.interpreter",
    "title": "Random Forest Prediction Decomposition and Feature Importance\nMeasure",
    "description": "An R re-implementation of the 'treeinterpreter' package on\nPyPI <https://pypi.org/project/treeinterpreter/>. Each\nprediction can be decomposed as 'prediction = bias +\nfeature_1_contribution + ... + feature_n_contribution'. This\ndecomposition is then used to calculate the Mean Decrease\nImpurity (MDI) and Mean Decrease Impurity using out-of-bag\nsamples (MDI-oob) feature importance measures based on the work\nof Li et al. (2019) <doi:10.48550/arXiv.1906.10845>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7782,
    "stars": 0
  },
  {
    "id": 17618,
    "package_name": "mlr3db",
    "title": "Data Base Backend for 'mlr3'",
    "description": "Extends the 'mlr3' package with a backend to transparently\nwork with databases such as 'SQLite', 'DuckDB', 'MySQL',\n'MariaDB', or 'PostgreSQL'. The package provides three\nadditional backends: 'DataBackendDplyr' relies on the\nabstraction of package 'dbplyr' to interact with most DBMS.\n'DataBackendDuckDB' operates on 'DuckDB' data bases and also on\nApache Parquet files. 'DataBackendPolars' operates on 'Polars'\ndata frames.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7623,
    "stars": 0
  },
  {
    "id": 13328,
    "package_name": "flowcatchR",
    "title": "Tools to analyze in vivo microscopy imaging data focused on\ntracking flowing blood cells",
    "description": "flowcatchR is a set of tools to analyze in vivo microscopy\nimaging data, focused on tracking flowing blood cells. It\nguides the steps from segmentation to calculation of features,\nfiltering out particles not of interest, providing also a set\nof utilities to help checking the quality of the performed\noperations (e.g. how good the segmentation was). It allows\ninvestigating the issue of tracking flowing cells such as in\nblood vessels, to categorize the particles in flowing, rolling\nand adherent. This classification is applied in the study of\nphenomena such as hemostasis and study of thrombosis\ndevelopment. Moreover, flowcatchR presents an integrated\nworkflow solution, based on the integration with a Shiny App\nand Jupyter notebooks, which is delivered alongside the\npackage, and can enable fully reproducible bioimage analysis in\nthe R environment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7482,
    "stars": 0
  },
  {
    "id": 25154,
    "package_name": "twoddpcr",
    "title": "Classify 2-d Droplet Digital PCR (ddPCR) data and quantify the\nnumber of starting molecules",
    "description": "The twoddpcr package takes Droplet Digital PCR (ddPCR)\ndroplet amplitude data from Bio-Rad's QuantaSoft and can\nclassify the droplets. A summary of the positive/negative\ndroplet counts can be generated, which can then be used to\nestimate the number of molecules using the Poisson\ndistribution. This is the first open source package that\nfacilitates the automatic classification of general two channel\nddPCR data. Previous work includes 'definetherain' (Jones et\nal., 2014) and 'ddpcRquant' (Trypsteen et al., 2015) which both\nhandle one channel ddPCR experiments only. The 'ddpcr' package\navailable on CRAN (Attali et al., 2016) supports automatic\ngating of a specific class of two channel ddPCR experiments\nonly.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7324,
    "stars": 0
  },
  {
    "id": 21405,
    "package_name": "rexposome",
    "title": "Exposome exploration and outcome data analysis",
    "description": "Package that allows to explore the exposome and to perform\nassociation analyses between exposures and health outcomes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7177,
    "stars": 0
  },
  {
    "id": 12137,
    "package_name": "eRTG3D",
    "title": "Empirically Informed Random Trajectory Generation in 3-D",
    "description": "Creates realistic random trajectories in a 3-D space\nbetween two given fix points, so-called conditional empirical\nrandom walks (CERWs). The trajectory generation is based on\nempirical distribution functions extracted from observed\ntrajectories (training data) and thus reflects the geometrical\nmovement characteristics of the mover. A digital elevation\nmodel (DEM), representing the Earth's surface, and a background\nlayer of probabilities (e.g. food sources, uplift potential,\nwaterbodies, etc.) can be used to influence the trajectories.\nUnterfinger M (2018). \"3-D Trajectory Simulation in Movement\nEcology: Conditional Empirical Random Walk\". Master's thesis,\nUniversity of Zurich.\n<https://www.geo.uzh.ch/dam/jcr:6194e41e-055c-4635-9807-53c5a54a3be7/MasterThesis_Unterfinger_2018.pdf>.\nTechnitis G, Weibel R, Kranstauber B, Safi K (2016). \"An\nalgorithm for empirically informed random trajectory generation\nbetween two endpoints\". GIScience 2016: Ninth International\nConference on Geographic Information Science, 9, online.\n<doi:10.5167/uzh-130652>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7101,
    "stars": 0
  },
  {
    "id": 17621,
    "package_name": "mlr3fda",
    "title": "Extending 'mlr3' to Functional Data Analysis",
    "description": "Extends the 'mlr3' ecosystem to functional analysis by\nadding support for irregular and regular functional data as\ndefined in the 'tf' package.  The package provides 'PipeOps'\nfor preprocessing functional columns and for extracting scalar\nfeatures, thereby allowing standard machine learning algorithms\nto be applied afterwards. Available operations include simple\nfunctional features such as the mean or maximum, smoothing,\ninterpolation, flattening, and functional 'PCA'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7076,
    "stars": 0
  },
  {
    "id": 18394,
    "package_name": "newsmap",
    "title": "Semi-Supervised Model for Geographical Document Classification",
    "description": "Semissupervised model for geographical document\nclassification (Watanabe 2018)\n<doi:10.1080/21670811.2017.1293487>. This package currently\ncontains seed dictionaries in English, German, French, Spanish,\nItalian, Russian, Hebrew, Arabic, Turkish, Japanese and Chinese\n(Simplified and Traditional).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.705,
    "stars": 0
  },
  {
    "id": 817,
    "package_name": "BreastSubtypeR",
    "title": "Cohort-aware methods for intrinsic molecular subtyping of breast\ncancer",
    "description": "BreastSubtypeR provides an assumption-aware, multi-method\nframework for intrinsic molecular subtyping of breast cancer.\nThe package harmonizes several published nearest-centroid (NC)\nand single-sample predictor (SSP) classifiers, supplies\nmethod-specific preprocessing and robust probe-to-gene mapping,\nand implements a cohort-aware AUTO mode that selectively\nenables classifiers compatible with the cohort composition. A\nlocal Shiny app (iBreastSubtypeR) is included for interactive\nanalyses and to support users without programming experience.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7042,
    "stars": 0
  },
  {
    "id": 10541,
    "package_name": "collinear",
    "title": "Automated Multicollinearity Management",
    "description": "Provides a comprehensive and automated workflow for\nmanaging multicollinearity in data frames with numeric and/or\ncategorical variables. The package integrates five robust\nmethods into a single function: (1) target encoding of\ncategorical variables based on response values (Micci-Barreca,\n2001 (Micci-Barreca, D. 2001 <doi:10.1145/507533.507538>); (2)\nautomated feature prioritization to preserve key predictors\nduring filtering; (3 and 4) pairwise correlation and VIF\nfiltering across all variable types (numeric–numeric,\nnumeric–categorical, and categorical–categorical); (5) adaptive\ncorrelation and VIF thresholds. Together, these methods enable\na reliable multicollinearity management in most use cases while\nmaintaining model integrity. The package also supports parallel\nprocessing and progress tracking via the packages 'future' and\n'progressr', and provides seamless integration with the\n'tidymodels' ecosystem through a dedicated recipe step.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.665,
    "stars": 0
  },
  {
    "id": 11108,
    "package_name": "ctc",
    "title": "Cluster and Tree Conversion.",
    "description": "Tools for export and import classification trees and\nclusters to other programs",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6474,
    "stars": 0
  },
  {
    "id": 17625,
    "package_name": "mlr3inferr",
    "title": "Inference on the Generalization Error",
    "description": "Confidence interval and resampling methods for inference\non the generalization error.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6353,
    "stars": 0
  },
  {
    "id": 17923,
    "package_name": "msaenet",
    "title": "Multi-Step Adaptive Estimation Methods for Sparse Regressions",
    "description": "Multi-step adaptive elastic-net (MSAENet) algorithm for\nfeature selection in high-dimensional regressions proposed in\nXiao and Xu (2015) <DOI:10.1080/00949655.2015.1016944>, with\nsupport for multi-step adaptive MCP-net (MSAMNet) and\nmulti-step adaptive SCAD-net (MSASNet) methods.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6258,
    "stars": 0
  },
  {
    "id": 8713,
    "package_name": "arulesCBA",
    "title": "Classification Based on Association Rules",
    "description": "Provides the infrastructure for association rule-based\nclassification including the algorithms CBA, CMAR, CPAR, C4.5,\nFOIL, PART, PRM, RCAR, and RIPPER to build associative\nclassifiers. Hahsler et al (2019) <doi:10.32614/RJ-2019-048>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6159,
    "stars": 0
  },
  {
    "id": 16491,
    "package_name": "live",
    "title": "Local Interpretable (Model-Agnostic) Visual Explanations",
    "description": "Interpretability of complex machine learning models is a\ngrowing concern. This package helps to understand key factors\nthat drive the decision made by complicated predictive model\n(so called black box model). This is achieved through local\napproximations that are either based on additive regression\nlike model or CART like model that allows for higher\ninteractions. The methodology is based on Tulio Ribeiro, Singh,\nGuestrin (2016) <doi:10.1145/2939672.2939778>. More details can\nbe found in Staniak, Biecek (2018) <doi:10.32614/RJ-2018-072>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5855,
    "stars": 0
  },
  {
    "id": 22248,
    "package_name": "scAnnotatR",
    "title": "Pretrained learning models for cell type prediction on single\ncell RNA-sequencing data",
    "description": "The package comprises a set of pretrained machine learning\nmodels to predict basic immune cell types. This enables all\nusers to quickly get a first annotation of the cell types\npresent in their dataset without requiring prior knowledge.\nscAnnotatR also allows users to train their own models to\npredict new cell types based on specific research needs.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5819,
    "stars": 0
  },
  {
    "id": 12448,
    "package_name": "enpls",
    "title": "Ensemble Partial Least Squares Regression",
    "description": "An algorithmic framework for measuring feature importance,\noutlier detection, model applicability domain evaluation, and\nensemble predictive modeling with (sparse) partial least\nsquares regressions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5563,
    "stars": 0
  },
  {
    "id": 7562,
    "package_name": "TSPred",
    "title": "Functions for Benchmarking Time Series Prediction",
    "description": "Functions for defining and conducting a time series\nprediction process including pre(post)processing,\ndecomposition, modelling, prediction and accuracy assessment.\nThe generated models and its yielded prediction errors can be\nused for benchmarking other time series prediction methods and\nfor creating a demand for the refinement of such methods. For\nthis purpose, benchmark data from prediction competitions may\nbe used.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5518,
    "stars": 0
  },
  {
    "id": 11900,
    "package_name": "dominatR",
    "title": "Feature Dominance-based R Package for Genomic Data",
    "description": "dominatR is an R package for quantifying and visualizing\nfeature dominance in datasets.  dominatR applies concepts drawn\nfrom physics such as center of mass and shannon's entropy to\neffectively visualize features (e.g. genes) that are present\nwithin a specific context or condition. The package integrates,\ndataframes, matrices and SummerizedExperiment objects and is\nable to perform common genomic normalization methods. The key\naspect is the generation of plots that serve to highlight\ncontext-relevant feature dominance.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5185,
    "stars": 0
  },
  {
    "id": 22301,
    "package_name": "scReClassify",
    "title": "scReClassify: post hoc cell type classification of single-cell\nRNA-seq data",
    "description": "A post hoc cell type classification tool to fine-tune cell\ntype annotations generated by any cell type classification\nprocedure with semi-supervised learning algorithm AdaSampling\ntechnique. The current version of scReClassify supports Support\nVector Machine and Random Forest as a base classifier.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5185,
    "stars": 0
  },
  {
    "id": 12200,
    "package_name": "ebm",
    "title": "Explainable Boosting Machines",
    "description": "An interface to the 'Python' 'InterpretML' framework for\nfitting explainable boosting machines (EBMs); see Nori et al.\n(2019) <doi:10.48550/arXiv.1909.09223> for. EBMs are a modern\ntype of generalized additive model that use tree-based, cyclic\ngradient boosting with automatic interaction detection. They\nare often as accurate as state-of-the-art blackbox models while\nremaining completely interpretable.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5051,
    "stars": 0
  },
  {
    "id": 18805,
    "package_name": "omicade4",
    "title": "Multiple co-inertia analysis of omics datasets",
    "description": "This package performes multiple co-inertia analysis of\nomics datasets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5024,
    "stars": 0
  },
  {
    "id": 2781,
    "package_name": "GSgalgoR",
    "title": "An Evolutionary Framework for the Identification and Study of\nPrognostic Gene Expression Signatures in Cancer",
    "description": "A multi-objective optimization algorithm for disease\nsub-type discovery based on a non-dominated sorting genetic\nalgorithm. The 'Galgo' framework combines the advantages of\nclustering algorithms for grouping heterogeneous 'omics' data\nand the searching properties of genetic algorithms for feature\nselection. The algorithm search for the optimal number of\nclusters determination considering the features that maximize\nthe survival difference between sub-types while keeping cluster\nconsistency high.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4771,
    "stars": 0
  },
  {
    "id": 10828,
    "package_name": "correctR",
    "title": "Corrected Test Statistics for Comparing Machine Learning Models\non Correlated Samples",
    "description": "Calculate a set of corrected test statistics for cases\nwhen samples are not independent, such as when classification\naccuracy values are obtained over resamples or through k-fold\ncross-validation, as proposed by Nadeau and Bengio (2003)\n<doi:10.1023/A:1024068626366> and presented in Bouckaert and\nFrank (2004) <doi:10.1007/978-3-540-24775-3_3>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4771,
    "stars": 0
  },
  {
    "id": 19294,
    "package_name": "pathMED",
    "title": "Scoring Personalized Molecular Portraits",
    "description": "PathMED is a collection of tools to facilitate precision\nmedicine studies with omics data (e.g. transcriptomics). Among\nits funcionalities, genesets scores for individual samples may\nbe calculated with several methods. These scores may be used to\ntrain machine learning models and to predict clinical features\non new data. For this, several machine learning methods are\nevaluated in order to select the best method based on internal\nvalidation and to tune the hyperparameters. Performance metrics\nand a ready-to-use model to predict the outcomes for new\npatients are returned.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4771,
    "stars": 0
  },
  {
    "id": 22720,
    "package_name": "shapley",
    "title": "Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid",
    "description": "This R package introduces Weighted Mean SHapley Additive\nexPlanations (WMSHAP), an innovative method for calculating\nSHAP values for a grid of fine-tuned base-learner machine\nlearning models as well as stacked ensembles, a method not\npreviously available due to the common reliance on single\nbest-performing models. By integrating the weighted mean SHAP\nvalues from individual base-learners comprising the ensemble or\nindividual base-learners in a tuning grid search, the package\nweights SHAP contributions according to each model's\nperformance, assessed by multiple either R squared (for both\nregression and classification models). alternatively, this\nsoftware also offers weighting SHAP values based on the area\nunder the precision-recall curve (AUCPR), the area under the\ncurve (AUC), and F2 measures for binary classifiers. It further\nextends this framework to implement weighted confidence\nintervals for weighted mean SHAP values, offering a more\ncomprehensive and robust feature importance evaluation over a\ngrid of machine learning models, instead of solely computing\nSHAP values for the best model. This methodology is\nparticularly beneficial for addressing the severe class\nimbalance (class rarity) problem by providing a transparent,\ngeneralized measure of feature importance that mitigates the\nrisk of reporting SHAP values for an overfitted or biased model\nand maintains robustness under severe class imbalance, where\nthere is no universal criteria of identifying the absolute best\nmodel. Furthermore, the package implements hypothesis testing\nto ascertain the statistical significance of SHAP values for\nindividual features, as well as comparative significance\ntesting of SHAP contributions between features. Additionally,\nit tackles a critical gap in feature selection literature by\npresenting criteria for the automatic feature selection of the\nmost important features across a grid of models or stacked\nensembles, eliminating the need for arbitrary determination of\nthe number of top features to be extracted. This utility is\ninvaluable for researchers analyzing feature significance,\nparticularly within severely imbalanced outcomes where\nconventional methods fall short. Moreover, it is also expected\nto report democratic feature importance across a grid of\nmodels, resulting in a more comprehensive and generalizable\nfeature selection. The package further implements a novel\nmethod for visualizing SHAP values both at subject level and\nfeature level as well as a plot for feature selection based on\nthe weighted mean SHAP ratios.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3952,
    "stars": 0
  },
  {
    "id": 8752,
    "package_name": "assignPOP",
    "title": "Population Assignment using Genetic, Non-Genetic or Integrated\nData in a Machine Learning Framework",
    "description": "Use Monte-Carlo and K-fold cross-validation coupled with\nmachine- learning classification algorithms to perform\npopulation assignment, with functionalities of evaluating\ndiscriminatory power of independent training samples,\nidentifying informative loci, reducing data dimensionality for\ngenomic data, integrating genetic and non-genetic data, and\nvisualizing results.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3945,
    "stars": 0
  },
  {
    "id": 70,
    "package_name": "AIMS",
    "title": "AIMS : Absolute Assignment of Breast Cancer Intrinsic Molecular\nSubtype",
    "description": "This package contains the AIMS implementation. It contains\nnecessary functions to assign the five intrinsic molecular\nsubtypes (Luminal A, Luminal B, Her2-enriched, Basal-like,\nNormal-like). Assignments could be done on individual samples\nas well as on dataset of gene expression data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3802,
    "stars": 0
  },
  {
    "id": 20234,
    "package_name": "promor",
    "title": "Proteomics Data Analysis and Modeling Tools",
    "description": "A comprehensive, user-friendly package for label-free\nproteomics data analysis and machine learning-based modeling.\nData generated from 'MaxQuant' can be easily used to conduct\ndifferential expression analysis, build predictive models with\ntop protein candidates, and assess model performance. promor\nincludes a suite of tools for quality control, visualization,\nmissing data imputation (Lazar et. al. (2016)\n<doi:10.1021/acs.jproteome.5b00981>), differential expression\nanalysis (Ritchie et. al. (2015) <doi:10.1093/nar/gkv007>), and\nmachine learning-based modeling (Kuhn (2008)\n<doi:10.18637/jss.v028.i05>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3802,
    "stars": 0
  },
  {
    "id": 17617,
    "package_name": "mlr3data",
    "title": "Collection of Machine Learning Data Sets for 'mlr3'",
    "description": "A small collection of interesting and educational machine\nlearning data sets which are used as examples in the 'mlr3'\nbook (<https://mlr3book.mlr-org.com>), the use case gallery\n(<https://mlr3gallery.mlr-org.com>), or in other examples. All\ndata sets are properly preprocessed and ready to be analyzed by\nmost machine learning algorithms.  Data sets are automatically\nadded to the dictionary of tasks if 'mlr3' is loaded.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3728,
    "stars": 0
  },
  {
    "id": 17736,
    "package_name": "modelgrid",
    "title": "A Framework for Creating, Managing and Training Multiple Caret\nModels",
    "description": "A minimalistic but flexible framework that facilitates the\ncreation, management and training of multiple 'caret' models. A\nmodel grid consists of two components: (1) a set of settings\nthat is shared by all models by default, and (2) specifications\nthat apply only to the individual models. When the model grid\nis trained, model and training specifications are first\nconsolidated from the shared and the model specific settings\ninto complete 'caret' model configurations. These models are\nthen trained with the 'train' function from the 'caret'\npackage.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3636,
    "stars": 0
  },
  {
    "id": 20076,
    "package_name": "preciseTAD",
    "title": "preciseTAD: A machine learning framework for precise TAD\nboundary prediction",
    "description": "preciseTAD provides functions to predict the location of\nboundaries of topologically associated domains (TADs) and\nchromatin loops at base-level resolution. As an input, it takes\nBED-formatted genomic coordinates of domain boundaries detected\nfrom low-resolution Hi-C data, and coordinates of\nhigh-resolution genomic annotations from ENCODE or other\nconsortia. preciseTAD employs several feature engineering\nstrategies and resampling techniques to address class\nimbalance, and trains an optimized random forest model for\npredicting low-resolution domain boundaries. Translated on a\nbase-level, preciseTAD predicts the probability for each base\nto be a boundary. Density-based clustering and scalable\npartitioning techniques are used to detect precise boundary\nregions and summit points. Compared with low-resolution\nboundaries, preciseTAD boundaries are highly enriched for CTCF,\nRAD21, SMC3, and ZNF143 signal and more conserved across cell\nlines. The pre-trained model can accurately predict boundaries\nin another cell line using CTCF, RAD21, SMC3, and ZNF143\nannotation data for this cell line.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3502,
    "stars": 0
  },
  {
    "id": 7078,
    "package_name": "Silhouette",
    "title": "Proximity Measure Based Diagnostics for Standard, Soft, and\nMulti-Way Clustering",
    "description": "Quantifies clustering quality by measuring both cohesion\nwithin clusters and separation between clusters. Implements\nadvanced silhouette width computations for diverse clustering\nstructures, including: simplified silhouette (Van der Laan et\nal., 2003) <doi:10.1080/0094965031000136012>, Probability of\nAlternative Cluster normalization methods (Raymaekers &\nRousseeuw, 2022) <doi:10.1080/10618600.2022.2050249>, fuzzy\nclustering and silhouette diagnostics using membership\nprobabilities (Campello & Hruschka, 2006; Menardi, 2011; Bhat &\nKiruthika, 2024) <doi:10.1016/j.fss.2006.07.006>,\n<doi:10.1007/s11222-010-9169-0>,\n<doi:10.1080/23737484.2024.2408534>, and multi-way clustering\nextensions such as block and tensor clustering (Schepers et\nal., 2008; Bhat & Kiruthika, 2025)\n<doi:10.1007/s00357-008-9005-9>,\n<doi:10.21203/rs.3.rs-6973596/v1>. Provides tools for\ncomputation and visualization (Rousseeuw, 1987)\n<doi:10.1016/0377-0427(87)90125-7> to support robust and\nreproducible cluster diagnostics across standard, soft, and\nmulti-way clustering settings.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3222,
    "stars": 0
  },
  {
    "id": 24458,
    "package_name": "tfaddons",
    "title": "Interface to 'TensorFlow SIG Addons'",
    "description": "'TensorFlow SIG Addons'\n<https://www.tensorflow.org/addons> is a repository of\ncommunity contributions that conform to well-established API\npatterns, but implement new functionality not available in core\n'TensorFlow'. 'TensorFlow' natively supports a large number of\noperators, layers, metrics, losses, optimizers, and more.\nHowever, in a fast moving field like Machine Learning, there\nare many interesting new developments that cannot be integrated\ninto core 'TensorFlow' (because their broad applicability is\nnot yet clear, or it is mostly used by a smaller subset of the\ncommunity).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2788,
    "stars": 0
  },
  {
    "id": 20775,
    "package_name": "rScudo",
    "title": "Signature-based Clustering for Diagnostic Purposes",
    "description": "SCUDO (Signature-based Clustering for Diagnostic Purposes)\nis a rank-based method for the analysis of gene expression\nprofiles for diagnostic and classification purposes. It is\nbased on the identification of sample-specific gene signatures\ncomposed of the most up- and down-regulated genes for that\nsample. Starting from gene expression data, functions in this\npackage identify sample-specific gene signatures and use them\nto build a graph of samples. In this graph samples are joined\nby edges if they have a similar expression profile, according\nto a pre-computed similarity matrix. The similarity between the\nexpression profiles of two samples is computed using a method\nsimilar to GSEA. The graph of samples can then be used to\nperform community clustering or to perform supervised\nclassification of samples in a testing set.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2553,
    "stars": 0
  },
  {
    "id": 12021,
    "package_name": "dsos",
    "title": "Dataset Shift with Outlier Scores",
    "description": "Test for no adverse shift in two-sample comparison when we\nhave a training set, the reference distribution, and a test\nset. The approach is flexible and relies on a robust and\npowerful test statistic, the weighted AUC. Technical details\nare in Kamulete, V. M. (2021) <arXiv:1908.04000>. Modern\nnotions of outlyingness such as trust scores and prediction\nuncertainty can be used as the underlying scores for example.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.248,
    "stars": 0
  },
  {
    "id": 11532,
    "package_name": "demuxSNP",
    "title": "scRNAseq demultiplexing using cell hashing and SNPs",
    "description": "This package assists in demultiplexing scRNAseq data using\nboth cell hashing and SNPs data. The SNP profile of each group\nos learned using high confidence assignments from the cell\nhashing data. Cells which cannot be assigned with high\nconfidence from the cell hashing data are assigned to their\nmost similar group based on their SNPs. We also provide some\nhelper function to optimise SNP selection, create training data\nand merge SNP data into the SingleCellExperiment framework.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1875,
    "stars": 0
  },
  {
    "id": 24146,
    "package_name": "switchBox",
    "title": "Utilities to train and validate classifiers based on pair\nswitching using the K-Top-Scoring-Pair (KTSP) algorithm",
    "description": "The package offer different classifiers based on\ncomparisons of pair of features (TSP), using various decision\nrules (e.g., majority wins principle).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1638,
    "stars": 0
  },
  {
    "id": 24324,
    "package_name": "tdarec",
    "title": "A 'recipes' Extension for Persistent Homology and Its\nVectorizations",
    "description": "Topological data analytic methods in machine learning rely\non vectorizations of the persistence diagrams that encode\npersistent homology, as surveyed by Ali &al (2000)\n<doi:10.48550/arXiv.2212.09703>.  Persistent homology can be\ncomputed using 'TDA' and 'ripserr' and vectorized using\n'TDAvec'.  The Tidymodels package collection modularizes\nmachine learning in R for straightforward extensibility; see\nKuhn & Silge (2022, ISBN:978-1-4920-9644-3).  These 'recipe'\nsteps and 'dials' tuners make efficient algorithms for\ncomputing and vectorizing persistence diagrams available for\nTidymodels workflows.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1538,
    "stars": 0
  },
  {
    "id": 3183,
    "package_name": "HybridExpress",
    "title": "Comparative analysis of RNA-seq data for hybrids and their\nprogenitors",
    "description": "HybridExpress can be used to perform comparative\ntranscriptomics analysis of hybrids (or allopolyploids)\nrelative to their progenitor species. The package features\nfunctions to perform exploratory analyses of sample grouping,\nidentify differentially expressed genes in hybrids relative to\ntheir progenitors, classify genes in expression categories (N =\n12) and classes (N = 5), and perform functional analyses. We\nalso provide users with graphical functions for the seamless\ncreation of publication-ready figures that are commonly used in\nthe literature.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1461,
    "stars": 0
  },
  {
    "id": 7693,
    "package_name": "TrIdent",
    "title": "TrIdent - Transduction Identification",
    "description": "The `TrIdent` R package automates the analysis of\ntransductomics data by detecting, classifying, and\ncharacterizing read coverage patterns associated with potential\ntransduction events. Transductomics is a DNA sequencing-based\nmethod for the detection and characterization of transduction\nevents in pure cultures and complex communities. Transductomics\nrelies on mapping sequencing reads from a viral-like particle\n(VLP)-fraction of a sample to contigs assembled from the\nmetagenome (whole-community) of the same sample. Reads from\nbacterial DNA carried by VLPs will map back to the bacterial\ncontigs of origin creating read coverage patterns indicative of\nongoing transduction.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1461,
    "stars": 0
  },
  {
    "id": 984,
    "package_name": "CMA",
    "title": "Synthesis of microarray-based classification",
    "description": "This package provides a comprehensive collection of\nvarious microarray-based classification algorithms both from\nMachine Learning and Statistics. Variable Selection,\nHyperparameter tuning, Evaluation and Comparison can be\nperformed combined or stepwise in a user-friendly environment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1335,
    "stars": 0
  },
  {
    "id": 23714,
    "package_name": "stackgbm",
    "title": "Stacked Gradient Boosting Machines",
    "description": "A minimalist implementation of model stacking by Wolpert\n(1992) <doi:10.1016/S0893-6080(05)80023-1> for boosted tree\nmodels. A classic, two-layer stacking model is implemented,\nwhere the first layer generates features using gradient\nboosting trees, and the second layer employs a logistic\nregression model that uses these features as inputs. Utilities\nfor training the base models and parameters tuning are\nprovided, allowing users to experiment with different ensemble\nconfigurations easily. It aims to provide a simple and\nefficient way to combine multiple gradient boosting models to\nimprove predictive model performance and robustness.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0969,
    "stars": 0
  },
  {
    "id": 19060,
    "package_name": "outForest",
    "title": "Multivariate Outlier Detection and Replacement",
    "description": "Provides a random forest based implementation of the\nmethod described in Chapter 7.1.2 (Regression model based\nanomaly detection) of Chandola et al. (2009)\n<doi:10.1145/1541880.1541882>. It works as follows: Each\nnumeric variable is regressed onto all other variables by a\nrandom forest. If the scaled absolute difference between\nobserved value and out-of-bag prediction of the corresponding\nrandom forest is suspiciously large, then a value is considered\nan outlier. The package offers different options to replace\nsuch outliers, e.g. by realistic values found via predictive\nmean matching. Once the method is trained on a reference data,\nit can be applied to new data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0917,
    "stars": 0
  },
  {
    "id": 22556,
    "package_name": "semisup",
    "title": "Semi-Supervised Mixture Model",
    "description": "Implements a parametric semi-supervised mixture model. The\npermutation test detects markers with main or interactive\neffects, without distinguishing them. Possible applications\ninclude genome-wide association analysis and differential\nexpression analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0792,
    "stars": 0
  },
  {
    "id": 8303,
    "package_name": "adverSCarial",
    "title": "adverSCarial, generate and analyze the vulnerability of\nscRNA-seq classifier to adversarial attacks",
    "description": "adverSCarial is an R Package designed for generating and\nanalyzing the vulnerability of scRNA-seq classifiers to\nadversarial attacks. The package is versatile and provides a\nformat for integrating any type of classifier. It offers\nfunctions for studying and generating two types of attacks,\nsingle gene attack and max change attack. The single-gene\nattack involves making a small modification to the input to\nalter the classification. The max-change attack involves making\na large modification to the input without changing its\nclassification. The CGD attack is based on an estimated\ngradient descent. against adversarial attacks. The package\nprovides a comprehensive solution for evaluating the robustness\nof scRNA-seq classifiers against adversarial attacks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0569,
    "stars": 0
  },
  {
    "id": 4730,
    "package_name": "NuPoP",
    "title": "An R package for nucleosome positioning prediction",
    "description": "NuPoP is an R package for Nucleosome Positioning\nPrediction.This package is built upon a duration hidden Markov\nmodel proposed in Xi et al, 2010; Wang et al, 2008. The core of\nthe package was written in Fotran. In addition to the R\npackage, a stand-alone Fortran software tool is also available\nat https://github.com/jipingw. The Fortran codes have complete\nfunctonality as the R package.  Note: NuPoP has two separate\nfunctions for prediction of nucleosome positioning, one for\nMNase-map trained models and the other for chemical map-trained\nmodels. The latter was implemented for four species including\nyeast, S.pombe, mouse and human, trained based on our recent\npublications. We noticed there is another package nuCpos by\nanother group for prediction of nucleosome positioning trained\nwith chemicals. A report to compare recent versions of NuPoP\nwith nuCpos can be found at\nhttps://github.com/jiping/NuPoP_doc. Some more information can\nbe found and will be posted at\nhttps://github.com/jipingw/NuPoP.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0414,
    "stars": 0
  },
  {
    "id": 8832,
    "package_name": "autokeras",
    "title": "R Interface to 'AutoKeras'",
    "description": "R Interface to 'AutoKeras' <https://autokeras.com/>.\n'AutoKeras' is an open source software library for Automated\nMachine Learning (AutoML). The ultimate goal of AutoML is to\nprovide easily accessible deep learning tools to domain experts\nwith limited data science or machine learning background.\n'AutoKeras' provides functions to automatically search for\narchitecture and hyperparameters of deep learning models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0394,
    "stars": 0
  },
  {
    "id": 20632,
    "package_name": "quitefastmst",
    "title": "Euclidean and Mutual Reachability Minimum Spanning Trees",
    "description": "Functions to compute Euclidean minimum spanning trees\nusing single-, sesqui-, and dual-tree Borůvka algorithms.\nThanks to K-d trees, they are fast in spaces of low intrinsic\ndimensionality.  Mutual reachability distances (used in the\ndefinition of the 'HDBSCAN*' algorithm) are also supported.\nThe package also features relatively fast fallback minimum\nspanning tree and nearest-neighbours algorithms for spaces of\nhigher dimensionality.  The 'Python' version of 'quitefastmst'\nis available via 'PyPI'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0326,
    "stars": 0
  },
  {
    "id": 11143,
    "package_name": "cuda.ml",
    "title": "R Interface for the RAPIDS cuML Suite of Libraries",
    "description": "R interface for RAPIDS cuML\n(<https://github.com/rapidsai/cuml>), a suite of\nGPU-accelerated machine learning libraries powered by CUDA\n(<https://en.wikipedia.org/wiki/CUDA>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.023,
    "stars": 0
  },
  {
    "id": 1758,
    "package_name": "DaMiRseq",
    "title": "Data Mining for RNA-seq data: normalization, feature selection\nand classification",
    "description": "The DaMiRseq package offers a tidy pipeline of data mining\nprocedures to identify transcriptional biomarkers and exploit\nthem for both binary and multi-class classification purposes.\nThe package accepts any kind of data presented as a table of\nraw counts and allows including both continous and factorial\nvariables that occur with the experimental setting. A series of\nfunctions enable the user to clean up the data by filtering\ngenomic features and samples, to adjust data by identifying and\nremoving the unwanted source of variation (i.e. batches and\nconfounding factors) and to select the best predictors for\nmodeling. Finally, a \"stacking\" ensemble learning technique is\napplied to build a robust classification model. Every step\nincludes a checkpoint that the user may exploit to assess the\neffects of data management by looking at diagnostic plots, such\nas clustering and heatmaps, RLE boxplots, MDS or correlation\nplot.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0212,
    "stars": 0
  },
  {
    "id": 12347,
    "package_name": "einops",
    "title": "Flexible, Powerful, and Readable Tensor Operations",
    "description": "Perform tensor operations using a concise yet expressive\nsyntax inspired by the Python library of the same name.\nReshape, rearrange, and combine multidimensional arrays for\nscientific computing, machine learning, and data analysis.\nEinops simplifies complex manipulations, making code more\nmaintainable and intuitive. The original implementation is\ndemonstrated in Rogozhnikov (2022)\n<https://openreview.net/forum?id=oapKSVM2bcj>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0212,
    "stars": 0
  },
  {
    "id": 19561,
    "package_name": "phenoTest",
    "title": "Tools to test association between gene expression and phenotype\nin a way that is efficient, structured, fast and scalable. We\nalso provide tools to do GSEA (Gene set enrichment analysis)\nand copy number variation.",
    "description": "Tools to test correlation between gene expression and\nphenotype in a way that is efficient, structured, fast and\nscalable. GSEA is also provided.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0187,
    "stars": 0
  },
  {
    "id": 18024,
    "package_name": "multiclassPairs",
    "title": "Build MultiClass Pair-Based Classifiers using TSPs or RF",
    "description": "A toolbox to train a single sample classifier that uses\nin-sample feature relationships. The relationships are\nrepresented as feature1 < feature2 (e.g. gene1 < gene2). We\nprovide two options to go with. First is based on 'switchBox'\npackage which uses Top-score pairs algorithm. Second is a novel\nimplementation based on random forest algorithm. For simple\nproblems we recommend to use one-vs-rest using TSP option due\nto its simplicity and for being easy to interpret.  For complex\nproblems RF performs better.  Both lines filter the features\nfirst then combine the filtered features to make the list of\nall the possible rules (i.e. rule1: feature1 < feature2, rule2:\nfeature1 < feature3, etc...).  Then the list of rules will be\nfiltered and the most important and informative rules will be\nkept. The informative rules will be assembled in an one-vs-rest\nmodel or in an RF model.  We provide a detailed description\nwith each function in this package to explain the filtration\nand training methodology in each line. Reference: Marzouka &\nEriksson (2021) <doi:10.1093/bioinformatics/btab088>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.017,
    "stars": 0
  },
  {
    "id": 3849,
    "package_name": "MAI",
    "title": "Mechanism-Aware Imputation",
    "description": "A two-step approach to imputing missing data in\nmetabolomics. Step 1 uses a random forest classifier to\nclassify missing values as either Missing Completely at\nRandom/Missing At Random (MCAR/MAR) or Missing Not At Random\n(MNAR). MCAR/MAR are combined because it is often difficult to\ndistinguish these two missing types in metabolomics data. Step\n2 imputes the missing values based on the classified missing\nmechanisms, using the appropriate imputation algorithms.\nImputation algorithms tested and available for MCAR/MAR include\nBayesian Principal Component Analysis (BPCA), Multiple\nImputation No-Skip K-Nearest Neighbors (Multi_nsKNN), and\nRandom Forest. Imputation algorithms tested and available for\nMNAR include nsKNN and a single imputation approach for\nimputation of metabolites where left-censoring is present.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5,
    "stars": 0
  },
  {
    "id": 17588,
    "package_name": "mlearning",
    "title": "'SciViews::R' - Machine Learning Algorithms with Unified\nInterface",
    "description": "A unified interface is provided to various machine\nlearning algorithms like linear or quadratic discriminant\nanalysis, k-nearest neighbors, random forest, support vector\nmachine, ... It allows to train, test, and apply\ncross-validation using similar functions and function arguments\nwith a minimalist and clean, formula-based interface. Missing\ndata are processed the same way as base and stats R functions\nfor all algorithms, both in training and testing. Confusion\nmatrices are also provided with a rich set of metrics\ncalculated and a few specific plots.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9823,
    "stars": 0
  },
  {
    "id": 17351,
    "package_name": "miRNAtap",
    "title": "miRNAtap: microRNA Targets - Aggregated Predictions",
    "description": "The package facilitates implementation of workflows\nrequiring miRNA predictions, it allows to integrate ranked\nmiRNA target predictions from multiple sources available online\nand aggregate them with various methods which improves quality\nof predictions above any of the single sources. Currently\npredictions are available for Homo sapiens, Mus musculus and\nRattus norvegicus (the last one through homology translation).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9731,
    "stars": 0
  },
  {
    "id": 22915,
    "package_name": "sigFeature",
    "title": "sigFeature: Significant feature selection using SVM-RFE &\nt-statistic",
    "description": "This package provides a novel feature selection algorithm\nfor binary classification using support vector machine\nrecursive feature elimination SVM-RFE and t-statistic. In this\nfeature selection process, the selected features are\ndifferentially significant between the two classes and also\nthey are good classifier with higher degree of classification\naccuracy.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9638,
    "stars": 0
  },
  {
    "id": 20197,
    "package_name": "procoil",
    "title": "Prediction of Oligomerization of Coiled Coil Proteins",
    "description": "The package allows for predicting whether a coiled coil\nsequence (amino acid sequence plus heptad register) is more\nlikely to form a dimer or more likely to form a trimer.\nAdditionally to the prediction itself, a prediction profile is\ncomputed which allows for determining the strengths to which\nthe individual residues are indicative for either class.\nPrediction profiles can also be visualized as curves or\nheatmaps.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9542,
    "stars": 0
  },
  {
    "id": 13444,
    "package_name": "foto",
    "title": "Fourier Transform Textural Ordination",
    "description": "A tool to use a principal component analysis on radially\naveraged two dimensional Fourier spectra to characterize image\ntexture. The method within the context of ecology was first\ndescribed by Couteron et al. (2005)\n<doi:10.1111/j.1365-2664.2005.01097.x> and expanded upon by\nSolorzano et al. (2018) <doi:10.1117/1.JRS.12.036006> using a\nmoving window approach.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9445,
    "stars": 0
  },
  {
    "id": 1012,
    "package_name": "CNVPanelizer",
    "title": "Reliable CNV detection in targeted sequencing applications",
    "description": "A method that allows for the use of a collection of\nnon-matched normal tissue samples. Our approach uses a\nnon-parametric bootstrap subsampling of the available reference\nsamples to estimate the distribution of read counts from\ntargeted sequencing. As inspired by random forest, this is\ncombined with a procedure that subsamples the amplicons\nassociated with each of the targeted genes. The obtained\ninformation allows us to reliably classify the copy number\naberrations on the gene level.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9243,
    "stars": 0
  },
  {
    "id": 1828,
    "package_name": "DepecheR",
    "title": "Determination of essential phenotypic elements of clusters in\nhigh-dimensional entities",
    "description": "The purpose of this package is to identify traits in a\ndataset that can separate groups. This is done on two levels.\nFirst, clustering is performed, using an implementation of\nsparse K-means. Secondly, the generated clusters are used to\npredict outcomes of groups of individuals based on their\ndistribution of observations in the different clusters. As\ncertain clusters with separating information will be\nidentified, and these clusters are defined by a sparse number\nof variables, this method can reduce the complexity of data, to\nonly emphasize the data that actually matters.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9243,
    "stars": 0
  },
  {
    "id": 18446,
    "package_name": "nipalsMCIA",
    "title": "Multiple Co-Inertia Analysis via the NIPALS Method",
    "description": "Computes Multiple Co-Inertia Analysis (MCIA), a\ndimensionality reduction (jDR) algorithm, for a multi-block\ndataset using a modification to the Nonlinear Iterative Partial\nLeast Squares method (NIPALS) proposed in (Hanafi et. al,\n2010). Allows multiple options for row- and table-level\npreprocessing, and speeds up computation of variance explained.\nVignettes detail application to bulk- and single cell-\nmulti-omics studies.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9243,
    "stars": 0
  },
  {
    "id": 17435,
    "package_name": "milr",
    "title": "Multiple-Instance Logistic Regression with LASSO Penalty",
    "description": "The multiple instance data set consists of many\nindependent subjects (called bags) and each subject is composed\nof several components (called instances). The outcomes of such\ndata set are binary or categorical responses, and, we can only\nobserve the subject-level outcomes. For example, in\nmanufacturing processes, a subject is labeled as \"defective\" if\nat least one of its own components is defective, and otherwise,\nis labeled as \"non-defective\". The 'milr' package focuses on\nthe predictive model for the multiple instance data set with\nbinary outcomes and performs the maximum likelihood estimation\nwith the Expectation-Maximization algorithm under the framework\nof logistic regression. Moreover, the LASSO penalty is attached\nto the likelihood function for simultaneous parameter\nestimation and variable selection.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8751,
    "stars": 0
  },
  {
    "id": 4044,
    "package_name": "MLSeq",
    "title": "Machine Learning Interface for RNA-Seq Data",
    "description": "This package applies several machine learning methods,\nincluding SVM, bagSVM, Random Forest and CART to RNA-Seq data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8573,
    "stars": 0
  },
  {
    "id": 20583,
    "package_name": "quantiseqr",
    "title": "Quantification of the Tumor Immune contexture from RNA-seq data",
    "description": "This package provides a streamlined workflow for the\nquanTIseq method, developed to perform the quantification of\nthe Tumor Immune contexture from RNA-seq data. The\nquantification is performed against the TIL10 signature\n(dissecting the contributions of ten immune cell types),\ncarefully crafted from a collection of human RNA-seq samples.\nThe TIL10 signature has been extensively validated using\nsimulated, flow cytometry, and immunohistochemistry data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8426,
    "stars": 0
  },
  {
    "id": 10597,
    "package_name": "cometr",
    "title": "'Comet' API for R",
    "description": "A convenient 'R' wrapper to the 'Comet' API, which is a\ncloud platform allowing you to track, compare, explain and\noptimize machine learning experiments and models. Experiments\ncan be viewed on the 'Comet' online dashboard at\n<https://www.comet.com>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8062,
    "stars": 0
  },
  {
    "id": 1794,
    "package_name": "DeMixT",
    "title": "Cell type-specific deconvolution of heterogeneous tumor samples\nwith two or three components using expression data from RNAseq\nor microarray platforms",
    "description": "DeMixT is a software package that performs deconvolution\non transcriptome data from a mixture of two or three\ncomponents.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7959,
    "stars": 0
  },
  {
    "id": 3950,
    "package_name": "MEB",
    "title": "A normalization-invariant minimum enclosing ball method to\ndetect differentially expressed genes for RNA-seq and scRNA-seq\ndata",
    "description": "This package provides a method to identify differential\nexpression genes in the same or different species. Given that\nnon-DE genes have some similarities in features, a scaling-free\nminimum enclosing ball (SFMEB) model is built to cover those\nnon-DE genes in feature space, then those DE genes, which are\nenormously different from non-DE genes, being regarded as\noutliers and rejected outside the ball. The method on this\npackage is described in the article 'A minimum enclosing ball\nmethod to detect differential expression genes for RNA-seq\ndata'. The SFMEB method is extended to the scMEB method that\nconsidering two or more potential types of cells or unknown\nlabels scRNA-seq dataset DEGs identification.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 4042,
    "package_name": "MLPUGS",
    "title": "Multi-Label Prediction Using Gibbs Sampling (and Classifier\nChains)",
    "description": "An implementation of classifier chains (CC's) for\nmulti-label prediction. Users can employ an external package\n(e.g. 'randomForest', 'C50'), or supply their own. The package\ncan train a single set of CC's or train an ensemble of CC's --\nin parallel if running in a multi-core environment. New\nobservations are classified using a Gibbs sampler since each\nunobserved label is conditioned on the others. The package\nincludes methods for evaluating the predictions for accuracy\nand aggregating across iterations and models to produce binary\nor probabilistic classifications.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 10598,
    "package_name": "comets",
    "title": "Covariance Measure Tests for Conditional Independence",
    "description": "Covariance measure tests for conditional independence\ntesting against conditional covariance and nonlinear\nconditional mean alternatives. The package implements versions\nof the generalised covariance measure test (Shah and Peters,\n2020, <doi:10.1214/19-aos1857>) and projected covariance\nmeasure test (Lundborg et al., 2023, <doi:10.1214/24-AOS2447>).\nThe tram-GCM test, for censored responses, is implemented\nincluding the Cox model and survival forests (Kook et al.,\n2024, <doi:10.1080/01621459.2024.2395588>). Application\nexamples to variable significance testing and modality\nselection can be found in Kook and Lundborg (2024,\n<doi:10.1093/bib/bbae475>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 10720,
    "package_name": "consICA",
    "title": "consensus Independent Component Analysis",
    "description": "consICA implements a data-driven deconvolution method –\nconsensus independent component analysis (ICA) to decompose\nheterogeneous omics data and extract features suitable for\npatient diagnostics and prognostics. The method separates\nbiologically relevant transcriptional signals from technical\neffects and provides information about the cellular composition\nand biological processes. The implementation of parallel\ncomputing in the package ensures efficient analysis of modern\nmulticore systems.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 16305,
    "package_name": "leakr",
    "title": "Data Leakage Detection Tools for Machine Learning",
    "description": "Provides utilities to detect common data leakage patterns\nincluding train/test contamination, temporal leakage, and data\nduplication, enhancing model reliability and reproducibility in\nmachine learning workflows. Generates diagnostic reports and\nvisual summaries to support data validation. Methods based on\nbest practices from Hastie, Tibshirani, and Friedman (2009,\nISBN:978-0387848570).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 21164,
    "package_name": "recorder",
    "title": "Toolkit to Validate New Data for a Predictive Model",
    "description": "A lightweight toolkit to validate new observations when\ncomputing their predictions with a predictive model. The\nvalidation process consists of two steps: (1) record relevant\nstatistics and meta data of the variables in the original\ntraining data for the predictive model and (2) use these data\nto run a set of basic validation tests on the new set of\nobservations.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 24015,
    "package_name": "survClust",
    "title": "Identification Of Clinically Relevant Genomic Subtypes Using\nOutcome Weighted Learning",
    "description": "survClust is an outcome weighted integrative clustering\nalgorithm used to classify multi-omic samples on their\navailable time to event information. The resulting clusters are\ncross-validated to avoid over overfitting and output\nclassification of samples that are molecularly distinct and\nclinically meaningful. It takes in binary (mutation) as well as\ncontinuous data (other omic types).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7604,
    "stars": 0
  },
  {
    "id": 8817,
    "package_name": "autoEnsemble",
    "title": "Automated Stacked Ensemble Classifier for Severe Class Imbalance",
    "description": "A stacking solution for modeling imbalanced and severely\nskewed data. It automates the process of building homogeneous\nor heterogeneous stacked ensemble models by selecting \"best\"\nmodels according to different criteria. In doing so, it\nstrategically searches for and selects diverse, high-performing\nbase-learners to construct ensemble models optimized for skewed\ndata. This package is particularly useful for addressing class\nimbalance in datasets, ensuring robust and effective model\noutcomes through advanced ensemble strategies which aim to\nstabilize the model, reduce its overfitting, and further\nimprove its generalizability.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7288,
    "stars": 0
  },
  {
    "id": 14753,
    "package_name": "guildai",
    "title": "Track Machine Learning Experiments",
    "description": "'Guild AI' is an open-source tool for managing machine\nlearning experiments. It's for scientists, engineers, and\nresearchers who want to run scripts, compare results, measure\nprogress, and automate machine learning workflows. 'Guild AI'\nis a light weight, external tool that runs locally. It works\nwith any framework, doesn't require any changes to your code,\nor access to any web services. Users can easily record\nexperiment metadata, track model changes, manage experiment\nartifacts, tune hyperparameters, and share results. 'Guild AI'\ncombines features from 'Git', 'SQLite', and 'Make' to provide a\nlab notebook for machine learning.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.716,
    "stars": 0
  },
  {
    "id": 2570,
    "package_name": "GARS",
    "title": "GARS: Genetic Algorithm for the identification of Robust Subsets\nof variables in high-dimensional and challenging datasets",
    "description": "Feature selection aims to identify and remove redundant,\nirrelevant and noisy variables from high-dimensional datasets.\nSelecting informative features affects the subsequent\nclassification and regression analyses by improving their\noverall performances. Several methods have been proposed to\nperform feature selection: most of them relies on univariate\nstatistics, correlation, entropy measurements or the usage of\nbackward/forward regressions. Herein, we propose an efficient,\nrobust and fast method that adopts stochastic optimization\napproaches for high-dimensional. GARS is an innovative\nimplementation of a genetic algorithm that selects robust\nfeatures in high-dimensional and challenging datasets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 15979,
    "package_name": "katdetectr",
    "title": "Detection, Characterization and Visualization of Kataegis in\nSequencing Data",
    "description": "Kataegis refers to the occurrence of regional\nhypermutation and is a phenomenon observed in a wide range of\nmalignancies. Using changepoint detection katdetectr aims to\nidentify putative kataegis foci from common data-formats\nhousing genomic variants.  Katdetectr has shown to be a robust\npackage for the detection, characterization and visualization\nof kataegis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 13406,
    "package_name": "forestError",
    "title": "A Unified Framework for Random Forest Prediction Error\nEstimation",
    "description": "Estimates the conditional error distributions of random\nforest predictions and common parameters of those\ndistributions, including conditional misclassification rates,\nconditional mean squared prediction errors, conditional biases,\nand conditional quantiles, by out-of-bag weighting of\nout-of-bag prediction errors as proposed by Lu and Hardin\n(2021). This package is compatible with several existing\npackages that implement random forests in R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6702,
    "stars": 0
  },
  {
    "id": 717,
    "package_name": "BioMoR",
    "title": "Bioinformatics Modeling with Recursion and Autoencoder-Based\nEnsemble",
    "description": "Provides tools for bioinformatics modeling using recursive\ntransformer-inspired architectures, autoencoders, random\nforests, XGBoost, and stacked ensemble models. Includes\nutilities for cross-validation, calibration, benchmarking, and\nthreshold optimization in predictive modeling workflows.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 24818,
    "package_name": "torchopt",
    "title": "Advanced Optimizers for Torch",
    "description": "Optimizers for 'torch' deep learning library. These\nfunctions include recent results published in the literature\nand are not part of the optimizers offered in 'torch'.\nProspective users should test these optimizers with their data,\nsince performance depends on the specific problem being solved.\nThe packages includes the following optimizers: (a) 'adabelief'\nby Zhuang et al (2020), <arXiv:2010.07468>; (b) 'adabound' by\nLuo et al.(2019), <arXiv:1902.09843>; (c) 'adahessian' by Yao\net al.(2021) <arXiv:2006.00719>; (d) 'adamw' by Loshchilov &\nHutter (2019), <arXiv:1711.05101>; (e) 'madgrad' by Defazio and\nJelassi (2021), <arXiv:2101.11075>; (f) 'nadam' by Dozat\n(2019), <https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf>;\n(g) 'qhadam' by Ma and Yarats(2019), <arXiv:1810.06801>; (h)\n'radam' by Liu et al. (2019), <arXiv:1908.03265>; (i) 'swats'\nby Shekar and Sochee (2018), <arXiv:1712.07628>; (j) 'yogi' by\nZaheer et al.(2019),\n<https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6454,
    "stars": 0
  },
  {
    "id": 15826,
    "package_name": "jackstraw",
    "title": "Statistical Inference for Unsupervised Learning",
    "description": "Test for association between the observed data and their\nestimated latent variables. The jackstraw package provides a\nresampling strategy and testing scheme to estimate statistical\nsignificance of association between the observed data and their\nlatent variables. Depending on the data type and the analysis\naim, the latent variables may be estimated by principal\ncomponent analysis (PCA), factor analysis (FA), K-means\nclustering, and related unsupervised learning algorithms. The\njackstraw methods learn over-fitting characteristics inherent\nin this circular analysis, where the observed data are used to\nestimate the latent variables and used again to test against\nthat estimated latent variables. When latent variables are\nestimated by PCA, the jackstraw enables statistical testing for\nassociation between observed variables and latent variables, as\nestimated by low-dimensional principal components (PCs). This\nessentially leads to identifying variables that are\nsignificantly associated with PCs. Similarly, unsupervised\nclustering, such as K-means clustering, partition around\nmedoids (PAM), and others, finds coherent groups in\nhigh-dimensional data. The jackstraw estimates statistical\nsignificance of cluster membership, by testing association\nbetween data and cluster centers. Clustering membership can be\nimproved by using the resulting jackstraw p-values and\nposterior inclusion probabilities (PIPs), with an application\nto unsupervised evaluation of cell identities in single cell\nRNA-seq (scRNA-seq).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6191,
    "stars": 0
  },
  {
    "id": 845,
    "package_name": "CAEN",
    "title": "Category encoding method for selecting feature genes for the\nclassification of single-cell RNA-seq",
    "description": "With the development of high-throughput techniques, more\nand more gene expression analysis tend to replace\nhybridization-based microarrays with the revolutionary\ntechnology.The novel method encodes the category again by\nemploying the rank of samples for each gene in each class. We\nthen consider the correlation coefficient of gene and class\nwith rank of sample and new rank of category. The highest\ncorrelation coefficient genes are considered as the feature\ngenes which are most effective to classify the samples.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 1303,
    "package_name": "ClusterSignificance",
    "title": "The ClusterSignificance package provides tools to assess if\nclass clusters in dimensionality reduced data representations\nhave a separation different from permuted data",
    "description": "The ClusterSignificance package provides tools to assess\nif class clusters in dimensionality reduced data\nrepresentations have a separation different from permuted data.\nThe term class clusters here refers to, clusters of points\nrepresenting known classes in the data. This is particularly\nuseful to determine if a subset of the variables, e.g. genes in\na specific pathway, alone can separate samples into these\nestablished classes. ClusterSignificance accomplishes this by,\nprojecting all points onto a one dimensional line. Cluster\nseparations are then scored and the probability of the seen\nseparation being due to chance is evaluated using a permutation\nmethod.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 16630,
    "package_name": "looking4clusters",
    "title": "Interactive Visualization of scRNA-Seq",
    "description": "Enables the interactive visualization of dimensional\nreduction, clustering, and cell properties for scRNA-Seq\nresults. It generates an interactive HTML page using either a\nnumeric matrix, SummarizedExperiment, SingleCellExperiment or\nSeurat objects as input. The input data can be projected into\ntwo-dimensional representations by applying dimensionality\nreduction methods such as PCA, MDS, t-SNE, UMAP, and NMF.\nDisplaying multiple dimensionality reduction results within the\nsame interface, with interconnected graphs, provides different\nperspectives that facilitate accurate cell classification. The\npackage also integrates unsupervised clustering techniques,\nwhose results that can be viewed interactively in the graphical\ninterface. In addition to visualization, this interface allows\nmanual selection of groups, labeling of cell entities based on\nprocessed meta-information, generation of new graphs displaying\ngene expression values for each cell, sample identification,\nand visual comparison of samples and clusters.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 18297,
    "package_name": "nempi",
    "title": "Inferring unobserved perturbations from gene expression data",
    "description": "Takes as input an incomplete perturbation profile and\ndifferential gene expression in log odds and infers unobserved\nperturbations and augments observed ones. The inference is done\nby iteratively inferring a network from the perturbations and\ninferring perturbations from the network. The network inference\nis done by Nested Effects Models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 13880,
    "package_name": "genie",
    "title": "Fast, Robust, and Outlier Resistant Hierarchical Clustering",
    "description": "Includes the reference implementation of Genie - a\nhierarchical clustering algorithm that links two point groups\nin such a way that an inequity measure (namely, the Gini index)\nof the cluster sizes does not significantly increase above a\ngiven threshold. This method most often outperforms many other\ndata segmentation approaches in terms of clustering quality as\ntested on a wide range of benchmark datasets. At the same time,\nGenie retains the high speed of the single linkage approach,\ntherefore it is also suitable for analysing larger data sets.\nFor more details see (Gagolewski et al. 2016\n<DOI:10.1016/j.ins.2016.05.003>). For an even faster and more\nfeature-rich implementation, including, amongst others, noise\npoint detection, see the 'genieclust' package (Gagolewski, 2021\n<DOI:10.1016/j.softx.2021.100722>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5977,
    "stars": 0
  },
  {
    "id": 4924,
    "package_name": "PAGFL",
    "title": "Joint Estimation of Latent Groups and Group-Specific\nCoefficients in (Time-Varying) Panel Data Models",
    "description": "Latent group structures are a common challenge in panel\ndata analysis. Disregarding group-level heterogeneity can\nintroduce bias. Conversely, estimating individual coefficients\nfor each cross-sectional unit is inefficient and may lead to\nhigh uncertainty. This package addresses the issue of\nunobservable group structures by implementing the pairwise\nadaptive group fused Lasso (PAGFL) by Mehrabani (2023)\n<doi:10.1016/j.jeconom.2022.12.002>. PAGFL identifies latent\ngroup structures and group-specific coefficients in a single\nstep. On top of that, we extend the PAGFL to time-varying\ncoefficient functions (FUSE-TIME), following Haimerl et al.\n(2025) <doi:10.48550/arXiv.2503.23165>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5911,
    "stars": 0
  },
  {
    "id": 16940,
    "package_name": "massiR",
    "title": "massiR: MicroArray Sample Sex Identifier",
    "description": "Predicts the sex of samples in gene expression microarray\ndatasets",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5911,
    "stars": 0
  },
  {
    "id": 197,
    "package_name": "AdapDiscom",
    "title": "Adaptive Sparse Regression for Block Missing Multimodal Data",
    "description": "Provides adaptive direct sparse regression for\nhigh-dimensional multimodal data with heterogeneous missing\npatterns and measurement errors. 'AdapDISCOM' extends the\n'DISCOM' framework with modality-specific adaptive weighting to\nhandle varying data structures and error magnitudes across\nblocks. The method supports flexible block configurations (any\nK blocks) and includes robust variants for heavy-tailed\ndistributions ('AdapDISCOM'-Huber) and fast implementations for\nlarge-scale applications (Fast-'AdapDISCOM'). Designed for\nrealistic multimodal scenarios where different data sources\nexhibit distinct missing data patterns and contamination\nlevels. Diakité et al. (2025) <doi:10.48550/arXiv.2508.00120>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5441,
    "stars": 0
  },
  {
    "id": 21137,
    "package_name": "recalibratiNN",
    "title": "Quantile Recalibration for Regression Models",
    "description": "Enables the diagnostics and enhancement of regression\nmodel calibration.It offers both global and local visualization\ntools for calibration diagnostics and provides one\nrecalibration method: Torres R, Nott DJ, Sisson SA, Rodrigues\nT, Reis JG, Rodrigues GS (2024)\n<doi:10.48550/arXiv.2403.05756>. The method leverages on\nProbabilistic Integral Transform (PIT) values to both evaluate\nand perform the calibration of statistical models. For a more\ndetailed description of the package, please refer to the\nbachelor's thesis available bellow.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5441,
    "stars": 0
  },
  {
    "id": 23708,
    "package_name": "stablehlo",
    "title": "Write stableHLO programs",
    "description": "The package offers a low level interface to create\nstableHLO programs.  These programs can be compiled and run on\ndifferent hardware backends (CPU, GPU, ...)  using the 'pjrt'\npackage.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5378,
    "stars": 0
  },
  {
    "id": 8959,
    "package_name": "banter",
    "title": "BioAcoustic eveNT classifiER",
    "description": "Create a hierarchical acoustic event species classifier\nout of multiple call type detectors as described in Rankin et\nal (2017) <doi:10.1111/mms.12381>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5224,
    "stars": 0
  },
  {
    "id": 12678,
    "package_name": "evaluomeR",
    "title": "Evaluation of Bioinformatics Metrics",
    "description": "Evaluating the reliability of your own metrics and the\nmeasurements done on your own datasets by analysing the\nstability and goodness of the classifications of such metrics.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5185,
    "stars": 0
  },
  {
    "id": 13739,
    "package_name": "gasper",
    "title": "Graph Signal Processing",
    "description": "Provides the standard operations for signal processing on\ngraphs: graph Fourier transform, spectral graph wavelet\ntransform, visualization tools. It also implements a data\ndriven method for graph signal denoising/regression, for\ndetails see De Loynes, Navarro, Olivier (2019)\n<arxiv:1906.01882>. The package also provides an interface to\nthe SuiteSparse Matrix Collection, <https://sparse.tamu.edu/>,\na large and widely used set of sparse matrix benchmarks\ncollected from a wide range of applications.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5051,
    "stars": 0
  },
  {
    "id": 6640,
    "package_name": "SCFA",
    "title": "SCFA: Subtyping via Consensus Factor Analysis",
    "description": "Subtyping via Consensus Factor Analysis (SCFA) can\nefficiently remove noisy signals from consistent molecular\npatterns in multi-omics data. SCFA first uses an autoencoder to\nselect only important features and then repeatedly performs\nfactor analysis to represent the data with different numbers of\nfactors. Using these representations, it can reliably identify\ncancer subtypes and accurately predict risk scores of patients.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 7216,
    "package_name": "SpatialDDLS",
    "title": "Deconvolution of Spatial Transcriptomics Data Based on Neural\nNetworks",
    "description": "Deconvolution of spatial transcriptomics data based on\nneural networks and single-cell RNA-seq data. SpatialDDLS\nimplements a workflow to create neural network models able to\nmake accurate estimates of cell composition of spots from\nspatial transcriptomics data using deep learning and the\nmeaningful information provided by single-cell RNA-seq data.\nSee Torroja and Sanchez-Cabo (2019)\n<doi:10.3389/fgene.2019.00978> and Mañanes et al. (2024)\n<doi:10.1093/bioinformatics/btae072> to get an overview of the\nmethod and see some examples of its performance.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 13121,
    "package_name": "fgga",
    "title": "Hierarchical ensemble method based on factor graph",
    "description": "Package that implements the FGGA algorithm. This package\nprovides a hierarchical ensemble method based ob factor graphs\nfor the consistent cross-ontology annotation of protein coding\ngenes. FGGA embodies elements of predicate logic, communication\ntheory, supervised learning and inference in graphical models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 23456,
    "package_name": "spatzie",
    "title": "Identification of enriched motif pairs from chromatin\ninteraction data",
    "description": "Identifies motifs that are significantly co-enriched from\nenhancer-promoter interaction data. While enhancer-promoter\nannotation is commonly used to define groups of interaction\nanchors, spatzie also supports co-enrichment analysis between\npreprocessed interaction anchors.  Supports BEDPE interaction\ndata derived from genome-wide assays such as HiC, ChIA-PET, and\nHiChIP. Can also be used to look for differentially enriched\nmotif pairs between two interaction experiments.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 16315,
    "package_name": "learningmachine",
    "title": "Machine Learning with Explanations and Uncertainty\nQuantification",
    "description": "Regression-based Machine Learning with explanations and\nuncertainty quantification.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4191,
    "stars": 0
  },
  {
    "id": 9918,
    "package_name": "catsim",
    "title": "Binary and Categorical Image Similarity Index",
    "description": "Computes a structural similarity metric (after the style\nof MS-SSIM for images) for binary and categorical 2D and 3D\nimages. Can be based on accuracy (simple matching), Cohen's\nkappa, Rand index, adjusted Rand index, Jaccard index, Dice\nindex, normalized mutual information, or adjusted mutual\ninformation. In addition, has fast computation of Cohen's\nkappa, the Rand indices, and the two mutual informations.\nImplements the methods of Thompson and Maitra (2020)\n<doi:10.48550/arXiv.2004.09073>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3979,
    "stars": 0
  },
  {
    "id": 19694,
    "package_name": "pjrt",
    "title": "R Interface to PJRT",
    "description": "Provides an R interface to PJRT (Pretty much Just another\nRunTime), which allows you to run XLA or stableHLO programs on\na variety of hardware backends including CPU, GPU, and TPU.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3829,
    "stars": 0
  },
  {
    "id": 1040,
    "package_name": "COSNet",
    "title": "Cost Sensitive Network for node label prediction on graphs with\nhighly unbalanced labelings",
    "description": "Package that implements the COSNet classification\nalgorithm. The algorithm predicts node labels in partially\nlabeled graphs where few positives are available for the class\nbeing predicted.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3802,
    "stars": 0
  },
  {
    "id": 5338,
    "package_name": "Pigengene",
    "title": "Infers biological signatures from gene expression data",
    "description": "Pigengene package provides an efficient way to infer\nbiological signatures from gene expression profiles. The\nsignatures are independent from the underlying platform, e.g.,\nthe input can be microarray or RNA Seq data. It can even infer\nthe signatures using data from one platform, and evaluate them\non the other. Pigengene identifies the modules (clusters) of\nhighly coexpressed genes using coexpression network analysis,\nsummarizes the biological information of each module in an\neigengene, learns a Bayesian network that models the\nprobabilistic dependencies between modules, and builds a\ndecision tree based on the expression of eigengenes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3802,
    "stars": 0
  },
  {
    "id": 8104,
    "package_name": "a4Core",
    "title": "Automated Affymetrix Array Analysis Core Package",
    "description": "Utility functions for the Automated Affymetrix Array\nAnalysis set of packages.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3802,
    "stars": 0
  },
  {
    "id": 13803,
    "package_name": "geNetClassifier",
    "title": "Classify diseases and build associated gene networks using gene\nexpression profiles",
    "description": "Comprehensive package to automatically train and validate\na multi-class SVM classifier based on gene expression data.\nProvides transparent selection of gene markers, their\ncoexpression networks, and an interface to query the\nclassifier.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3802,
    "stars": 0
  },
  {
    "id": 23954,
    "package_name": "subsemble",
    "title": "An Ensemble Method for Combining Subset-Specific Algorithm Fits",
    "description": "The Subsemble algorithm is a general subset ensemble\nprediction method, which can be used for small, moderate, or\nlarge datasets. Subsemble partitions the full dataset into\nsubsets of observations, fits a specified underlying algorithm\non each subset, and uses a unique form of k-fold\ncross-validation to output a prediction function that combines\nthe subset-specific fits. An oracle result provides a\ntheoretical performance guarantee for Subsemble. The paper,\n\"Subsemble: An ensemble method for combining subset-specific\nalgorithm fits\" is authored by Stephanie Sapp, Mark J. van der\nLaan & John Canny (2014) <doi:10.1080/02664763.2013.864263>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3738,
    "stars": 0
  },
  {
    "id": 4918,
    "package_name": "PAA",
    "title": "PAA (Protein Array Analyzer)",
    "description": "PAA imports single color (protein) microarray data that\nhas been saved in gpr file format - esp. ProtoArray data. After\npreprocessing (background correction, batch filtering,\nnormalization) univariate feature preselection is performed\n(e.g., using the \"minimum M statistic\" approach - hereinafter\nreferred to as \"mMs\"). Subsequently, a multivariate feature\nselection is conducted to discover biomarker candidates.\nTherefore, either a frequency-based backwards elimination\naproach or ensemble feature selection can be used. PAA provides\na complete toolbox of analysis tools including several\ndifferent plots for results examination and evaluation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3424,
    "stars": 0
  },
  {
    "id": 11211,
    "package_name": "cytoMEM",
    "title": "Marker Enrichment Modeling (MEM)",
    "description": "MEM, Marker Enrichment Modeling, automatically generates\nand displays quantitative labels for cell populations that have\nbeen identified from single-cell data. The input for MEM is a\ndataset that has pre-clustered or pre-gated populations with\ncells in rows and features in columns. Labels convey a list of\nmeasured features and the features' levels of relative\nenrichment on each population. MEM can be applied to a wide\nvariety of data types and can compare between MEM labels from\nflow cytometry, mass cytometry, single cell RNA-seq, and\nspectral flow cytometry using RMSD.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3222,
    "stars": 0
  },
  {
    "id": 4978,
    "package_name": "PDATK",
    "title": "Pancreatic Ductal Adenocarcinoma Tool-Kit",
    "description": "Pancreatic ductal adenocarcinoma (PDA) has a relatively\npoor prognosis and is one of the most lethal cancers. Molecular\nclassification of gene expression profiles holds the potential\nto identify meaningful subtypes which can inform therapeutic\nstrategy in the clinical setting. The Pancreatic Cancer\nAdenocarcinoma Tool-Kit (PDATK) provides an S4 class-based\ninterface for performing unsupervised subtype discovery,\ncross-cohort meta-clustering, gene-expression-based\nclassification, and subsequent survival analysis to identify\nprognostically useful subtypes in pancreatic cancer and beyond.\nTwo novel methods, Consensus Subtypes in Pancreatic Cancer\n(CSPC) and Pancreatic Cancer Overall Survival Predictor (PCOSP)\nare included for consensus-based meta-clustering and\noverall-survival prediction, respectively. Additionally, four\npublished subtype classifiers and three published prognostic\ngene signatures are included to allow users to easily recreate\npublished results, apply existing classifiers to new data, and\nbenchmark the relative performance of new methods. The use of\nexisting Bioconductor classes as input to all PDATK classes and\nmethods enables integration with existing Bioconductor\ndatasets, including the 21 pancreatic cancer patient cohorts\navailable in the MetaGxPancreas data package. PDATK has been\nused to replicate results from Sandhu et al (2019)\n[https://doi.org/10.1200/cci.18.00102] and an additional paper\nis in the works using CSPC to validate subtypes from the\nincluded published classifiers, both of which use the data\navailable in MetaGxPancreas. The inclusion of subtype centroids\nand prognostic gene signatures from these and other\npublications will enable researchers and clinicians to classify\nnovel patient gene expression data, allowing the direct\nclinical application of the classifiers included in PDATK.\nOverall, PDATK provides a rich set of tools to identify and\nvalidate useful prognostic and molecular subtypes based on\ngene-expression data, benchmark new classifiers against\nexisting ones, and apply discovered classifiers on novel\npatient data to inform clinical decision making.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3096,
    "stars": 0
  },
  {
    "id": 2533,
    "package_name": "FuzzyClass",
    "title": "Fuzzy and Non-Fuzzy Classifiers",
    "description": "It provides classifiers which can be used for discrete\nvariables and for continuous variables based on the Naive Bayes\nand Fuzzy Naive Bayes hypothesis. Those methods were developed\nby researchers belong to the 'Laboratory of Technologies for\nVirtual Teaching and Statistics (LabTEVE)' and 'Laboratory of\nApplied Statistics to Image Processing and Geoprocessing\n(LEAPIG)' at 'Federal University of Paraiba, Brazil'. They\nconsidered some statistical distributions and their papers were\npublished in the scientific literature, as for instance, the\nGaussian classifier using fuzzy parameters, proposed by\n'Moraes, Ferreira and Machado' (2021)\n<doi:10.1007/s40815-020-00936-4>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 6998,
    "package_name": "SemDist",
    "title": "Information Accretion-based Function Predictor Evaluation",
    "description": "This package implements methods to calculate information\naccretion for a given version of the gene ontology and uses\nthis data to calculate remaining uncertainty, misinformation,\nand semantic similarity for given sets of predicted annotations\nand true annotations from a protein function predictor.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 14016,
    "package_name": "geva",
    "title": "Gene Expression Variation Analysis (GEVA)",
    "description": "Statistic methods to evaluate variations of differential\nexpression (DE) between multiple biological conditions. It\ntakes into account the fold-changes and p-values from previous\ndifferential expression (DE) results that use large-scale data\n(*e.g.*, microarray and RNA-seq) and evaluates which genes\nwould react in response to the distinct experiments. This\nevaluation involves an unique pipeline of statistical methods,\nincluding weighted summarization, quantile detection, cluster\nanalysis, and ANOVA tests, in order to classify a subset of\nrelevant genes whose DE is similar or dependent to certain\nbiological factors.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 15386,
    "package_name": "idr2d",
    "title": "Irreproducible Discovery Rate for Genomic Interactions Data",
    "description": "A tool to measure reproducibility between genomic\nexperiments that produce two-dimensional peaks (interactions\nbetween peaks), such as ChIA-PET, HiChIP, and HiC. idr2d is an\nextension of the original idr package, which is intended for\n(one-dimensional) ChIP-seq peaks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 22397,
    "package_name": "scoup",
    "title": "Simulate Codons with Darwinian Selection Modelled as an OU\nProcess",
    "description": "An elaborate molecular evolutionary framework that\nfacilitates straightforward simulation of codon genetic\nsequences subjected to different degrees and/or patterns of\nDarwinian selection. The model is built upon the fitness\nlandscape paradigm of Sewall Wright, as popularised by the\nmutation-selection model of Halpern and Bruno. This enables\nrealistic evolutionary process of living organisms to be\nreproducible seamlessly. For example, an Ornstein-Uhlenbeck\nfitness update algorithm is incorporated herein. Consequently,\notherwise complex biological processes, such as the effect of\nthe interplay between genetic drift and fitness landscape\nfluctuations on the inference of diversifying selection, may\nnow be investigated with minimal effort. Frequency-dependent\nand stochastic fitness landscape update techniques are\navailable.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 3761,
    "package_name": "LedPred",
    "title": "Learning from DNA to Predict Enhancers",
    "description": "This package aims at creating a predictive model of\nregulatory sequences used to score unknown sequences based on\nthe content of DNA motifs, next-generation sequencing (NGS)\npeaks and signals and other numerical scores of the sequences\nusing supervised classification. The package contains a\nworkflow based on the support vector machine (SVM) algorithm\nthat maps features to sequences, optimize SVM parameters and\nfeature number and creates a model that can be stored and used\nto score the regulatory potential of unknown sequences.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2553,
    "stars": 0
  },
  {
    "id": 13672,
    "package_name": "gaga",
    "title": "GaGa hierarchical model for high-throughput data analysis",
    "description": "Implements the GaGa model for high-throughput data\nanalysis, including differential expression analysis,\nsupervised gene clustering and classification. Additionally, it\nperforms sequential sample size calculations using the GaGa and\nLNNGV models (the latter from EBarrays package).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2553,
    "stars": 0
  },
  {
    "id": 17596,
    "package_name": "mlim",
    "title": "Single and Multiple Imputation with Automated Machine Learning",
    "description": "Machine learning algorithms have been used for performing\nsingle missing data imputation and most recently, multiple\nimputations. However, this is the first attempt for using\nautomated machine learning algorithms for performing both\nsingle and multiple imputation. Automated machine learning is a\nprocedure for fine-tuning the model automatic, performing a\nrandom search for a model that results in less error, without\noverfitting the data. The main idea is to allow the model to\nset its own parameters for imputing each variable separately\ninstead of setting fixed predefined parameters to impute all\nvariables of the dataset. Using automated machine learning, the\npackage fine-tunes an Elastic Net (default) or Gradient\nBoosting, Random Forest, Deep Learning, Extreme Gradient\nBoosting, or Stacked Ensemble machine learning model (from one\nor a combination of other supported algorithms) for imputing\nthe missing observations. This procedure has been implemented\nfor the first time by this package and is expected to\noutperform other packages for imputing missing data that do not\nfine-tune their models. The multiple imputation is implemented\nvia bootstrapping without letting the duplicated observations\nto harm the cross-validation procedure, which is the way\nimputed variables are evaluated. Most notably, the package\nimplements automated procedure for handling imputing imbalanced\ndata (class rarity problem), which happens when a factor\nvariable has a level that is far more prevalent than the\nother(s). This is known to result in biased predictions, hence,\nbiased imputation of missing data. However, the autobalancing\nprocedure ensures that instead of focusing on maximizing\naccuracy (classification error) in imputing factor variables, a\nfairer procedure and imputation method is practiced.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2041,
    "stars": 0
  },
  {
    "id": 2956,
    "package_name": "GrafGen",
    "title": "Classification of Helicobacter Pylori Genomes",
    "description": "To classify Helicobacter pylori genomes according to\ngenetic distance from nine reference populations. The nine\nreference populations are hpgpAfrica, hpgpAfrica-distant,\nhpgpAfroamerica, hpgpEuroamerica, hpgpMediterranea, hpgpEurope,\nhpgpEurasia, hpgpAsia, and hpgpAklavik86-like. The vertex\npopulations are Africa, Europe and Asia.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 4776,
    "package_name": "ORFhunteR",
    "title": "Predict open reading frames in nucleotide sequences",
    "description": "The ORFhunteR package is a R and C++ library for an\nautomatic determination and annotation of open reading frames\n(ORF) in a large set of RNA molecules. It efficiently\nimplements the machine learning model based on vectorization of\nnucleotide sequences and the random forest classification\nalgorithm. The ORFhunteR package consists of a set of functions\nwritten in the R language in conjunction with C++. The\nefficiency of the package was confirmed by the examples of the\nanalysis of RNA molecules from the NCBI RefSeq and Ensembl\ndatabases. The package can be used in basic and applied\nbiomedical research related to the study of the transcriptome\nof normal as well as altered (for example, cancer) human cells.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 5362,
    "package_name": "PoDCall",
    "title": "Positive Droplet Calling for DNA Methylation Droplet Digital PCR",
    "description": "Reads files exported from 'QX Manager or QuantaSoft'\ncontaining amplitude values from a run of ddPCR (96 well plate)\nand robustly sets thresholds to determine positive droplets for\neach channel of each individual well. Concentration and\nnormalized concentration in addition to other metrics is then\ncalculated for each well. Results are returned as a table,\noptionally written to file, as well as optional plots\n(scatterplot and histogram) for both channels per well written\nto file. The package includes a shiny application which\nprovides an interactive and user-friendly interface to the full\nfunctionality of PoDCall.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 18477,
    "package_name": "nlpred",
    "title": "Estimators of Non-Linear Cross-Validated Risks Optimized for\nSmall Samples",
    "description": "Methods for obtaining improved estimates of non-linear\ncross-validated risks are obtained using targeted minimum\nloss-based estimation, estimating equations, and one-step\nestimation (Benkeser, Petersen, van der Laan (2019),\n<doi:10.1080/01621459.2019.1668794>). Cross-validated area\nunder the receiver operating characteristics curve (LeDell,\nPetersen, van der Laan (2015), <doi:10.1214/15-EJS1035>) and\nother metrics are included.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 6100,
    "package_name": "RadioGx",
    "title": "Analysis of Large-Scale Radio-Genomic Data",
    "description": "Computational tool box for radio-genomic analysis which\nintegrates radio-response data, radio-biological modelling and\ncomprehensive cell line annotations for hundreds of cancer cell\nlines. The 'RadioSet' class enables creation and manipulation\nof standardized datasets including information about cancer\ncells lines, radio-response assays and dose-response\nindicators. Included methods allow fitting and plotting\ndose-response data using established radio-biological models\nalong with quality control to validate results. Additional\nfunctions related to fitting and plotting dose response curves,\nquantifying statistical correlation and calculating area under\nthe curve (AUC) or survival fraction (SF) are included. For\nmore details please see the included documentation, references,\nas well as: Manem, V. et al (2018) <doi:10.1101/449793>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1461,
    "stars": 0
  },
  {
    "id": 13852,
    "package_name": "geneClassifiers",
    "title": "Application of gene classifiers",
    "description": "This packages aims for easy accessible application of\nclassifiers which have been published in literature using an\nExpressionSet as input.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0792,
    "stars": 0
  },
  {
    "id": 5723,
    "package_name": "RECA",
    "title": "Relevant Component Analysis for Supervised Distance Metric\nLearning",
    "description": "Relevant Component Analysis (RCA) tries to find a linear\ntransformation of the feature space such that the effect of\nirrelevant variability is reduced in the transformed space.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0212,
    "stars": 0
  },
  {
    "id": 11481,
    "package_name": "deep",
    "title": "A Neural Networks Framework",
    "description": "This package provides a layer oriented way of creating\nneural networks, the framework is intended to give the user\ntotal control of the internals of a net without much effort.\nUse classes like PerceptronLayer to create a layer of percetron\nneurons, and specify how many you want. The package does all\nthe tricky stuff internally leaving you focused in what you\nwant. I wrote this package during a neural networks course to\nhelp me with the problem set.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0212,
    "stars": 0
  },
  {
    "id": 10285,
    "package_name": "cleandata",
    "title": "To Inspect and Manipulate Data; and to Keep Track of This\nProcess",
    "description": "Functions to work with data frames to prepare data for\nfurther analysis. The functions for imputation, encoding,\npartitioning, and other manipulation can produce log files to\nkeep track of process.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0149,
    "stars": 0
  },
  {
    "id": 1067,
    "package_name": "CRImage",
    "title": "CRImage a package to classify cells and calculate tumour\ncellularity",
    "description": "CRImage provides functionality to process and analyze\nimages, in particular to classify cells in biological images.\nFurthermore, in the context of tumor images, it provides\nfunctionality to calculate tumour cellularity.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 2643,
    "package_name": "GGPA",
    "title": "graph-GPA: A graphical model for prioritizing GWAS results and\ninvestigating pleiotropic architecture",
    "description": "Genome-wide association studies (GWAS) is a widely used\ntool for identification of genetic variants associated with\nphenotypes and diseases, though complex diseases featuring many\ngenetic variants with small effects present difficulties for\ntraditional these studies. By leveraging pleiotropy, the\nstatistical power of a single GWAS can be increased. This\npackage provides functions for fitting graph-GPA, a statistical\nframework to prioritize GWAS results by integrating pleiotropy.\n'GGPA' package provides user-friendly interface to fit\ngraph-GPA models, implement association mapping, and generate a\nphenotype graph.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 4335,
    "package_name": "MethPed",
    "title": "A DNA methylation classifier tool for the identification of\npediatric brain tumor subtypes",
    "description": "Classification of pediatric tumors into biologically\ndefined subtypes is challenging and multifaceted approaches are\nneeded. For this aim, we developed a diagnostic classifier\nbased on DNA methylation profiles. We offer MethPed as an\neasy-to-use toolbox that allows researchers and clinical\ndiagnosticians to test single samples as well as large cohorts\nfor subclass prediction of pediatric brain tumors.  The current\nversion of MethPed can classify the following tumor\ndiagnoses/subgroups: Diffuse Intrinsic Pontine Glioma (DIPG),\nEpendymoma, Embryonal tumors with multilayered rosettes (ETMR),\nGlioblastoma (GBM), Medulloblastoma (MB) - Group 3 (MB_Gr3),\nGroup 4 (MB_Gr3), Group WNT (MB_WNT), Group SHH (MB_SHH) and\nPilocytic Astrocytoma (PiloAstro).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 7577,
    "package_name": "TTMap",
    "title": "Two-Tier Mapper: a clustering tool based on topological data\nanalysis",
    "description": "TTMap is a clustering method that groups together samples\nwith the same deviation in comparison to a control group. It is\nspecially useful when the data is small. It is parameter free.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 9371,
    "package_name": "biosigner",
    "title": "Signature discovery from omics data",
    "description": "Feature selection is critical in omics data analysis to\nextract restricted and meaningful molecular signatures from\ncomplex and high-dimension data, and to build robust\nclassifiers. This package implements a new method to assess the\nrelevance of the variables for the prediction performances of\nthe classifier. The approach can be run in parallel with the\nPLS-DA, Random Forest, and SVM binary classifiers. The\nsignatures and the corresponding 'restricted' models are\nreturned, enabling future predictions on new datasets. A Galaxy\nimplementation of the package is available within the\nWorkflow4metabolomics.org online infrastructure for\ncomputational metabolomics.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 10273,
    "package_name": "clayringsmiletus",
    "title": "Clay Stacking Rings Found in Miletus (Data)",
    "description": "Stacking rings are tools used to stack pottery in a Kiln.\nA relatively large group of stacking rings was found in the\narea of the sanctuary of Dionysos in Miletus in the 1970s.\nMeasurements and additional info is gathered in this package\nand made available for use by other researchers. The data along\nwith its archaeological context and analysis has been published\nin Steinmann (2020) <doi:10.34780/aa.v0i1.1014>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 10362,
    "package_name": "clubpro",
    "title": "Classification Using Binary Procrustes Rotation",
    "description": "Implements a classification method described by Grice\n(2011, ISBN:978-0-12-385194-9) using binary procrustes\nrotation; a simplified version of procrustes rotation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 10613,
    "package_name": "compSPOT",
    "title": "compSPOT: Tool for identifying and comparing significantly\nmutated genomic hotspots",
    "description": "Clonal cell groups share common mutations within cancer,\nprecancer, and even clinically normal appearing tissues. The\nfrequency and location of these mutations may predict prognosis\nand cancer risk. It has also been well established that certain\ngenomic regions have increased sensitivity to acquiring\nmutations. Mutation-sensitive genomic regions may therefore\nserve as markers for predicting cancer risk. This package\ncontains multiple functions to establish significantly mutated\nhotspots, compare hotspot mutation burden between samples, and\nperform exploratory data analysis of the correlation between\nhotspot mutation burden and personal risk factors for cancer,\nsuch as age, gender, and history of carcinogen exposure. This\npackage allows users to identify robust genomic markers to help\nestablish cancer risk.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 14281,
    "package_name": "ggtreeDendro",
    "title": "Drawing 'dendrogram' using 'ggtree'",
    "description": "Offers a set of 'autoplot' methods to visualize tree-like\nstructures (e.g., hierarchical clustering and\nclassification/regression trees) using 'ggtree'. You can adjust\ngraphical parameters using grammar of graphic syntax and\nintegrate external data to the tree.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 17208,
    "package_name": "metabinR",
    "title": "Abundance and Compositional Based Binning of Metagenomes",
    "description": "Provide functions for performing abundance and\ncompositional based binning on metagenomic samples, directly\nfrom FASTA or FASTQ files. Functions are implemented in Java\nand called via rJava. Parallel implementation that operates\ndirectly on input FASTA/FASTQ files for fast execution.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 17389,
    "package_name": "microbiomeExplorer",
    "title": "Microbiome Exploration App",
    "description": "The MicrobiomeExplorer R package is designed to facilitate\nthe analysis and visualization of marker-gene survey feature\ndata. It allows a user to perform and visualize typical\nmicrobiome analytical workflows either through the command line\nor an interactive Shiny application included with the package.\nIn addition to applying common analytical workflows the\napplication enables automated analysis report generation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 18293,
    "package_name": "neighbours",
    "title": "Neighbourhood Functions for Local-Search Algorithms",
    "description": "Neighbourhood functions are key components of local-search\nalgorithms such as Simulated Annealing or Threshold Accepting.\nThese functions take a solution and return a slightly-modified\ncopy of it, i.e. a neighbour. The package provides a function\nneighbourfun() that constructs such neighbourhood functions,\nbased on parameters such as admissible ranges for elements in a\nsolution.  Supported are numeric and logical solutions. The\nalgorithms were originally created for portfolio-optimisation\napplications, but can be used for other models as well.\nSeveral recipes for neighbour computations are taken from\n\"Numerical Methods and Optimization in Finance\" by M. Gilli, D.\nMaringer and E. Schumann (2019, ISBN:978-0128150658).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 18745,
    "package_name": "octad",
    "title": "Open Cancer TherApeutic Discovery (OCTAD)",
    "description": "OCTAD provides a platform for virtually screening\ncompounds targeting precise cancer patient groups. The\nessential idea is to identify drugs that reverse the gene\nexpression signature of disease by tamping down over-expressed\ngenes and stimulating weakly expressed ones. The package offers\ndeep-learning based reference tissue selection, disease gene\nexpression signature creation, pathway enrichment analysis,\ndrug reversal potency scoring, cancer cell line selection, drug\nenrichment analysis and in silico hit validation. It currently\ncovers ~20,000 patient tissue samples covering 50 cancer types,\nand expression profiles for ~12,000 distinct compounds.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 19737,
    "package_name": "planttfhunter",
    "title": "Identification and classification of plant transcription factors",
    "description": "planttfhunter is used to identify plant transcription\nfactors (TFs) from protein sequence data and classify them into\nfamilies and subfamilies using the classification scheme\nimplemented in PlantTFDB. TFs are identified using pre-built\nhidden Markov model profiles for DNA-binding domains. Then,\nauxiliary and forbidden domains are used with DNA-binding\ndomains to classify TFs into families and subfamilies (when\napplicable). Currently, TFs can be classified in 58 different\nTF families/subfamilies.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 23650,
    "package_name": "ssPATHS",
    "title": "ssPATHS: Single Sample PATHway Score",
    "description": "This package generates pathway scores from expression data\nfor single samples after training on a reference cohort. The\nscore is generated by taking the expression of a gene set\n(pathway) from a reference cohort and performing linear\ndiscriminant analysis to distinguish samples in the cohort that\nhave the pathway augmented and not. The separating hyperplane\nis then used to score new samples.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 4760,
    "package_name": "ONAM",
    "title": "Fitting Interpretable Neural Additive Models Using\nOrthogonalization",
    "description": "An algorithm for fitting interpretable additive neural\nnetworks for identifiable and visualizable feature effects\nusing post hoc orthogonalization. Fit custom neural networks\nintuitively using established 'R' 'formula' notation, including\ninteraction effects of arbitrary order while preserving\nidentifiability to enable a functional decomposition of the\nprediction function. For more details see Koehler et al. (2025)\n<doi:10.1038/s44387-025-00033-7>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9777,
    "stars": 0
  },
  {
    "id": 13905,
    "package_name": "gensvm",
    "title": "A Generalized Multiclass Support Vector Machine",
    "description": "The GenSVM classifier is a generalized multiclass support\nvector machine (SVM). This classifier aims to find decision\nboundaries that separate the classes with as wide a margin as\npossible. In GenSVM, the loss function is very flexible in the\nway that misclassifications are penalized.  This allows the\nuser to tune the classifier to the dataset at hand and\npotentially obtain higher classification accuracy than\nalternative multiclass SVMs.  Moreover, this flexibility means\nthat GenSVM has a number of other multiclass SVMs as special\ncases. One of the other advantages of GenSVM is that it is\ntrained in the primal space, allowing the use of warm starts\nduring optimization.  This means that for common tasks such as\ncross validation or repeated model fitting, GenSVM can be\ntrained very quickly. Based on: G.J.J. van den Burg and P.J.F.\nGroenen (2018) <https://www.jmlr.org/papers/v17/14-526.html>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.959,
    "stars": 0
  },
  {
    "id": 15738,
    "package_name": "islify",
    "title": "Automatic scoring and classification of cell-based assay images",
    "description": "This software is meant to be used for classification of\nimages of cell-based assays for neuronal surface autoantibody\ndetection or similar techniques. It takes imaging files as\ninput and creates a composite score from these, that for\nexample can be used to classify samples as negative or positive\nfor a certain antibody-specificity. The reason for its name is\nthat I during its creation have thought about the individual\npicture as an archielago where we with different filters\ncontrol the water level as well as ground characteristica,\nthereby finding islands of interest.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9542,
    "stars": 0
  },
  {
    "id": 1181,
    "package_name": "Cepo",
    "title": "Cepo for the identification of differentially stable genes",
    "description": "Defining the identity of a cell is fundamental to\nunderstand the heterogeneity of cells to various environmental\nsignals and perturbations. We present Cepo, a new method to\nexplore cell identities from single-cell RNA-sequencing data\nusing differential stability as a new metric to define cell\nidentity genes. Cepo computes cell-type specific gene\nstatistics pertaining to differential stable gene expression.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.932,
    "stars": 0
  },
  {
    "id": 8563,
    "package_name": "antiProfiles",
    "title": "Implementation of gene expression anti-profiles",
    "description": "Implements gene expression anti-profiles as described in\nCorrada Bravo et al., BMC Bioinformatics 2012, 13:272\ndoi:10.1186/1471-2105-13-272.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9031,
    "stars": 0
  },
  {
    "id": 7342,
    "package_name": "SubCellBarCode",
    "title": "SubCellBarCode: Integrated workflow for robust mapping and\nvisualizing whole human spatial proteome",
    "description": "Mass-Spectrometry based spatial proteomics have enabled\nthe proteome-wide mapping of protein subcellular localization\n(Orre et al. 2019, Molecular Cell). SubCellBarCode R package\nrobustly classifies proteins into corresponding subcellular\nlocalization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.8195,
    "stars": 0
  },
  {
    "id": 20860,
    "package_name": "randomPlantedForest",
    "title": "Random Planted Forest: A Directly Interpretable Tree Ensemble",
    "description": "An implementation of the Random Planted Forest algorithm\nfor directly interpretable tree ensembles based on a functional\nANOVA decomposition.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.8195,
    "stars": 0
  },
  {
    "id": 8103,
    "package_name": "a4Classif",
    "title": "Automated Affymetrix Array Analysis Classification Package",
    "description": "Functionalities for classification of Affymetrix\nmicroarray data, integrating within the Automated Affymetrix\nArray Analysis set of packages.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 10358,
    "package_name": "clst",
    "title": "Classification by local similarity threshold",
    "description": "Package for modified nearest-neighbor classification based\non calculation of a similarity threshold distinguishing\nwithin-group from between-group comparisons.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 15778,
    "package_name": "iterativeBMA",
    "title": "The Iterative Bayesian Model Averaging (BMA) algorithm",
    "description": "The iterative Bayesian Model Averaging (BMA) algorithm is\na variable selection and classification algorithm with an\napplication of classifying 2-class microarray samples, as\ndescribed in Yeung, Bumgarner and Raftery (Bioinformatics 2005,\n21: 2394-2402).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 21408,
    "package_name": "rfPred",
    "title": "Assign rfPred functional prediction scores to a missense\nvariants list",
    "description": "Based on external numerous data files where rfPred scores\nare pre-calculated on all genomic positions of the human exome,\nthe package gives rfPred scores to missense variants identified\nby the chromosome, the position (hg19 version), the referent\nand alternative nucleotids and the uniprot identifier of the\nprotein. Note that for using the package, the user has to\ndownload the TabixFile and index (approximately 3.3 Go).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 2353,
    "package_name": "FMradio",
    "title": "Factor Modeling for Radiomics Data",
    "description": "Functions that support stable prediction and\nclassification with radiomics data through factor-analytic\nmodeling. For details, see Peeters et al. (2019)\n<doi:10.48550/arXiv.1903.11696>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7404,
    "stars": 0
  },
  {
    "id": 3110,
    "package_name": "HaploCatcher",
    "title": "A Predictive Haplotyping Package",
    "description": "Used for predicting a genotype’s allelic state at a\nspecific locus/QTL/gene. This is accomplished by using both a\ngenotype matrix and a separate file which has categorizations\nabout loci/QTL/genes of interest for the individuals in the\ngenotypic matrix. A training population can be created from a\npanel of individuals who have been previously screened for\nspecific loci/QTL/genes, and this previous screening could be\nsummarized into a category. Using the categorization of\nindividuals which have been genotyped using a genome wide\nmarker platform, a model can be trained to predict what\ncategory (haplotype) an individual belongs in based on their\ngenetic sequence in the region associated with the\nlocus/QTL/gene. These trained models can then be used to\npredict the haplotype of a locus/QTL/gene for individuals which\nhave been genotyped with a genome wide platform yet not\ngenotyped for the specific locus/QTL/gene. This package is\nbased off work done by Winn et al 2021. For more specific\ninformation on this method, refer to\n<doi:10.1007/s00122-022-04178-w>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 4050,
    "package_name": "MLwrap",
    "title": "Machine Learning Modelling for Everyone",
    "description": "A minimal library specifically designed to make the\nestimation of Machine Learning (ML) techniques as easy and\naccessible as possible, particularly within the framework of\nthe Knowledge Discovery in Databases (KDD) process in data\nmining. The package provides essential tools to structure and\nexecute each stage of a predictive or classification modeling\nworkflow, aligning closely with the fundamental steps of the\nKDD methodology, from data selection and preparation, through\nmodel building and tuning, to the interpretation and evaluation\nof results using Sensitivity Analysis. The 'MLwrap' workflow is\norganized into four core steps; preprocessing(), build_model(),\nfine_tuning(), and sensitivity_analysis(). It also includes\nglobal and pairwise interaction analysis based on Friedman’s\nH-statistic to support a more detailed interpretation of\ncomplex feature relationships.These steps correspond,\nrespectively, to data preparation and transformation, model\nconstruction, hyperparameter optimization, and sensitivity\nanalysis. The user can access comprehensive model evaluation\nresults including fit assessment metrics, plots, predictions,\nand performance diagnostics for ML models implemented through\n'Neural Networks', 'Random Forest', 'XGBoost' (Extreme Gradient\nBoosting), and 'Support Vector Machines' (SVM) algorithms. By\nstreamlining these phases, 'MLwrap' aims to simplify the\nimplementation of ML techniques, allowing analysts and data\nscientists to focus on extracting actionable insights and\nmeaningful patterns from large datasets, in line with the\nobjectives of the KDD process.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 5071,
    "package_name": "PIUMA",
    "title": "Phenotypes Identification Using Mapper from topological data\nAnalysis",
    "description": "The PIUMA package offers a tidy pipeline of Topological\nData Analysis frameworks to identify and characterize\ncommunities in high and heterogeneous dimensional data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 9109,
    "package_name": "bcn",
    "title": "Boosted Configuration Networks",
    "description": "Boosted Configuration (neural) Networks for supervised\nlearning.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 19258,
    "package_name": "partCNV",
    "title": "Infer locally aneuploid cells using single cell RNA-seq data",
    "description": "This package uses a statistical framework for rapid and\naccurate detection of aneuploid cells with local copy number\ndeletion or amplification. Our method uses an EM algorithm with\nmixtures of Poisson distributions while incorporating\ncytogenetics information (e.g., regional deletion or\namplification) to guide the classification (partCNV). When\napplicable, we further improve the accuracy by integrating a\nHidden Markov Model for feature selection (partCNVH).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 11488,
    "package_name": "deepgmm",
    "title": "Deep Gaussian Mixture Models",
    "description": "Deep Gaussian mixture models as proposed by Viroli and\nMcLachlan (2019) <doi:10.1007/s11222-017-9793-z> provide a\ngeneralization of classical Gaussian mixtures to multiple\nlayers. Each layer contains a set of latent variables that\nfollow a mixture of Gaussian distributions. To avoid\noverparameterized solutions, dimension reduction is applied at\neach layer by way of factor models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6532,
    "stars": 0
  },
  {
    "id": 24993,
    "package_name": "triplot",
    "title": "Explaining Correlated Features in Machine Learning Models",
    "description": "Tools for exploring effects of correlated features in\npredictive models. The predict_triplot() function delivers\ninstance-level explanations that calculate the importance of\nthe groups of explanatory variables. The model_triplot()\nfunction delivers data-level explanations. The generic plot\nfunction visualises in a concise way importance of hierarchical\ngroups of predictors. All of the the tools are model agnostic,\ntherefore works for any predictive machine learning models.\nFind more details in Biecek (2018) <arXiv:1806.08915>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6532,
    "stars": 0
  },
  {
    "id": 10359,
    "package_name": "clstutils",
    "title": "Tools for performing taxonomic assignment",
    "description": "Tools for performing taxonomic assignment based on\nphylogeny using pplacer and clst.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6435,
    "stars": 0
  },
  {
    "id": 6708,
    "package_name": "SGCP",
    "title": "SGCP: A semi-supervised pipeline for gene clustering using\nself-training approach in gene co-expression networks",
    "description": "SGC is a semi-supervised pipeline for gene clustering in\ngene co-expression networks. SGC consists of multiple novel\nsteps that enable the computation of highly enriched modules in\nan unsupervised manner. But unlike all existing frameworks, it\nfurther incorporates a novel step that leverages Gene Ontology\ninformation in a semi-supervised clustering method that further\nimproves the quality of the computed modules.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6435,
    "stars": 0
  },
  {
    "id": 3444,
    "package_name": "Iscores",
    "title": "Proper Scoring Rules for Missing Value Imputation",
    "description": "Implementation of a KL-based scoring rule to assess the\nquality of different missing value imputations in the broad\nsense as introduced in Michel et al. (2021) <arXiv:2106.03742>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6232,
    "stars": 0
  },
  {
    "id": 9655,
    "package_name": "broomstick",
    "title": "Convert Decision Tree Objects into Tidy Data Frames",
    "description": "Convert Decision Tree objects into tidy data frames, by\nusing the framework laid out by the package broom, this means\nthat decision tree output can be easily reshaped, porocessed,\nand combined with tools like 'dplyr', 'tidyr' and 'ggplot2'.\nLike the package broom, broomstick provides three S3 generics:\ntidy, to summarise decision tree specific features - tidy\nreturns the variable importance table; augment adds columns to\nthe original data such as predictions and residuals; and\nglance, which provides a one-row summary of model-level\nstatistics.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6085,
    "stars": 0
  },
  {
    "id": 2247,
    "package_name": "ExhaustiveSearch",
    "title": "A Fast and Scalable Exhaustive Feature Selection Framework",
    "description": "The goal of this package is to provide an easy to use,\nfast and scalable exhaustive search framework. Exhaustive\nfeature selections typically require a very large number of\nmodels to be fitted and evaluated. Execution speed and memory\nmanagement are crucial factors here. This package provides\nsolutions for both. Execution speed is optimized by using a\nmulti-threaded C++ backend, and memory issues are solved by by\nonly storing the best results during execution and thus keeping\nmemory usage constant.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6021,
    "stars": 0
  },
  {
    "id": 4353,
    "package_name": "MiPP",
    "title": "Misclassification Penalized Posterior Classification",
    "description": "This package finds optimal sets of genes that seperate\nsamples into two or more classes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6021,
    "stars": 0
  },
  {
    "id": 16587,
    "package_name": "logicFS",
    "title": "Identification of SNP Interactions",
    "description": "Identification of interactions between binary variables\nusing Logic Regression. Can, e.g., be used to find interesting\nSNP interactions. Contains also a bagging version of logic\nregression for classification.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6021,
    "stars": 0
  },
  {
    "id": 24124,
    "package_name": "swag",
    "title": "Sparse Wrapper Algorithm",
    "description": "An algorithm that trains a meta-learning procedure that\ncombines screening and wrapper methods to find a set of\nextremely low-dimensional attribute combinations. This package\nworks on top of the 'caret' package and proceeds in a\nforward-step manner. More specifically, it builds and tests\nlearners starting from very few attributes until it includes a\nmaximal number of attributes by increasing the number of\nattributes at each step. Hence, for each fixed number of\nattributes, the algorithm tests various (randomly selected)\nlearners and picks those with the best performance in terms of\ntraining error. Throughout, the algorithm uses the information\ncoming from the best learners at the previous step to build and\ntest learners in the following step. In the end, it outputs a\nset of strong low-dimensional learners.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6021,
    "stars": 0
  },
  {
    "id": 9139,
    "package_name": "beam",
    "title": "Fast Bayesian Inference in Large Gaussian Graphical Models",
    "description": "Fast Bayesian inference of marginal and conditional\nindependence structures from high-dimensional data. Leday and\nRichardson (2019), Biometrics, <doi:10.1111/biom.13064>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.5563,
    "stars": 0
  },
  {
    "id": 25189,
    "package_name": "ucimlrepo",
    "title": "Explore UCI ML Repository Datasets",
    "description": "Find and import datasets from the University of California\nIrvine Machine Learning (UCI ML) Repository into R. Supports\nworking with data from UCI ML repository inside of R scripts,\nnotebooks, and 'Quarto'/'RMarkdown' documents. Access the UCI\nML repository directly at <https://archive.ics.uci.edu/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.5563,
    "stars": 0
  },
  {
    "id": 16547,
    "package_name": "localICE",
    "title": "Local Individual Conditional Expectation",
    "description": "Local Individual Conditional Expectation ('localICE') is a\nlocal explanation approach from the field of eXplainable\nArtificial Intelligence (XAI). localICE is a model-agnostic XAI\napproach which provides three-dimensional local explanations\nfor particular data instances. The approach is proposed in the\nmaster thesis of Martin Walter as an extension to ICE (see\nReference). The three dimensions are the two features at the\nhorizontal and vertical axes as well as the target represented\nby different colors. The approach is applicable for\nclassification and regression problems to explain interactions\nof two features towards the target. For classification models,\nthe number of classes can be more than two and each class is\nadded as a different color to the plot. The given instance is\nadded to the plot as two dotted lines according to the feature\nvalues. The localICE-package can explain features of type\nfactor and numeric of any machine learning model. Automatically\nsupported machine learning packages are 'mlr', 'randomForest',\n'caret' or all other with an S3 predict function. For further\nmodel types from other libraries, a predict function has to be\nprovided as an argument in order to get access to the model.\nReference to the ICE approach: Alex Goldstein, Adam Kapelner,\nJustin Bleich, Emil Pitkin (2013) <arXiv:1309.6392>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.5441,
    "stars": 0
  },
  {
    "id": 545,
    "package_name": "BadRegionFinder",
    "title": "BadRegionFinder: an R/Bioconductor package for identifying\nregions with bad coverage",
    "description": "BadRegionFinder is a package for identifying regions with\na bad, acceptable and good coverage in sequence alignment data\navailable as bam files. The whole genome may be considered as\nwell as a set of target regions. Various visual and textual\ntypes of output are available.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.4771,
    "stars": 0
  },
  {
    "id": 7070,
    "package_name": "SigCheck",
    "title": "Check a gene signature's prognostic performance against random\nsignatures, known signatures, and permuted data/metadata",
    "description": "While gene signatures are frequently used to predict\nphenotypes (e.g. predict prognosis of cancer patients), it it\nnot always clear how optimal or meaningful they are (cf David\nVenet, Jacques E. Dumont, and Vincent Detours' paper \"Most\nRandom Gene Expression Signatures Are Significantly Associated\nwith Breast Cancer Outcome\"). Based on suggestions in that\npaper, SigCheck accepts a data set (as an ExpressionSet) and a\ngene signature, and compares its performance on survival and/or\nclassification tasks against a) random gene signatures of the\nsame length; b) known, related and unrelated gene signatures;\nand c) permuted data and/or metadata.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.4771,
    "stars": 0
  },
  {
    "id": 9820,
    "package_name": "cancerclass",
    "title": "Development and validation of diagnostic tests from\nhigh-dimensional molecular data",
    "description": "The classification protocol starts with a feature\nselection step and continues with nearest-centroid\nclassification. The accurarcy of the predictor can be evaluated\nusing training and test set validation, leave-one-out\ncross-validation or in a multiple random validation protocol.\nMethods for calculation and visualization of continuous\nprediction scores allow to balance sensitivity and specificity\nand define a cutoff value according to clinical requirements.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.415,
    "stars": 0
  },
  {
    "id": 3582,
    "package_name": "KnowSeq",
    "title": "KnowSeq R/Bioc package: The Smart Transcriptomic Pipeline",
    "description": "KnowSeq proposes a novel methodology that comprises the\nmost relevant steps in the Transcriptomic gene expression\nanalysis. KnowSeq expects to serve as an integrative tool that\nallows to process and extract relevant biomarkers, as well as\nto assess them through a Machine Learning approaches. Finally,\nthe last objective of KnowSeq is the biological knowledge\nextraction from the biomarkers (Gene Ontology enrichment,\nPathway listing and Visualization and Evidences related to the\naddressed disease). Although the package allows analyzing all\nthe data manually, the main strenght of KnowSeq is the\npossibilty of carrying out an automatic and intelligent HTML\nreport that collect all the involved steps in one document. It\nis important to highligh that the pipeline is totally modular\nand flexible, hence it can be started from whichever of the\ndifferent steps. KnowSeq expects to serve as a novel tool to\nhelp to the experts in the field to acquire robust knowledge\nand conclusions for the data and diseases to study.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 5779,
    "package_name": "RGSEA",
    "title": "Random Gene Set Enrichment Analysis",
    "description": "Combining bootstrap aggregating and Gene set enrichment\nanalysis (GSEA), RGSEA is a classfication algorithm with high\nrobustness and no over-fitting problem. It performs well\nespecially for the data generated from different exprements.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 12510,
    "package_name": "epigenomix",
    "title": "Epigenetic and gene transcription data normalization and\nintegration with mixture models",
    "description": "A package for the integrative analysis of RNA-seq or\nmicroarray based gene transcription and histone modification\ndata obtained by ChIP-seq. The package provides methods for\ndata preprocessing and matching as well as methods for fitting\nbayesian mixture models in order to detect genes with\ndifferences in both data types.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 16755,
    "package_name": "maPredictDSC",
    "title": "Phenotype prediction using microarray data: approach of the best\noverall team in the IMPROVER Diagnostic Signature Challenge",
    "description": "This package implements the classification pipeline of the\nbest overall team (Team221) in the IMPROVER Diagnostic\nSignature Challenge. Additional functionality is added to\ncompare 27 combinations of data preprocessing, feature\nselection and classifier types.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 17178,
    "package_name": "messina",
    "title": "Single-gene classifiers and outlier-resistant detection of\ndifferential expression for two-group and survival problems",
    "description": "Messina is a collection of algorithms for constructing\noptimally robust single-gene classifiers, and for identifying\ndifferential expression in the presence of outliers or unknown\nsample subgroups.  The methods have application in identifying\nlead features to develop into clinical tests (both diagnostic\nand prognostic), and in identifying differential expression\nwhen a fraction of samples show unusual patterns of expression.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 25418,
    "package_name": "vbmp",
    "title": "Variational Bayesian Multinomial Probit Regression",
    "description": "Variational Bayesian Multinomial Probit Regression with\nGaussian Process Priors. It estimates class membership\nposterior probability employing variational and sparse\napproximation to the full posterior. This software also\nincorporates feature weighting by means of Automatic Relevance\nDetermination.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 18516,
    "package_name": "nnetsauce",
    "title": "Randomized and Quasi-Randomized networks for Statistical/Machine\nLearning",
    "description": "Randomized and Quasi-Randomized networks for\nStatistical/Machine Learning",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2,
    "stars": 0
  },
  {
    "id": 26,
    "package_name": "ACTCD",
    "title": "Asymptotic Classification Theory for Cognitive Diagnosis",
    "description": "Cluster analysis for cognitive diagnosis based on the Asymptotic Classification Theory (Chiu, Douglas & Li, 2009; <doi:10.1007/s11336-009-9125-0>). Given the sample statistic of sum-scores, cluster analysis techniques can be used to classify examinees into latent classes based on their attribute patterns. In addition to the algorithms used to classify data, three labeling approaches are proposed to label clusters so that examinees' attribute profiles can be obtained.",
    "version": "1.3-0",
    "maintainer": "Wenchao Ma <wenchao.ma@ua.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 42,
    "package_name": "ADPclust",
    "title": "Fast Clustering Using Adaptive Density Peak Detection",
    "description": "An implementation of ADPclust clustering procedures (Fast\n    Clustering Using Adaptive Density Peak Detection). The work is built and\n    improved upon the idea of Rodriguez and Laio (2014)<DOI:10.1126/science.1242072>. \n    ADPclust clusters data by finding density peaks in a density-distance plot \n    generated from local multivariate Gaussian density estimation. It includes \n    an automatic centroids selection and parameter optimization algorithm, which \n    finds the number of clusters and cluster centroids by comparing average \n    silhouettes on a grid of testing clustering results; It also includes a user \n    interactive algorithm that allows the user to manually selects cluster \n    centroids from a two dimensional \"density-distance plot\". Here is the \n    research article associated with this package: \"Wang, Xiao-Feng, and \n    Yifan Xu (2015)<DOI:10.1177/0962280215609948> Fast clustering using adaptive \n    density peak detection.\" Statistical methods in medical research\". url:\n    http://smm.sagepub.com/content/early/2015/10/15/0962280215609948.abstract. ",
    "version": "0.7",
    "maintainer": "Yifan (Ethan) Xu <ethan.yifanxu@gmail.com>",
    "url": "https://github.com/ethanyxu/ADPclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 52,
    "package_name": "AFFECT",
    "title": "Accelerated Functional Failure Time Model with\nError-Contaminated Survival Times",
    "description": "We aim to deal with data with measurement error in the response and misclassification censoring status under an AFT model. This package primarily contains three functions, which are used to generate artificial data, correction for error-prone data and estimate the functional covariates for an AFT model.",
    "version": "0.1.2",
    "maintainer": "Hsiao-Ting Huang <nikkihuang309700034@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 96,
    "package_name": "ANN2",
    "title": "Artificial Neural Networks for Anomaly Detection",
    "description": "Training of neural networks for classification and regression tasks\n    using mini-batch gradient descent. Special features include a function for \n    training autoencoders, which can be used to detect anomalies, and some \n    related plotting functions. Multiple activation functions are supported, \n    including tanh, relu, step and ramp. For the use of the step and ramp \n    activation functions in detecting anomalies using autoencoders, see \n    Hawkins et al. (2002) <doi:10.1007/3-540-46145-0_17>. Furthermore, \n    several loss functions are supported, including robust ones such as Huber \n    and pseudo-Huber loss, as well as L1 and L2 regularization. The possible \n    options for optimization algorithms are RMSprop, Adam and SGD with momentum.\n    The package contains a vectorized C++ implementation that facilitates \n    fast training through mini-batch learning.",
    "version": "2.4.0",
    "maintainer": "Bart Lammers <bart.f.lammers@gmail.com>",
    "url": "https://github.com/bflammers/ANN2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 132,
    "package_name": "ARIMAANN",
    "title": "Time Series Forecasting using ARIMA-ANN Hybrid Model",
    "description": "Testing, Implementation, and Forecasting of the ARIMA-ANN hybrid model. The ARIMA-ANN hybrid model combines the distinct strengths of the Auto-Regressive Integrated Moving Average (ARIMA) model and the Artificial Neural Network (ANN) model for time series forecasting.For method details see Zhang, GP (2003) <doi:10.1016/S0925-2312(01)00702-0>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 134,
    "package_name": "ARMALSTM",
    "title": "Fitting of Hybrid ARMA-LSTM Models",
    "description": "The real-life time series data are hardly pure linear or nonlinear. Merging a linear time series model like the autoregressive moving average (ARMA) model with a nonlinear neural network model such as the Long Short-Term Memory (LSTM) model can be used as a hybrid model for more accurate modeling purposes. Both the autoregressive integrated moving average (ARIMA) and autoregressive fractionally integrated moving average (ARFIMA) models can be implemented. Details can be found in Box et al. (2015, ISBN: 978-1-118-67502-1) and Hochreiter and Schmidhuber (1997) <doi:10.1162/neco.1997.9.8.1735>.",
    "version": "0.1.0",
    "maintainer": "Debopam Rakshit <rakshitdebopam@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 147,
    "package_name": "ASML",
    "title": "Algorithm Portfolio Selection with Machine Learning",
    "description": "A wrapper for machine learning (ML) methods to select among a portfolio of algorithms based on the value of a key performance indicator (KPI). A number of features is used to adjust a model to predict the value of the KPI for each algorithm, then, for a new value of the features the KPI is estimated and the algorithm with the best one is chosen. To learn it can use the regression methods in 'caret' package or a custom function defined by the user. Several graphics available to analyze the results obtained. This library has been used in Ghaddar et al. (2023) <doi:10.1287/ijoc.2022.0090>).",
    "version": "1.1.0",
    "maintainer": "Brais González-Rodríguez <brais.gonzalez.rodriguez@uvigo.gal>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 148,
    "package_name": "ASMap",
    "title": "Linkage Map Construction using the MSTmap Algorithm",
    "description": "Functions for Accurate and Speedy linkage map construction, manipulation and diagnosis of Doubled Haploid, Backcross and Recombinant Inbred 'R/qtl' objects. This includes extremely fast linkage map clustering and optimal marker ordering using 'MSTmap' (see Wu et al.,2008).",
    "version": "1.0-8",
    "maintainer": "Julian Taylor <julian.taylor@adelaide.edu.au>",
    "url": "https://github.com/DrJ001/ASMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 224,
    "package_name": "AiES",
    "title": "Axon Integrity Evaluation System for Microscopy Images",
    "description": "Provides tools for the quantitative analysis of axon integrity in microscopy images. \n  It implements image pre-processing, adaptive thresholding, feature extraction, and support vector machine-based classification to compute indices such as the Axon Integrity Index (AII) and Degeneration Index (DI).\n  The package is designed for reproducible and automated analysis in neuroscience research.",
    "version": "0.99.6",
    "maintainer": "Shinji Tokunaga <tokunaga@ncnp.go.jp>",
    "url": "https://github.com/BreezyCave/AiES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 251,
    "package_name": "AmpGram",
    "title": "Prediction of Antimicrobial Peptides",
    "description": "Predicts antimicrobial peptides using random forests trained on the\n    n-gram encoded peptides. The implemented algorithm can be accessed from\n    both the command line and shiny-based GUI. The AmpGram model is too large \n    for CRAN and it has to be downloaded separately from the repository:\n    <https://github.com/michbur/AmpGramModel>.",
    "version": "1.0",
    "maintainer": "Michal Burdukiewicz <michalburdukiewicz@gmail.com>",
    "url": "https://github.com/michbur/AmpGram",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 283,
    "package_name": "AntAngioCOOL",
    "title": "Anti-Angiogenic Peptide Prediction",
    "description": "Machine learning based package to predict anti-angiogenic peptides using heterogeneous sequence descriptors. 'AntAngioCOOL' exploits five descriptor types of a peptide of interest to do prediction including: pseudo amino acid composition, k-mer composition, k-mer composition (reduced alphabet), physico-chemical profile and atomic profile. According to the obtained results, 'AntAngioCOOL' reached to a satisfactory performance in anti-angiogenic peptide prediction on a benchmark non-redundant independent test dataset.",
    "version": "1.2",
    "maintainer": "Javad Zahiri <zahiri@modares.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 285,
    "package_name": "Anthropometry",
    "title": "Statistical Methods for Anthropometric Data",
    "description": "Statistical methodologies especially developed to analyze anthropometric data. These methods are aimed \t\tat providing effective solutions to some commons problems related to Ergonomics and Anthropometry. They are based on clustering, the \t\tstatistical concept of data depth, statistical shape analysis and archetypal analysis. Please see Vinue (2017) <doi:10.18637/jss.v077.i06>.",
    "version": "1.21",
    "maintainer": "Guillermo Vinue <guillermo.vinue@uv.es>",
    "url": "https://www.R-project.org, https://www.uv.es/vivigui/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 307,
    "package_name": "AriGaMyANNSVR",
    "title": "Hybrid ARIMA-GARCH and Two Specially Designed ML-Based Models",
    "description": "Describes a series first. After that does time series analysis using one hybrid model and two specially structured Machine Learning (ML) (Artificial Neural Network or ANN and Support Vector Regression or SVR) models. More information can be obtained from Paul and Garai (2022) <doi:10.1007/s41096-022-00128-3>. ",
    "version": "0.1.0",
    "maintainer": "Mr. Sandip Garai <sandipnicksandy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 330,
    "package_name": "AutoPipe",
    "title": "Automated Transcriptome Classifier Pipeline: Comprehensive\nTranscriptome Analysis",
    "description": "An unsupervised fully-automated pipeline for transcriptome analysis or a supervised option to identify characteristic genes from predefined subclasses.\n              We rely on the 'pamr' <http://www.bioconductor.org/packages//2.7/bioc/html/pamr.html> clustering algorithm to cluster the Data and then draw a heatmap of the clusters with the most significant genes and the \n              least significant genes according to the 'pamr' algorithm. This way we get easy to grasp heatmaps that show us for each cluster which are the clusters most defining genes.",
    "version": "0.1.6",
    "maintainer": "Karam Daka <k.dacca@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 331,
    "package_name": "AutoPlots",
    "title": "Creating Echarts Visualizations as Easy as Possible",
    "description": "Create beautiful and interactive visualizations in a single function call. The 'data.table' package is utilized to perform the data wrangling necessary to prepare your data for the plot types you wish to build, along with allowing fast processing for big data. There are two broad classes of plots available: standard plots and machine learning evaluation plots. There are lots of parameters available in each plot type function for customizing the plots (such as faceting) and data wrangling (such as variable transformations and aggregation).",
    "version": "1.0.0",
    "maintainer": "Adrian Antico <adrianantico@gmail.com>",
    "url": "https://github.com/AdrianAntico/AutoPlots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 332,
    "package_name": "AutoScore",
    "title": "An Interpretable Machine Learning-Based Automatic Clinical Score\nGenerator",
    "description": "A novel interpretable machine learning-based framework to automate the development of a clinical scoring model for predefined outcomes. Our novel framework consists of six modules: variable ranking with machine learning, variable transformation, score derivation, model selection, domain knowledge-based score fine-tuning, and performance evaluation.The details are described in our research paper<doi:10.2196/21798>. Users or clinicians could seamlessly generate parsimonious sparse-score risk models (i.e., risk scores), which can be easily implemented and validated in clinical practice. We hope to see its application in various medical case studies.",
    "version": "1.1.0",
    "maintainer": "Feng Xie <xief@u.duke.nus.edu>",
    "url": "https://github.com/nliulab/AutoScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 389,
    "package_name": "BCClong",
    "title": "Bayesian Consensus Clustering for Multiple Longitudinal Features",
    "description": "It is very common nowadays for a study to collect multiple\n    features and appropriately integrating multiple longitudinal features\n    simultaneously for defining individual clusters becomes increasingly\n    crucial to understanding population heterogeneity and predicting\n    future outcomes.  'BCClong' implements a Bayesian consensus clustering\n    (BCC) model for multiple longitudinal features via a generalized\n    linear mixed model. Compared to existing packages, several key\n    features make the 'BCClong' package appealing: (a) it allows\n    simultaneous clustering of mixed-type (e.g., continuous, discrete and\n    categorical) longitudinal features, (b) it allows each longitudinal\n    feature to be collected from different sources with measurements taken\n    at distinct sets of time points (known as irregularly sampled\n    longitudinal data), (c) it relaxes the assumption that all features\n    have the same clustering structure by estimating the feature-specific\n    (local) clusterings and consensus (global) clustering.",
    "version": "1.0.3",
    "maintainer": "Zhiwen Tan <21zt9@queensu.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 480,
    "package_name": "BNPmix",
    "title": "Bayesian Nonparametric Mixture Models",
    "description": "Functions to perform Bayesian nonparametric univariate and multivariate density estimation and clustering, by means of Pitman-Yor mixtures, and dependent Dirichlet process mixtures for partially exchangeable data. See Corradin et al. (2021) <doi:10.18637/jss.v100.i15> for more details.  ",
    "version": "1.1.0",
    "maintainer": "Riccardo Corradin <riccardo.corradin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 574,
    "package_name": "BayesCPclust",
    "title": "A Bayesian Approach for Clustering Constant-Wise Change-Point\nData",
    "description": "A Gibbs sampler algorithm was developed to estimate change points in constant-wise data sequences while performing clustering simultaneously. The algorithm is described in da Cruz, A. C. and de Souza, C. P. E \"A Bayesian Approach for Clustering Constant-wise Change-point Data\" <doi:10.48550/arXiv.2305.17631>.",
    "version": "0.1.0",
    "maintainer": "Ana Carolina da Cruz <adacruz@uwo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 577,
    "package_name": "BayesCVI",
    "title": "Bayesian Cluster Validity Index",
    "description": "Algorithms for computing and generating plots with and without error bars for Bayesian cluster validity index (BCVI) (O. Preedasawakul, and N. Wiroonsri, A Bayesian Cluster Validity Index, Computational Statistics & Data Analysis, 202, 108053, 2025. <doi:10.1016/j.csda.2024.108053>) based on several underlying cluster validity indexes (CVIs) including Calinski-Harabasz, Chou-Su-Lai, Davies-Bouldin, Dunn,  Pakhira-Bandyopadhyay-Maulik, Point biserial correlation, the score function, Starczewski, and Wiroonsri indices for hard clustering, and Correlation Cluster Validity, the generalized C, HF, KWON, KWON2, Modified Pakhira-Bandyopadhyay-Maulik, Pakhira-Bandyopadhyay-Maulik, Tang, Wiroonsri-Preedasawakul, Wu-Li, and Xie-Beni indices for soft clustering. The package is compatible with K-means, fuzzy C means, EM clustering, and hierarchical clustering (single, average, and complete linkage). Though BCVI is compatible with any underlying existing CVIs, we recommend users to use either WI or WP as the underlying CVI.",
    "version": "1.0.2",
    "maintainer": "Onthada Preedasawakul <o.preedasawakul@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 590,
    "package_name": "BayesFluxR",
    "title": "Implementation of Bayesian Neural Networks",
    "description": "Implementation of 'BayesFlux.jl' for R; It extends the famous \n             'Flux.jl' machine learning library to Bayesian Neural Networks. \n             The goal is not to have the fastest production ready \n             library, but rather to allow more people to be able \n             to use and research on Bayesian Neural Networks. ",
    "version": "0.1.3",
    "maintainer": "Enrico Wegner <e.wegner@student.maastrichtuniversity.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 602,
    "package_name": "BayesMFSurv",
    "title": "Bayesian Misclassified-Failure Survival Model",
    "description": "Contains a split population survival estimator that models the \n    misclassification probability of failure versus right-censored events.  \n    The split population survival estimator is described in \n    Bagozzi et al. (2019) <doi:10.1017/pan.2019.6>.",
    "version": "0.1.0",
    "maintainer": "Nicolas Schmidt <nschmidt@cienciassociales.edu.uy>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 681,
    "package_name": "BiObjClass",
    "title": "Classification of Algorithms",
    "description": "Implements the Bi-objective Lexicographical Classification method and Performance Assessment Ratio at 10% metric for algorithm classification. Constructs matrices representing algorithm performance under multiple criteria, facilitating decision-making in algorithm selection and evaluation. Analyzes and compares algorithm performance based on various metrics to identify the most suitable algorithms for specific tasks. This package includes methods for algorithm classification and evaluation, with examples provided in the documentation. Carvalho (2019) presents a statistical evaluation of algorithmic computational experimentation with infeasible solutions <doi:10.48550/arXiv.1902.00101>. Moreira and Carvalho (2023) analyze power in preprocessing methodologies for datasets with missing values <doi:10.1080/03610918.2023.2234683>.",
    "version": "0.1.0",
    "maintainer": "Tiago Costa Soares <tiagocsoares22@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 695,
    "package_name": "BigQuic",
    "title": "Big Quadratic Inverse Covariance Estimation",
    "description": "Use Newton's method, coordinate descent, and METIS clustering\n    to solve the L1 regularized Gaussian MLE inverse covariance\n    matrix estimation problem.",
    "version": "1.1-13",
    "maintainer": "Khalid B. Kunji <kkunji@hbku.edu.qa>",
    "url": "https://www.r-project.org,\nhttps://bigdata.oden.utexas.edu/software/1035/\nhttp://glaros.dtc.umn.edu/gkhome/views/metis\nhttps://www.pcg-random.org/download.html\nhttps://gcc.gnu.org/projects/gomp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 699,
    "package_name": "BinMat",
    "title": "Processes Binary Data Obtained from Fragment Analysis (Such as\nAFLPs, ISSRs, and RFLPs)",
    "description": "A molecular genetics tool that processes binary data from fragment analysis. It consolidates replicate sample pairs, outputs summary statistics, and produces hierarchical clustering trees and nMDS plots. This package was developed from the publication available here: <doi:10.1016/j.biocontrol.2020.104426>. The GUI version of this package is available on the R Shiny online server at: <https://clarkevansteenderen.shinyapps.io/BINMAT/> or it is accessible via GitHub by typing: shiny::runGitHub(\"BinMat\", \"clarkevansteenderen\") into the console in R. Two real-world datasets accompany the package: an AFLP dataset of Bunias orientalis samples from Tewes et. al. (2017) <doi:10.1111/1365-2745.12869>, and an ISSR dataset of Nymphaea specimens from Reid et. al. (2021) <doi:10.1016/j.aquabot.2021.103372>. The authors of these publications are thanked for allowing the use of their data.",
    "version": "0.1.6",
    "maintainer": "Clarke van Steenderen <vsteenderen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 760,
    "package_name": "Bioi",
    "title": "Biological Image Analysis",
    "description": "Single linkage clustering and connected component analyses are often performed on biological images. 'Bioi' provides a set of functions for performing these tasks. This functionality is implemented in several key functions that can extend to from 1 to many dimensions. The single linkage clustering method implemented here can be used on n-dimensional data sets, while connected component analyses are limited to 3 or fewer dimensions.",
    "version": "0.2.10",
    "maintainer": "Zachary Colburn <zcolburn@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 804,
    "package_name": "Boruta",
    "title": "Wrapper Algorithm for All Relevant Feature Selection",
    "description": "An all relevant feature selection wrapper algorithm.\n It finds relevant features by comparing original attributes' importance with importance achievable at random, estimated using their permuted copies (shadows).",
    "version": "9.0.0",
    "maintainer": "Miron Bartosz Kursa <M.Kursa@icm.edu.pl>",
    "url": "https://gitlab.com/mbq/Boruta/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 839,
    "package_name": "C443",
    "title": "See a Forest for the Trees",
    "description": "Get insight into a forest of classification trees, by calculating similarities between the trees, and subsequently clustering them. Each cluster is represented by it's most central cluster member. The package implements the methodology described in Sies & Van Mechelen (2020) <doi:10.1007/s00357-019-09350-4>.",
    "version": "3.4.0",
    "maintainer": "Aniek Sies <aniek.sies@kuleuven.be>",
    "url": "https://github.com/KULeuven-PPW-OKPIV/C443",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 840,
    "package_name": "C50",
    "title": "C5.0 Decision Trees and Rule-Based Models",
    "description": "C5.0 decision trees and rule-based models for pattern\n    recognition that extend the work of Quinlan (1993,\n    ISBN:1-55860-238-0).",
    "version": "0.2.0",
    "maintainer": "Max Kuhn <mxkuhn@gmail.com>",
    "url": "https://topepo.github.io/C5.0/, https://github.com/topepo/C5.0/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 854,
    "package_name": "CALIBERrfimpute",
    "title": "Multiple Imputation Using MICE and Random Forest",
    "description": "Functions to impute using random forest under full conditional specifications (multivariate imputation by chained equations). The methods are described in Shah and others (2014) <doi:10.1093/aje/kwt312>.",
    "version": "1.0-7",
    "maintainer": "Anoop Shah <anoop@doctors.org.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 867,
    "package_name": "CASCORE",
    "title": "Covariate Assisted Spectral Clustering on Ratios of Eigenvectors",
    "description": "Functions for implementing the novel algorithm CASCORE, which is designed to detect latent community structure in graphs with node covariates. This algorithm can handle models such as the covariate-assisted degree corrected stochastic block model (CADCSBM). CASCORE specifically addresses the disagreement between the community structure inferred from the adjacency information and the community structure inferred from the covariate information. For more detailed information, please refer to the reference paper: Yaofang Hu and Wanjie Wang (2022) <arXiv:2306.15616>. \n    In addition to CASCORE, this package includes several classical community detection algorithms that are compared to CASCORE in our paper. These algorithms are: Spectral Clustering On Ratios-of Eigenvectors (SCORE), normalized PCA, ordinary PCA, network-based clustering, covariates-based clustering and covariate-assisted spectral clustering (CASC). By providing these additional algorithms, the package enables users to compare their performance with CASCORE in community detection tasks.",
    "version": "0.1.2",
    "maintainer": "Yaofang Hu <yaofangh@smu.edu>",
    "url": "https://arxiv.org/abs/2306.15616",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 894,
    "package_name": "CCM",
    "title": "Correlation Classification Method",
    "description": "Classification method described in Dancik et al (2011) <doi:10.1158/0008-5472.CAN-11-2427> that classifies a sample according to the class with the maximum mean (or any other function of) correlation between the test and training samples with known classes.",
    "version": "1.2",
    "maintainer": "Garrett M. Dancik <dancikg@easternct.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 895,
    "package_name": "CCMMR",
    "title": "Minimization of the Convex Clustering Loss Function",
    "description": "Implements the convex clustering through majorization-minimization (CCMM) algorithm described in Touw, Groenen, and Terada (2022) <doi:10.48550/arXiv.2211.01877> to perform minimization of the convex clustering loss function.",
    "version": "0.2.1",
    "maintainer": "Daniel Touw <touw@ese.eur.nl>",
    "url": "https://github.com/djwtouw/CCMMR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 903,
    "package_name": "CDF",
    "title": "Centroid Decision Forest for High-Dimensional Classification",
    "description": "Implements the Centroid Decision Forest (CDF) as a single user-facing\n  function CDF(). The method selects discriminative features via a multi-class\n  class separability score (CSS), splits by nearest class centroid, and aggregates\n  tree votes to produce predictions and class probabilities. Returns CSS-based\n  feature importance as well. Amjad Ali, Saeed Aldahmani, Zardad Khan (2025) <doi:10.48550/arXiv.2503.19306>.",
    "version": "0.1.0",
    "maintainer": "Amjad Ali <amjadali@uaeu.ac.ae>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 916,
    "package_name": "CEC",
    "title": "Cross-Entropy Clustering",
    "description": "Splits data into Gaussian type clusters using the Cross-Entropy \n    Clustering ('CEC') method. This method allows for the simultaneous use of \n    various types of Gaussian mixture models, for performing the reduction of \n    unnecessary clusters, and for discovering new clusters by splitting them. \n    'CEC' is based on the work of Spurek, P. and Tabor, J. (2014) \n    <doi:10.1016/j.patcog.2014.03.006>.",
    "version": "0.11.2",
    "maintainer": "Simon Garnier <garnier@njit.edu>",
    "url": "https://github.com/swarm-lab/cec, https://swarm-lab.github.io/cec/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 918,
    "package_name": "CEEMDANML",
    "title": "CEEMDAN Decomposition Based Hybrid Machine Learning Models",
    "description": "Noise in the time-series data significantly affects the accuracy of the Machine Learning (ML) models (Artificial Neural Network and Support Vector Regression are considered here). Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) decomposes the time series data into sub-series and help to improve the model performance. The models can achieve higher prediction accuracy than the traditional ML models. Two models have been provided here for time series forecasting. More information may be obtained from Garai and Paul (2023) <doi:10.1016/j.iswa.2023.200202>.",
    "version": "0.1.0",
    "maintainer": "Mr. Sandip Garai <sandipnicksandy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 921,
    "package_name": "CERFIT",
    "title": "Causal Effect Random Forest of Interaction Trees",
    "description": "Fits a Causal Effect Random Forest of Interaction Tress (CERFIT) which is a modification of the Random Forest algorithm where each split is chosen to maximize subgroup treatment heterogeneity. Doing this allows it to estimate the individualized treatment effect for each observation in either randomized controlled trial (RCT) or observational data. For more information see L. Li, R. A. Levine, and J. Fan (2022) <doi:10.1002/sta4.457>.",
    "version": "0.1.1",
    "maintainer": "Justin Thorp <jjtthorp@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 952,
    "package_name": "CICA",
    "title": "Clusterwise Independent Component Analysis",
    "description": "Clustering multi-subject resting state functional Magnetic Resonance Imaging data. This methods enables the clustering of subjects based on multi-subject resting state functional Magnetic Resonance Imaging data. Objects are clustered based on similarities and differences in cluster-specific estimated components obtained by Independent Component Analysis.",
    "version": "1.1.1",
    "maintainer": "Jeffrey Durieux <durieux.jeffrey@gmail.com>",
    "url": "https://www.sciencedirect.com/science/article/pii/S0165027022002448",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1026,
    "package_name": "COLP",
    "title": "Causal Discovery for Categorical Data with Label Permutation",
    "description": "Discover causality for bivariate categorical data. This package aims to enable users to discover causality for bivariate observational categorical data. See Ni, Y. (2022) <arXiv:2209.08579> \"Bivariate Causal Discovery for Categorical Data via Classification with Optimal Label Permutation. Advances in Neural Information Processing Systems 35 (in press)\".",
    "version": "1.0.0",
    "maintainer": "Yang Ni <yni@stat.tamu.edu>",
    "url": "https://github.com/nySTAT/COLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1028,
    "package_name": "COMBO",
    "title": "Correcting Misclassified Binary Outcomes in Association Studies",
    "description": "Use frequentist and Bayesian methods to estimate parameters from a\n  binary outcome misclassification model. These methods correct for the problem\n  of \"label switching\" by assuming that the sum of outcome sensitivity and \n  specificity is at least 1. A description of the analysis methods is\n  available in Hochstedler and Wells (2023) <doi:10.48550/arXiv.2303.10215>. ",
    "version": "1.2.0",
    "maintainer": "Kimberly Hochstedler Webb <kah343@cornell.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1038,
    "package_name": "CORElearn",
    "title": "Classification, Regression and Feature Evaluation",
    "description": "A suite of machine learning algorithms written in C++ with the R \n interface contains several learning techniques for classification and regression.\n Predictive models include e.g., classification and regression trees with\n optional constructive induction and models in the leaves, random forests, kNN, \n naive Bayes, and locally weighted regression. All predictions obtained with these\n models can be explained and visualized with the 'ExplainPrediction' package.  \n This package is especially strong in feature evaluation where it contains several variants of\n Relief algorithm and many impurity based attribute evaluation functions, e.g., Gini, \n information gain, MDL, and DKM. These methods can be used for feature selection \n or discretization of numeric attributes.\n The OrdEval algorithm and its visualization is used for evaluation\n of data sets with ordinal features and class, enabling analysis according to the \n Kano model of customer satisfaction. \n Several algorithms support parallel multithreaded execution via OpenMP.  \n The top-level documentation is reachable through ?CORElearn.",
    "version": "1.57.3.1",
    "maintainer": "Marko Robnik-Sikonja <marko.robnik@fri.uni-lj.si>",
    "url": "http://lkm.fri.uni-lj.si/rmarko/software/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1046,
    "package_name": "COveR",
    "title": "Clustering with Overlaps",
    "description": "Provide functions for overlaps clustering, fuzzy clustering and interval-valued data manipulation. The package implement the following algorithms:\n\tOKM (Overlapping Kmeans) from Cleuziou, G. (2007) <doi:10.1109/icpr.2008.4761079> ;\n\tNEOKM (Non-exhaustive overlapping Kmeans) from Whang, J. J., Dhillon, I. S., and Gleich, D. F. (2015) <doi:10.1137/1.9781611974010.105> ;\n\tFuzzy Cmeans from Bezdek, J. C. (1981) <doi:10.1007/978-1-4757-0450-1> ;\n\tFuzzy I-Cmeans from de A.T. De Carvalho, F. (2005) <doi:10.1016/j.patrec.2006.08.014>.",
    "version": "1.1.0",
    "maintainer": "Nicolas Hiot <nicolas.hiot@univ-orleans.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1056,
    "package_name": "CPoptim",
    "title": "Convex Partition Optimisation",
    "description": "Convex Partition is a black-box optimisation algorithm for single\n  objective real-parameters functions. The basic principle is to progressively\n  estimate and exploit a regression tree similar to a CART (Classification and\n  Regression Tree) of the objective function. For more details see \n  'de Paz' (2024) <doi:10.1007/978-3-031-62836-8_3> and\n  'Loh' (2011) <doi:10.1002/widm.8> .",
    "version": "0.1.0",
    "maintainer": "Erick G.G. de Paz <giles.erick@colpos.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1070,
    "package_name": "CRTConjoint",
    "title": "Conditional Randomization Testing (CRT) Approach for Conjoint\nAnalysis",
    "description": "Computes p-value according to the CRT using the HierNet test statistic. For more details, see Ham, Imai, Janson (2022) \"Using Machine Learning to Test Causal Hypotheses in Conjoint Analysis\" <arXiv:2201.08343>.",
    "version": "0.1.0",
    "maintainer": "Dae Woong Ham <daewoongham@g.harvard.edu>",
    "url": "https://github.com/daewoongham97/CRTConjoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1095,
    "package_name": "CTShiny",
    "title": "Interactive Document for Working with Classification Tree\nAnalysis",
    "description": "An interactive document on  the topic of classification tree analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/CTShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1096,
    "package_name": "CTShiny2",
    "title": "Interactive Document for Working with Classification Tree\nAnalysis",
    "description": "An interactive document on  the topic of classification tree analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/CTShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1116,
    "package_name": "CaDENCE",
    "title": "Conditional Density Estimation Network Construction and\nEvaluation",
    "description": "Parameters of a user-specified probability distribution are modelled by a multi-layer perceptron artificial neural network. This framework can be used to implement probabilistic nonlinear models including mixture density networks, heteroscedastic regression models, zero-inflated models, etc. following Cannon (2012) <doi:10.1016/j.cageo.2011.08.023>.",
    "version": "1.2.5",
    "maintainer": "Alex J. Cannon <alex.cannon@canada.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1124,
    "package_name": "Calculator.LR.FNs",
    "title": "Calculator for LR Fuzzy Numbers",
    "description": "Arithmetic operations scalar multiplication, addition, subtraction, multiplication and division of LR fuzzy numbers (which are on the basis of extension principle) have a complicate form for using in fuzzy Statistics, fuzzy Mathematics, machine learning, fuzzy data analysis and etc. Calculator for LR Fuzzy Numbers package relieve and aid applied users to achieve a simple and closed form for some complicated operator based on LR fuzzy numbers and also the user can easily draw the membership function of the obtained result by this package. ",
    "version": "1.3",
    "maintainer": "Abbas Parchami <parchami@uk.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1183,
    "package_name": "Certara.DarwinReporter",
    "title": "Data Visualization Utilities for 'pyDarwin' Machine Learning\nPharmacometric Model Development",
    "description": "Utilize the 'shiny' interface for visualizing results from a 'pyDarwin' (<https://certara.github.io/pyDarwin/>)\n    machine learning pharmacometric model search. It generates Goodness-of-Fit plots and summary tables for selected models,\n    allowing users to customize diagnostic outputs within the interface. The underlying R code for generating plots and\n    tables can be extracted for use outside the interactive session. Model diagnostics can also be incorporated into an\n    R Markdown document and rendered in various output formats.",
    "version": "2.0.1",
    "maintainer": "James Craig <james.craig@certara.com>",
    "url": "https://certara.github.io/R-DarwinReporter/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1187,
    "package_name": "Certara.RDarwin",
    "title": "Interface for 'pyDarwin' Machine Learning Pharmacometric Model\nDevelopment",
    "description": "Utilities that support the usage of 'pyDarwin' (<https://certara.github.io/pyDarwin/>) for ease of setup \n    and execution of a machine learning based pharmacometric model search with Certara's Non-Linear Mixed Effects (NLME) \n    modeling engine.",
    "version": "1.2.0",
    "maintainer": "James Craig <james.craig@certara.com>",
    "url": "https://certara.github.io/R-Darwin/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1216,
    "package_name": "ChemoSpec",
    "title": "Exploratory Chemometrics for Spectroscopy",
    "description": "A collection of functions for top-down exploratory data analysis\n    of spectral data including nuclear magnetic resonance (NMR), infrared (IR),\n    Raman, X-ray fluorescence (XRF) and other similar types of spectroscopy.\n    Includes functions for plotting and inspecting spectra, peak alignment,\n    hierarchical cluster analysis (HCA), principal components analysis (PCA) and\n    model-based clustering. Robust methods appropriate for this type of\n    high-dimensional data are available. ChemoSpec is designed for structured\n    experiments, such as metabolomics investigations, where the samples fall into\n    treatment and control groups. Graphical output is formatted consistently for\n    publication quality plots. ChemoSpec is intended to be very user friendly and\n    to help you get usable results quickly. A vignette covering typical operations\n    is available.",
    "version": "6.3.1",
    "maintainer": "Bryan A. Hanson <hanson@depauw.edu>",
    "url": "https://bryanhanson.github.io/ChemoSpec/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1217,
    "package_name": "ChemoSpec2D",
    "title": "Exploratory Chemometrics for 2D Spectroscopy",
    "description": "A collection of functions for exploratory chemometrics of 2D spectroscopic data sets such as COSY (correlated spectroscopy) and HSQC (heteronuclear single quantum coherence) 2D NMR (nuclear magnetic resonance) spectra. 'ChemoSpec2D' deploys methods aimed primarily at classification of samples and the identification of spectral features which are important in distinguishing samples from each other. Each 2D spectrum (a matrix) is treated as the unit of observation, and thus the physical sample in the spectrometer corresponds to the  sample from a statistical perspective.  In addition to chemometric tools, a few tools are provided for plotting 2D spectra, but these are not intended to replace the functionality typically available on the spectrometer. 'ChemoSpec2D' takes many of its cues from 'ChemoSpec' and tries to create consistent graphical output and to be very user friendly.",
    "version": "0.5.1",
    "maintainer": "Bryan A. Hanson <hanson@depauw.edu>",
    "url": "https://github.com/bryanhanson/ChemoSpec2D",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1247,
    "package_name": "CircularSilhouette",
    "title": "Fast Silhouette on Circular or Linear Data Clusters",
    "description": "Calculating silhouette information for clusters on\n circular or linear data using fast algorithms. These algorithms run in\n linear time on sorted data, in contrast to quadratic time by the\n definition of silhouette. When used together with the fast and optimal\n circular clustering method FOCC (Debnath & Song 2021)\n <doi:10.1109/TCBB.2021.3077573> implemented in R package 'OptCirClust',\n circular silhouette can be maximized to find the optimal number of\n circular clusters; it can also be used to estimate the period of noisy\n periodical data.",
    "version": "0.0.1",
    "maintainer": "Joe Song <joemsong@cs.nmsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1256,
    "package_name": "ClassificationEnsembles",
    "title": "Automatically Builds 12 Classification Models",
    "description": "Automatically builds 12 classification models from data. The package returns 26 plots, 5 tables and a summary report.\n    The package automatically builds six individual classification models, including error (RMSE) and predictions. That data is used to create an ensemble, which is then modeled using six methods.\n    The process is repeated as many times as the user requests. The mean of the results are presented in a summary table. \n    The package returns the confusion matrices for all 12 models, tables of the correlation of the numeric data, the results of the variance inflation process, the head of the ensemble and the head of the data frame.",
    "version": "0.7.1",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "url": "https://github.com/InfiniteCuriosity/ClassificationEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1262,
    "package_name": "ClickClust",
    "title": "Model-Based Clustering of Categorical Sequences",
    "description": "Clustering categorical sequences by means of finite mixtures with Markov model components is the main utility of ClickClust. The package also allows detecting blocks of equivalent states by forward and backward state selection procedures.",
    "version": "1.1.6",
    "maintainer": "Volodymyr Melnykov <vmelnykov@ua.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1266,
    "package_name": "ClimClass",
    "title": "Climate Classification According to Several Indices",
    "description": "Classification of climate according to Koeppen - Geiger, of aridity\n    indices, of continentality indices, of water balance after Thornthwaite, of\n    viticultural bioclimatic indices. Drawing climographs: Thornthwaite, Peguy,\n    Bagnouls-Gaussen.",
    "version": "2.1.1",
    "maintainer": "Fabio Zottele <fabio.zottele@fmach.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1278,
    "package_name": "CluMP",
    "title": "Clustering of Micro Panel Data",
    "description": "Two-step feature-based clustering method designed for micro panel (longitudinal) data with the artificial panel data generator. See Sobisek, Stachova, Fojtik (2018) <arXiv:1807.05926>.",
    "version": "0.8.1",
    "maintainer": "Jan Fojtik <9afojtik@gmail.com>",
    "url": "https://arxiv.org/ftp/arxiv/papers/1807/1807.05926.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1281,
    "package_name": "ClusBoot",
    "title": "Bootstrap a Clustering Solution to Establish the Stability of\nthe Clusters",
    "description": "Providing a cluster allocation for n samples, either with an $n \\times p$ data matrix or an $n \\times n$ distance\n                matrix, a bootstrap procedure is performed. The proportion of bootstrap replicates where a pair of samples\n                cluster in the same cluster indicates who tightly the samples in a particular cluster clusters together.",
    "version": "1.2.2",
    "maintainer": "Sugnet Lubbe <slubbe@sun.ac.za>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1282,
    "package_name": "ClusPred",
    "title": "Simultaneous Semi-Parametric Estimation of Clustering and\nRegression",
    "description": "Parameter estimation of regression models with fixed group effects, when the group variable is missing while group-related variables are available. Parametric and semi-parametric approaches described in Marbac et al. (2020) <arXiv:2012.14159> are implemented.",
    "version": "1.1.0",
    "maintainer": "Matthieu Marbac <matthieu.marbac-lourdelle@ensai.fr>",
    "url": "https://arxiv.org/abs/2012.14159",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1283,
    "package_name": "ClusROC",
    "title": "ROC Analysis in Three-Class Classification Problems for\nClustered Data",
    "description": "Statistical methods for ROC surface analysis in three-class classification problems for clustered data and in presence of covariates. In particular, the package allows to obtain covariate-specific point and interval estimation for:\n  (i) true class fractions (TCFs) at fixed pairs of thresholds;\n  (ii) the ROC surface;\n  (iii) the volume under ROC surface (VUS);\n  (iv) the optimal pairs of thresholds.\n  Methods considered in points (i), (ii) and (iv) are proposed and discussed in To et al. (2022) <doi:10.1177/09622802221089029>. Referring to point (iv), three  different selection criteria are implemented: Generalized Youden Index (GYI), Closest to Perfection (CtP) and Maximum Volume (MV). Methods considered in point (iii) are proposed and discussed in Xiong et al. (2018) <doi:10.1177/0962280217742539>. Visualization tools are also provided. We refer readers to the articles cited above for all details. ",
    "version": "1.0.3",
    "maintainer": "Duc-Khanh To <tdkhanh@hcmus.edu.vn>",
    "url": "https://github.com/toduckhanh/ClusROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1285,
    "package_name": "ClusTorus",
    "title": "Prediction and Clustering on the Torus by Conformal Prediction",
    "description": "Provides various tools of for clustering multivariate angular \n  data on the torus. The package provides angular \n  adaptations of usual clustering methods such as the k-means \n  clustering, pairwise angular distances, which can be used as an \n  input for distance-based clustering algorithms, and implements\n  clustering based on the conformal prediction framework. Options \n  for the conformal scores include scores based on a kernel density \n  estimate, multivariate von Mises mixtures, and naive k-means clusters. \n  Moreover, the package provides some basic data handling tools for \n  angular data.",
    "version": "0.2.2",
    "maintainer": "Seungki Hong <skgaboja@snu.ac.kr>",
    "url": "https://github.com/sungkyujung/ClusTorus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1286,
    "package_name": "ClussCluster",
    "title": "Simultaneous Detection of Clusters and Cluster-Specific Genes in\nHigh-Throughput Transcriptome Data",
    "description": "Implements a new method 'ClussCluster' descried in Ge Jiang and Jun Li, \"Simultaneous Detection of Clusters and Cluster-Specific Genes in High-throughput Transcriptome Data\" (Unpublished).\n\tSimultaneously perform clustering analysis and signature \tgene selection on high-dimensional transcriptome data sets. \tTo do so, 'ClussCluster' incorporates a Lasso-type \tregularization penalty term to the objective function of K-\tmeans so that cell-type-specific signature genes can be \tidentified while clustering the cells.",
    "version": "0.1.0",
    "maintainer": "Li Jun <jun.li@nd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1289,
    "package_name": "ClustBlock",
    "title": "Clustering of Datasets",
    "description": "Hierarchical and partitioning algorithms to cluster blocks of variables. The partitioning algorithm includes an option called noise cluster to set aside atypical blocks of variables. Different thresholds per cluster can be sets. The CLUSTATIS method (for quantitative blocks) (Llobell, Cariou, Vigneau, Labenne & Qannari (2020) <doi:10.1016/j.foodqual.2018.05.013>, Llobell, Vigneau & Qannari (2019) <doi:10.1016/j.foodqual.2019.02.017>)  and the CLUSCATA method (for Check-All-That-Apply data) (Llobell, Cariou, Vigneau, Labenne & Qannari (2019) <doi:10.1016/j.foodqual.2018.09.006>, Llobell, Giacalone, Labenne & Qannari (2019) <doi:10.1016/j.foodqual.2019.05.017>) are the core of this package. The CATATIS methods allows to compute some indices and tests to control the quality of CATA data. Multivariate analysis and clustering of subjects for quantitative multiblock data, CATA, RATA, Free Sorting and JAR experiments are available. Clustering of rows in multi-block context (notably with ClusMB strategy) is also included.",
    "version": "4.1.1",
    "maintainer": "Fabien Llobell <fabienllobellresearch@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1292,
    "package_name": "ClustImpute",
    "title": "K-Means Clustering with Build-in Missing Data Imputation",
    "description": "This k-means algorithm is able to cluster data with missing values and as a by-product completes the data set. The implementation can deal with missing values in multiple variables and is computationally efficient since it iteratively uses the current cluster assignment to define a plausible distribution for missing value imputation. Weights are used to shrink early random draws for missing values (i.e., draws based on the cluster assignments after few iterations) towards the global mean of each feature. This shrinkage slowly fades out after a fixed number of iterations to reflect the increasing credibility of cluster assignments. See the vignette for details.",
    "version": "0.2.4",
    "maintainer": "Oliver Pfaffel <opfaffel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1294,
    "package_name": "ClustOfVar",
    "title": "Clustering of Variables",
    "description": "Cluster analysis of a set of variables. Variables can be quantitative, qualitative or a mixture of both.",
    "version": "1.2",
    "maintainer": "Marie Chavent <Marie.Chavent@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1304,
    "package_name": "ClusterStability",
    "title": "Assessment of Stability of Individual Objects or Clusters in\nPartitioning Solutions",
    "description": "Allows one to assess the stability of individual objects, clusters \n    and whole clustering solutions based on repeated runs of the K-means and K-medoids \n    partitioning algorithms.",
    "version": "1.0.4",
    "maintainer": "Etienne Lord <m.etienne.lord@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1305,
    "package_name": "ClusterVAR",
    "title": "Fitting Latent Class Vector-Autoregressive (VAR) Models",
    "description": "Estimates latent class vector-autoregressive models via EM algorithm on time-series data for model-based clustering and classification. Includes model selection criteria for selecting the number of lags and clusters.",
    "version": "0.0.8",
    "maintainer": "Anja Ernst <a.f.ernst@rug.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1307,
    "package_name": "Clustering",
    "title": "Techniques for Evaluating Clustering",
    "description": "The design of this package allows us to run different clustering packages and compare the results between them, to determine which algorithm behaves best from the data provided. See Martos, L.A.P., García-Vico, Á.M., González, P. et al.(2023) <doi:10.1007/s13748-022-00294-2> \"Clustering: an R library to facilitate the analysis and comparison of cluster algorithms.\", Martos, L.A.P., García-Vico, Á.M., González, P. et al. \"A Multiclustering Evolutionary Hyperrectangle-Based Algorithm\" <doi:10.1007/s44196-023-00341-3> and L.A.P., García-Vico, Á.M., González, P. et al. \"An Evolutionary Fuzzy System for Multiclustering in Data Streaming\" <doi:10.1016/j.procs.2023.12.058>.",
    "version": "1.7.10",
    "maintainer": "Luis Alfonso Perez Martos <lapm0001@gmail.com>",
    "url": "https://github.com/laperez/clustering",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1309,
    "package_name": "CoClust",
    "title": "A Copula-Based Clustering Algorithm",
    "description": "A copula based clustering algorithm that finds clusters according to the complex multivariate dependence structure of the data generating process. The updated version of the algorithm is described in Di Lascio, F.M.L. and Giannerini, S. (2019). \"Clustering dependent observations with copula functions\". Statistical Papers, 60, p.35-51. <doi:10.1007/s00362-016-0822-3>.",
    "version": "1.0-0",
    "maintainer": "Francesca Marta Lilja Di Lascio <marta.dilascio@unibz.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1335,
    "package_name": "CollapseLevels",
    "title": "Collapses Levels, Computes Information Value and WoE",
    "description": "Contains functions to help in selecting and exploring features ( or variables ) in binary classification problems.\n             Provides functions to compute and display information value and weight of evidence (WoE) of the variables , and to convert numeric variables to categorical variables by binning.\n             Functions are also provided  to determine which levels ( or categories ) of a categorical variable can be collapsed (or combined ) based on their response rates.\n             The functions provided only work for binary classification problems.",
    "version": "0.3.0",
    "maintainer": "Krishanu Mukherjee <toton1181@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1353,
    "package_name": "CompClassMetrics",
    "title": "Classification Measures when Subclasses are Involved",
    "description": "Accuracy metrics are commonly used to assess the discriminating ability of diagnostic tests or biomarkers. Among them, metrics based on the ROC framework are particularly popular. When classification involves subclasses, the package 'CompClassMetrics' includes functions that can provide the point estimate, confidence interval as well as true values if a parametric setting is known. For more details see Nan and Tian (2025) <doi:10.1177/09622802251343600> and Nan and Tian (2023) <doi:10.1002/sim.9908> and Feng and Tian (2020) <doi:10.1177/0962280220938077>.",
    "version": "0.1.0",
    "maintainer": "Nan Nan <nannan@buffalo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1363,
    "package_name": "CompareTests",
    "title": "Correct for Verification Bias in Diagnostic Accuracy & Agreement",
    "description": "A standard test is observed on all specimens.  We treat the second test (or sampled test) as being conducted on only a stratified sample of specimens.  Verification Bias is this situation when the specimens for doing the second (sampled) test is not under investigator control.  We treat the total sample as stratified two-phase sampling and use inverse probability weighting.  We estimate diagnostic accuracy (category-specific classification probabilities; for binary tests reduces to specificity and sensitivity, and also predictive values) and agreement statistics (percent agreement, percent agreement by category, Kappa (unweighted), Kappa (quadratic weighted) and symmetry tests (reduces to McNemar's test for binary tests)).  See: Katki HA, Li Y, Edelstein DW, Castle PE.  Estimating the agreement and diagnostic accuracy of two diagnostic tests when one test is conducted on only a subsample of specimens. Stat Med. 2012 Feb 28; 31(5) <doi:10.1002/sim.4422>.",
    "version": "1.3",
    "maintainer": "Hormuzd Katki <katkih@mail.nih.gov>",
    "url": "https://dceg.cancer.gov/about/staff-directory/katki-hormuzd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1370,
    "package_name": "CompositionalClust",
    "title": "Clustering with Compositional Data",
    "description": "Cluster analysis with compositional data using the alpha--transformation. Relevant papers include: Tsagris M. and Kontemeniotis N. (2025), <doi:10.48550/arXiv.2509.05945>. Tsagris M.T., Preston S. and Wood A.T.A. (2011), <doi:10.48550/arXiv.1106.1451>. Garcia-Escudero Luis A., Gordaliza Alfonso, Matran Carlos, Mayo-Iscar Agustin. (2008), <doi:10.1214/07-AOS515>.",
    "version": "1.2",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1373,
    "package_name": "CompositionalRF",
    "title": "Multivariate Random Forest with Compositional Responses",
    "description": "Multivariate random forests with compositional responses and Euclidean predictors is performed. The compositional data are first transformed using the additive log-ratio transformation, or the alpha-transformation of Tsagris, Preston and Wood (2011), <doi:10.48550/arXiv.1106.1451>, and then the multivariate random forest of Rahman R., Otridge J. and Pal R. (2017), <doi:10.1093/bioinformatics/btw765>, is applied. ",
    "version": "1.4",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1387,
    "package_name": "CondiS",
    "title": "Censored Data Imputation for Direct Modeling",
    "description": "Impute the survival times for censored observations based on their conditional survival distributions derived from the Kaplan-Meier estimator. 'CondiS' can replace the censored observations with the best approximations from the statistical model, allowing for direct application of machine learning-based methods. When covariates are available, 'CondiS' is extended by incorporating the covariate information through machine learning-based regression modeling ('CondiS_X'), which can further improve the imputed survival time.",
    "version": "0.1.2",
    "maintainer": "Yizhuo Wang <ywang70@mdanderson.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1393,
    "package_name": "ConfusionTableR",
    "title": "Confusion Matrix Toolset",
    "description": "Takes the outputs of a 'caret' confusion matrix and allows for the quick conversion of these list items to lists.\n    The intended usage is to allow the tool to work with the outputs of machine learning classification models. \n    This tool works with classification problems for binary and multi-classification problems and allows for the record level conversion of the confusion matrix outputs.\n    This is useful, as it allows quick conversion of these objects for storage in database systems and to track ML model performance over time.\n    Traditionally, this approach has been used for highlighting model representation and feature slippage. ",
    "version": "1.0.4",
    "maintainer": "Gary Hutson <hutsons-hacks@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1408,
    "package_name": "ContaminatedMixt",
    "title": "Clustering and Classification with the Contaminated Normal",
    "description": "Fits mixtures of multivariate contaminated normal distributions\n        (with eigen-decomposed scale matrices) via the expectation conditional-\n\tmaximization algorithm under a clustering or classification paradigm\n\tMethods are described in Antonio Punzo, Angelo Mazza, and Paul D McNicholas (2018) <doi:10.18637/jss.v085.i10>.",
    "version": "1.3.8",
    "maintainer": "Angelo Mazza <a.mazza@unict.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1410,
    "package_name": "ConvergenceClubs",
    "title": "Finding Convergence Clubs",
    "description": "Functions for clustering regions that form convergence clubs, according to the definition of Phillips and Sul (2009) <doi:10.1002/jae.1080>. A package description is available in Sichera and Pizzuto (2019).",
    "version": "2.2.5",
    "maintainer": "Roberto Sichera <rob.sichera@gmail.com>",
    "url": "https://CRAN.R-project.org/package=ConvergenceClubs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1413,
    "package_name": "ConvertPar",
    "title": "Estimating IRT Parameters via Machine Learning Algorithms",
    "description": "A tool to estimate IRT item parameters (2 PL) using CTT-based item statistics from small samples via artificial neural networks and regression trees.",
    "version": "0.1",
    "maintainer": "Eda Akdogdu Yildiz <akdogdueda@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1454,
    "package_name": "CoxAIPW",
    "title": "Doubly Robust Inference for Cox Marginal Structural Model with\nInformative Censoring",
    "description": "Doubly robust estimation and inference of log hazard ratio under the Cox marginal structural model with informative censoring. An augmented inverse probability weighted estimator that involves 3 working models, one for conditional failure time T, one for conditional censoring time C and one for propensity score. Both models for T and C can depend on both a binary treatment A and additional baseline covariates Z, while the propensity score model only depends on Z. With the help of cross-fitting techniques, achieves the rate-doubly robust property that allows the use of most machine learning or non-parametric methods for all 3 working models, which are not permitted in classic inverse probability weighting or doubly robust estimators. When the proportional hazard assumption is violated, CoxAIPW estimates a causal estimated that is a weighted average of the time-varying log hazard ratio. Reference: Luo, J. (2023). Statistical Robustness - Distributed Linear Regression, Informative Censoring, Causal Inference, and Non-Proportional Hazards [Unpublished doctoral dissertation]. University of California San Diego.; Luo & Xu (2022) <doi:10.48550/arXiv.2206.02296>; Rava (2021) <https://escholarship.org/uc/item/8h1846gs>.",
    "version": "0.0.3",
    "maintainer": "Jiyu Luo <charlesluo1002@gmail.com>",
    "url": "https://github.com/charlesluo1002/CoxAIPW",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1475,
    "package_name": "CrossValidate",
    "title": "Classes and Methods for Cross Validation of \"Class Prediction\"\nAlgorithms",
    "description": "Defines classes and methods to cross-validate various\n  binary classification algorithms used for \"class prediction\"\n  problems.",
    "version": "2.3.5",
    "maintainer": "Kevin R. Coombes <krc@silicovore.com>",
    "url": "http://oompa.r-forge.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1482,
    "package_name": "Cubist",
    "title": "Rule- And Instance-Based Regression Modeling",
    "description": "Regression modeling using rules with added instance-based\n    corrections.",
    "version": "0.5.1",
    "maintainer": "Max Kuhn <mxkuhn@gmail.com>",
    "url": "https://topepo.github.io/Cubist/, https://github.com/topepo/Cubist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1488,
    "package_name": "CustomerScoringMetrics",
    "title": "Evaluation Metrics for Customer Scoring Models Depending on\nBinary Classifiers",
    "description": "Functions for evaluating and visualizing predictive model performance (specifically: binary classifiers) in the field of customer scoring. These metrics include lift, lift index, gain percentage, top-decile lift, F1-score, expected misclassification cost and absolute misclassification cost. See Berry & Linoff (2004, ISBN:0-471-47064-3), Witten and Frank (2005, 0-12-088407-0) and Blattberg, Kim & Neslin (2008, ISBN:978–0–387–72578–9) for details. Visualization functions are included for lift charts and gain percentage charts. All metrics that require class predictions offer the possibility to dynamically determine cutoff values for transforming real-valued probability predictions into class predictions.",
    "version": "1.0.0",
    "maintainer": "Koen W. De Bock <kdebock@audencia.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1501,
    "package_name": "CytoProfile",
    "title": "Cytokine Profiling Analysis Tool",
    "description": "Provides comprehensive cytokine profiling analysis through quality control using biologically meaningful cutoffs on raw cytokine measurements and by testing for distributional symmetry to recommend appropriate transformations. Offers exploratory data analysis with summary statistics, enhanced boxplots, and barplots, along with univariate and multivariate analytical capabilities for in-depth cytokine profiling such as Principal Component Analysis based on Andrzej Maćkiewicz and Waldemar Ratajczak (1993) <doi:10.1016/0098-3004(93)90090-R>, Sparse Partial Least Squares Discriminant Analysis based on Lê Cao K-A, Boitard S, and Besse P (2011) <doi:10.1186/1471-2105-12-253>, Random Forest based on Breiman, L. (2001) <doi:10.1023/A:1010933404324>, and Extreme Gradient Boosting based on Tianqi Chen and Carlos Guestrin (2016) <doi:10.1145/2939672.2939785>.",
    "version": "0.2.3",
    "maintainer": "Shubh Saraswat <shubh.saraswat00@gmail.com>",
    "url": "https://github.com/saraswatsh/CytoProfile,\nhttps://cytoprofile.cytokineprofile.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1518,
    "package_name": "DALEXtra",
    "title": "Extension for 'DALEX' Package",
    "description": "Provides wrapper of various machine learning models. \n  In applied machine learning, there \n  is a strong belief that we need to strike a balance \n  between interpretability and accuracy. \n  However, in field of the interpretable machine learning, \n  there are more and more new ideas for explaining black-box models, \n  that are implemented in 'R'. \n  'DALEXtra' creates 'DALEX' Biecek (2018) <arXiv:1806.08915> explainer for many type of models\n  including those created using 'python' 'scikit-learn' and 'keras' libraries, and 'java' 'h2o' library. \n  Important part of the package is Champion-Challenger analysis and innovative approach\n  to model performance across subsets of test data presented in Funnel Plot. ",
    "version": "2.3.0",
    "maintainer": "Szymon Maksymiuk <sz.maksymiuk@gmail.com>",
    "url": "https://ModelOriented.github.io/DALEXtra/,\nhttps://github.com/ModelOriented/DALEXtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1529,
    "package_name": "DBHC",
    "title": "Sequence Clustering with Discrete-Output HMMs",
    "description": "Provides an implementation of a mixture of hidden Markov models \n    (HMMs) for discrete sequence data in the Discrete Bayesian HMM Clustering \n    (DBHC) algorithm. The DBHC algorithm is an HMM Clustering \n    algorithm that finds a mixture of discrete-output HMMs while using \n    heuristics based on Bayesian Information Criterion (BIC) to search for the \n    optimal number of HMM states and the optimal number of clusters. ",
    "version": "0.0.3",
    "maintainer": "Gabriel Budel <gabysp_budel@hotmail.com>",
    "url": "https://github.com/gabybudel/DBHC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1585,
    "package_name": "DET",
    "title": "Representation of DET Curve with Confidence Intervals",
    "description": "\n    Builds both ROC (Receiver Operating Characteristic) and DET (Detection Error Tradeoff) curves from a set of predictors, which are the\n    results of a binary classification system. The curves give a general vision of the performance of the classifier,\n    and are useful for comparing performance of different systems.",
    "version": "3.0.2",
    "maintainer": "\"Curran, James\" <j.curran@auckland.ac.nz>",
    "url": "https://github.com/jmcurran/DET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1642,
    "package_name": "DIscBIO",
    "title": "A User-Friendly Pipeline for Biomarker Discovery in Single-Cell\nTranscriptomics",
    "description": "An open, multi-algorithmic pipeline for easy, fast and efficient\n  analysis of cellular sub-populations and the molecular signatures that\n  characterize them. The pipeline consists of four successive steps: data\n  pre-processing, cellular clustering with pseudo-temporal ordering, defining\n  differential expressed genes and biomarker identification. More details on\n  Ghannoum et. al. (2021) <doi:10.3390/ijms22031399>. This package implements\n  extensions of the work published by Ghannoum et. al. (2019)\n  <doi:10.1101/700989>.",
    "version": "1.2.2",
    "maintainer": "Waldir Leoncio <w.l.netto@medisin.uio.no>",
    "url": "https://github.com/ocbe-uio/DIscBIO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1686,
    "package_name": "DPCD",
    "title": "Dirichlet Process Clustering with Dissimilarities",
    "description": "A Bayesian hierarchical model for clustering dissimilarity data using the Dirichlet process. The latent configuration of objects and the number of clusters are automatically inferred during the fitting process. The package supports multiple models which are available to detect clusters of various shapes and sizes using different covariance structures. Additional functions are included to ensure adequate model fits through prior and posterior predictive checks. ",
    "version": "0.0.1",
    "maintainer": "Sam Morrissette <samuel.morrissette01@gmail.com>",
    "url": "https://github.com/SamMorrissette/DPCD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1692,
    "package_name": "DPpack",
    "title": "Differentially Private Statistical Analysis and Machine Learning",
    "description": "An implementation of common statistical analysis and models with\n    differential privacy (Dwork et al., 2006a) <doi:10.1007/11681878_14>\n    guarantees. The package contains, for example, functions providing\n    differentially private computations of mean, variance, median, histograms,\n    and contingency tables. It also implements some statistical models and\n    machine learning algorithms such as linear regression (Kifer et al., 2012)\n    <https://proceedings.mlr.press/v23/kifer12.html>\n    and SVM (Chaudhuri et al., 2011)\n    <https://jmlr.org/papers/v12/chaudhuri11a.html>. In addition, it implements\n    some popular randomization mechanisms, including\n    the Laplace mechanism (Dwork et al., 2006a)\n    <doi:10.1007/11681878_14>, Gaussian mechanism (Dwork et al., 2006b)\n    <doi:10.1007/11761679_29>, analytic Gaussian mechanism (Balle & Wang, 2018)\n    <https://proceedings.mlr.press/v80/balle18a.html>, and exponential mechanism\n    (McSherry & Talwar, 2007) <doi:10.1109/FOCS.2007.66>.",
    "version": "0.2.2",
    "maintainer": "Spencer Giddens <giddens2spencer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1711,
    "package_name": "DRquality",
    "title": "Quality Measurements for Dimensionality Reduction",
    "description": "Several quality measurements for investigating the performance of dimensionality reduction methods are provided here. In addition a new quality measurement called Gabriel classification error is made accessible, which was published in Thrun, M. C., Märte, J., & Stier, Q: \"Analyzing Quality Measurements for Dimensionality Reduction\" (2023), Machine Learning and Knowledge Extraction (MAKE), <DOI:10.3390/make5030056>.",
    "version": "0.2.1",
    "maintainer": "Michael Thrun <m.thrun@gmx.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1772,
    "package_name": "DataFusionGDM",
    "title": "Machine Learning for Integrating Partially Overlapped Genetic\nDatasets",
    "description": "Tools to simulate genetic distance matrices, align and compare them via\n    multidimensional scaling (MDS) and Procrustes, and evaluate imputation with\n    the Bootstrapping Evaluation for Structural Missingness Imputation (BESMI)\n    framework. Methods align with Zhu et al. (2025) <doi:10.3389/fpls.2025.1543956>\n    and the associated software resource Zhu (2025) <doi:10.26188/28602953>.",
    "version": "1.3.2",
    "maintainer": "Jiashuai Zhu <jiashuai.zhu@student.unimelb.edu.au>",
    "url": "https://github.com/jiashuaiz/DataFusion-GDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1785,
    "package_name": "DatabionicSwarm",
    "title": "Swarm Intelligence for Self-Organized Clustering",
    "description": "Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: \"Swarm Intelligence for Self-Organized Clustering\" (2020), Artificial Intelligence, <DOI:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: \"Projection Based Clustering through Self-Organization and Swarm Intelligence\" (2018) <DOI:10.1007/978-3-658-20540-9>.",
    "version": "2.0.0",
    "maintainer": "Michael Thrun <m.thrun@gmx.net>",
    "url": "https://www.deepbionics.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1860,
    "package_name": "DidacticBoost",
    "title": "A Simple Implementation and Demonstration of Gradient Boosting",
    "description": "A basic, clear implementation of tree-based gradient boosting\n    designed to illustrate the core operation of boosting models. Tuning\n    parameters (such as stochastic subsampling, modified learning rate, or\n    regularization) are not implemented. The only adjustable parameter is the\n    number of training rounds. If you are looking for a high performance boosting\n    implementation with tuning parameters, consider the 'xgboost' package.",
    "version": "0.1.1",
    "maintainer": "David Shaub <davidshaub@gmx.com>",
    "url": "https://github.com/dashaub/DidacticBoost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1876,
    "package_name": "DirectedClustering",
    "title": "Directed Weighted Clustering Coefficient",
    "description": "Allows the computation of clustering coefficients for directed and weighted networks by using different approaches. \n    It allows to compute clustering coefficients that are not present in 'igraph' package. \n    A description of clustering coefficients can be found in \"Directed clustering in weighted networks: a new perspective\", Clemente, G.P., Grassi, R. (2017),  \t\n    <doi:10.1016/j.chaos.2017.12.007>.",
    "version": "1.0.0",
    "maintainer": "Gian Paolo Clemente <gianpaolo.clemente@unicatt.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1891,
    "package_name": "DiscreteGapStatistic",
    "title": "An Extension of the Gap Statistic for Ordinal/Categorical Data",
    "description": "The gap statistic approach is extended to estimate the number of clusters for categorical response format data. This approach and accompanying software is designed to be used with the output of any clustering algorithm and with distances specifically designed for categorical (i.e. multiple choice) or ordinal survey response data.",
    "version": "1.1.2",
    "maintainer": "Eduardo Cortes <ecortesg@buffalo.edu>",
    "url": "https://github.com/ecortesgomez/DiscreteGapStatistic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1953,
    "package_name": "DynForest",
    "title": "Random Forest with Multivariate Longitudinal Predictors",
    "description": "Based on random forest principle, 'DynForest' is able to include \n    multiple longitudinal predictors to provide individual predictions. \n    Longitudinal predictors are modeled through the random forest. The \n    methodology is fully described for a survival outcome in: \n    Devaux, Helmer, Genuer & Proust-Lima (2023) \n    <doi: 10.1177/09622802231206477>.",
    "version": "1.2.0",
    "maintainer": "Anthony Devaux <anthony.devauxbarault@gmail.com>",
    "url": "https://github.com/anthonydevaux/DynForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1982,
    "package_name": "ECTTDNN",
    "title": "Cointegration Based Timedelay Neural Network Model",
    "description": "This cointegration based Time Delay Neural Network Model hybrid model allows the researcher to make use of the information extracted by the cointegrating vector as an input in the neural network model.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1984,
    "package_name": "ECoL",
    "title": "Complexity Measures for Supervised Problems",
    "description": "Provides measures to characterize the complexity of classification \n    and regression problems based on aspects that quantify the linearity of the \n    data, the presence of informative feature, the sparsity and dimensionality \n    of the datasets. This package provides bug fixes, generalizations and \n    implementations of many state of the art measures. The measures are \n    described in the papers: Lorena et al. (2019) <doi:10.1145/3347711> and \n    Lorena et al. (2018) <doi:10.1007/s10994-017-5681-1>.",
    "version": "0.3.0",
    "maintainer": "Luis Garcia <lpfgarcia@icmc.usp.br>",
    "url": "https://github.com/lpfgarcia/ECoL/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1997,
    "package_name": "EEML",
    "title": "Ensemble Explainable Machine Learning Models",
    "description": "We introduced a novel ensemble-based explainable machine learning model using Model Confidence Set (MCS) and two stage Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm. The model combined the predictive capabilities of different machine-learning models and integrates the interpretability of explainability methods. To develop the proposed algorithm, a two-stage Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) framework was employed. The package has been developed using the algorithm of Paul et al. (2023) <doi:10.1007/s40009-023-01218-x> and Yeasin and Paul (2024) <doi:10.1007/s11227-023-05542-3>.",
    "version": "0.1.1",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2001,
    "package_name": "EFAfactors",
    "title": "Determining the Number of Factors in Exploratory Factor Analysis",
    "description": "Provides a collection of standard factor retention methods in Exploratory Factor \n             Analysis (EFA), making it easier to determine the number of factors. Traditional \n             methods such as the scree plot by Cattell (1966) <doi:10.1207/s15327906mbr0102_10>, \n             Kaiser-Guttman Criterion (KGC) by Guttman (1954) <doi:10.1007/BF02289162> and \n             Kaiser (1960) <doi:10.1177/001316446002000116>, and flexible Parallel Analysis \n             (PA) by Horn (1965) <doi:10.1007/BF02289447> based on eigenvalues form PCA or EFA \n             are readily available. This package also implements several newer methods, such as \n             the Empirical Kaiser Criterion (EKC) by Braeken and van Assen (2017) \n             <doi:10.1037/met0000074>, Comparison Data (CD) by Ruscio and Roche (2012) \n             <doi:10.1037/a0025697>, and Hull method by Lorenzo-Seva et al. (2011) \n             <doi:10.1080/00273171.2011.564527>, as well as some AI-based methods like \n             Comparison Data Forest (CDF) by Goretzko and Ruscio (2024) \n             <doi:10.3758/s13428-023-02122-4> and Factor Forest (FF) by Goretzko and Buhner \n             (2020) <doi:10.1037/met0000262>. Additionally, it includes a deep neural network \n             (DNN) trained on large-scale datasets that can efficiently and reliably determine \n             the number of factors.",
    "version": "1.2.4",
    "maintainer": "Haijiang Qin <haijiang133@outlook.com>",
    "url": "https://haijiangqin.com/EFAfactors/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2017,
    "package_name": "EIX",
    "title": "Explain Interactions in 'XGBoost'",
    "description": "Structure mining from 'XGBoost' and 'LightGBM' models.\n    Key functionalities of this package cover: visualisation of tree-based ensembles models,\n    identification of interactions, measuring of variable importance,\n    measuring of interaction importance, explanation of single prediction \n    with break down plots (based on 'xgboostExplainer' and 'iBreakDown' packages). \n    To download the 'LightGBM' use the following link: <https://github.com/Microsoft/LightGBM>.\n    'EIX' is a part of the 'DrWhy.AI' universe.",
    "version": "1.2.0",
    "maintainer": "Szymon Maksymiuk <sz.maksymiuk@gmail.com>",
    "url": "https://github.com/ModelOriented/EIX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2025,
    "package_name": "ELMR",
    "title": "Extreme Machine Learning (ELM)",
    "description": "Training and prediction functions are provided for the Extreme Learning Machine algorithm (ELM). The ELM use a Single Hidden Layer Feedforward Neural Network (SLFN) with random generated weights and no gradient-based backpropagation. The training time is very short and the online version allows to update the model using small chunk of the training set at each iteration. The only parameter to tune is the hidden layer size and the learning function.",
    "version": "1.0",
    "maintainer": "Alessio Petrozziello <alessio.petrozziello@port.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2034,
    "package_name": "EMCluster",
    "title": "EM Algorithm for Model-Based Clustering of Finite Mixture\nGaussian Distribution",
    "description": "EM algorithms and several efficient\n        initialization methods for model-based clustering of finite\n        mixture Gaussian distribution with unstructured dispersion\n        in both of unsupervised and semi-supervised learning.",
    "version": "0.2-17",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "url": "https://github.com/snoweye/EMCluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2045,
    "package_name": "EMMIXgene",
    "title": "A Mixture Model-Based Approach to the Clustering of Microarray\nExpression Data",
    "description": "Provides unsupervised selection and clustering of microarray data\n    using mixture models. Following the methods described in McLachlan, Bean and\n    Peel (2002) <doi:10.1093/bioinformatics/18.3.413> a subset of genes are selected\n    based one the likelihood ratio statistic for the test of one versus two\n    components when fitting mixtures of t-distributions to the expression data\n    for each gene. The dimensionality of this gene subset is further reduced through\n    the use of mixtures of factor analyzers, allowing the tissue samples to be\n    clustered by fitting mixtures of normal distributions.",
    "version": "0.1.4",
    "maintainer": "Andrew Thomas Jones <andrewthomasjones@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2050,
    "package_name": "EMP",
    "title": "Expected Maximum Profit Classification Performance Measure",
    "description": "Functions for estimating EMP (Expected Maximum Profit Measure) in Credit Risk Scoring and Customer Churn Prediction, according to Verbraken et al (2013, 2014) <DOI:10.1109/TKDE.2012.50>, <DOI:10.1016/j.ejor.2014.04.001>.",
    "version": "2.0.6",
    "maintainer": "Cristian Bravo <cbravoro@uwo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2056,
    "package_name": "EMbC",
    "title": "Expectation-Maximization Binary Clustering",
    "description": "Unsupervised, multivariate, binary clustering for meaningful annotation of data, taking into account the uncertainty in the data. A specific constructor for trajectory analysis in movement ecology yields behavioural annotation of trajectories based on estimated local measures of velocity and turning angle, eventually with solar position covariate as a daytime indicator, (\"Expectation-Maximization Binary Clustering for Behavioural Annotation\").",
    "version": "2.0.4",
    "maintainer": "Joan Garriga <jgarriga@ceab.csic.es>",
    "url": "<doi:10.1371/journal.pone.0151984>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2065,
    "package_name": "EPX",
    "title": "Ensemble of Phalanxes",
    "description": "An ensemble method for the statistical detection of\n    a rare class in two-class classification problems. The method uses an\n    ensemble of classifiers where the constituent\n    models of the ensemble use disjoint subsets (phalanxes) of explanatory\n    variables. We provide an implementation of the phalanx-formation algorithm.\n    Please see Tomal et al. (2015) <doi:10.1214/14-AOAS778>,\n    Tomal et al. (2016) <doi:10.1021/acs.jcim.5b00663>, and\n    Tomal et al. (2019) <arXiv:1706.06971> for more details.",
    "version": "1.0.4",
    "maintainer": "Jabed Tomal <jtomal@tru.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2067,
    "package_name": "EQRN",
    "title": "Extreme Quantile Regression Neural Networks for Risk Forecasting",
    "description": "This framework enables forecasting and extrapolating measures of conditional risk\n    (e.g. of extreme or unprecedented events), including quantiles and exceedance probabilities,\n    using extreme value statistics and flexible neural network architectures.\n    It allows for capturing complex multivariate dependencies,\n    including dependencies between observations, such as sequential dependence (time-series).\n    The methodology was introduced in Pasche and Engelke (2024) <doi:10.1214/24-AOAS1907>\n    (also available in preprint: Pasche and Engelke (2022) <doi:10.48550/arXiv.2208.07590>).",
    "version": "0.1.2",
    "maintainer": "Olivier C. Pasche <olivier_pasche@alumni.epfl.ch>",
    "url": "https://github.com/opasche/EQRN, https://opasche.github.io/EQRN/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2102,
    "package_name": "EZFragility",
    "title": "Compute Neural Fragility for Ictal iEEG Time Series",
    "description": "Provides tools to compute the neural fragility matrix from intracranial electrocorticographic (iEEG) recordings, enabling the analysis of brain dynamics during seizures. The package implements the method described by Li et al. (2017) <doi:10.23919/ACC.2017.7963378> and includes functions for data preprocessing ('Epoch'), fragility computation ('calcAdjFrag'), and visualization.",
    "version": "2.0.1",
    "maintainer": "Jiefei Wang <szwjf08@gmail.com>",
    "url": "https://github.com/Jiefei-Wang/EZFragility",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2126,
    "package_name": "EcotoneFinder",
    "title": "Characterising and Locating Ecotones and Communities",
    "description": "Analytical methods to locate and characterise ecotones, ecosystems and environmental patchiness along ecological gradients. Methods are implemented for isolated sampling or for space/time series. It includes Detrended Correspondence Analysis (Hill & Gauch (1980) <doi:10.1007/BF00048870>), fuzzy clustering (De Cáceres et al. (2010) <doi:10.1080/01621459.1963.10500845>), biodiversity indices (Jost (2006) <doi:10.1111/j.2006.0030-1299.14714.x>), and network analyses (Epskamp et al. (2012) <doi:10.18637/jss.v048.i04>) - as well as tools to explore the number of clusters in the data. Functions to produce synthetic ecological datasets are also provided.",
    "version": "0.2.3",
    "maintainer": "Antoine Bagnaro <antoine.bagnaro@wanadoo.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2159,
    "package_name": "EnsCat",
    "title": "Clustering of Categorical Data",
    "description": "An implementation of the clustering methods of categorical data\n    discussed in Amiri, S., Clarke, B., and Clarke, J. (2015). Clustering categorical \n    data via ensembling dissimilarity matrices.  Preprint <arXiv:1506.07930>.",
    "version": "1.1",
    "maintainer": "Saeid Amiri <saeid.amiri1@gmail.com>",
    "url": "https://github.com/jlp2duke/EnsCat/wiki/How-To-with-Examples",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2220,
    "package_name": "EventDetectR",
    "title": "Event Detection Framework",
    "description": "Detect events in time-series data. Combines multiple well-known R packages like 'forecast' and 'neuralnet' to deliver an easily configurable tool for multivariate event detection.",
    "version": "0.3.5",
    "maintainer": "Sowmya Chandrasekaran <sowzz.17@gmail.com>",
    "url": "https://github.com/frehbach/EventDetectR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2277,
    "package_name": "FADPclust",
    "title": "Functional Data Clustering Using Adaptive Density Peak Detection",
    "description": "An implementation of a clustering algorithm for functional data based on adaptive density peak detection technique, in which the density is estimated by functional k-nearest neighbor density estimation based on a proposed semi-metric between functions. The proposed functional data clustering algorithm is computationally fast since it does not need iterative process. (Alex Rodriguez and Alessandro Laio (2014) <doi:10.1126/science.1242072>; Xiao-Feng Wang and Yifan Xu (2016) <doi:10.1177/0962280215609948>).",
    "version": "1.1.1",
    "maintainer": "Rui Ren <xmurr@stu.xmu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2283,
    "package_name": "FARDEEP",
    "title": "Fast and Robust Deconvolution of Tumor Infiltrating Lymphocyte\nfrom Expression Profiles using Least Trimmed Squares",
    "description": "Using the idea of least trimmed square, it could automatically detects and removes outliers from data before estimating the coefficients. It is a robust machine learning tool which can be applied to gene-expression deconvolution technique. Yuning Hao, Ming Yan, Blake R. Heath, Yu L. Lei and Yuying Xie (2019) <doi:10.1101/358366>.",
    "version": "1.0.1",
    "maintainer": "Yuying Xie <xyy@egr.msu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2299,
    "package_name": "FCPS",
    "title": "Fundamental Clustering Problems Suite",
    "description": "Over sixty clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the mirrored density plot (MD-plot) of clusterability are implemented. The packages is published in Thrun, M.C., Stier Q.: \"Fundamental Clustering Algorithms Suite\" (2021), SoftwareX, <DOI:10.1016/j.softx.2020.100642>. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: \"Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems\" (2020), Data in Brief, <DOI:10.1016/j.dib.2020.105501>.",
    "version": "1.3.5",
    "maintainer": "Michael Thrun <m.thrun@gmx.net>",
    "url": "https://www.deepbionics.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2317,
    "package_name": "FFTrees",
    "title": "Generate, Visualise, and Evaluate Fast-and-Frugal Decision Trees",
    "description": "Create, visualize, and test fast-and-frugal decision trees (FFTs) using the algorithms and methods described by Phillips, Neth, Woike & Gaissmaier (2017), <doi:10.1017/S1930297500006239>. \n    FFTs are simple and transparent decision trees for solving binary classification problems. \n    FFTs can be preferable to more complex algorithms because they require very little information, are easy to understand and communicate, and are robust against overfitting.",
    "version": "2.1.0",
    "maintainer": "Hansjoerg Neth <h.neth@uni.kn>",
    "url": "https://CRAN.R-project.org/package=FFTrees,\nhttps://www.nathanieldphillips.co/FFTrees/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2335,
    "package_name": "FKmL",
    "title": "Fréchet Distance-Based K-Means and Extensions for Longitudinal\nData",
    "description": "Implements shape-based clustering algorithms for multidimensional longitudinal data based on the Fréchet distance. It implements two main methods: MFKmL (Multidimensional Fréchet distance-based K-means for Longitudinal data), an extension of the K-means algorithm using the Fréchet distance originally developed in the 'kmlShape' package, adapted for multidimensional trajectories; and SFKmL (Sparse multidimensional Fréchet distance-based K-medoids for Longitudinal data), a K-medoids-based clustering algorithm that incorporates variable selection. These tools are designed to enhance clustering performance in high-dimensional longitudinal data settings, particularly those with time delays, variations in trajectory speed, irregular sampling intervals, and noise. This package implements methods derived from Kang et al. (2023) <doi:10.1007/s11222-023-10237-z>.",
    "version": "0.1.1",
    "maintainer": "Ji Hyun Park <jhn1105@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2342,
    "package_name": "FLR",
    "title": "Fuzzy Logic Rule Classifier",
    "description": "FLR algorithm for classification",
    "version": "1.0",
    "maintainer": "Constantinos Mavridis <consmavr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2355,
    "package_name": "FNN",
    "title": "Fast Nearest Neighbor Search Algorithms and Applications",
    "description": "Cover-tree and kd-tree fast k-nearest neighbor search algorithms and related applications\n        including KNN classification, regression and information measures are implemented.",
    "version": "1.1.4.1",
    "maintainer": "Shengqiao Li <lishengqiao@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2356,
    "package_name": "FOCI",
    "title": "Feature Ordering by Conditional Independence",
    "description": "Feature Ordering by Conditional Independence (FOCI) is a variable selection algorithm based on the measure of conditional dependence.\n    For more information, see the paper: Azadkia and Chatterjee (2019),\"A simple measure of conditional dependence\" <arXiv:1910.12327>.",
    "version": "0.1.3",
    "maintainer": "Mona Azadkia <monaazadkia@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2360,
    "package_name": "FPDclustering",
    "title": "PD-Clustering and Related Methods",
    "description": "Probabilistic distance clustering (PD-clustering) is an iterative, distribution-free, probabilistic clustering method. PD-clustering assigns units to a cluster according to their probability of membership under the constraint that the product of the probability and the distance of each point to any cluster center is a constant. PD-clustering is a flexible method that can be used with elliptical clusters, outliers, or noisy data. PDQ is an extension of the algorithm for clusters of different sizes. GPDC and TPDC use a dissimilarity measure based on densities. Factor PD-clustering (FPDC) is a factor clustering method that involves a linear transformation of variables and a cluster optimizing the PD-clustering criterion. It works on high-dimensional data sets.",
    "version": "2.3.5",
    "maintainer": "Cristina Tortora <grikris1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2361,
    "package_name": "FPLdata",
    "title": "Read in Fantasy Premier League Data",
    "description": "This data contains a large variety of information on players and their\n  current attributes on Fantasy Premier League\n  <https://fantasy.premierleague.com/>. In particular, it contains a\n  `next_gw_points` (next gameweek points) value for each player\n  given their attributes in the current week. Rows represent player-gameweeks,\n  i.e. for each player there is a row for each gameweek. This\n  makes the data suitable for modelling a player's next gameweek points, given\n  attributes such as form, total points, and cost at the current gameweek.\n  This data can therefore be used to create Fantasy Premier League bots that\n  may use a machine learning algorithm and a linear programming solver\n  (for example) to return the best possible transfers and team to pick for\n  each gameweek, thereby fully automating the decision making process in\n  Fantasy Premier League. This function simply supplies the required data\n  for such a task.",
    "version": "0.1.0",
    "maintainer": "Andrew Little <andrewlittlebristol@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2370,
    "package_name": "FREEtree",
    "title": "Tree Method for High Dimensional Longitudinal Data",
    "description": "This tree-based method deals with high dimensional longitudinal \n\tdata with correlated features through the use of a piecewise random effect \n\tmodel. FREE tree also exploits the network structure of the features, by \n\tfirst clustering them using Weighted Gene Co-expression Network Analysis \n\t('WGCNA'). It then conducts a screening step within each cluster of features \n\tand a selecting step among the surviving features, which provides a relatively\n\tunbiased way to do feature selection. By using dominant principle components \n\tas regression variables at each leaf and the original features as splitting \n\tvariables at splitting nodes, FREE tree delivers easily interpretable results\n\twhile improving computational efficiency.",
    "version": "0.1.0",
    "maintainer": "Athanasse Zafirov <zafirov@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2386,
    "package_name": "FTRL",
    "title": "FTRL proximal algorithm for large scale online lerning",
    "description": "Fast C++ implementation of FTRL-proximal algorithm. Package can",
    "version": "0.1.1",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "url": "https://github.com/dselivanov/FTRL",
    "exports": [],
    "topics": ["ftrl", "logistic-regression", "machine-learning", "r", "sgd"],
    "score": "NA",
    "stars": 50
  },
  {
    "id": 2391,
    "package_name": "FWRGB",
    "title": "Fresh Weight Determination from Visual Image of the Plant",
    "description": "Fresh biomass determination is the key to evaluating crop genotypes' response to diverse input and stress conditions and forms the basis for calculating net primary production. However, as conventional phenotyping approaches for measuring fresh biomass is time-consuming, laborious and destructive, image-based phenotyping methods are being widely used now. In the image-based approach, the fresh weight of the above-ground part of the plant depends on the projected area. For determining the projected area, the visual image of the plant is converted into the grayscale image by simply averaging the Red(R), Green (G) and Blue (B) pixel values. Grayscale image is then converted into a binary image using Otsu’s thresholding method Otsu, N. (1979) <doi:10.1109/TSMC.1979.4310076> to separate plant area from the background (image segmentation). The segmentation process was accomplished by selecting the pixels with values over the threshold value belonging to the plant region and other pixels to the background region. The resulting binary image consists of white and black pixels representing the plant and background regions. Finally, the number of pixels inside the plant region was counted and converted to square centimetres (cm2) using the reference object (any object whose actual area is known previously) to get the projected area. After that, the projected area is used as input to the machine learning model (Linear Model, Artificial Neural Network, and Support Vector Regression) to determine the plant's fresh weight.",
    "version": "0.1.0",
    "maintainer": "Tanuj Misra <tanujmisra102@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2404,
    "package_name": "FairMclus",
    "title": "Clustering for Data with Sensitive Attribute",
    "description": "Clustering for categorical and mixed-type of data, to preventing classification biases due to race, \n            gender or others sensitive attributes.\n            This algorithm is an extension of the methodology proposed by \"Santos & Heras (2020) <doi:10.28945/4643>\".",
    "version": "2.2.1",
    "maintainer": "Carlos Santos-Mangudo   <carlossantos.csm@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2418,
    "package_name": "FastKNN",
    "title": "Fast k-Nearest Neighbors",
    "description": "Compute labels for a test set according to the k-Nearest Neighbors classification. This is a fast way to do k-Nearest Neighbors classification because the distance matrix -between the features of the observations- is an input to the function rather than being calculated in the function itself every time.",
    "version": "0.0.1",
    "maintainer": "Gaston Besanson <besanson@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2434,
    "package_name": "FeatureImpCluster",
    "title": "Feature Importance for Partitional Clustering",
    "description": "Implements a novel approach for measuring feature importance in k-means clustering. Importance of a feature is measured by the misclassification rate relative to the baseline cluster assignment due to a random permutation of feature values. An explanation of permutation feature importance in general can be found here: <https://christophm.github.io/interpretable-ml-book/feature-importance.html>.",
    "version": "0.1.5",
    "maintainer": "Oliver Pfaffel <opfaffel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2435,
    "package_name": "FeatureTerminatoR",
    "title": "Feature Selection Engine to Remove Features with Minimal\nPredictive Power",
    "description": "The aim is to take in data.frame inputs and utilises methods, such as recursive feature engineering, to enable the features to be removed.\n    What this does differently from the other packages, is that it gives you the choice to remove the variables manually, or it automated this process.\n    Feature selection is a concept in machine learning, and statistical pipelines, whereby unimportant, or less predictive variables are eliminated from the analysis, see Boughaci (2018) <doi:10.1007/s40595-018-0107-y>. ",
    "version": "1.0.0",
    "maintainer": "Gary Hutson <hutsons-hacks@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2460,
    "package_name": "FisherEM",
    "title": "The FisherEM Algorithm to Simultaneously Cluster and Visualize\nHigh-Dimensional Data",
    "description": "The FisherEM algorithm, proposed by Bouveyron & Brunet (2012) <doi:10.1007/s11222-011-9249-9>,\n        is an efficient method for the clustering of high-dimensional data. FisherEM models and \n        clusters the data in a discriminative and low-dimensional latent subspace. It also provides\n        a low-dimensional representation of the clustered data. A sparse version of Fisher-EM\n        algorithm is also provided.",
    "version": "1.6",
    "maintainer": "Charles Bouveyron <charles.bouveyron@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2496,
    "package_name": "ForesToolboxRS",
    "title": "Remote Sensing Tools for Forest Monitoring",
    "description": "Remote sensing tools for forest monitoring includes a series of",
    "version": "0.2.0",
    "maintainer": "",
    "url": "https://github.com/ytarazona/ForesToolboxRS",
    "exports": [],
    "topics": ["change-detection", "deforestation", "machine-learning", "remote-sensing"],
    "score": "NA",
    "stars": 58
  },
  {
    "id": 2525,
    "package_name": "FuncNN",
    "title": "Functional Neural Networks",
    "description": "A collection of functions which fit functional neural network models. In\n            other words, this package will allow users to build deep learning models \n            that have either functional or scalar responses paired with functional and \n            scalar covariates. We implement the theoretical discussion found \n            in Thind, Multani and Cao (2020) <arXiv:2006.09590> through the help of a main fitting and \n            prediction function as well as a number of helper functions to assist with \n            cross-validation, tuning, and the display of estimated functional weights.",
    "version": "1.0",
    "maintainer": "Barinder Thind <barinder.thi@gmail.com>",
    "url": "https://arxiv.org/abs/2006.09590, https://github.com/b-thi/FuncNN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2540,
    "package_name": "FuzzyQ",
    "title": "Fuzzy Quantification of Common and Rare Species",
    "description": "Fuzzy clustering of species in an ecological community as common or\n    rare based on their abundance and occupancy. It also includes functions to\n    compute confidence intervals of classification metrics and plot results. See\n    Balbuena et al. (2020, <doi:10.1101/2020.08.12.247502>).",
    "version": "0.1.0",
    "maintainer": "Juan A. Balbuena <j.a.balbuena@uv.es>",
    "url": "https://ligophorus.github.io/FuzzyQ/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2545,
    "package_name": "FuzzySpec",
    "title": "Fuzzy Spectral Clustering with Variable-Weighted Adjacency\nMatrices",
    "description": "Implementation of the FVIBES, the Fuzzy Variable-Importance Based Eigenspace Separation algorithm as described in the paper by Ghashti, J.S., Hare, W., and J.R.J. Thompson (2025). Variable-Weighted Adjacency Constructions for Fuzzy Spectral Clustering. Submitted.",
    "version": "1.0.0",
    "maintainer": "Jesse S. Ghashti <jesse.ghashti@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2562,
    "package_name": "GAMens",
    "title": "Applies GAMbag, GAMrsm and GAMens Ensemble Classifiers for\nBinary Classification",
    "description": "Implements the GAMbag, GAMrsm and GAMens ensemble\n    classifiers for binary classification (De Bock et al., 2010) <doi:10.1016/j.csda.2009.12.013>. The ensembles\n    implement Bagging (Breiman, 1996) <doi:10.1023/A:1010933404324>, the Random Subspace Method (Ho, 1998) <doi:10.1109/34.709601>\n    , or both, and use Hastie and Tibshirani's (1990, ISBN:978-0412343902) generalized additive models (GAMs)\n    as base classifiers. Once an ensemble classifier has been trained, it can\n    be used for predictions on new data. A function for cross validation is also\n    included.",
    "version": "1.2.1",
    "maintainer": "Koen W. De Bock <kdebock@audencia.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2565,
    "package_name": "GAPR",
    "title": "Generalized Association Plots",
    "description": "Provides a comprehensive framework for visualizing associations and interaction structures in matrix-formatted data using Generalized Association Plots (GAP). The package implements multiple proximity computation methods (e.g., correlation, distance metrics), ordering techniques including hierarchical clustering (HCT) and Rank-2-Ellipse (R2E) seriation, and optional flipping strategies to enhance visual symmetry. It supports a variety of covariate-based color annotations, allows flexible customization of layout and output, and is suitable for analyzing multivariate data across domains such as social sciences, genomics, and medical research. The method is based on Generalized Association Plots introduced by Chen (2002) <https://www3.stat.sinica.edu.tw/statistica/J12N1/J12N11/J12N11.html> and further extended by Wu, Tien, and Chen (2010) <doi:10.1016/j.csda.2008.09.029>.",
    "version": "0.1.4",
    "maintainer": "Shu-Yu Lin <shuyuuu89@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2607,
    "package_name": "GENEAclassify",
    "title": "Segmentation and Classification of Accelerometer Data",
    "description": "Segmentation and classification procedures for data from the 'Activinsights GENEActiv' <https://activinsights.com/technology/geneactiv/> accelerometer that provides the user with a model to guess behaviour from test data where behaviour is missing.\n    Includes a step counting algorithm, a function to create segmented data with custom features and a function to use recursive partitioning provided in the function rpart() of the 'rpart' package to create classification models.",
    "version": "1.5.5",
    "maintainer": "Jia Ying Chua <jiayingc@activinsights.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2622,
    "package_name": "GETdesigns",
    "title": "Generalized Extended Triangular Designs ('GETdesigns')",
    "description": "Since their introduction by Bose and Nair (1939) <https://www.jstor.org/stable/40383923>, partially balanced incomplete block (PBIB) designs remain an important class of incomplete block designs. The concept of association scheme was used by Bose and Shimamoto (1952) <doi:10.1080/01621459.1952.10501161> for the classification of these designs. The constraint of resources always motivates the experimenter to advance towards PBIB designs, more specifically to higher associate class PBIB designs from balanced incomplete block designs. It is interesting to note that many times higher associate PBIB designs perform better than their counterpart lower associate PBIB designs for the same set of parameters v, b, r, k and lambda_i (i=1,2...m). This package contains functions named GETD() for generating m-associate (m>=2) class PBIB designs along with parameters (v, b, r, k and lambda_i, i = 1, 2,…,m) based on Generalized Triangular (GT) Association Scheme. It also calculates the Information matrix, Average variance factor and canonical efficiency factor of the generated design. These designs, besides having good efficiency, require smaller number of replications and smallest possible concurrence of treatment pairs.",
    "version": "1.2.0",
    "maintainer": "Ashutosh Dalal <ashutosh.dalal97@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2653,
    "package_name": "GIC",
    "title": "A General Iterative Clustering Algorithm",
    "description": "An iterative algorithm that improves the proximity matrix (PM) from a random forest (RF) and the resulting clusters as measured by the silhouette score.",
    "version": "1.0.0",
    "maintainer": "Ziqiang Lin <linziqiang0314@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2684,
    "package_name": "GMCM",
    "title": "Fast Estimation of Gaussian Mixture Copula Models",
    "description": "Unsupervised Clustering and Meta-analysis using Gaussian Mixture\n    Copula Models.",
    "version": "1.4",
    "maintainer": "Anders Ellern Bilgrau <anders.ellern.bilgrau@gmail.com>",
    "url": "https://github.com/AEBilgrau/GMCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2686,
    "package_name": "GMDH2",
    "title": "Binary Classification via GMDH-Type Neural Network Algorithms",
    "description": "Performs binary classification via Group Method of Data Handling (GMDH) - type neural network algorithms. There exist two main algorithms available in GMDH() and dceGMDH() functions. GMDH() performs classification via GMDH algorithm for a binary response and returns important variables. dceGMDH() performs classification via diverse classifiers ensemble based on GMDH (dce-GMDH) algorithm. Also, the package produces a well-formatted table of descriptives for a binary response. Moreover, it produces confusion matrix, its related statistics and scatter plot (2D and 3D) with classification labels of binary classes to assess the prediction performance. All 'GMDH2' functions are designed for a binary response (Dag et al., 2019, <https://download.atlantis-press.com/article/125911202.pdf>).",
    "version": "1.8",
    "maintainer": "Osman Dag <osman.dag@outlook.com>",
    "url": "http://www.softmed.hacettepe.edu.tr/GMDH2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2689,
    "package_name": "GMMBoost",
    "title": "Likelihood-Based Boosting for Generalized Mixed Models",
    "description": "Likelihood-based boosting approaches for generalized mixed models are provided.",
    "version": "1.1.5",
    "maintainer": "Andreas Groll <groll@mathematik.uni-muenchen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2719,
    "package_name": "GPCsign",
    "title": "Gaussian Process Classification as Described in Bachoc et al.\n(2020)",
    "description": "Parameter estimation and prediction of Gaussian Process Classifier models as described in Bachoc et al. (2020) <doi:10.1007/S10898-020-00920-0>. Important functions : gpcm(), predict.gpcm(), update.gpcm().",
    "version": "0.1.1",
    "maintainer": "Morgane Menz <morgane.menz@ifpen.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2728,
    "package_name": "GPSeqClus",
    "title": "Sequential Clustering Algorithm for Location Data",
    "description": "Applies sequential clustering algorithm to animal location data \n    based on user-defined parameters. Plots interactive cluster maps and \n    provides a summary dataframe with attributes for each cluster commonly\n    used as covariates in subsequent modeling efforts. Additional functions\n    provide individual keyhole markup language plots for quick assessment,\n    and export of global positioning system exchange format files for\n    navigation purposes. \n    Methods can be found at <doi:10.1111/2041-210X.13572>.",
    "version": "1.4.0",
    "maintainer": "Justin Clapp <justin.clapp@wyo.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2749,
    "package_name": "GRNNs",
    "title": "General Regression Neural Networks Package",
    "description": "This General Regression Neural Networks Package uses various distance functions.\n     It was motivated by Specht (1991, ISBN:1045-9227), and updated from previous published paper \n     Li et al. (2016) <doi:10.1016/j.palaeo.2015.11.005>. This package includes various functions, \n     although \"euclidean\" distance is used traditionally.",
    "version": "0.1.0",
    "maintainer": "Shufeng LI <lisf@xtbg.org.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2787,
    "package_name": "GTbasedIM",
    "title": "Game Theory-Based Influence Measures",
    "description": "Understanding how features influence a specific response variable becomes crucial in classification problems, with applications ranging from medical diagnosis to customer behavior analysis. \n\t\t\t This packages provides tools to compute such an influence measure grounded on game theory concepts.\n\t\t\t In particular, the influence measures presented in Davila-Pena, Saavedra-Nieves, and Casas-Méndez (2024) <doi:10.48550/arXiv.2408.02481> can be obtained. ",
    "version": "1.0.0",
    "maintainer": "Laura Davila-Pena <lauradavila.pena@usc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2789,
    "package_name": "GUEST",
    "title": "Graphical Models in Ultrahigh-Dimensional and Error-Prone Data\nvia Boosting Algorithm",
    "description": "We consider the ultrahigh-dimensional and error-prone data. Our goal aims to estimate the precision matrix and identify the graphical structure of the random variables with measurement error corrected. We further adopt the estimated precision matrix to the linear discriminant function to do classification for multi-label classes.",
    "version": "0.2.0",
    "maintainer": "Hui-Shan Tsao <n410412@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2863,
    "package_name": "GeneralizedUmatrix",
    "title": "Credible Visualization for Two-Dimensional Projections of Data",
    "description": "Projections are common dimensionality reduction methods, which represent high-dimensional data in a two-dimensional space. However, when restricting the output space to two dimensions, which results in a two dimensional scatter plot (projection) of the data, low dimensional similarities do not represent high dimensional distances coercively [Thrun, 2018] <DOI: 10.1007/978-3-658-20540-9>. This could lead to a misleading interpretation of the underlying structures [Thrun, 2018]. By means of the 3D topographic map the generalized Umatrix is able to depict errors of these two-dimensional scatter plots. The package is derived from the book of Thrun, M.C.: \"Projection Based Clustering through Self-Organization and Swarm Intelligence\" (2018) <DOI:10.1007/978-3-658-20540-9> and the main algorithm called simplified self-organizing map for dimensionality reduction methods is published in <DOI: 10.1016/j.mex.2020.101093>.",
    "version": "1.3.1",
    "maintainer": "Michael Thrun <m.thrun@gmx.net>",
    "url": "https://www.deepbionics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2864,
    "package_name": "GeneralizedUmatrixGPU",
    "title": "Credible Visualization for Two-Dimensional Projections of Data",
    "description": "Projections are common dimensionality reduction methods, which represent high-dimensional data in a two-dimensional space. However, when restricting the output space to two dimensions, which results in a two dimensional scatter plot (projection) of the data, low dimensional similarities do not represent high dimensional distances coercively [Thrun, 2018] <DOI: 10.1007/978-3-658-20540-9>. This could lead to a misleading interpretation of the underlying structures [Thrun, 2018]. By means of the 3D topographic map the generalized Umatrix is able to depict errors of these two-dimensional scatter plots. The package is derived from the book of Thrun, M.C.: \"Projection Based Clustering through Self-Organization and Swarm Intelligence\" (2018) <DOI:10.1007/978-3-658-20540-9> and the main algorithm called simplified self-organizing map for dimensionality reduction methods is published in Thrun, M.C. and Ultsch, A.: \"Uncovering High-dimensional Structures of Projections from Dimensionality Reduction Methods\" (2020) <DOI:10.1016/j.mex.2020.101093>.",
    "version": "0.1.8",
    "maintainer": "Quirin Stier <Quirin_Stier@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2866,
    "package_name": "GenericML",
    "title": "Generic Machine Learning Inference",
    "description": "Generic Machine Learning Inference on heterogeneous treatment effects in randomized experiments as proposed in Chernozhukov, Demirer, Duflo and Fernández-Val (2020) <arXiv:1712.04802>. This package's workhorse is the 'mlr3' framework of Lang et al. (2019) <doi:10.21105/joss.01903>, which enables the specification of a wide variety of machine learners. The main functionality, GenericML(), runs Algorithm 1 in Chernozhukov, Demirer, Duflo and Fernández-Val (2020) <arXiv:1712.04802> for a suite of user-specified machine learners. All steps in the algorithm are customizable via setup functions. Methods for printing and plotting are available for objects returned by GenericML(). Parallel computing is supported.",
    "version": "0.2.2",
    "maintainer": "Max Welz <welz@ese.eur.nl>",
    "url": "https://github.com/mwelz/GenericML/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2871,
    "package_name": "GenoTriplo",
    "title": "Genotyping Triploids (or Diploids) from Luminescence Data",
    "description": "Genotyping of triploid individuals from luminescence data (marker probeset A and B). Works also for diploids.\n\tTwo main functions: Run_Clustering() that regroups individuals with a same genotype based on proximity and\n\tRun_Genotyping() that assigns a genotype to each cluster. For Shiny interface use: launch_GenoShiny().",
    "version": "1.1.3",
    "maintainer": "Julien Roche <julien.roche@inrae.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2936,
    "package_name": "Gmedian",
    "title": "Geometric Median, k-Medians Clustering and Robust Median PCA",
    "description": "Fast algorithms for robust estimation with large samples of multivariate observations. Estimation of the geometric median, robust k-Gmedian clustering, and robust PCA based on the Gmedian covariation matrix.",
    "version": "1.2.7",
    "maintainer": "Herve Cardot <herve.cardot@u-bourgogne.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2950,
    "package_name": "GoogleImage2Array",
    "title": "Create Array Data from 2D Image Thumbnails via Google Image\nSearch",
    "description": "Images are provided as an array dataset of 2D image thumbnails from Google Image Search <https://www.google.com/search>.\n  This array data may be suitable for a training data of machine learning or deep learning as a first trial.",
    "version": "0.99.2",
    "maintainer": "Satoshi Kume <satoshi.kume.1984@gmail.com>",
    "url": "https://github.com/kumeS/GoogleImage2Array",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2980,
    "package_name": "GrpString",
    "title": "Patterns and Statistical Differences Between Two Groups of\nStrings",
    "description": "Methods include converting series of event names to strings, finding common patterns\n    in a group of strings, discovering featured patterns when comparing two groups of strings as well\n    as the number and starting position of each pattern in each string, obtaining transition matrix, \n    computing transition entropy, statistically comparing the difference between two groups of strings,\n    and clustering string groups. Event names can be any action names or labels such as events in log\n    files or areas of interest (AOIs) in eye tracking research.",
    "version": "0.3.2",
    "maintainer": "Hui (Tom) Tang <htang2013@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2982,
    "package_name": "GsymPoint",
    "title": "Estimation of the Generalized Symmetry Point, an Optimal\nCutpoint in Continuous Diagnostic Tests",
    "description": "Estimation of the cutpoint defined by the Generalized Symmetry point in a binary classification setting based on a continuous diagnostic test or marker. Two methods have been implemented to construct confidence intervals for this optimal cutpoint, one based on the Generalized Pivotal Quantity and the other based on Empirical Likelihood. Numerical and graphical outputs for these two methods are easily obtained.",
    "version": "1.1.2",
    "maintainer": "Mónica López-Ratón <monica.lopez.raton@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3011,
    "package_name": "HDLSSkST",
    "title": "Distribution-Free Exact High Dimensional Low Sample Size\nk-Sample Tests",
    "description": "Testing homogeneity of k multivariate distributions is a classical and challenging problem in\n             statistics, and this becomes even more challenging when the dimension of the data exceeds the sample size.\n             We construct some tests for this purpose which are exact level (size) alpha tests based on clustering. \n             These tests are easy to implement and distribution-free in finite sample situations. Under appropriate \n             regularity conditions, these tests have the consistency property in HDLSS asymptotic regime, where the \n             dimension of data grows to infinity while the sample size remains fixed. We also consider a multiscale \n             approach, where the results for different number of partitions are aggregated judiciously. Details are in \n             Biplab Paul, Shyamal K De and Anil K Ghosh (2021) <doi:10.1016/j.jmva.2021.104897>; Soham Sarkar and Anil K Ghosh (2019) \n             <doi:10.1109/TPAMI.2019.2912599>; William M Rand (1971) <doi:10.1080/01621459.1971.10482356>;  \n             Cyrus R Mehta and Nitin R Patel (1983) <doi:10.2307/2288652>; Joseph C Dunn (1973) \n             <doi:10.1080/01969727308546046>; Sture Holm (1979) <doi:10.2307/4615733>; \n             Yoav Benjamini and Yosef Hochberg (1995) <doi: 10.2307/2346101>.",
    "version": "2.1.0",
    "maintainer": "Biplab Paul <paul.biplab497@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3025,
    "package_name": "HDclassif",
    "title": "High Dimensional Supervised Classification and Clustering",
    "description": "Discriminant analysis and data clustering methods for high\n    dimensional data, based on the assumption that high-dimensional data live in\n    different subspaces with low dimensionality proposing a new parametrization of\n    the Gaussian mixture model which combines the ideas of dimension reduction and\n    constraints on the model.",
    "version": "2.2.2",
    "maintainer": "Laurent Berge <laurent.berge@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3026,
    "package_name": "HDclust",
    "title": "Clustering High Dimensional Data with Hidden Markov Model on\nVariable Blocks",
    "description": "Clustering of high dimensional data with Hidden Markov Model on Variable Blocks (HMM-VB) fitted via Baum-Welch algorithm. Clustering is performed by the Modal Baum-Welch algorithm (MBW), which finds modes of the density function. Lin Lin and Jia Li (2017) <https://jmlr.org/papers/v18/16-342.html>.",
    "version": "1.0.4",
    "maintainer": "Jia Li <jiali@psu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3029,
    "package_name": "HDoutliers",
    "title": "Leland Wilkinson's Algorithm for Detecting Multidimensional\nOutliers",
    "description": "An implementation of an algorithm for outlier detection that can handle a) data with a mixed categorical and continuous variables, b) many columns of data, c) many rows of data, d) outliers that mask other outliers, and e) both unidimensional and multidimensional datasets. Unlike ad hoc methods found in many machine learning papers, HDoutliers is based on a distributional model that uses probabilities to determine outliers.",
    "version": "1.0.4",
    "maintainer": "Chris Fraley  <fraley@u.washington.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3055,
    "package_name": "HMDA",
    "title": "Holistic Multimodel Domain Analysis for Exploratory Machine\nLearning",
    "description": "Holistic Multimodel Domain Analysis (HMDA) is a robust and transparent framework designed for exploratory machine learning research, aiming to enhance the process of feature assessment and selection. HMDA addresses key limitations of traditional machine learning methods by evaluating the consistency across multiple high-performing models within a fine-tuned modeling grid, thereby improving the interpretability and reliability of feature importance assessments. Specifically, it computes Weighted Mean SHapley Additive exPlanations (WMSHAP), which aggregate feature contributions from multiple models based on weighted performance metrics. HMDA also provides confidence intervals to demonstrate the stability of these feature importance estimates. This framework is particularly beneficial for analyzing complex, multidimensional datasets common in health research, supporting reliable exploration of mental health outcomes such as suicidal ideation, suicide attempts, and other psychological conditions. Additionally, HMDA includes automated procedures for feature selection based on WMSHAP ratios and performs dimension reduction analyses to identify underlying structures among features. For more details see Haghish (2025) <doi:10.13140/RG.2.2.32473.63846>.",
    "version": "0.1.1",
    "maintainer": "E. F. Haghish <haghish@hotmail.com>",
    "url": "http://dx.doi.org/10.13140/RG.2.2.32473.63846,\nhttps://github.com/haghish/HMDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3117,
    "package_name": "Harvest.Tree",
    "title": "Harvest the Classification Tree",
    "description": "Aimed at applying the Harvest classification tree algorithm, modified algorithm of classic classification tree.The harvested tree has advantage of deleting redundant rules in trees, leading to a simplify and more efficient tree model.It was firstly used in drug discovery field, but it also performs well in other kinds of data, especially when the region of a class is disconnected. This package also improves the basic harvest classification tree algorithm by extending the field of data of algorithm to both continuous and categorical variables. To learn more about the harvest classification tree algorithm, you can go to http://www.stat.ubc.ca/Research/TechReports/techreports/220.pdf for more information. ",
    "version": "1.1",
    "maintainer": "Bingyuan Liu <adler1016@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3152,
    "package_name": "HierPortfolios",
    "title": "Hierarchical Risk Clustering Portfolio Allocation Strategies",
    "description": "Machine learning hierarchical risk clustering portfolio allocation strategies. \n The implemented methods are:\n  Hierarchical risk parity (De Prado, 2016) <DOI: 10.3905/jpm.2016.42.4.059>.\n  Hierarchical clustering-based asset allocation (Raffinot, 2017)  \n  <DOI: 10.3905/jpm.2018.44.2.089>.\n  Hierarchical equal risk contribution portfolio (Raffinot, 2018)\n  <DOI: 10.2139/ssrn.3237540>.\n  A Constrained Hierarchical Risk Parity Algorithm with Cluster-based Capital Allocation (Pfitzingera and Katzke, 2019)\n  <https://www.ekon.sun.ac.za/wpapers/2019/wp142019/wp142019.pdf>.",
    "version": "1.0.2",
    "maintainer": "Carlos Trucios <ctrucios@unicamp.br>",
    "url": "https://github.com/ctruciosm/HierPortfolios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3158,
    "package_name": "HistDAWass",
    "title": "Histogram-Valued Data Analysis",
    "description": "In the framework of Symbolic Data Analysis, a relatively new\n    approach to the statistical analysis of multi-valued data, we consider\n    histogram-valued data, i.e., data described by univariate histograms. The\n    methods and the basic statistics for histogram-valued data are mainly based\n    on the L2 Wasserstein metric between distributions, i.e., the Euclidean metric\n    between quantile functions. The package contains unsupervised classification\n    techniques, least square regression and tools for histogram-valued data and for\n    histogram time series. An introducing paper is Irpino A. Verde R. (2015) <doi: 10.1007/s11634-014-0176-4>.",
    "version": "1.0.8",
    "maintainer": "Antonio Irpino <antonio.irpino@unicampania.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3191,
    "package_name": "IADT",
    "title": "Interaction Difference Test for Prediction Models",
    "description": "Provides functions to conduct a model-agnostic asymptotic hypothesis test for the identification of interaction effects in black-box machine learning models. The null hypothesis assumes that a given set of covariates does not contribute to interaction effects in the prediction model. The test statistic is based on the difference of variances of partial dependence functions (Friedman (2008) <doi:10.1214/07-AOAS148> and Welchowski (2022) <doi:10.1007/s13253-021-00479-7>) with respect to the original black-box predictions and the predictions under the null hypothesis. The hypothesis test can be applied to any black-box prediction model, and the null hypothesis of the test can be flexibly specified according to the research question of interest. Furthermore, the test is computationally fast to apply as the null distribution does not require resampling or refitting black-box prediction models.",
    "version": "1.2.1",
    "maintainer": "Thomas Welchowski <welchow@imbie.meb.uni-bonn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3204,
    "package_name": "IBLM",
    "title": "Interpretable Boosted Linear Models",
    "description": "Implements Interpretable Boosted Linear Models (IBLMs). These combine a conventional generalized linear model (GLM) with a machine learning component, such as XGBoost. The package also provides tools within for explaining and analyzing these models. For more details see Gawlowski and Wang (2025) <https://ifoa-adswp.github.io/IBLM/reference/figures/iblm_paper.pdf>.",
    "version": "1.0.2",
    "maintainer": "Karol Gawlowski <Karol.Gawlowski@citystgeorges.ac.uk>",
    "url": "https://ifoa-adswp.github.io/IBLM/,\nhttps://github.com/IFoA-ADSWP/IBLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3218,
    "package_name": "ICEbox",
    "title": "Individual Conditional Expectation Plot Toolbox",
    "description": "Implements Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. ICE plots refine Friedman's partial dependence plot by graphing the functional relationship between the predicted response and a covariate of interest for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate of interest, suggesting where and to what extent they may exist.",
    "version": "1.1.5",
    "maintainer": "Adam Kapelner <kapelner@qc.cuny.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3276,
    "package_name": "ILRCM",
    "title": "Convert Irregular Longitudinal Data to Regular Intervals and\nPerform Clustering",
    "description": "Convert irregularly spaced longitudinal data into regular intervals for further analysis, \n             and perform clustering using advanced machine learning techniques. \n             The package is designed for handling complex longitudinal datasets, \n             optimizing them for research in healthcare, demography, and other fields \n             requiring temporal data modeling.",
    "version": "0.2.0",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3284,
    "package_name": "IMIFA",
    "title": "Infinite Mixtures of Infinite Factor Analysers and Related\nModels",
    "description": "Provides flexible Bayesian estimation of Infinite Mixtures of Infinite Factor Analysers and related models, for nonparametrically clustering high-dimensional data, introduced by Murphy et al. (2020) <doi:10.1214/19-BA1179>. The IMIFA model conducts Bayesian nonparametric model-based clustering with factor analytic covariance structures without recourse to model selection criteria to choose the number of clusters or cluster-specific latent factors, mostly via efficient Gibbs updates. Model-specific diagnostic tools are also provided, as well as many options for plotting results, conducting posterior inference on parameters of interest, posterior predictive checking, and quantifying uncertainty.",
    "version": "2.2.0",
    "maintainer": "Keefe Murphy <keefe.murphy@mu.ie>",
    "url": "https://cran.r-project.org/package=IMIFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3287,
    "package_name": "IMP",
    "title": "Interactive Model Performance Evaluation",
    "description": "Contains functions for evaluating & comparing the performance of Binary classification models. Functions can be called either statically or interactively (as Shiny Apps).",
    "version": "1.1",
    "maintainer": "Anup Nair <nairanup50695@gmail.com>",
    "url": "https://github.com/anup50695/IMPPackage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3295,
    "package_name": "INFOSET",
    "title": "Computing a New Informative Distribution Set of Asset Returns",
    "description": "Estimation of the most-left informative set of gross returns \n             (i.e., the informative set).\n             The procedure to compute the informative set adjusts the method \n             proposed by \n             Mariani et al. (2022a) <doi:10.1007/s11205-020-02440-6> \n             and \n             Mariani et al. (2022b) <doi:10.1007/s10287-022-00422-2> \n             to gross returns of financial assets. \n             This is accomplished through an adaptive algorithm\n             that identifies sub-groups of gross returns in \n             each iteration by approximating their distribution with a\n             sequence of two-component log-normal mixtures. \n             These sub-groups emerge when a significant change\n             in the distribution occurs below the median of the \n             financial returns, with their boundary termed as\n             the “change point\" of the mixture. \n             The process concludes when no further change points are detected.\n             The outcome encompasses parameters of the leftmost mixture \n             distributions and change points of the\n             analyzed financial time series.\n             The functionalities of the INFOSET package include: (i) modelling asset distribution\n             detecting the parameters which describe left tail behaviour (infoset function), (ii) clustering, (iii) labeling of the financial\n             series for predictive and classification purposes through a Left Risk measure based on the first change point (LR_cp function)\n             (iv) portfolio construction (ptf_construction function).\n             The package also provide a specific function to construct rolling windows of different length size and overlapping time.",
    "version": "4.1",
    "maintainer": "Gloria Polinesi <g.polinesi@staff.univpm.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3303,
    "package_name": "INSPECTumours",
    "title": "IN-vivo reSPonsE Classification of Tumours",
    "description": "This is a shiny app used for the statistical classifying and analysing\n    pre-clinical tumour responses.",
    "version": "0.1.0",
    "maintainer": "Bairu Zhang <bairu.zhang@astrazeneca.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3317,
    "package_name": "IPMRF",
    "title": "Intervention in Prediction Measure for Random Forests",
    "description": "Computes intervention in prediction measure for assessing variable importance for random forests. See details at I. Epifanio (2017) <DOI:10.1186/s12859-017-1650-8>. ",
    "version": "1.3",
    "maintainer": "Irene Epifanio <epifanio@uji.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3326,
    "package_name": "IRR2FPR",
    "title": "Computing False Positive Rate from Inter-Rater Reliability",
    "description": "Implements a 'Shiny Item Analysis' module and functions for computing false positive rate \n  and other binary classification metrics from inter-rater reliability based on Bartoš & Martinková (2024) \n   <doi:10.1111/bmsp.12343>.",
    "version": "0.1.1",
    "maintainer": "František Bartoš <f.bartos96@gmail.com>",
    "url": "https://github.com/FBartos/IRR2FPR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3353,
    "package_name": "ISS",
    "title": "Isotonic Subgroup Selection",
    "description": "Methodology for subgroup selection in the context of isotonic regression including methods for sub-Gaussian errors, classification, homoscedastic Gaussian errors and quantile regression. See the documentation of ISS(). Details can be found in the paper by Müller, Reeve, Cannings and Samworth (2023) <arXiv:2305.04852v2>.",
    "version": "1.0.0",
    "maintainer": "Manuel M. Müller <mm2559@cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3360,
    "package_name": "IVDML",
    "title": "Double Machine Learning with Instrumental Variables and\nHeterogeneity",
    "description": "Instrumental variable (IV) estimators for homogeneous and\n    heterogeneous treatment effects with efficient machine learning instruments.\n    The estimators are based on double/debiased machine learning allowing for\n    nonlinear and potentially high-dimensional control variables. Details can \n    be found in Scheidegger, Guo and Bühlmann (2025) \"Inference for \n    heterogeneous treatment effects with efficient instruments and machine \n    learning\" <doi:10.48550/arXiv.2503.03530>.",
    "version": "1.0.1",
    "maintainer": "Cyrill Scheidegger <cyrill.scheidegger@stat.math.ethz.ch>",
    "url": "https://github.com/cyrillsch/IVDML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3374,
    "package_name": "ImML",
    "title": "Machine Learning Algorithms Fitting and Validation for Forestry",
    "description": "Fitting and validation of machine learning algorithms\n             for volume prediction of trees, currently for conifer trees based on\n\t\t\t diameter at breast height and height as explanatory variables.",
    "version": "0.1.5",
    "maintainer": "Salvatore Mangiafico <mangiafico@njaes.rutgers.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3375,
    "package_name": "ImNN",
    "title": "Neural Networks for Predicting Volume of Forest Trees",
    "description": "Neural network has potential in forestry modelling. This package is designed to create and assess Artificial Intelligence based Neural Networks with varying architectures for prediction of volume of forest trees using two input features: height and diameter at breast height, as they are the key factors in predicting volume, therefore development and validation of efficient volume prediction neural network model is necessary. This package has been developed using the algorithm of Tabassum et al. (2022) <doi:10.18805/ag.D-5555>.",
    "version": "0.1.0",
    "maintainer": "M. Iqbal Jeelani <jeelani.miqbal@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3378,
    "package_name": "Imneuron",
    "title": "AI Powered Neural Network Solutions for Regression Tasks",
    "description": "It offers a sophisticated and versatile tool for creating and evaluating artificial intelligence based neural network models tailored for regression analysis on datasets with continuous target variables. Leveraging the power of neural networks, it allows users to experiment with various hidden neuron configurations across two layers, optimizing model performance through \"5 fold\"\" or \"10 fold\"\" cross validation. The package normalizes input data to ensure efficient training and assesses model accuracy using key metrics such as R squared (R2), Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Percentage Error (PER). By storing and visualizing the best performing models, it provides a comprehensive solution for precise and efficient regression modeling making it an invaluable tool for data scientists and researchers aiming to harness AI for predictive analytics.",
    "version": "0.1.0",
    "maintainer": "M Iqbal Jeelani <jeelani.miqbal@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3407,
    "package_name": "InfoTrad",
    "title": "Calculates the Probability of Informed Trading (PIN)",
    "description": "Estimates the probability of informed trading (PIN) initially introduced by Easley et. al. (1996) <doi:10.1111/j.1540-6261.1996.tb04074.x> . Contribution of the package is that it uses likelihood factorizations of Easley et. al. (2010) <doi:10.1017/S0022109010000074> (EHO factorization) and Lin and Ke (2011) <doi:10.1016/j.finmar.2011.03.001> (LK factorization). Moreover, the package uses different estimation algorithms. Specifically, the grid-search algorithm proposed by Yan and Zhang (2012) <doi:10.1016/j.jbankfin.2011.08.003> , hierarchical agglomerative clustering approach proposed by Gan et. al. (2015) <doi:10.1080/14697688.2015.1023336> and later extended by Ersan and Alici (2016) <doi:10.1016/j.intfin.2016.04.001> .",
    "version": "1.2",
    "maintainer": "Murat Tinic <tinic@bilkent.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3408,
    "package_name": "Information",
    "title": "Data Exploration with Information Theory (Weight-of-Evidence and\nInformation Value)",
    "description": "Performs exploratory data analysis and variable screening for\n    binary classification models using weight-of-evidence (WOE) and information\n    value (IV). In order to make the package as efficient as possible, aggregations\n    are done in data.table and creation of WOE vectors can be distributed across\n    multiple cores. The package also supports exploration for uplift models (NWOE\n    and NIV).",
    "version": "0.0.9",
    "maintainer": "Larsen Kim <kblarsen4@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3419,
    "package_name": "IntNMF",
    "title": "Integrative Clustering of Multiple Genomic Dataset",
    "description": "Carries out integrative clustering analysis using multiple types of genomic dataset using integrative Non-negative Matrix factorization. ",
    "version": "1.3.0",
    "maintainer": "Prabhakar Chalise <pchalise@kumc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3464,
    "package_name": "JANE",
    "title": "Just Another Latent Space Network Clustering Algorithm",
    "description": "Fit latent space network cluster models using an expectation-maximization algorithm. Enables flexible modeling of unweighted or weighted network data (with or without noise edges), supporting both directed and undirected networks (with or without degree and strength heterogeneity). Designed to handle large networks efficiently, it allows users to explore network structure through latent space representations, identify clusters (i.e., community detection) within network data, and simulate networks with varying clustering, connectivity patterns, and noise edges. Methodology for the implementation is described in Arakkal and Sewell (2025) <doi:10.1016/j.csda.2025.108228>. ",
    "version": "2.1.0",
    "maintainer": "Alan Arakkal <alan-arakkal@uiowa.edu>",
    "url": "https://a1arakkal.github.io/JANE/,\nhttps://github.com/a1arakkal/JANE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3482,
    "package_name": "JOUSBoost",
    "title": "Implements Under/Oversampling for Probability Estimation",
    "description": "Implements under/oversampling for probability estimation.  To be\n    used with machine learning methods such as AdaBoost, random forests, etc.",
    "version": "2.1.0",
    "maintainer": "Matthew Olson <maolson@wharton.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3512,
    "package_name": "KAML",
    "title": "Kinship Adjusted Multiple Loci Best Linear Unbiased Prediction",
    "description": "Kinship Adjusted Multiple Loci Best Linear Unbiased Prediction",
    "version": "1.5.1",
    "maintainer": "Lilin Yin <ylilin@163.com>",
    "url": "https://github.com/YinLiLin/KAML",
    "exports": [],
    "topics": ["genomic-prediction", "genomic-selection", "machine-learning"],
    "score": "NA",
    "stars": 46
  },
  {
    "id": 3532,
    "package_name": "KMEANS.KNN",
    "title": "KMeans and KNN Clustering Package",
    "description": "Implementation of Kmeans clustering algorithm and a supervised KNN (K Nearest Neighbors) learning method. It allows users to perform unsupervised clustering and supervised classification on their datasets. Additional features include data normalization, imputation of missing values, and the choice of distance metric. The package also provides functions to determine the optimal number of clusters for Kmeans and the best k-value for KNN: knn_Function(), find_Knn_best_k(), KMEANS_FUNCTION(), and find_Kmeans_best_k().",
    "version": "0.1.0",
    "maintainer": "LALLOGO Lassané <lassanelallogo2002@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3543,
    "package_name": "KQM",
    "title": "K Quantiles Medoids (KQM) Clustering",
    "description": "K Quantiles Medoids (KQM) clustering applies quantiles to divide data of each dimension into K mean intervals. Combining quantiles of all the dimensions of the data and fully permuting quantiles on each dimension is the strategy to determine a pool of candidate initial cluster centers. To find the best initial cluster centers from the pool of candidate initial cluster centers, two methods based on quantile strategy and PAM strategy respectively are proposed. During a clustering process, medoids of clusters are used to update cluster centers in each iteration. Comparison between KQM and the method of randomly selecting initial cluster centers shows that KQM is almost always getting clustering results with smaller total sum squares of distances.",
    "version": "1.1.1",
    "maintainer": "Yarong Yang <Yi.YA_yaya@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3544,
    "package_name": "KRIS",
    "title": "Keen and Reliable Interface Subroutines for Bioinformatic\nAnalysis",
    "description": "Provides useful functions which are needed for bioinformatic analysis such as calculating linear principal components from numeric data and Single-nucleotide polymorphism (SNP) dataset, calculating fixation index (Fst) using Hudson method, creating scatter plots in 3 views, handling with PLINK binary file format, detecting rough structures and outliers using unsupervised clustering, and calculating matrix multiplication in the faster way for big data.",
    "version": "1.1.6",
    "maintainer": "Kridsadakorn Chaichoompu <kridsadakorn@biostatgen.org>",
    "url": "https://gitlab.com/kris.ccp/kris",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3545,
    "package_name": "KRLS",
    "title": "Kernel-Based Regularized Least Squares",
    "description": "Package implements Kernel-based Regularized Least Squares (KRLS), a machine learning method to fit multidimensional functions y=f(x) for regression and classification problems without relying on linearity or additivity assumptions. KRLS finds the best fitting function by minimizing the squared loss of a Tikhonov regularization problem, using Gaussian kernels as radial basis functions. For further details see Hainmueller and Hazlett (2014).",
    "version": "1.0-0",
    "maintainer": "Jens Hainmueller <jhain@stanford.edu>",
    "url": "https://www.r-project.org, https://www.stanford.edu/~jhain/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3550,
    "package_name": "KSIC",
    "title": "Korea Standard Industrial Classification (KSIC)",
    "description": "Provides tools for working with the Korea Standard Industrial Classification (KSIC). Includes datasets for the 9th, 10th, and 11th revisions. Functions include searching codes and names by keyword, converting codes across revisions, validating KSIC codes, and navigating the classification hierarchy (e.g., identifying parent or child categories). Intended for use in statistical analysis, data processing, and research involving South Korea’s industrial classification system.",
    "version": "1.0.2",
    "maintainer": "Jongjin Yun <jongjin@uos.ac.kr>",
    "url": "https://github.com/urbanjj/KSIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3560,
    "package_name": "Kcop",
    "title": "Smooth Test for Equality of Copulas and Clustering Multivariate",
    "description": "Implements approaches of non-parametric smooth test to \n            compare simultaneously K(K>1) copulas and non-parametric clustering \n            of multivariate populations with arbitrary sizes. \n            See Yves I. Ngounou Bakam and Denys Pommeret (2022) <arXiv:2112.05623> and\n            Yves I. Ngounou Bakam and Denys Pommeret (2022) <arXiv:2211.06338>.",
    "version": "1.0.0",
    "maintainer": "Yves Ismael Ngounou Bakam <yvesngounou20@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3574,
    "package_name": "Kira",
    "title": "Machine Learning",
    "description": "Machine learning, containing several algorithms for supervised and unsupervised classification, in addition to a function that plots the Receiver Operating Characteristic (ROC) and Precision-Recall (PRC) curve graphs, and also a function that returns several metrics used for model evaluation, the latter can be used in ranking results from other packs.",
    "version": "1.0.7",
    "maintainer": "Paulo Cesar Ossani <ossanipc@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3575,
    "package_name": "Kmedians",
    "title": "K-Medians",
    "description": "Online, Semi-online, and Offline K-medians algorithms are\n    given. For both methods, the algorithms can be initialized\n    randomly or with the help of a robust hierarchical\n    clustering. The number of clusters can be selected with the\n    help of a penalized criterion. We provide functions to provide\n    robust clustering. Function gen_K() enables to generate a sample\n    of data following a contaminated Gaussian mixture.\n    Functions Kmedians() and Kmeans() consists in a K-median and a\n    K-means algorithms while Kplot() enables to produce graph for both\n    methods. \n    Cardot, H., Cenac, P. and Zitt, P-A. (2013). \"Efficient and fast estimation of the geometric median in Hilbert spaces with an averaged stochastic gradient algorithm\". Bernoulli, 19, 18-43. <doi:10.3150/11-BEJ390>.\n    Cardot, H. and Godichon-Baggioni, A. (2017). \"Fast Estimation of the Median Covariation Matrix with Application to Online Robust Principal Components Analysis\". Test, 26(3), 461-480 <doi:10.1007/s11749-016-0519-x>.\n    Godichon-Baggioni, A. and Surendran, S. \"A penalized criterion for selecting the number of clusters for K-medians\"     <arXiv:2209.03597> \n    Vardi, Y. and Zhang, C.-H. (2000). \"The multivariate L1-median and associated data depth\". Proc. Natl. Acad. Sci. USA, 97(4):1423-1426. <doi:10.1073/pnas.97.4.1423>.",
    "version": "2.2.0",
    "maintainer": "Antoine Godichon-Baggioni <antoine.godichon_baggioni@upmc.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3606,
    "package_name": "LBBNN",
    "title": "Latent Binary Bayesian Neural Networks Using 'torch'",
    "description": "Latent binary Bayesian neural networks (LBBNNs) are implemented using \n    'torch', an R interface to the LibTorch backend. Supports mean-field variational \n    inference as well as flexible variational posteriors using normalizing flows. \n    The standard LBBNN implementation follows Hubin and Storvik (2024) <doi:10.3390/math12060788>, \n    using the local reparametrization trick as in Skaaret-Lund et al. (2024) \n    <https://openreview.net/pdf?id=d6kqUKzG3V>. Input-skip connections are also supported, \n    as described in Høyheim et al. (2025) <doi:10.48550/arXiv.2503.10496>.",
    "version": "0.1.2",
    "maintainer": "Lars Skaaret-Lund <lars.skaaret-lund@nmbu.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3615,
    "package_name": "LCCkNN",
    "title": "Adaptive k-Nearest Neighbor Classifier Based on Local Curvature\nEstimation",
    "description": "Implements the kK-NN algorithm, an adaptive k-nearest neighbor classifier that adjusts the neighborhood size based on local data curvature. The method estimates local Gaussian curvature by approximating the shape operator of the data manifold. This approach aims to improve classification performance, particularly in datasets with limited samples.",
    "version": "0.1.0",
    "maintainer": "Gabriel Pereira <gabrielfreitaspereira10@gmail.com>",
    "url": "https://github.com/Gabrielforest/LCCkNN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3621,
    "package_name": "LDATree",
    "title": "Oblique Classification Trees with Uncorrelated Linear\nDiscriminant Analysis Splits",
    "description": "A classification tree method that uses Uncorrelated Linear Discriminant Analysis (ULDA) for variable selection, split determination, and model fitting in terminal nodes.  It automatically handles missing values and offers visualization tools. For more details, see Wang (2024) <doi:10.48550/arXiv.2410.23147>.",
    "version": "0.2.0",
    "maintainer": "Siyu Wang <iamwangsiyu@gmail.com>",
    "url": "https://github.com/Moran79/LDATree,\nhttp://iamwangsiyu.com/LDATree/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3626,
    "package_name": "LDNN",
    "title": "Longitudinal Data Neural Network",
    "description": "This is a Neural Network regression model implementation using 'Keras', consisting of 10 Long Short-Term Memory layers that are fully connected along with the rest of the inputs.",
    "version": "1.10",
    "maintainer": "Vasileios Karapoulios <billkarap123@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3647,
    "package_name": "LLM",
    "title": "Logit Leaf Model Classifier for Binary Classification",
    "description": "Fits the Logit Leaf Model, makes predictions and visualizes the output. (De Caigny et al., (2018) <DOI:10.1016/j.ejor.2018.02.009>).",
    "version": "1.1.0",
    "maintainer": "Arno De Caigny <a.de-caigny@ieseg.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3673,
    "package_name": "LPCM",
    "title": "Local Principal Curve Methods",
    "description": "Fitting multivariate data patterns with local principal curves, including tools for data compression (projection) and measuring goodness-of-fit; with some additional functions for mean shift clustering.  See Einbeck, Tutz and Evers (2005) <doi:10.1007/s11222-005-4073-8> and Ameijeiras-Alonso and Einbeck (2023) <doi:10.1007/s11634-023-00575-1>.",
    "version": "0.47-6",
    "maintainer": "Jochen Einbeck <jochen.einbeck@durham.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3677,
    "package_name": "LPKsample",
    "title": "LP Nonparametric High Dimensional K-Sample Comparison",
    "description": "LP nonparametric high-dimensional K-sample comparison method that includes \n    (i) confirmatory test, (ii) exploratory analysis, and (iii) options to output a \n\tdata-driven LP-transformed matrix for classification. The primary reference is \n\tMukhopadhyay, S. and Wang, K. (2020, Biometrika); <arXiv:1810.01724>.",
    "version": "2.1",
    "maintainer": "Kaijun Wang <kaijunwang.19@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3719,
    "package_name": "LUCIDus",
    "title": "LUCID with Multiple Omics Data",
    "description": "An implementation of estimating the Latent Unknown Clusters By Integrating Multi-omics Data (LUCID) model (Peng (2019) <doi:10.1093/bioinformatics/btz667>). LUCID conducts integrated clustering using exposures, omics information (and outcome information as an option). This package implements three different integration strategies for multi-omics data analysis within the LUCID framework: LUCID early integration (the original LUCID model), LUCID in parallel (intermediate integration), and LUCID in serial (late integration). Automated model selection for each LUCID model is available to obtain the optimal number of latent clusters, and an integrated imputation approach is implemented to handle sporadic and list-wise missingness in multi-omics data. Lasso-type regularity for exposure and omics features were added. S3 methods for summary and plotting functions were fixed. Fixed minor bugs.",
    "version": "3.0.3",
    "maintainer": "Qiran Jia <qiranjia@usc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3746,
    "package_name": "Laurae",
    "title": "Advanced High Performance Data Science Toolbox for R",
    "description": "This package is for advanced statistics, visualization, and",
    "version": "0.0.0.9001",
    "maintainer": "",
    "url": "https://github.com/Laurae2/Laurae",
    "exports": [],
    "topics": ["data-science", "laurae", "machine-learning", "r", "supervised-learning", "xgboost"],
    "score": "NA",
    "stars": 205
  },
  {
    "id": 3753,
    "package_name": "LearnClust",
    "title": "Learning Hierarchical Clustering Algorithms",
    "description": "Classical hierarchical clustering algorithms, agglomerative and divisive clustering. Algorithms are implemented as a theoretical way, step by step.\n  It includes some detailed functions that explain each step. Every function allows options to get different results using different techniques. \n  The package explains non expert users how hierarchical clustering algorithms work.",
    "version": "1.1",
    "maintainer": "Roberto Alcantara <roberto.alcantara@edu.uah.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3757,
    "package_name": "LearnSL",
    "title": "Learn Supervised Classification Methods Through Examples and\nCode",
    "description": "Supervised classification methods, which (if asked) can provide\n    step-by-step explanations of the algorithms used, as described in\n    PK Josephine et. al., (2021) <doi:10.59176/kjcs.v1i1.1259>; and datasets to\n    test them on, which highlight the strengths and weaknesses of each technique.",
    "version": "1.0.0",
    "maintainer": "Víctor Amador Padilla <victor.amador@edu.uah.es>",
    "url": "https://github.com/ComiSeng/LearnSL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3765,
    "package_name": "LiblineaR",
    "title": "Linear Predictive Models Based on the LIBLINEAR C/C++ Library",
    "description": "A wrapper around the LIBLINEAR C/C++ library for machine\n        learning (available at\n        <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>). LIBLINEAR is\n        a simple library for solving large-scale regularized linear\n        classification and regression. It currently supports\n        L2-regularized classification (such as logistic regression,\n        L2-loss linear SVM and L1-loss linear SVM) as well as\n        L1-regularized classification (such as L2-loss linear SVM and\n        logistic regression) and L2-regularized support vector\n        regression (with L1- or L2-loss). The main features of\n        LiblineaR include multi-class classification (one-vs-the rest,\n        and Crammer & Singer method), cross validation for model\n        selection, probability estimates (logistic regression only) or\n        weights for unbalanced data. The estimation of the models is\n        particularly fast as compared to other libraries.",
    "version": "2.10-24",
    "maintainer": "Thibault Helleputte <thibault.helleputte@dnalytics.com>",
    "url": "<https://dnalytics.com/software/liblinear/>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3777,
    "package_name": "LilRhino",
    "title": "For Implementation of Feed Reduction, Learning Examples, NLP and\nCode Management",
    "description": "This is for code management functions, NLP tools, a Monty Hall simulator, and for implementing my own variable reduction technique called Feed Reduction. The Feed Reduction technique is not yet published, but is merely a tool for implementing a series of binary neural networks meant for reducing data into N dimensions, where N is the number of possible values of the response variable.",
    "version": "1.2.2",
    "maintainer": "Travis Barton <travisdatabarton@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3787,
    "package_name": "Linkage",
    "title": "Clustering Communication Networks Using the Stochastic Topic\nBlock Model Through Linkage.fr",
    "description": "It allows to cluster communication networks using the Stochastic\n  Topic Block Model <doi:10.1007/s11222-016-9713-7> by posting jobs through \n  the API of the linkage.fr server, which implements the clustering method.\n  The package also allows to visualize the clustering results returned by the server.",
    "version": "0.9",
    "maintainer": "Charles Bouveyron <charles.bouveyron@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3808,
    "package_name": "LogicForest",
    "title": "Logic Forest",
    "description": "Logic Forest is an ensemble machine learning method that identifies important and interpretable combinations of binary predictors using logic regression trees to model complex relationships with an outcome. Wolf, B.J., Slate, E.H., Hill, E.G. (2010) <doi:10.1093/bioinformatics/btq354>.",
    "version": "2.1.2",
    "maintainer": "Melica Nikahd <melica.nikahd@osumc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3817,
    "package_name": "LongituRF",
    "title": "Random Forests for Longitudinal Data",
    "description": "Random forests are a statistical learning method widely used in many areas of scientific research essentially for its ability to learn complex relationships between input and output variables and also its capacity to handle high-dimensional data. However, current random forests approaches are not flexible enough to handle longitudinal data.  In this package, we propose a general approach of random forests for high-dimensional longitudinal data. It includes a flexible stochastic model which allows the covariance structure to vary over time. Furthermore, we introduce a new method which takes intra-individual covariance into consideration to build random forests. The method is fully detailled in Capitaine et.al. (2020) <doi:10.1177/0962280220946080> Random forests for high-dimensional longitudinal data.",
    "version": "0.9",
    "maintainer": "Louis Capitaine <Louis.capitaine@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3828,
    "package_name": "M2SMF",
    "title": "Multi-Modal Similarity Matrix Factorization for Integrative\nMulti-Omics Data Analysis",
    "description": "A new method to implement clustering from multiple modality data of certain samples, \n    the function M2SMF() jointly factorizes multiple similarity matrices into a shared sub-matrix \n    and several modality private sub-matrices, which is further used for clustering. Along with \n    this method, we also provide function to calculate the similarity matrix and function to \n    evaluate the best cluster number from the original data.",
    "version": "2.0",
    "maintainer": "Xiaoyao Yin <yinxy1992@sina.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3829,
    "package_name": "M2SMJF",
    "title": "Multi-Modal Similarity Matrix Joint Factorization",
    "description": "A new method to implement clustering from multiple modality data of certain samples, \n    the function M2SMjF() jointly factorizes multiple similarity matrices into a shared sub-matrix \n    and several modality private sub-matrices, which is further used for clustering. Along with \n    this method, we also provide function to calculate the similarity matrix and function to \n    evaluate the best cluster number from the original data.",
    "version": "1.0",
    "maintainer": "Xiaoyao Yin <yinxy1992@sina.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3833,
    "package_name": "M3JF",
    "title": "Multi-Modal Matrix Joint Factorization for Integrative\nMulti-Omics Data Analysis",
    "description": "Multi modality data matrices are factorized conjointly into the multiplication of a shared sub-matrix and multiple modality specific sub-matrices, group sparse constraint is applied to the shared sub-matrix to capture the homogeneous and heterogeneous information, respectively. Then the samples are classified by clustering the shared sub-matrix with kmeanspp(), a new version of kmeans() developed here to obtain concordant results. The package also provides the cluster number estimation by rotation cost. Moreover, cluster specific features could be retrieved using hypergeometric tests.",
    "version": "0.1.0",
    "maintainer": "Xiaoyao Yin <xyyin@xmail.ncba.ac.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3868,
    "package_name": "MARSANNhybrid",
    "title": "MARS Based ANN Hybrid Model",
    "description": "Multivariate Adaptive Regression Spline (MARS) based Artificial Neural Network (ANN) hybrid model is combined Machine learning hybrid approach which selects important variables using MARS and then fits ANN on the extracted important variables.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3871,
    "package_name": "MARSSVRhybrid",
    "title": "MARS SVR Hybrid",
    "description": "Multivariate Adaptive Regression Spline (MARS) based Support Vector Regression (SVR) hybrid model is combined Machine learning hybrid approach which selects important variables using MARS and then fits SVR on the extracted important variables.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3890,
    "package_name": "MBCbook",
    "title": "Companion Package for the Book \"Model-Based Clustering and\nClassification for Data Science\"",
    "description": "The companion package provides all original data sets and functions that are used in the book \"Model-Based Clustering and Classification for Data Science\" by Charles Bouveyron, Gilles Celeux, T. Brendan Murphy and Adrian E. Raftery (2019, ISBN:9781108644181).",
    "version": "0.1.2",
    "maintainer": "Charles Bouveyron <charles.bouveyron@gmail.com>",
    "url": "https://github.com/cbouveyron/MBCbook",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3908,
    "package_name": "MCID",
    "title": "Estimating the Minimal Clinically Important Difference",
    "description": "Apply the marginal classification method to achieve the purpose of providing \n    the point and interval estimates for the minimal clinically important difference \n    based on the classical anchor-based method. For more details of the methodology, please \n    see Zehua Zhou, Leslie J. Bisson and Jiwei Zhao (2021) <arXiv:2108.11589>.  ",
    "version": "0.1.0",
    "maintainer": "Zehua Zhou <zehuazho@buffalo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3926,
    "package_name": "MCSim",
    "title": "Determine the Optimal Number of Clusters",
    "description": "Identifies the optimal number of clusters by calculating the similarity between\n             two clustering methods at the same number of clusters using the corrected indices of Rand and\n             Jaccard as described in Albatineh and Niewiadomska-Bugaj (2011). The number of clusters at\n             which the index attain its maximum more frequently is a candidate for being the optimal\n             number of clusters.",
    "version": "1.0",
    "maintainer": "Ahmed N. Albatineh <aalbatineh@hsc.edu.kw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3956,
    "package_name": "MEDseq",
    "title": "Mixtures of Exponential-Distance Models with Covariates",
    "description": "Implements a model-based clustering method for categorical life-course sequences relying on mixtures of exponential-distance models introduced by Murphy et al. (2021) <doi:10.1111/rssa.12712>. A range of flexible precision parameter settings corresponding to weighted generalisations of the Hamming distance metric are considered, along with the potential inclusion of a noise component. Gating covariates can be supplied in order to relate sequences to baseline characteristics and sampling weights are also accommodated. The models are fitted using the EM algorithm and tools for visualising the results are also provided.",
    "version": "1.4.2",
    "maintainer": "Keefe Murphy <keefe.murphy@mu.ie>",
    "url": "https://cran.r-project.org/package=MEDseq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3964,
    "package_name": "MERO",
    "title": "Performing Monte Carlo Expectation Maximization Random Forest\nImputation for Biological Data",
    "description": "Perform missing value imputation for biological data using the random forest algorithm, the imputation aim to keep the original mean and standard deviation consistent after imputation.",
    "version": "0.1.2",
    "maintainer": "Mohamed Soudy <MohmedSoudy2009@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3966,
    "package_name": "MEclustnet",
    "title": "Fit the Mixture of Experts Latent Position Cluster Model to\nNetwork Data",
    "description": "Functions to facilitate model-based clustering of nodes in a network in a mixture of experts setting, which incorporates covariate information on the nodes in the modelling process. Isobel Claire Gormley and Thomas Brendan Murphy (2010) <doi:10.1016/j.stamet.2010.01.002>.",
    "version": "1.2.2",
    "maintainer": "Isobel Claire Gormley <claire.gormley@ucd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3981,
    "package_name": "MGMM",
    "title": "Missingness Aware Gaussian Mixture Models",
    "description": "Parameter estimation and classification for Gaussian Mixture Models (GMMs) in the presence of missing data. This package complements existing implementations by allowing for both missing elements in the input vectors and full (as opposed to strictly diagonal) covariance matrices. Estimation is performed using an expectation conditional maximization algorithm that accounts for missingness of both the cluster assignments and the vector components. The output includes the marginal cluster membership probabilities; the mean and covariance of each cluster; the posterior probabilities of cluster membership; and a completed version of the input data, with missing values imputed to their posterior expectations. For additional details, please see McCaw ZR, Julienne H, Aschard H. \"Fitting Gaussian mixture models on incomplete data.\" <doi:10.1186/s12859-022-04740-9>.",
    "version": "1.0.1.1",
    "maintainer": "Zachary McCaw <zmccaw@alumni.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3982,
    "package_name": "MGMS2",
    "title": "'MGMS2' for Polymicrobial Samples",
    "description": "A glycolipid mass spectrometry technology has the potential to accurately identify individual bacterial species from polymicrobial samples. To develop bacterial identification algorithms (e.g. machine learning) using this glycolipid technology, it is necessary to generate a large number of various in-silico polymicrobial mass spectra that are similar to real mass spectra. 'MGMS2' (Membrane Glycolipid Mass Spectrum Simulator) generates such in-silico mass spectra, considering errors in m/z (mass-to-charge ratio) and variances of intensity values, occasions of missing signature ions, and noise peaks. It estimates summary statistics of monomicrobial mass spectra for each strain or species and simulates polymicrobial glycolipid mass spectra using the summary statistics of monomicrobial mass spectra. References: Ryu, S.Y., Wendt, G.A., Chandler, C.E., Ernst, R.K. and Goodlett, D.R. (2019) <doi:10.1021/acs.analchem.9b03340> \"Model-based Spectral Library Approach for Bacterial Identification via Membrane Glycolipids.\" Gibb, S. and Strimmer, K. (2012) <doi:10.1093/bioinformatics/bts447> \"MALDIquant: a versatile R package for the analysis of mass spectrometry data.\"",
    "version": "1.0.2",
    "maintainer": "George Wendt <gwendt@unr.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4012,
    "package_name": "MKMeans",
    "title": "A Modern K-Means (MKMeans) Clustering Algorithm",
    "description": "It's a Modern K-Means clustering algorithm which works for data of any number of dimensions, has no limit with the number of clusters expected, offers both methods with and without initial cluster centers, and can start with any initial cluster centers for the method with initial cluster centers.",
    "version": "3.4.4",
    "maintainer": "Yarong Yang <Yi.YA_yaya@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4022,
    "package_name": "MLBC",
    "title": "Bias Correction Methods for Models Using Synthetic Data",
    "description": "Implements three bias-correction techniques from Battaglia et al. (2025 <doi:10.48550/arXiv.2402.15585>) to improve inference in regression models with covariates generated by AI or machine learning.",
    "version": "0.2.2",
    "maintainer": "Konrad Kurczynski <konrad.kurczynski@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4025,
    "package_name": "MLCOPULA",
    "title": "Classification Models with Copula Functions",
    "description": "Provides several classifiers based on probabilistic models. These classifiers allow to model the dependence structure of continuous features through bivariate copula functions and graphical models, see Salinas-Gutiérrez et al. (2014) <doi:10.1007/s00180-013-0457-y>.",
    "version": "1.1.0",
    "maintainer": "Rogelio Salinas Gutiérrez <rsalinas@correo.uaa.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4027,
    "package_name": "MLDataR",
    "title": "Collection of Machine Learning Datasets for Supervised Machine\nLearning",
    "description": "Contains a collection of datasets for working with machine learning tasks.\n    It will contain datasets for supervised machine learning Jiang (2020)<doi:10.1016/j.beth.2020.05.002> and will include datasets for classification and regression.\n    The aim of this package is to use data generated around health and other domains.",
    "version": "1.0.1",
    "maintainer": "Gary Hutson <hutsons-hacks@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4032,
    "package_name": "MLFS",
    "title": "Machine Learning Forest Simulator",
    "description": "Climate-sensitive, single-tree forest simulator based on\n  data-driven machine learning. It simulates the main forest processes—\n  radial growth, height growth, mortality, crown recession, regeneration,\n  and harvesting—so users can assess stand development under climate and\n  management scenarios. The height model is described by Skudnik and\n  Jevšenak (2022) <doi:10.1016/j.foreco.2022.120017>, the basal-area\n  increment model by Jevšenak and Skudnik (2021) <doi:10.1016/j.foreco.2020.118601>,\n  and an overview of the MLFS package, workflow, and applications is\n  provided by Jevšenak, Arnič, Krajnc, and Skudnik (2023), Ecological\n  Informatics <doi:10.1016/j.ecoinf.2023.102115>.",
    "version": "0.4.3",
    "maintainer": "Jernej Jevsenak <jernej.jevsenak@gmail.com>",
    "url": "https://CRAN.R-project.org/package=MLFS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4043,
    "package_name": "MLSP",
    "title": "Machine Learning Models for Soil Properties",
    "description": "Creates a spectroscopy guideline with a highly accurate prediction model for soil properties using machine learning or deep learning algorithms such as LASSO, Random Forest, Cubist, etc., and decide which algorithm generates the best model for different soil types.",
    "version": "0.1.0",
    "maintainer": "Pengyuan Chen <pch276@uky.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4046,
    "package_name": "MLeval",
    "title": "Machine Learning Model Evaluation",
    "description": "Straightforward and detailed evaluation of machine learning models. 'MLeval' can produce receiver operating characteristic (ROC) curves, precision-recall (PR) curves, calibration curves, and PR gain curves. 'MLeval' accepts a data frame of class probabilities and ground truth labels, or, it can automatically interpret the Caret train function results from repeated cross validation, then select the best model and analyse the results. 'MLeval' produces a range of evaluation metrics with confidence intervals.",
    "version": "0.3",
    "maintainer": "Christopher R John <chris.r.john86@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4047,
    "package_name": "MLmetrics",
    "title": "Machine Learning Evaluation Metrics",
    "description": "A collection of evaluation metrics, including loss, score and\n    utility functions, that measure regression, classification and ranking performance.",
    "version": "1.1.3",
    "maintainer": "Yachen Yan <yanyachen21@gmail.com>",
    "url": "https://github.com/yanyachen/MLmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4048,
    "package_name": "MLmorph",
    "title": "Integrating Morphological Modeling and Machine Learning for\nDecision Support",
    "description": "Integrating morphological modeling with machine learning to support\n   structured decision-making (e.g., in management and consulting). The package\n   enumerates a morphospace of feasible configurations and uses random forests\n   to estimate class probabilities over that space, bridging deductive model\n   exploration with empirical validation. It includes utilities for factorizing\n   inputs, model training, morphospace construction, and an interactive 'shiny'\n   app for scenario exploration.",
    "version": "0.1.1",
    "maintainer": "Oskar Kosch <contact@oskarkosch.com>",
    "url": "https://github.com/theogrost/MLmorph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4064,
    "package_name": "MMOC",
    "title": "Multi-Omic Spectral Clustering using the Flag Manifold",
    "description": "Multi-omic (or any multi-view) spectral clustering methods often assume the same number of clusters across all datasets. We supply methods for multi-omic spectral clustering when the number of distinct clusters differs among the omics profiles (views). ",
    "version": "0.1.1.0",
    "maintainer": "Charlie Carpenter <charles.carpenter@cuanschutz.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4069,
    "package_name": "MNARclust",
    "title": "Clustering Data with Non-Ignorable Missingness using\nSemi-Parametric Mixture Models",
    "description": "Clustering of data under a non-ignorable missingness mechanism. Clustering is achieved by a semi-parametric mixture model and missingness is managed by using the pattern-mixture approach. More details of the approach are available in Du Roy de Chaumaray et al. (2020) <arXiv:2009.07662>. ",
    "version": "1.1.0",
    "maintainer": "Matthieu Marbac <matthieu.marbac-lourdelle@ensai.fr>",
    "url": "https://arxiv.org/abs/2009.07662",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4121,
    "package_name": "MRMCsamplesize",
    "title": "Sample Size Estimations for Planning Multi-Reader Multi-Case\n(MRMC) Studies Without Pilot Data",
    "description": "Sample size estimations for MRMC studies based on the Obuchowski-Rockette (OR) methodology is implemented. The function can calculate sample sizes where the \n             endpoint of interest in the study is either ROC AUC (Area-Under-the-Receiver-Operating-Characteristics-Curve) or sensitivity. The package can also return sample sizes for studies\n             expected to have clustering effect (e.g.- multiple pulmonary nodules per patient). All calculations assume that the study design is fully crossed (paired-reader, paired-case) where\n             each reader reads/interprets each case and that there are two interventions/imaging-modalities/techniques in the study. In addition to MRMC, it can also be used to estimate sample sizes \n             for standalone studies where sensitivity or AUC are the primary endpoints.\n             The methods implemented are based on the methods described in Zhou et.al. (2011) <doi:10.1002/9780470906514> and Obuchowski (2000) <doi:10.1097/EDE.0b013e3181a663cc>. ",
    "version": "1.0.0",
    "maintainer": "Dennis Robert <dennis.robert.nm@gmail.com>",
    "url": "https://github.com/technOslerphile/MRMCsamplesize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4133,
    "package_name": "MSCA",
    "title": "Unsupervised Clustering of Multiple Censored Time-to-Event\nEndpoints",
    "description": "Provides basic tools and wrapper functions for computing clusters of instances described by multiple time-to-event censored endpoints. From long-format datasets, where one instance is described by one or more dated records, the main function, `make_state_matrices()`, creates state matrices. Based on these matrices, optimised procedures using the Jaccard distance between instances enable the construction of longitudinal typologies. The package is under active development, with additional tools for graphical representation of typologies planned. For methodological details, see our accompanying paper: `Delord M, Douiri A (2025) <doi:10.1186/s12874-025-02476-7>`.",
    "version": "1.2.1",
    "maintainer": "Marc Delord <mdelord@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4143,
    "package_name": "MSML",
    "title": "Model Selection Based on Machine Learning (ML)",
    "description": "Model evaluation based on a modified version of the recursive feature elimination algorithm. This package is designed to determine the optimal model(s) by leveraging all available features. ",
    "version": "1.0.0.1",
    "maintainer": "Moksedul Momin <cvasu.momin@gmail.com>",
    "url": "https://github.com/mommy003/MSML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4154,
    "package_name": "MSclassifR",
    "title": "Automated Classification of Mass Spectra",
    "description": "Functions to classify mass spectra in known categories and to determine discriminant mass-to-charge values (m/z). Includes easy-to-use preprocessing pipelines for Matrix Assisted Laser Desorption Ionisation - Time Of Flight Mass Spectrometry (MALDI-TOF) mass spectra, methods to select discriminant m/z from labelled libraries, and tools to predict categories (species, phenotypes, etc.) from selected features. Also provides utilities to build design matrices from peak intensities and labels. While this package was developed with the aim of identifying very similar species or phenotypes of bacteria from  MALDI-TOF MS, the functions of this package can also be used to classify other categories associated to mass spectra; or from mass spectra obtained with other mass spectrometry techniques. Parallelized processing and optional C++-accelerated functions are available (notably to deal with large datasets) from version 0.5.0.  If you use this package in your research, please cite the associated publication (<doi:10.1016/j.eswa.2025.128796>). For a comprehensive guide, additional applications, and detailed examples, see <https://github.com/agodmer/MSclassifR_examples>.",
    "version": "0.5.0",
    "maintainer": "Alexandre Godmer <alexandre.godmer@aphp.fr>",
    "url": "https://github.com/agodmer/MSclassifR_examples,\nhttps://doi.org/10.1016/j.eswa.2025.128796",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4155,
    "package_name": "MSclust",
    "title": "Multiple-Scaled Clustering",
    "description": "Model based clustering using \n    the multivariate multiple Scaled t (MST) and multivariate multiple \n    scaled contaminated normal (MSCN) distributions. The MST is an \n    extension of the multivariate Student-t distribution to include \n    flexible tail behaviors, Forbes, F. & Wraith, D. (2014) <doi:10.1007/s11222-013-9414-4>. The MSCN represents a  heavy-tailed\n    generalization of the multivariate normal (MN) distribution to\n    model elliptical contoured scatters in the presence of mild outliers\n    (also referred to as \"bad\" points) and automatically detect bad points, Punzo, A. & Tortora, C. (2021) <doi:10.1177/1471082X19890935>.",
    "version": "1.0.4",
    "maintainer": "Cristina Tortora <grikris1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4217,
    "package_name": "MagmaClustR",
    "title": "Clustering and Prediction using Multi-Task Gaussian Processes\nwith Common Mean",
    "description": "An implementation for the multi-task Gaussian processes with common \n    mean framework. Two main algorithms, called 'Magma' and 'MagmaClust', \n    are available to perform predictions for supervised learning problems, in\n    particular for time series or any functional/continuous data applications.\n    The corresponding articles has been respectively proposed by Arthur Leroy, \n    Pierre Latouche, Benjamin Guedj and Servane Gey (2022) \n    <doi:10.1007/s10994-022-06172-1>, and Arthur Leroy, Pierre Latouche, \n    Benjamin Guedj and Servane Gey (2023) <https://jmlr.org/papers/v24/20-1321.html>.\n    Theses approaches leverage the learning of cluster-specific mean processes,\n    which are common across similar tasks, to provide enhanced prediction\n    performances (even far from data) at a linear computational cost (in\n    the number of tasks).  'MagmaClust' is a generalisation of 'Magma'\n    where the tasks are simultaneously clustered into groups, each being\n    associated to a specific mean process.  User-oriented functions in the\n    package are decomposed into training, prediction and plotting\n    functions. Some basic features (classic kernels, training, prediction) of\n    standard Gaussian processes are also implemented. ",
    "version": "1.2.1",
    "maintainer": "Arthur Leroy <arthur.leroy.pro@gmail.com>",
    "url": "https://github.com/ArthurLeroy/MagmaClustR,\nhttps://arthurleroy.github.io/MagmaClustR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4219,
    "package_name": "MajKMeans",
    "title": "k-Means Algorithm with a Majorization-Minimization Method",
    "description": "A hybrid of the K-means algorithm and a Majorization-Minimization method to introduce a robust clustering. The reference paper is: Julien Mairal, (2015) <doi:10.1137/140957639>. The two most important functions in package 'MajKMeans' are cluster_km() and cluster_MajKm(). cluster_km() clusters data without Majorization-Minimization and cluster_MajKm() clusters data with Majorization-Minimization method. Both of these functions calculate the sum of squares (SS) of clustering.",
    "version": "0.1.0",
    "maintainer": "Sheikhi Ayyub  <sheikhy.a@uk.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4220,
    "package_name": "MajMinKmeans",
    "title": "k-Means Algorithm with a Majorization-Minimization Method",
    "description": "A hybrid of the K-means algorithm and a Majorization-Minimization method to introduce a robust clustering. The reference paper is: Julien Mairal, (2015) <doi:10.1137/140957639>. The two most important functions in package 'MajMinKmeans' are cluster_km() and cluster_MajKm(). Cluster_km() clusters data without Majorization-Minimization and cluster_MajKm() clusters data with Majorization-Minimization method. Both of these functions calculate the sum of squares (SS) of clustering. Another useful function is MajMinOptim(), which helps to find the optimum values of the Majorization-Minimization estimator.",
    "version": "0.1.0",
    "maintainer": "Sheikhi Ayyub  <sheikhy.a@uk.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4226,
    "package_name": "ManlyMix",
    "title": "Manly Mixture Modeling and Model-Based Clustering",
    "description": "The utility of this package includes finite mixture modeling and model-based clustering through Manly mixture models by Zhu and Melnykov (2016) <DOI:10.1016/j.csda.2016.01.015>. It also provides capabilities for forward and backward model selection procedures.  ",
    "version": "0.1.15.1",
    "maintainer": "Xuwen Zhu <xzhu20@cba.ua.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4236,
    "package_name": "MapperAlgo",
    "title": "Topological Data Analysis: Mapper Algorithm",
    "description": "The Mapper algorithm from Topological Data Analysis, the steps are as follows 1. Define a filter (lens) function on the data. 2. Perform clustering within each level set. 3. Generate a complex from the clustering results.",
    "version": "1.0.7",
    "maintainer": "ChiChien Wang <kennywang2003@gmail.com>",
    "url": "https://github.com/TDA-R/MapperAlgo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4250,
    "package_name": "MatTransMix",
    "title": "Clustering with Matrix Gaussian and Matrix Transformation\nMixture Models",
    "description": "Provides matrix Gaussian mixture models, matrix transformation mixture models and their model-based clustering results. The parsimonious models of the mean matrices and variance covariance matrices are implemented with a total of 196 variations. For more information, please check: Xuwen Zhu, Shuchismita Sarkar, and Volodymyr Melnykov (2021), \"MatTransMix: an R package for matrix model-based clustering and parsimonious mixture modeling\",  <doi:10.1007/s00357-021-09401-9>.",
    "version": "0.1.18",
    "maintainer": "Xuwen Zhu <xzhu20@ua.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4262,
    "package_name": "MatrixMixtures",
    "title": "Model-Based Clustering via Matrix-Variate Mixture Models",
    "description": "Implements finite mixtures of matrix-variate contaminated normal distributions via expectation conditional-maximization algorithm for model-based clustering, as described in Tomarchio et al.(2020) <arXiv:2005.03861>. One key advantage of this model is the ability to automatically detect potential outlying matrices by computing their a posteriori probability of being typical or atypical points. Finite mixtures of matrix-variate t and matrix-variate normal distributions are also implemented by using expectation-maximization algorithms.",
    "version": "1.0.0",
    "maintainer": "Michael P.B. Gallaugher <michael_gallaugher@baylor.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4295,
    "package_name": "Mercator",
    "title": "Clustering and Visualizing Distance Matrices",
    "description": "Defines the classes used to explore, cluster and\n  visualize distance matrices, especially those arising from binary\n  data. See Abrams and colleagues, 2021, <doi:10.1093/bioinformatics/btab037>.",
    "version": "1.1.7",
    "maintainer": "Kevin R. Coombes <krc@silicovore.com>",
    "url": "http://oompa.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4314,
    "package_name": "MetaNLP",
    "title": "Natural Language Processing for Meta Analysis",
    "description": "Given a CSV file with titles and abstracts, the package creates a\n    document-term matrix that is lemmatized and stemmed and can directly be used to\n    train machine learning methods for automatic title-abstract screening in the\n    preparation of a meta analysis.",
    "version": "0.1.4",
    "maintainer": "Maximilian Pilz <maximilian.pilz@itwm.fraunhofer.de>",
    "url": "https://github.com/imbi-heidelberg/MetaNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4329,
    "package_name": "MetabolSSMF",
    "title": "Simplex-Structured Matrix Factorisation for Metabolomics\nAnalysis",
    "description": "Provides a framework to perform soft clustering using \n        simplex-structured matrix factorisation (SSMF). The package contains a set of functions\n        for determining the optimal number of prototypes, the optimal algorithmic\n        parameters, the estimation confidence intervals and the diversity of clusters.\n        Abdolali, Maryam & Gillis, Nicolas (2020) <doi:10.1137/20M1354982>.",
    "version": "0.1.0",
    "maintainer": "Wenxuan Liu <wenxuan.liu@ucdconnect.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4332,
    "package_name": "MetabolomicsBasics",
    "title": "Basic Functions to Investigate Metabolomics Data Matrices",
    "description": "A set of functions to investigate raw data from (metabol)omics experiments intended to be used on a raw data matrix, i.e. following peak picking and signal deconvolution. Functions can be used to normalize data, detect biomarkers and perform sample classification. A detailed description of best practice usage may be found in the publication <doi:10.1007/978-1-4939-7819-9_20>.",
    "version": "1.4.7",
    "maintainer": "Jan Lisec <jan.lisec@bam.de>",
    "url": "https://github.com/janlisec/MetabolomicsBasics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4345,
    "package_name": "Metrics",
    "title": "Evaluation Metrics for Machine Learning",
    "description": "An implementation of evaluation metrics in R that are commonly\n             used in supervised machine learning. It implements metrics for\n             regression, time series, binary classification, classification,\n             and information retrieval problems. It has zero dependencies and\n             a consistent, simple interface for all functions.",
    "version": "0.1.4",
    "maintainer": "Michael Frasco <mfrasco6@gmail.com>",
    "url": "https://github.com/mfrasco/Metrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4391,
    "package_name": "MixGHD",
    "title": "Model Based Clustering, Classification and Discriminant Analysis\nUsing the Mixture of Generalized Hyperbolic Distributions",
    "description": "Carries out model-based clustering, classification and discriminant analysis using five different models. The models are all based on the  generalized hyperbolic distribution. The first model 'MGHD' (Browne and McNicholas (2015) <doi:10.1002/cjs.11246>) is the classical mixture of generalized hyperbolic distributions. The 'MGHFA' (Tortora et al. (2016) <doi:10.1007/s11634-015-0204-z>) is the mixture of generalized hyperbolic factor analyzers for high dimensional data sets. The 'MSGHD' is the mixture of  multiple scaled generalized hyperbolic distributions, the 'cMSGHD'  is a 'MSGHD' with convex contour plots and the 'MCGHD', mixture of  coalesced generalized hyperbolic distributions is a new more flexible model (Tortora et al. (2019)<doi:10.1007/s00357-019-09319-3>. The paper related to the software can be found at <doi:10.18637/jss.v098.i03>.",
    "version": "2.3.7",
    "maintainer": "Cristina Tortora <grikris1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4393,
    "package_name": "MixMatrix",
    "title": "Classification with Matrix Variate Normal and t Distributions",
    "description": "Provides sampling and density functions for matrix\n    variate normal, t, and inverted t distributions;  ML estimation for matrix\n    variate normal and t distributions using the EM algorithm,\n    including some restrictions on the parameters; and classification by linear and\n    quadratic discriminant analysis for matrix variate normal and t\n    distributions described in Thompson et al. (2019) <doi:10.1080/10618600.2019.1696208>.\n    Performs clustering with matrix variate normal and t mixture models.",
    "version": "0.2.8",
    "maintainer": "Geoffrey Thompson <gzthompson@gmail.com>",
    "url": "https://github.com/gzt/MixMatrix/,\nhttps://gzt.github.io/MixMatrix/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4398,
    "package_name": "MixSim",
    "title": "Simulating Data to Study Performance of Clustering Algorithms",
    "description": "The utility of this package is in simulating mixtures of Gaussian\n        distributions with different levels of overlap between mixture\n        components.  Pairwise overlap, defined as a sum of two\n        misclassification probabilities, measures the degree of\n        interaction between components and can be readily employed to\n        control the clustering complexity of datasets simulated from\n        mixtures. These datasets can then be used for systematic\n        performance investigation of clustering and finite mixture\n        modeling algorithms. Among other capabilities of 'MixSim', there\n        are computing the exact overlap for Gaussian mixtures,\n        simulating Gaussian and non-Gaussian data, simulating outliers\n        and noise variables, calculating various measures of agreement\n        between two partitionings, and constructing parallel\n        distribution plots for the graphical display of finite mixture\n        models.",
    "version": "1.1-8",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4406,
    "package_name": "MixfMRI",
    "title": "Mixture fMRI Clustering Analysis",
    "description": "Utilizing model-based clustering (unsupervised)\n        for functional magnetic resonance imaging (fMRI) data.\n        The developed methods (Chen and Maitra (2023) <doi:10.1002/hbm.26425>)\n        include 2D and 3D clustering analyses\n        (for p-values with voxel locations) and\n        segmentation analyses (for p-values alone) for fMRI data where p-values\n        indicate significant level of activation responding to stimulate\n        of interesting. The analyses are mainly identifying active\n        voxel/signal associated with normal brain behaviors.\n        Analysis pipelines (R scripts) utilizing this package\n        (see examples in 'inst/workflow/') is also implemented with high\n        performance techniques.",
    "version": "0.1-4",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "url": "https://github.com/snoweye/MixfMRI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4408,
    "package_name": "MixtureMissing",
    "title": "Robust and Flexible Model-Based Clustering for Data Sets with\nMissing Values at Random",
    "description": "Implementations of various robust and flexible model-based clustering methods for data sets with missing values at random. \n    Two main models are: Multivariate Contaminated Normal Mixture (MCNM, Tong and Tortora, 2022, <doi:10.1007/s11634-021-00476-1>) and \n    Multivariate Generalized Hyperbolic Mixture (MGHM, Wei et al., 2019, <doi:10.1016/j.csda.2018.08.016>). Mixtures via some special or limiting\n    cases of the multivariate generalized hyperbolic distribution are also included: Normal-Inverse Gaussian, Symmetric Normal-Inverse Gaussian, \n    Skew-Cauchy, Cauchy, Skew-t, Student's t, Normal, Symmetric Generalized Hyperbolic, Hyperbolic Univariate Marginals, \n    Hyperbolic, and Symmetric Hyperbolic. Funding: This work was partially supported by the National Science foundation NSF Grant NO. 2209974.",
    "version": "3.0.5",
    "maintainer": "Hung Tong <hungtongmx@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4412,
    "package_name": "MoEClust",
    "title": "Gaussian Parsimonious Clustering Models with Covariates and a\nNoise Component",
    "description": "Clustering via parsimonious Gaussian Mixtures of Experts using the MoEClust models introduced by Murphy and Murphy (2020) <doi:10.1007/s11634-019-00373-8>. This package fits finite Gaussian mixture models with a formula interface for supplying gating and/or expert network covariates using a range of parsimonious covariance parameterisations from the GPCM family via the EM/CEM algorithm. Visualisation of the results of such models using generalised pairs plots and the inclusion of an additional noise component is also facilitated. A greedy forward stepwise search algorithm is provided for identifying the optimal model in terms of the number of components, the GPCM covariance parameterisation, and the subsets of gating/expert network covariates.",
    "version": "1.6.0",
    "maintainer": "Keefe Murphy <keefe.murphy@mu.ie>",
    "url": "https://cran.r-project.org/package=MoEClust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4420,
    "package_name": "ModTools",
    "title": "Building Regression and Classification Models",
    "description": "Consistent user interface to the most common regression and classification algorithms, such as random forest, neural networks, C5 trees and support vector machines, complemented with a handful of auxiliary functions, such as variable importance and a tuning function for the parameters.",
    "version": "0.9.13",
    "maintainer": "Andri Signorell <andri@signorell.net>",
    "url": "https://andrisignorell.github.io/ModTools/,\nhttps://github.com/AndriSignorell/ModTools/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4421,
    "package_name": "Modalclust",
    "title": "Hierarchical Modal Clustering",
    "description": "Performs Modal Clustering (MAC) including Hierarchical Modal Clustering (HMAC) along with their parallel implementation (PHMAC) over several processors.  These model-based non-parametric clustering techniques can extract  clusters in very high dimensions with arbitrary density shapes. By default clustering is performed over several resolutions and the results are summarised as a hierarchical tree. Associated plot functions are also provided. There is a package vignette that provides many examples. This version adheres to CRAN policy of not spanning more than two child processes by default.",
    "version": "0.7",
    "maintainer": "Surajit Ray <surajit.ray@glasgow.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4425,
    "package_name": "Modelcharts",
    "title": "Classification Model Charts",
    "description": "Provides two important functions for producing Gain chart and Lift chart for any classification model.",
    "version": "0.1.0",
    "maintainer": "Krishna Harsha K H <khkrishnaharsha123@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4475,
    "package_name": "MuViCP",
    "title": "MultiClass Visualizable Classification using Combination of\nProjections",
    "description": "An ensemble classifier for multiclass classification. This is a novel classifier that natively works as an ensemble. It projects data on a large number of matrices, and uses very simple classifiers on each of these projections. The results are then combined, ideally via Dempster-Shafer Calculus.",
    "version": "1.3.2",
    "maintainer": "Mohit Dayal <mohitdayal2000@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4491,
    "package_name": "MultiClassROC",
    "title": "ROC Curves for Multi-Class Analysis",
    "description": "Function multiroc() can be used for computing and visualizing Receiver Operating Characteristics (ROC) and Area Under the Curve (AUC) for multi-class classification problems. It supports both One-vs-One approach by M.Bishop, C. (2006, ISBN:978-0-387-31073-2) and One-vs-All approach by Murphy P., K. (2012, ISBN:9780262018029).",
    "version": "0.1.0",
    "maintainer": "Marton Varga <vargamarton0723@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4524,
    "package_name": "MultivariateRandomForest",
    "title": "Models Multivariate Cases Using Random Forests",
    "description": "Models and predicts multiple output features in single random forest considering the \n    linear relation among the output features, see details in Rahman et al (2017)<doi:10.1093/bioinformatics/btw765>.",
    "version": "1.1.5",
    "maintainer": "Raziur Rahman <razeeebuet@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4527,
    "package_name": "MulvariateRandomForestVarImp",
    "title": "Variable Importance Measures for Multivariate Random Forests",
    "description": "Calculates two sets of post-hoc variable importance measures for multivariate random forests. The first set of variable importance measures are given by the sum of mean split improvements for splits defined by feature j measured on user-defined examples (i.e., training or testing samples). The second set of importance measures are calculated on a per-outcome variable basis as the sum of mean absolute difference of node values for each split defined by feature j measured on user-defined examples (i.e., training or testing samples). The user can optionally threshold both sets of importance measures to include only splits that are statistically significant as measured using an F-test. ",
    "version": "0.0.2",
    "maintainer": "Dogonadze Nika <nika.dogonadze@toptal.com>",
    "url": "https://github.com/Megatvini/VIM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4535,
    "package_name": "NAC",
    "title": "Network-Adjusted Covariates for Community Detection",
    "description": "Incorporating node-level covariates for community detection has gained increasing attention these years. This package provides the function for implementing the novel community detection algorithm known as Network-Adjusted Covariates for Community Detection (NAC), which is designed to detect latent community structure in graphs with node-level information, i.e., covariates. This algorithm can handle models such as the degree-corrected stochastic block model (DCSBM) with covariates. NAC specifically addresses the discrepancy between the community structure inferred from the adjacency information and the community structure inferred from the covariates information. For more detailed information, please refer to the reference paper: Yaofang Hu and Wanjie Wang (2023) <arXiv:2306.15616>. In addition to NAC, this package includes several other existing community detection algorithms that are compared to NAC in the reference paper. These algorithms are Spectral Clustering On Ratios-of Eigenvectors (SCORE), network-based regularized spectral clustering (Net-based), covariate-based spectral clustering (Cov-based), covariate-assisted spectral clustering (CAclustering) and semidefinite programming (SDP). ",
    "version": "0.1.0",
    "maintainer": "Yaofang Hu <yaofangh@smu.edu>",
    "url": "https://arxiv.org/abs/2306.15616",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4544,
    "package_name": "NB.MClust",
    "title": "Negative Binomial Model-Based Clustering",
    "description": "Model-based clustering of high-dimensional non-negative\n             data that follow Generalized Negative Binomial distribution. All functions \n             in this package applies to either continuous or integer data. Correlation\n            between variables are allowed, while samples are assumed to be independent.",
    "version": "1.1.1",
    "maintainer": "Qian Li <qian.li10000@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4551,
    "package_name": "NBLDA",
    "title": "Negative Binomial Linear Discriminant Analysis",
    "description": "We proposed a package for the classification task which uses Negative Binomial distribution within Linear Discriminant Analysis (NBLDA). It is an extension of the 'PoiClaClu' package to Negative Binomial distribution. The classification algorithms are based on the papers Dong et al. (2016, ISSN: 1471-2105) and Witten, DM (2011, ISSN: 1932-6157) for NBLDA and PLDA, respectively. Although PLDA is a sparse algorithm and can be used for variable selection, the algorithm proposed by Dong et al. is not sparse. Therefore, it uses all variables in the classifier. Here, we extend Dong et al.'s algorithm to the sparse case by shrinking overdispersion towards 0 (Yu et al., 2013, ISSN: 1367-4803) and offset parameter towards 1 (as proposed by Witten DM, 2011). We support only the classification task with this version.",
    "version": "1.0.1",
    "maintainer": "Dincer Goksuluk <dincergoksuluk@erciyes.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4554,
    "package_name": "NBShiny",
    "title": "Interactive Document for Working with Naive Bayes Classification",
    "description": "An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4555,
    "package_name": "NBShiny2",
    "title": "Interactive Document for Working with Naive Bayes Classification",
    "description": "An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4556,
    "package_name": "NBShiny3",
    "title": "Interactive Document for Working with Naive Bayes Classification",
    "description": "An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4562,
    "package_name": "NCSampling",
    "title": "Nearest Centroid (NC) Sampling",
    "description": "Provides functionality for performing Nearest Centroid (NC) Sampling. The NC sampling procedure was developed for forestry applications and selects plots for ground measurement so as to maximize the efficiency of imputation estimates. It uses multiple auxiliary variables and multivariate clustering to search for an optimal sample. Further details are given in Melville G. & Stone C. (2016) <doi:10.1080/00049158.2016.1218265>. ",
    "version": "1.0",
    "maintainer": "Gavin Melville <gavin.melville@dpi.nsw.gov.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4612,
    "package_name": "NOVA",
    "title": "Neural Output Visualization and Analysis",
    "description": "A comprehensive toolkit for analyzing and visualizing neural data \n    outputs, including Principal Component Analysis (PCA) trajectory plotting, \n    Multi-Electrode Array (MEA) heatmap generation, and variable importance \n    analysis. Provides publication-ready visualizations with flexible \n    customization options for neuroscience research applications.",
    "version": "0.1.1",
    "maintainer": "Alex Tudoras <alex.tudorasmiravet@ucsf.edu>",
    "url": "https://github.com/atudoras/NOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4614,
    "package_name": "NPBBBDAefficiency",
    "title": "A-Efficiency for Nested Partially Balanced Bipartite Block\n(NPBBB) Designs",
    "description": "Nested Partially Balanced Bipartite Block (NPBBB) designs involve two levels of blocking: (i) The block design (ignoring sub-block classification) serves as a partially balanced bipartite block (PBBB) design, and (ii) The sub-block design (ignoring block classification) also serves as a PBBB design. More details on constructions of the PBBB designs and their characterization properties are available in Vinayaka et al.(2023) <doi:10.1080/03610926.2023.2251623>. This package calculates A-efficiency values for both block and sub-block structures, along with all parameters of a given NPBBB design.",
    "version": "0.1.0",
    "maintainer": "Vinayaka <vinayaka.b3vs@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4616,
    "package_name": "NPCD",
    "title": "Nonparametric Methods for Cognitive Diagnosis",
    "description": "An array of nonparametric and parametric estimation methods for cognitive diagnostic models, including nonparametric classification of examinee attribute profiles, joint maximum likelihood estimation (JMLE) of examinee attribute profiles and item parameters, and nonparametric refinement of the Q-matrix, as well as conditional maximum likelihood estimation (CMLE) of examinee attribute profiles given item parameters and CMLE of item parameters given examinee attribute profiles. Currently the nonparametric methods in the package support both conjunctive and disjunctive models, and the parametric methods in the package support the DINA model, the DINO model, the NIDA model, the G-NIDA model, and the R-RUM model. ",
    "version": "1.0-11",
    "maintainer": "Yi Zheng <yi.isabel.zheng@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4617,
    "package_name": "NPCDTools",
    "title": "The Nonparametric Classification Methods for Cognitive Diagnosis",
    "description": "Statistical tools for analyzing cognitive diagnosis (CD) data collected from small settings using the nonparametric classification (NPCD) framework. The core methods of the NPCD framework includes the nonparametric classification (NPC) method developed by Chiu and Douglas (2013) <DOI:10.1007/s00357-013-9132-9> and the general NPC (GNPC) method developed by Chiu, Sun, and Bian (2018) <DOI:10.1007/s11336-017-9595-4> and Chiu and Köhn (2019) <DOI:10.1007/s11336-019-09660-x>. An extension of the NPCD framework included in the package is the nonparametric method for multiple-choice items (MC-NPC) developed by Wang, Chiu, and Koehn (2023) <DOI:10.3102/10769986221133088>.  Functions associated with various extensions concerning the evaluation, validation, and feasibility of the CD analysis are also provided. These topics include the completeness of Q-matrix, Q-matrix refinement method, as well as Q-matrix estimation. ",
    "version": "1.0",
    "maintainer": "Weixuan Xiao <wx2299@tc.columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4639,
    "package_name": "NU.Learning",
    "title": "Nonparametric and Unsupervised Learning from Cross-Sectional\nObservational Data",
    "description": "Especially when cross-sectional data are observational, effects of treatment selection\n  bias and confounding are best revealed by using Nonparametric and Unsupervised methods to\n  \"Design\" the analysis of the given data ...rather than the collection of \"designed data\".\n  Specifically, the \"effect-size distribution\" that best quantifies a potentially causal\n  relationship between a numeric y-Outcome variable and either a binary t-Treatment or\n  continuous e-Exposure variable needs to consist of BLOCKS of relatively well-matched\n  experimental units (e.g. patients) that have the most similar X-confounder characteristics.\n  Since our NU Learning approach will form BLOCKS by \"clustering\" experimental units in\n  confounder X-space, the implicit statistical model for learning is One-Way ANOVA. Within\n  Block measures of effect-size are then either [a] LOCAL Treatment Differences (LTDs) between\n  Within-Cluster y-Outcome Means (\"new\" minus \"control\") when treatment choice is\n  Binary or else [b] LOCAL Rank Correlations (LRCs) when the e-Exposure variable is numeric\n  with (hopefully many) more than two levels. An Instrumental Variable (IV) method is also\n  provided so that Local Average y-Outcomes (LAOs) within BLOCKS may also contribute\n  information for effect-size inferences when X-Covariates are assumed to influence Treatment\n  choice or Exposure level but otherwise have no direct effects on y-Outcomes. Finally, a\n  \"Most-Like-Me\" function provides histograms of effect-size distributions to aid\n  Doctor-Patient (or Researcher-Society) communications about Heterogeneous Outcomes.\n  Obenchain and Young (2013) <doi:10.1080/15598608.2013.772821>; Obenchain, Young and Krstic\n  (2019) <doi:10.1016/j.yrtph.2019.104418>.",
    "version": "1.5",
    "maintainer": "Bob Obenchain <wizbob@att.net>",
    "url": "https://www.r-project.org, http://localcontrolstatistics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4653,
    "package_name": "NbClust",
    "title": "Determining the Best Number of Clusters in a Data Set",
    "description": "It provides 30 indexes for determining the optimal number of clusters in a data set and offers the best clustering scheme from different results to the user. ",
    "version": "3.0.1",
    "maintainer": "Malika Charrad <malika.charrad.1@ulaval.ca>",
    "url": "https://sites.google.com/site/malikacharrad/research/nbclust-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4664,
    "package_name": "NetCluster",
    "title": "Clustering for networks",
    "description": "Facilitates network clustering and evaluation of cluster\n        configurations.",
    "version": "0.2",
    "maintainer": "Sean J Westwood <seanjw@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4666,
    "package_name": "NetDA",
    "title": "Network-Based Discriminant Analysis Subject to Multi-Label\nClasses",
    "description": "Implementation of discriminant analysis with network structures in predictors accommodated to do classification and prediction.",
    "version": "0.2.0",
    "maintainer": "Li-Pang Chen <lchen723@nccu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4692,
    "package_name": "NeuralEstimators",
    "title": "Likelihood-Free Parameter Estimation using Neural Networks",
    "description": "An 'R' interface to the 'Julia' package 'NeuralEstimators.jl'. The package facilitates the user-friendly development of neural Bayes estimators, which are neural networks that map data to a point summary of the posterior distribution (Sainsbury-Dale et al., 2024, <doi:10.1080/00031305.2023.2249522>). These estimators are likelihood-free and amortised, in the sense that, once the neural networks are trained on simulated data, inference from observed data can be made in a fraction of the time required by conventional approaches. The package also supports amortised Bayesian or frequentist inference using neural networks that approximate the posterior or likelihood-to-evidence ratio (Zammit-Mangion et al., 2025, Sec. 3.2, 5.2, <doi:10.48550/arXiv.2404.12484>). The package accommodates any model for which simulation is feasible by allowing users to define models implicitly through simulated data.",
    "version": "0.2.0",
    "maintainer": "Matthew Sainsbury-Dale <msainsburydale@gmail.com>",
    "url": "https://github.com/msainsburydale/NeuralEstimators,\nhttps://msainsburydale.github.io/NeuralEstimators.jl/dev/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4693,
    "package_name": "NeuralNetTools",
    "title": "Visualization and Analysis Tools for Neural Networks",
    "description": "Visualization and analysis tools to aid in the interpretation of\n    neural network models.  Functions are available for plotting,\n    quantifying variable importance, conducting a sensitivity analysis, and\n    obtaining a simple list of model weights.",
    "version": "1.5.3",
    "maintainer": "Marcus W. Beck <mbafs2012@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4694,
    "package_name": "NeuralSens",
    "title": "Sensitivity Analysis of Neural Networks",
    "description": "Analysis functions to quantify inputs importance in neural network models.\n  Functions are available for calculating and plotting the inputs importance and obtaining\n  the activation function of each neuron layer and its derivatives. The importance of a given\n  input is defined as the distribution of the derivatives of the output with respect to that\n  input in each training data point <doi:10.18637/jss.v102.i07>.",
    "version": "1.1.3",
    "maintainer": "Jaime Pizarroso Gonzalo <jpizarroso@comillas.edu>",
    "url": "https://github.com/JaiPizGon/NeuralSens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4696,
    "package_name": "NeuroDecodeR",
    "title": "Decode Information from Neural Activity",
    "description": "Neural decoding is method of analyzing neural data that  \n    uses a pattern classifiers to predict experimental conditions based \n    on neural activity. 'NeuroDecodeR' is a system of objects that \n    makes it easy to run neural decoding analyses. For more information\n    on neural decoding see Meyers & Kreiman (2011)\n    <doi:10.7551/mitpress/8404.003.0024>.",
    "version": "0.2.0",
    "maintainer": "Ethan Meyers <ethan.meyers@gmail.com>",
    "url": "https://emeyers.github.io/NeuroDecodeR/,\nhttps://github.com/emeyers/NeuroDecodeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4732,
    "package_name": "Numero",
    "title": "Statistical Framework to Define Subgroups in Complex Datasets",
    "description": "High-dimensional datasets that do not exhibit a clear intrinsic clustered structure pose a challenge to conventional clustering algorithms. For this reason, we developed an unsupervised framework that helps scientists to better subgroup their datasets based on visual cues, please see Gao S, Mutter S, Casey A, Makinen V-P (2019) Numero: a statistical framework to define multivariable subgroups in complex population-based datasets, Int J Epidemiology, 48:369-37, <doi:10.1093/ije/dyy113>. The framework includes the necessary functions to construct a self-organizing map of the data, to evaluate the statistical significance of the observed data patterns, and to visualize the results.",
    "version": "1.10.1",
    "maintainer": "Ville-Petteri Makinen <vpmakine@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4745,
    "package_name": "ODMeans",
    "title": "OD-Means: k-Means for Origin-Destination",
    "description": "OD-means is a hierarchical adaptive k-means algorithm based on origin-destination pairs. \n    In the first layer of the hierarchy, the clusters are separated automatically based on the variation \n    of the within-cluster distance of each cluster until convergence. The second layer of the hierarchy \n    corresponds to the sub clustering process of small clusters based on the distance between the origin \n    and destination of each cluster.",
    "version": "0.2.1",
    "maintainer": "Sebastian Moreno <sebastian.moreno.araya@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4746,
    "package_name": "ODRF",
    "title": "Oblique Decision Random Forest for Classification and Regression",
    "description": "The oblique decision tree (ODT) uses linear combinations of predictors as partitioning variables in a decision tree. Oblique Decision Random Forest (ODRF) is an ensemble of multiple ODTs generated by feature bagging. Oblique Decision Boosting Tree (ODBT) applies feature bagging during the training process of ODT-based boosting trees to ensemble multiple boosting trees. All three methods can be used for classification and regression, and ODT and ODRF serve as supplements to the classical CART of Breiman (1984) <DOI:10.1201/9781315139470> and Random Forest of Breiman (2001) <DOI:10.1023/A:1010933404324> respectively. ",
    "version": "0.0.5",
    "maintainer": "Yu Liu <liuyuchina123@gmail.com>",
    "url": "https://liuyu-star.github.io/ODRF/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4762,
    "package_name": "OOBCurve",
    "title": "Out of Bag Learning Curve",
    "description": "Provides functions to calculate the out-of-bag learning curve for random forests for any measure that is available in the 'mlr' package. Supported random forest packages are 'randomForest' and 'ranger' and trained models of these packages with the train function of 'mlr'. The main function is OOBCurve() that calculates the out-of-bag curve depending on the number of trees. With the OOBCurvePars() function out-of-bag curves can also be calculated for 'mtry', 'sample.fraction' and 'min.node.size' for the 'ranger' package.",
    "version": "0.3",
    "maintainer": "Philipp Probst <philipp_probst@gmx.de>",
    "url": "https://github.com/PhilippPro/OOBCurve",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4774,
    "package_name": "ORCME",
    "title": "Order Restricted Clustering for Microarray Experiments",
    "description": "Provides clustering of genes with similar \n  dose response (or time course) profiles. It implements the method \n  described by Lin et al. (2012).",
    "version": "2.0.2",
    "maintainer": "Rudradev Sengupta <rudradev.sengupta@uhasselt.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4778,
    "package_name": "ORIClust",
    "title": "Order-Restricted Information Criterion-Based Clustering\nAlgorithm",
    "description": "A user-friendly R-based software package for\n        gene clustering. Clusters are given by genes matched to\n        prespecified profiles across various ordered treatment groups.\n        It is particularly useful for analyzing data obtained from\n        short time-course or dose-response microarray experiments.",
    "version": "1.0-2",
    "maintainer": "Tianqing Liu <tianqingliu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4781,
    "package_name": "ORKM",
    "title": "The Online Regularized K-Means Clustering Algorithm",
    "description": "Algorithm of online regularized k-means to deal with online multi(single) view data.\n The philosophy of the package is described in Guo G. (2024) \n <doi:10.1016/j.ins.2024.121133>. ",
    "version": "1.0.0",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4783,
    "package_name": "ORTSC",
    "title": "Connects to Google Cloud API for Label Detection",
    "description": "Connects to Google cloud vision <https://cloud.google.com/vision> to perform label detection and repurpose this feature for image classification.",
    "version": "1.0.0",
    "maintainer": "Mohamed Soudy <MohmedSoudy2009@gmail.com>",
    "url": "https://github.com/MohmedSoudy/ORTSC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4794,
    "package_name": "OTE",
    "title": "Optimal Trees Ensembles for Regression, Classification and Class\nMembership Probability Estimation",
    "description": "Functions for creating ensembles of optimal trees for regression, classification (Khan, Z., Gul, A., Perperoglou, A., Miftahuddin, M., Mahmoud, O., Adler, W., & Lausen, B. (2019). (2019) <doi:10.1007/s11634-019-00364-9>) and class membership probability estimation (Khan, Z, Gul, A, Mahmoud, O, Miftahuddin, M, Perperoglou, A, Adler, W & Lausen, B (2016) <doi:10.1007/978-3-319-25226-1_34>) are given. A few trees are selected from an initial set of trees grown by random forest for the ensemble on the basis of their individual and collective performance. Three different methods of tree selection for the case of classification are given. The prediction functions return estimates of the test responses and their class membership probabilities. Unexplained variations, error rates, confusion matrix, Brier scores, etc. are also returned for the test data.",
    "version": "1.0.1",
    "maintainer": "Zardad Khan <zardadkhan@awkum.edu.pk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4798,
    "package_name": "OTclust",
    "title": "Mean Partition, Uncertainty Assessment, Cluster Validation and\nVisualization Selection for Cluster Analysis",
    "description": "Providing mean partition for ensemble clustering by optimal transport alignment(OTA), uncertainty measures for both partition-wise and cluster-wise assessment and multiple visualization functions to show uncertainty, for instance, membership heat map and plot of covering point set. A partition refers to an overall clustering result. Jia Li, Beomseok Seo, and Lin Lin (2019) <doi:10.1002/sam.11418>. Lixiang Zhang, Lin Lin, and Jia Li (2020) <doi:10.1093/bioinformatics/btaa165>.",
    "version": "1.0.6",
    "maintainer": "Lixiang Zhang <phoelief@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4828,
    "package_name": "OmicsPrepR",
    "title": "Unified Preprocessing Toolkit for Proteomics and Metabolomics",
    "description": "Provides unified workflows for quality control, normalization, and visualization of proteomic and metabolomic data. The package simplifies preprocessing through automated imputation, scaling, and principal component analysis (PCA)-based exploratory analysis, enabling researchers to prepare omics datasets efficiently for downstream statistical and machine learning analyses.",
    "version": "0.1.1",
    "maintainer": "Isaac Osei <ikemillar65@gmail.com>",
    "url": "https://github.com/ikemillar/OmicsPrepR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4844,
    "package_name": "OncoSubtype",
    "title": "Predict Cancer Subtypes Based on TCGA Data using Machine\nLearning Method",
    "description": "Provide functionality for cancer subtyping using nearest centroids or machine learning methods based on TCGA data.",
    "version": "1.0.0",
    "maintainer": "Dadong Zhang <dadong.zhang.shared@gmail.com>",
    "url": "https://github.com/DadongZ/OncoSubtype",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4851,
    "package_name": "OneR",
    "title": "One Rule Machine Learning Classification Algorithm with\nEnhancements",
    "description": "Implements the One Rule (OneR) Machine Learning classification algorithm (Holte, R.C. (1993) <doi:10.1023/A:1022631118932>) with enhancements for sophisticated handling of numeric data and missing values together with extensive diagnostic functions. It is useful as a baseline for machine learning models and the rules are often helpful heuristics.",
    "version": "2.2",
    "maintainer": "Holger von Jouanne-Diedrich <holger.jouanne-diedrich@h-ab.de>",
    "url": "https://github.com/vonjd/OneR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4868,
    "package_name": "OpenRepGrid.ic",
    "title": "Interpretive Clustering for Repertory Grids",
    "description": "Shiny UI to identify cliques of related constructs in repertory grid data. \n    See Burr, King, & Heckmann (2020) <doi:10.1080/14780887.2020.1794088> for a description \n    of the interpretive clustering (IC) method.",
    "version": "0.6.2",
    "maintainer": "Mark Heckmann <heckmann.mark@gmail.com>",
    "url": "https://github.com/markheckmann/OpenRepGrid.ic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4928,
    "package_name": "PAMhm",
    "title": "Generate Heatmaps Based on Partitioning Around Medoids (PAM)",
    "description": "Data are partitioned (clustered) into k clusters \"around medoids\", which is\n    a more robust version of K-means implemented in the function pam() in the 'cluster' package.\n    The PAM algorithm is described in Kaufman and Rousseeuw (1990) <doi:10.1002/9780470316801>.\n    Please refer to the pam() function documentation for more references.\n    Clustered data is plotted as a split heatmap allowing visualisation of representative\n    \"group-clusters\" (medoids) in the data as separated fractions of the graph while those\n    \"sub-clusters\" are visualised as a traditional heatmap based on hierarchical clustering.",
    "version": "0.1.2",
    "maintainer": "Vidal Fey <vidal.fey@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4961,
    "package_name": "PCDimension",
    "title": "Finding the Number of Significant Principal Components",
    "description": "Implements methods to automate the Auer-Gervini graphical\n  Bayesian approach for determining the number of significant\n  principal components. Automation uses clustering, change points, or\n  simple statistical models to distinguish \"long\" from \"short\" steps\n  in a graph showing the posterior number of components as a function\n  of a prior parameter. See <doi:10.1101/237883>.",
    "version": "1.1.14",
    "maintainer": "Kevin R. Coombes <krc@silicovore.com>",
    "url": "http://oompa.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4973,
    "package_name": "PCRedux",
    "title": "Quantitative Polymerase Chain Reaction (qPCR) Data Mining and\nMachine Learning Toolkit as Described in Burdukiewicz (2022)\n<doi:10.21105/Joss.04407>",
    "description": "Extracts features from amplification curve data of quantitative \n    Polymerase Chain Reactions (qPCR) according to Pabinger et al. 2014 \n    <doi:10.1016/j.bdq.2014.08.002> for machine learning purposes. Helper \n    functions prepare the amplification curve data for processing as functional \n    data (e.g., Hausdorff distance) or enable the plotting of amplification \n    curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() \n    functions of Burdukiewicz et al. (2018) <doi:10.1016/j.bdq.2018.08.001> \n    can be used to predict amplification curves with an hook effect-like \n    curvature. The pcrfit_single() function can be used to extract features \n    from an amplification curve.",
    "version": "1.2-1",
    "maintainer": "Andrej-Nikolai Spiess <draspiess@gmail.com>",
    "url": "https://CRAN.R-project.org/package=PCRedux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4989,
    "package_name": "PDtoolkit",
    "title": "Collection of Tools for PD Rating Model Development and\nValidation",
    "description": "The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.",
    "version": "1.2.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "url": "https://github.com/andrija-djurovic/PDtoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4990,
    "package_name": "PEAXAI",
    "title": "Probabilistic Efficiency Analysis Using Explainable Artificial\nIntelligence",
    "description": "Provides a probabilistic framework that integrates Data Envelopment\n  Analysis (DEA) (Banker et al., 1984) <doi:10.1287/mnsc.30.9.1078> with machine\n  learning classifiers (Kuhn, 2008) <doi:10.18637/jss.v028.i05> to estimate both the\n  (in)efficiency status and the probability of efficiency for decision-making\n  units. The approach trains predictive models on DEA-derived efficiency labels\n  (Charnes et al., 1985) <doi:10.1016/0304-4076(85)90133-2>, enabling explainable\n  artificial intelligence (XAI) workflows with global and local interpretability\n  tools, including permutation importance (Molnar et al., 2018) <doi:10.21105/joss.00786>,\n  Shapley value explanations (Strumbelj & Kononenko, 2014) <doi:10.1007/s10115-013-0679-x>,\n  and sensitivity analysis (Cortez, 2011) <https://CRAN.R-project.org/package=rminer>.\n  The framework also supports probability-threshold peer selection and counterfactual\n  improvement recommendations for benchmarking and policy evaluation. The probabilistic\n  efficiency framework is detailed in González-Moyano et al. (2025)\n  \"Probability-based Technical Efficiency Analysis through Machine Learning\",\n  in review for publication.",
    "version": "0.1.0",
    "maintainer": "Ricardo González Moyano <ricardo.gonzalezm@umh.es>",
    "url": "https://github.com/rgonzalezmoyano/PEAXAI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5061,
    "package_name": "PHclust",
    "title": "Poisson Hurdle Clustering for Sparse Microbiome Data",
    "description": "Clustering analysis for sparse microbiome data, based on a Poisson hurdle model.",
    "version": "0.1.0",
    "maintainer": "Zhili Qiao <zlqiao@iastate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5069,
    "package_name": "PINstimation",
    "title": "Estimation of the Probability of Informed Trading",
    "description": "A comprehensive bundle of utilities for the estimation of probability of informed trading models: original PIN in Easley and O'Hara (1992) and Easley et al. (1996); Multilayer PIN (MPIN) in Ersan (2016); Adjusted PIN (AdjPIN) in Duarte and Young (2009); and volume-synchronized PIN (VPIN) in Easley et al. (2011, 2012). Implementations of various estimation methods suggested in the literature are included. Additional compelling features comprise posterior probabilities, an implementation of an expectation-maximization (EM) algorithm, and PIN decomposition into layers, and into bad/good components. Versatile data simulation tools, and trade classification algorithms are among the supplementary utilities. The package provides fast, compact, and precise utilities to tackle the sophisticated, error-prone, and time-consuming estimation procedure of informed trading, and this solely using the raw trade-level data. ",
    "version": "0.2.0",
    "maintainer": "Montasser Ghachem <montasser.ghachem@pinstimation.com>",
    "url": "https://www.pinstimation.com,\nhttps://github.com/monty-se/PINstimation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5076,
    "package_name": "PKLMtest",
    "title": "Classification Based MCAR Test",
    "description": "Implementation of a KL-based (Kullback-Leibler) test for MCAR (Missing Completely At Random) in the context of missing data as introduced in Michel et al. (2021)  <arXiv:2109.10150>. ",
    "version": "1.0.1",
    "maintainer": "Meta-Lina Spohn <metalina.spohn@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5105,
    "package_name": "PND.heter.cluster",
    "title": "Estimating the Cluster Specific Treatment Effects in Partially\nNested Designs",
    "description": "Implements the methods for assessing heterogeneous cluster-specific treatment effects in partially nested designs as described in Liu (2024) <doi:10.1037/met0000723>. The estimation uses the multiply robust method, allowing for the use of machine learning methods in model estimation (e.g., random forest, neural network, and the super learner ensemble).  Partially nested designs (also known as partially clustered designs) are designs where individuals in the treatment arm are assigned to clusters (e.g., teachers, tutoring groups, therapists), whereas individuals in the control arm have no such clustering. ",
    "version": "0.1.0",
    "maintainer": "Xiao Liu <xiao.liu@austin.utexas.edu>",
    "url": "https://github.com/xliu12/PND.heter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5118,
    "package_name": "POPInf",
    "title": "Assumption-Lean and Data-Adaptive Post-Prediction Inference",
    "description": "Implementation of assumption-lean and data-adaptive post-prediction inference (POPInf), for valid and efficient statistical inference based on data predicted by machine learning. See Miao, Miao, Wu, Zhao, and Lu (2023) <arXiv:2311.14220>.",
    "version": "1.0.0",
    "maintainer": "Jiacheng Miao <jiacheng.miao@wisc.edu>",
    "url": "https://arxiv.org/abs/2311.14220,\nhttps://github.com/qlu-lab/POPInf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5128,
    "package_name": "PPCI",
    "title": "Projection Pursuit for Cluster Identification",
    "description": "Implements recently developed projection \n    pursuit algorithms for finding optimal linear cluster\n    separators. The clustering algorithms use optimal\n    hyperplane separators based on minimum density, Pavlidis et. al (2016) <http://jmlr.org/papers/volume17/15-307/15-307.pdf>;\n    minimum normalised cut, Hofmeyr (2017) <doi:10.1109/TPAMI.2016.2609929>;\n    and maximum variance ratio clusterability, Hofmeyr and Pavlidis (2015) <doi:10.1109/SSCI.2015.116>.",
    "version": "0.1.5",
    "maintainer": "David Hofmeyr <dhofmeyr@sun.ac.za>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5137,
    "package_name": "PPforest",
    "title": "Projection Pursuit Classification Forest",
    "description": "Implements projection pursuit forest algorithm for supervised classification.",
    "version": "0.2.0",
    "maintainer": "Natalia da Silva <natalia.dasilva@fcea.edu.uy>",
    "url": "https://github.com/natydasilva/PPforest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5138,
    "package_name": "PPtreeViz",
    "title": "Projection Pursuit Classification Tree Visualization",
    "description": "Tools for exploring projection pursuit classification tree using\n    various projection pursuit indexes.",
    "version": "2.0.4",
    "maintainer": "Eun-Kyung Lee <lee.eunk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5206,
    "package_name": "PUPMSI",
    "title": "Moisture Sorption Isotherm Modeling Program",
    "description": "Contains sixteen moisture sorption isotherm models, which evaluate the fitness of adsorption and desorption curves for further understanding of the relationship between moisture content and water activity. Fitness evaluation is conducted through parameter estimation and error analysis. Moreover, graphical representation, hysteresis area estimation, and isotherm classification through the equation of Blahovec & Yanniotis (2009) <doi:10.1016/j.jfoodeng.2008.08.007> which is based on the classification system introduced by Brunauer et. al. (1940) <doi:10.1021/ja01864a025> are also included for the visualization of models and hysteresis.",
    "version": "0.1.0",
    "maintainer": "Chester Deocaris <ccdeocaris@pup.edu.ph>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5251,
    "package_name": "ParamHelpers",
    "title": "Helpers for Parameters in Black-Box Optimization, Tuning and\nMachine Learning",
    "description": "Functions for parameter descriptions and operations in\n    black-box optimization, tuning and machine learning. Parameters can be\n    described (type, constraints, defaults, etc.), combined to parameter\n    sets and can in general be programmed on. A useful OptPath object\n    (archive) to log function evaluations is also provided.",
    "version": "1.14.2",
    "maintainer": "Martin Binder <mlr.developer@mb706.com>",
    "url": "https://paramhelpers.mlr-org.com,\nhttps://github.com/mlr-org/ParamHelpers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5264,
    "package_name": "PatientLevelPrediction",
    "title": "Develop Clinical Prediction Models Using the Common Data Model",
    "description": "A user friendly way to create patient level prediction models using\n  the Observational Medical Outcomes Partnership Common Data Model. Given a cohort\n  of interest and an outcome of interest, the package can use data in the Common\n  Data Model to build a large set of features. These features can then be used to\n  fit a predictive model with a number of machine learning algorithms. This is\n  further described in Reps (2017) <doi:10.1093/jamia/ocy032>.",
    "version": "6.5.1",
    "maintainer": "Egill Fridgeirsson <e.fridgeirsson@erasmusmc.nl>",
    "url": "https://ohdsi.github.io/PatientLevelPrediction/,\nhttps://github.com/OHDSI/PatientLevelPrediction",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5292,
    "package_name": "PerfMeas",
    "title": "Performance Measures for Ranking and Classification Tasks",
    "description": "Implementation of different performance measures for classification and ranking tasks  including  Area Under the Receiving Characteristic Curve (AUROC) and Area Under the Precision Recall Curve (AUPRC), precision at a given recall, F-score for single and multiple classes.",
    "version": "1.2.5",
    "maintainer": "Giorgio Valentini <valentini@di.unimi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5311,
    "package_name": "PheCAP",
    "title": "High-Throughput Phenotyping with EHR using a Common Automated\nPipeline",
    "description": "Implement surrogate-assisted feature extraction (SAFE) and\n             common machine learning approaches to train and validate phenotyping models.\n             Background and details about the methods can be found at \n             Zhang et al. (2019) <doi:10.1038/s41596-019-0227-6>,\n             Yu et al. (2017) <doi:10.1093/jamia/ocw135>, and \n             Liao et al. (2015) <doi:10.1136/bmj.h1885>.",
    "version": "1.2.1",
    "maintainer": "PARSE LTD <software@parse-health.org>",
    "url": "https://celehs.github.io/PheCAP/, https://github.com/celehs/PheCAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5321,
    "package_name": "PhitestR",
    "title": "Analyzing the Heterogeneity of Single-Cell Populations",
    "description": "A bioinformatics method developed for analyzing the heterogeneity of single-cell populations. Phitest provides an objective and automatic method to evaluate the performance of clustering and quality of cell clusters.",
    "version": "0.2.0",
    "maintainer": "Wei Vivian Li <vivian.li@rutgers.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5329,
    "package_name": "PhysActBedRest",
    "title": "Marks Periods of 'Bedrest' in Actigraph Accelerometer Data",
    "description": "Contains a function to categorize accelerometer readings collected in free-living (e.g., for 24 hours/day for 7 days), preprocessed and compressed as counts (unit-less value) in a specified time period termed epoch (e.g., 1 minute) as either bedrest (sleep) or active.  The input is a matrix with a timestamp column and a column with number of counts per epoch. The output is the same dataframe with an additional column termed bedrest. In the bedrest column each line (epoch) contains a function-generated classification 'br' or 'a' denoting bedrest/sleep and activity, respectively.  The package is designed to be used after wear/nonwear marking function in the 'PhysicalActivity' package.  Version 1.1 adds preschool thresholds and corrects for possible errors in algorithm implementation.    ",
    "version": "1.1",
    "maintainer": "J. Dustin Tracy <tracy@chapman.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5330,
    "package_name": "PhysicalActivity",
    "title": "Process Accelerometer Data for Physical Activity Measurement",
    "description": "It provides a function \"wearingMarking\" for classification of monitor\n    wear and nonwear time intervals in accelerometer data collected to assess\n    physical activity. The package also contains functions for making plot for \n    accelerometer data and obtaining the summary of various information including \n    daily monitor wear time and the mean monitor wear time during valid days.      \n    \"deliveryPred\" and \"markDelivery\" can classify days for ActiGraph delivery by mail;\n    \"deliveryPreprocess\" can process accelerometry data for analysis by zeropadding incomplete \n    days and removing low activity days; \"markPAI\" can categorize physical activity\n    intensity level based on user-defined cut-points of accelerometer counts. It also\n    supports importing ActiGraph AGD files with \"readActigraph\" and \"queryActigraph\" functions.",
    "version": "0.2-4",
    "maintainer": "Leena Choi <leena.choi@Vanderbilt.Edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5428,
    "package_name": "PredPsych",
    "title": "Predictive Approaches in Psychology",
    "description": "\n    Recent years have seen an increased interest in novel methods\n    for analyzing quantitative data from experimental psychology. Currently, however, they lack an\n    established and accessible software framework. Many existing implementations provide no guidelines,\n    consisting of small code snippets, or sets of packages. In addition, the use of existing packages\n    often requires advanced programming experience. 'PredPsych' is a user-friendly toolbox based on\n    machine learning predictive algorithms. It comprises of multiple functionalities for multivariate\n    analyses of quantitative behavioral data based on machine learning models.",
    "version": "0.5",
    "maintainer": "Atesh Koul <atesh.koul@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5436,
    "package_name": "PriceIndices",
    "title": "Calculating Bilateral and Multilateral Price Indexes",
    "description": "Preparing a scanner data set for price dynamics calculations (data selecting, data classification, data matching, data filtering). Computing bilateral and multilateral indexes. For details on these methods see: Diewert and Fox (2020) \n    <doi:10.1080/07350015.2020.1816176>, Białek (2019) <doi:10.2478/jos-2019-0014> or Białek (2020) <doi:10.2478/jos-2020-0037>.",
    "version": "0.2.6",
    "maintainer": "Jacek Białek <jacek.bialek@uni.lodz.pl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5455,
    "package_name": "ProcData",
    "title": "Process Data Analysis",
    "description": "Provides tools for exploratory process data analysis. Process data refers to the data describing\n    participants' problem-solving processes in computer-based assessments. It is often recorded in computer\n    log files. This package provides functions to read, process, and write process data. It also implements\n    two feature extraction methods to compress the information stored in process data into standard \n    numerical vectors. This package also provides recurrent neural network based models that relate response processes \n    with other binary or scale variables of interest. The functions that involve training and evaluating neural networks \n    are wrappers of functions in 'keras'.",
    "version": "0.3.2",
    "maintainer": "Xueying Tang <xueyingtang1989@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5459,
    "package_name": "ProfileGLMM",
    "title": "Bayesian Profile Regression using Generalised Linear Mixed\nModels",
    "description": "Implements a Bayesian profile regression using a generalized linear mixed model as output model. The package allows for binary (probit mixed model) and continuous (linear mixed model) outcomes and both continuous and categorical clustering variables. The package utilizes 'RcppArmadillo' and 'RcppDist' for high-performance statistical computing in C++. For more details see Amestoy & al. (2025) <doi:10.48550/arXiv.2510.08304>.",
    "version": "1.0.2",
    "maintainer": "Matteo Amestoy <m.amestoy@amsterdamumc.nl>",
    "url": "https://github.com/MatteoAmestoy/ProfileGLMM-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5464,
    "package_name": "ProjectionBasedClustering",
    "title": "Projection Based Clustering",
    "description": "A clustering approach applicable to every projection method is proposed here. The two-dimensional scatter plot of any projection method can construct a topographic map which displays unapparent data structures by using distance and density information of the data. The generalized U*-matrix renders this visualization in the form of a topographic map, which can be used to automatically define the clusters of high-dimensional data. The whole system is based on Thrun and Ultsch, \"Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data\" <DOI:10.1007/s00357-020-09373-2>. Selecting the correct projection method will result in a visualization in which mountains surround each cluster. The number of clusters can be determined by counting valleys on the topographic map. Most projection methods are wrappers for already available methods in R. By contrast, the neighbor retrieval visualizer (NeRV) is based on C++ source code of the 'dredviz' software package, and the Curvilinear Component Analysis (CCA) is translated from 'MATLAB' ('SOM Toolbox' 2.0) to R.",
    "version": "1.2.2",
    "maintainer": "Michael Thrun <m.thrun@gmx.net>",
    "url": "https://www.deepbionics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5468,
    "package_name": "PropClust",
    "title": "Propensity Clustering and Decomposition",
    "description": "Implementation of propensity clustering and\n        decomposition as described in Ranola et al. (2013) <doi:10.1186/1752-0509-7-21>. \n        Propensity decomposition can be viewed on the\n        one hand as a generalization of the eigenvector-based\n        approximation of correlation networks, and on the other hand as\n        a generalization of random multigraph models and\n        conformity-based decompositions.",
    "version": "1.4-7",
    "maintainer": "Peter Langfelder <Peter.Langfelder@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5476,
    "package_name": "ProxReg",
    "title": "Linear Models for Prediction and Classification using Proximal\nOperators",
    "description": "Implements optimization techniques for Lasso regression, R.Tibshirani(1996)<doi:10.1111/j.2517-6161.1996.tb02080.x> using Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) and Iterative Shrinkage-Thresholding Algorithm (ISTA) based on proximal operators, A.Beck(2009)<doi:10.1137/080716542>. The package is useful for high-dimensional regression problems and includes cross-validation procedures to select optimal penalty parameters.",
    "version": "1.1.2",
    "maintainer": "YingHong Chen <yinghongchen1402@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5565,
    "package_name": "QuanDA",
    "title": "Quantile-Based Discriminant Analysis for High-Dimensional\nImbalanced Classification",
    "description": "Implements quantile-based discriminant analysis (QuanDA) for\n    imbalanced classification in high-dimensional, low-sample-size settings.\n    The method fits penalized quantile regression directly on discrete class labels and\n    tunes the quantile level to reflect class imbalance.",
    "version": "1.0.0",
    "maintainer": "Qian Tang <tang1015@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5568,
    "package_name": "QuantNorm",
    "title": "Mitigating the Adverse Impact of Batch Effects in Sample Pattern\nDetection",
    "description": "Modifies the distance matrix obtained from data with batch effects, so as to improve the performance of sample pattern detection, such as clustering, dimension reduction, and construction of networks between subjects. The method has been published in Bioinformatics (Fei et al, 2018, <doi:10.1093/bioinformatics/bty117>). Also available on 'GitHub' <https://github.com/tengfei-emory/QuantNorm>.",
    "version": "1.0.5",
    "maintainer": "Teng Fei <tfei@emory.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5593,
    "package_name": "R.temis",
    "title": "Integrated Text Mining Solution",
    "description": "An integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, lexical summary, terms\n    co-occurrences and documents similarity measures, graphs of terms,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",
    "version": "0.1.4",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5629,
    "package_name": "RAFS",
    "title": "Robust Aggregative Feature Selection",
    "description": "A cross-validated minimal-optimal feature selection algorithm.\n It utilises popularity counting, hierarchical clustering with feature dissimilarity measures,\n and prefiltering with all-relevant feature selection method to obtain the minimal-optimal set of features.",
    "version": "0.2.5",
    "maintainer": "Radosław Piliszek <radek@piliszek.it>",
    "url": "https://www.mdfs.it/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5688,
    "package_name": "RCTS",
    "title": "Clustering Time Series While Resisting Outliers",
    "description": "Robust Clustering of Time Series (RCTS) has the functionality to cluster time series using both the classical and the robust interactive fixed effects framework. \n  The classical framework is developed in Ando & Bai (2017) <doi:10.1080/01621459.2016.1195743>. The implementation within this package excludes the SCAD-penalty on the estimations of beta. \n  This robust framework is developed in Boudt & Heyndels (2022) <doi:10.1016/j.ecosta.2022.01.002> and is made robust against different kinds of outliers.\n  The algorithm iteratively updates beta (the coefficients of the observable variables), group membership, and the latent factors (which can be common and/or group-specific) along\n  with their loadings. The number of groups and factors can be estimated if they are unknown.",
    "version": "0.2.4",
    "maintainer": "Ewoud Heyndels <ewoud.heyndels@vub.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5709,
    "package_name": "RDHonest",
    "title": "Honest Inference in Regression Discontinuity Designs",
    "description": "Honest and nearly-optimal confidence intervals in fuzzy and sharp\n    regression discontinuity designs and for inference at a point based on local\n    linear regression. The implementation is based on Armstrong and Kolesár (2018)\n    <doi:10.3982/ECTA14434>, and Kolesár and Rothe (2018)\n    <doi:10.1257/aer.20160945>. Supports covariates, clustering, and weighting.",
    "version": "1.0.1",
    "maintainer": "Michal Kolesár <kolesarmi@googlemail.com>",
    "url": "https://github.com/kolesarm/RDHonest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5760,
    "package_name": "RFclust",
    "title": "Random Forest Cluster Analysis",
    "description": "Tools to perform random forest consensus clustering of different data types. The package is designed to accept a list of matrices from different assays, typically from high-throughput molecular profiling so that class discovery may be jointly performed. For references, please see Tao Shi & Steve Horvath (2006) <doi:10.1198/106186006X94072> & Monti et al (2003) <doi:10.1023/A:1023949509487> .",
    "version": "0.1.2",
    "maintainer": "Ankur Chakravarthy <ankur.chakravarthy.10@ucl.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5762,
    "package_name": "RFlocalfdr",
    "title": "Significance Level for Random Forest Impurity Importance Scores",
    "description": "Sets a significance level for  Random Forest MDI (Mean Decrease in Impurity, Gini or\n             sum of squares) variable importance scores, using an empirical Bayes approach.\n             See Dunne et al. (2022)  <doi:10.1101/2022.04.06.487300>.",
    "version": "0.9",
    "maintainer": "Robert Dunne <rob.dunne@csiro.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5763,
    "package_name": "RFlocalfdr.data",
    "title": "Data for the Vignette and Examples in 'RFlocalfdr'",
    "description": "Data for the vignette and examples in 'RFlocalfdr'. Contains a dataset of 1103547 importance values,\n       and the table of variables used in the random forest splits. The data is Chromosome 22 taken from Auton et al.\n       (2015) <doi:10.1038/nature15393>. It also contains a 51 samples by 22283 genes data set taken from\n       Spira et al. (2004) <doi:10.1165/rcmb.2004-0273OC>.",
    "version": "0.0.3",
    "maintainer": "Robert Dunne <rob.dunne@csiro.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5766,
    "package_name": "RGAN",
    "title": "Generative Adversarial Nets (GAN) in R",
    "description": "An easy way to get started with Generative Adversarial Nets (GAN) in R. The GAN algorithm was initially \n    described by Goodfellow et al. 2014 <https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf>. A GAN can be used to learn the joint distribution of complex data by \n    comparison. A GAN consists of two neural networks a Generator and a Discriminator, where the two\n    neural networks play an adversarial minimax game.\n    Built-in GAN models make the training of GANs in R possible in one line and make it easy to \n    experiment with different design choices (e.g. different network architectures, value functions, optimizers).\n    The built-in GAN models work with tabular data (e.g. to produce synthetic data) and image data. \n    Methods to post-process the output of GAN models to enhance the quality of samples are available.",
    "version": "0.1.1",
    "maintainer": "Marcel Neunhoeffer <marcel.neunhoeffer@gmail.com>",
    "url": "https://github.com/mneunhoe/RGAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5774,
    "package_name": "RGF",
    "title": "Regularized Greedy Forest",
    "description": "Regularized Greedy Forest wrapper of the 'Regularized Greedy Forest' <https://github.com/RGF-team/rgf/tree/master/python-package> 'python' package, which also includes a Multi-core implementation (FastRGF) <https://github.com/RGF-team/rgf/tree/master/FastRGF>.",
    "version": "1.1.1",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "url": "https://github.com/RGF-team/rgf/tree/master/R-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5793,
    "package_name": "RHPCBenchmark",
    "title": "Benchmarks for High-Performance Computing Environments",
    "description": "Microbenchmarks for determining the run time\n  performance of aspects of the R programming environment and packages\n  relevant to high-performance computation.  The benchmarks are divided into\n  three categories: dense matrix linear algebra kernels, sparse matrix linear\n  algebra kernels, and machine learning functionality.",
    "version": "0.1.0",
    "maintainer": "James McCombs <jmccombs@iu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5814,
    "package_name": "RInSp",
    "title": "R Individual Specialization",
    "description": "Functions to calculate several ecological indices of individual \n    and population niche width (Araujo's E, clustering and pairwise similarity \n    among individuals, IS, Petraitis' W, and Roughgarden's WIC/TNW) to assess \n    individual specialization based on data of resource use. Resource use can \n    be quantified by counts of categories, measures of mass or length, or \n    proportions. Monte Carlo resampling procedures are available for hypothesis \n    testing against multinomial null models.\n    Details are provided in Zaccarelli et al. (2013) <doi:10.1111/2041-210X.12079>\n    and associated references.",
    "version": "1.2.5",
    "maintainer": "Dr. Nicola Zaccarelli <nicola.zaccarelli@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5827,
    "package_name": "RKEEL",
    "title": "Using 'KEEL' in R Code",
    "description": "'KEEL' is a popular 'Java' software for a large number of different knowledge data discovery tasks.\n    This package takes the advantages of 'KEEL' and R, allowing to use 'KEEL' algorithms in simple R code.\n    The implemented R code layer between R and 'KEEL' makes easy both using 'KEEL' algorithms in R as implementing new algorithms for 'RKEEL' in a very simple way.\n    It includes more than 100 algorithms for classification, regression, preprocess, association rules and imbalance learning, which allows a more complete experimentation process.\n    For more information about 'KEEL', see <http://www.keel.es/>.",
    "version": "1.3.4",
    "maintainer": "Jose M. Moyano <jmoyano1@us.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5835,
    "package_name": "RLT",
    "title": "Reinforcement Learning Trees",
    "description": "Random forest with a variety of additional features for regression, classification and survival analysis. \n             The features include: parallel computing with OpenMP, embedded model for selecting the splitting variable,\n             based on Zhu, Zeng & Kosorok (2015) <doi:10.1080/01621459.2015.1036994>, subject weight, variable weight, \n             tracking subjects used in each tree, etc.",
    "version": "3.2.6",
    "maintainer": "Ruoqing Zhu <teazrq@gmail.com>",
    "url": "https://cran.r-project.org/package=RLT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5848,
    "package_name": "RMCC",
    "title": "Airborne LiDAR Filtering Method Based on Multiscale Curvature",
    "description": "Multiscale Curvature Classification of ground returns in 3-D LiDAR \n    point clouds, designed for forested environments. 'RMCC' is a porting to R of the \n    'MCC-lidar' method by Evans and Hudak (2007) <doi:10.1109/TGRS.2006.890412>.",
    "version": "0.1.2",
    "maintainer": "Jean-Romain Roussel <info@r-lidar.com>",
    "url": "https://github.com/r-lidar/RMCC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5855,
    "package_name": "RMOA",
    "title": "Connect R with MOA for Massive Online Analysis",
    "description": "Connect R with MOA (Massive Online Analysis -\n    <https://moa.cms.waikato.ac.nz/>) to build classification models and\n    regression models on streaming data or out-of-RAM data.\n    Also streaming recommendation models are made available.",
    "version": "1.1.0",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "http://www.bnosac.be, https://github.com/jwijffels/RMOA,\nhttps://moa.cms.waikato.ac.nz/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5864,
    "package_name": "RMTL",
    "title": "Regularized Multi-Task Learning",
    "description": "Efficient solvers for 10 regularized multi-task learning algorithms applicable for regression, classification, joint feature selection, task clustering, low-rank learning, sparse learning and network incorporation. Based on the accelerated gradient descent method, the algorithms feature a state-of-art computational complexity O(1/k^2). Sparse model structure is induced by the solving the proximal operator. The detail of the package is described in the paper of Han Cao and Emanuel Schwarz (2018) <doi:10.1093/bioinformatics/bty831>.",
    "version": "0.9.9",
    "maintainer": "Han Cao <hank9cao@gmail.com>",
    "url": "https://github.com/transbioZI/RMTL/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5869,
    "package_name": "RMaCzek",
    "title": "Czekanowski's Diagrams",
    "description": "Allows for production of Czekanowski's Diagrams with clusters. See K. Bartoszek, A. Vasterlund (2020) <doi:10.2478/bile-2020-0008> and K. Bartoszek, Y. Luo (2023) <doi:10.14708/ma.v51i2.7259>. The suggested 'FuzzyDBScan' package (which allows for fuzzy clustering) can be obtained from\n    <https://github.com/henrifnk/FuzzyDBScan/> (or from CRAN's Archive <https://cran.r-project.org/src/contrib/Archive/FuzzyDBScan/>).",
    "version": "1.6.1",
    "maintainer": "Krzysztof Bartoszek <krzbar@protonmail.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5908,
    "package_name": "ROCR",
    "title": "Visualizing the Performance of Scoring Classifiers",
    "description": "ROC graphs, sensitivity/specificity curves, lift charts,\n  and precision/recall plots are popular examples of trade-off\n  visualizations for specific pairs of performance measures. ROCR is a\n  flexible tool for creating cutoff-parameterized 2D performance curves\n  by freely combining two from over 25 performance measures (new\n  performance measures can be added using a standard interface).\n  Curves from different cross-validation or bootstrapping runs can be\n  averaged by different methods, and standard deviations, standard\n  errors or box plots can be used to visualize the variability across\n  the runs. The parameterization can be visualized by printing cutoff\n  values at the corresponding curve positions, or by coloring the\n  curve according to cutoff. All components of a performance plot can\n  be quickly adjusted using a flexible parameter dispatching\n  mechanism. Despite its flexibility, ROCR is easy to use, with only\n  three commands and reasonable default values for all optional\n  parameters.",
    "version": "1.0-11",
    "maintainer": "Felix G.M. Ernst <felix.gm.ernst@outlook.com>",
    "url": "http://ipa-tys.github.io/ROCR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5912,
    "package_name": "ROCit",
    "title": "Performance Assessment of Binary Classifier with Visualization",
    "description": "Sensitivity (or recall or true positive rate), false positive rate, specificity, precision (or positive predictive value), negative predictive value, misclassification rate, accuracy, F-score- these are popular metrics for assessing performance of binary classifier for certain threshold. These metrics are calculated at certain threshold values. Receiver operating characteristic (ROC) curve is a common tool for assessing overall diagnostic ability of the binary classifier. Unlike depending on a certain threshold, area under ROC curve (also known as AUC), is a summary statistic about how well a binary classifier performs overall for the classification task. ROCit package provides flexibility to easily evaluate threshold-bound metrics. Also, ROC curve, along with AUC, can be obtained using different methods, such as empirical, binormal and non-parametric. ROCit encompasses a wide variety of methods for constructing confidence interval of ROC curve and AUC. ROCit also features the option of constructing empirical gains table, which is a handy tool for direct marketing. The package offers options for commonly used visualization, such as, ROC curve, KS plot, lift plot. Along with in-built default graphics setting, there are rooms for manual tweak by providing the necessary values as function arguments. ROCit is a powerful tool offering a range of things, yet it is very easy to use. ",
    "version": "2.1.2",
    "maintainer": "Md Riaz Ahmed Khan <mdriazahmed.khan@jacks.sdstate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5946,
    "package_name": "ROSE",
    "title": "Random Over-Sampling Examples",
    "description": "Functions to deal with binary classification\n  problems in the presence of imbalanced classes. Synthetic balanced samples are  \n  generated according to ROSE (Menardi and Torelli, 2013).  \n  Functions that implement more traditional remedies to the class imbalance\n  are also provided, as well as different metrics to evaluate a learner accuracy.\n  These are estimated by holdout, bootstrap or cross-validation methods. ",
    "version": "0.0-4",
    "maintainer": "Nicola Lunardon <nicola.lunardon@unimib.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5964,
    "package_name": "RPEnsemble",
    "title": "Random Projection Ensemble Classification",
    "description": "Implements the methodology of \"Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959--1035\". The random projection ensemble classifier is a general method for classification of high-dimensional data, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower-dimensional space. The random projections are divided into non-overlapping blocks, and within each block the projection yielding the smallest estimate of the test error is selected. The random projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections, with a data-driven voting threshold to determine the final assignment. ",
    "version": "0.5",
    "maintainer": "Timothy I. Cannings <timothy.cannings@ed.ac.uk>",
    "url": "https://arxiv.org/abs/1504.04595,\nhttps://www.maths.ed.ac.uk/~tcannings/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5965,
    "package_name": "RPIV",
    "title": "Residual Prediction Test for Well-Specification of Instrumental\nVariable Models",
    "description": "A test for the well-specification of the linear instrumental\n    variable model. The test is based on trying to predict the residuals of a\n    two-stage least-squares regression using a random forest. Details can be\n    found in Scheidegger, Londschien and Bühlmann (2025) \"A residual prediction\n    test for the well-specification of linear instrumental variable models\" \n    <doi:10.48550/arXiv.2506.12771>.",
    "version": "1.0.0",
    "maintainer": "Cyrill Scheidegger <cyrill.scheidegger@stat.math.ethz.ch>",
    "url": "https://github.com/cyrillsch/RPIV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5967,
    "package_name": "RPMM",
    "title": "Recursively Partitioned Mixture Model",
    "description": "\n    Recursively Partitioned Mixture Model for Beta and Gaussian Mixtures.  \n    This is a model-based clustering algorithm that returns a hierarchy\n    of classes, similar to hierarchical clustering, but also similar to\n    finite mixture models.",
    "version": "1.25",
    "maintainer": "E. Andres Houseman <eahouseman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5990,
    "package_name": "RRF",
    "title": "Regularized Random Forest",
    "description": "Feature Selection with Regularized Random Forest. This\n    package is based on the 'randomForest' package by Andy Liaw.\n    The key difference is the RRF() function that builds a\n    regularized random forest. Fortran original by Leo Breiman \n    and Adele Cutler, R port by Andy Liaw and Matthew Wiener, \n    Regularized random forest for classification by Houtao Deng, \n    Regularized random forest for regression by Xin Guan.\n    Reference: Houtao Deng (2013) <doi:10.48550/arXiv.1306.0237>.",
    "version": "1.9.4.1",
    "maintainer": "Houtao Deng <softwaredeng@gmail.com>",
    "url": "https://sites.google.com/site/houtaodeng/rrf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6012,
    "package_name": "RSDA",
    "title": "R to Symbolic Data Analysis",
    "description": "Symbolic Data Analysis (SDA) was proposed by professor Edwin Diday in 1987, the main purpose of SDA is to substitute the set of rows (cases) in the data table for  a concept (second order statistical unit). This package implements, to the symbolic case, certain techniques of automatic classification, as well as some linear models.",
    "version": "3.2.5",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "url": "https://oldemarrodriguez.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6018,
    "package_name": "RSKC",
    "title": "Robust Sparse K-Means",
    "description": "This RSKC package contains a function RSKC which runs the robust sparse K-means clustering algorithm.",
    "version": "2.4.2",
    "maintainer": "Yumi Kondo <y.kondo@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6019,
    "package_name": "RSNNS",
    "title": "Neural Networks using the Stuttgart Neural Network Simulator\n(SNNS)",
    "description": "The Stuttgart Neural Network Simulator (SNNS) is a library\n    containing many standard implementations of neural networks. This\n    package wraps the SNNS functionality to make it available from\n    within R. Using the 'RSNNS' low-level interface, all of the\n    algorithmic functionality and flexibility of SNNS can be accessed.\n    Furthermore, the package contains a convenient high-level\n    interface, so that the most common neural network topologies and\n    learning algorithms integrate seamlessly into R.",
    "version": "0.4-17",
    "maintainer": "Christoph Bergmeir <c.bergmeir@decsai.ugr.es>",
    "url": "https://github.com/cbergmeir/RSNNS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6022,
    "package_name": "RSSL",
    "title": "Implementations of Semi-Supervised Learning Approaches for\nClassification",
    "description": "A collection of implementations of semi-supervised classifiers\n    and methods to evaluate their performance. The package includes implementations\n    of, among others, Implicitly Constrained Learning, Moment Constrained Learning,\n    the Transductive SVM, Manifold regularization, Maximum Contrastive Pessimistic\n    Likelihood estimation, S4VM and WellSVM.",
    "version": "0.9.8",
    "maintainer": "Jesse Krijthe <jkrijthe@gmail.com>",
    "url": "https://github.com/jkrijthe/RSSL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6046,
    "package_name": "RTCC",
    "title": "Detecting Trait Clustering in Environmental Gradients",
    "description": "The Randomized Trait Community Clustering method (Triado-Margarit et al., 2019,\n    <doi:10.1038/s41396-019-0454-4>) is a statistical approach which allows to determine whether\n    if an observed trait clustering pattern is related to an increasing environmental constrain.\n    The method 1) determines whether exists or not a trait clustering on the sampled communities\n    and 2) assess if the observed clustering signal is related or not to an increasing environmental\n    constrain along an environmental gradient. Also, when the effect of the environmental gradient\n    is not linear, allows to determine consistent thresholds on the community assembly based on trait-values.",
    "version": "0.1.1",
    "maintainer": "Mateu Menendez-Serra <mateu.menendez@ceab.csic.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6061,
    "package_name": "RTaxometrics",
    "title": "Taxometric Analysis",
    "description": "We provide functions to perform taxometric analyses. This package contains 46 functions, but only 5 should be called directly by users. CheckData() should be run prior to any taxometric analysis to ensure that the data are appropriate for taxometric analysis. RunTaxometrics() performs taxometric analyses for a sample of data. RunCCFIProfile() performs a series of taxometric analyses to generate a CCFI profile. CreateData() generates a sample of categorical or dimensional data. ClassifyCases() assigns cases to groups using the base-rate classification method.",
    "version": "3.2.1",
    "maintainer": "John Ruscio <ruscio@tcnj.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6062,
    "package_name": "RTextTools",
    "title": "Automatic Text Classification via Supervised Learning",
    "description": "A machine learning package for automatic text classification \n\tthat makes it simple for novice users to get started with machine \n\tlearning, while allowing experienced users to easily experiment \n\twith different settings and algorithm combinations. The package \n\tincludes eight algorithms for ensemble classification (svm, slda, \n\tboosting, bagging, random forests, glmnet, decision trees, neural \n\tnetworks), comprehensive analytics, and thorough documentation.",
    "version": "1.4.3",
    "maintainer": "Loren Collingwood <loren.collingwood@gmail.com>",
    "url": "http://www.rtexttools.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6079,
    "package_name": "RWeka",
    "title": "R/Weka Interface",
    "description": "An R interface to Weka (Version 3.9.3).\n   Weka is a collection of machine learning algorithms for data mining\n   tasks written in Java, containing tools for data pre-processing,\n   classification, regression, clustering, association rules, and\n   visualization.  Package 'RWeka' contains the interface code, the\n   Weka jar is in a separate package 'RWekajars'.  For more information\n   on Weka see <https://www.cs.waikato.ac.nz/ml/weka/>.",
    "version": "0.4-46",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6109,
    "package_name": "RandPro",
    "title": "Random Projection with Classification",
    "description": "Performs random projection using Johnson-Lindenstrauss (JL) Lemma (see William B.Johnson and Joram Lindenstrauss (1984) <doi:10.1090/conm/026/737400>). Random Projection is a dimension reduction technique, where the data in the high dimensional space is projected into the low dimensional space using JL transform. The original high dimensional data matrix is multiplied with the low dimensional projection matrix which results in reduced matrix. The projection matrix can be generated using the projection function that is independent to the original data. Then finally apply the classification task on the projected data.  ",
    "version": "0.2.2",
    "maintainer": "Siddharth R <r.siddharthcse@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6150,
    "package_name": "Rborist",
    "title": "Extensible, Parallelizable Implementation of the Random Forest\nAlgorithm",
    "description": "Scalable implementation of classification and regression forests, as described by Breiman (2001), <DOI:10.1023/A:1010933404324>.",
    "version": "0.3-11",
    "maintainer": "Mark Seligman <mseligman@suiji.org>",
    "url": "https://github.com/suiji/Rborist.CRAN,\nhttps://github.com/suiji/Arborist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6197,
    "package_name": "RcmdrPlugin.temis",
    "title": "Graphical Integrated Text Mining Solution",
    "description": "An 'R Commander' plug-in providing an integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, vocabulary tables, terms\n    co-occurrences and documents similarity measures, time series analysis,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",
    "version": "0.7.12",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6242,
    "package_name": "RcppML",
    "title": "Rcpp Machine Learning Library",
    "description": "Fast machine learning algorithms including matrix factorization \n    and divisive clustering for large sparse and dense matrices.",
    "version": "0.3.7",
    "maintainer": "Zachary DeBruine <zacharydebruine@gmail.com>",
    "url": "https://github.com/zdebruine/RcppML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6293,
    "package_name": "ReSurv",
    "title": "Machine Learning Models for Predicting Claim Counts",
    "description": "Prediction of claim counts using the feature based development factors introduced in the manuscript Hiabu M., Hofman E. and Pittarello G. (2023) <doi:10.48550/arXiv.2312.14549>. \n             Implementation of Neural Networks, Extreme Gradient Boosting, \n             and Cox model with splines to optimise the partial log-likelihood of proportional hazard models.",
    "version": "1.0.0",
    "maintainer": "Emil Hofman <emil_hofman@hotmail.dk>",
    "url": "https://github.com/edhofman/ReSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6310,
    "package_name": "RecordLinkage",
    "title": "Record Linkage Functions for Linking and Deduplicating Data Sets",
    "description": "Provides functions for linking and deduplicating data sets.\n  Methods based on a stochastic approach are implemented as well as \n  classification algorithms from the machine learning domain. For details, \n  see our paper \"The RecordLinkage Package: Detecting Errors in Data\" \n  Sariyar M / Borg A (2010) <doi:10.32614/RJ-2010-017>. ",
    "version": "0.4-12.5",
    "maintainer": "Murat Sariyar <murat.sariyar@bfh.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6327,
    "package_name": "RegEnRF",
    "title": "Regression-Enhanced Random Forests",
    "description": "A novel generalized Random Forest method, that can improve on\n    RFs by borrowing the strength of penalized parametric regression. Based on\n    Zhang et al. (2019) <doi:10.48550/arXiv.1904.10416>.",
    "version": "1.0.0",
    "maintainer": "Umberto Minora <umbertofilippo@tiscali.it>",
    "url": "https://github.com/umbe1987/regenrf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6362,
    "package_name": "ResIndex",
    "title": "Generate Simple yet Effective Metric of Feature Importance for\nClassification Problems",
    "description": "An intuitive and explainable metric of Feature Importance for Classification Problems.\n    Resolution Index measures the extent to which a Feature clusters different classes when data is \n    sorted on it. User provides a DataFrame, column name of the Class, sample size and number of \n    iterations used for calculation. Resolution Index for each Feature is returned, which can be \n    effectively used to rank Features and reduce Dimensionality of Training data. For more details\n    on Feature Selection see Theng and Bhoyar (2023) <doi:10.1007/s10115-023-02010-5>.",
    "version": "0.1.0",
    "maintainer": "Anand Jha <anandorjha18@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6377,
    "package_name": "RfEmpImp",
    "title": "Multiple Imputation using Chained Random Forests",
    "description": "An R package for multiple imputation using chained random forests.\n    Implemented methods can handle missing data in mixed types of variables by\n    using prediction-based or node-based conditional distributions constructed\n    using random forests. For prediction-based imputation, the method based on\n    the empirical distribution of out-of-bag prediction errors of random forests\n    and the method based on normality assumption for prediction errors of random\n    forests are provided for imputing continuous variables. And the method based\n    on predicted probabilities is provided for imputing categorical variables.\n    For node-based imputation, the method based on the conditional distribution\n    formed by the predicting nodes of random forests, and the method based on\n    proximity measures of random forests are provided. More details of the\n    statistical methods can be found in Hong et al. (2020) <arXiv:2004.14823>.",
    "version": "2.1.8",
    "maintainer": "Shangzhi Hong <shangzhi-hong@hotmail.com>",
    "url": "https://github.com/shangzhi-hong/RfEmpImp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6386,
    "package_name": "Rforestry",
    "title": "Random Forests, Linear Trees, and Gradient Boosting for\nInference and Interpretability",
    "description": "Provides fast implementations of Random Forests, \n    Gradient Boosting, and Linear Random Forests, with an emphasis on inference \n    and interpretability. Additionally contains methods for variable \n    importance, out-of-bag prediction, regression monotonicity, and\n    several methods for missing data imputation.",
    "version": "0.11.1.0",
    "maintainer": "Theo Saarinen <theo_s@berkeley.edu>",
    "url": "https://github.com/forestry-labs/Rforestry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6406,
    "package_name": "Riemann",
    "title": "Learning with Data on Riemannian Manifolds",
    "description": "We provide a variety of algorithms for manifold-valued data, including Fréchet summaries, hypothesis testing, clustering, visualization, and other learning tasks. See Bhattacharya and Bhattacharya (2012) <doi:10.1017/CBO9781139094764> for general exposition to statistics on manifolds.",
    "version": "0.1.6",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "https://www.kisungyou.com/Riemann/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6432,
    "package_name": "Rmalschains",
    "title": "Continuous Optimization using Memetic Algorithms with Local\nSearch Chains (MA-LS-Chains)",
    "description": "An implementation of an algorithm family for continuous\n    optimization called memetic algorithms with local search chains\n    (MA-LS-Chains), as proposed in Molina et al. (2010) <doi:10.1162/evco.2010.18.1.18102> and Molina et al. (2011) <doi:10.1007/s00500-010-0647-2>. Rmalschains is further discussed in Bergmeir et al. (2016) <doi:10.18637/jss.v075.i04>. Memetic algorithms are hybridizations of genetic\n    algorithms with local search methods. They are especially suited\n    for continuous optimization.",
    "version": "0.2-10",
    "maintainer": "Christoph Bergmeir <c.bergmeir@decsai.ugr.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6435,
    "package_name": "Rmfrac",
    "title": "Simulation and Statistical Analysis of Multifractional Processes",
    "description": "Simulation of several fractional and multifractional processes. Includes Brownian and fractional Brownian motions, bridges and Gaussian Haar-based multifractional processes (GHBMP). Implements the methods from Ayache, Olenko and Samarakoon (2025) <doi:10.48550/arXiv.2503.07286> for simulation of GHBMP. Estimation of Hurst functions and local fractal dimension. Clustering realisations based on the Hurst functions. Several functions to estimate and plot geometric statistics of the processes and time series. Provides a 'shiny' application for interactive use of the functions from the package.",
    "version": "0.1.1",
    "maintainer": "Nemini Samarakoon <neminisamarakoon95@gmail.com>",
    "url": "https://github.com/Nemini-S/Rmfrac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6437,
    "package_name": "Rmixmod",
    "title": "Classification with Mixture Modelling",
    "description": "Interface of 'MIXMOD' software for supervised, unsupervised and\n    semi-supervised classification with mixture modelling <doi: 10.18637/jss.v067.i06>.",
    "version": "2.1.10",
    "maintainer": "Quentin Grimonprez <quentingrim@yahoo.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6468,
    "package_name": "RobPC",
    "title": "Robust Panel Clustering Algorithm",
    "description": "Performs both classical and robust panel clustering by applying Principal Component Analysis (PCA) for dimensionality reduction and clustering via standard K-Means or Trimmed K-Means. The method is designed to ensure stable and reliable clustering, even in the presence of outliers. Suitable for analyzing panel data in domains such as economic research, financial time-series, healthcare analytics, and social sciences. The package allows users to choose between classical K-Means for standard clustering and Trimmed K-Means for robust clustering, making it a flexible tool for various applications. For this package, we have benefited from the studies Rencher (2003), Wang and Lu (2021) <DOI:10.25236/AJBM.2021.031018>, Cuesta-Albertos et al. (1997) <https://www.jstor.org/stable/2242558?seq=1>.",
    "version": "1.4",
    "maintainer": "Hasan Bulut <hasan.bulut@omu.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6485,
    "package_name": "RobustMetrics",
    "title": "Calculates Robust Performance Metrics for Imbalanced\nClassification Problems",
    "description": "Calculates robust Matthews Correlation Coefficient (MCC) and robust F-Beta Scores, as introduced by Holzmann and Klar (2024) <doi:10.48550/arXiv.2404.07661>. \n    These performance metrics are designed for imbalanced classification problems.\n    Plots the receiver operating characteristic curve (ROC curve) together with the recall / 1-precision curve.",
    "version": "0.1.1",
    "maintainer": "Bernhard Klar <bernhard.klar@kit.edu>",
    "url": "https://github.com/BernhardKlar/RobustMetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6502,
    "package_name": "RoughSets",
    "title": "Data Analysis Using Rough Set and Fuzzy Rough Set Theories",
    "description": "Implementations of algorithms for data analysis based on the\n    rough set theory (RST) and the fuzzy rough set theory (FRST). We not only\n    provide implementations for the basic concepts of RST and FRST but also\n    popular algorithms that derive from those theories. The methods included in the\n    package can be divided into several categories based on their functionality:\n    discretization, feature selection, instance selection, rule induction and\n    classification based on nearest neighbors. RST was introduced by Zdzisław\n    Pawlak in 1982 as a sophisticated mathematical tool to model and process\n    imprecise or incomplete information. By using the indiscernibility relation for\n    objects/instances, RST does not require additional parameters to analyze the\n    data. FRST is an extension of RST. The FRST combines concepts of vagueness and\n    indiscernibility that are expressed with fuzzy sets (as proposed by Zadeh, in\n    1965) and RST.",
    "version": "1.3-8",
    "maintainer": "Christoph Bergmeir <c.bergmeir@decsai.ugr.es>",
    "url": "https://github.com/janusza/RoughSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6514,
    "package_name": "RprobitB",
    "title": "Bayesian Probit Choice Modeling",
    "description": "\n    Bayes estimation of probit choice models in cross-sectional and panel \n    settings. The package can analyze binary, multivariate, ordered, and ranked \n    choices, as well as heterogeneity of choice behavior among deciders. The \n    main functionality includes model fitting via Gibbs sampling, tools for \n    convergence  diagnostic, choice data simulation, in-sample and out-of-sample \n    choice prediction, and model selection using information criteria and Bayes \n    factors. The latent class model extension facilitates preference-based \n    decider classification, where the number of latent classes can be inferred \n    via the Dirichlet process or a weight-based updating heuristic. This allows \n    for flexible modeling of choice behavior without the need to impose \n    structural constraints. For a reference on the method, see Oelschlaeger and \n    Bauer (2021) <https://trid.trb.org/view/1759753>.",
    "version": "1.2.0",
    "maintainer": "Lennart Oelschläger <oelschlaeger.lennart@gmail.com>",
    "url": "https://loelschlaeger.de/RprobitB/,\nhttps://github.com/loelschlaeger/RprobitB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6539,
    "package_name": "Rstg",
    "title": "STG : Feature Selection using STochastic Gates",
    "description": "'STG' is a method for feature selection in neural network. The procedure is based on probabilistic relaxation of the l0 norm of features, or the count of the number of selected features. The framework simultaneously learns either a nonlinear regression or classification function while selecting a small subset of features. Read more: Yamada et al. (2020) <https://proceedings.mlr.press/v119/yamada20a.html>.",
    "version": "0.0.1",
    "maintainer": "Yutaro Yamada <yutaro.yamada@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6545,
    "package_name": "Rtapas",
    "title": "Random Tanglegram Partitions",
    "description": "Applies a given global-fit method to random partial tanglegrams of a fixed size\n             to identify the associations, terminals, and nodes that maximize phylogenetic\n             (in)congruence. It also includes functions to compute more easily the confidence\n             intervals of classification metrics and plot results, reducing computational time.\n             See Llaberia-Robledillo et al., (2023) <doi:10.1093/sysbio/syad016>.",
    "version": "1.2",
    "maintainer": "Mar Llaberia-Robledillo <mar.llaberia@uv.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6578,
    "package_name": "SAFARI",
    "title": "Shape Analysis for AI-Reconstructed Images",
    "description": "Provides functionality for image processing and shape analysis in \n    the context of reconstructed medical images generated by deep learning-based\n    methods or standard image processing algorithms and produced from different\n    medical imaging types, such as X-ray, Computational Tomography (CT),\n    Magnetic Resonance Imaging (MRI), and pathology imaging. Specifically,\n    offers tools to segment regions of interest and to extract quantitative\n    shape descriptors for applications in signal processing,\n    statistical analysis and modeling, and machine learning.",
    "version": "0.1.0",
    "maintainer": "Esteban Fernandez Morales <esteban.fernandezmorales@utdallas.edu>",
    "url": "https://github.com/estfernandez/SAFARI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6582,
    "package_name": "SAGMM",
    "title": "Clustering via Stochastic Approximation and Gaussian Mixture\nModels",
    "description": "Computes clustering by fitting Gaussian mixture models (GMM) via stochastic approximation following the methods of Nguyen and Jones (2018) <doi:10.1201/9780429446177>. It also provides some test data generation and plotting functionality to assist with this process.",
    "version": "0.2.5",
    "maintainer": "Andrew Thomas Jones <andrewthomasjones@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6587,
    "package_name": "SAM",
    "title": "Sparse Additive Modelling",
    "description": "Computationally efficient tools for high dimensional predictive\n        modeling (regression and classification). SAM is short for sparse \n        additive modeling, and adopts the computationally efficient basis \n        spline technique. We solve  the optimization problems by various \n        computational algorithms including the block coordinate descent \n        algorithm, fast iterative soft-thresholding algorithm, and newton method. \n        The computation is further accelerated by warm-start and active-set tricks.",
    "version": "1.1.3",
    "maintainer": "Haoming Jiang <jianghm.ustc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6590,
    "package_name": "SAMGEP",
    "title": "A Semi-Supervised Method for Prediction of Phenotype Event Times",
    "description": "A novel semi-supervised machine learning algorithm to predict phenotype event times using Electronic Health Record (EHR) data.",
    "version": "0.1.0-1",
    "maintainer": "Yuri Ahuja <Yuri_Ahuja@hms.harvard.edu>",
    "url": "https://github.com/celehs/SAMGEP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6611,
    "package_name": "SAutomata",
    "title": "Inference and Learning in Stochastic Automata",
    "description": "Machine learning provides algorithms that can learn from data and make inferences or predictions. Stochastic automata is a class of input/output devices which can model components. This work provides implementation an inference algorithm for stochastic automata which is similar to the Viterbi algorithm. Moreover, we specify a learning algorithm using the expectation-maximization technique and provide a more efficient implementation of the Baum-Welch algorithm for stochastic automata. This work is based on Inference and learning in stochastic automata was by Karl-Heinz Zimmermann(2017) <doi:10.12732/ijpam.v115i3.15>.",
    "version": "0.1.0",
    "maintainer": "Muhammad Kashif Hanif <mkashifhanif@gcuf.edu.pk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6669,
    "package_name": "SDModels",
    "title": "Spectrally Deconfounded Models",
    "description": "Screen for and analyze non-linear sparse direct effects in the presence of unobserved confounding using the spectral deconfounding techniques (Ćevid, Bühlmann, and Meinshausen (2020)<jmlr.org/papers/v21/19-545.html>, Guo, Ćevid, and Bühlmann (2022) <doi:10.1214/21-AOS2152>). These methods have been shown to be a good estimate for the true direct effect if we observe many covariates, e.g., high-dimensional settings, and we have fairly dense confounding. Even if the assumptions are violated, it seems like there is not much to lose, and the deconfounded models will, in general, estimate a function closer to the true one than classical least squares optimization. 'SDModels' provides functions SDAM() for Spectrally Deconfounded Additive Models (Scheidegger, Guo, and Bühlmann (2025) <doi:10.1145/3711116>) and SDForest() for Spectrally Deconfounded Random Forests (Ulmer, Scheidegger, and Bühlmann (2025) <doi:10.1080/10618600.2025.2569602>).",
    "version": "2.0.2",
    "maintainer": "Markus Ulmer <markus.ulmer@stat.math.ethz.ch>",
    "url": "https://www.markus-ulmer.ch/SDModels/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6690,
    "package_name": "SEMdeep",
    "title": "Structural Equation Modeling with Deep Neural Network and\nMachine Learning Algorithms",
    "description": "Training and validation of a custom (or data-driven) Structural\n    Equation Models using Deep Neural Networks or Machine Learning algorithms, which\n\textend the fitting procedures of the 'SEMgraph' R package <doi:10.32614/CRAN.package.SEMgraph>.",
    "version": "1.1.1",
    "maintainer": "Barbara Tarantino <barbara.tarantino01@universitadipavia.it>",
    "url": "https://github.com/BarbaraTarantino/SEMdeep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6701,
    "package_name": "SFDesign",
    "title": "Space-Filling Designs",
    "description": "Construct various types of space-filling designs, including Latin hypercube designs, clustering-based designs, maximin designs, maximum projection designs, and uniform designs (Joseph 2016 <doi:10.1080/08982112.2015.1100447>). It also offers the option to optimize designs based on user-defined criteria. This work is supported by U.S. National Science Foundation grant DMS-2310637.",
    "version": "0.1.3",
    "maintainer": "Shangkun Wang <shangkunwang01@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6706,
    "package_name": "SFtools",
    "title": "Space Filling Based Tools for Data Mining",
    "description": "Contains space filling based tools for\n    machine learning and data mining. Some functions offer\n    several computational techniques and deal with the out of\n    memory for large big data by using the ff package.",
    "version": "0.1.0",
    "maintainer": "Mohamed Laib <laib.med@gmail.com>",
    "url": "https://sites.google.com/site/mohamedlaibwebpage/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6717,
    "package_name": "SHAPforxgboost",
    "title": "SHAP Plots for 'XGBoost'",
    "description": "Aid in visual data investigations\n using SHAP (SHapley Additive exPlanation) visualization plots for 'XGBoost' and 'LightGBM'. \n It provides summary plot, dependence plot, interaction plot, and force plot and relies on\n the SHAP implementation provided by 'XGBoost' and 'LightGBM'.",
    "version": "0.2.0",
    "maintainer": "Yang Liu <lyhello@gmail.com>",
    "url": "https://github.com/liuyanguu/SHAPforxgboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6728,
    "package_name": "SID",
    "title": "Structural Intervention Distance",
    "description": "The code computes the structural intervention distance (SID) between a true directed acyclic graph (DAG) and an estimated DAG. Definition and details about the implementation can be found in J. Peters and P. Bühlmann: \"Structural intervention distance (SID) for evaluating causal graphs\", Neural Computation 27, pages 771-799, 2015  <doi:10.1162/NECO_a_00708>.",
    "version": "1.1",
    "maintainer": "Fred Gruber <fgruber@gmail.com>",
    "url": "https://github.com/fkgruber/SID_cran",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6729,
    "package_name": "SIDES",
    "title": "Subgroup Identification Based on Differential Effect Search",
    "description": "Provides function to apply \"Subgroup Identification based on Differential Effect Search\" (SIDES) method proposed by Lipkovich et al. (2011) <doi:10.1002/sim.4289>.",
    "version": "1.18",
    "maintainer": "Marie-Karelle Riviere <eldamjh@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6753,
    "package_name": "SIS",
    "title": "Sure Independence Screening",
    "description": "Variable selection techniques are essential tools for model\n    selection and estimation in high-dimensional statistical models. Through this\n    publicly available package, we provide a unified environment to carry out\n    variable selection using iterative sure independence screening (SIS) (Fan and Lv (2008)<doi:10.1111/j.1467-9868.2008.00674.x>) and all\n    of its variants in generalized linear models (Fan and Song (2009)<doi:10.1214/10-AOS798>) and the Cox proportional hazards\n    model (Fan, Feng and Wu (2010)<doi:10.1214/10-IMSCOLL606>).",
    "version": "0.8-8",
    "maintainer": "Yang Feng <yangfengstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6754,
    "package_name": "SISIR",
    "title": "Select Intervals Suited for Functional Regression",
    "description": "Interval fusion and selection procedures for regression with \n             functional inputs. Methods include a semiparametric approach based\n             on Sliced Inverse Regression (SIR), as described in \n             <doi:10.1007/s11222-018-9806-6> (standard ridge and sparse SIR are \n             also included in the package) and a random forest based approach, \n             as described in <doi:10.1002/sam.11705>.",
    "version": "0.2.3",
    "maintainer": "Nathalie Vialaneix <nathalie.vialaneix@inrae.fr>",
    "url": "https://forgemia.inra.fr/sfcb/sisir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6760,
    "package_name": "SKNN",
    "title": "A Super K-Nearest Neighbor (SKNN) Classification Algorithm",
    "description": "It's a Super K-Nearest Neighbor(SKNN) classification method with using kernel density to describe weight of the distance between a training observation and the testing sample. Comparison of performance between SKNN and KNN shows that SKNN is significantly superior to KNN.",
    "version": "4.1.2",
    "maintainer": "Yarong Yang <Yi.YA_yaya@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6770,
    "package_name": "SLModels",
    "title": "Stepwise Linear Models for Binary Classification Problems under\nYouden Index Optimisation",
    "description": "Stepwise models for the optimal linear combination of continuous variables in binary classification problems under Youden Index optimisation. Information on the models implemented can be found at Aznar-Gimeno et al. (2021) <doi:10.3390/math9192497>.",
    "version": "0.1.2",
    "maintainer": "Rocio Aznar-Gimeno <raznar@itainnova.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6790,
    "package_name": "SMLoutliers",
    "title": "Outlier Detection Using Statistical and Machine Learning Methods",
    "description": "Local Correlation Integral (LOCI) method for outlier identification is implemented here. The LOCI method developed here is invented in Breunig, et al. (2000), see <doi:10.1145/342009.335388>.",
    "version": "0.1",
    "maintainer": "Siddharth Jain <siddharthjain242@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6792,
    "package_name": "SMMAL",
    "title": "Semi-Supervised Estimation of Average Treatment Effects",
    "description": "Provides a pipeline for estimating the average treatment effect via semi-supervised learning. Outcome regression is fit with cross-fitting using various machine learning method or user customized function. Doubly robust ATE estimation leverages both labeled and unlabeled data under a semi-supervised missing-data framework. For more details see Hou et al. (2021) <doi:10.48550/arxiv.2110.12336>. A detailed vignette is included.",
    "version": "0.0.5",
    "maintainer": "Jue Hou <hou00123@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6803,
    "package_name": "SNFtool",
    "title": "Similarity Network Fusion",
    "description": "Similarity Network Fusion takes multiple views of a network and fuses them together to construct an overall status matrix. The input to our algorithm can be feature vectors, pairwise distances, or pairwise similarities. The learned status matrix can then be used for retrieval, clustering, and classification.",
    "version": "2.3.1",
    "maintainer": "Benjamin Brew <goldenberglab@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6820,
    "package_name": "SOMEnv",
    "title": "SOM Algorithm for the Analysis of Multivariate Environmental\nData",
    "description": "Analysis of multivariate environmental high frequency data by Self-Organizing Map and k-means clustering algorithms. By means of the graphical user interface it provides a comfortable way to elaborate by self-organizing map algorithm rather big datasets (txt files up to 100 MB ) obtained by environmental high-frequency monitoring by sensors/instruments. The functions present in the package are based on 'kohonen' and 'openair' packages implemented by functions embedding Vesanto et al. (2001) <http://www.cis.hut.fi/projects/somtoolbox/package/papers/techrep.pdf>  heuristic rules for map initialization parameters, k-means clustering algorithm and map features visualization. Cluster profiles visualization as well as graphs dedicated to the visualization of time-dependent variables Licen et al. (2020) <doi:10.4209/aaqr.2019.08.0414> are provided.",
    "version": "1.1.2",
    "maintainer": "Sabina Licen <slicen@units.it>",
    "url": "https://github.com/SomEnv/somenv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6931,
    "package_name": "SUMO",
    "title": "Generating Multi-Omics Datasets for Testing and Benchmarking",
    "description": "Provides tools to simulate multi-omics datasets with predefined signal structures. The generated data can be used for testing, validating, and benchmarking integrative analysis methods such as factor models and clustering approaches. This version includes enhanced signal customization, visualization tools (scatter, histogram, 3D), MOFA-based analysis pipelines, PowerPoint export, and statistical profiling of datasets. Designed for both method development and teaching, SUMO supports real and synthetic data pipelines with interpretable outputs. Tini, Giulia, et al (2019) <doi:10.1093/bib/bbx167>.",
    "version": "1.2.3",
    "maintainer": "Bernard Isekah Osang'ir <Bernard.Osangir@sckcen.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6972,
    "package_name": "ScottKnott",
    "title": "The ScottKnott Clustering Algorithm",
    "description": "Perform the balanced (Scott and Knott, 1974) and unbalanced <doi:10.1590/1984-70332017v17n1a1> Scott & Knott algorithm.",
    "version": "1.3-3",
    "maintainer": "Ivan Bezerra Allaman <ivanalaman@gmail.com>",
    "url": "https://github.com/ivanalaman/ScottKnott,\nhttps://lec.pro.br/software/pac-r/scottknott",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6973,
    "package_name": "ScottKnottESD",
    "title": "The Scott-Knott Effect Size Difference (ESD) Test",
    "description": "The Scott-Knott Effect Size Difference (ESD) test is a mean comparison approach that leverages a hierarchical clustering to partition the set of treatment means (e.g., means of variable importance scores, means of model performance) into statistically distinct groups with non-negligible difference [Tantithamthavorn et al., (2018) <doi:10.1109/TSE.2018.2794977>].",
    "version": "2.0.3",
    "maintainer": "Chakkrit Tantithamthavorn <kla@chakkrit.com>",
    "url": "https://github.com/klainfo/ScottKnottESD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7002,
    "package_name": "SemanticDistance",
    "title": "Compute Semantic Distance Between Text Constituents",
    "description": "Cleans and formats language transcripts guided by a series of transformation options (e.g., lemmatize words, omit stopwords, split strings across rows). 'SemanticDistance' computes two distinct metrics of cosine semantic distance (experiential and embedding). These values reflect pairwise cosine distance between different elements or chunks of a language sample. 'SemanticDistance' can process monologues (e.g., stories, ordered text), dialogues (e.g., conversation transcripts), word pairs arrayed in columns, and unordered word lists. Users specify options for how they wish to chunk distance calculations. These options include: rolling ngram-to-word distance (window of n-words to each new word), ngram-to-ngram distance (2-word chunk to the next 2-word chunk), pairwise distance between words arrayed in columns, matrix comparisons (i.e., all possible pairwise distances between words in an unordered list), turn-by-turn distance (talker to talker in a dialogue transcript). 'SemanticDistance' includes visualization options for analyzing distances as time series data and simple semantic network dynamics (e.g., clustering, undirected graph network).",
    "version": "0.1.1",
    "maintainer": "Jamie Reilly <jamie_reilly@temple.edu>",
    "url": "https://github.com/Reilly-ConceptsCognitionLab/SemanticDistance,\nhttps://reilly-conceptscognitionlab.github.io/SemanticDistance/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7010,
    "package_name": "SenTinMixt",
    "title": "Parsimonious Mixtures of MSEN and MTIN Distributions",
    "description": "Implements parsimonious mixtures of MSEN and MTIN distributions via expectation-\n    maximization based algorithms for model-based clustering. For each mixture\n    component, parsimony is reached via the eigen-decomposition of the scale \n    matrices and by imposing a constraint on the tailedness parameter. This produces\n    a family of 28 parsimonious mixture models for each distribution.",
    "version": "1.0.0",
    "maintainer": "Salvatore D. Tomarchio <daniele.tomarchio@unict.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7062,
    "package_name": "SiFINeT",
    "title": "Single Cell Feature Identification with Network Topology",
    "description": "Cluster-independent method based on topology structure of gene co-expression network for identifying feature gene sets, extracting cellular subpopulations, and elucidating intrinsic relationships among these subpopulations. Without prior cell clustering, SifiNet circumvents potential inaccuracies in clustering that may influence subsequent analyses. This method is introduced in Qi Gao, Zhicheng Ji, Liuyang Wang, Kouros Owzar, Qi-Jing Li, Cliburn Chan, Jichun Xie \"SifiNet: a robust and accurate method to identify feature gene sets and annotate cells\" (2024) <doi:10.1093/nar/gkae307>.",
    "version": "1.13",
    "maintainer": "Qi Gao <gqi@med.umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7068,
    "package_name": "Sieve",
    "title": "Nonparametric Estimation by the Method of Sieves",
    "description": "Performs multivariate nonparametric regression/classification by the method of sieves (using orthogonal basis). The method is suitable for moderate high-dimensional features (dimension < 100). The l1-penalized sieve estimator, a nonparametric generalization of Lasso, is adaptive to the feature dimension with provable theoretical guarantees. We also include a nonparametric stochastic gradient descent estimator, Sieve-SGD, for online or large scale batch problems. Details of the methods can be found in: <arXiv:2206.02994> <arXiv:2104.00846><arXiv:2310.12140>.",
    "version": "2.1",
    "maintainer": "Tianyu Zhang <tianyuz3@andrew.cmu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7079,
    "package_name": "SillyPutty",
    "title": "Silly Putty Clustering",
    "description": "Implements a simple, novel clustering algorithm based on\n  optimizing the silhouette width. See <doi:10.1101/2023.11.07.566055>\n  for details.",
    "version": "0.4.2",
    "maintainer": "Kevin R. Coombes <krc@silicovore.com>",
    "url": "http://oompa.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7138,
    "package_name": "SmallCountRounding",
    "title": "Small Count Rounding of Tabular Data",
    "description": "A statistical disclosure control tool to protect frequency tables in cases where small values are sensitive. The function PLSrounding() performs small count rounding of necessary inner cells so that all small frequencies of cross-classifications to be published (publishable cells) are rounded. This is equivalent to changing micro data since frequencies of unique combinations are changed. Thus, additivity and consistency are guaranteed. The methodology is described in Langsrud and Heldal (2018) <https://www.researchgate.net/publication/327768398_An_Algorithm_for_Small_Count_Rounding_of_Tabular_Data>.",
    "version": "1.2.5",
    "maintainer": "Øyvind Langsrud <oyl@ssb.no>",
    "url": "https://github.com/statisticsnorway/ssb-smallcountrounding,\nhttps://statisticsnorway.github.io/ssb-smallcountrounding/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7140,
    "package_name": "SmartMeterAnalytics",
    "title": "Methods for Smart Meter Data Analysis",
    "description": "Methods for analysis of energy consumption data (electricity, gas, \n    water) at different data measurement intervals. The package provides feature extraction \n    methods and algorithms to prepare data for data mining and machine learning \n    applications. Deatiled descriptions of the methods and their application can be found \n    in Hopf (2019, ISBN:978-3-86309-669-4) \"Predictive Analytics for Energy Efficiency and \n    Energy Retailing\" <doi:10.20378/irbo-54833> and Hopf et al. (2016) <doi:10.1007/s12525-018-0290-9> \n    \"Enhancing energy efficiency in the residential sector with smart meter data analytics\".",
    "version": "1.1.1",
    "maintainer": "Konstantin Hopf <konstantin.hopf@uni-bamberg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7155,
    "package_name": "SoftClustering",
    "title": "Soft Clustering Algorithms",
    "description": "It contains soft clustering algorithms, in particular approaches derived from rough set theory: Lingras & West original rough k-means, Peters' refined rough k-means, and PI rough k-means. It also contains classic k-means and a corresponding illustrative demo.",
    "version": "2.1.3",
    "maintainer": "G. Peters <peters.activities@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7163,
    "package_name": "SoilTaxonomy",
    "title": "A System of Soil Classification for Making and Interpreting Soil\nSurveys",
    "description": "Taxonomic dictionaries, formative element lists, and functions related to the maintenance, development and application of U.S. Soil Taxonomy. \n   Data and functionality are based on official U.S. Department of Agriculture sources including the latest edition of the Keys to Soil Taxonomy. Descriptions and metadata are obtained from the National Soil Information System or Soil Survey Geographic databases. Other sources are referenced in the data documentation. \n   Provides tools for understanding and interacting with concepts in the U.S. Soil Taxonomic System. Most of the current utilities are for working with taxonomic concepts at the \"higher\" taxonomic levels: Order, Suborder, Great Group, and Subgroup.",
    "version": "0.2.8",
    "maintainer": "Andrew Brown <andrew.g.brown@usda.gov>",
    "url": "https://github.com/ncss-tech/SoilTaxonomy,\nhttps://ncss-tech.github.io/SoilTaxonomy/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7166,
    "package_name": "Sojourn.Data",
    "title": "Supporting Objects for Sojourn Accelerometer Methods",
    "description": "Stores objects (e.g. neural networks) that are needed for\n    using Sojourn accelerometer methods. For more information, see\n    Lyden K, Keadle S, Staudenmayer J, & Freedson P (2014)\n    <doi:10.1249/MSS.0b013e3182a42a2d>, Ellingson LD, Schwabacher IJ,\n    Kim Y, Welk GJ, & Cook DB (2016) <doi:10.1249/MSS.0000000000000915>,\n    and Hibbing PR, Ellingson LD, Dixon PM, & Welk GJ (2018)\n    <doi:10.1249/MSS.0000000000001486>.",
    "version": "0.3.0",
    "maintainer": "Paul R. Hibbing <paulhibbing@gmail.com>",
    "url": "https://github.com/paulhibbing/Sojourn.Data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7198,
    "package_name": "SparseFunClust",
    "title": "Sparse Functional Clustering",
    "description": "Provides a general framework for performing sparse functional\n    clustering as originally described in Floriello and Vitelli (2017)\n    <doi:10.1016/j.jmva.2016.10.008>, with the possibility of jointly handling\n    data misalignment (see Vitelli, 2019, <doi:10.48550/arXiv.1912.00687>).",
    "version": "1.0.0",
    "maintainer": "Waldir Leoncio <w.l.netto@medisin.uio.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7239,
    "package_name": "SpecDetec",
    "title": "Change Points Detection with Spectral Clustering",
    "description": "Calculate change point based on spectral clustering with the option to automatically calculate the number of clusters if this information is not available.",
    "version": "1.0.0",
    "maintainer": "Luis Uzai <uzai_ff@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7244,
    "package_name": "SpectralClMixed",
    "title": "Spectral Clustering for Mixed Type Data",
    "description": "Performs cluster analysis of mixed-type data using Spectral Clustering, see F. Mbuga and, C. Tortora (2022) <doi:10.3390/stats5010001>.",
    "version": "1.0.2",
    "maintainer": "Cristina Tortora <grikris1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7249,
    "package_name": "Spectrum",
    "title": "Fast Adaptive Spectral Clustering for Single and Multi-View Data",
    "description": "A self-tuning spectral clustering method for single or multi-view data. 'Spectrum' uses a new type of adaptive density aware kernel that strengthens connections in the graph based on common nearest neighbours. It uses a tensor product graph data integration and diffusion procedure to integrate different data sources and reduce noise. 'Spectrum' uses either the eigengap or multimodality gap heuristics to determine the number of clusters. The method is sufficiently flexible so that a wide range of Gaussian and non-Gaussian structures can be clustered with automatic selection of K.",
    "version": "1.1",
    "maintainer": "Christopher R John <chris.r.john86@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7286,
    "package_name": "StakeholderAnalysis",
    "title": "Measuring Stakeholder Influence",
    "description": "Proposes an original instrument for measuring stakeholder influence on the development of an infrastructure project that is carried through by a municipality, drawing on stakeholder classifications (Mitchell, Agle, & Wood, 1997) and input-output modelling (Hester & Adams, 2013). Mitchell R., Agle B.R., & Wood D.J. <doi:10.2307/259247> Hester, P.T., & Adams, K.M. (2013) <doi:10.1016/j.procs.2013.09.282>.",
    "version": "1.2",
    "maintainer": "Lech Kujawski <lech.kujawski@ug.edu.pl>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7326,
    "package_name": "StratifiedRF",
    "title": "Builds Trees by Sampling Variables in Groups",
    "description": "Random Forest-like tree ensemble that works with groups of predictor variables. When building a tree, a number of variables is taken randomly from each group separately, thus ensuring that it considers variables from each group for the splits. Useful when rows contain information about different things (e.g. user information and product information) and it's not sensible to make a prediction with information from only one group of variables, or when there are far more variables from one group than the other and it's desired to have groups appear evenly on trees.\n    Trees are grown using the C5.0 algorithm rather than the usual CART algorithm. Supports parallelization (multithreaded), missing values in predictors, and categorical variables (without doing One-Hot encoding in the processing). Can also be used to create a regular (non-stratified) Random Forest-like model, but made up of C5.0 trees and with some additional control options.\n    As it's built with C5.0 trees, it works only for classification (not for regression).",
    "version": "0.2.2",
    "maintainer": "David Cortes <david.cortes.rivera@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7383,
    "package_name": "SurvHiDim",
    "title": "High Dimensional Survival Data Analysis",
    "description": "High dimensional time to events data analysis with variable selection technique.\n             Currently support LASSO, clustering and Bonferroni's correction.",
    "version": "0.1.1",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7395,
    "package_name": "SurvivalClusteringTree",
    "title": "Clustering Analysis Using Survival Tree and Forest Algorithms",
    "description": "An outcome-guided algorithm is developed to identify clusters of samples with similar characteristics and survival rate. The algorithm first builds a random forest and then defines distances between samples based on the fitted random forest. Given the distances, we can apply hierarchical clustering algorithms to define clusters. Details about this method is described in <https://github.com/luyouepiusf/SurvivalClusteringTree>.",
    "version": "1.1.1",
    "maintainer": "Lu You <lu.you@epi.usf.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7401,
    "package_name": "SweepDiscovery",
    "title": "Selective Sweep Discovery Tool",
    "description": "Selective sweep is a biological phenomenon in which genetic variation between neighboring beneficial mutant alleles is swept away due to the effect of genetic hitchhiking. Detection of selective sweep is not well acquainted as well as it is a laborious job. This package is a user friendly approach for detecting selective sweep in genomic regions. It uses a Random Forest based machine learning approach to predict selective sweep from VCF files as an input. Input of this function, train data and new data, can be computed using the project <https://github.com/AbhikSarkar1999/SweepDiscovery> in 'GitHub'. This package has been developed by using the concept of  Pavlidis and Alachiotis (2017) <doi:10.1186/s40709-017-0064-0>.",
    "version": "0.1.1",
    "maintainer": "Abhik Sarkar <abhik.jenkins.sarkar@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7423,
    "package_name": "T4cluster",
    "title": "Tools for Cluster Analysis",
    "description": "Cluster analysis is one of the most fundamental problems in data science. We provide a variety of algorithms from clustering to the learning on the space of partitions. See Hennig, Meila, and Rocci (2016, ISBN:9781466551886) for general exposition to cluster analysis.",
    "version": "0.1.4",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "https://www.kisungyou.com/T4cluster/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7424,
    "package_name": "T4transport",
    "title": "Tools for Computational Optimal Transport",
    "description": "Transport theory has seen much success in many fields of statistics and machine learning. We provide a variety of algorithms to compute Wasserstein distance, barycenter, and others. See Peyré and Cuturi (2019) <doi:10.1561/2200000073> for the general exposition to the study of computational optimal transport.",
    "version": "0.1.7",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "https://www.kisungyou.com/T4transport/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7447,
    "package_name": "TCIU",
    "title": "Spacekime Analytics, Time Complexity and Inferential Uncertainty",
    "description": "Provide the core functionality to transform longitudinal data to\n    complex-time (kime) data using analytic and numerical techniques, visualize the original \n    time-series and reconstructed kime-surfaces, perform model based (e.g., tensor-linear regression)\n    and model-free classification and clustering methods in the book Dinov, ID and Velev, MV. (2021)\n    \"Data Science: Time Complexity, Inferential Uncertainty, and Spacekime Analytics\", De Gruyter STEM Series,\n    ISBN 978-3-11-069780-3. <https://www.degruyter.com/view/title/576646>.\n    The package includes 18 core functions which can be separated into three groups.\n    1) draw longitudinal data, such as Functional magnetic resonance imaging(fMRI) time-series, and forecast or transform the time-series data.\n    2) simulate real-valued time-series data, e.g., fMRI time-courses, detect the activated areas,\n    report the corresponding p-values, and visualize the p-values in the 3D brain space.\n    3) Laplace transform and kimesurface reconstructions of the fMRI data.",
    "version": "1.2.7",
    "maintainer": "Yueyang Shen <petersyy@umich.edu>",
    "url": "https://github.com/SOCR/TCIU,\nhttps://www.socr.umich.edu/spacekime/,\nhttps://www.socr.umich.edu/TCIU/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7450,
    "package_name": "TDA",
    "title": "Statistical Tools for Topological Data Analysis",
    "description": "Tools for Topological Data Analysis. The package focuses on statistical analysis of persistent homology and density clustering. For that, this package provides an R interface for the efficient algorithms of the C++ libraries 'GUDHI' <https://project.inria.fr/gudhi/software/>, 'Dionysus' <https://www.mrzv.org/software/dionysus/>, and 'PHAT' <https://bitbucket.org/phat-code/phat/>. This package also implements methods from Fasy et al. (2014) <doi:10.1214/14-AOS1252> and Chazal et al. (2015) <doi:10.20382/jocg.v6i2a8>  for analyzing the statistical significance of persistent homology features.",
    "version": "1.9.4",
    "maintainer": "Jisu Kim <jkim82133@snu.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7451,
    "package_name": "TDAkit",
    "title": "Toolkit for Topological Data Analysis",
    "description": "Topological data analysis studies structure and shape of the data using topological features. We provide a variety of algorithms to learn with persistent homology of the data based on functional summaries for clustering, hypothesis testing, visualization, and others. We refer to Wasserman (2018) <doi:10.1146/annurev-statistics-031017-100045> for a statistical perspective on the topic. ",
    "version": "0.1.3",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7452,
    "package_name": "TDApplied",
    "title": "Machine Learning and Inference for Topological Data Analysis",
    "description": "Topological data analysis is a powerful tool for finding non-linear global structure\n    in whole datasets. The main tool of topological data analysis is persistent homology, which computes\n    a topological shape descriptor of a dataset called a persistence diagram. 'TDApplied' provides \n    useful and efficient methods for analyzing groups of persistence diagrams with machine learning and statistical inference,\n    and these functions can also interface with other data science packages to form flexible and integrated\n    topological data analysis pipelines.",
    "version": "3.0.4",
    "maintainer": "Shael Brown <shaelebrown@gmail.com>",
    "url": "https://github.com/shaelebrown/TDApplied",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7455,
    "package_name": "TDCM",
    "title": "The Transition Diagnostic Classification Model Framework",
    "description": "Estimate the transition diagnostic classification model (TDCM) \n    described in Madison & Bradshaw (2018) <doi:10.1007/s11336-018-9638-5>, a \n    longitudinal extension of the log-linear cognitive diagnosis model (LCDM) in \n    Henson, Templin & Willse (2009) <doi:10.1007/s11336-008-9089-5>. As the LCDM \n    subsumes many other diagnostic classification models (DCMs), many other DCMs \n    can be estimated longitudinally via the TDCM. The 'TDCM' package includes \n    functions to estimate the single-group and multigroup TDCM, summarize \n    results of interest including item parameters, growth proportions, \n    transition probabilities, transitional reliability, attribute correlations, \n    model fit, and growth plots.",
    "version": "0.1.0",
    "maintainer": "Michael E. Cotterell <mepcott@uga.edu>",
    "url": "https://github.com/cotterell/tdcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7460,
    "package_name": "TDSTNN",
    "title": "Time Delay Spatio Temporal Neural Network",
    "description": "STARMA (Space-Time Autoregressive Moving Average) models are commonly utilized in modeling and forecasting spatiotemporal time series data. However, the intricate nonlinear dynamics observed in many space-time rainfall patterns often exceed the capabilities of conventional STARMA models. This R package enables the fitting of Time Delay Spatio-Temporal Neural Networks, which are adept at handling such complex nonlinear dynamics efficiently. For detailed methodology, please refer to Saha et al. (2020) <doi:10.1007/s00704-020-03374-2>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7480,
    "package_name": "TFunHDDC",
    "title": "Clustering of Functional Data via Mixtures of t-Distributions",
    "description": "Extension of 'funHDDC' Schmutz et al. (2018) \n    <doi:10.1007/s00180-020-00958-4> for cases including\n    outliers by fitting t-distributions for robust groups. 'TFunHDDC' can cluster\n    univariate or multivariate data produced by the 'fda' package for data using\n    a b-splines or Fourier basis.",
    "version": "1.0.2",
    "maintainer": "Cristina Anton <popescuc@macewan.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7487,
    "package_name": "TIGERr",
    "title": "Technical Variation Elimination with Ensemble Learning\nArchitecture",
    "description": "\n    The R implementation of TIGER. \n    TIGER integrates random forest algorithm into an innovative ensemble learning architecture. Benefiting from this advanced architecture, TIGER is resilient to outliers, free from model tuning and less likely to be affected by specific hyperparameters.\n    TIGER supports targeted and untargeted metabolomics data and is competent to perform both intra- and inter-batch technical variation removal. TIGER can also be used for cross-kit adjustment to ensure data obtained from different analytical assays can be effectively combined and compared.\n    Reference: Han S. et al. (2022) <doi:10.1093/bib/bbab535>.",
    "version": "1.0.0",
    "maintainer": "Siyu Han <siyu.han@helmholtz-muenchen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7520,
    "package_name": "TPLSr",
    "title": "Thresholded Partial Least Squares Model for Neuroimaging Data",
    "description": "Uses thresholded partial least squares algorithm to create a regression or classification model. For more information, see Lee, Bradlow, and Kable <doi:10.1016/j.crmeth.2022.100227>.",
    "version": "1.0.5",
    "maintainer": "Sangil Lee <sangillee3rd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7540,
    "package_name": "TSCI",
    "title": "Tools for Causal Inference with Possibly Invalid Instrumental\nVariables",
    "description": "Two stage curvature identification with machine learning for causal \n    inference in settings when instrumental variable regression is not suitable\n    because of potentially invalid instrumental variables. Based on Guo and \n    Buehlmann (2022) \"Two Stage Curvature Identification with Machine Learning: \n    Causal Inference with Possibly Invalid Instrumental Variables\"\n    <doi:10.48550/arXiv.2203.12808>. The vignette is available in Carl, Emmenegger, Bühlmann and Guo (2025) \n    \"TSCI: Two Stage Curvature Identification for Causal Inference with Invalid Instruments in R\" <doi:10.18637/jss.v114.i07>.",
    "version": "3.0.5",
    "maintainer": "David Carl <david.carl@phd.unibocconi.it>",
    "url": "https://github.com/dlcarl/TSCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7555,
    "package_name": "TSLA",
    "title": "Tree-Guided Rare Feature Selection and Logic Aggregation",
    "description": "Implementation of the tree-guided feature selection and logic aggregation approach introduced in Chen et al. (2024) <doi:10.1080/01621459.2024.2326621>. The method enables the selection and aggregation of large-scale rare binary features with a known hierarchical structure using a convex, linearly-constrained regularized regression framework. The package facilitates the application of this method to both linear regression and binary classification problems by solving the optimization problem via the smoothing proximal gradient descent algorithm (Chen et al. (2012) <doi:10.1214/11-AOAS514>).",
    "version": "0.1.2",
    "maintainer": "Jianmin Chen <jianminc000@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7565,
    "package_name": "TSSVM",
    "title": "Time Series Forecasting using SVM Model",
    "description": "Implementation and forecasting univariate time series data using the Support Vector Machine model. Support Vector Machine is one of the prominent machine learning approach for non-linear time series forecasting. For method details see Kim, K. (2003) <doi:10.1016/S0925-2312(03)00372-2>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7583,
    "package_name": "TULIP",
    "title": "A Toolbox for Linear Discriminant Analysis with Penalties",
    "description": "Integrates several popular high-dimensional methods based on Linear Discriminant Analysis (LDA) and provides a comprehensive and user-friendly toolbox for linear, semi-parametric and tensor-variate classification as mentioned in Yuqing Pan, Qing Mai and Xin Zhang (2019) <arXiv:1904.03469>. Functions are included for covariate adjustment, model fitting, cross validation and prediction.",
    "version": "1.0.2",
    "maintainer": "Yuqing Pan <yuqing.pan@stat.fsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7623,
    "package_name": "TensorClustering",
    "title": "Model-Based Tensor Clustering",
    "description": "Performs model-based tensor clustering methods including Tensor Gaussian Mixture Model (TGMM), Tensor Envelope Mixture Model (TEMM) by Deng and Zhang (2021) <DOI: 10.1111/biom.13486>, Doubly-Enhanced EM (DEEM) algorithm by Mai, Zhang, Pan and Deng (2021) <DOI: 10.1080/01621459.2021.1904959>. ",
    "version": "1.0.2",
    "maintainer": "Kai Deng <kd18h@stat.fsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7643,
    "package_name": "TextForecast",
    "title": "Regression Analysis and Forecasting Using Textual Data from a\nTime-Varying Dictionary",
    "description": "Provides functionalities based on the paper \"Time Varying Dictionary and the Predictive Power of FED Minutes\" (Lima, 2018) <doi:10.2139/ssrn.3312483>. It selects the most predictive terms, that we call time-varying dictionary using supervised machine learning techniques as lasso and elastic net.     ",
    "version": "0.1.3",
    "maintainer": "Lucas Godeiro <lucas.godeiro@hotmail.com>",
    "url": "https://github.com/lucasgodeiro/TextForecast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7644,
    "package_name": "TextMiningGUI",
    "title": "Text Mining GUI Interface",
    "description": "Graphic interface for text analysis, implement a few methods such as biplots, correspondence analysis, co-occurrence, clustering, topic models, correlations and sentiments.",
    "version": "0.3",
    "maintainer": "Conrado Reyes <coreyes@gmail.com>",
    "url": "https://c0reyes.github.io/TextMiningGUI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7656,
    "package_name": "Thresher",
    "title": "Threshing and Reaping for Principal Components",
    "description": "Defines the classes used to identify\n  outliers (threshing) and compute the number of significant principal\n  components and number of clusters (reaping) in a joint application\n  of PCA and hierarchical clustering. See Wang et al., 2018,\n  <doi:10.1186/s12859-017-1998-9>.",
    "version": "1.1.5",
    "maintainer": "Kevin R. Coombes <krc@silicovore.com>",
    "url": "http://oompa.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7669,
    "package_name": "TimeVTree",
    "title": "Survival Analysis of Time Varying Coefficients Using a\nTree-Based Approach",
    "description": "Estimates time varying regression effects under Cox type models in\n    survival data using classification and regression tree. The codes in this package were \n    originally written in S-Plus for the paper \"Survival Analysis with Time-Varying Regression\n    Effects Using a Tree-Based Approach,\" by Xu, R. and Adak, S. (2002) <doi:10.1111/j.0006-341X.2002.00305.x>, Biometrics, 58: 305-315.\n    Development of this package was supported by NIH grants AG053983 and AG057707,\n    and by the UCSD Altman Translational Research Institute, NIH grant UL1TR001442.\n    The content is solely the responsibility of the authors and does not necessarily\n    represent the official views of the NIH.\n    The example data are from the Honolulu Heart Program/Honolulu Asia Aging Study (HHP/HAAS).",
    "version": "0.3.1",
    "maintainer": "Euyhyun Lee <e4lee@ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7680,
    "package_name": "TooManyCellsR",
    "title": "An R Wrapper for 'TooManyCells'",
    "description": "An R wrapper for using 'TooManyCells', a command line program for clustering, visualizing, and quantifying cell clade relationships. See <https://gregoryschwartz.github.io/too-many-cells/> for more details.",
    "version": "0.1.1.0",
    "maintainer": "Gregory W. Schwartz <gsch@pennmedicine.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7700,
    "package_name": "TrafficBDE",
    "title": "Traffic Predictions Using Neural Networks",
    "description": "Estimate and return either the traffic speed or the car entries in the city of Thessaloniki using historical traffic data. It's used in transport pilot of the 'BigDataEurope' project. There are functions for processing these data, training a neural network, select the most appropriate model and predict the traffic speed or the car entries for a selected time date.",
    "version": "0.1.2",
    "maintainer": "Kleanthis Koupidis <koupidis.okfgr@gmail.com>",
    "url": "https://github.com/okgreece/TrafficBDE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7703,
    "package_name": "TransGraph",
    "title": "Transfer Graph Learning",
    "description": "Transfer learning, aiming to use auxiliary domains to help improve learning of the target domain of interest when multiple heterogeneous datasets are available, has been a hot topic in statistical machine learning. The recent transfer learning methods with statistical guarantees mainly focus on the overall parameter transfer for supervised models in the ideal case with the informative auxiliary domains with overall similarity. In contrast, transfer learning for unsupervised graph learning is in its infancy and largely follows the idea of overall parameter transfer as for supervised learning. \n             In this package, the transfer learning for several complex graphical models is implemented, including Tensor Gaussian graphical models, non-Gaussian directed acyclic graph (DAG), and Gaussian graphical mixture models. Notably, this package promotes local transfer at node-level and subgroup-level in DAG structural learning and Gaussian graphical mixture models, respectively, which are more flexible and robust than the existing overall parameter transfer. As by-products, transfer learning for undirected graphical model (precision matrix) via D-trace loss, transfer learning for mean vector estimation, and single non-Gaussian learning via topological layer method are also included in this package. \n             Moreover, the aggregation of auxiliary information is an important issue in transfer learning, and this package provides multiple user-friendly aggregation methods, including sample weighting, similarity weighting, and most informative selection.    \n             (Note: the transfer for tensor GGM has been temporarily removed in the current version as its dependent R package Tlasso has been archived. The historical version TransGraph_1.0.0.tar.gz can be downloaded at <https://cran.r-project.org/src/contrib/Archive/TransGraph/>)\n             Reference: \n             Ren, M., Zhen Y., and Wang J. (2024) <https://jmlr.org/papers/v25/22-1313.html> \"Transfer learning for tensor graphical models\".    \n             Ren, M., He X., and Wang J. (2023) <doi:10.48550/arXiv.2310.10239> \"Structural transfer learning of non-Gaussian DAG\".    \n             Zhao, R., He X., and Wang J. (2022) <https://jmlr.org/papers/v23/21-1173.html> \"Learning linear non-Gaussian directed acyclic graph with diverging number of nodes\".",
    "version": "1.1.0",
    "maintainer": "Mingyang Ren <renmingyang17@mails.ucas.ac.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7752,
    "package_name": "TunePareto",
    "title": "Multi-Objective Parameter Tuning for Classifiers",
    "description": "Generic methods for parameter tuning of classification algorithms using multiple scoring functions (Muessel et al. (2012), <doi:10.18637/jss.v046.i05>).",
    "version": "2.5.3",
    "maintainer": "Hans Kestler <hans.kestler@uni-ulm.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7759,
    "package_name": "TwoPhaseCorR",
    "title": "Construct Two-Phase Experimental Designs with Correlated Errors",
    "description": "Tools for constructing and analyzing two-phase experimental designs under correlated error structures. Version 1.1.1 includes improved efficiency factor classification with tolerance control, updated plot visualizations, and improved clarity of the results. The conceptual framework and the term two-phase were introduced by McIntyre (1955) <doi:10.2307/3001770>).",
    "version": "1.1.1",
    "maintainer": "Akhilesh Jha <jha.akhilesh09@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7767,
    "package_name": "UAHDataScienceSC",
    "title": "Learn Supervised Classification Methods Through Examples and\nCode",
    "description": "Supervised classification methods, which (if asked) can provide\n    step-by-step explanations of the algorithms used, as described in\n    PK Josephine et. al., (2021) <doi:10.59176/kjcs.v1i1.1259>; and datasets to\n    test them on, which highlight the strengths and weaknesses of each technique.",
    "version": "1.0.0",
    "maintainer": "Andriy Protsak Protsak <andriy.protsak@edu.uah.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7787,
    "package_name": "UNPaC",
    "title": "Non-Parametric Cluster Significance Testing with Reference to a\nUnimodal Null Distribution",
    "description": "Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is described in Helgeson, Vock, and Bair (2021) <doi:10.1111/biom.13376>.",
    "version": "1.1.1",
    "maintainer": "Erika S. Helgeson <helge@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7823,
    "package_name": "UniversalCVI",
    "title": "Hard and Soft Cluster Validity Indices",
    "description": "Algorithms for checking the accuracy of a clustering result with known classes, computing cluster validity indices, and generating plots for comparing them.\n    The package is compatible with K-means, fuzzy C means, EM clustering, and hierarchical clustering (single, average, and complete linkage).\n    The details of the indices in this package can be found in:\n    J. C. Bezdek, M. Moshtaghi, T. Runkler, C. Leckie (2016) <doi:10.1109/TFUZZ.2016.2540063>,\n    T. Calinski, J. Harabasz (1974) <doi:10.1080/03610927408827101>,\n    C. H. Chou, M. C. Su, E. Lai (2004) <doi:10.1007/s10044-004-0218-1>,\n    D. L. Davies, D. W. Bouldin (1979) <doi:10.1109/TPAMI.1979.4766909>,\n    J. C. Dunn (1973) <doi:10.1080/01969727308546046>,\n    F. Haouas, Z. Ben Dhiaf, A. Hammouda, B. Solaiman (2017) <doi:10.1109/FUZZ-IEEE.2017.8015651>,\n    M. Kim, R. S. Ramakrishna (2005) <doi:10.1016/j.patrec.2005.04.007>,\n    S. H. Kwon (1998) <doi:10.1049/EL:19981523>,\n    S. H. Kwon, J. Kim, S. H. Son (2021) <doi:10.1049/ell2.12249>,\n    G. W. Miligan (1980) <doi:10.1007/BF02293907>,\n    M. K. Pakhira, S. Bandyopadhyay, U. Maulik (2004) <doi:10.1016/j.patcog.2003.06.005>,\n    M. Popescu, J. C. Bezdek, T. C. Havens, J. M. Keller (2013) <doi:10.1109/TSMCB.2012.2205679>,\n    S. Saitta, B. Raphael, I. Smith (2007) <doi:10.1007/978-3-540-73499-4_14>,\n    A. Starczewski (2017) <doi:10.1007/s10044-015-0525-8>,\n    Y. Tang, F. Sun, Z. Sun (2005) <doi:10.1109/ACC.2005.1470111>,\n    N. Wiroonsri (2024) <doi:10.1016/j.patcog.2023.109910>,\n    N. Wiroonsri, O. Preedasawakul (2023) <doi:10.48550/arXiv.2308.14785>,\n    C. H. Wu, C. S. Ouyang, L. W. Chen, L. W. Lu (2015) <doi:10.1109/TFUZZ.2014.2322495>, \n    X. Xie, G. Beni (1991) <doi:10.1109/34.85677> and\n    Rousseeuw (1987) and Kaufman and Rousseeuw(2009) <doi:10.1016/0377-0427(87)90125-7> and \n    <doi:10.1002/9780470316801>\n    C. Alok. (2010).",
    "version": "1.3.0",
    "maintainer": "Nathakhun Wiroonsri <nathakhun.wir@kmutt.ac.th>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7834,
    "package_name": "VALIDICLUST",
    "title": "VALID Inference for Clusters Separation Testing",
    "description": "Given a partition resulting from any clustering algorithm, the implemented tests allow valid post-clustering inference by testing if a given variable significantly separates two of the estimated clusters. \n             Methods are detailed in: Hivert B, Agniel D, Thiebaut R & Hejblum BP (2022). \n             \"Post-clustering difference testing: valid inference and practical considerations\", <arXiv:2210.13172>.",
    "version": "0.1.0",
    "maintainer": "Benjamin Hivert <benjamin.hivert@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7869,
    "package_name": "VICatMix",
    "title": "Variational Mixture Models for Clustering Categorical Data",
    "description": "A variational Bayesian finite mixture model for the clustering of categorical data, and can implement variable selection and semi-supervised outcome guiding if desired. Incorporates an option to perform model averaging over multiple initialisations to reduce the effects of local optima and improve the automatic estimation of the true number of clusters. For further details, see the paper by Rao and Kirk (2024) <doi:10.48550/arXiv.2406.16227>.",
    "version": "1.0",
    "maintainer": "Jackie Rao <jackie.rao@mrc-bsu.cam.ac.uk>",
    "url": "https://github.com/j-ackierao/VICatMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7880,
    "package_name": "VMDML",
    "title": "Variational Mode Decomposition Based Machine Learning Models",
    "description": "Application of Variational Mode Decomposition based different Machine Learning models for univariate time series forecasting. For method details see (i) K. Dragomiretskiy and D. Zosso (2014) <doi:10.1109/TSP.2013.2288675>; (ii)  Pankaj Das (2020) <http://krishi.icar.gov.in/jspui/handle/123456789/44138>.",
    "version": "0.1.1",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7903,
    "package_name": "VecDep",
    "title": "Measuring Copula-Based Dependence Between Random Vectors",
    "description": "Provides functions for estimation (parametric, semi-parametric and non-parametric)\n             of copula-based dependence coefficients between a finite collection of random vectors,\n             including phi-dependence measures and Bures-Wasserstein dependence measures. \n             An algorithm for agglomerative hierarchical variable clustering is also implemented.\n             Following the articles De Keyser & Gijbels (2024) <doi:10.1016/j.jmva.2024.105336>,\n             De Keyser & Gijbels (2024) <doi:10.1016/j.ijar.2023.109090>, and De Keyser & Gijbels (2024)\n             <doi:10.48550/arXiv.2404.07141>.",
    "version": "0.1.3",
    "maintainer": "Steven De Keyser <steven.dekeyser@kuleuven.be>",
    "url": "https://github.com/StevenDeKeyser98/VecDep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7937,
    "package_name": "VsusP",
    "title": "Variable Selection using Shrinkage Priors",
    "description": "Bayesian variable selection using shrinkage priors to identify significant variables in high-dimensional datasets. The package includes methods for determining the number of significant variables through innovative clustering techniques of posterior distributions, specifically utilizing the 2-Means and Sequential 2-Means (S2M) approaches. The package aims to simplify the variable selection process with minimal tuning required in statistical analysis.",
    "version": "1.0.0",
    "maintainer": "Nilson Chapagain <nilson.chapagain@gmail.com>",
    "url": "https://github.com/nilson01/VsusP-variable-selection-using-shrinkage-priors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7951,
    "package_name": "WCluster",
    "title": "Clustering and PCA with Weights, and Data Nuggets Clustering",
    "description": "K-means clustering, hierarchical clustering, and PCA with observational \n    weights and/or variable weights. It also includes the corresponding functions \n    for data nuggets which serve as representative samples of large datasets.\n    Cherasia et al., (2022) <doi:10.1007/978-3-031-22687-8_20>. \n    Amaratunga et al., (2009) <doi:10.1002/9780470317129>.",
    "version": "1.3.0",
    "maintainer": "Rituparna Dey <rituparnadey525@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7965,
    "package_name": "WOAkMedoids",
    "title": "Whale Optimization Algorithm for K-Medoids Clustering",
    "description": "Implements the Whale Optimization Algorithm(WOA) for k-medoids clustering, providing tools for effective and efficient cluster analysis in various data sets. The methodology is based on \"The Whale Optimization Algorithm\" by Mirjalili and Lewis (2016) <doi:10.1016/j.advengsoft.2016.01.008>.",
    "version": "0.1.0",
    "maintainer": "Chenan Huang <hualianchan@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7991,
    "package_name": "WaveletLSTM",
    "title": "Wavelet Based LSTM Model",
    "description": "A wavelet-based LSTM model is a type of neural network architecture that uses wavelet technique to pre-process the input data before passing it through a Long Short-Term Memory (LSTM) network. The wavelet-based LSTM model is a powerful approach that combines the benefits of wavelet analysis and LSTM networks to improve the accuracy of predictions in various applications. This package has been developed using the algorithm of Anjoy and Paul (2017) and Paul and Garai (2021) <DOI:10.1007/s00521-017-3289-9> <doi:10.1007/s00500-021-06087-4>.",
    "version": "0.1.0",
    "maintainer": "Dr. Md Yeasin <yeasin.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7992,
    "package_name": "WaveletML",
    "title": "Wavelet Decomposition Based Hybrid Machine Learning Models",
    "description": "Wavelet decomposes a series into multiple sub series called detailed and smooth components which helps to capture volatility at multi resolution level by various models. Two hybrid Machine Learning (ML) models (Artificial Neural Network and Support Vector Regression have been used) have been developed in combination with stochastic models, feature selection, and optimization algorithms for prediction of the data. The algorithms have been developed following Paul and Garai (2021)  <doi:10.1007/s00500-021-06087-4>. ",
    "version": "0.1.0",
    "maintainer": "Mr. Sandip Garai <sandipnicksandy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7994,
    "package_name": "WaveletRF",
    "title": "Wavelet-RF Hybrid Model for Time Series Forecasting",
    "description": "The Wavelet Decomposition followed by Random Forest Regression (RF) models have been applied for time series forecasting. The maximum overlap discrete wavelet transform (MODWT) algorithm was chosen as it works for any length of the series. The series is first divided into training and testing sets. In each of the wavelet decomposed series, the  supervised machine learning approach namely random forest was employed to train the model. This package also provides accuracy metrics in the form of Root Mean Square Error (RMSE) and Mean Absolute Prediction Error (MAPE). This package is based on the algorithm of Ding et al. (2021) <DOI: 10.1007/s11356-020-12298-3>.",
    "version": "0.1.0",
    "maintainer": "Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8014,
    "package_name": "WeightedCluster",
    "title": "Clustering of Weighted Data",
    "description": "Clusters state sequences and weighted data. It provides an optimized weighted PAM algorithm as well as functions for aggregating replicated cases, computing cluster quality measures for a range of clustering solutions, sequence analysis typology validation using parametric bootstraps and plotting (fuzzy) clusters of state sequences. It further provides a fuzzy and crisp CLARA algorithm to cluster large database with sequence analysis, and a methodological framework for Robustness Assessment of Regressions using Cluster Analysis Typologies (RARCAT).",
    "version": "2.0",
    "maintainer": "Matthias Studer <matthias.studer@unige.ch>",
    "url": "http://mephisto.unige.ch/weightedcluster/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8017,
    "package_name": "WeightedROC",
    "title": "Fast, Weighted ROC Curves",
    "description": "Fast computation of\n Receiver Operating Characteristic (ROC) curves\n and Area Under the Curve (AUC)\n for weighted binary classification problems\n (weights are example-specific cost values).",
    "version": "2020.1.31",
    "maintainer": "Toby Dylan Hocking <toby.hocking@r-project.org>",
    "url": "https://github.com/tdhock/WeightedROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8031,
    "package_name": "WordListsAnalytics",
    "title": "Multiple Data Analysis Tools for Property Listing Tasks",
    "description": "Application to estimate statistical values using properties provided by a group of individuals to describe\n  concepts using 'shiny'. It estimates the underlying distribution to generate new descriptive words\n  Canessa et al. (2023) <doi:10.3758/s13428-022-01811-w>, applies a new clustering model, and uses simulations to estimate \n  the probability that two persons describe the same words based on their descriptions\n  Canessa et al. (2022) <doi:10.3758/s13428-022-02030-z>.",
    "version": "0.2.4",
    "maintainer": "Sebastian Moreno <sebastian.moreno.araya@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8037,
    "package_name": "WormTensor",
    "title": "A Clustering Method for Time-Series Whole-Brain Activity Data of\n'C. elegans'",
    "description": "A toolkit to detect clusters from distance matrices. \n    The distance matrices are assumed to be calculated between the cells of \n    multiple animals ('Caenorhabditis elegans') from input time-series matrices. \n    Some functions for generating distance matrices, performing clustering, \n    evaluating the clustering, and visualizing the results of clustering and \n    evaluation are available. We're also providing the download function to \n    retrieve the calculated distance matrices from \n    'figshare' <https://figshare.com>.",
    "version": "0.1.2",
    "maintainer": "Kentaro Yamamoto <yamaken37.the.answer@gmail.com>",
    "url": "https://github.com/rikenbit/WormTensor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8064,
    "package_name": "Xplortext",
    "title": "Statistical Analysis of Textual Data",
    "description": "Provides a set of functions devoted to multivariate exploratory statistics on textual data. Classical methods such as correspondence analysis and agglomerative hierarchical clustering are available. Chronologically constrained agglomerative hierarchical clustering enriched with labelled-by-words trees is offered. Given a division of the corpus into parts, their characteristic words and documents are identified. Further, accessing to 'FactoMineR' functions is very easy. Two of them are relevant in textual domain. MFA() addresses multiple lexical table allowing applications such as dealing with multilingual corpora as well as simultaneously analyzing both open-ended and closed questions in surveys. See <http://xplortext.unileon.es> for examples.",
    "version": "1.5.5",
    "maintainer": "Ramón Alvarez-Esteban <ramon.alvarez@unileon.es>",
    "url": "https://xplortext.unileon.es",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8098,
    "package_name": "ZooID",
    "title": "Load, Segment and Classify Zooplankton Images",
    "description": "This tool provides functions to load, segment and classify zooplankton images. \n  The image processing algorithms and the machine learning classifiers in this package are (will be, since these have not been added yet) direct ports of \n  an early 'python' implementation that can be found at <https://github.com/arickGrootveld/ZooID>. The model weights and datasets (also not added yet) \n  that are a part of this package can also be found at Arick Grootveld, Eva R. Kozak, Carmen Franco-Gordo (2023) <doi:10.5281/zenodo.7979996>. ",
    "version": "0.2.0",
    "maintainer": "Arick Grootveld <arick.grootveld@gmail.com>",
    "url": "https://github.com/arickGrootveld/ZooID_RPackage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8128,
    "package_name": "abclass",
    "title": "Angle-Based Classification",
    "description": "Multi-category angle-based large-margin classifiers.\n    See Zhang and Liu (2014) <doi:10.1093/biomet/asu017> for details.",
    "version": "0.5.0",
    "maintainer": "Wenjie Wang <wang@wwenjie.org>",
    "url": "https://wwenjie.org/abclass,\nhttps://github.com/wenjie2wang/abclass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8136,
    "package_name": "abess",
    "title": "Fast Best Subset Selection",
    "description": "Extremely efficient toolkit for solving the best subset selection problem <https://www.jmlr.org/papers/v23/21-1060.html>. This package is its R interface. The package implements and generalizes algorithms designed in <doi:10.1073/pnas.2014241117> that exploits a novel sequencing-and-splicing technique to guarantee exact support recovery and globally optimal solution in polynomial times for linear model. It also supports best subset selection for logistic regression, Poisson regression, Cox proportional hazard model, Gamma regression, multiple-response regression, multinomial logistic regression, ordinal regression, Ising model reconstruction <doi:10.1080/01621459.2025.2571245>, (sequential) principal component analysis, and robust principal component analysis. The other valuable features such as the best subset of group selection <doi:10.1287/ijoc.2022.1241> and sure independence screening <doi:10.1111/j.1467-9868.2008.00674.x> are also provided.  ",
    "version": "0.4.11",
    "maintainer": "Jin Zhu <zhuj1jqx@gmail.com>",
    "url": "https://github.com/abess-team/abess,\nhttps://abess-team.github.io/abess/,\nhttps://abess.readthedocs.io",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8201,
    "package_name": "activelearning",
    "title": "A Collection of Active Learning Methods in R",
    "description": "Active learning is a machine learning paradigm",
    "version": "0.1.2",
    "maintainer": "John A. Ramey <johnramey@gmail.com>",
    "url": "https://github.com/ramhiser/activelearning",
    "exports": [],
    "topics": ["active-learning", "machine-learning", "r"],
    "score": "NA",
    "stars": 47
  },
  {
    "id": 8213,
    "package_name": "adabag",
    "title": "Applies Multiclass AdaBoost.M1, SAMME and Bagging",
    "description": "It implements Freund and Schapire's Adaboost.M1 algorithm and Breiman's Bagging\n\talgorithm using classification trees as individual classifiers. Once these classifiers have been\n\ttrained, they can be used to predict on new data. Also, cross validation estimation of the error can\n\tbe done. Since version 2.0 the function margins() is available to calculate the margins for these\n\tclassifiers. Also a higher flexibility is achieved giving access to the rpart.control() argument\n\tof 'rpart'. Four important new features were introduced on version 3.0, AdaBoost-SAMME (Zhu \n\tet al., 2009) is implemented and a new function errorevol() shows the error of the ensembles as\n\ta function of the number of iterations. In addition, the ensembles can be pruned using the option \n\t'newmfinal' in the predict.bagging() and predict.boosting() functions and the posterior probability of\n\teach class for observations can be obtained. Version 3.1 modifies the relative importance measure\n\tto take into account the gain of the Gini index given by a variable in each tree and the weights of \n\tthese trees. Version 4.0 includes the margin-based ordered aggregation for Bagging pruning (Guo\n\tand Boukir, 2013) and a function to auto prune the 'rpart' tree. Moreover, three new plots are also \n\tavailable importanceplot(), plot.errorevol() and plot.margins(). Version 4.1 allows to predict on \n\tunlabeled data. Version 4.2 includes the parallel computation option for some of the functions. \n\tVersion 5.0 includes the Boosting and Bagging algorithms for label ranking (Albano, Sciandra\n\tand Plaia, 2023). ",
    "version": "5.1",
    "maintainer": "Esteban Alfaro <Esteban.Alfaro@uclm.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8217,
    "package_name": "adana",
    "title": "Adaptive Nature-Inspired Algorithms for Hybrid Genetic\nOptimization",
    "description": "The Genetic Algorithm (GA) is a type of optimization method of Evolutionary Algorithms. It uses the biologically inspired operators such as mutation, crossover, selection and replacement.Because of their global search and robustness abilities, GAs have been widely utilized in machine learning, expert systems, data science, engineering, life sciences and many other areas of research and business. However, the regular GAs need the techniques to improve their efficiency in computing time and performance in finding global optimum using some adaptation and hybridization strategies. The adaptive GAs (AGA) increase the convergence speed and success of regular GAs by setting the parameters crossover and mutation probabilities dynamically. The hybrid GAs combine the exploration strength of a stochastic GAs with the exact convergence ability of any type of deterministic local search algorithms such as simulated-annealing, in addition to other nature-inspired algorithms such as ant colony optimization, particle swarm optimization etc. The package 'adana' includes a rich working environment with its many functions that make possible to build and work regular GA, adaptive GA, hybrid GA and hybrid adaptive GA for any kind of optimization problems. Cebeci, Z. (2021, ISBN: 9786254397448).",
    "version": "1.1.0",
    "maintainer": "Erkut Tekeli <etekeli@atu.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8238,
    "package_name": "adcontabil",
    "title": "Accounting Analysis",
    "description": "Provides methods for processing corporate balance sheets with a focus on the Brazilian reporting format. Includes data standardization, classification by accounting categories, and aggregation of values. Supports accounting and financial analyses of companies, improving efficiency and ensuring reproducibility of empirical studies.",
    "version": "1.1.8",
    "maintainer": "Lissandro Costa de Sousa <lisandrosousa54@gmail.com>",
    "url": "https://github.com/LissandroSousa/adcontabil.R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8245,
    "package_name": "additive",
    "title": "Bindings for Additive TidyModels",
    "description": "Fit Generalized Additive Models (GAM) using 'mgcv' with 'parsnip'/'tidymodels'\n    via 'additive' <doi:10.5281/zenodo.4784245>. 'tidymodels' is a collection of\n    packages for machine learning; see Kuhn and Wickham (2020) <https://www.tidymodels.org>).\n    The technical details of 'mgcv' are described in Wood (2017)\n    <doi:10.1201/9781315370279>.",
    "version": "1.0.1",
    "maintainer": "Hamada S. Badr <badr@jhu.edu>",
    "url": "https://hsbadr.github.io/additive/,\nhttps://github.com/hsbadr/additive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8270,
    "package_name": "adjROC",
    "title": "Computing Sensitivity at a Fix Value of Specificity and Vice\nVersa as Well as Bootstrap Metrics for ROC Curves",
    "description": "For a binary classification the adjusted sensitivity and specificity \n             are measured for a given fixed threshold. If the threshold for either \n             sensitivity or specificity is not given, the crossing point between \n             the sensitivity and specificity curves are returned. For bootstrap \n             procedures, mean and CI bootstrap values of sensitivity, specificity, \n             crossing point between specificity and specificity as well as AUC \n             and AUCPR can be evaluated.",
    "version": "0.3",
    "maintainer": "E. F. Haghish <haghish@uio.no>",
    "url": "https://github.com/haghish/adjROC,\nhttps://www.sv.uio.no/psi/english/people/aca/haghish/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8288,
    "package_name": "admix",
    "title": "Package Admix for Admixture (aka Contamination) Models",
    "description": "Implements techniques to estimate the unknown quantities\n    related to two-component admixture models, where the two components\n    can belong to any distribution (note that in the case of multinomial\n    mixtures, the two components must belong to the same family).\n    Estimation methods depend on the assumptions made on the unknown\n    component density; see Bordes and Vandekerkhove (2010)\n    <doi:10.3103/S1066530710010023>, Patra and Sen (2016)\n    <doi:10.1111/rssb.12148>, and Milhaud, Pommeret, Salhi, Vandekerkhove\n    (2024) <doi:10.3150/23-BEJ1593>. In practice, one can estimate both\n    the mixture weight and the unknown component density in a wide variety\n    of frameworks. On top of that, hypothesis tests can be performed in\n    one and two-sample contexts to test the unknown component density (see\n    Milhaud, Pommeret, Salhi and Vandekerkhove (2022)\n    <doi:10.1016/j.jspi.2021.05.010>, and Milhaud, Pommeret, Salhi,\n    Vandekerkhove (2024) <doi:10.3150/23-BEJ1593>). Finally, clustering of\n    unknown mixture components is also feasible in a K-sample setting (see\n    Milhaud, Pommeret, Salhi, Vandekerkhove (2024) \n    <https://jmlr.org/papers/v25/23-0914.html>).",
    "version": "2.5.1",
    "maintainer": "Xavier Milhaud <xavier.milhaud.research@gmail.com>",
    "url": "https://github.com/XavierMilhaud/admix-Rpackage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8296,
    "package_name": "adproclus",
    "title": "Additive Profile Clustering Algorithms",
    "description": "Obtain overlapping clustering models for object-by-variable data\n        matrices using the Additive Profile Clustering (ADPROCLUS) method. \n        Also contains the low dimensional ADPROCLUS method \n        for simultaneous dimension reduction and overlapping clustering. \n        For reference see Depril, Van Mechelen, Mirkin (2008) \n        <doi:10.1016/j.csda.2008.04.014> and Depril, Van Mechelen, Wilderjans \n        (2012) <doi:10.1007/s00357-012-9112-5>.",
    "version": "2.0.0",
    "maintainer": "Henry Heppe <heppe.henry@gmail.com>",
    "url": "https://github.com/henry-heppe/adproclus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8300,
    "package_name": "adsoRptionCV",
    "title": "Cross-Validation Methods for Adsorption Isotherm Models",
    "description": "\n    Provides cross-validation tools for adsorption isotherm models, supporting both linear and non-linear forms. \n    Current methods cover commonly used isotherms including the Freundlich, Langmuir, and Temkin models. \n    This package implements K-fold and leave-one-out cross-validation (LOOCV) with optional clustering-based\n    fold assignment to preserve underlying data structures during validation. Model predictive performance is assessed \n    using mean squared error (MSE), with optional graphical visualization of fold-wise MSEs to support intuitive evaluation of model accuracy. \n    This package is intended to facilitate rigorous model validation in adsorption studies and aid researchers in selecting robust isotherm models.\n    For more details, see Montgomery et al. (2012) <isbn: 978-0-470-54281-1>, Lumumba et al. (2024) <doi:10.11648/j.ajtas.20241305.13>, and Yates et al. (2022) <doi:10.1002/ecm.1557>.",
    "version": "0.1.0",
    "maintainer": "Paul Angelo C. Manlapaz <pacmanlapaz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8378,
    "package_name": "ai",
    "title": "Build, Predict and Analyse Artificial Intelligence Models",
    "description": "An interface for data processing, building models, predicting values and analysing outcomes. Fitting Linear Models, Robust Fitting of Linear Models, k-Nearest Neighbor Classification, 1-Nearest Neighbor Classification, and Conditional Inference Trees are available.",
    "version": "1.0.4.44",
    "maintainer": "Rafal Urniaz <rafal.urniaz@cantab.net>",
    "url": "https://github.com/urniaz/ai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8381,
    "package_name": "aifeducation",
    "title": "Artificial Intelligence for Education",
    "description": "In social and educational settings, the use of Artificial\n    Intelligence (AI) is a challenging task. Relevant data is often only\n    available in handwritten forms, or the use of data is restricted by\n    privacy policies. This often leads to small data sets. Furthermore, in\n    the educational and social sciences, data is often unbalanced in terms\n    of frequencies. To support educators as well as educational and social\n    researchers in using the potentials of AI for their work, this package\n    provides a unified interface for neural nets in 'PyTorch' to deal with\n    natural language problems. In addition, the package ships with a shiny\n    app, providing a graphical user interface.  This allows the usage of\n    AI for people without skills in writing python/R scripts.  The tools\n    integrate existing mathematical and statistical methods for dealing\n    with small data sets via pseudo-labeling (e.g. Cascante-Bonilla et al.\n    (2020) <doi:10.48550/arXiv.2001.06001>) and imbalanced data via the\n    creation of synthetic cases (e.g.  Islam et al. (2012)\n    <doi:10.1016/j.asoc.2021.108288>).  Performance evaluation of AI is\n    connected to measures from content analysis which educational and\n    social researchers are generally more familiar with (e.g. Berding &\n    Pargmann (2022) <doi:10.30819/5581>, Gwet (2014)\n    <ISBN:978-0-9708062-8-4>, Krippendorff (2019)\n    <doi:10.4135/9781071878781>). Estimation of energy consumption and CO2\n    emissions during model training is done with the 'python' library\n    'codecarbon'.  Finally, all objects created with this package allow to\n    share trained AI models with other people.",
    "version": "1.1.3",
    "maintainer": "Berding Florian <florian.berding@uni-hamburg.de>",
    "url": "https://fberding.github.io/aifeducation/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8404,
    "package_name": "akc",
    "title": "Automatic Knowledge Classification",
    "description": "A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well.",
    "version": "0.9.9.3",
    "maintainer": "Tian-Yuan Huang <huang.tian-yuan@qq.com>",
    "url": "https://github.com/hope-data-science/akc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8431,
    "package_name": "ale",
    "title": "Interpretable Machine Learning and Statistical Inference with\nAccumulated Local Effects (ALE)",
    "description": "Accumulated Local Effects (ALE) were initially developed as a model-agnostic approach for global explanations of the results of black-box machine learning algorithms. ALE has a key advantage over other approaches like partial dependency plots (PDP) and SHapley Additive exPlanations (SHAP): its values represent a clean functional decomposition of the model. As such, ALE values are not affected by the presence or absence of interactions among variables in a mode. Moreover, its computation is relatively rapid. This package reimplements the algorithms for calculating ALE data and develops highly interpretable visualizations for plotting these ALE values. It also extends the original ALE concept to add bootstrap-based confidence intervals and ALE-based statistics that can be used for statistical inference. For more details, see Okoli, Chitu. 2023. “Statistical Inference Using Machine Learning and Classical Techniques Based on Accumulated Local Effects (ALE).” arXiv. <doi:10.48550/arXiv.2310.09877>.",
    "version": "0.5.3",
    "maintainer": "Chitu Okoli <Chitu.Okoli@skema.edu>",
    "url": "https://github.com/tripartio/ale, https://tripartio.github.io/ale/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8435,
    "package_name": "algaeClassify",
    "title": "Tools to Query the 'Algaebase' Online Database, Standardize\nPhytoplankton Taxonomic Data, and Perform Functional Group\nClassifications",
    "description": "Functions that facilitate the use of accepted taxonomic nomenclature, collection of\n\tfunctional trait data, and assignment of functional group classifications to phytoplankton\n\tspecies. Possible classifications include Morpho-functional group (MFG; Salmaso et al. 2015 \n\t<doi:10.1111/fwb.12520>) and CSR (Reynolds 1988; Functional morphology and the \n\tadaptive strategies of phytoplankton. In C.D. Sandgren (ed). Growth and reproductive \n\tstrategies of freshwater phytoplankton, 388-433. Cambridge University Press, New York). \n\tVersions 2.0.0 and later includes new functions for querying the \n\t'algaebase' online taxonomic database (www.algaebase.org), however these functions require\n\ta valid API key that must be acquired from the 'algaebase' administrators. \n\tNote that none of the 'algaeClassify' authors are affiliated with 'algaebase' in any way. Taxonomic \n\tnames can also be checked against a variety of taxonomic databases using \n\tthe 'Global Names Resolver' service via its API (<https://resolver.globalnames.org/api>). In addition,\n\tcurrently accepted and outdated synonyms, and higher taxonomy, can be extracted for lists of \n\tspecies from the 'ITIS' database using wrapper functions for the ritis package.\n\tThe 'algaeClassify' package is a product of the GEISHA (Global Evaluation of the Impacts of \n\tStorms on freshwater Habitat and Structure of phytoplankton Assemblages), funded by CESAB \n    (Centre for Synthesis and Analysis of Biodiversity) and the U.S. Geological Survey John Wesley Powell Center for\n\tSynthesis and Analysis, with data and other support provided by members of GLEON \n\t(Global Lake Ecology Observation Network). \n\tDISCLAIMER: This software has been approved for release by the \n\tU.S. Geological Survey (USGS). Although the software has been subjected to rigorous review, \n\tthe USGS reserves the right to update the software as needed pursuant to further analysis and \n\treview. No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the \n\tfunctionality of the software and related material nor shall the fact of release constitute \n\tany such warranty. Furthermore, the software is released on condition that neither the USGS \n\tnor the U.S. Government shall be held liable for any damages resulting from its authorized \n\tor unauthorized use.",
    "version": "2.0.5",
    "maintainer": "Vijay Patil <vij.patil@gmail.com>",
    "url": "https://doi.org/10.5066/F7S46Q3F",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8456,
    "package_name": "alookr",
    "title": "Model Classifier for Binary Classification",
    "description": "A collection of tools that support data splitting, predictive modeling, and model evaluation. \n    A typical function is to split a dataset into a training dataset and a test dataset. \n    Then compare the data distribution of the two datasets.\n    Another feature is to support the development of predictive models and to compare the performance of several predictive models, \n    helping to select the best model. ",
    "version": "0.4.0",
    "maintainer": "Choonghyun Ryu <choonghyun.ryu@gmail.com>",
    "url": "https://choonghyunryu.github.io/alookr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8483,
    "package_name": "amap",
    "title": "Another Multidimensional Analysis Package",
    "description": "Tools for Clustering and Principal Component Analysis\n        (With robust methods, and parallelized functions).",
    "version": "0.8-20",
    "maintainer": "Antoine Lucas <antoinelucas@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8494,
    "package_name": "amelie",
    "title": "Anomaly Detection with Normal Probability Functions",
    "description": "Implements anomaly detection as binary classification for cross-sectional data. Uses maximum likelihood estimates and normal probability functions to classify observations as anomalous. The method is presented in the following lecture from the Machine Learning course by Andrew Ng: <https://www.coursera.org/learn/machine-learning/lecture/C8IJp/algorithm/>, and is also described in: Aleksandar Lazarevic, Levent Ertoz, Vipin Kumar, Aysel Ozgur, Jaideep Srivastava (2003) <doi:10.1137/1.9781611972733.3>.",
    "version": "0.2.1",
    "maintainer": "Dmitriy Bolotov <dbolotov@live.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8546,
    "package_name": "anocva",
    "title": "A Non-Parametric Statistical Test to Compare Clustering\nStructures",
    "description": "Provides ANOCVA (ANalysis Of Cluster VAriability), a non-parametric statistical test\n    to compare clustering structures with applications in functional magnetic resonance imaging\n    data (fMRI). The ANOCVA allows us to compare the clustering structure of multiple groups\n    simultaneously and also to identify features that contribute to the differential clustering.",
    "version": "0.1.1",
    "maintainer": "Maciel C. Vidal <calebe@ime.usp.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8564,
    "package_name": "anticlust",
    "title": "Subset Partitioning via Anticlustering",
    "description": "The method of anticlustering partitions a pool of elements into groups (i.e., anticlusters) with the goal of maximizing between-group similarity or within-group heterogeneity. The anticlustering approach thereby reverses the logic of cluster analysis that strives for high within-group homogeneity and clear separation between groups.  Computationally, anticlustering is accomplished by maximizing instead of minimizing a clustering objective function, such as the intra-cluster variance (used in k-means clustering) or the sum of pairwise distances within clusters. The main function anticlustering() gives access to optimal and heuristic anticlustering methods described in Papenberg and Klau (2021; <doi:10.1037/met0000301>), Brusco et al. (2020; <doi:10.1111/bmsp.12186>), Papenberg (2024;  <doi:10.1111/bmsp.12315>), Papenberg, Wang, et al. (2025; <doi:10.1016/j.crmeth.2025.101137>), Papenberg, Breuer, et al. (2025; <doi:10.1017/psy.2025.10052>), and Yang et al. (2022; <doi:10.1016/j.ejor.2022.02.003>). The optimal algorithms require that an integer linear programming solver is installed. This package will install 'lpSolve' (<https://cran.r-project.org/package=lpSolve>) as a default solver, but it is also possible to use the package 'Rglpk' (<https://cran.r-project.org/package=Rglpk>), which requires the GNU linear programming kit (<https://www.gnu.org/software/glpk/glpk.html>), the package 'Rsymphony' (<https://cran.r-project.org/package=Rsymphony>), which requires the SYMPHONY ILP solver (<https://github.com/coin-or/SYMPHONY>), or the commercial solver Gurobi, which provides its own R package that is not available via CRAN (<https://www.gurobi.com/downloads/>). 'Rglpk', 'Rsymphony', 'gurobi' and their system dependencies have to be manually installed by the user because they are only suggested dependencies. Full access to the bicriterion anticlustering method proposed by Brusco et al. (2020) is given via the function bicriterion_anticlustering(), while kplus_anticlustering() implements the full functionality of the k-plus anticlustering approach proposed by Papenberg (2024). Some other functions are available to solve classical clustering problems. The function balanced_clustering() applies a cluster analysis under size constraints, i.e., creates equal-sized clusters. The function matching() can be used for (unrestricted, bipartite, or K-partite) matching. The function wce() can be used optimally solve the (weighted) cluster editing problem, also known as correlation clustering, clique partitioning problem or transitivity clustering.",
    "version": "0.8.13",
    "maintainer": "Martin Papenberg <martin.papenberg@hhu.de>",
    "url": "https://github.com/m-Py/anticlust,\nhttps://m-py.github.io/anticlust/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8566,
    "package_name": "antigen.garnish",
    "title": "Tumor neoantigen prediction",
    "description": "Ensemble tumor neoantigen prediction from complex",
    "version": "2.3.1",
    "maintainer": "",
    "url": "https://github.com/andrewrech/antigen.garnish",
    "exports": [],
    "topics": ["bioinformatics", "immunoinformatics", "immunology", "machine-learning", "peptides"],
    "score": "NA",
    "stars": 49
  },
  {
    "id": 8590,
    "package_name": "apcluster",
    "title": "Affinity Propagation Clustering",
    "description": "Implements Affinity Propagation clustering introduced by Frey and\n\tDueck (2007) <DOI:10.1126/science.1136800>. The algorithms are largely\n        analogous to the 'Matlab' code published by Frey and Dueck.\n        The package further provides leveraged affinity propagation and an\n        algorithm for exemplar-based agglomerative clustering that can also be\n        used to join clusters obtained from affinity propagation. Various\n        plotting functions are available for analyzing clustering results.",
    "version": "1.4.14",
    "maintainer": "Ulrich Bodenhofer <ulrich@bodenhofer.com>",
    "url": "https://github.com/UBod/apcluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8603,
    "package_name": "aplot",
    "title": "Decorate a 'ggplot' with Associated Information",
    "description": "For many times, we are not just aligning plots as what 'cowplot' and 'patchwork' did. Users would like to align associated information that requires axes to be exactly matched in subplots, e.g. hierarchical clustering with a heatmap. Inspired by the 'Method 2' in 'ggtree' (G Yu (2018) <doi:10.1093/molbev/msy194>), 'aplot' provides utilities to aligns associated subplots to a main plot at different sides (left, right, top and bottom) with axes exactly matched. ",
    "version": "0.2.9",
    "maintainer": "Guangchuang Yu <guangchuangyu@gmail.com>",
    "url": "https://github.com/YuLab-SMU/aplot, https://yulab-smu.top/aplot/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8615,
    "package_name": "appnn",
    "title": "Amyloid Propensity Prediction Neural Network",
    "description": "Amyloid propensity prediction neural network (APPNN) is an amyloidogenicity propensity predictor based on a machine learning approach through recursive feature selection and feed-forward neural networks, taking advantage of newly published sequences with experimental, in vitro, evidence of amyloid formation.",
    "version": "1.0-1",
    "maintainer": "Carlos Família <carlosfamilia@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8629,
    "package_name": "aqp",
    "title": "Algorithms for Quantitative Pedology",
    "description": "The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice.",
    "version": "2.2-1",
    "maintainer": "Dylan Beaudette <dylan.beaudette@usda.gov>",
    "url": "https://ncss-tech.github.io/aqp/, https://ncss-tech.github.io/AQP/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8636,
    "package_name": "arc",
    "title": "Association Rule Classification",
    "description": "Implements the Classification-based on\n    Association Rules (CBA) algorithm for association rule classification.\n    The package, also described in Hahsler et al. (2019) <doi:10.32614/RJ-2019-048>,\n    contains several convenience methods that allow to automatically\n    set CBA parameters (minimum confidence, minimum support) and it also natively\n    handles numeric attributes by integrating a pre-discretization step.\n    The rule generation phase is handled by the 'arules' package. \n    To further decrease the size of the CBA models produced by the 'arc' package, postprocessing by the \n    'qCBA' package is suggested.",
    "version": "1.4.2",
    "maintainer": "Tomas Kliegr <kliegr@gmail.com>",
    "url": "https://github.com/kliegr/arc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8667,
    "package_name": "arenar",
    "title": "Arena for the Exploration and Comparison of any ML Models",
    "description": "Generates data for challenging machine learning models in 'Arena'\n    <https://arena.drwhy.ai> - an interactive web application. You can start\n    the server with XAI (Explainable Artificial Intelligence) plots to be\n    generated on-demand or precalculate and auto-upload data file beside\n    shareable 'Arena' URL.",
    "version": "0.2.0",
    "maintainer": "Piotr Piątyszek <piotrp@wektor.xyz>",
    "url": "https://arenar.drwhy.ai, https://github.com/ModelOriented/ArenaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8680,
    "package_name": "aricode",
    "title": "Efficient Computations of Standard Clustering Comparison\nMeasures",
    "description": "Implements an efficient O(n) algorithm based on bucket-sorting for \n    fast computation of standard clustering comparison measures. Available measures\n    include adjusted Rand index (ARI), normalized information distance (NID), \n    normalized mutual information (NMI), adjusted mutual information (AMI), \n    normalized variation information (NVI) and entropy, as described in Vinh et al (2009) \n    <doi:10.1145/1553374.1553511>. Include AMI (Adjusted Mutual Information) since version 0.1.2, \n    a modified version of ARI (MARI), as described in Sundqvist et al. <doi:10.1007/s00180-022-01230-7> \n    and simple Chi-square distance since version 1.0.0.",
    "version": "1.0.3",
    "maintainer": "Julien Chiquet <julien.chiquet@inrae.fr>",
    "url": "https://github.com/jchiquet/aricode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8701,
    "package_name": "arrg",
    "title": "Flexible Argument Parsing for R Scripts",
    "description": "Argument parsing for R scripts, with support for long and short\n    Unix-style options including option clustering, positional arguments\n    including those of variable length, and multiple usage patterns which may\n    take different subsets of options.",
    "version": "0.1.0",
    "maintainer": "Jon Clayden <code@clayden.org>",
    "url": "https://github.com/jonclayden/arrg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8712,
    "package_name": "arules",
    "title": "Mining Association Rules and Frequent Itemsets",
    "description": "Provides the infrastructure for representing, manipulating\n    and analyzing transaction data and patterns (frequent itemsets and\n    association rules).  Also provides C implementations of the\n    association mining algorithms Apriori and Eclat.  Hahsler, Gruen and\n    Hornik (2005) <doi:10.18637/jss.v014.i15>.",
    "version": "1.7-11",
    "maintainer": "Michael Hahsler <mhahsler@lyle.smu.edu>",
    "url": "https://github.com/mhahsler/arules",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8792,
    "package_name": "attention",
    "title": "Self-Attention Algorithm",
    "description": "Self-Attention algorithm helper functions and demonstration vignettes of increasing depth on how to construct the Self-Attention algorithm, this is based on Vaswani et al. (2017) <doi:10.48550/arXiv.1706.03762>, Dan Jurafsky and James H. Martin (2022, ISBN:978-0131873216) <https://web.stanford.edu/~jurafsky/slp3/> \"Speech and Language Processing (3rd ed.)\" and Alex Graves (2020) <https://www.youtube.com/watch?v=AIiwuClvH6k> \"Attention and Memory in Deep Learning\".",
    "version": "0.4.0",
    "maintainer": "Bastiaan Quast <bquast@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8806,
    "package_name": "augSIMEX",
    "title": "Analysis of Data with Mixed Measurement Error and\nMisclassification in Covariates",
    "description": "Implementation of the augmented\n            Simulation-Extrapolation (SIMEX) algorithm proposed by Yi et al. (2015) <doi:10.1080/01621459.2014.922777>\n            for analyzing the data with mixed measurement error and misclassification. The main\n            function provides a similar summary output as that of glm() function. Both parametric and\n            empirical SIMEX are considered in the package.",
    "version": "3.7.4",
    "maintainer": "Qihuang Zhang <qihuang.zhang@uwaterloo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8815,
    "package_name": "autoBagging",
    "title": "Learning to Rank Bagging Workflows with Metalearning",
    "description": "A framework for automated machine learning. Concretely, the focus is on the optimisation of bagging workflows. A bagging workflows is composed by three phases: (i) generation: which and how many predictive models to learn; (ii) pruning: after learning a set of models, the worst ones are cut off from the ensemble; and (iii) integration: how the models are combined for predicting a new observation. autoBagging optimises these processes by combining metalearning and a learning to rank approach to learn from metadata. It automatically ranks 63 bagging workflows by exploiting past performance and dataset characterization. A complete description of the method can be found in: Pinto, F., Cerqueira, V., Soares, C., Mendes-Moreira, J. (2017): \"autoBagging: Learning to Rank Bagging Workflows with Metalearning\" arXiv preprint arXiv:1706.09367.",
    "version": "0.1.0",
    "maintainer": "Vitor Cerqueira <cerqueira.vitormanuel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8829,
    "package_name": "autohrf",
    "title": "Automated Generation of Data-Informed GLM Models in Task-Based\nfMRI Data Analysis",
    "description": "Analysis of task-related functional magnetic resonance imaging (fMRI) activity at the level of individual participants is commonly based on general linear modelling (GLM) that allows us to estimate to what extent the blood oxygenation level dependent (BOLD) signal can be explained by task response predictors specified in the GLM model. The predictors are constructed by convolving the hypothesised timecourse of neural activity with an assumed hemodynamic response function (HRF). To get valid and precise estimates of task response, it is important to construct a model of neural activity that best matches actual neuronal activity. The construction of models is most often driven by predefined assumptions on the components of brain activity and their duration based on the task design and specific aims of the study. However, our assumptions about the onset and duration of component processes might be wrong and can also differ across brain regions. This can result in inappropriate or suboptimal models, bad fitting of the model to the actual data and invalid estimations of brain activity. Here we present an approach in which theoretically driven models of task response are used to define constraints based on which the final model is derived computationally using the actual data. Specifically, we developed 'autohrf' — a package for the 'R' programming language that allows for data-driven estimation of HRF models. The package uses genetic algorithms to efficiently search for models that fit the underlying data well. The package uses automated parameter search to find the onset and duration of task predictors which result in the highest fitness of the resulting GLM based on the fMRI signal under predefined restrictions. We evaluate the usefulness of the 'autohrf' package on publicly available datasets of task-related fMRI activity. Our results suggest that by using 'autohrf' users can find better task related brain activity models in a quick and efficient manner.",
    "version": "1.1.3",
    "maintainer": "Jure Demšar <jure.demsar@fri.uni-lj.si>",
    "url": "https://github.com/demsarjure/autohrf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8838,
    "package_name": "automl",
    "title": "Deep Learning with Metaheuristic",
    "description": "Fits from simple regression to highly customizable deep neural networks \n    either with gradient descent or metaheuristic, using automatic hyper parameters \n    tuning and custom cost function.\n    A mix inspired by the common tricks on Deep Learning and Particle Swarm Optimization.",
    "version": "1.3.2",
    "maintainer": "Alex Boulangé <aboul@free.fr>",
    "url": "https://aboulaboul.github.io/automl\nhttps://github.com/aboulaboul/automl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8845,
    "package_name": "autostats",
    "title": "Auto Stats",
    "description": "Automatically do statistical exploration. Create formulas using 'tidyselect' syntax, and then determine cross-validated model accuracy and variable contributions using 'glm' and 'xgboost'. Contains additional helper functions to create and modify formulas. Has a flagship function to quickly determine relationships between categorical and continuous variables in the data set.",
    "version": "0.4.1",
    "maintainer": "Harrison Tietze <harrison4192@gmail.com>",
    "url": "https://harrison4192.github.io/autostats/,\nhttps://github.com/Harrison4192/autostats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8847,
    "package_name": "autotab",
    "title": "Variational Autoencoders for Heterogeneous Tabular Data",
    "description": "Build and train a variational autoencoder (VAE) for mixed-type\n    tabular data (continuous, binary, categorical).\n    Models are implemented using 'TensorFlow' and 'Keras' via the 'reticulate' \n    interface, enabling reproducible VAE training for heterogeneous tabular \n    datasets.",
    "version": "0.1.1",
    "maintainer": "Sarah Milligan <slm1999@bu.edu>",
    "url": "https://github.com/SarahMilligan-hub/AutoTab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8866,
    "package_name": "aweSOM",
    "title": "Interactive Self-Organizing Maps",
    "description": "Self-organizing maps (also known as SOM, see Kohonen (2001) <doi:10.1007/978-3-642-56927-2>) are a method for dimensionality reduction and clustering of continuous data. This package introduces interactive (html) graphics for easier analysis of SOM results. It also features an interactive interface, for push-button training and visualization of SOM on numeric, categorical or mixed data, as well as tools to evaluate the quality of SOM.",
    "version": "1.3",
    "maintainer": "Julien Boelaert <julien.boelaert@univ-lille.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8907,
    "package_name": "bacistool",
    "title": "Bayesian Classification and Information Sharing (BaCIS) Tool for\nthe Design of Multi-Group Phase II Clinical Trials",
    "description": "Provides the design of multi-group phase\n    II clinical trials with binary outcomes using the hierarchical Bayesian\n    classification and information sharing (BaCIS) model. Subgroups are classified\n    into two clusters on the basis of their outcomes mimicking the hypothesis\n    testing framework. Subsequently, information sharing takes place within\n    subgroups in the same cluster, rather than across all subgroups. This method can\n    be applied to the design and analysis of multi-group clinical trials with binary\n    outcomes. Reference: Nan Chen and J. Jack Lee (2019) <doi:10.1002/bimj.201700275>.",
    "version": "1.0.0",
    "maintainer": "J. Jack Lee <jjlee@mdanderson.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8928,
    "package_name": "bahc",
    "title": "Filter Covariance and Correlation Matrices with\nBootstrapped-Averaged Hierarchical Ansatz",
    "description": "A method to filter correlation and covariance matrices by averaging\n     bootstrapped filtered hierarchical clustering and boosting. See Ch. Bongiorno and D. Challet,\n     Covariance matrix filtering with bootstrapped hierarchies (2020) <arXiv:2003.05807> and\n     Ch. Bongiorno and D. Challet, Reactive Global Minimum Variance Portfolios with k-BAHC covariance cleaning\n     (2020) <arXiv:2005.08703>.",
    "version": "0.3.0",
    "maintainer": "Damien Challet <damien.challet@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8949,
    "package_name": "banditpam",
    "title": "Almost Linear-Time k-Medoids Clustering",
    "description": "Interface to a high-performance implementation of k-medoids clustering described in Tiwari, Zhang, Mayclin, Thrun, Piech and Shomorony (2020) \"BanditPAM: Almost Linear Time k-medoids Clustering via Multi-Armed Bandits\" <https://proceedings.neurips.cc/paper/2020/file/73b817090081cef1bca77232f4532c5d-Paper.pdf>.",
    "version": "1.0-2",
    "maintainer": "Balasubramanian Narasimhan <naras@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8954,
    "package_name": "banffIT",
    "title": "Automated Standardized Assignment of the Banff Classification",
    "description": "Assigns standardized diagnoses using the Banff Classification\n    (Category 1 to 6 diagnoses, including Acute and Chronic active T-cell\n    mediated rejection as well as Active, Chronic active, and Chronic antibody\n    mediated rejection). The main function considers a minimal dataset\n    containing biopsies information in a specific format (described by a data\n    dictionary), verifies its content and format (based on the data dictionary),\n    assigns diagnoses, and creates a summary report. The package is developed on\n    the reference guide to the Banff classification of renal allograft pathology\n    Roufosse C, Simmonds N, Clahsen-van Groningen M, et al. A (2018) <doi:10.1097/TP.0000000000002366>.\n    The full description of the Banff classification is available at <https://banfffoundation.org/>.",
    "version": "2.0.0",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "url": "https://github.com/PersonalizedTransplantCare/banffIT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8974,
    "package_name": "bartXViz",
    "title": "Visualization of BART and BARP using SHAP",
    "description": "Complex machine learning models are often difficult to interpret. Shapley values serve as a powerful tool to understand and explain why a model makes a particular prediction. This package computes variable contributions using permutation-based Shapley values for Bayesian Additive Regression Trees (BART) and its extension with Post-Stratification (BARP). The permutation-based SHAP method proposed by Strumbel and Kononenko (2014) <doi:10.1007/s10115-013-0679-x> is grounded in data obtained via MCMC sampling. Similar to the BART model introduced by Chipman, George, and McCulloch (2010) <doi:10.1214/09-AOAS285>, this package leverages Bayesian posterior samples generated during model estimation, allowing variable contributions to be computed without requiring additional sampling. The BART model is designed to work with the following R packages: 'BART' <doi:10.18637/jss.v097.i01>, 'bartMachine' <doi:10.18637/jss.v070.i04>, and 'dbarts' <https://CRAN.R-project.org/package=dbarts>. For XGBoost and baseline adjustments, the approach by Lundberg et al. (2020) <doi:10.1038/s42256-019-0138-9> is also considered. The BARP model proposed by Bisbee (2019) <doi:10.1017/S0003055419000480> was implemented with reference to <https://github.com/jbisbee1/BARP> and is designed to work with modified functions based on that implementation. BARP extends post-stratification by computing variable contributions within each stratum defined by stratifying variables. The resulting Shapley values are visualized through both global and local explanation methods.",
    "version": "1.0.9",
    "maintainer": "Dong-eun Lee <ldongeun.leel@gmail.com>",
    "url": "https://github.com/ldongeunl/bartXViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8986,
    "package_name": "baseline",
    "title": "Baseline Correction of Spectra",
    "description": "Collection of baseline correction algorithms, along with a framework and a Tcl/Tk enabled GUI for optimising baseline algorithm parameters. Typical use of the package is for removing background effects from spectra originating from various types of spectroscopy and spectrometry, possibly optimizing this with regard to regression or classification results. Correction methods include polynomial fitting, weighted local smoothers and many more.",
    "version": "1.3-7",
    "maintainer": "Kristian Hovde Liland <kristian.liland@nmbu.no>",
    "url": "https://github.com/khliland/baseline/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8988,
    "package_name": "basemodels",
    "title": "Baseline Models for Classification and Regression",
    "description": "Providing equivalent functions for the dummy\n    classifier and regressor used in 'Python' 'scikit-learn' library. Our goal\n    is to allow R users to easily identify baseline performance for their\n    classification and regression problems. Our baseline models use no\n    predictors, and are useful in cases of class imbalance, multiclass\n    classification, and when users want to quickly identify how much\n    improvement their statistical and machine learning models are over several\n    baseline models. We use a \"better\" default (proportional guessing) for\n    the dummy classifier than the 'Python' implementation (\"prior\", which is\n    the most frequent class in the training set). The functions in the\n    package can be used on their own, or introduce methods named\n    'dummy_regressor' or 'dummy_classifier' that can be used within the\n    caret package pipeline.",
    "version": "1.1.0",
    "maintainer": "Ying-Ju Chen <ychen4@udayton.edu>",
    "url": "https://github.com/Ying-Ju/basemodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9002,
    "package_name": "basket",
    "title": "Basket Trial Analysis",
    "description": "Implementation of multisource exchangeability models for Bayesian analyses of prespecified subgroups arising in the context of basket trial design and monitoring.  The R 'basket' package facilitates implementation of the binary, symmetric multi-source exchangeability model (MEM) with posterior inference arising from both exact computation and Markov chain Monte Carlo sampling. Analysis output includes full posterior samples as well as posterior probabilities, highest posterior density (HPD) interval boundaries, effective sample sizes (ESS), mean and median estimations, posterior exchangeability probability matrices, and maximum a posteriori MEMs. In addition to providing \"basketwise\" analyses, the package includes similar calculations for \"clusterwise\" analyses for which subgroups are combined into meta-baskets, or clusters, using graphical clustering algorithms that treat the posterior exchangeability probabilities as edge weights. In addition plotting tools are provided to visualize basket and cluster densities as well as their exchangeability.  References include Hyman, D.M., Puzanov, I., Subbiah, V., Faris, J.E., Chau, I., Blay, J.Y., Wolf, J., Raje, N.S., Diamond, E.L., Hollebecque, A. and Gervais, R (2015) <doi:10.1056/NEJMoa1502309>; Hobbs, B.P. and Landin, R. (2018) <doi:10.1002/sim.7893>; Hobbs, B.P., Kane, M.J., Hong, D.S. and Landin, R. (2018) <doi:10.1093/annonc/mdy457>; and Kaizer, A.M., Koopmeiners, J.S. and Hobbs, B.P. (2017) <doi:10.1093/biostatistics/kxx031>.",
    "version": "0.10.11",
    "maintainer": "Michael J. Kane <michael.kane@yale.edu>",
    "url": "https://github.com/kaneplusplus/basket",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9085,
    "package_name": "bbl",
    "title": "Boltzmann Bayes Learner",
    "description": "Supervised learning using Boltzmann Bayes model inference, \n    which extends naive Bayes model to include interactions. Enables \n    classification of data into multiple response groups based on a large \n    number of discrete predictors that can take factor values of \n    heterogeneous levels. Either pseudo-likelihood or mean field \n    inference can be used with L2 regularization, cross-validation, and \n    prediction on new data. \n    <doi:10.18637/jss.v101.i05>.",
    "version": "1.0.0",
    "maintainer": "Jun Woo <junwoo035@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9132,
    "package_name": "bdsvd",
    "title": "Block Structure Detection Using Singular Vectors",
    "description": "Performs block diagonal covariance matrix detection using singular vectors (BD-SVD), which can be extended to hierarchical variable clustering (HC-SVD). The methods are described in Bauer (2024) <doi:10.1080/10618600.2024.2422985> and Bauer (202X) <doi:10.48550/arXiv.2308.06820>.",
    "version": "0.2.1",
    "maintainer": "Jan O. Bauer <j.bauer@vu.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9188,
    "package_name": "betafunctions",
    "title": "Functions for Working with Two- And Four-Parameter Beta\nProbability Distributions and Psychometric Analysis of\nClassifications",
    "description": "Package providing a number of functions for working with Two- and \n    Four-parameter Beta and closely related distributions (i.e., the Gamma-\n    Binomial-, and Beta-Binomial distributions).\n        Includes, among other things: \n    - d/p/q/r functions for Four-Parameter Beta distributions and Generalized\n    \"Binomial\" (continuous) distributions, and d/p/r- functions for Beta-\n    Binomial distributions.\n    - d/p/q/r functions for Two- and Four-Parameter Beta distributions\n    parameterized in terms of their means and variances rather than their\n    shape-parameters.\n    - Moment generating functions for Binomial distributions, Beta-Binomial \n    distributions, and observed value distributions.\n    - Functions for estimating classification accuracy and consistency, \n    making use of the Classical Test-Theory based 'Livingston and Lewis' (L&L) \n    and 'Hanson and Brennan' approaches.\n      A shiny app is available, providing a GUI for the L&L approach when used \n    for binary classifications. For url to the app, see documentation for the \n    LL.CA() function.\n    Livingston and Lewis (1995) <doi:10.1111/j.1745-3984.1995.tb00462.x>.\n    Lord (1965) <doi:10.1007/BF02289490>.\n    Hanson (1991) <https://files.eric.ed.gov/fulltext/ED344945.pdf>.",
    "version": "1.9.0",
    "maintainer": "Haakon Eidem Haakstad <h.e.haakstad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9205,
    "package_name": "bfcluster",
    "title": "Buttler-Fickel Distance and R2 for Mixed-Scale Cluster Analysis",
    "description": "\n    Implements the distance measure for mixed-scale variables proposed by \n    Buttler and Fickel (1995), based on normalized mean pairwise distances \n    (Gini mean difference), and an R2 statistic to assess clustering quality.",
    "version": "1.0.0",
    "maintainer": "Moritz Schäfer <moritz1.schaefer@uni-a.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9225,
    "package_name": "bhpm",
    "title": "Bayesian Hierarchical Poisson Models for Multiple Grouped\nOutcomes with Clustering",
    "description": "Bayesian hierarchical methods for the detection of differences in rates of related outcomes for multiple treatments for clustered observations (Carragher et al. (2020) <doi:10.1002/sim.8563>). This software was developed for the Precision Drug Theraputics: Risk Prediction in Pharmacoepidemiology project as part of a Rutherford Fund Fellowship at Health Data Research (UK), Medical Research Council (UK) award reference MR/S003967/1 (<https://gtr.ukri.org/>). Principal Investigator: Raymond Carragher.",
    "version": "1.8.1",
    "maintainer": "Raymond Carragher <rcarragh@gmail.com>",
    "url": "https://github.com/rcarragh/bhpm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9282,
    "package_name": "bikm1",
    "title": "Co-Clustering Adjusted Rand Index and Bikm1 Procedure for\nContingency and Binary Data-Sets",
    "description": "Co-clustering of the rows and columns of a contingency or binary matrix, or double binary matrices and model selection for the number of row and column clusters. Three models are considered: the Poisson latent block model for contingency matrix, the binary latent block model for binary matrix and a new model we develop: the multiple latent block model for double binary matrices. A new procedure named bikm1 is implemented to investigate more efficiently the grid of numbers of clusters. Then, the studied model selection criteria are the integrated completed likelihood (ICL) and the Bayesian integrated likelihood (BIC). Finally, the co-clustering adjusted Rand index (CARI) to measure agreement between co-clustering partitions is implemented. Robert Valerie, Vasseur Yann, Brault Vincent (2021) <doi:10.1007/s00357-020-09379-w>.",
    "version": "1.1.0",
    "maintainer": "Valerie Robert <valerie.robert.math@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9313,
    "package_name": "binr",
    "title": "Cut Numeric Values into Evenly Distributed Groups",
    "description": "Implementation of algorithms for cutting numerical values\n    exhibiting a potentially highly skewed distribution into evenly distributed\n    groups (bins). This functionality can be applied for binning discrete\n    values, such as counts, as well as for discretization of continuous values,\n    for example, during generation of features used in machine learning\n    algorithms.",
    "version": "1.1.1",
    "maintainer": "Sergei Izrailev <sizrailev@jabiruventures.com>",
    "url": "https://github.com/jabiru/binr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9318,
    "package_name": "binsreg",
    "title": "Binscatter Estimation and Inference",
    "description": "Provides tools for statistical analysis using the binscatter methods developed by Cattaneo, Crump, Farrell and Feng (2024a) <doi:10.48550/arXiv.1902.09608>, Cattaneo, Crump, Farrell and Feng (2024b) <https://nppackages.github.io/references/Cattaneo-Crump-Farrell-Feng_2024_NonlinearBinscatter.pdf> and Cattaneo, Crump, Farrell and Feng (2024c) <doi:10.48550/arXiv.1902.09615>. Binscatter provides a flexible way of describing the relationship between two variables based on partitioning/binning of the independent variable of interest. binsreg(), binsqreg() and binsglm() implement binscatter least squares regression, quantile regression and generalized linear regression respectively, with particular focus on constructing binned scatter plots. They also implement robust (pointwise and uniform) inference of regression functions and derivatives thereof. binstest() implements hypothesis testing procedures for parametric functional forms of and nonparametric shape restrictions on the regression function. binspwc() implements hypothesis testing procedures for pairwise group comparison of binscatter estimators. binsregselect() implements data-driven procedures for selecting the number of bins for binscatter estimation. All the commands allow for covariate adjustment, smoothness restrictions and clustering.",
    "version": "1.1",
    "maintainer": "Yingjie Feng <fengyingjiepku@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9319,
    "package_name": "binst",
    "title": "Data Preprocessing, Binning for Classification and Regression",
    "description": "Various supervised and unsupervised binning tools\n    including using entropy, recursive partition methods\n    and clustering.",
    "version": "0.2.1",
    "maintainer": "Chapman Siu <chpmn.siu@gmail.com>",
    "url": "https://github.com/jules-and-dave/binst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9327,
    "package_name": "bioacoustics",
    "title": "Analyse Audio Recordings and Automatically Extract Animal\nVocalizations",
    "description": "Contains all the necessary tools to process audio recordings of\n             various formats (e.g., WAV, WAC, MP3, ZC), filter noisy files, \n             display audio signals, detect and extract automatically acoustic\n             features for further analysis such as classification.",
    "version": "0.2.10",
    "maintainer": "Jean Marchal <jean.marchal@wavx.ca>",
    "url": "https://github.com/wavx/bioacoustics/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9364,
    "package_name": "biopixR",
    "title": "Extracting Insights from Biological Images",
    "description": "Combines the 'magick' and 'imager' packages to streamline image analysis, focusing on feature extraction and quantification from biological images, especially \n    microparticles. By providing high throughput pipelines and clustering capabilities, 'biopixR' facilitates efficient insight generation for researchers (Schneider J. et al. (2019) \n    <doi:10.21037/jlpm.2019.04.05>).",
    "version": "1.2.0",
    "maintainer": "Tim Brauckhoff <brauctile@disroot.org>",
    "url": "https://github.com/Brauckhoff/biopixR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9365,
    "package_name": "bioregion",
    "title": "Comparison of Bioregionalisation Methods",
    "description": "The main purpose of this package is to propose a transparent methodological framework to compare bioregionalisation methods based on hierarchical and non-hierarchical clustering algorithms (Kreft & Jetz (2010) <doi:10.1111/j.1365-2699.2010.02375.x>) and network algorithms (Lenormand et al. (2019) <doi:10.1002/ece3.4718> and Leroy et al. (2019) <doi:10.1111/jbi.13674>).",
    "version": "1.2.0",
    "maintainer": "Maxime Lenormand <maxime.lenormand@inrae.fr>",
    "url": "https://github.com/bioRgeo/bioregion,\nhttps://bioRgeo.github.io/bioregion/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9385,
    "package_name": "biplotbootGUI",
    "title": "Bootstrap on Classical Biplots and Clustering Disjoint Biplot",
    "description": "A GUI with which the user can construct and interact with Bootstrap methods on Classical Biplots and with Clustering and/or Disjoint Biplot. This GUI is also aimed for estimate any numerical data matrix using the Clustering and Disjoint Principal component (CDPCA) methodology.",
    "version": "1.3",
    "maintainer": "Ana Belen Nieto Librero <ananieto@usal.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9388,
    "package_name": "birdnetR",
    "title": "Deep Learning for Automated (Bird) Sound Identification",
    "description": "Use 'BirdNET', a state-of-the-art deep learning classifier, to automatically identify (bird) sounds.\n    Analyze bioacoustic datasets without any computer science background using a pre-trained model or a custom trained classifier.\n    Predict bird species occurrence based on location and week of the year.\n    Kahl, S., Wood, C. M., Eibl, M., & Klinck, H. (2021) <doi:10.1016/j.ecoinf.2021.101236>.",
    "version": "0.3.2",
    "maintainer": "Felix Günther <felix.guenther@informatik.tu-chemnitz.de>",
    "url": "https://birdnet-team.github.io/birdnetR/,\nhttps://github.com/birdnet-team/birdnetR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9390,
    "package_name": "birdscanR",
    "title": "Migration Traffic Rate Calculation Package for 'Birdscan MR1'\nRadars",
    "description": "Extract data from 'Birdscan MR1' 'SQL' vertical-looking radar databases, filter, and process them to Migration Traffic Rates (#objects per hour and km) or density (#objects per km3) of, for example birds, and insects. Object classifications in the 'Birdscan MR1' databases are based on the dataset of Haest et al. (2021) <doi:10.5281/zenodo.5734960>). Migration Traffic Rates and densities can be calculated separately for different height bins (with a height resolution of choice) as well as over time periods of choice (e.g., 1/2 hour, 1 hour, 1 day, day/night, the full time period of observation, and anything in between). Two plotting functions are also included to explore the data in the 'SQL' databases and the resulting Migration Traffic Rate results. For details on the Migration Traffic Rate calculation procedures, see Schmid et al. (2019) <doi:10.1111/ecog.04025>.",
    "version": "0.3.0",
    "maintainer": "Birgen Haest <birgen.haest@vogelwarte.ch>",
    "url": "https://github.com/BirdScanCommunity/birdscanR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9442,
    "package_name": "blockcluster",
    "title": "Co-Clustering Package for Binary, Categorical, Contingency and\nContinuous Data-Sets",
    "description": "Simultaneous clustering of rows and columns, usually designated by\n    biclustering, co-clustering or block clustering, is an important technique\n    in two way data analysis. It consists of estimating a mixture model which\n    takes into account the block clustering problem on both the individual and\n    variables sets. The 'blockcluster' package provides a bridge between the C++\n    core library build on top of the 'STK++' library, and the R statistical\n    computing environment. This package allows to co-cluster binary\n    <doi:10.1016/j.csda.2007.09.007>, contingency <doi:10.1080/03610920903140197>,\n    continuous <doi:10.1007/s11634-013-0161-3> and categorical data-sets\n    <doi:10.1007/s11222-014-9472-2>. It also provides utility functions to\n    visualize the results. This package may be useful for various applications\n    in fields of Data mining, Information retrieval, Biology, computer vision\n    and many more. More information about the project and comprehensive tutorial\n    can be found on the link mentioned in URL.",
    "version": "4.5.5",
    "maintainer": "Serge Iovleff <Serge.Iovleff@stkpp.org>",
    "url": "https://gitlab.inria.fr/iovleff/blockcluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9461,
    "package_name": "blox",
    "title": "Block Diagonal Matrix Approximation",
    "description": "Finds the best block diagonal matrix approximation of a symmetric matrix. This can be exploited for divisive hierarchical clustering using singular vectors, named HC-SVD. The method is described in Bauer (202Xa) <doi:10.48550/arXiv.2308.06820>.",
    "version": "0.0.1",
    "maintainer": "Jan O. Bauer <j.bauer@vu.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9494,
    "package_name": "bnns",
    "title": "Bayesian Neural Network with 'Stan'",
    "description": "Offers a flexible formula-based interface for building and training Bayesian Neural Networks powered by 'Stan'. The package supports modeling complex relationships while providing rigorous uncertainty quantification via posterior distributions. With features like user chosen priors, clear predictions, and support for regression, binary, and multi-class classification, it is well-suited for applications in clinical trials, finance, and other fields requiring robust Bayesian inference and decision-making. References: Neal(1996) <doi:10.1007/978-1-4612-0745-0>.",
    "version": "0.1.2",
    "maintainer": "Swarnendu Chatterjee <swarnendu.stat@gmail.com>",
    "url": "https://github.com/swarnendu-stat/bnns,\nhttps://swarnendu-stat.github.io/bnns/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9499,
    "package_name": "bnviewer",
    "title": "Bayesian Networks Interactive Visualization and Explainable\nArtificial Intelligence",
    "description": "Bayesian networks provide an intuitive framework for probabilistic reasoning \n             and its graphical nature can be interpreted quite clearly. Graph based methods \n             of machine learning are becoming more popular because they offer a richer model \n             of knowledge that can be understood by a human in a graphical format. The 'bnviewer' \n             is an R Package that allows the interactive visualization of Bayesian Networks. \n             The aim of this package is to improve the Bayesian Networks visualization over \n             the basic and static views offered by existing packages.",
    "version": "0.1.6",
    "maintainer": "Robson Fernandes <robson.fernandes@usp.br>",
    "url": "http://robsonfernandes.net/bnviewer/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9543,
    "package_name": "bootcluster",
    "title": "Bootstrapping Estimates of Clustering Stability",
    "description": "Implementation of the bootstrapping approach for the estimation of clustering stability and its application in estimating the number of clusters, as introduced by Yu et al (2016)<doi:10.1142/9789814749411_0007>. Implementation of the non-parametric bootstrap approach to assessing the stability of module detection in a graph, the extension for the selection of a parameter set that defines a graph from data in a way that optimizes stability and the corresponding visualization functions, as introduced by Tian et al (2021) <doi:10.1002/sam.11495>. Implemented out-of-bag stability estimation function and k-select Smin-based k-selection function as introduced by Liu et al (2022) <doi:10.1002/sam.11593>. Implemented ensemble clustering method based-on k-means clustering method, spectral clustering method and hierarchical clustering method.",
    "version": "0.4.3",
    "maintainer": "Tianmou Liu <tianmouliu@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9570,
    "package_name": "boxplotcluster",
    "title": "Clustering Method Based on Boxplot Statistics",
    "description": "Following Arroyo-Maté-Roque (2006), the function calculates the distance between rows or columns of the dataset using the generalized Minkowski metric as described by Ichino-Yaguchi (1994). The distance measure gives more weight to differences between quartiles than to differences between extremes, making it less sensitive to outliers. Further,the function calculates the silhouette width (Rousseeuw 1987) for different numbers of clusters and selects the number of clusters that maximizes the average silhouette width, unless a specific number of clusters is provided by the user. The approach implemented in this package is based on the following publications: Rousseeuw (1987) <doi:10.1016/0377-0427(87)90125-7>; Ichino-Yaguchi (1994) <doi:10.1109/21.286391>; Arroyo-Maté-Roque (2006) <doi:10.1007/3-540-34416-0_7>.",
    "version": "0.3",
    "maintainer": "Gianmarco Alberti <gianmarcoalberti@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9581,
    "package_name": "bpgmm",
    "title": "Bayesian Model Selection Approach for Parsimonious Gaussian\nMixture Models",
    "description": "Model-based clustering using Bayesian parsimonious Gaussian mixture models.\n  MCMC (Markov chain Monte Carlo) are used for parameter estimation. The RJMCMC (Reversible-jump Markov chain Monte Carlo) is used for model selection. \n  GREEN et al. (1995) <doi:10.1093/biomet/82.4.711>.",
    "version": "1.1.1",
    "maintainer": "Yaoxiang Li <yl814@georgetown.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9627,
    "package_name": "briKmeans",
    "title": "Package for Brik, Fabrik and Fdebrik Algorithms to Initialise\nKmeans",
    "description": "Implementation of the BRIk, FABRIk and FDEBRIk algorithms \n        to initialise k-means. These methods are intended for the \n        clustering of multivariate and functional data, respectively.\n        They make use of the Modified Band Depth and bootstrap to \n        identify appropriate initial seeds for k-means, which are \n        proven to be better options than many techniques in the \n        literature. Torrente and Romo (2021) <doi:10.1007/s00357-020-09372-3>\n        It makes use of the functions kma and kma.similarity, from the \n        archived package fdakma, by Alice Parodi et al.",
    "version": "1.0",
    "maintainer": "Aurora Torrente <etorrent@est-econ.uc3m.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9634,
    "package_name": "brif",
    "title": "A Tree and Forest Tool for Classification and Regression",
    "description": "Build decision trees and random forests for classification and regression. The implementation strikes a balance between minimizing computing efforts and maximizing the expected predictive accuracy, thus scales well to large data sets. Multi-threading is available through 'OpenMP' <https://gcc.gnu.org/wiki/openmp>.  ",
    "version": "1.4.1",
    "maintainer": "Yanchao Liu <yanchaoliu@wayne.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9645,
    "package_name": "brnn",
    "title": "Bayesian Regularization for Feed-Forward Neural Networks",
    "description": "Bayesian regularization for feed-forward neural networks.",
    "version": "0.9.4",
    "maintainer": "Paulino Perez Rodriguez <perpdgo@colpos.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9658,
    "package_name": "brsim",
    "title": "Brainerd-Robinson Similarity Coefficient Matrix",
    "description": "Provides the facility to calculate the Brainerd-Robinson similarity coefficient for the rows of an input table, and to calculate the significance of each coefficient based on a permutation approach; a heatmap is produced to visually represent the similarity matrix. Optionally, hierarchical agglomerative clustering can be performed and the silhouette method is used to identify an optimal number of clusters; the results of the clustering can be optionally used to sort the heatmap.",
    "version": "0.3",
    "maintainer": "Gianmarco Alberti <gianmarcoalberti@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9677,
    "package_name": "bsnsing",
    "title": "Build Decision Trees with Optimal Multivariate Splits",
    "description": "Functions for training an optimal decision tree classifier, making predictions and generating latex code for plotting. Works for two-class and multi-class classification problems. The algorithm seeks the optimal Boolean rule consisting of multiple variables to split a node, resulting in shorter trees. Use bsnsing() to build a tree, predict() to make predictions and plot() to plot the tree into latex and PDF. See Yanchao Liu (2022) <arXiv:2205.15263> for technical details. Source code and more data sets are at <https://github.com/profyliu/bsnsing/>.",
    "version": "1.0.1",
    "maintainer": "Yanchao Liu <yanchaoliu@wayne.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9688,
    "package_name": "bst",
    "title": "Gradient Boosting",
    "description": "Functional gradient descent algorithm for a variety of convex and non-convex loss functions, for both classical and robust regression and classification problems. See Wang (2011) <doi:10.2202/1557-4679.1304>, Wang (2012) <doi:10.3414/ME11-02-0020>, Wang (2018) <doi:10.1080/10618600.2018.1424635>, Wang (2018) <doi:10.1214/18-EJS1404>.",
    "version": "0.3-24",
    "maintainer": "Zhu Wang <zwang145@uthsc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9697,
    "package_name": "btml",
    "title": "Bayesian Treed Machine Learning for Personalized Prediction and\nPrecision Diagnostics",
    "description": "Generalization of the Bayesian classification and regression tree (CART) model that partitions subjects into terminal nodes and tailors machine learning model to each terminal node.",
    "version": "0.1.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9698,
    "package_name": "btrm",
    "title": "Bayesian Treed Regression Model for Personalized Prediction and\nPrecision Diagnostics",
    "description": "Generalization of the Bayesian classification and regression tree (CART) model that partitions subjects into terminal nodes and tailors regression model to each terminal node.",
    "version": "0.2.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9772,
    "package_name": "cacIRT",
    "title": "Classification Accuracy and Consistency under Item Response\nTheory",
    "description": "Computes classification accuracy and consistency indices under Item Response Theory. Implements the total score IRT-based methods in Lee, Hanson & Brennen (2002) and Lee (2010), the IRT-based methods in Rudner (2001, 2005), and the total score nonparametric methods in Lathrop & Cheng (2014). For dichotomous and polytomous tests.",
    "version": "1.4",
    "maintainer": "Quinn N. Lathrop <quinn.lathrop@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9854,
    "package_name": "cardinalR",
    "title": "Collection of Data Structures",
    "description": "A collection of functions to generate a large variety of\n    structures in high dimensions. These data structures are useful for\n    testing, validating, and improving algorithms used in dimensionality\n    reduction, clustering, machine learning, and visualization.",
    "version": "1.0.6",
    "maintainer": "Jayani P. Gamage <jayanilakshika76@gmail.com>",
    "url": "https://jayanilakshika.github.io/cardinalR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9864,
    "package_name": "caretSDM",
    "title": "Build Species Distribution Modeling using 'caret'",
    "description": "Use machine learning algorithms and advanced geographic information system tools to\n    build Species Distribution Modeling in a extensible and modern fashion. ",
    "version": "1.2.3",
    "maintainer": "Luíz Fernando Esser <luizesser@gmail.com>",
    "url": "https://luizesser.github.io/caretSDM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9886,
    "package_name": "cases",
    "title": "Stratified Evaluation of Subgroup Classification Accuracy",
    "description": "Enables simultaneous statistical inference for the accuracy of multiple classifiers in multiple subgroups (strata). For instance, allows to perform multiple comparisons in diagnostic accuracy studies with co-primary endpoints sensitivity and specificity (Westphal M, Zapf A. Statistical inference for diagnostic test accuracy studies with multiple comparisons. Statistical Methods in Medical Research. 2024;0(0). <doi:10.1177/09622802241236933>).",
    "version": "0.2.0",
    "maintainer": "Max Westphal <dev@maxwestphal.io>",
    "url": "https://github.com/maxwestphal/cases,\nhttps://maxwestphal.github.io/cases/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9902,
    "package_name": "catch",
    "title": "Covariate-Adjusted Tensor Classification in High-Dimensions",
    "description": "Performs classification and variable selection on high-dimensional tensors (multi-dimensional arrays) after adjusting for additional covariates (scalar or vectors) as CATCH model in Pan, Mai and Zhang (2018) <arXiv:1805.04421>. The low-dimensional covariates and the high-dimensional tensors are jointly modeled to predict a categorical outcome in a multi-class discriminant analysis setting. The Covariate-Adjusted Tensor Classification in High-dimensions (CATCH) model is fitted in two steps: (1) adjust for the covariates within each class; and (2) penalized estimation with the adjusted tensor using a cyclic block coordinate descent algorithm. The package can provide a solution path for tuning parameter in the penalized estimation step. Special case of the CATCH model includes linear discriminant analysis model and matrix (or tensor) discriminant analysis without covariates.",
    "version": "1.0.1",
    "maintainer": "Yuqing Pan <yuqing.pan@stat.fsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9926,
    "package_name": "causalDT",
    "title": "Causal Distillation Trees",
    "description": "Causal Distillation Tree (CDT) is a novel machine learning method \n    for estimating interpretable subgroups with heterogeneous treatment effects. \n    CDT allows researchers to fit any machine learning model (or metalearner) to \n    estimate heterogeneous treatment effects for each individual, and then \n    \"distills\" these predicted heterogeneous treatment effects into \n    interpretable subgroups by fitting an ordinary decision tree to predict the\n    previously-estimated heterogeneous treatment effects. This package \n    provides tools to estimate causal distillation trees (CDT), as detailed in\n    Huang, Tang, and Kenney (2025) <doi:10.48550/arXiv.2502.07275>.",
    "version": "1.0.0",
    "maintainer": "Tiffany Tang <ttang4@nd.edu>",
    "url": "https://tiffanymtang.github.io/causalDT/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9933,
    "package_name": "causalToolbox",
    "title": "Toolbox for Causal Inference with emphasize on Heterogeneous Treatment Effect Estimator",
    "description": "Estimate heterogeneous treatment effects in experimental and",
    "version": "0.0.2.4",
    "maintainer": "Sören Künzel <srk@berkeley.edu>, Allen Tang <actang.me@gmail.com>",
    "url": "https://github.com/forestry-labs/causalToolbox",
    "exports": [],
    "topics": ["bayesian-additive-regression-trees", "causal-inference", "inference", "interpretability", "machine-learning", "random-forest", "treatment-effects"],
    "score": "NA",
    "stars": 48
  },
  {
    "id": 9943,
    "package_name": "caviarpd",
    "title": "Cluster Analysis via Random Partition Distributions",
    "description": "Cluster analysis is performed using pairwise distance information and a random partition distribution. The method is\n             implemented for two random partition distributions. It draws samples and then obtains and plots clustering estimates.\n             An implementation of a selection algorithm is provided for the mass parameter of the partition distribution. Since \n             pairwise distances are the principal input to this procedure, it is most comparable to the hierarchical and k-medoids\n             clustering methods. The method is Dahl, Andros, Carter (2022+) <doi:10.1002/sam.11602>.",
    "version": "0.3.21",
    "maintainer": "David B. Dahl <dahl@stat.byu.edu>",
    "url": "https://github.com/dbdahl/caviarpd-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9945,
    "package_name": "cba",
    "title": "Clustering for Business Analytics",
    "description": "Implements clustering techniques such as Proximus and Rock, utility functions for efficient computation of cross distances and data manipulation. ",
    "version": "0.2-25",
    "maintainer": "Christian Buchta <christian.buchta@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9964,
    "package_name": "cccm",
    "title": "Crossed Classification Credibility Model",
    "description": "Calculates the credit debt for the next period based on the available data using the cross-classification credibility model.",
    "version": "0.1.0",
    "maintainer": "Muhlis Ozdemir <muhlisozdemir@gazi.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9968,
    "package_name": "ccda",
    "title": "Combined Cluster and Discriminant Analysis",
    "description": "Implements the combined cluster and discriminant analysis method for finding homogeneous groups of data with known origin as described in Kovacs et. al (2014): Classification into homogeneous groups using combined cluster and discriminant analysis (CCDA). Environmental Modelling & Software. <doi:10.1016/j.envsoft.2014.01.010>.",
    "version": "1.1.1",
    "maintainer": "Solt Kovacs <ccda@caesar.elte.hu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9973,
    "package_name": "cclust",
    "title": "Convex Clustering Methods and Clustering Indexes",
    "description": "Convex Clustering methods, including K-means algorithm,\n  On-line Update algorithm (Hard Competitive Learning) and Neural Gas\n  algorithm (Soft Competitive Learning), and calculation of several\n  indexes for finding the number of clusters in a data set.",
    "version": "0.6-26",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10004,
    "package_name": "cdmTools",
    "title": "Useful Tools for Cognitive Diagnosis Modeling",
    "description": "Provides useful tools for cognitive diagnosis modeling (CDM). The package includes functions for empirical Q-matrix estimation and validation, such as the Hull method (Nájera, Sorrel, de la Torre, & Abad, 2021, <doi:10.1111/bmsp.12228>) and the discrete factor loading method (Wang, Song, & Ding, 2018, <doi:10.1007/978-3-319-77249-3_29>). It also contains dimensionality assessment procedures for CDM, including parallel analysis and automated fit comparison as explored in Nájera, Abad, and Sorrel (2021, <doi:10.3389/fpsyg.2021.614470>). Other relevant methods and features for CDM applications, such as the restricted DINA model (Nájera et al., 2023; <doi:10.3102/10769986231158829>), the general nonparametric classification method (Chiu et al., 2018; <doi:10.1007/s11336-017-9595-4>), and corrected estimation of the classification accuracy via multiple imputation (Kreitchmann et al., 2022; <doi:10.3758/s13428-022-01967-5>) are also available. Lastly, the package provides some useful functions for CDM simulation studies, such as random Q-matrix generation and detection of complete/identified Q-matrices.",
    "version": "1.0.6",
    "maintainer": "Pablo Nájera <p.najeraalvarez@gmail.com>",
    "url": "https://github.com/pablo-najera/cdmTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10030,
    "package_name": "cemco",
    "title": "Fit 'CemCO' Algorithm",
    "description": "'CemCO' algorithm, a model-based (Gaussian) clustering algorithm that removes/minimizes the effects of undesirable covariates during the clustering process both in cluster centroids and in cluster covariance structures (Relvas C. & Fujita A., (2020) <arXiv:2004.02333>).",
    "version": "0.2",
    "maintainer": "Andre Fujita <andrefujita@usp.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10065,
    "package_name": "ceterisParibus",
    "title": "Ceteris Paribus Profiles",
    "description": "Ceteris Paribus Profiles (What-If Plots) are designed to present model \n    responses around selected points in a feature space. \n    For example around a single prediction for an interesting observation. \n    Plots are designed to work in a model-agnostic fashion, they are working \n    for any predictive Machine Learning model and allow for model comparisons.\n    Ceteris Paribus Plots supplement the Break Down Plots from 'breakDown' package.",
    "version": "0.6",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "url": "https://pbiecek.github.io/ceterisParibus/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10233,
    "package_name": "circlus",
    "title": "Clustering and Simulation of Spherical Cauchy and PKBD Models",
    "description": "Provides tools for estimation and clustering of spherical data, seamlessly integrated with the 'flexmix' package. Includes the necessary M-step implementations for both Poisson Kernel-Based Distribution (PKBD) and spherical Cauchy distribution. Additionally, the package provides random number generators for PKBD and spherical Cauchy distribution. Methods are based on Golzy M., Markatou M. (2020) <doi:10.1080/10618600.2020.1740713>, Kato S., McCullagh P. (2020) <doi:10.3150/20-bej1222> and Sablica L., Hornik K., Leydold J. (2023) <doi:10.1214/23-ejs2149>.",
    "version": "0.0.2",
    "maintainer": "Lukas Sablica <lsablica@wu.ac.at>",
    "url": "https://github.com/lsablica/circlus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10246,
    "package_name": "citrus",
    "title": "Customer Intelligence Tool for Rapid Understandable Segmentation",
    "description": "A tool to easily run and visualise supervised and unsupervised state of the art customer segmentation. \n    It is built like a pipeline covering the 3 main steps in a segmentation project: pre-processing, modelling, and plotting.\n    Users can either run the pipeline as a whole, or choose to run any one of the three individual steps.\n    It is equipped with a supervised option (tree optimisation) and an unsupervised option (k-clustering) as default models.",
    "version": "1.0.2",
    "maintainer": "Dom Clarke <dom.clarke@peak.ai>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10255,
    "package_name": "cjbart",
    "title": "Heterogeneous Effects Analysis of Conjoint Experiments",
    "description": "A tool for analyzing conjoint experiments using Bayesian Additive Regression Trees ('BART'), a machine learning method developed by Chipman, George and McCulloch (2010) <doi:10.1214/09-AOAS285>. This tool focuses specifically on estimating, identifying, and visualizing the heterogeneity within marginal component effects, at the observation- and individual-level. It uses a variable importance measure ('VIMP') with delete-d jackknife variance estimation, following Ishwaran and Lu (2019) <doi:10.1002/sim.7803>, to obtain bias-corrected estimates of which variables drive heterogeneity in the predicted individual-level effects.",
    "version": "0.3.2",
    "maintainer": "Thomas Robinson <ts.robinson1994@gmail.com>",
    "url": "https://github.com/tsrobinson/cjbart",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10258,
    "package_name": "clValid",
    "title": "Validation of Clustering Results",
    "description": "Statistical and biological validation of clustering results. This package implements Dunn Index, Silhouette, Connectivity, Stability, BHI and BSI. Further information can be found in Brock, G et al. (2008) <doi: 10.18637/jss.v025.i04>.",
    "version": "0.7",
    "maintainer": "Vasyl Pihur <vpihur@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10262,
    "package_name": "clap",
    "title": "Detecting Class Overlapping Regions in Multidimensional Data",
    "description": "The issue of overlapping regions in multidimensional data arises when different classes or clusters share similar feature representations, making it challenging to delineate distinct boundaries between them accurately. This package provides methods for detecting and visualizing these overlapping regions using partitional clustering techniques based on nearest neighbor distances.",
    "version": "0.1.0",
    "maintainer": "Priyanga Dilini Talagala <pritalagala@gmail.com>",
    "url": "https://github.com/pridiltal/clap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10264,
    "package_name": "clarifai",
    "title": "Access to Clarifai API",
    "description": "Get description of images from Clarifai API. For more information,\n    see <http://clarifai.com>. Clarifai uses a large deep learning cloud to come\n    up with descriptive labels of the things in an image. It also provides how\n    confident it is about each of the labels.",
    "version": "0.4.2",
    "maintainer": "Gaurav Sood <gsood07@gmail.com>",
    "url": "http://github.com/soodoku/clarifai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10267,
    "package_name": "class",
    "title": "Functions for Classification",
    "description": "Various functions for classification, including k-nearest\n  neighbour, Learning Vector Quantization and Self-Organizing Maps.",
    "version": "7.3-23",
    "maintainer": "Brian Ripley <Brian.Ripley@R-project.org>",
    "url": "http://www.stats.ox.ac.uk/pub/MASS4/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10278,
    "package_name": "clda",
    "title": "Convolution-Based Linear Discriminant Analysis",
    "description": "Contains a time series classification method that obtains a set of filters that maximize the between-class and minimize the within-class distances.",
    "version": "0.1",
    "maintainer": "Grover E. Castro Guzman <grover@usp.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10294,
    "package_name": "clevr",
    "title": "Clustering and Link Prediction Evaluation in R",
    "description": "Tools for evaluating link prediction and clustering algorithms \n    with respect to ground truth. Includes efficient implementations of \n    common performance measures such as pairwise precision/recall, \n    cluster homogeneity/completeness, variation of information, \n    Rand index etc.",
    "version": "0.1.2",
    "maintainer": "Neil Marchant <ngmarchant@gmail.com>",
    "url": "https://github.com/cleanzr/clevr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10301,
    "package_name": "clickb",
    "title": "Web Data Analysis by Bayesian Mixture of Markov Models",
    "description": "Designed for web usage data analysis, it implements tools to process web sequences and identify web browsing profiles through sequential classification. Sequences' clusters are identified by using a model-based approach, specifically mixture of discrete time first-order Markov models for categorical web sequences. A Bayesian approach is used to estimate model parameters and identify sequences classification as proposed by Fruehwirth-Schnatter and Pamminger (2010) <doi:10.1214/10-BA606>.",
    "version": "0.1",
    "maintainer": "Furio Urso <furio.urso@unipa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10333,
    "package_name": "cliot",
    "title": "Clinical Indices and Outcomes Tools",
    "description": "Collection of indices and tools relating to clinical research that aid epidemiological cohort or retrospective chart review with big data. All indices and tools take commonly used lab values, patient demographics, and clinical measurements to compute various risk and predictive values for survival or further classification/stratification. References to original literature and validation contained in each function documentation. Includes all commonly available calculators available online. ",
    "version": "1.0.0",
    "maintainer": "Neel Agarwal <neel.agarwal.216@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10344,
    "package_name": "clockplot",
    "title": "Plot Event Times on a 24-Hour Clock",
    "description": "Provides a novel visualization technique for plotting timestamped events\n    on a 24-hour circular clock face. This is particularly useful for analyzing\n    daily patterns, event clustering, and gaps in temporal data. The package\n    also generalizes this approach to create cyclic charts for other periods,\n    including weekly and monthly cycles, enabling effective event planning and\n    pattern analysis across multiple time frames.",
    "version": "0.8.3",
    "maintainer": "Abdullah Al Mahmud <almahmud.sbi@gmail.com>",
    "url": "https://github.com/mahmudstat/clockplot/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10372,
    "package_name": "clustEff",
    "title": "Clusters of Effects Curves in Quantile Regression Models",
    "description": "Clustering method to cluster both effects curves, through quantile regression coefficient modeling, and curves in functional data analysis. Sottile G. and Adelfio G. (2019) <doi:10.1007/s00180-018-0817-8>.",
    "version": "0.3.1",
    "maintainer": "Gianluca Sottile <gianluca.sottile@unipa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10373,
    "package_name": "clustMD",
    "title": "Model Based Clustering for Mixed Data",
    "description": "Model-based clustering of mixed data (i.e. data which consist of\n    continuous, binary, ordinal or nominal variables) using a parsimonious\n    mixture of latent Gaussian variable models.",
    "version": "1.2.1",
    "maintainer": "Damien McParland <damien.mcparland@ucd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10374,
    "package_name": "clustMixType",
    "title": "k-Prototypes Clustering for Mixed Variable-Type Data",
    "description": "Functions to perform k-prototypes partitioning clustering for\n    mixed variable-type data according to Z.Huang (1998): Extensions to the k-Means\n    Algorithm for Clustering Large Data Sets with Categorical Variables, Data Mining\n    and Knowledge Discovery 2, 283-304.",
    "version": "0.4-2",
    "maintainer": "Gero Szepannek <gero.szepannek@web.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10380,
    "package_name": "clusterCrit",
    "title": "Clustering Indices",
    "description": "Package providing functions for computing a collection of clustering validation or quality criteria and partition comparison indices.",
    "version": "1.3.0",
    "maintainer": "Iago Giné-Vázquez <iago.gin-vaz@protonmail.com>",
    "url": "https://gitlab.com/iagogv/clusterCrit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10382,
    "package_name": "clusterGGM",
    "title": "Sparse Gaussian Graphical Modeling with Variable Clustering",
    "description": "Perform sparse estimation of a Gaussian graphical model (GGM) with node aggregation through variable clustering. Currently, the package implements the clusterpath estimator of the Gaussian graphical model (CGGM) (Touw, Alfons, Groenen & Wilms, 2025; <doi:10.48550/arXiv.2407.00644>).",
    "version": "0.1.1",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "https://github.com/aalfons/clusterGGM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10384,
    "package_name": "clusterMI",
    "title": "Cluster Analysis with Missing Values by Multiple Imputation",
    "description": "Allows clustering of incomplete observations by addressing missing values using multiple imputation. For achieving this goal, the methodology consists in three steps, following Audigier and Niang 2022 <doi:10.1007/s11634-022-00519-1>. I) Missing data imputation using dedicated models. Four multiple imputation methods are proposed, two are based on joint modelling and two are fully sequential methods, as discussed in  Audigier et al. (2021) <doi:10.48550/arXiv.2106.04424>. II) cluster analysis of imputed data sets. Six clustering methods are available (distances-based or model-based), but custom methods can also be easily used. III) Partition pooling. The set of partitions is aggregated using Non-negative Matrix Factorization based method. An associated instability measure is computed by bootstrap (see Fang, Y. and Wang, J., 2012 <doi:10.1016/j.csda.2011.09.003>). Among applications, this instability measure can be used to choose a number of clusters with missing values.\n    The package also proposes several diagnostic tools to tune the number of imputed data sets, to tune the number of iterations in fully sequential imputation, to check the fit of imputation models, etc.",
    "version": "1.5",
    "maintainer": "Vincent Audigier <vincent.audigier@cnam.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10389,
    "package_name": "clusterSim",
    "title": "Searching for Optimal Clustering Procedure for a Data Set",
    "description": "Distance measures (GDM1, GDM2,\tSokal-Michener, Bray-Curtis, for symbolic interval-valued data), cluster quality indices (Calinski-Harabasz, Baker-Hubert, Hubert-Levine, Silhouette, Krzanowski-Lai, Hartigan, Gap,\tDavies-Bouldin),\tdata normalization formulas (metric data, interval-valued symbolic data), data generation (typical and non-typical data), HINoV method,\treplication analysis, linear ordering methods, spectral clustering, agreement indices between two partitions, plot functions (for categorical and symbolic interval-valued data). \n (MILLIGAN, G.W., COOPER, M.C. (1985) <doi:10.1007/BF02294245>, \n HUBERT, L., ARABIE, P. (1985) <doi:10.1007%2FBF01908075>, \n RAND, W.M. (1971) <doi:10.1080/01621459.1971.10482356>, \n JAJUGA, K., WALESIAK, M. (2000) <doi:10.1007/978-3-642-57280-7_11>, \n MILLIGAN, G.W., COOPER, M.C. (1988) <doi:10.1007/BF01897163>, \n JAJUGA, K., WALESIAK, M., BAK, A. (2003) <doi:10.1007/978-3-642-55721-7_12>, \n DAVIES, D.L., BOULDIN, D.W. (1979) <doi:10.1109/TPAMI.1979.4766909>, \n CALINSKI, T., HARABASZ, J. (1974) <doi:10.1080/03610927408827101>,\n HUBERT, L. (1974) <doi:10.1080/01621459.1974.10480191>, \n TIBSHIRANI, R., WALTHER, G., HASTIE, T. (2001) <doi:10.1111/1467-9868.00293>, \n BRECKENRIDGE, J.N. (2000) <doi:10.1207/S15327906MBR3502_5>, \n WALESIAK, M., DUDEK, A. (2008) <doi:10.1007/978-3-540-78246-9_11>).",
    "version": "0.51-5",
    "maintainer": "Andrzej Dudek <andrzej.dudek@ue.wroc.pl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10392,
    "package_name": "clusterability",
    "title": "Performs Tests for Cluster Tendency of a Data Set",
    "description": "Test for cluster tendency (clusterability) of a data set.\n    The methods implemented - reducing the data set to a single dimension using principal component analysis or computing\n    pairwise distances, and performing a multimodality test like the Dip Test or Silverman's Critical Bandwidth Test - \n    are described in Adolfsson, Ackerman, and Brownstein (2019) <doi:10.1016/j.patcog.2018.10.026>. Such methods can inform whether clustering algorithms\n    are appropriate for a data set.",
    "version": "0.2.1.0",
    "maintainer": "Zachariah Neville <zachariahneville@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10393,
    "package_name": "clusterhap",
    "title": "Clustering Genotypes in Haplotypes",
    "description": "One haplotype is a combination of SNP\n  (Single Nucleotide Polymorphisms) within the QTL (Quantitative Trait Loci).\n  clusterhap groups together all individuals of a population with the same haplotype.\n  Each group contains individual with the same allele in each SNP,\n  whether or not missing data. Thus, clusterhap groups individuals,\n  that to be imputed, have a non-zero probability of having the same alleles\n  in the entire sequence of SNP's. Moreover, clusterhap calculates such\n  probability from relative frequencies.",
    "version": "0.1",
    "maintainer": "Gaston Quero <gastonquero@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10394,
    "package_name": "clustering.sc.dp",
    "title": "Optimal Distance-Based Clustering for Multidimensional Data with\nSequential Constraint",
    "description": "A dynamic programming algorithm for optimal clustering multidimensional data with sequential constraint. The algorithm minimizes the sum of squares of within-cluster distances. The sequential constraint allows only subsequent items of the input data to form a cluster. The sequential constraint is typically required in clustering data streams or items with time stamps such as video frames, GPS signals of a vehicle, movement data of a person, e-pen data, etc. The algorithm represents an extension of 'Ckmeans.1d.dp' to multiple dimensional spaces. Similarly to the one-dimensional case, the algorithm guarantees optimality and repeatability of clustering. Method clustering.sc.dp() can find the optimal clustering if the number of clusters is known. Otherwise, methods findwithinss.sc.dp() and backtracking.sc.dp() can be used. See Szkaliczki, T. (2016) \"clustering.sc.dp: Optimal Clustering with Sequential Constraint by Using Dynamic Programming\" <doi: 10.32614/RJ-2016-022> for more information.",
    "version": "1.1",
    "maintainer": "Tibor Szkaliczki <szkaliczki.tibor@sztaki.hu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10397,
    "package_name": "clusternomics",
    "title": "Integrative Clustering for Heterogeneous Biomedical Datasets",
    "description": "Integrative context-dependent clustering for heterogeneous\n    biomedical datasets. Identifies local clustering structures in related\n    datasets, and a global clusters that exist across the datasets.",
    "version": "0.1.1",
    "maintainer": "Evelina Gabasova <egabasova@gmail.com>",
    "url": "https://github.com/evelinag/clusternomics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10401,
    "package_name": "clustlearn",
    "title": "Learn Clustering Techniques Through Examples and Code",
    "description": "Clustering methods, which (if asked) can provide step-by-step\n    explanations of the algorithms used, as described in Ezugwu et. al., (2022)\n    <doi:10.1016/j.engappai.2022.104743>; and datasets to test them on, which\n    highlight the strengths and weaknesses of each technique, as presented in\n    the clustering section of 'scikit-learn' (Pedregosa et al., 2011)\n    <https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>.",
    "version": "1.0.0",
    "maintainer": "Eduardo Ruiz Sabajanes <eduardo.ruizs@edu.uah.es>",
    "url": "https://github.com/Ediu3095/clustlearn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10402,
    "package_name": "clustord",
    "title": "Cluster Ordinal Data via Proportional Odds or Ordered Stereotype",
    "description": "Biclustering, row clustering and column clustering using the proportional odds model (POM), ordered stereotype model (OSM) or binary model for ordinal categorical data. Fernández, D., Arnold, R., Pledger, S., Liu, I., & Costilla, R. (2019) <doi:10.1007/s11634-018-0324-3>.",
    "version": "1.3.4",
    "maintainer": "Louise McMillan <louise.mcmillan@vuw.ac.nz>",
    "url": "https://vuw-clustering.github.io/clustord/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10403,
    "package_name": "clustra",
    "title": "Clustering Longitudinal Trajectories",
    "description": "Clusters longitudinal trajectories over time (can be unequally \n    spaced, unequal length time series and/or partially overlapping series) on\n    a common time axis. Performs k-means clustering on a single continuous \n    variable measured over time, where each mean is defined by a thin plate \n    spline fit to all points in a cluster. Distance is MSE across trajectory \n    points to cluster spline. Provides graphs of derived cluster splines, \n    silhouette plots, and Adjusted Rand Index evaluations of the number\n    of clusters. Scales well to large data with multicore parallelism available\n    to speed computation.",
    "version": "0.2.1",
    "maintainer": "George Ostrouchov <go@tennessee.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10404,
    "package_name": "clustrd",
    "title": "Methods for Joint Dimension Reduction and Clustering",
    "description": "A class of methods that combine dimension reduction and clustering of continuous, categorical or mixed-type data (Markos, Iodice D'Enza and van de Velden 2019; <DOI:10.18637/jss.v091.i10>). For continuous data, the package contains implementations of factorial K-means (Vichi and Kiers 2001; <DOI:10.1016/S0167-9473(00)00064-5>) and reduced K-means (De Soete and Carroll 1994; <DOI:10.1007/978-3-642-51175-2_24>); both methods that combine principal component analysis with K-means clustering. For categorical data, the package provides MCA K-means (Hwang, Dillon and Takane 2006; <DOI:10.1007/s11336-004-1173-x>), i-FCB (Iodice D'Enza and Palumbo 2013, <DOI:10.1007/s00180-012-0329-x>) and Cluster Correspondence Analysis (van de Velden, Iodice D'Enza and Palumbo 2017; <DOI:10.1007/s11336-016-9514-0>), which combine multiple correspondence analysis with K-means. For mixed-type data, it provides mixed Reduced K-means and mixed Factorial K-means (van de Velden, Iodice D'Enza and Markos 2019; <DOI:10.1002/wics.1456>), which combine PCA for mixed-type data with K-means.",
    "version": "1.4.0",
    "maintainer": "Angelos Markos <amarkos@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10405,
    "package_name": "clustree",
    "title": "Visualise Clusterings at Different Resolutions",
    "description": "Deciding what resolution to use can be a difficult question when\n    approaching a clustering analysis. One way to approach this problem is to\n    look at how samples move as the number of clusters increases. This package\n    allows you to produce clustering trees, a visualisation for interrogating\n    clusterings as resolution increases.",
    "version": "0.5.1",
    "maintainer": "Luke Zappia <luke@lazappi.id.au>",
    "url": "https://github.com/lazappi/clustree,\nhttps://lazappi.github.io/clustree/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10407,
    "package_name": "clustur",
    "title": "Clustering",
    "description": "A tool that implements the clustering algorithms from 'mothur' (Schloss PD et al. (2009) <doi:10.1128/AEM.01541-09>). 'clustur' make use of the cluster() and make.shared() command from 'mothur'. Our cluster() function has five different algorithms implemented: 'OptiClust', 'furthest', 'nearest', 'average', and 'weighted'. 'OptiClust' is an optimized clustering method for Operational Taxonomic Units, and you can learn more here, (Westcott SL, Schloss PD (2017) <doi:10.1128/mspheredirect.00073-17>). The make.shared() command is always applied at the end of the clustering command. This functionality allows us to generate and create clustering and abundance data efficiently.",
    "version": "0.1.3",
    "maintainer": "Patrick Schloss <pschloss@umich.edu>",
    "url": "http://www.schlosslab.org/clustur/,\nhttps://github.com/SchlossLab/clustur",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10408,
    "package_name": "clustvarsel",
    "title": "Variable Selection for Gaussian Model-Based Clustering",
    "description": "Variable selection for Gaussian model-based clustering as implemented in the 'mclust' package. The methodology allows to find the (locally) optimal subset of variables in a data set that have group/cluster information. A greedy or headlong search can be used, either in a forward-backward or backward-forward direction, with or without sub-sampling at the hierarchical clustering stage for starting 'mclust' models. By default the algorithm uses a sequential search, but parallelisation is also available.",
    "version": "2.3.5",
    "maintainer": "Luca Scrucca <luca.scrucca@unibo.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10414,
    "package_name": "cmahalanobis",
    "title": "Calculate Distance Measures for DataFrames",
    "description": "It provides functions that calculate Mahalanobis distance, Euclidean distance, Manhattan distance, Chebyshev distance, Hamming distance, Canberra distance, Minkowski dissimilarity (distance defined for p >= 1), Cosine dissimilarity, Bhattacharyya dissimilarity, Jaccard distance, Hellinger distance, Bray-Curtis dissimilarity, Sorensen-Dice dissimilarity between each pair of species in a list of data frames. These statistics are fundamental in various fields, such as cluster analysis, classification, and other applications of machine learning and data mining, where assessing similarity or dissimilarity between data is crucial. The package is designed to be flexible and easily integrated into data analysis workflows, providing reliable tools for evaluating distances in multidimensional contexts.",
    "version": "1.0.0",
    "maintainer": "Flavio Gioia <flaviogioia.fg@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10416,
    "package_name": "cmbClust",
    "title": "Conditional Mixture Modeling and Model-Based Clustering",
    "description": "Conditional mixture model fitted via EM (Expectation Maximization) algorithm for model-based clustering, including parsimonious procedure, optimal conditional order exploration, and visualization.",
    "version": "0.0.1",
    "maintainer": "Yang Wang <wangy4@cofc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10464,
    "package_name": "cobiclust",
    "title": "Biclustering via Latent Block Model Adapted to Overdispersed\nCount Data",
    "description": "Implementation of a probabilistic method for biclustering\n    adapted to overdispersed count data. It is a Gamma-Poisson Latent\n    Block Model.  It also implements two selection criteria in order to\n    select the number of biclusters.",
    "version": "0.1.2",
    "maintainer": "Julie Aubert <julie.aubert@inrae.fr>",
    "url": "https://github.com/julieaubert/cobiclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10499,
    "package_name": "codez",
    "title": "Seq2Seq Encoder-Decoder Model for Time-Feature Analysis Based on\nTensorflow",
    "description": "Proposes Seq2seq Time-Feature Analysis using an Encoder-Decoder to project into latent space and a Forward Network to predict the next sequence.",
    "version": "1.0.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/codez",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10560,
    "package_name": "colorhcplot",
    "title": "Colorful Hierarchical Clustering Dendrograms",
    "description": "Build dendrograms with sample groups highlighted by different colors. Visualize results of hierarchical clustering analyses as dendrograms whose leaves and labels are colored according to sample grouping. Assess whether data point grouping aligns to naturally occurring clusters. ",
    "version": "1.5.1",
    "maintainer": "Damiano Fantini <damiano.fantini@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10614,
    "package_name": "comparator",
    "title": "Comparison Functions for Clustering and Record Linkage",
    "description": "Implements functions for comparing strings, sequences and \n    numeric vectors for clustering and record linkage applications. \n    Supported comparison functions include: generalized edit distances \n    for comparing sequences/strings, Monge-Elkan similarity for fuzzy \n    comparison of token sets, and L-p distances for comparing numeric \n    vectors. Where possible, comparison functions are implemented in \n    C/C++ to ensure good performance.",
    "version": "0.1.4",
    "maintainer": "Neil Marchant <ngmarchant@gmail.com>",
    "url": "https://github.com/ngmarchant/comparator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10632,
    "package_name": "complexNet",
    "title": "Complex Network Generation",
    "description": "Providing a set of functions to easily generate and iterate complex networks.\n The functions can be used to generate realistic networks with a wide range of different clustering, density, and average path length.\n For more information consult research articles by Amiyaal Ilany and Erol Akcay (2016) <doi:10.1093/icb/icw068> and Ilany and Erol Akcay (2016) <doi:10.1101/026120>, which have inspired many methods in this package.",
    "version": "0.2.0",
    "maintainer": "Marco Smolla <drsmolla@icloud.com>",
    "url": "https://marcosmolla.github.io/complexNet/,\nhttps://github.com/marcosmolla/complexNet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10650,
    "package_name": "conTree",
    "title": "Contrast Trees and Boosting",
    "description": "Contrast trees represent a new approach for assessing the\n    accuracy of many types of machine learning estimates that are not\n    amenable to standard (cross) validation methods; see \"Contrast\n    trees and distribution boosting\", Jerome H. Friedman (2020)\n    <doi:10.1073/pnas.1921562117>. In situations where inaccuracies\n    are detected, boosted contrast trees can often improve\n    performance. Functions are provided to to build such trees in\n    addition to a special case, distribution boosting, an assumption\n    free method for estimating the full probability distribution of an\n    outcome variable given any set of joint input predictor variable\n    values.",
    "version": "0.3-1",
    "maintainer": "Balasubramanian Narasimhan <naras@stanford.edu>",
    "url": "https://jhfhub.github.io/conTree_tutorial/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10681,
    "package_name": "confidence",
    "title": "Confidence Estimation of Environmental State Classifications",
    "description": "Functions for estimating and reporting multi-year averages and\n    corresponding confidence intervals and distributions. A potential use case\n    is reporting the chemical and ecological status of surface waters according\n    to the European Water Framework Directive.",
    "version": "1.1-3",
    "maintainer": "Dennis Walvoort <dennis.Walvoort@wur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10692,
    "package_name": "conformalClassification",
    "title": "Transductive and Inductive Conformal Predictions for\nClassification Problems",
    "description": "Implementation of transductive conformal prediction (see Vovk, 2013, <doi:10.1007/978-3-642-41142-7_36>) and inductive conformal prediction (see Balasubramanian et al., 2014, ISBN:9780124017153) for classification problems.",
    "version": "1.0.0",
    "maintainer": "Niharika Gauraha <niharika.gauraha@farmbio.uu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10697,
    "package_name": "conformalpvalue",
    "title": "Computes Conformal p-Values",
    "description": "Computes marginal conformal p-values using conformal prediction in binary classification tasks. Conformal prediction is a framework that augments machine learning algorithms with a measure of uncertainty, in the form of prediction regions that attain a user-specified level of confidence. This package specifically focuses on providing conformal p-values that can be used to assess the confidence of the classification predictions. For more details, see Tyagi and Guo (2023) <https://proceedings.mlr.press/v204/tyagi23a.html>.",
    "version": "0.1.0",
    "maintainer": "Chhavi Tyagi <tyagi.chhavi2222@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10719,
    "package_name": "conrad",
    "title": "Client for the Microsoft's 'Cognitive Services Text to Speech\nREST' API",
    "description": "Convert text into synthesized speech and get a list of supported voices for a region. \n    Microsoft's 'Cognitive Services Text to Speech REST' API <https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-text-to-speech?tabs=streaming>  \n    supports neural text to speech voices, which support specific languages and dialects that are identified by locale. ",
    "version": "1.0.0.1",
    "maintainer": "Howard Baek <howardbaek.fh@gmail.com>",
    "url": "https://github.com/fhdsl/conrad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10801,
    "package_name": "cord",
    "title": "Community Estimation in G-Models via CORD",
    "description": "Partitions data points (variables) into communities/clusters, \n    similar to clustering algorithms such as k-means and hierarchical \n    clustering. This package implements a clustering algorithm based on a new \n    metric CORD, defined for high-dimensional parametric or semiparametric \n    distributions. For more details see Bunea et al. (2020), Annals of Statistics \n    <doi:10.1214/18-AOS1794>.",
    "version": "0.2.0",
    "maintainer": "Xi (Rossi) LUO <xi.rossi.luo@gmail.com>",
    "url": "https://doi.org/10.1214/18-AOS1794",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10825,
    "package_name": "corrRF",
    "title": "Clustered Random Forests for Optimal Prediction and Inference of\nClustered Data",
    "description": "A clustered random forest algorithm for fitting random forests for data of independent clusters, that exhibit within cluster dependence. \n    Details of the method can be found in Young and Buehlmann (2025) <doi:10.48550/arXiv.2503.12634>. ",
    "version": "1.1.0",
    "maintainer": "Elliot H. Young <ey244@cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10835,
    "package_name": "correspondenceTables",
    "title": "Creating Correspondence Tables Between Two Statistical\nClassifications",
    "description": "\n    A candidate correspondence table between two classifications can be created when there are correspondence tables leading from the first classification to the second one via intermediate 'pivot' classifications. \n    The correspondence table between two statistical classifications can be updated when one of the classifications gets updated to a new version.",
    "version": "0.7.4",
    "maintainer": "Mátyás Mészáros <matyas.meszaros@ec.europa.eu>",
    "url": "https://github.com/eurostat/correspondenceTables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10842,
    "package_name": "corrsieve",
    "title": "Software for Summarising and Evaluating STRUCTURE Output",
    "description": "Statistical summary of STRUCTURE output. STRUCTURE is a K-means clustering method for inferring population structure and assigning individuals to populations using genetic data. Pritchard JK, Stephens M, Donnelly PJ (2000) <DOI:10.1093/genetics/155.2.945>. <https://web.stanford.edu/group/pritchardlab/structure.html>.",
    "version": "1.6-9",
    "maintainer": "Michael G. Campana <campanam@si.edu>",
    "url": "https://github.com/campanam/rCorrSieve",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10860,
    "package_name": "costsensitive",
    "title": "Cost-Sensitive Multi-Class Classification",
    "description": "Reduction-based techniques for cost-sensitive multi-class classification, in which each observation has a different cost for classifying it into one class, and the goal is to predict the class with the minimum expected cost for each new observation.\n\tImplements Weighted All-Pairs (Beygelzimer, A., Langford, J., & Zadrozny, B., 2008, <doi:10.1007/978-0-387-79361-0_1>), Weighted One-Vs-Rest (Beygelzimer, A., Dani, V., Hayes, T., Langford, J., & Zadrozny, B., 2005, <https://dl.acm.org/citation.cfm?id=1102358>) and Regression One-Vs-Rest.\n\tWorks with arbitrary classifiers taking observation weights, or with regressors. Also implements cost-proportionate rejection sampling for working with classifiers\n\tthat don't accept observation weights.",
    "version": "0.1.2.10",
    "maintainer": "David Cortes <david.cortes.rivera@gmail.com>",
    "url": "https://github.com/david-cortes/costsensitive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10865,
    "package_name": "countSTAR",
    "title": "Flexible Modeling of Count Data",
    "description": "For Bayesian and classical inference and prediction with count-valued data,\n    Simultaneous Transformation and Rounding (STAR) Models provide a flexible, interpretable,\n    and easy-to-use approach. STAR models the observed count data using a rounded \n    continuous data model and incorporates a transformation for greater flexibility.\n    Implicitly, STAR formalizes the commonly-applied yet incoherent procedure of \n    (i) transforming count-valued data and subsequently \n    (ii) modeling the transformed data using Gaussian models. \n    STAR is well-defined for count-valued data, which is reflected in predictive accuracy, \n    and is designed to account for zero-inflation, bounded or censored data, and over- or underdispersion. \n    Importantly, STAR is easy to combine with existing MCMC or point estimation\n    methods for continuous data, which allows seamless adaptation of continuous data\n    models (such as linear regressions, additive models, BART, random forests,\n    and gradient boosting machines) for count-valued data. The package also includes several\n    methods for modeling count time series data, namely via warped Dynamic Linear Models. \n    For more details and background on these methodologies, see the works of \n    Kowal and Canale (2020) <doi:10.1214/20-EJS1707>, \n    Kowal and Wu (2022) <doi:10.1111/biom.13617>, \n    King and Kowal (2022) <arXiv:2110.14790>, and \n    Kowal and Wu (2023) <arXiv:2110.12316>.",
    "version": "1.0.2",
    "maintainer": "Brian King <brianking387@gmail.com>",
    "url": "https://bking124.github.io/countSTAR/\nhttps://github.com/bking124/countSTAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10964,
    "package_name": "cpt",
    "title": "Classification Permutation Test",
    "description": "Non-parametric test for equality of multivariate distributions.  Trains a classifier to classify (multivariate) observations as coming from one of several distributions.  If the classifier is able to classify the observations better than would be expected by chance (using permutation inference), then the null hypothesis that the distributions are equal is rejected.  ",
    "version": "1.0.2",
    "maintainer": "Johann Gagnon-Bartsch <johanngb@umich.edu>",
    "url": "http://dept.stat.lsa.umich.edu/~johanngb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10971,
    "package_name": "cramR",
    "title": "Cram Method for Efficient Simultaneous Learning and Evaluation",
    "description": "Performs the Cram method, a general and efficient approach to simultaneous learning and evaluation using a generic machine learning algorithm. In a single pass of batched data, the proposed method repeatedly trains a machine learning algorithm and tests its empirical performance. Because it utilizes the entire sample for both learning and evaluation, cramming is significantly more data-efficient than sample-splitting. Unlike cross-validation, Cram evaluates the final learned model directly, providing sharper inference aligned with real-world deployment. The method naturally applies to both policy learning and contextual bandits, where decisions are based on individual features to maximize outcomes. The package includes cram_policy() for learning and evaluating individualized binary treatment rules, cram_ml() to train and assess the population-level performance of machine learning models, and cram_bandit() for on-policy evaluation of contextual bandit algorithms. For all three functions, the package provides estimates of the average outcome that would result if the model were deployed, along with standard errors and confidence intervals for these estimates. Details of the method are described in Jia, Imai, and Li (2024) <https://www.hbs.edu/ris/Publication%20Files/2403.07031v1_a83462e0-145b-4675-99d5-9754aa65d786.pdf> and Jia et al. (2025) <doi:10.48550/arXiv.2403.07031>.",
    "version": "0.1.1",
    "maintainer": "Yanis Vandecasteele <yanisvdc.ensae@gmail.com>",
    "url": "https://github.com/yanisvdc/cramR,\nhttps://yanisvdc.github.io/cramR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10989,
    "package_name": "creditmodel",
    "title": "Toolkit for Credit Modeling, Analysis and Visualization",
    "description": "\n  Provides a highly efficient R tool suite for Credit Modeling, Analysis and Visualization.Contains infrastructure functionalities such as data exploration and preparation, missing values treatment, outliers treatment, variable derivation, variable selection, dimensionality reduction, grid search for hyper parameters, data mining and visualization, model evaluation, strategy analysis etc. This package is designed to make the development of binary classification models (machine learning based models as well as credit scorecard) simpler and faster. The references including: 1 Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: Development and Implementation Using SAS; 2 Bezdek, James C.FCM: The fuzzy c-means clustering algorithm. Computers & Geosciences (0098-3004),<DOI:10.1016/0098-3004(84)90020-7>.",
    "version": "1.3.1",
    "maintainer": "Dongping Fan <fdp@pku.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11026,
    "package_name": "crookR",
    "title": "Synthetic Crook Deformations in Stem Point Clouds",
    "description": "Simulates parameterized single- and double-directional stem deformations in tree point clouds derived from terrestrial or mobile laser scanning, enabling the generation of realistic synthetic datasets for training and validating machine learning models in wood defect detection, quality assessment, and precision forestry. For more details see Pires (2025) <doi:10.54612/a.7hln0kr0ta>.",
    "version": "0.1.0",
    "maintainer": "Raul de Paula Pires <rauldepaulapires@gmail.com>",
    "url": "https://github.com/raudep/crookR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11055,
    "package_name": "crseEventStudy",
    "title": "A Robust and Powerful Test of Abnormal Stock Returns in\nLong-Horizon Event Studies",
    "description": "Based on Dutta et al. (2018) <doi:10.1016/j.jempfin.2018.02.004>, this package provides their standardized test for abnormal returns in long-horizon event studies. The methods used improve the major weaknesses of size, power, and robustness of long-run statistical tests described in Kothari/Warner (2007) <doi:10.1016/B978-0-444-53265-7.50015-9>. Abnormal returns are weighted by their statistical precision (i.e., standard deviation), resulting in abnormal standardized returns. This procedure efficiently captures the heteroskedasticity problem. Clustering techniques following Cameron et al. (2011) <doi:10.1198/jbes.2010.07136> are adopted for computing cross-sectional correlation robust standard errors. The statistical tests in this package therefore accounts for potential biases arising from returns' cross-sectional correlation, autocorrelation, and volatility clustering without power loss.",
    "version": "1.2.2",
    "maintainer": "Siegfried Köstlmeier <siegfried.koestlmeier@gmail.com>",
    "url": "https://github.com/skoestlmeier/crseEventStudy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11062,
    "package_name": "crumble",
    "title": "Flexible and General Mediation Analysis Using Riesz Representers",
    "description": "Implements a modern, unified estimation strategy for common \n\tmediation estimands (natural effects, organic effects, interventional effects, \n\tand recanting twins) in combination with modified treatment policies as \n\tdescribed in Liu, Williams, Rudolph, and Díaz (2024) \n\t<doi:10.48550/arXiv.2408.14620>. Estimation makes use of recent advancements \n\tin Riesz-learning to estimate a set of required nuisance parameters with \n\tdeep learning. The result is the capability to estimate mediation effects with \n\tbinary, categorical, continuous, or multivariate exposures with \n\thigh-dimensional mediators and mediator-outcome confounders using machine \n\tlearning.",
    "version": "0.1.2",
    "maintainer": "Nicholas Williams <ntwilliams.personal@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11087,
    "package_name": "csmpv",
    "title": "Biomarker Confirmation, Selection, Modelling, Prediction, and\nValidation",
    "description": "\n   There are diverse purposes such as biomarker confirmation, novel biomarker discovery, constructing predictive models, model-based prediction, and validation. \n   It handles binary, continuous, and time-to-event outcomes at the sample or patient level.\n   - Biomarker confirmation utilizes established functions like glm() from 'stats', coxph() from 'survival', surv_fit(), and ggsurvplot() from 'survminer'.\n   - Biomarker discovery and variable selection are facilitated by three LASSO-related functions LASSO2(), LASSO_plus(), and LASSO2plus(), leveraging the 'glmnet' R package with additional steps.\n   - Eight versatile modeling functions are offered, each designed for predictive models across various outcomes and data types.\n     1) LASSO2(), LASSO_plus(), LASSO2plus(), and LASSO2_reg() perform variable selection using LASSO methods and construct predictive models based on selected variables.\n     2) XGBtraining() employs 'XGBoost' for model building and is the only function not involving variable selection.\n     3) Functions like LASSO2_XGBtraining(), LASSOplus_XGBtraining(), and LASSO2plus_XGBtraining() combine LASSO-related variable selection with 'XGBoost' for model construction.\n   - All models support prediction and validation, requiring a testing dataset comparable to the training dataset.\n   Additionally, the package introduces XGpred() for risk prediction based on survival data, with the XGpred_predict() function available for predicting risk groups in new datasets.\n   The methodology is based on our new algorithms and various references:\n   - Hastie et al. (1992, ISBN 0 534 16765-9), \n   - Therneau et al. (2000, ISBN 0-387-98784-3), \n   - Kassambara et al. (2021) <https://CRAN.R-project.org/package=survminer>,\n   - Friedman et al. (2010) <doi:10.18637/jss.v033.i01>,\n   - Simon et al. (2011) <doi:10.18637/jss.v039.i05>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=rms>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=Hmisc>,\n   - Chen and Guestrin (2016) <doi:10.48550/arXiv.1603.02754>,\n   - Aoki et al. (2023) <doi:10.1200/JCO.23.01115>.",
    "version": "1.0.5",
    "maintainer": "Aixiang Jiang <aijiang@bccrc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11096,
    "package_name": "cstab",
    "title": "Selection of Number of Clusters via Normalized Clustering\nInstability",
    "description": "Selection of the number of clusters in cluster analysis using\n    stability methods.",
    "version": "0.2-2",
    "maintainer": "Jonas M. B. Haslbeck <jonas.haslbeck@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11119,
    "package_name": "ctmva",
    "title": "Continuous-Time Multivariate Analysis",
    "description": "Implements a basis function or functional data analysis framework\n             for several techniques of multivariate analysis in continuous-time \n             setting. Specifically, we introduced continuous-time analogues of\n             several classical techniques of multivariate analysis, such as \n             principal component analysis, canonical correlation analysis, \n             Fisher linear discriminant analysis, K-means clustering, and so \n             on. Details are in Biplab Paul, Philip T. Reiss, Erjia Cui and Noemi Foa (2025) \n             \"Continuous-time multivariate analysis\" <doi: 10.1080/10618600.2024.2374570>.",
    "version": "1.5.0",
    "maintainer": "Biplab Paul <paul.biplab497@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11127,
    "package_name": "ctsfeatures",
    "title": "Analyzing Categorical Time Series",
    "description": "An implementation of several functions for feature extraction in \n    categorical time series datasets. Specifically, some features related to \n    marginal distributions and serial dependence patterns can be computed. These \n    features can be used to feed clustering and classification algorithms for\n    categorical time series, among others. The package also includes some\n    interesting datasets containing biological sequences. Practitioners from a \n    broad variety of fields could benefit from the general framework provided \n    by 'ctsfeatures'.",
    "version": "1.2.2",
    "maintainer": "Angel Lopez-Oriona <oriona38@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11175,
    "package_name": "cutpointr",
    "title": "Determine and Evaluate Optimal Cutpoints in Binary\nClassification Tasks",
    "description": "Estimate cutpoints that optimize a specified metric in binary classification tasks\n    and validate performance using bootstrapping. Some methods for more robust cutpoint\n    estimation are supported, e.g. a parametric method assuming normal distributions,\n    bootstrapped cutpoints, and smoothing of the metric values per cutpoint using\n    Generalized Additive Models. Various plotting functions are included. For an overview\n    of the package see Thiele and Hirschfeld (2021) <doi:10.18637/jss.v098.i11>.",
    "version": "1.2.1",
    "maintainer": "Christian Thiele <c.thiele@gmx-topmail.de>",
    "url": "https://github.com/thie1e/cutpointr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11243,
    "package_name": "dTBM",
    "title": "Multi-Way Spherical Clustering via Degree-Corrected Tensor Block\nModels",
    "description": "Implement weighted higher-order initialization and angle-based iteration for multi-way spherical clustering under degree-corrected tensor block model. See reference Jiaxin Hu and Miaoyan Wang (2023) <doi:10.1109/TIT.2023.3239521>.",
    "version": "3.0",
    "maintainer": "Jiaxin Hu <jhu267@wisc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11270,
    "package_name": "daltoolbox",
    "title": "Leveraging Experiment Lines to Data Analytics",
    "description": "The natural increase in the complexity of current research experiments and data demands better tools to enhance productivity in Data Analytics. The package is a framework designed to address the modern challenges in data analytics workflows. The package is inspired by Experiment Line concepts. It aims to provide seamless support for users in developing their data mining workflows by offering a uniform data model and method API. It enables the integration of various data mining activities, including data preprocessing, classification, regression, clustering, and time series prediction. It also offers options for hyper-parameter tuning and supports integration with existing libraries and languages. Overall, the package provides researchers with a comprehensive set of functionalities for data science, promoting ease of use, extensibility, and integration with various tools and libraries. Information on Experiment Line is based on Ogasawara et al. (2009) <doi:10.1007/978-3-642-02279-1_20>.",
    "version": "1.2.747",
    "maintainer": "Eduardo Ogasawara <eogasawara@ieee.org>",
    "url": "https://cefet-rj-dal.github.io/daltoolbox/,\nhttps://github.com/cefet-rj-dal/daltoolbox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11271,
    "package_name": "daltoolboxdp",
    "title": "Python-Based Extensions for Data Analytics Workflows",
    "description": "\n  Provides Python-based extensions to enhance data analytics workflows, \n  particularly for tasks involving data preprocessing and predictive modeling. \n  Includes tools for data sampling, transformation, feature selection, \n  balancing strategies (e.g., SMOTE), and model construction. \n  These capabilities leverage Python libraries via the reticulate interface, \n  enabling seamless integration with a broader machine learning ecosystem. \n  Supports instance selection and hybrid workflows that combine R and Python \n  functionalities for flexible and reproducible analytical pipelines. \n  The architecture is inspired by the Experiment Lines approach, which promotes \n  modularity, extensibility, and interoperability across tools. \n  More information on Experiment Lines is available in \n  Ogasawara et al. (2009) <doi:10.1007/978-3-642-02279-1_20>.",
    "version": "1.2.737",
    "maintainer": "Eduardo Ogasawara <eogasawara@ieee.org>",
    "url": "https://cefet-rj-dal.github.io/daltoolboxdp/,\nhttps://github.com/cefet-rj-dal/daltoolboxdp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11278,
    "package_name": "dann",
    "title": "Discriminant Adaptive Nearest Neighbor Classification",
    "description": "Discriminant Adaptive Nearest Neighbor Classification is a \n    variation of k nearest neighbors where the shape of the neighborhood is \n    data driven. This package implements dann and sub_dann from\n    Hastie (1996) <https://web.stanford.edu/~hastie/Papers/dann_IEEE.pdf>.",
    "version": "1.0.2",
    "maintainer": "Greg McMahan <gmcmacran@gmail.com>",
    "url": "https://github.com/gmcmacran/dann",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11296,
    "package_name": "data.tree",
    "title": "General Purpose Hierarchical Data Structure",
    "description": "Create tree structures from hierarchical data, and traverse the\n    tree in various orders. Aggregate, cumulate, print, plot, convert to and from\n    data.frame and more. Useful for decision trees, machine learning, finance,\n    conversion from and to JSON, and many other applications.",
    "version": "1.2.0",
    "maintainer": "Christoph Glur <christoph.glur@powerpartners.pro>",
    "url": "https://github.com/gluc/data.tree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11320,
    "package_name": "datamicroarray",
    "title": "Collection of Data Sets for Classification",
    "description": "A collection of scripts to download, process, and load",
    "version": "0.2.3",
    "maintainer": "John A. Ramey <johnramey@gmail.com>",
    "url": "https://github.com/ramhiser/datamicroarray",
    "exports": [],
    "topics": ["cancer", "colon-cancer", "high-dimensional-data", "machine-learning", "r"],
    "score": "NA",
    "stars": 107
  },
  {
    "id": 11342,
    "package_name": "datasetsICR",
    "title": "Datasets from the Book \"An Introduction to Clustering with R\"",
    "description": "Companion to the book \"An Introduction to Clustering with R\" by P. Giordani, M.B. Ferraro and F. Martella (Springer, Singapore, 2020). The datasets are used in some case studies throughout the text.",
    "version": "1.0",
    "maintainer": "Paolo Giordani <paolo.giordani@uniroma1.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11405,
    "package_name": "dclust",
    "title": "Divisive Hierarchical Clustering",
    "description": "Contains a single function dclust() for divisive hierarchical clustering based on \n    recursive k-means partitioning (k = 2). Useful for clustering large datasets\n    where computation of a n x n distance matrix is not feasible (e.g. n > 10,000 records).\n    For further information see Steinbach, Karypis and Kumar (2000) <http://glaros.dtc.umn.edu/gkhome/fetch/papers/docclusterKDDTMW00.pdf>.",
    "version": "0.1.0",
    "maintainer": "Shaun Wilkinson <shaunpwilkinson@gmail.com>",
    "url": "http://github.com/shaunpwilkinson/dclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11406,
    "package_name": "dcm2",
    "title": "Calculating the M2 Model Fit Statistic for Diagnostic\nClassification Models",
    "description": "A collection of functions for calculating the M2 model fit\n    statistic for diagnostic classification models as described by Liu et al.\n    (2016) <DOI:10.3102/1076998615621293>. These functions provide multiple\n    sources of information for model fit according to the M2 statistic,\n    including the M2 statistic, the *p* value for that M2 statistic, and the\n    Root Mean Square Error of Approximation based on the M2 statistic.",
    "version": "1.0.2",
    "maintainer": "Jeffrey Hoover <jeffrey.c.hoover@gmail.com>",
    "url": "https://github.com/atlas-aai/dcm2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11407,
    "package_name": "dcmdata",
    "title": "Data Sets for Diagnostic Classification Modeling",
    "description": "Access data sets for demonstrating or testing diagnostic\n    classification models. Simulated data sets can be used to compare estimated\n    model output to true data-generating values. Real data sets can be used to\n    demonstrate real-world applications of diagnostic models.",
    "version": "0.1.0",
    "maintainer": "W. Jake Thompson <wjakethompson@gmail.com>",
    "url": "https://dcmdata.r-dcm.org, https://github.com/r-dcm/dcmdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11410,
    "package_name": "dcmstan",
    "title": "Generate 'Stan' Code for Diagnostic Classification Models",
    "description": "Diagnostic classification models are psychometric models used\n    to categorically estimate respondents mastery, or proficiency, on a\n    set of predefined skills (Bradshaw, 2016,\n    <doi:10.1002/9781118956588.ch13>).  Diagnostic models can be estimated\n    with 'Stan'; however, the necessary scripts can be long and\n    complicated. This package automates the creation of 'Stan' scripts for\n    diagnostic classification models. Specify different types of\n    diagnostic models, define prior distributions, and automatically\n    generate the necessary 'Stan' code for estimating the model.",
    "version": "0.1.0",
    "maintainer": "W. Jake Thompson <wjakethompson@gmail.com>",
    "url": "https://dcmstan.r-dcm.org, https://github.com/r-dcm/dcmstan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11414,
    "package_name": "dcsvm",
    "title": "Density Convoluted Support Vector Machines",
    "description": "Implements an efficient algorithm for solving sparse-penalized support vector machines with kernel density convolution. This package is designed for high-dimensional classification tasks, supporting lasso (L1) and elastic-net penalties for sparse feature selection and providing options for tuning kernel bandwidth and penalty weights. The 'dcsvm' is applicable to fields such as bioinformatics, image analysis, and text classification, where high-dimensional data commonly arise. Learn more about the methodology and algorithm at Wang, Zhou, Gu, and Zou (2023) <doi:10.1109/TIT.2022.3222767>.",
    "version": "0.0.1",
    "maintainer": "Boxiang Wang <boxiang-wang@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11421,
    "package_name": "ddalpha",
    "title": "Depth-Based Classification and Calculation of Data Depth",
    "description": "Contains procedures for depth-based supervised learning, which are entirely non-parametric, in particular the DDalpha-procedure (Lange, Mosler and Mozharovskyi, 2014 <doi:10.1007/s00362-012-0488-4>). The training data sample is transformed by a statistical depth function to a compact low-dimensional space, where the final classification is done. It also offers an extension to functional data and routines for calculating certain notions of statistical depth functions. 50 multivariate and 5 functional classification problems are included. (Pokotylo, Mozharovskyi and Dyckerhoff, 2019 <doi:10.18637/jss.v091.i05>).",
    "version": "1.3.16",
    "maintainer": "Oleksii Pokotylo <alexey.pokotylo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11422,
    "package_name": "ddc",
    "title": "Distance Density Clustering Algorithm",
    "description": "A distance density clustering (DDC) algorithm in R. DDC uses dynamic time warping (DTW) to compute a similarity matrix, based on which cluster centers and cluster assignments are found. DDC inherits dynamic time warping (DTW) arguments and constraints. The cluster centers are centroid points that are calculated using the DTW Barycenter Averaging (DBA) algorithm. The clustering process is divisive. At each iteration, cluster centers are updated and data is reassigned to cluster centers. Early stopping is possible. The output includes cluster centers and clustering assignment, as described in the paper (Ma et al (2017) <doi:10.1109/ICDMW.2017.11>).",
    "version": "1.0.1",
    "maintainer": "Ruizhe Ma <maruizhe.cs@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11428,
    "package_name": "ddml",
    "title": "Double/Debiased Machine Learning",
    "description": "Estimate common causal parameters using double/debiased machine \n    learning as proposed by Chernozhukov et al. (2018) <doi:10.1111/ectj.12097>. \n    'ddml' simplifies estimation based on (short-)stacking as discussed in \n    Ahrens et al. (2024) <doi:10.1002/jae.3103>, which leverages multiple base \n    learners to increase robustness to the underlying data generating process.",
    "version": "0.3.1",
    "maintainer": "Thomas Wiemann <thomas.wiemann@chicagobooth.edu>",
    "url": "https://github.com/thomaswiemann/ddml,\nhttps://thomaswiemann.com/ddml/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11483,
    "package_name": "deepMOU",
    "title": "Clustering of Short Texts by Mixture of Unigrams and Its Deep\nExtensions",
    "description": "Functions providing an easy and intuitive way for fitting and clusters data using the Mixture of Unigrams models by means the Expectation-Maximization algorithm (Nigam, K. et al. (2000). <doi:10.1023/A:1007692713085>), Mixture of Dirichlet-Multinomials estimated by Gradient Descent (Anderlucci, Viroli (2020) <doi:10.1007/s11634-020-00399-3>) and Deep Mixture of Multinomials whose estimates are obtained with Gibbs sampling scheme (Viroli, Anderlucci (2020) <doi:10.1007/s11222-020-09989-9>). There are also functions for graphical representation of clusters obtained.",
    "version": "0.1.1",
    "maintainer": "Martin D'Ippolito <martinmy69@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11484,
    "package_name": "deepNN",
    "title": "Deep Learning",
    "description": "Implementation of some Deep Learning methods. Includes multilayer perceptron, different activation functions, regularisation strategies, stochastic gradient descent and dropout. Thanks go to the following references for helping to inspire and develop the package: Ian Goodfellow, Yoshua Bengio, Aaron Courville, Francis Bach (2016, ISBN:978-0262035613) Deep Learning. Terrence J. Sejnowski (2018, ISBN:978-0262038034) The Deep Learning Revolution. Grant Sanderson (3brown1blue) <https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi> Neural Networks YouTube playlist. Michael A. Nielsen <http://neuralnetworksanddeeplearning.com/> Neural Networks and Deep Learning.",
    "version": "1.2",
    "maintainer": "Benjamin Taylor <benjamin.taylor.software@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11491,
    "package_name": "deepnet",
    "title": "Deep Learning Toolkit in R",
    "description": "Implement some deep learning architectures and neural network\n    algorithms, including BP,RBM,DBN,Deep autoencoder and so on.",
    "version": "0.2.1",
    "maintainer": "Xiao Rong <runxiao@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11492,
    "package_name": "deepregression",
    "title": "Fitting Deep Distributional Regression",
    "description": "\n    Allows for the specification of semi-structured deep distributional regression models which are fitted in a neural network as \n    proposed by Ruegamer et al. (2023) <doi:10.18637/jss.v105.i02>.\n    Predictors can be modeled using structured (penalized) linear effects, structured non-linear effects or using an unstructured deep network model.",
    "version": "2.3.2",
    "maintainer": "David Ruegamer <david.ruegamer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11496,
    "package_name": "deeptrafo",
    "title": "Fitting Deep Conditional Transformation Models",
    "description": "Allows for the specification of deep conditional transformation \n    models (DCTMs) and ordinal neural network transformation models, as \n    described in Baumann et al (2021) <doi:10.1007/978-3-030-86523-8_1> and \n    Kook et al (2022) <doi:10.1016/j.patcog.2021.108263>. Extensions such as\n    autoregressive DCTMs (Ruegamer et al, 2023, <doi:10.1007/s11222-023-10212-8>)\n    and transformation ensembles (Kook et al, 2022, <doi:10.48550/arXiv.2205.12729>)\n    are implemented. The software package is described in Kook et al (2024,\n    <doi:10.18637/jss.v111.i10>).",
    "version": "1.0-0",
    "maintainer": "Lucas Kook <lucasheinrich.kook@gmail.com>",
    "url": "https://github.com/neural-structured-additive-learning/deeptrafo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11549,
    "package_name": "densityClust",
    "title": "Clustering by Fast Search and Find of Density Peaks",
    "description": "An improved implementation (based on k-nearest neighbors) of\n    the density peak clustering algorithm, originally described by Alex\n    Rodriguez and Alessandro Laio (Science, 2014 vol. 344). It can handle large \n    datasets (> 100,000 samples) very efficiently. It was initially implemented \n    by Thomas Lin Pedersen, with inputs from Sean Hughes and later improved by \n    Xiaojie Qiu to handle large datasets with kNNs.",
    "version": "0.3.3",
    "maintainer": "Thomas Lin Pedersen <thomasp85@gmail.com>",
    "url": "https://github.com/thomasp85/densityClust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11551,
    "package_name": "densityratio",
    "title": "Distribution Comparison Through Density Ratio Estimation",
    "description": "Fast, flexible and user-friendly tools for distribution comparison\n  through direct density ratio estimation. The estimated density ratio can be \n  used for covariate shift adjustment, outlier-detection, change-point detection,\n  classification and evaluation of synthetic data quality. The package implements\n  multiple non-parametric estimation techniques (unconstrained least-squares\n  importance fitting, ulsif(), Kullback-Leibler importance estimation procedure,\n  kliep(), spectral density ratio estimation, spectral(), kernel mean matching,\n  kmm(), and least-squares hetero-distributional subspace search, lhss()).\n  with automatic tuning of hyperparameters. Helper functions are available for\n  two-sample testing and visualizing the density ratios. For an overview on \n  density ratio estimation, see Sugiyama et al. (2012) <doi:10.1017/CBO9781139035613>\n  for a general overview, and the help files for references on the specific \n  estimation techniques.",
    "version": "0.2.2",
    "maintainer": "Thom Volker <thombenjaminvolker@gmail.com>",
    "url": "https://thomvolker.github.io/densityratio/,\nhttps://github.com/thomvolker/densityratio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11665,
    "package_name": "diathor",
    "title": "Calculate Ecological Information and Diatom Based Indices",
    "description": "\n    Calculate multiple biotic indices using diatoms from environmental samples. Diatom species are recognized by their species' name using a heuristic search, and their ecological data is retrieved from multiple sources.\n    It includes number/shape of chloroplasts diversity indices, size classes, ecological guilds, and multiple biotic indices.\n    It outputs both a dataframe with all the results and plots of all the obtained data in a defined output folder.\n    - Sample data was taken from Nicolosi Gelis, Cochero & Gómez (2020, <doi:10.1016/j.ecolind.2019.105951>).\n    - The package uses the 'Diat.Barcode' database to calculate morphological and ecological information by Rimet & Couchez (2012, <doi:10.1051/kmae/2012018>),and the combined classification of guilds and size classes established by B-Béres et al. (2017, <doi:10.1016/j.ecolind.2017.07.007>).\n    - Current diatom-based biotic indices include the DES index by Descy (1979)\n    - EPID index by Dell'Uomo (1996, ISBN: 3950009002)\n    - IDAP index by Prygiel & Coste (1993, <doi:10.1007/BF00028033>)\n    - ID-CH index by Hürlimann & Niederhauser (2007)\n    - IDP index by Gómez & Licursi (2001, <doi:10.1023/A:1011415209445>)\n    - ILM index by Leclercq & Maquet (1987)\n    - IPS index by Coste (1982)\n    - LOBO index by Lobo, Callegaro, & Bender (2002, ISBN:9788585869908)\n    - SLA by Sládeček (1986, <doi:10.1002/aheh.19860140519>)\n    - TDI index by Kelly, & Whitton (1995, <doi:10.1007/BF00003802>)\n    - SPEAR(herbicide) index by Wood, Mitrovic, Lim, Warne, Dunlop, & Kefford (2019, <doi:10.1016/j.ecolind.2018.12.035>)\n    - PBIDW index by Castro-Roa & Pinilla-Agudelo (2014)\n    - DISP index by Stenger-Kovács et al. (2018, <doi:10.1016/j.ecolind.2018.07.026>)\n    - EDI index by Chamorro et al. (2024, <doi:10.1021/acsestwater.4c00126>)\n    - DDI index by Álvarez-Blanco et al. (2013, <doi: 10.1007/s10661-012-2607-z>)\n    - PDISE index by Kahlert et al. (2023, <doi:10.1007/s10661-023-11378-4>).",
    "version": "0.1.5",
    "maintainer": "Joaquin Cochero <jcochero@ilpla.edu.ar>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11675,
    "package_name": "didec",
    "title": "Directed Dependence Coefficient",
    "description": "Directed Dependence Coefficient (didec) is a measure of directed dependence. Multivariate Feature Ordering by Conditional Independence (MFOCI) is a variable selection algorithm based on didec. Hierarchical Variable Clustering (VarClustPartition) is a variable clustering method based on didec. For more information, see the paper by \n    Ansari and Fuchs (2024, <doi:10.48550/arXiv.2212.01621>), and the paper by \n    Fuchs and Wang (2024, <doi:10.1016/j.ijar.2024.109185>).",
    "version": "0.1.0",
    "maintainer": "Yuping Wang <yuping.wang@plus.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11705,
    "package_name": "diffusionMap",
    "title": "Diffusion Map",
    "description": "Implements diffusion map method of data\n    parametrization, including creation and visualization of\n    diffusion map, clustering with diffusion K-means and\n\t  regression using adaptive regression model.\n\t  Richards (2009) <doi:10.1088/0004-637X/691/1/32>.",
    "version": "1.2.0",
    "maintainer": "Robrecht Cannoodt <rcannood@gmail.com>",
    "url": "https://github.com/rcannood/diffusionMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11706,
    "package_name": "diffval",
    "title": "Vegetation Patterns",
    "description": "Find, visualize and explore patterns of differential taxa in\n    vegetation data (namely in a phytosociological table), using the\n    Differential Value (DiffVal). Patterns are searched through\n    mathematical optimization algorithms. Ultimately, Total Differential\n    Value (TDV) optimization aims at obtaining classifications of\n    vegetation data based on differential taxa, as in the traditional\n    geobotanical approach (Monteiro-Henriques 2025,\n    <doi:10.3897/VCS.140466>). The Gurobi optimizer, as well\n    as the R package 'gurobi', can be installed from\n    <https://www.gurobi.com/products/gurobi-optimizer/>.  The useful\n    vignette Gurobi Installation Guide, from package 'prioritizr', can be\n    found here:\n    <https://prioritizr.net/articles/gurobi_installation_guide.html>.",
    "version": "1.2.0",
    "maintainer": "Tiago Monteiro-Henriques <tmh.dev@icloud.com>",
    "url": "https://point-veg.gitlab.io/diffval/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11723,
    "package_name": "dipm",
    "title": "Depth Importance in Precision Medicine (DIPM) Method",
    "description": "An implementation by Chen, Li, and Zhang (2022) <doi: 10.1093/bioadv/vbac041> of the Depth Importance in Precision Medicine (DIPM) method \n             in Chen and Zhang (2022) <doi:10.1093/biostatistics/kxaa021> and Chen and \n             Zhang (2020) <doi:10.1007/978-3-030-46161-4_16>. The DIPM method is a classification \n             tree that searches for subgroups with especially poor or strong performance in a given treatment group.",
    "version": "1.12",
    "maintainer": "Cai Li <cai.li.stats@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11752,
    "package_name": "discoveR",
    "title": "Exploratory Data Analysis System",
    "description": "Performs an exploratory data analysis through a 'shiny' interface. It includes basic methods such as the mean, median, mode, normality test, among others. It also includes clustering techniques such as Principal Components Analysis, Hierarchical Clustering and the K-Means Method.",
    "version": "3.1.7",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "url": "https://promidat.website/, https://github.com/PROMiDAT/discoveR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11756,
    "package_name": "discretization",
    "title": "Data Preprocessing, Discretization for Classification",
    "description": "A collection of supervised discretization\n        algorithms. It can also be grouped in terms of top-down or\n        bottom-up, implementing the discretization algorithms.",
    "version": "1.0-1.1",
    "maintainer": "HyunJi Kim <polaris7867@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11779,
    "package_name": "distanceHD",
    "title": "Distance Metrics for High-Dimensional Clustering",
    "description": "We provide three distance metrics for measuring the separation between two clusters in high-dimensional spaces. The first metric is the centroid distance, which calculates the Euclidean distance between the centers of the two groups. The second is a ridge Mahalanobis distance, which incorporates a ridge correction constant, alpha, to ensure that the covariance matrix is invertible. The third metric is the maximal data piling distance, which computes the orthogonal distance between the affine spaces spanned by each class. These three distances are asymptotically interconnected and are applicable in tasks such as discrimination, clustering, and outlier detection in high-dimensional settings.",
    "version": "1.2",
    "maintainer": "Jung Ae Lee <jungaeleeb@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11826,
    "package_name": "diversityForest",
    "title": "Innovative Complex Split Procedures in Random Forests Through\nCandidate Split Sampling",
    "description": "Implementation of three methods based on the diversity forest (DF) algorithm \n  (Hornung, 2022, <doi:10.1007/s42979-021-00920-1>), a split-finding approach that \n  enables complex split procedures in random forests.\n  The package includes:\n    1. Interaction forests (IFs) (Hornung & Boulesteix, 2022, <doi:10.1016/j.csda.2022.107460>): \n    Model quantitative and qualitative interaction effects using bivariable splitting. \n    Come with the Effect Importance Measure (EIM), which can be used to identify variable \n    pairs that have well-interpretable quantitative and qualitative interaction effects \n    with high predictive relevance.\n\t2. Two random forest-based variable importance measures (VIMs) for multi-class outcomes: \n\tthe class-focused VIM, which ranks covariates by their ability to distinguish individual \n\toutcome classes from the others, and the discriminatory VIM, which measures overall \n\tcovariate influence irrespective of class-specific relevance.\n    3. The basic form of diversity forests that uses conventional univariable, binary \n    splitting (Hornung, 2022).\n  Except for the multi-class VIMs, all methods support categorical, metric, and survival \n  outcomes. The package includes visualization tools for interpreting the identified \n  covariate effects.\n  Built as a fork of the 'ranger' R package (main author: Marvin N. Wright), which \n  implements random forests using an efficient C++ implementation.",
    "version": "0.6.0",
    "maintainer": "Roman Hornung <hornung@ibe.med.uni-muenchen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11838,
    "package_name": "dlib",
    "title": "Allow Access to the 'Dlib' C++ Library",
    "description": "Interface for 'Rcpp' users to 'dlib' <http://dlib.net> which is a\n    'C++' toolkit containing machine learning algorithms and computer vision tools.\n    It is used in a wide range of domains including robotics, embedded devices,\n    mobile phones, and large high performance computing environments. This package\n    allows R users to use 'dlib' through 'Rcpp'.",
    "version": "1.0.3.1",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11852,
    "package_name": "dmbc",
    "title": "Model Based Clustering of Binary Dissimilarity Measurements",
    "description": "Functions for fitting a Bayesian model for grouping binary\n    dissimilarity matrices in homogeneous clusters. Currently, it includes\n    methods only for binary data (<doi:10.18637/jss.v100.i16>).",
    "version": "1.0.3",
    "maintainer": "Sergio Venturini <sergio.venturini@unicatt.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11862,
    "package_name": "dnn",
    "title": "Deep Neural Network Tools for Probability and Statistic Models",
    "description": "Contains a robust set of tools designed for constructing deep neural networks, which are highly adaptable with user-defined loss function and probability models. It includes several practical applications, such as the (deepAFT) model, which utilizes a deep neural network approach to enhance the accelerated failure time (AFT) model for survival data. Another example is the (deepGLM) model that applies deep neural network to the generalized linear model (glm), accommodating data types with continuous, categorical and Poisson distributions.",
    "version": "0.0.7",
    "maintainer": "Bingshu E. Chen <bingshu.chen@queensu.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11879,
    "package_name": "doc2vec",
    "title": "Distributed Representations of Sentences, Documents and Topics",
    "description": "Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,\n    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. \n    The techniques in the package are detailed in the paper \"Distributed Representations of Sentences and Documents\" by Mikolov et al. (2014), available at <doi:10.48550/arXiv.1405.4053>.\n    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. \n    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.\n    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps\n    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm \n    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense\n    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the \n    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can \n    be found which are representative of the topic.\n    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at <doi:10.48550/arXiv.2008.09470>. ",
    "version": "0.2.2",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "https://github.com/bnosac/doc2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11942,
    "package_name": "dpcc",
    "title": "Dynamic Programming for Convex Clustering",
    "description": "Use dynamic programming method to solve l1 convex clustering with identical weights.",
    "version": "1.0.0",
    "maintainer": "Bingyuan Zhang <zhang@sigmath.es.osaka-u.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11971,
    "package_name": "drclust",
    "title": "Simultaneous Clustering and (or) Dimensionality Reduction",
    "description": "Methods for simultaneous clustering and dimensionality reduction such as: Double k-means, Reduced k-means, Factorial k-means, Clustering with Disjoint PCA but also methods for exclusively dimensionality reduction: Disjoint PCA, Disjoint FA. The statistical methods implemented refer to the following articles: de Soete G., Carroll J. (1994) \"K-means clustering in a low-dimensional Euclidean space\" <doi:10.1007/978-3-642-51175-2_24> ; Vichi M. (2001) \"Double k-means Clustering for Simultaneous Classification of Objects and Variables\" <doi:10.1007/978-3-642-59471-7_6> ; Vichi M., Kiers H.A.L. (2001) \"Factorial k-means analysis for two-way data\" <doi:10.1016/S0167-9473(00)00064-5> ; Vichi M., Saporta G. (2009) \"Clustering and disjoint principal component analysis\" <doi:10.1016/j.csda.2008.05.028> ; Vichi M. (2017) \"Disjoint factor analysis with cross-loadings\" <doi:10.1007/s11634-016-0263-9>.",
    "version": "0.1.1",
    "maintainer": "Ionel Prunila <ionel.prunila@uniroma1.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11978,
    "package_name": "drf",
    "title": "Distributional Random Forests",
    "description": "An implementation of distributional random forests as introduced in Cevid & Michel & Meinshausen & Buhlmann (2020) <doi:10.48550/arXiv.2005.14458>.",
    "version": "1.2.0",
    "maintainer": "Jeffrey Naf <jeffrey.naf@unige.ch>",
    "url": "https://github.com/lorismichel/drf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12016,
    "package_name": "dslabs",
    "title": "Data Science Labs",
    "description": "Datasets and functions that can be used for data analysis practice, homework and projects in data science courses and workshops. 26 datasets are available for case studies in data visualization, statistical inference, modeling, linear regression, data wrangling and machine learning.",
    "version": "0.9.1",
    "maintainer": "Rafael A. Irizarry <rafael_irizarry@dfci.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12030,
    "package_name": "dtComb",
    "title": "Statistical Combination of Diagnostic Tests",
    "description": "A system for combining two diagnostic tests using various approaches\n              that include statistical and machine-learning-based methodologies. \n              These approaches are divided into four groups: linear combination \n              methods, non-linear combination methods, mathematical operators, \n              and machine learning algorithms. See \n              the <https://biotools.erciyes.edu.tr/dtComb/> website \n              for more information, documentation, and examples.",
    "version": "1.0.7",
    "maintainer": "Gokmen Zararsiz <gokmen.zararsiz@gmail.com>",
    "url": "https://github.com/gokmenzararsiz/dtComb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12091,
    "package_name": "dynaTree",
    "title": "Dynamic Trees for Learning and Design",
    "description": "Inference by sequential Monte Carlo for \n  dynamic tree regression and classification models\n  with hooks provided for sequential design and optimization, \n  fully online learning with drift, variable selection, and \n  sensitivity analysis of inputs.  Illustrative \n  examples from the original dynamic trees paper \n  (Gramacy, Taddy & Polson (2011); <doi:10.1198/jasa.2011.ap09769>) are facilitated\n  by demos in the package; see demo(package=\"dynaTree\").",
    "version": "1.2-17",
    "maintainer": "Robert B. Gramacy  <rbg@vt.edu>",
    "url": "https://bobby.gramacy.com/r_packages/dynaTree/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12096,
    "package_name": "dynamicTreeCut",
    "title": "Methods for Detection of Clusters in Hierarchical Clustering\nDendrograms",
    "description": "Contains methods for detection of clusters in hierarchical clustering dendrograms.",
    "version": "1.63-1",
    "maintainer": "Peter Langfelder <Peter.Langfelder@gmail.com>",
    "url": "http://www.genetics.ucla.edu/labs/horvath/CoexpressionNetwork/BranchCutting/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12105,
    "package_name": "dynmix",
    "title": "Estimation of Dynamic Finite Mixtures",
    "description": "Allows to perform the dynamic mixture estimation with state-space components and normal regression components, and clustering with normal mixture. Quasi-Bayesian estimation, as well as, that based on the Kerridge inaccuracy approximation are implemented. Main references: Nagy and Suzdaleva (2013) <doi:10.1016/j.apm.2013.05.038>; Nagy et al. (2011) <doi:10.1002/acs.1239>.",
    "version": "2.2",
    "maintainer": "Krzysztof Drachal <kdrachal@wne.uw.edu.pl>",
    "url": "https://CRAN.R-project.org/package=dynmix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12117,
    "package_name": "e1071",
    "title": "Misc Functions of the Department of Statistics, Probability\nTheory Group (Formerly: E1071), TU Wien",
    "description": "Functions for latent class analysis, short time Fourier\n\t     transform, fuzzy clustering, support vector machines,\n\t     shortest path computation, bagged clustering, naive Bayes\n\t     classifier, generalized k-nearest neighbour ...",
    "version": "1.7-17",
    "maintainer": "David Meyer <David.Meyer@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12118,
    "package_name": "e2tree",
    "title": "Explainable Ensemble Trees",
    "description": "The Explainable Ensemble Trees 'e2tree' approach has been proposed by Aria et al. (2024) <doi:10.1007/s00180-022-01312-6>. It aims to explain and interpret decision tree ensemble models using a single tree-like structure. 'e2tree' is a new way of explaining an ensemble tree trained through 'randomForest' or 'xgboost' packages.",
    "version": "0.2.0",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "url": "https://github.com/massimoaria/e2tree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12146,
    "package_name": "earth",
    "title": "Multivariate Adaptive Regression Splines",
    "description": "Build regression models using the techniques in Friedman's\n    papers \"Fast MARS\" and \"Multivariate Adaptive Regression\n    Splines\" <doi:10.1214/aos/1176347963>.\n    (The term \"MARS\" is trademarked and thus not used in\n    the name of the package.)",
    "version": "5.3.4",
    "maintainer": "Stephen Milborrow <milbo@sonic.net>",
    "url": "http://www.milbo.users.sonic.net/earth/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12149,
    "package_name": "earthtones",
    "title": "Derive a Color Palette from a Particular Location on Earth",
    "description": "Downloads a satellite image via ESRI and maptiles (these are\n    originally from a variety of aerial photography sources), \n    translates the image into a perceptually uniform color space,\n    runs one of a few different clustering algorithms on the colors in the image \n    searching for a user-supplied number of colors,\n    and returns the resulting color palette.  ",
    "version": "0.2.0",
    "maintainer": "Will Cornwell <wcornwell@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12151,
    "package_name": "easy.glmnet",
    "title": "Functions to Simplify the Use of 'glmnet' for Machine Learning",
    "description": "Provides several functions to simplify using the 'glmnet' package: converting data frames into matrices ready for 'glmnet'; b) imputing missing variables multiple times; c) fitting and applying prediction models straightforwardly; d) assigning observations to folds in a balanced way; e) cross-validate the models; f) selecting the most representative model across imputations and folds; and g) getting the relevance of the model regressors; as described in several publications: Solanes et al. (2022) <doi:10.1038/s41537-022-00309-w>, Palau et al. (2023) <doi:10.1016/j.rpsm.2023.01.001>, Sobregrau et al. (2024) <doi:10.1016/j.jpsychores.2024.111656>.",
    "version": "1.0",
    "maintainer": "Joaquim Radua <quimradua@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12186,
    "package_name": "eat",
    "title": "Efficiency Analysis Trees",
    "description": "Functions are provided to determine production frontiers and technical \n    efficiency measures through non-parametric techniques based upon regression trees. \n    The package includes code for estimating radial input, output, directional and \n    additive measures, plotting graphical representations of the scores and the production \n    frontiers by means of trees, and determining rankings of importance of input variables \n    in the analysis. Additionally, an adaptation of Random Forest by a set of individual \n    Efficiency Analysis Trees for estimating technical efficiency is also included. More \n    details in: <doi:10.1016/j.eswa.2020.113783>.",
    "version": "0.1.4",
    "maintainer": "Miriam Esteve <mestevecampello@gmail.com>",
    "url": "https://efficiencytools.wordpress.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12201,
    "package_name": "ebmc",
    "title": "Ensemble-Based Methods for Class Imbalance Problem",
    "description": "Four ensemble-based methods (SMOTEBoost, RUSBoost, UnderBagging, and SMOTEBagging) for class imbalance problem are implemented for binary classification. Such methods adopt ensemble methods and data re-sampling techniques to improve model performance in presence of class imbalance problem. One special feature offers the possibility to choose multiple supervised learning algorithms to build weak learners within ensemble models. References: Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O. Hall, and Kevin W. Bowyer (2003) <doi:10.1007/978-3-540-39804-2_12>, Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Amri Napolitano (2010) <doi:10.1109/TSMCA.2009.2029559>, R. Barandela, J. S. Sanchez, R. M. Valdovinos (2003) <doi:10.1007/s10044-003-0192-z>, Shuo Wang and Xin Yao (2009) <doi:10.1109/CIDM.2009.4938667>, Yoav Freund and Robert E. Schapire (1997) <doi:10.1006/jcss.1997.1504>.",
    "version": "1.0.1",
    "maintainer": "\"Hsiang Hao, Chen\" <kbman1101@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12207,
    "package_name": "ecan",
    "title": "Ecological Analysis and Visualization",
    "description": "Support ecological analyses such as ordination and clustering. \n    Contains consistent and easy wrapper functions of 'stat', 'vegan', and \n    'labdsv' packages, and visualisation functions of ordination and clustering.",
    "version": "0.2.1",
    "maintainer": "Toshikazu Matsumura <matutosi@gmail.com>",
    "url": "https://github.com/matutosi/ecan\nhttps://github.com/matutosi/ecan/tree/develop (devel)",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12231,
    "package_name": "ecode",
    "title": "Ordinary Differential Equation Systems in Ecology",
    "description": "A framework to simulate ecosystem dynamics through ordinary differential equations (ODEs). \n    You create an ODE model, tells 'ecode' to explore its behaviour, and perform numerical \n    simulations on the model. 'ecode' also allows you to fit model parameters by machine learning \n    algorithms. Potential users include researchers who are interested in the dynamics of ecological \n    community and biogeochemical cycles.",
    "version": "0.1.0",
    "maintainer": "Haoran Wu <haoran.wu@wolfson.ox.ac.uk>",
    "url": "https://github.com/HaoranPopEvo/ecode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12297,
    "package_name": "edl",
    "title": "Toolbox for Error-Driven Learning Simulations with Two-Layer\nNetworks",
    "description": "Error-driven learning (based on the Widrow & Hoff (1960)<https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf> learning rule, and essentially the same as Rescorla-Wagner's learning equations (Rescorla & Wagner, 1972, ISBN: 0390718017), which are also at the core of Naive Discrimination Learning, (Baayen et al, 2011, <doi:10.1037/a0023851>) can be used to explain bottom-up human learning (Hoppe et al, <doi:10.31234/osf.io/py5kd>), but is also at the core of artificial neural networks applications in the form of the Delta rule. This package provides a set of functions for building small-scale simulations to investigate the dynamics of error-driven learning and it's interaction with the structure of the input. For modeling error-driven learning using the Rescorla-Wagner equations the package 'ndl' (Baayen et al, 2011, <doi:10.1037/a0023851>) is available on CRAN at <https://cran.r-project.org/package=ndl>. However, the package currently only allows tracing of a cue-outcome combination, rather than returning the learned networks. To fill this gap, we implemented a new package with a few functions that facilitate inspection of the networks for small error driven learning simulations. Note that our functions are not optimized for training large data sets (no parallel processing), as they are intended for small scale simulations and course examples. (Consider the python implementation 'pyndl' <https://pyndl.readthedocs.io/en/latest/> for that purpose.) ",
    "version": "1.1",
    "maintainer": "Jacolien van Rij <j.c.van.rij@rug.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12308,
    "package_name": "eegkitdata",
    "title": "Electroencephalography Toolkit Datasets",
    "description": "Contains the example EEG data used in the package eegkit. Also contains code for easily creating larger EEG datasets from the EEG Database on the UCI Machine Learning Repository.",
    "version": "1.1",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12321,
    "package_name": "effects",
    "title": "Effect Displays for Linear, Generalized Linear, and Other Models",
    "description": "\n  Graphical and tabular effect displays, e.g., of interactions, for \n  various statistical models with linear predictors.",
    "version": "4.2-4",
    "maintainer": "John Fox <jfox@mcmaster.ca>",
    "url": "https://cran.r-project.org/package=effects,\nhttps://www.john-fox.ca/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12333,
    "package_name": "ehymet",
    "title": "Methodologies for Functional Data Based on the Epigraph and\nHypograph Indices",
    "description": "Implements methods for functional data analysis based on the epigraph \n  and hypograph indices. These methods transform \n  functional datasets, whether in one or multiple dimensions, into multivariate \n  datasets. The transformation involves applying the epigraph, hypograph, and \n  their modified versions to both the original curves and their first and second \n  derivatives. The calculation of these indices is tailored to the dimensionality \n  of the functional dataset, with special considerations for dependencies between \n  dimensions in multidimensional cases. This approach extends traditional multivariate\n  data analysis techniques to the functional data setting. A key application of \n  this package is the EHyClus method, which enhances clustering analysis for \n  functional data across one or multiple dimensions using the epigraph and \n  hypograph indices. See Pulido et al. (2023) <doi:10.1007/s11222-023-10213-7>\n  and Pulido et al. (2024) <doi:10.48550/arXiv.2307.16720>.",
    "version": "0.1.1",
    "maintainer": "Belen Pulido <bpulidob4@gmail.com>",
    "url": "https://github.com/bpulidob/ehymet,\nhttps://bpulidob.github.io/ehymet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12361,
    "package_name": "elasticnet",
    "title": "Elastic-Net for Sparse Estimation and Sparse PCA",
    "description": "Provides functions for fitting the entire\n        solution path of the Elastic-Net and also provides functions\n        for doing sparse PCA.  ",
    "version": "1.3",
    "maintainer": "Hui Zou <zouxx019@umn.edu>",
    "url": "http://users.stat.umn.edu/~zouxx019/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12384,
    "package_name": "elmNNRcpp",
    "title": "The Extreme Learning Machine Algorithm",
    "description": "Training and predict functions for Single Hidden-layer Feedforward Neural Networks (SLFN) using the Extreme Learning Machine (ELM) algorithm. The ELM algorithm differs from the traditional gradient-based algorithms for very short training times (it doesn't need any iterative tuning, this makes learning time very fast) and there is no need to set any other parameters like learning rate, momentum, epochs, etc. This is a reimplementation of the 'elmNN' package using 'RcppArmadillo' after the 'elmNN' package was archived. For more information, see \"Extreme learning machine: Theory and applications\" by Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew (2006), Elsevier B.V, <doi:10.1016/j.neucom.2005.12.126>.",
    "version": "1.0.5",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "url": "https://github.com/mlampros/elmNNRcpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12442,
    "package_name": "engression",
    "title": "Engression Modelling",
    "description": "Fits engression models for nonlinear distributional regression. Predictors and targets can be univariate or multivariate. Functionality includes estimation of conditional mean, estimation of conditional quantiles, or sampling from the fitted distribution. Training is done full-batch on CPU (the python version offers GPU-accelerated stochastic gradient descent). Based on \"Engression: Extrapolation for nonlinear regression?\" by Xinwei Shen and Nicolai Meinshausen (2023). Also supports classification (experimental). \n <arxiv:2307.00835>.",
    "version": "0.1.4",
    "maintainer": "Nicolai Meinshausen <meinshausen@stat.math.ethz.ch>",
    "url": "https://github.com/xwshen51/engression/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12654,
    "package_name": "etree",
    "title": "Classification and Regression with Structured and Mixed-Type\nData",
    "description": "Implementation of Energy Trees, a statistical model to perform \n    classification and regression with structured and mixed-type data. The\n    model has a similar structure to Conditional Trees, but brings in Energy\n    Statistics to test independence between variables that are possibly \n    structured and of different nature. Currently, the package covers functions\n    and graphs as structured covariates. It builds upon 'partykit' to\n    provide functionalities for fitting, printing, plotting, and predicting with\n    Energy Trees. Energy Trees are described in Giubilei et al. (2022) \n    <arXiv:2207.04430>. ",
    "version": "0.1.0",
    "maintainer": "Riccardo Giubilei <riccardogbl@gmail.com>",
    "url": "https://github.com/ricgbl/etree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12665,
    "package_name": "eunis.habitats",
    "title": "EUNIS Habitat Classification",
    "description": "The EUNIS habitat classification is a comprehensive pan-European\n    system for habitat identification\n    <https://www.eea.europa.eu/data-and-maps/data/eunis-habitat-classification-1>.\n    This is an R data package providing the EUNIS classification system.\n    The classification is hierarchical and covers all types of habitats from\n    natural to artificial, from terrestrial to freshwater and marine. The\n    habitat types are identified by specific codes, names and descriptions and\n    come with schema crosswalks to other habitat typologies.",
    "version": "0.1.0",
    "maintainer": "Ramiro Magno <rmagno@pattern.institute>",
    "url": "https://github.com/ramiromagno/eunis.habitats,\nhttps://rmagno.eu/eunis.habitats/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12681,
    "package_name": "evclass",
    "title": "Evidential Distance-Based Classification",
    "description": "Different evidential classifiers, which provide\n    outputs in the form of Dempster-Shafer mass functions. The methods are: \n    the evidential K-nearest neighbor rule, the evidential neural \n    network, radial basis function neural networks, logistic regression,\n    feed-forward neural networks.",
    "version": "2.0.2",
    "maintainer": "Thierry Denoeux <tdenoeux@utc.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12715,
    "package_name": "evprof",
    "title": "Electric Vehicle Charging Sessions Profiling and Modelling",
    "description": "Tools for modelling electric vehicle charging sessions into\n    generic groups with similar connection patterns called \"user profiles\",\n    using Gaussian Mixture Models clustering. The clustering and profiling\n    methodology is described in Cañigueral and Meléndez (2021, ISBN:0142-0615) \n    <doi:10.1016/j.ijepes.2021.107195>.",
    "version": "1.2.0",
    "maintainer": "Marc Cañigueral <marccanyigueral@gmail.com>",
    "url": "https://github.com/resourcefully-dev/evprof/,\nhttps://resourcefully-dev.github.io/evprof/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12718,
    "package_name": "evtclass",
    "title": "Extreme Value Theory for Open Set Classification - GPD and GEV\nClassifiers",
    "description": "Two classifiers for open set recognition and novelty detection based on extreme value theory. The first classifier is based on the generalized Pareto distribution (GPD) and the second classifier is based on the generalized extreme value (GEV) distribution. For details, see Vignotto, E., & Engelke, S. (2018) <arXiv:1808.09902>.",
    "version": "1.0",
    "maintainer": "Edoardo Vignotto <edoardo.vignotto@unige.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12719,
    "package_name": "evtree",
    "title": "Evolutionary Learning of Globally Optimal Trees",
    "description": "Commonly used classification and regression tree methods like the CART algorithm\n             are recursive partitioning methods that build the model in a forward stepwise search.\n\t     Although this approach is known to be an efficient heuristic, the results of recursive\n\t     tree methods are only locally optimal, as splits are chosen to maximize homogeneity at\n\t     the next step only. An alternative way to search over the parameter space of trees is\n\t     to use global optimization methods like evolutionary algorithms. The 'evtree' package\n\t     implements an evolutionary algorithm for learning globally optimal classification and\n\t     regression trees in R. CPU and memory-intensive tasks are fully computed in C++ while\n\t     the 'partykit' package is leveraged to represent the resulting trees in R, providing\n\t     unified infrastructure for summaries, visualizations, and predictions.",
    "version": "1.0-8",
    "maintainer": "Thomas Grubinger <ThomasGrubinger@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12730,
    "package_name": "exametrika",
    "title": "Test Theory Analysis and Biclustering",
    "description": "Implements comprehensive test data engineering methods as described in \n    Shojima (2022, ISBN:978-9811699856). Provides statistical techniques for \n    engineering and processing test data: Classical Test Theory (CTT) with \n    reliability coefficients for continuous ability assessment; Item Response \n    Theory (IRT) including Rasch, 2PL, and 3PL models with item/test information \n    functions; Latent Class Analysis (LCA) for nominal clustering; Latent Rank \n    Analysis (LRA) for ordinal clustering with automatic determination of cluster \n    numbers; Biclustering methods including infinite relational models for \n    simultaneous clustering of examinees and items without predefined cluster \n    numbers; and Bayesian Network Models (BNM) for visualizing inter-item \n    dependencies. Features local dependence analysis through LRA and biclustering, \n    parameter estimation, dimensionality assessment, and network structure \n    visualization for educational, psychological, and social science research.",
    "version": "1.8.0",
    "maintainer": "Koji Kosugi <kosugitti@gmail.com>",
    "url": "https://kosugitti.github.io/exametrika/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12759,
    "package_name": "expandFunctions",
    "title": "Feature Matrix Builder",
    "description": "Generates feature matrix outputs from R object inputs\n    using a variety of expansion functions.  The generated\n    feature matrices have applications as inputs\n    for a variety of machine learning algorithms.\n    The expansion functions are based on coercing the input\n    to a matrix, treating the columns as features and\n    converting individual columns or combinations into blocks of\n    columns.\n    Currently these include expansion of columns by\n    efficient sparse embedding by vectors of lags,\n    quadratic expansion into squares and unique products,\n    powers by vectors of degree,\n    vectors of orthogonal polynomials functions,\n    and block random affine projection transformations (RAPTs).\n    The transformations are\n    magrittr- and cbind-friendly, and can be used in a\n    building block fashion.  For instance, taking the cos() of\n    the output of the RAPT transformation generates a\n    stationary kernel expansion via Bochner's theorem, and this\n    expansion can then be cbind-ed with other features.\n    Additionally, there are utilities for replacing features,\n    removing rows with NAs,\n    creating matrix samples of a given distribution,\n    a simple wrapper for LASSO with CV,\n    a Freeman-Tukey transform,\n    generalizations of the outer function,\n    matrix size-preserving discrete difference by row,\n    plotting, etc.",
    "version": "0.1.0",
    "maintainer": "Scott Miller <sam3CRAN@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12857,
    "package_name": "fabMix",
    "title": "Overfitting Bayesian Mixtures of Factor Analyzers with\nParsimonious Covariance and Unknown Number of Components",
    "description": "Model-based clustering of multivariate continuous data using Bayesian mixtures of factor analyzers (Papastamoulis (2019) <DOI:10.1007/s11222-019-09891-z> (2018) <DOI:10.1016/j.csda.2018.03.007>). The number of clusters is estimated using overfitting mixture models (Rousseau and Mengersen (2011) <DOI:10.1111/j.1467-9868.2011.00781.x>): suitable prior assumptions ensure that asymptotically the extra components will have zero posterior weight, therefore, the inference is based on the ``alive'' components. A Gibbs sampler is implemented in order to (approximately) sample from the posterior distribution of the overfitting mixture. A prior parallel tempering scheme is also available, which allows to run multiple parallel chains with different prior distributions on the mixture weights. These chains run in parallel and can swap states using a Metropolis-Hastings move. Eight different parameterizations give rise to parsimonious representations of the covariance per cluster (following Mc Nicholas and Murphy (2008) <DOI:10.1007/s11222-008-9056-0>). The model parameterization and number of factors is selected according to the Bayesian Information Criterion. Identifiability issues related to label switching are dealt by post-processing the simulated output with the Equivalence Classes Representatives algorithm (Papastamoulis and Iliopoulos (2010) <DOI:10.1198/jcgs.2010.09008>, Papastamoulis (2016) <DOI:10.18637/jss.v069.c01>). ",
    "version": "5.1",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "https://github.com/mqbssppe/overfittingFABMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12861,
    "package_name": "fabisearch",
    "title": "Change Point Detection in High-Dimensional Time Series Networks",
    "description": "Implementation of the Factorized Binary Search (FaBiSearch) methodology for the estimation of the number and the location of multiple change points in the network (or clustering) structure of multivariate high-dimensional time series. The method is motivated by the detection of change points in functional connectivity networks for functional magnetic resonance imaging (fMRI) data. FaBiSearch uses non-negative matrix factorization (NMF), an unsupervised dimension reduction technique, and a new binary search algorithm to identify multiple change points.  It requires minimal assumptions. Lastly, we provide interactive, 3-dimensional, brain-specific network visualization capability in a flexible, stand-alone function. This function can be conveniently used with any node coordinate atlas, and nodes can be color coded according to community membership, if applicable. The output is an elegantly displayed network laid over a cortical surface, which can be rotated in the 3-dimensional space. The main routines of the package are detect.cps(), for multiple change point detection, est.net(), for estimating a network between stationary multivariate time series, net.3dplot(), for plotting the estimated functional connectivity networks, and opt.rank(), for finding the optimal rank in NMF for a given data set. The functions have been extensively tested on simulated multivariate high-dimensional time series data and fMRI data. For details on the FaBiSearch methodology, please see Ondrus et al. (2021) <arXiv:2103.06347>. For a more detailed explanation and applied examples of the fabisearch package, please see Ondrus and Cribben (2022), preprint.",
    "version": "0.0.4.5",
    "maintainer": "Martin Ondrus <mondrus@ualberta.ca>",
    "url": "https://github.com/mondrus96/FaBiSearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12882,
    "package_name": "factoextra",
    "title": "Extract and Visualize the Results of Multivariate Data Analyses",
    "description": "Provides some easy-to-use functions to extract and visualize the\n    output of multivariate data analyses, including 'PCA' (Principal Component\n    Analysis), 'CA' (Correspondence Analysis), 'MCA' (Multiple Correspondence\n    Analysis), 'FAMD' (Factor Analysis of Mixed Data), 'MFA' (Multiple Factor Analysis) and 'HMFA' (Hierarchical Multiple\n    Factor Analysis) functions from different R packages. It contains also functions\n    for simplifying some clustering analysis steps and provides 'ggplot2' - based\n    elegant data visualization.",
    "version": "1.0.7",
    "maintainer": "Alboukadel Kassambara <alboukadel.kassambara@gmail.com>",
    "url": "http://www.sthda.com/english/rpkgs/factoextra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12893,
    "package_name": "factree",
    "title": "Factor-Augmented Clustering Tree",
    "description": "Implements the Factor-Augmented Clustering Tree (FACT) algorithm\n    for clustering time series data. The method constructs a classification \n    tree where splits are determined by covariates, and the splitting criterion\n    is based on a group factor model representation of the time series within \n    each node. Both threshold-based and permutation-based tests are supported \n    for splitting decisions, with an option for parallel computation.\n    For methodological details, see Hu, Li, Luo, and Wang (2025, in preparation), \n    Factor-Augmented Clustering Tree for Time Series.",
    "version": "0.1.0",
    "maintainer": "Jiaqi Hu <hujiaqi@mail.ustc.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12900,
    "package_name": "fairGATE",
    "title": "Fair Gated Algorithm for Targeted Equity",
    "description": "Tools for training and analysing fairness-aware gated neural \n    networks for subgroup-aware prediction and interpretation in clinical datasets. \n    Methods draw on prior work in mixture-of-experts neural networks by\n    Jordan and Jacobs (1994) <doi:10.1007/978-1-4471-2097-1_113>,\n    fairness-aware learning by Hardt, Price, and Srebro (2016) <doi:10.48550/arXiv.1610.02413>,\n    and personalised treatment prediction for depression by Iniesta, Stahl, and McGuffin (2016) \n    <doi:10.1016/j.jpsychires.2016.03.016>.",
    "version": "0.1.1",
    "maintainer": "Rhys Holland <rhys.holland@icloud.com>",
    "url": "https://github.com/rhysholland/FairGATE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12901,
    "package_name": "fairGNN",
    "title": "Fairness-Aware Gated Neural Networks",
    "description": "Tools for training and analysing fairness-aware gated neural \n    networks for subgroup-aware prediction and interpretation in clinical datasets. \n    Methods draw on prior work in mixture-of-experts neural networks by\n    Jordan and Jacobs (1994) <doi:10.1007/978-1-4471-2097-1_113>,\n    fairness-aware learning by Hardt, Price, and Srebro (2016) <doi:10.48550/arXiv.1610.02413>,\n    and personalised treatment prediction for depression by Iniesta, Stahl, and McGuffin (2016) \n    <doi:10.1016/j.jpsychires.2016.03.016>.",
    "version": "0.1.0",
    "maintainer": "Rhys Holland <rhys.holland@icloud.com>",
    "url": "https://github.com/rhysholland/fairGNN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12903,
    "package_name": "fairmetrics",
    "title": "Fairness Evaluation Metrics with Confidence Intervals for Binary\nProtected Attributes",
    "description": "A collection of functions for computing fairness metrics for machine learning and statistical models, including confidence intervals for each metric. The package supports the evaluation of group-level fairness criterion commonly used in fairness research, particularly in healthcare for binary protected attributes. It is based on the overview of fairness in machine learning written by Gao et al (2024) <doi:10.48550/arXiv.2406.09307>.",
    "version": "1.0.7",
    "maintainer": "Benjamin Smith <benyamin.smith@mail.utoronto.ca>",
    "url": "https://jianhuig.github.io/fairmetrics/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12904,
    "package_name": "fairml",
    "title": "Fair Models in Machine Learning",
    "description": "Fair machine learning regression models which take sensitive attributes into account in\n  model estimation. Currently implementing Komiyama et al. (2018) \n  <http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf>, Zafar et al.\n  (2019) <https://www.jmlr.org/papers/volume20/18-262/18-262.pdf> and my own\n  approach from Scutari, Panero and Proissl (2022)\n  <doi:10.1007/s11222-022-10143-w> that uses ridge regression to enforce fairness.",
    "version": "0.9",
    "maintainer": "Marco Scutari <scutari@bnlearn.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12908,
    "package_name": "fake",
    "title": "Flexible Data Simulation Using the Multivariate Normal\nDistribution",
    "description": "This R package can be used to generate artificial data conditionally on pre-specified (simulated or user-defined) relationships between the variables and/or observations. Each observation is drawn from a multivariate Normal distribution where the mean vector and covariance matrix reflect the desired relationships. Outputs can be used to evaluate the performances of variable selection, graphical modelling, or clustering approaches by comparing the true and estimated structures (B Bodinier et al (2021) <arXiv:2106.02521>).",
    "version": "1.4.0",
    "maintainer": "Barbara Bodinier <barbara.bodinier@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12912,
    "package_name": "fakmct",
    "title": "Fuzzy Adaptive Resonance Theory K-Means Clustering Technique",
    "description": "A set of function for clustering data observation with hybrid method Fuzzy ART and K-Means \n            by Sengupta, Ghosh & Dan (2011) <doi:10.1080/0951192X.2011.602362>.",
    "version": "0.1.0",
    "maintainer": "Alfi Nurrahmah <221810140@stis.ac.id>",
    "url": "<https://github.com/alfinurrahmah/fakmct>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12930,
    "package_name": "farff",
    "title": "A Faster 'ARFF' File Reader and Writer",
    "description": "Reads and writes 'ARFF' files. 'ARFF' (Attribute-Relation\n    File Format) files are like 'CSV' files, with a little bit of added\n    meta information in a header and standardized NA values. They are\n    quite often used for machine learning data sets and were introduced\n    for the 'WEKA' machine learning 'Java' toolbox. See\n    <https://waikato.github.io/weka-wiki/formats_and_processing/arff_stable/>\n    for further info on 'ARFF' and for\n    <http://www.cs.waikato.ac.nz/ml/weka/> for more info on 'WEKA'.\n    'farff' gets rid of the 'Java' dependency that 'RWeka' enforces, and\n    it is at least a faster reader (for bigger files). It uses 'readr' as\n    parser back-end for the data section of the 'ARFF' file. Consistency\n    with 'RWeka' is tested on 'Github' and 'Travis CI' with hundreds of\n    'ARFF' files from 'OpenML'.",
    "version": "1.1.1",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "url": "https://github.com/mlr-org/farff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12958,
    "package_name": "fastText",
    "title": "Efficient Learning of Word Representations and Sentence\nClassification",
    "description": "An interface to the 'fastText' <https://github.com/facebookresearch/fastText> library for efficient learning of word representations and sentence classification. The 'fastText' algorithm is explained in detail in (i) \"Enriching Word Vectors with subword Information\", Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, 2017, <doi:10.1162/tacl_a_00051>; (ii) \"Bag of Tricks for Efficient Text Classification\", Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2017, <doi:10.18653/v1/e17-2068>; (iii) \"FastText.zip: Compressing text classification models\", Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, Tomas Mikolov, 2016, <arXiv:1612.03651>.",
    "version": "1.0.4",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "url": "https://github.com/mlampros/fastText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12959,
    "package_name": "fastTextR",
    "title": "An Interface to the 'fastText' Library",
    "description": "An interface to the 'fastText' library\n\t<https://github.com/facebookresearch/fastText>. The package\n\tcan be used for text classification and to learn word vectors.\n\tAn example how to use 'fastTextR' can be found in the 'README' file.",
    "version": "2.1.0",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "url": "https://github.com/EmilHvitfeldt/fastTextR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12965,
    "package_name": "fastai",
    "title": "Interface to 'fastai'",
    "description": "The 'fastai' <https://docs.fast.ai/index.html> library \n             simplifies training fast and accurate neural networks \n             using modern best practices. It is based on research \n             in to deep learning best practices undertaken \n             at 'fast.ai', including 'out of the box' support\n             for vision, text, tabular, audio, time series, and \n             collaborative filtering models. ",
    "version": "2.2.2",
    "maintainer": "Turgut Abdullayev <turqut.a.314@gmail.com>",
    "url": "https://github.com/EagerAI/fastai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12970,
    "package_name": "fastcluster",
    "title": "Fast Hierarchical Clustering Routines for R and 'Python'",
    "description": "This is a two-in-one package which provides interfaces to\n        both R and 'Python'. It implements fast hierarchical, agglomerative\n        clustering routines. Part of the functionality is designed as drop-in\n        replacement for existing routines: linkage() in the 'SciPy' package\n        'scipy.cluster.hierarchy', hclust() in R's 'stats' package, and the\n        'flashClust' package. It provides the same functionality with the\n        benefit of a much faster implementation. Moreover, there are\n        memory-saving routines for clustering of vector data, which go beyond\n        what the existing packages provide. For information on how to install\n        the 'Python' files, see the file INSTALL in the source distribution.\n        Based on the present package, Christoph Dalitz also wrote a pure 'C++'\n        interface to 'fastcluster':\n        <https://lionel.kr.hs-niederrhein.de/~dalitz/data/hclust/>.",
    "version": "1.3.0",
    "maintainer": "Daniel Müllner <daniel@danifold.net>",
    "url": "https://danifold.net/fastcluster.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12981,
    "package_name": "fastkmedoids",
    "title": "Faster K-Medoids Clustering Algorithms: FastPAM, FastCLARA,\nFastCLARANS",
    "description": "R wrappers of C++ implementation of Faster K-Medoids clustering algorithms (FastPAM, FastCLARA and FastCLARANS) proposed in Erich Schubert, Peter J. Rousseeuw 2019 <doi:10.1007/978-3-030-32047-8_16>.",
    "version": "1.2",
    "maintainer": "Xun Li <lixun910@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13003,
    "package_name": "fastshap",
    "title": "Fast Approximate Shapley Values",
    "description": "Computes fast (relative to other implementations) approximate \n    Shapley values for any supervised learning model. Shapley values help to \n    explain the predictions from any black box model using ideas from game \n    theory; see Strumbel and Kononenko (2014) <doi:10.1007/s10115-013-0679-x> \n    for details.",
    "version": "0.1.1",
    "maintainer": "Brandon Greenwell <greenwell.brandon@gmail.com>",
    "url": "https://github.com/bgreenwell/fastshap,\nhttps://bgreenwell.github.io/fastshap/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13034,
    "package_name": "fclust",
    "title": "Fuzzy Clustering",
    "description": "Algorithms for fuzzy clustering, cluster validity indices and plots for cluster validity and visualizing fuzzy clustering results.",
    "version": "2.1.3",
    "maintainer": "Paolo Giordani <paolo.giordani@uniroma1.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13041,
    "package_name": "fdWasserstein",
    "title": "Application of Optimal Transport to Functional Data Analysis",
    "description": "These functions were developed to support statistical analysis on functional covariance operators.\n  The package contains functions to:\n  - compute 2-Wasserstein distances between Gaussian Processes as in\n    Masarotto, Panaretos & Zemel (2019) <doi:10.1007/s13171-018-0130-1>;\n  - compute the Wasserstein barycenter (Frechet mean) as in Masarotto,\n    Panaretos & Zemel (2019) <doi:10.1007/s13171-018-0130-1>;\n  - perform analysis of variance testing procedures for functional\n    covariances and tangent space principal component analysis of\n    covariance operators as in Masarotto, Panaretos & Zemel (2022)\n    <arXiv:2212.04797>.\n  - perform a soft-clustering based on the Wasserstein distance where\n    functional data are classified based on their covariance structure\n    as in Masarotto & Masarotto (2023) <doi:10.1111/sjos.12692>.",
    "version": "1.0",
    "maintainer": "Valentina Masarotto <v.masarotto@math.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13043,
    "package_name": "fda.usc",
    "title": "Functional Data Analysis and Utilities for Statistical Computing",
    "description": "Routines for exploratory and descriptive analysis of functional data such as depth measurements, atypical curves detection, regression models, supervised classification, unsupervised classification and functional analysis of variance.",
    "version": "2.2.0",
    "maintainer": "Manuel Oviedo de la Fuente <manuel.oviedo@udc.es>",
    "url": "https://github.com/moviedo5/fda.usc,\nhttps://moviedo5.github.io/fda.usc/,\nhttps://www.jstatsoft.org/v51/i04/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13046,
    "package_name": "fdaMocca",
    "title": "Model-Based Clustering for Functional Data with Covariates",
    "description": "Routines for model-based functional cluster analysis for functional data with optional covariates. The idea is to cluster functional subjects (often called functional objects) into homogenous groups by using spline smoothers (for functional data) together with scalar covariates. The spline coefficients and the covariates are modelled as a multivariate Gaussian mixture model, where the number of mixtures corresponds to the number of clusters. The parameters of the model are estimated by maximizing the observed mixture likelihood via an EM algorithm (Arnqvist and Sjöstedt de Luna, 2019) <doi:10.48550/arXiv.1904.10265>. The clustering method is used to analyze annual lake sediment from lake Kassjön (Northern Sweden) which cover more than 6400 years and can be seen as historical records of weather and climate.",
    "version": "0.1-2",
    "maintainer": "Natalya Pya <nat.pya@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13060,
    "package_name": "fdm2id",
    "title": "Data Mining and R Programming for Beginners",
    "description": "Contains functions to simplify the use of data mining methods (classification, regression, clustering, etc.), for students and beginners in R programming. Various R packages are used and wrappers are built around the main functions, to standardize the use of data mining methods (input/output): it brings a certain loss of flexibility, but also a gain of simplicity. The package name came from the French \"Fouille de Données en Master 2 Informatique Décisionnelle\".",
    "version": "0.9.9",
    "maintainer": "Alexandre Blansché <alexandre.blansche@univ-lorraine.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13074,
    "package_name": "featurefinder",
    "title": "Feature Finder",
    "description": "Finds features through a detailed analysis of model residuals using rpart classification and regression trees. Scans the residuals of a model across subsets of the data to identify areas where the model differs from the actual data.",
    "version": "1.2",
    "maintainer": "Richard Davis <davisconsulting@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13077,
    "package_name": "featuretoolsR",
    "title": "Interact with the 'Python' Module 'Featuretools'",
    "description": "A 'reticulate'-based interface to the 'Python' module 'Featuretools'.",
    "version": "0.4.4",
    "maintainer": "Magnus Furugård <magnus.furugard@gmail.com>",
    "url": "https://github.com/praktiskt/featuretoolsR",
    "exports": [],
    "topics": ["feature-engineering", "featuretools", "machine-learning", "r-package", "rstats"],
    "score": "NA",
    "stars": 50
  },
  {
    "id": 13181,
    "package_name": "finto",
    "title": "Access the 'Finto' API",
    "description": "Access and retrieve vocabulary data 'Finto' API <https://api.finto.fi/>, which is a centralized service for interoperable thesauri, ontology and classification schemes for different subject areas.",
    "version": "0.1.1",
    "maintainer": "Akewak Jeba <akjeba@utu.fi>",
    "url": "https://fennicahub.github.io/finto/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13197,
    "package_name": "fishbc",
    "title": "Fishes of British Columbia",
    "description": "Provides raw and curated data on the codes,\n    classification and conservation status of freshwater fishes in British\n    Columbia. Marine fishes will be added in a future release.",
    "version": "0.2.1",
    "maintainer": "Evan Amies-Galonski <evan@poissonconsulting.ca>",
    "url": "https://github.com/poissonconsulting/fishbc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13222,
    "package_name": "fitmix",
    "title": "Finite Mixture Model Fitting of Lifespan Datasets",
    "description": "Fits the lifespan datasets of biological systems such as yeast, fruit flies, and other similar biological units with well-known finite mixture models introduced by Farewell V. (1982) <doi:10.2307/2529885> and Al-Hussaini et al. (2000) <doi:10.1080/00949650008812033>. Estimates parameter space fitting of a lifespan dataset with finite mixtures of parametric distributions. Computes the following tasks; 1) Estimates parameter space of the finite mixture model \n             by implementing the expectation maximization (EM) algorithm. 2) Finds a sequence of four goodness-of-fit measures consist of Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), Kolmogorov-Smirnov (KS), and log-likelihood (log-likelihood) statistics. 3)The initial values is determined by k-means clustering.",
    "version": "0.1.0",
    "maintainer": "Emine Guven <emine.guven33@gmail.com>",
    "url": "https://github.com/guven-code/fitmix/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13231,
    "package_name": "fixes",
    "title": "Tools for Creating and Visualizing Fixed-Effects Event Study\nModels",
    "description": "Provides functions for creating, analyzing, and visualizing event study models using fixed-effects regression. Supports staggered adoption, multiple confidence intervals, flexible clustering, and panel/time transformations in a simple workflow.",
    "version": "0.5.0",
    "maintainer": "Yosuke Abe <yosuke.abe0507@gmail.com>",
    "url": "https://github.com/yo5uke/fixes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13248,
    "package_name": "flashClust",
    "title": "Implementation of optimal hierarchical clustering",
    "description": "Fast implementation of hierarchical clustering",
    "version": "1.01-2",
    "maintainer": "Peter Langfelder <Peter.Langfelder@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13263,
    "package_name": "flexclust",
    "title": "Flexible Cluster Algorithms",
    "description": "The main function kcca implements a general framework for\n  k-centroids cluster analysis supporting arbitrary distance measures\n  and centroid computation. Further cluster methods include hard\n  competitive learning, neural gas, and QT clustering. There are\n  numerous visualization methods for cluster results (neighborhood\n  graphs, convex cluster hulls, barcharts of centroids, ...), and\n  bootstrap methods for the analysis of cluster stability.",
    "version": "1.5.0",
    "maintainer": "Bettina Grün <Bettina.Gruen@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13269,
    "package_name": "flexmix",
    "title": "Flexible Mixture Modeling",
    "description": "A general framework for finite mixtures of regression\n  models using the EM algorithm is implemented. The E-step and all\n  data handling are provided, while the M-step can be supplied by the\n  user to easily define new models. Existing drivers implement\n  mixtures of standard linear models, generalized linear models and\n  model-based clustering.",
    "version": "2.3-20",
    "maintainer": "Bettina Gruen <Bettina.Gruen@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13272,
    "package_name": "flexord",
    "title": "Flexible Clustering of Ordinal and Mixed-with-Ordinal Data",
    "description": "Extends the capabilities for flexible partitioning and model-based clustering\n       available in the packages 'flexclust' and 'flexmix' to handle ordinal and\n       mixed-with-ordinal data types via new distance, centroid and driver functions that\n       make various assumptions regarding ordinality. Using them within the flex-scheme\n       allows for easy comparisons across methods.",
    "version": "1.0.0",
    "maintainer": "Lena Ortega Menjivar <lena.ortega-menjivar@boku.ac.at>",
    "url": "https://zettlchen.github.io/flexord/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13279,
    "package_name": "flexurba",
    "title": "Construct Flexible Urban Delineations",
    "description": "Enables the construction of flexible urban delineations that\n    can be tailored to specific applications or research questions, see Van\n    Migerode et al. (2024) <DOI:10.1177/23998083241262545> and Van Migerode\n    et al. (2025) <DOI:10.5281/zenodo.15173220>.\n    Originally developed to flexibly reconstruct the Degree of\n    Urbanisation classification of cities,\n    towns and rural areas developed by Dijkstra et al. \n    (2021) <DOI:10.1016/j.jue.2020.103312>. Now it also support a broader \n    range of delineation approaches, using multiple datasets – including\n    population, built-up area, and night-time light grids – and different\n    thresholding methods.",
    "version": "0.2.2",
    "maintainer": "Céline Van Migerode <celine.vanmigerode@kuleuven.be>",
    "url": "https://github.com/cvmigero/flexurba,\nhttps://gitlab.kuleuven.be/spatial-networks-lab/research-projects/flexurba,\nhttps://flexurba-spatial-networks-lab-research-projects--e74426d1c66ecc.pages.gitlab.kuleuven.be",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13350,
    "package_name": "fmeffects",
    "title": "Model-Agnostic Interpretations with Forward Marginal Effects",
    "description": "Create local, regional, and global explanations for any machine learning model with forward marginal effects. You provide a model and data, and 'fmeffects' computes feature effects. The package is based on the theory in: C. A. Scholbeck, G. Casalicchio, C. Molnar, B. Bischl, and C. Heumann (2022) <doi:10.48550/arXiv.2201.08837>.",
    "version": "0.1.4",
    "maintainer": "Holger Löwe <hbj.loewe@gmail.com>",
    "url": "https://holgstr.github.io/fmeffects/,\nhttps://github.com/holgstr/fmeffects",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13405,
    "package_name": "forestControl",
    "title": "Approximate False Positive Rate Control in Selection Frequency\nfor Random Forest",
    "description": "Approximate false positive rate control in selection frequency for\n    random forest using the methods described by Ender Konukoglu and Melanie Ganz (2014) <arXiv:1410.2838>.\n    Methods for calculating the selection frequency threshold at false positive rates\n    and selection frequency false positive rate feature selection.",
    "version": "0.2.2",
    "maintainer": "Tom Wilson <tpw2@aber.ac.uk>",
    "url": "https://github.com/aberHRML/forestControl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13451,
    "package_name": "fpc",
    "title": "Flexible Procedures for Clustering",
    "description": "Various methods for clustering and cluster validation.\n  Fixed point clustering. Linear regression clustering. Clustering by \n  merging Gaussian mixture components. Symmetric \n  and asymmetric discriminant projections for visualisation of the \n  separation of groupings. Cluster validation statistics\n  for distance based clustering including corrected Rand index. \n  Standardisation of cluster validation statistics by random clusterings and \n  comparison between many clustering methods and numbers of clusters based on\n  this.  \n  Cluster-wise cluster stability assessment. Methods for estimation of \n  the number of clusters: Calinski-Harabasz, Tibshirani and Walther's \n  prediction strength, Fang and Wang's bootstrap stability. \n  Gaussian/multinomial mixture fitting for mixed \n  continuous/categorical variables. Variable-wise statistics for cluster\n  interpretation. DBSCAN clustering. Interface functions for many \n  clustering methods implemented in R, including estimating the number of\n  clusters with kmeans, pam and clara. Modality diagnosis for Gaussian\n  mixtures. For an overview see package?fpc.",
    "version": "2.2-13",
    "maintainer": "Christian Hennig <christian.hennig@unibo.it>",
    "url": "https://www.unibo.it/sitoweb/christian.hennig/en/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13489,
    "package_name": "frbs",
    "title": "Fuzzy Rule-Based Systems for Classification and Regression Tasks",
    "description": "An implementation of various learning algorithms based on fuzzy rule-based systems (FRBSs) for dealing with classification and regression tasks. Moreover, it allows to construct an FRBS model defined by human experts. \n    FRBSs are based on the concept of fuzzy sets, proposed by Zadeh in 1965, which aims at\n    representing the reasoning of human experts in a set of IF-THEN rules, to\n    handle real-life problems in, e.g., control, prediction and inference, data\n    mining, bioinformatics data processing, and robotics. FRBSs are also known\n    as fuzzy inference systems and fuzzy models. During the modeling of an\n    FRBS, there are two important steps that need to be conducted: structure\n    identification and parameter estimation. Nowadays, there exists a wide\n    variety of algorithms to generate fuzzy IF-THEN rules automatically from\n    numerical data, covering both steps. Approaches that have been used in the\n    past are, e.g., heuristic procedures, neuro-fuzzy techniques, clustering\n    methods, genetic algorithms, squares methods, etc. Furthermore, in this\n    version we provide a universal framework named 'frbsPMML', which is adopted\n    from the Predictive Model Markup Language (PMML), for representing FRBS\n    models. PMML is an XML-based language to provide a standard for describing\n    models produced by data mining and machine learning algorithms. Therefore,\n    we are allowed to export and import an FRBS model to/from 'frbsPMML'.\n    Finally, this package aims to implement the most widely used standard\n    procedures, thus offering a standard package for FRBS modeling to the R\n    community.",
    "version": "3.2-0",
    "maintainer": "Christoph Bergmeir <c.bergmeir@decsai.ugr.es>",
    "url": "http://sci2s.ugr.es/dicits/software/FRBS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13568,
    "package_name": "funFEM",
    "title": "Clustering in the Discriminative Functional Subspace",
    "description": "The funFEM algorithm (Bouveyron et al., 2014) allows to cluster functional data by modeling the curves within a common and discriminative functional subspace.",
    "version": "1.2",
    "maintainer": "Charles Bouveyron <charles.bouveyron@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13570,
    "package_name": "funMoDisco",
    "title": "Motif Discovery in Functional Data",
    "description": "Efficiently implementing two complementary methodologies for discovering motifs in functional data: ProbKMA and FunBIalign. \n    Cremona and Chiaromonte (2023) \"Probabilistic K-means with Local Alignment for Clustering and Motif Discovery in Functional Data\" <doi:10.1080/10618600.2022.2156522> is a probabilistic K-means algorithm that leverages local alignment and fuzzy clustering to identify recurring patterns (candidate functional motifs) across and within curves, allowing different portions of the same curve to belong to different clusters. It includes a family of distances and a normalization to discover various motif types and learns motif lengths in a data-driven manner. It can also be used for local clustering of misaligned data.\n    Di Iorio, Cremona, and Chiaromonte (2023) \"funBIalign: A Hierarchical Algorithm for Functional Motif Discovery Based on Mean Squared Residue Scores\" <doi:10.48550/arXiv.2306.04254> applies hierarchical agglomerative clustering with a functional generalization of the Mean Squared Residue Score to identify motifs of a specified length in curves. This deterministic method includes a small set of user-tunable parameters.\n    Both algorithms are suitable for single curves or sets of curves. The package also includes a flexible function to simulate functional data with embedded motifs, allowing users to generate benchmark datasets for validating and comparing motif discovery methods.",
    "version": "1.1.0",
    "maintainer": "Jacopo Di Iorio <jacopo.di.iorio@emory.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13601,
    "package_name": "fuseMLR",
    "title": "Fusing Machine Learning in R",
    "description": "Recent technological advances have enable the simultaneous collection\n    of multi-omics data i.e., different types or modalities of molecular data, \n    presenting challenges for integrative prediction modeling due to the heterogeneous,\n    high-dimensional nature and possible missing modalities of some individuals. \n    We introduce this package for late integrative prediction modeling, enabling \n    modality-specific variable selection and prediction modeling, followed by the \n    aggregation of the modality-specific predictions to train a final meta-model. \n    This package facilitates conducting late integration predictive modeling in a \n    systematic, structured, and reproducible way.",
    "version": "0.0.2",
    "maintainer": "Cesaire J. K. Fouodo <cesaire.kuetefouodo@uni-luebeck.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13607,
    "package_name": "fusionclust",
    "title": "Clustering and Feature Screening using L1 Fusion Penalty",
    "description": "Provides the Big Merge Tracker and COSCI algorithms for convex clustering and \n    feature screening using L1 fusion penalty. See Radchenko, P. and Mukherjee, G. (2017) <doi:10.1111/rssb.12226> and \n    T.Banerjee et al. (2017) <doi:10.1016/j.jmva.2017.08.001> for more details.",
    "version": "1.0.0",
    "maintainer": "Trambak Banerjee <trambakb@usc.edu>",
    "url": "https://github.com/trambakbanerjee/fusionclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13621,
    "package_name": "fuzzyforest",
    "title": "Fuzzy Forests",
    "description": "Fuzzy forests, a new algorithm based on random forests,\n    is designed to reduce the bias seen in random forest feature selection\n    caused by the presence of correlated features.  Fuzzy forests uses\n    recursive feature elimination random forests to select\n    features from separate blocks of correlated features where the\n    correlation within each block of features is high\n    and the correlation between blocks of features is low.\n    One final random forest is fit using the surviving features.\n    This package fits random forests using the 'randomForest' package and\n    allows for easy use of 'WGCNA' to split features into distinct blocks.\n    See D. Conn, Ngun, T., C. Ramirez, and G. Li (2019) <doi:10.18637/jss.v091.i09>\n    for further details.",
    "version": "1.0.8",
    "maintainer": "Daniel Conn <djconn17@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13648,
    "package_name": "gKRLS",
    "title": "Generalized Kernel Regularized Least Squares",
    "description": "Kernel regularized least squares, also known as kernel ridge regression, \n    is a flexible machine learning method. This package implements this method by \n    providing a smooth term for use with 'mgcv' and uses random sketching to \n    facilitate scalable estimation on large datasets. It provides additional \n    functions for calculating marginal effects after estimation and for use with \n    ensembles ('SuperLearning'), double/debiased machine learning ('DoubleML'), \n    and robust/clustered standard errors ('sandwich'). Chang and Goplerud (2024)\n    <doi:10.1017/pan.2023.27> provide further details.",
    "version": "1.0.4",
    "maintainer": "Max Goplerud <mgoplerud@austin.utexas.edu>",
    "url": "https://github.com/mgoplerud/gKRLS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13676,
    "package_name": "gainML",
    "title": "Machine Learning-Based Analysis of Potential Power Gain from\nPassive Device Installation on Wind Turbine Generators",
    "description": "Provides an effective machine learning-based tool that quantifies the gain of passive device installation on wind turbine generators.\n  H. Hwangbo, Y. Ding, and D. Cabezon (2019) <arXiv:1906.05776>.",
    "version": "0.1.0",
    "maintainer": "Hoon Hwangbo <hhwangb1@utk.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13693,
    "package_name": "gamclass",
    "title": "Functions and Data for a Course on Modern Regression and\nClassification",
    "description": "Functions and data are provided that support a course\n        that emphasizes statistical issues of inference and generalizability.  \n        The functions are designed to make it straightforward to illustrate\n        the use of cross-validation, the training/test approach, simulation, \n        and model-based estimates of accuracy.  Methods considered are\n        Generalized Additive Modeling, Linear and Quadratic Discriminant\n        Analysis, Tree-based methods, and Random Forests. ",
    "version": "0.62.7",
    "maintainer": "John Maindonald <john@statsresearch.co.nz>",
    "url": "https://github.com/jhmaindonald/gamclass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13698,
    "package_name": "gamlss.add",
    "title": "Extra Additive Terms for Generalized Additive Models for\nLocation Scale and Shape",
    "description": "Interface for extra smooth functions including tensor products, \n             neural networks and decision trees.",
    "version": "5.1-14",
    "maintainer": "Mikis Stasinopoulos <d.stasinopoulos@gre.ac.uk>",
    "url": "https://www.gamlss.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13719,
    "package_name": "ganDataModel",
    "title": "Build a Metric Subspaces Data Model for a Data Source",
    "description": "Neural networks are applied to create a density value function which approximates density values for a data source. The trained neural network is analyzed for different levels. For each level metric subspaces with density values above a level are determined. The obtained set of metric subspaces and the trained neural network are assembled into a data model. A prerequisite is the definition of a data source, the generation of generative data and the calculation of density values. These tasks are executed using package 'ganGenerativeData' <https://cran.r-project.org/package=ganGenerativeData>.",
    "version": "2.0.1",
    "maintainer": "Werner Mueller <werner.mueller5@chello.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13720,
    "package_name": "ganGenerativeData",
    "title": "Generate Generative Data for a Data Source",
    "description": "Generative Adversarial Networks are applied to generate generative data for a data source. A generative model consisting of a generator and a discriminator network is trained. During iterative training the distribution of generated data is converging to that of the data source. Direct applications of generative data are the created functions for data evaluation, missing data completion and data classification. A software service for accelerated training of generative models on graphics processing units is available. Reference: Goodfellow et al. (2014) <doi:10.48550/arXiv.1406.2661>.",
    "version": "2.1.6",
    "maintainer": "Werner Mueller <werner.mueller5@chello.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13765,
    "package_name": "gbts",
    "title": "Hyperparameter Search for Gradient Boosted Trees",
    "description": "An implementation of hyperparameter optimization for Gradient\n    Boosted Trees on binary classification and regression problems. The current\n    version provides two optimization methods: Bayesian optimization and random\n    search. Instead of giving the single best model, the final output is an \n    ensemble of Gradient Boosted Trees constructed via the method of ensemble \n    selection.",
    "version": "1.2.0",
    "maintainer": "Waley W. J. Liang <wliang10@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13778,
    "package_name": "gclus",
    "title": "Clustering Graphics",
    "description": "Orders panels in scatterplot matrices and parallel coordinate\n displays by some merit index. Package contains various indices of merit,\n ordering functions, and enhanced versions of pairs and parcoord which\n color panels according to their merit level.",
    "version": "1.3.3",
    "maintainer": "Catherine Hurley <catherine.hurley@mu.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13812,
    "package_name": "geeasy",
    "title": "Solve Generalized Estimating Equations for Clustered Data",
    "description": "Estimation of generalized linear models with\n    correlated/clustered observations by use of generalized estimating\n    equations (GEE). See e.g. Halekoh and Højsgaard, (2005,\n    <doi:10.18637/jss.v015.i02>), for details. Several types of\n    clustering are supported, including exchangeable variance\n    structures, AR1 structures, M-dependent, user-specified variance\n    structures and more. The model fitting computations are performed\n    using modified code from the 'geeM' package, while the interface\n    and output objects have been written to resemble the 'geepack'\n    package. The package also contains additional tools for working\n    with and inspecting results from the 'geepack' package, e.g. a\n    'confint' method for 'geeglm' objects from 'geepack'.",
    "version": "0.1.3",
    "maintainer": "Søren Højsgaard <sorenh@math.aau.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13875,
    "package_name": "genetic.algo.optimizeR",
    "title": "Genetic Algorithm Optimization",
    "description": "Genetic algorithm are a class of optimization\n    algorithms inspired by the process of natural selection and genetics.\n    This package is for learning purposes and allows users to optimize\n    various functions or parameters by mimicking biological evolution\n    processes such as selection, crossover, and mutation. Ideal for tasks\n    like machine learning parameter tuning, mathematical function\n    optimization, and solving an optimization problem that involves finding \n    the best solution in a discrete space.",
    "version": "0.3.3",
    "maintainer": "Dany Mukesha <danymukesha@gmail.com>",
    "url": "https://danymukesha.github.io/genetic.algo.optimizeR/,\nhttps://github.com/danymukesha/genetic.algo.optimizeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14083,
    "package_name": "ggcorrheatmap",
    "title": "Make Flexible 'ggplot2' Correlation Heatmaps",
    "description": "Create correlation heatmaps with 'ggplot2' and customise them\n    with flexible annotation and clustering. Symmetric heatmaps can use\n    triangular or mixed layouts, removing redundant information or displaying\n    complementary information in the two halves. There is also support for\n    general heatmaps not displaying correlations.",
    "version": "0.3.0",
    "maintainer": "Leo Dahl <leokosdah@gmail.com>",
    "url": "https://github.com/leod123/ggcorrheatmap,\nhttps://leod123.github.io/ggcorrheatmap/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14120,
    "package_name": "ggfortify",
    "title": "Data Visualization Tools for Statistical Analysis Results",
    "description": "Unified plotting tools for statistics commonly used, such as GLM,\n    time series, PCA families, clustering and survival analysis. The package offers\n    a single plotting interface for these analysis results and plots in a unified\n    style using 'ggplot2'.",
    "version": "0.4.19",
    "maintainer": "Yuan Tang <terrytangyuan@gmail.com>",
    "url": "https://github.com/sinhrks/ggfortify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14377,
    "package_name": "glmertree",
    "title": "Generalized Linear Mixed Model Trees",
    "description": "Recursive partitioning based on (generalized) linear mixed models\n    (GLMMs) combining lmer()/glmer() from 'lme4' and lmtree()/glmtree() from \n    'partykit'. The fitting algorithm is described in more detail in Fokkema,\n    Smits, Zeileis, Hothorn & Kelderman (2018; <DOI:10.3758/s13428-017-0971-x>).\n    For detecting and modeling subgroups in growth curves with GLMM trees see\n    Fokkema & Zeileis (2024; <DOI:10.3758/s13428-024-02389-1>).",
    "version": "0.2-6",
    "maintainer": "Marjolein Fokkema <M.Fokkema@fsw.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14384,
    "package_name": "glmmML",
    "title": "Generalized Linear Models with Clustering",
    "description": "Binomial and Poisson regression for clustered data, fixed\n        and random effects with bootstrapping.",
    "version": "1.1.7",
    "maintainer": "Göran Broström <goran.brostrom@umu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14396,
    "package_name": "glmnetr",
    "title": "Nested Cross Validation for the Relaxed Lasso and Other Machine\nLearning Models",
    "description": "\n    Cross validation informed Relaxed LASSO (or more generally elastic net), gradient boosting machine ('xgboost'), Random Forest ('RandomForestSRC'), Oblique Random Forest ('aorsf'), Artificial Neural Network (ANN), Recursive Partitioning ('RPART') or step wise regression models are fit.  Cross validation leave out samples (leading to nested cross validation) or bootstrap out-of-bag samples are used to evaluate and compare performances between these models with results presented in tabular or graphical means.  Calibration plots can also be generated, again based upon (outer nested) cross validation or bootstrap leave out (out of bag) samples.\n    Note, at the time of this writing, in order to fit gradient boosting machine models one must install the packages 'DiceKriging' and 'rgenoud' using the install.packages() function. \n    For some datasets, for example when the design matrix is not of full rank, 'glmnet' may have very long run times when fitting the relaxed lasso model, from our experience when fitting Cox models on data with many predictors and many patients, making it difficult to get solutions from either glmnet() or cv.glmnet().  This may be remedied by using the 'path=TRUE' option when calling glmnet() and cv.glmnet().  Within the 'glmnetr' package the approach of path=TRUE is taken by default. \n    other packages doing similar include 'nestedcv' <https://cran.r-project.org/package=nestedcv>, 'glmnetSE' <https://cran.r-project.org/package=glmnetSE> which may provide different functionality when performing a nested CV. \n    Use of the 'glmnetr' has many similarities to the 'glmnet' package and it could be helpful for the user of 'glmnetr' also become familiar with the 'glmnet' package <https://cran.r-project.org/package=glmnet>, with the \"An Introduction to 'glmnet'\" and \"The Relaxed Lasso\" being especially useful in this regard. ",
    "version": "0.6-3",
    "maintainer": "Walter K Kremers <kremers.walter@mayo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14444,
    "package_name": "gmfd",
    "title": "Inference and Clustering of Functional Data",
    "description": "Some methods for the inference and clustering of univariate and \n             multivariate functional data, using a generalization of Mahalanobis\n             distance, along with some functions useful for the analysis of functional data.\n             For further details, see Martino A., Ghiglietti, A., Ieva, F. and Paganoni A. M. (2017) <arXiv:1708.00386>.",
    "version": "1.0.1",
    "maintainer": "Andrea Martino <andrea.martino@polimi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14466,
    "package_name": "gnn",
    "title": "Generative Neural Networks",
    "description": "Tools to set up, train, store, load, investigate and analyze\n             generative neural networks. In particular, functionality for\n\t     generative moment matching networks is provided.",
    "version": "0.0-5",
    "maintainer": "Marius Hofert <mhofert@hku.hk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14575,
    "package_name": "grafzahl",
    "title": "Supervised Machine Learning for Textual Data Using Transformers\nand 'Quanteda'",
    "description": "Duct tape the 'quanteda' ecosystem (Benoit et al., 2018) <doi:10.21105/joss.00774> to modern Transformer-based text classification models (Wolf et al., 2020) <doi:10.18653/v1/2020.emnlp-demos.6>, in order to facilitate supervised machine learning for textual data. This package mimics the behaviors of 'quanteda.textmodels' and provides a function to setup the 'Python' environment to use the pretrained models from 'Hugging Face' <https://huggingface.co/>. More information: <doi:10.5117/CCR2023.1.003.CHAN>.",
    "version": "0.0.12",
    "maintainer": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "url": "https://gesistsa.github.io/grafzahl/,\nhttps://github.com/gesistsa/grafzahl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14592,
    "package_name": "graphclust",
    "title": "Hierarchical Graph Clustering for a Collection of Networks",
    "description": "Graph clustering using an agglomerative algorithm to maximize the\n  integrated classification likelihood criterion and a mixture of stochastic\n  block models. The method is described in the article \"Model-based clustering \n  of multiple networks with a hierarchical algorithm\" by \n  T. Rebafka (2022) <arXiv:2211.02314>.",
    "version": "1.3",
    "maintainer": "Tabea Rebafka <tabea.rebafka@sorbonne-universite.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14620,
    "package_name": "greed",
    "title": "Clustering and Model Selection with the Integrated\nClassification Likelihood",
    "description": "An ensemble of algorithms that enable the clustering of networks and data matrices (such as counts, categorical or continuous) with different type of generative models. Model selection and clustering is performed in combination by optimizing the Integrated Classification Likelihood (which is equivalent to minimizing the description length). Several models are available such as: Stochastic Block Model, degree corrected Stochastic Block Model, Mixtures of Multinomial, Latent Block Model. The optimization is performed thanks to a combination of greedy local search and a genetic algorithm (see <arXiv:2002:11577> for more details).",
    "version": "0.6.2",
    "maintainer": "Etienne Côme <etienne.come@univ-eiffel.fr>",
    "url": "https://comeetie.github.io/greed/,\nhttps://github.com/comeetie/greed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14654,
    "package_name": "grnn",
    "title": "General regression neural network",
    "description": "The program GRNN implements the algorithm proposed by\n        Specht (1991).",
    "version": "0.1.0",
    "maintainer": "Pierre-Olivier Chasset <pierre-olivier@chasset.net>",
    "url": "http://flow.chasset.net/r-grnn/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14685,
    "package_name": "grplasso",
    "title": "Fitting User-Specified Models with Group Lasso Penalty",
    "description": "Fits user-specified (GLM-) models with group lasso penalty.",
    "version": "0.4-7",
    "maintainer": "Lukas Meier <meier@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14687,
    "package_name": "grpreg",
    "title": "Regularization Paths for Regression Models with Grouped\nCovariates",
    "description": "Efficient algorithms for fitting the regularization path of linear\n  regression, GLM, and Cox regression models with grouped penalties.  This\n  includes group selection methods such as group lasso, group MCP, and\n  group SCAD as well as bi-level selection methods such as the group\n  exponential lasso, the composite MCP, and the group bridge.  For more\n  information, see Breheny and Huang (2009) <doi:10.4310/sii.2009.v2.n3.a10>,\n  Huang, Breheny, and Ma (2012) <doi:10.1214/12-sts392>, Breheny and Huang\n  (2015) <doi:10.1007/s11222-013-9424-2>, and Breheny (2015)\n  <doi:10.1111/biom.12300>, or visit the package homepage\n  <https://pbreheny.github.io/grpreg/>.",
    "version": "3.5.0",
    "maintainer": "Patrick Breheny <patrick-breheny@uiowa.edu>",
    "url": "https://pbreheny.github.io/grpreg/,\nhttps://github.com/pbreheny/grpreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14783,
    "package_name": "h2o",
    "title": "R Interface for the 'H2O' Scalable Machine Learning Platform",
    "description": "R interface for 'H2O', the scalable open source machine learning\n    platform that offers parallelized implementations of many supervised and\n    unsupervised machine learning algorithms such as Generalized Linear\n    Models (GLM), Gradient Boosting Machines (including XGBoost), Random Forests,\n    Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes,\n    Generalized Additive Models (GAM), ANOVA GLM, Cox Proportional Hazards, K-Means, PCA, ModelSelection,\n    Word2Vec, as well as a fully automatic machine learning algorithm (H2O AutoML).",
    "version": "3.44.0.3",
    "maintainer": "Tomas Fryda <tomas.fryda@h2o.ai>",
    "url": "https://github.com/h2oai/h2o-3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14784,
    "package_name": "h2o4gpu",
    "title": "Interface to 'H2O4GPU'",
    "description": "Interface to 'H2O4GPU' <https://github.com/h2oai/h2o4gpu>, a collection of 'GPU' solvers for machine learning algorithms.",
    "version": "0.3.3",
    "maintainer": "Navdeep Gill <navdeep@h2o.ai>",
    "url": "https://github.com/h2oai/h2o4gpu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14785,
    "package_name": "h2otools",
    "title": "Machine Learning Model Evaluation for 'h2o' Package",
    "description": "\n  Enhances the H2O platform by providing tools for detailed evaluation of machine learning models. It includes functions for bootstrapped performance evaluation, extended F-score calculations, and various other metrics, aimed at improving model assessment. ",
    "version": "0.4",
    "maintainer": "E. F. Haghish <haghish@hotmail.com>",
    "url": "https://github.com/haghish/h2otools,\nhttps://www.sv.uio.no/psi/english/people/academic/haghish/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14812,
    "package_name": "hamlet",
    "title": "Hierarchical Optimal Matching and Machine Learning Toolbox",
    "description": "Various functions and algorithms are provided here for solving optimal matching \n  tasks in the context of preclinical cancer studies. Further, various helper and plotting \n  functions are provided for unsupervised and supervised machine learning as well as longitudinal \n  mixed-effects modeling of tumor growth response patterns.",
    "version": "0.9.8",
    "maintainer": "Teemu Daniel Laajala <teelaa@utu.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14817,
    "package_name": "handwriterRF",
    "title": "Handwriting Analysis with Random Forests",
    "description": "Perform forensic handwriting analysis of two scanned handwritten documents. This package implements the statistical method described by Madeline Johnson and Danica Ommen (2021) <doi:10.1002/sam.11566>. Similarity measures and a random forest produce a score-based likelihood ratio that quantifies the strength of the evidence in favor of the documents being written by the same writer or different writers.",
    "version": "1.1.1",
    "maintainer": "Stephanie Reinders <reinders.stephanie@gmail.com>",
    "url": "https://github.com/CSAFE-ISU/handwriterRF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14819,
    "package_name": "hann",
    "title": "Hopfield Artificial Neural Networks",
    "description": "Builds and optimizes Hopfield artificial neural networks (Hopfield, 1982, <doi:10.1073/pnas.79.8.2554>). One-layer and three-layer models are implemented. The energy of the Hopfield network is minimized with formula from Krotov and Hopfield (2016, <doi:10.48550/ARXIV.1606.01164>). Optimization (supervised learning) is done through a gradient-based method. Classification is done with S3 methods predict(). Parallelization with 'OpenMP' is used if available during compilation.",
    "version": "1.1",
    "maintainer": "Emmanuel Paradis <Emmanuel.Paradis@ird.fr>",
    "url": "https://github.com/emmanuelparadis/hann",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14825,
    "package_name": "haplotyper",
    "title": "Tool for Clustering Genotypes in Haplotypes",
    "description": "Function to  identify  haplotypes\n  within QTL (Quantitative Trait Loci). One haplotype is a combination of SNP\n  (Single Nucleotide Polymorphisms) within the QTL. This function groups\n  together all individuals of a population with the same haplotype.\n  Each group contains individual with the same allele in each SNP,\n  whether or not missing data. Thus, haplotyper groups individuals,\n  that to be imputed, have a non-zero probability of having the same alleles\n  in the entire sequence of SNP's. Moreover, haplotyper calculates such\n  probability from relative frequencies.",
    "version": "0.1",
    "maintainer": "Gaston Quero <gastonquero@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14832,
    "package_name": "harmonizer",
    "title": "Harmonizing CN8 and PC8 Product Codes",
    "description": "Several functions are provided to harmonize CN8 (Combined Nomenclature \n\t8 digits) and PC8 (Production Communautaire 8 digits) product codes over \n\ttime and the classification systems HS6 and BEC. Harmonization of CN8 \n\tcodes are possible by default from 1995 to 2022 and of PC8 from 2001 to \n\t2021, respectively. ",
    "version": "0.3.2",
    "maintainer": "Christoph Baumgartner <Christoph.Baumgartner@uibk.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14863,
    "package_name": "hclust1d",
    "title": "Hierarchical Clustering of Univariate (1d) Data",
    "description": "Univariate agglomerative hierarchical clustering with a comprehensive list of choices of a linkage function in O(n*log n) time. The better algorithmic time complexity is paired with an efficient 'C++' implementation.",
    "version": "0.1.1",
    "maintainer": "Szymon Nowakowski <s.nowakowski@mimuw.edu.pl>",
    "url": "https://github.com/SzymonNowakowski/hclust1d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14864,
    "package_name": "hclustTeach",
    "title": "Hierarchical Cluster Analysis (Learning Didactically)",
    "description": "Implements hierarchical clustering methods (single linkage, complete linkage, average linkage, and centroid linkage) \n    with stepwise printing and dendrograms for didactic purposes.",
    "version": "0.1.0",
    "maintainer": "Gualberto Segundo Agamez Montalvo <gsagamez@dema.ufc.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14865,
    "package_name": "hclusteasy",
    "title": "Determining Hierarchical Clustering Easily",
    "description": "Facilitates hierarchical clustering analysis with functions\n    to read data in 'txt', 'xlsx', and 'xls' formats, apply normalization\n    techniques to the dataset, perform hierarchical clustering and\n    construct scatter plot from principal component analysis to evaluate\n    the groups obtained.",
    "version": "0.1.0",
    "maintainer": "Henrique Andrade <henrique.4ndrade@outlook.com>",
    "url": "https://github.com/tsukubai/hclusteasy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14868,
    "package_name": "hdImpute",
    "title": "A Batch Process for High Dimensional Imputation",
    "description": "A correlation-based batch process for fast, accurate imputation for \n    high dimensional missing data problems via chained random forests.\n    See Waggoner (2023) <doi:10.1007/s00180-023-01325-9> for more on 'hdImpute',\n    Stekhoven and Bühlmann (2012) <doi:10.1093/bioinformatics/btr597> for more on 'missForest', \n    and Mayer (2022) <https://github.com/mayer79/missRanger> for more on 'missRanger'.",
    "version": "0.2.1",
    "maintainer": "Philip Waggoner <philip.waggoner@gmail.com>",
    "url": "https://github.com/pdwaggoner/hdImpute",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14870,
    "package_name": "hda",
    "title": "Heteroscedastic Discriminant Analysis",
    "description": "Functions to perform dimensionality reduction for classification if the covariance matrices of the classes are unequal. ",
    "version": "0.2-14",
    "maintainer": "Gero Szepannek <gero.szepannek@web.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14886,
    "package_name": "hdi",
    "title": "High-Dimensional Inference",
    "description": "Implementation of multiple approaches to perform inference in high-dimensional models.",
    "version": "0.1-10",
    "maintainer": "Lukas Meier <meier@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14903,
    "package_name": "healthcareai",
    "title": "Tools for Healthcare Machine Learning",
    "description": "Aims to make machine learning in healthcare as easy as possible.",
    "version": "2.5.1",
    "maintainer": "",
    "url": "https://github.com/HealthCatalyst/healthcareai-r",
    "exports": [],
    "topics": ["healthcare", "machine-learning", "r"],
    "score": "NA",
    "stars": 253
  },
  {
    "id": 14920,
    "package_name": "heatmapFlex",
    "title": "Tools to Generate Flexible Heatmaps",
    "description": "A set of tools supporting more flexible heatmaps. The graphics is grid-like using\n  the old graphics system. The main function is heatmap.n2(), which is a wrapper around the various functions\n  constructing individual parts of the heatmap, like sidebars, picket plots, legends etc. The function supports zooming\n  and splitting, i.e., having (unlimited) small heatmaps underneath each other in one plot deriving from the same data set,\n  e.g., clustered and ordered by a supervised clustering method.",
    "version": "0.1.2",
    "maintainer": "Vidal Fey <vidal.fey@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14928,
    "package_name": "hedgedrf",
    "title": "An Implementation of the Hedged Random Forest Algorithm",
    "description": "This algorithm is described in detail in the paper \"Hedging Forecast Combinations With an Application to the Random Forest\" by Beck et al. (2024) <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5032102>. The package provides a function hedgedrf() that can be used to train a Hedged Random Forest model on a dataset, and a function predict.hedgedrf() that can be used to make predictions with the model.",
    "version": "1.0.1",
    "maintainer": "Elliot Beck <elliotleeroy.beck@uzh.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14994,
    "package_name": "hicream",
    "title": "HIC diffeREntial Analysis Method",
    "description": "Perform Hi-C data differential analysis based on pixel-level\n    differential analysis and a post hoc inference strategy to quantify\n    signal in clusters of pixels. Clusters of pixels are obtained through\n    a connectivity-constrained two-dimensional hierarchical clustering.",
    "version": "0.0.2",
    "maintainer": "Elise Jorge <elise.jorge@inrae.fr>",
    "url": "https://forge.inrae.fr/scales/hicream",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14998,
    "package_name": "hierBipartite",
    "title": "Bipartite Graph-Based Hierarchical Clustering",
    "description": "\n    Bipartite graph-based hierarchical clustering, developed for pharmacogenomic \n    datasets and datasets sharing the same data structure. The goal is to \n    construct a hierarchical clustering of groups of samples based on \n    association patterns between two sets of variables. In the context of\n    pharmacogenomic datasets, the samples are cell lines, and the two sets of \n    variables are typically expression levels and drug sensitivity values. \n    For this method, sparse canonical correlation analysis from \n    Lee, W., Lee, D., Lee, Y. and Pawitan, Y. (2011) <doi:10.2202/1544-6115.1638> \n    is first applied to extract association patterns for each group of samples. \n    Then, a nuclear norm-based dissimilarity measure is used to construct a \n    dissimilarity matrix between groups based on the extracted associations. \n    Finally, hierarchical clustering is applied.",
    "version": "0.0.2",
    "maintainer": "Calvin Chi <calvin.chi@berkeley.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15002,
    "package_name": "hierarchicalSets",
    "title": "Set Data Visualization Using Hierarchies",
    "description": "Pure set data visualization approaches are often limited in\n    scalability due to the combinatorial explosion of distinct set families as\n    the number of sets under investigation increases. hierarchicalSets applies\n    a set centric hierarchical clustering of the sets under investigation and\n    uses this hierarchy as a basis for a range of scalable visual\n    representations. hierarchicalSets is especially well suited for collections\n    of sets that describe comparable comparable entities as it relies on the\n    sets to have a meaningful relational structure.",
    "version": "1.0.4",
    "maintainer": "Thomas Lin Pedersen <thomasp85@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15053,
    "package_name": "hmcdm",
    "title": "Hidden Markov Cognitive Diagnosis Models for Learning",
    "description": "Fitting hidden Markov models of learning under the cognitive diagnosis framework.\n  The estimation of the hidden Markov diagnostic classification model,\n  the first order hidden Markov model, the reduced-reparameterized unified learning model,\n  and the joint learning model for responses and response times.",
    "version": "2.1.2",
    "maintainer": "Sunbeom Kwon <sunbeom2@illinois.edu>",
    "url": "https://github.com/tmsalab/hmcdm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15089,
    "package_name": "hopkins",
    "title": "Calculate Hopkins Statistic for Clustering",
    "description": "Calculate Hopkins statistic to assess the clusterability of data. See Wright (2023) <doi:10.32614/RJ-2022-055>.",
    "version": "1.1",
    "maintainer": "Kevin Wright <kw.stat@gmail.com>",
    "url": "https://kwstat.github.io/hopkins/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15108,
    "package_name": "hpiR",
    "title": "House Price Indexes",
    "description": "Compute house price indexes and series using a variety of different methods and\n    models common through the real estate literature.  Evaluate index 'goodness' based\n    on accuracy, volatility and revision statistics. Background on basic model construction\n    for repeat sales models can be found at: Case and Quigley (1991) \n    <https://ideas.repec.org/a/tpr/restat/v73y1991i1p50-58.html> and for hedonic pricing models at: \n    Bourassa et al (2006) <doi:10.1016/j.jhe.2006.03.001>. The package author's working paper on the \n    random forest approach to house price indexes can be found at: <http://www.github.com/andykrause/hpi_research>.",
    "version": "0.3.2",
    "maintainer": "Andy Krause <andyxkrause@gmail.com>",
    "url": "https://www.github.com/andykrause/hpiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15127,
    "package_name": "htetree",
    "title": "Causal Inference with Tree-Based Machine Learning Algorithms",
    "description": "Estimating heterogeneous treatment effects with tree-based machine\n    learning algorithms and visualizing estimated results in flexible and \n    presentation-ready ways. For more information, see Brand, Xu, Koch, \n    and Geraldo (2021) <doi:10.1177/0081175021993503>. Our current package \n    first started as a fork of the 'causalTree' package on 'GitHub' and we \n    greatly appreciate the authors for their extremely useful and free package.",
    "version": "0.1.20",
    "maintainer": "Jiahui Xu <jiahuixu@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15155,
    "package_name": "hues",
    "title": "Distinct Colour Palettes Based on 'iwanthue'",
    "description": "Creating effective colour palettes for figures is \n    challenging. This package generates and plot palettes of optimally \n    distinct colours in perceptually uniform colour space, based on \n    'iwanthue' <http://tools.medialab.sciences-po.fr/iwanthue/>. \n    This is done through k-means clustering of CIE Lab colour space, \n    according to user-selected constraints on hue, chroma, and \n    lightness.",
    "version": "0.2.0",
    "maintainer": "John Baumgartner <johnbaums@gmail.com>",
    "url": "https://github.com/johnbaums/hues",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15182,
    "package_name": "hybridts",
    "title": "Hybrid Time Series Forecasting Using Error Remodeling Approach",
    "description": "Method and tool for generating hybrid time series forecasts using\n        an error remodeling approach. These forecasting approaches utilize a recursive \n        technique for modeling the linearity of the series using a linear method \n        (e.g., ARIMA, Theta, etc.) and then models (forecasts) the residuals of the linear forecaster\n        using non-linear neural networks (e.g., ANN, ARNN, etc.). The hybrid architectures comprise three steps: \n        firstly, the linear patterns of the series are forecasted which are followed by an error re-modeling step, \n        and finally, the forecasts from both the steps are combined to produce the final output. This method additionally \n        provides the confidence intervals as needed. Ten different models can be implemented using this package.\n        This package generates different types of hybrid error correction models for time series forecasting \n        based on the algorithms by Zhang. (2003), Chakraborty et al. (2019), Chakraborty et al. (2020), \n        Bhattacharyya et al. (2021), Chakraborty et al. (2022), and Bhattacharyya et al. (2022)\n        <doi:10.1016/S0925-2312(01)00702-0> <doi:10.1016/j.physa.2019.121266> \n        <doi:10.1016/j.chaos.2020.109850> <doi:10.1109/IJCNN52387.2021.9533747> \n        <doi:10.1007/978-3-030-72834-2_29> <doi:10.1007/s11071-021-07099-3>.",
    "version": "0.1.0",
    "maintainer": "Tanujit Chakraborty <tanujitisi@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15211,
    "package_name": "hypergate",
    "title": "Machine Learning of Hyperrectangular Gating Strategies for\nHigh-Dimensional Cytometry",
    "description": "Given a high-dimensional dataset that typically represents a cytometry dataset, and a subset of the datapoints, this algorithm outputs an hyperrectangle so that datapoints within the hyperrectangle best correspond to the specified subset. In essence, this allows the conversion of clustering algorithms' outputs to gating strategies outputs.",
    "version": "0.8.5",
    "maintainer": "Etienne Becht <etienne.becht@protonmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15217,
    "package_name": "hypoRF",
    "title": "Random Forest Two-Sample Tests",
    "description": "An implementation of Random Forest-based two-sample tests as introduced in Hediger & Michel & Naef (2022). ",
    "version": "1.0.1",
    "maintainer": "Simon Hediger <simon.hediger@uzh.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15237,
    "package_name": "iBreakDown",
    "title": "Model Agnostic Instance Level Variable Attributions",
    "description": "Model agnostic tool for decomposition of predictions from black boxes.\n    Supports additive attributions and attributions with interactions.\n    The Break Down Table shows contributions of every variable to a final prediction. \n    The Break Down Plot presents variable contributions in a concise graphical way. \n    This package works for classification and regression models. \n    It is an extension of the 'breakDown' package (Staniak and Biecek 2018) <doi:10.32614/RJ-2018-072>,\n    with new and faster strategies for orderings. \n    It supports interactions in explanations and has interactive visuals (implemented with 'D3.js' library). \n    The methodology behind is described in the 'iBreakDown' article (Gosiewska and Biecek 2019) <arXiv:1903.11420>\n    This package is a part of the 'DrWhy.AI' universe (Biecek 2018) <arXiv:1806.08915>.",
    "version": "2.1.2",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "url": "https://ModelOriented.github.io/iBreakDown/,\nhttps://github.com/ModelOriented/iBreakDown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15250,
    "package_name": "iClusterVB",
    "title": "Fast Integrative Clustering and Feature Selection for High\nDimensional Data",
    "description": "A variational Bayesian approach for fast integrative\n    clustering and feature selection, facilitating the analysis of\n    multi-view, mixed type, high-dimensional datasets with applications in\n    fields like cancer research, genomics, and more.",
    "version": "0.1.4",
    "maintainer": "Abdalkarim Alnajjar <abdalkarim.alnajjar@queensu.ca>",
    "url": "https://github.com/AbdalkarimA/iClusterVB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15254,
    "package_name": "iForecast",
    "title": "Machine Learning Time Series Forecasting",
    "description": "Compute onestep and multistep time series forecasts for machine learning models.",
    "version": "1.1.2",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15301,
    "package_name": "iai",
    "title": "Interface to 'Interpretable AI' Modules",
    "description": "An interface to the algorithms of 'Interpretable AI'\n    <https://www.interpretable.ai> from the R programming language.\n    'Interpretable AI' provides various modules, including 'Optimal Trees' for\n    classification, regression, prescription and survival analysis, 'Optimal\n    Imputation' for missing data imputation and outlier detection, and 'Optimal\n    Feature Selection' for exact sparse regression. The 'iai' package is an\n    open-source project. The 'Interpretable AI' software modules are proprietary\n    products, but free academic and evaluation licenses are available.",
    "version": "1.10.2",
    "maintainer": "Jack Dunn <jack@interpretable.ai>",
    "url": "https://www.interpretable.ai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15421,
    "package_name": "ihclust",
    "title": "Iterative Hierarchical Clustering (IHC)",
    "description": "Provides a set of tools to\n  i) identify geographic areas with significant change over time in drug utilization, and \n  ii) characterize common change over time patterns among the time series for multiple geographic areas.\n  For reference, see below:\n    1. Song, J., Carey, M., Zhu, H., Miao, H., Ram´ırez, J. C., & Wu, H. (2018) <doi:10.1504/IJCBDD.2018.10011910>\n    2. Wu, S., Wu, H. (2013) <doi:10.1186/1471-2105-14-6>\n    3. Carey, M., Wu, S., Gan, G. & Wu, H. (2016) <doi:10.1016/j.idm.2016.07.001>.",
    "version": "0.1.0",
    "maintainer": "Elin Cho <elincho524@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15432,
    "package_name": "image.CornerDetectionF9",
    "title": "Find Corners in Digital Images with FAST-9",
    "description": "An implementation of the \"FAST-9\" corner detection algorithm explained in the paper 'FASTER and better: A machine learning approach to corner detection' by Rosten E., Porter R. and Drummond T. (2008), available at <doi:10.48550/arXiv.0810.2434>.\n    The package allows to detect corners in digital images.",
    "version": "0.1.1",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "https://github.com/bnosac/image",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15437,
    "package_name": "image.libfacedetection",
    "title": "Convolutional Neural Network for Face Detection",
    "description": "An open source library for face detection in images. \n    Provides a pretrained convolutional neural network based on <https://github.com/ShiqiYu/libfacedetection> which can be used to detect faces which have size greater than 10x10 pixels.",
    "version": "0.1.1",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "https://github.com/bnosac/image",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15442,
    "package_name": "imagefx",
    "title": "Extract Features from Images",
    "description": "Synthesize images into characteristic features for time-series analysis or machine learning applications.  The package was originally intended for monitoring volcanic eruptions in video data by highlighting and extracting regions above the vent associated with plume activity.  However, the functions within are general and have wide applications for image processing, analyzing, filtering, and plotting.       ",
    "version": "0.4.1",
    "maintainer": "Alex J.C. Witsil <alexjcwitsil@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15445,
    "package_name": "imageseg",
    "title": "Deep Learning Models for Image Segmentation",
    "description": "A general-purpose workflow for image segmentation using TensorFlow models based on the U-Net architecture by Ronneberger et al. (2015) <arXiv:1505.04597> and the U-Net++ architecture by Zhou et al. (2018) <arXiv:1807.10165>. We provide pre-trained models for assessing canopy density and understory vegetation density from vegetation photos. In addition, the package provides a workflow for easily creating model input and model architectures for general-purpose image segmentation based on grayscale or color images, both for binary and multi-class image segmentation.",
    "version": "0.5.0",
    "maintainer": "Juergen Niedballa <niedballa@izw-berlin.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15449,
    "package_name": "imanr",
    "title": "Identify the Racial Complex of Native Corns from Mexico",
    "description": "A model that provides researchers with a powerful tool for the classification\n    and study of native corn by aiding in the identification of racial complexes\n    which are fundamental to Mexico's agriculture and culture. This package has been\n    developed based on data collected by \"Proyecto Global de Maíces Nativos México\",\n    which has conducted exhaustive surveys across the country to document the\n    qualitative and quantitative characteristics of different types of native maize.\n    The trained model uses a robust and diverse dataset, enabling it to achieve an\n    80% accuracy in classifying maize racial complexes. The characteristics included\n    in the analysis comprise geographic location, grain and cob colors, as well as\n    various physical measurements, such as lengths and widths.",
    "version": "2.0.0",
    "maintainer": "Rafael Nieves-Alvarez <nievesalvarez1618@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15450,
    "package_name": "imbalanceDatRel",
    "title": "Relocated Data Oversampling for Imbalanced Data Classification",
    "description": "Relocates oversampled data from a specific oversampling method to\n    cover area determined by pure and proper class cover catch digraphs (PCCCD). \n    It prevents any data to be generated in class overlapping area.\n    For more details, see the corresponding publication: \n    F. Sağlam (2025) <doi:10.1007/s10994-025-06755-8>.",
    "version": "0.1.6",
    "maintainer": "Fatih Saglam <saglamf89@gmail.com>",
    "url": "https://doi.org/10.1007/s10994-025-06755-8",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15460,
    "package_name": "iml",
    "title": "Interpretable Machine Learning",
    "description": "Interpretability methods to analyze the behavior and\n    predictions of any machine learning model.  Implemented methods are:\n    Feature importance described by Fisher et al. (2018)\n    <doi:10.48550/arxiv.1801.01489>, accumulated local effects plots described by Apley\n    (2018) <doi:10.48550/arxiv.1612.08468>, partial dependence plots described by\n    Friedman (2001) <www.jstor.org/stable/2699986>, individual conditional\n    expectation ('ice') plots described by Goldstein et al.  (2013)\n    <doi:10.1080/10618600.2014.907095>, local models (variant of 'lime')\n    described by Ribeiro et. al (2016) <doi:10.48550/arXiv.1602.04938>, the Shapley\n    Value described by Strumbelj et. al (2014)\n    <doi:10.1007/s10115-013-0679-x>, feature interactions described by\n    Friedman et. al <doi:10.1214/07-AOAS148> and tree surrogate models.",
    "version": "0.11.4",
    "maintainer": "Giuseppe Casalicchio <giuseppe.casalicchio@lmu.de>",
    "url": "https://giuseppec.github.io/iml/,\nhttps://github.com/giuseppec/iml/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15464,
    "package_name": "immunaut",
    "title": "Machine Learning Immunogenicity and Vaccine Response Analysis",
    "description": "Used for analyzing immune responses and predicting vaccine efficacy using machine learning and advanced data processing techniques. 'Immunaut' integrates both unsupervised and supervised learning methods, managing outliers and capturing immune response variability. It performs multiple rounds of predictive model testing to identify robust immunogenicity signatures that can predict vaccine responsiveness. The platform is designed to handle high-dimensional immune data, enabling researchers to uncover immune predictors and refine personalized vaccination strategies across diverse populations.",
    "version": "1.0.2",
    "maintainer": "Ivan Tomic <info@ivantomic.com>",
    "url": "https://github.com/atomiclaboratory/immunaut,\n<https://atomic-lab.org>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15490,
    "package_name": "imputeMissings",
    "title": "Impute Missing Values in a Predictive Context",
    "description": "Compute missing values on a training data set and impute them on a new data set. Current available options are median/mode and random forest.",
    "version": "0.0.4",
    "maintainer": "Michel Ballings <michel.ballings@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15501,
    "package_name": "inTrees",
    "title": "Interpret Tree Ensembles",
    "description": "For tree ensembles such as random forests, regularized random forests and gradient boosted trees, this package provides functions for: extracting, measuring and pruning rules; selecting a compact rule set; summarizing rules into a learner; calculating frequent variable interactions; formatting rules in latex code.  Reference: Interpreting tree ensembles with inTrees (Houtao Deng, 2019, <doi:10.1007/s41060-018-0144-8>).",
    "version": "1.4",
    "maintainer": "Houtao Deng <softwaredeng@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15502,
    "package_name": "inaparc",
    "title": "Initialization Algorithms for Partitioning Cluster Analysis",
    "description": "Partitioning clustering algorithms divide data sets into k subsets or partitions so-called clusters. They require some initialization procedures for starting the algorithms. Initialization of cluster prototypes is one of such kind of procedures for most of the partitioning algorithms. Cluster prototypes are the centers of clusters, i.e. centroids or medoids, representing the clusters in a data set. In order to initialize cluster prototypes, the package 'inaparc' contains a set of the functions that are the implementations of several linear time-complexity and loglinear time-complexity methods in addition to some novel techniques. Initialization of fuzzy membership degrees matrices is another important task for starting the probabilistic and possibilistic partitioning algorithms. In order to initialize membership degrees matrices required by these algorithms, a number of functions based on some traditional and novel initialization techniques are also available in the package 'inaparc'.",
    "version": "1.2.1",
    "maintainer": "Zeynel Cebeci <zcebeci@cu.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15575,
    "package_name": "insect",
    "title": "Informatic Sequence Classification Trees",
    "description": "Provides tools for probabilistic taxon assignment with informatic sequence classification trees. See Wilkinson et al (2018) <doi:10.7287/peerj.preprints.26812v1>.",
    "version": "1.4.4",
    "maintainer": "Shaun Wilkinson <shaunpwilkinson@gmail.com>",
    "url": "https://github.com/shaunpwilkinson/insect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15588,
    "package_name": "insuranceData",
    "title": "A Collection of Insurance Datasets Useful in Risk Classification\nin Non-life Insurance",
    "description": "Insurance datasets, which are often used in claims severity and claims frequency modelling. It helps testing new regression models in those problems, such as GLM, GLMM, HGLM, non-linear mixed models etc. Most of the data sets are applied in the project \"Mixed models in ratemaking\" supported by grant NN 111461540 from Polish National Science Center.    ",
    "version": "1.0",
    "maintainer": "Alicja Wolny--Dominiak <alicja.wolny-dominiak@ue.katowice.pl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15691,
    "package_name": "ipft",
    "title": "Indoor Positioning Fingerprinting Toolset",
    "description": "Algorithms and utility functions for indoor positioning using fingerprinting techniques. \n    These functions are designed for manipulation of RSSI (Received Signal Strength Intensity) data \n    sets, estimation of positions,comparison of the performance of different models, and graphical \n    visualization of data. Machine learning algorithms and methods such as k-nearest neighbors or\n    probabilistic fingerprinting are implemented in this package to perform analysis\n    and estimations over RSSI data sets.",
    "version": "0.7.2",
    "maintainer": "Emilio Sansano <esansano@uji.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15702,
    "package_name": "ipsfs",
    "title": "Intuitionistic, Pythagorean, and Spherical Fuzzy Similarity\nMeasure",
    "description": "Advanced fuzzy logic based techniques are implemented to compute the similarity among different objects or items. Typically, application areas consist of transforming raw data into the corresponding advanced fuzzy logic representation and determining the similarity between two objects using advanced fuzzy similarity techniques in various fields of research, such as text classification, pattern recognition, software projects, decision-making, medical diagnosis, and market prediction. Functions are designed to compute the membership, non-membership, hesitant-membership, indeterminacy-membership, and refusal-membership for the input matrices. Furthermore, it also includes a large number of advanced fuzzy logic based similarity measure functions to compute the Intuitionistic fuzzy similarity (IFS), Pythagorean fuzzy similarity (PFS), and Spherical fuzzy similarity (SFS) between two objects or items based on their fuzzy relationships. It also includes working examples for each function with sample data sets.",
    "version": "1.0.0",
    "maintainer": "Rama Ranjan Panda<rrpanda.phd2018.cs@nitrr.ac.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15730,
    "package_name": "isa2",
    "title": "The Iterative Signature Algorithm",
    "description": "The ISA is a biclustering algorithm that finds modules \n  in an input matrix. A module or bicluster is a block of the\n  reordered input matrix.",
    "version": "0.3.6",
    "maintainer": "Gabor Csardi <csardi.gabor@gmail.com>",
    "url": "https://github.com/gaborcsardi/ISA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15735,
    "package_name": "isingLenzMC",
    "title": "Monte Carlo for Classical Ising Model",
    "description": "Classical Ising Model is a land mark system in statistical physics.The model explains the physics of spin glasses and magnetic materials, and cooperative phenomenon in general, for example phase transitions and neural networks.This package provides utilities to simulate one dimensional Ising Model with Metropolis and Glauber Monte Carlo with single flip dynamics in periodic boundary conditions. Utility functions for exact solutions are provided. Such as transfer matrix for 1D. Utility functions for exact solutions are provided. Example use cases are as follows: Measuring effective ergodicity and power-laws in so called functional-diffusion. ",
    "version": "0.2.8",
    "maintainer": "Mehmet Suzen <mehmet.suzen@physics.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15737,
    "package_name": "islasso",
    "title": "The Induced Smoothed Lasso",
    "description": "An implementation of the induced smoothing (IS) idea to lasso regularization models to allow estimation and inference on the model coefficients (currently hypothesis testing only). Linear, logistic, Poisson and gamma regressions with several link functions are implemented. The algorithm is described in the original paper; see <doi:10.1177/0962280219842890> and discussed in a tutorial <doi:10.13140/RG.2.2.16360.11521>.",
    "version": "1.6.2",
    "maintainer": "Gianluca Sottile <gianluca.sottile@unipa.it>",
    "url": "https://gianluca-sottile.github.io/islasso/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15748,
    "package_name": "isoboost",
    "title": "Isotonic Boosting Classification Rules",
    "description": "In classification problems a monotone relation between some\n  predictors and the classes may be assumed. In this package 'isoboost' \n  we propose new boosting algorithms, based on LogitBoost, that \n  incorporate this isotonicity information, yielding more accurate \n  and easily interpretable rules.",
    "version": "1.0.1",
    "maintainer": "David Conde <dconde@eio.uva.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15762,
    "package_name": "isopam",
    "title": "Clustering of Sites with Species Data",
    "description": "Clustering algorithm developed for use with plot inventories of species. It groups plots by subsets of diagnostic species rather than overall species composition. There is an unsupervised and a supervised mode, the latter accepting suggestions for species with greater weight and cluster medoids. ",
    "version": "3.3",
    "maintainer": "Sebastian Schmidtlein <schmidtlein@kit.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15813,
    "package_name": "iwaqr",
    "title": "Irrigation Water Quality Assessment and Visualizations",
    "description": "Calculates irrigation water quality ratios and has functions that could be used to plot several popular diagrams for irrigation water quality classification.",
    "version": "1.8.4",
    "maintainer": "Wajid Ali <wajeedaali@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15818,
    "package_name": "jScore",
    "title": "Calculates the j-Score Between Two Clustering Assignments",
    "description": "The jscore() function in the package calculates the J-Score metric between two clustering\n    assignments. The score is designed to address some problems with existing common metrics such \n    as problem of matching. The details of J-score is described in Ahmadinejad and Liu. (2021) <arXiv:2109.01306>.",
    "version": "0.1.0",
    "maintainer": "Navid Ahmadinejad <navidfr@asu.edu>",
    "url": "https://github.com/liliulab/jscore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15841,
    "package_name": "janus",
    "title": "Optimized Recommending System Based on 'tensorflow'",
    "description": "Proposes a coarse-to-fine optimization of a recommending system based on deep-neural networks using 'tensorflow'.",
    "version": "1.0.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/janus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15846,
    "package_name": "jcext",
    "title": "Extended Classification of Weather Types",
    "description": "Provides a gridded classification of weather types by applying the Jenkinson and Collison classification. For a given region (it can be either local region or the whole map),it computes at each grid the 11 weather \n             types during the period considered for the analysis. See Otero et al., (2017) <doi:10.1007/s00382-017-3705-y> for more information.",
    "version": "0.1.1",
    "maintainer": "Noelia Otero <noeli1680@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15873,
    "package_name": "jmotif",
    "title": "Time Series Analysis Toolkit Based on Symbolic Aggregate\nDiscretization, i.e. SAX",
    "description": "Implements time series z-normalization, SAX, HOT-SAX, VSM, SAX-VSM, RePair, and RRA\n    algorithms facilitating time series motif (i.e., recurrent pattern), discord (i.e., anomaly),\n    and characteristic pattern discovery along with interpretable time series classification.",
    "version": "1.2.1",
    "maintainer": "Pavel Senin <seninp@gmail.com>",
    "url": "https://github.com/jMotif/jmotif-R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15885,
    "package_name": "joinet",
    "title": "Penalised Multivariate Regression ('Multi-Target Learning')",
    "description": "Implements penalised multivariate regression (i.e., for multiple outcomes and many features) by stacked generalisation (<doi:10.1093/bioinformatics/btab576>). For positively correlated outcomes, a single multivariate regression is typically more predictive than multiple univariate regressions. Includes functions for model fitting, extracting coefficients, outcome prediction, and performance measurement. For optional comparisons, install 'remMap' from GitHub (<https://github.com/cran/remMap>).",
    "version": "1.0.0",
    "maintainer": "Armin Rauschenberger <armin.rauschenberger@uni.lu>",
    "url": "https://github.com/rauschenberger/joinet,\nhttps://rauschenberger.github.io/joinet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15959,
    "package_name": "kaggler",
    "title": "Kaggle API Client",
    "description": "An interface for interacting with the public API offered by",
    "version": "0.0.0.9000",
    "maintainer": "",
    "url": "https://github.com/mkearney/kaggler",
    "exports": [],
    "topics": ["api-client", "api-wrapper", "data", "data-api", "kaggle", "kaggle-api", "kaggle-competition", "machine-learning", "mkearney-dataset", "mkearney-r-package", "neural-networks", "r", "r-package", "rstats"],
    "score": "NA",
    "stars": 61
  },
  {
    "id": 15963,
    "package_name": "kamila",
    "title": "Methods for Clustering Mixed-Type Data",
    "description": "Implements methods for clustering mixed-type data,\n  specifically combinations of continuous and nominal data. Special attention\n  is paid to the often-overlooked problem of equitably balancing the\n  contribution of the continuous and categorical variables. This package\n  implements KAMILA clustering, a novel method for clustering\n  mixed-type data in the spirit of k-means clustering. It does not require\n  dummy coding of variables, and is efficient enough to scale to rather large\n  data sets. Also implemented is Modha-Spangler clustering, which uses a\n  brute-force strategy to maximize the cluster separation simultaneously in the\n  continuous and categorical variables. For more information, see Foss, Markatou,\n  Ray, & Heching (2016) <doi:10.1007/s10994-016-5575-7> and Foss & Markatou\n  (2018) <doi:10.18637/jss.v083.i13>.",
    "version": "0.1.2",
    "maintainer": "Alexander Foss <alexanderhfoss@gmail.com>",
    "url": "https://github.com/ahfoss/kamila",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15966,
    "package_name": "kanova",
    "title": "Quasi Analysis of Variance for K-Functions",
    "description": "One-way and two-way analysis of variance for replicated point\n\tpatterns, grouped by one or two classification factors, on the\n\tbasis of the corresponding K-functions.",
    "version": "0.3-20",
    "maintainer": "Rolf Turner <rolfturner@posteo.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15992,
    "package_name": "kdml",
    "title": "Kernel Distance Metric Learning for Mixed-Type Data",
    "description": "Distance metrics for mixed-type data consisting of continuous, nominal, and ordinal variables. This methodology uses additive and product kernels to calculate similarity functions and metrics, and selects variables relevant to the underlying distance through bandwidth selection via maximum similarity cross-validation. These methods can be used in any distance-based algorithm, such as distance-based clustering. For further details, we refer the reader to Ghashti and Thompson (2024) <doi:10.1007/s00357-024-09493-z> for dkps() methodology, and Ghashti (2024) <doi:10.14288/1.0443975> for dkss() methodology.",
    "version": "1.1.1",
    "maintainer": "John R. J. Thompson <john.thompson@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16007,
    "package_name": "keras",
    "title": "R Interface to 'Keras'",
    "description": "Interface to 'Keras' <https://keras.io>, a high-level neural\n  networks 'API'. 'Keras' was developed with a focus on enabling fast experimentation,\n  supports both convolution based networks and recurrent networks (as well as\n  combinations of the two), and runs seamlessly on both 'CPU' and 'GPU' devices.",
    "version": "2.16.0",
    "maintainer": "Tomasz Kalinowski <tomasz@posit.co>",
    "url": "https://tensorflow.rstudio.com/,\nhttps://github.com/rstudio/keras3/tree/r2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16013,
    "package_name": "kernelFactory",
    "title": "Kernel Factory: An Ensemble of Kernel Machines",
    "description": "Binary classification based on an ensemble of kernel machines (\"Ballings, M. and Van den Poel, D. (2013), Kernel Factory: An Ensemble of Kernel Machines. Expert Systems With Applications, 40(8), 2904-2913\"). Kernel factory is an ensemble method where each base classifier (random forest) is fit on the kernel matrix of a subset of the training data.",
    "version": "0.3.0",
    "maintainer": "Michel Ballings <Michel.Ballings@GMail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16018,
    "package_name": "kernlab",
    "title": "Kernel-Based Machine Learning Lab",
    "description": "Kernel-based machine learning methods for classification,\n        regression, clustering, novelty detection, quantile regression\n        and dimensionality reduction.  Among other methods 'kernlab'\n        includes Support Vector Machines, Spectral Clustering, Kernel\n        PCA, Gaussian Processes and a QP solver.",
    "version": "0.9-33",
    "maintainer": "Alexandros Karatzoglou <alexandros.karatzoglou@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16020,
    "package_name": "kernplus",
    "title": "A Kernel Regression-Based Multidimensional Wind Turbine Power\nCurve",
    "description": "Provides wind energy practitioners with an effective machine learning-based\n    tool that estimates a multivariate power curve and predicts the wind power output\n    for a specific environmental condition.",
    "version": "0.1.2",
    "maintainer": "Hoon Hwangbo <hhwangb1@utk.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16023,
    "package_name": "kerntools",
    "title": "Kernel Functions and Tools for Machine Learning Applications",
    "description": "Kernel functions for diverse types of data (including, but not\n    restricted to: nonnegative and real vectors, real matrices, categorical\n    and ordinal variables, sets, strings), plus other utilities like kernel\n    similarity, kernel Principal Components Analysis (PCA) and features'\n    importance for Support Vector Machines (SVMs), which expand other 'R'\n    packages like 'kernlab'.",
    "version": "1.2.0",
    "maintainer": "Elies Ramon <eramon@everlyrusher.com>",
    "url": "https://github.com/elies-ramon/kerntools,\nhttps://elies-ramon.github.io/kerntools/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16025,
    "package_name": "keyATM",
    "title": "Keyword Assisted Topic Models",
    "description": "Fits keyword assisted topic models (keyATM) using collapsed Gibbs samplers. The keyATM combines the latent dirichlet allocation (LDA) models with a small number of keywords selected by researchers in order to improve the interpretability and topic classification of the LDA. The keyATM can also incorporate covariates and directly model time trends. The keyATM is proposed in Eshima, Imai, and Sasaki (2024) <doi:10.1111/ajps.12779>.",
    "version": "0.5.4",
    "maintainer": "Shusei Eshima <shuseieshima@gmail.com>",
    "url": "https://keyatm.github.io/keyATM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16040,
    "package_name": "kgc",
    "title": "Koeppen-Geiger Climatic Zones",
    "description": "Aids in identifying the Koeppen-Geiger (KG) climatic zone for \n    a given location. The Koeppen-Geiger climate zones were first published in 1884, as a system\n    to classify regions of the earth by their relative heat and humidity through the year, for \n    the benefit of human health, plant and agriculture and other human activity [1]. This climate\n    zone classification system, applicable to all of the earths surface, has continued to be \n    developed by scientists up to the present day.  Recently one of use (FZ) has published updated,\n    higher accuracy KG climate zone definitions [2]. In this package we use these updated \n    high-resolution maps as the data source [3]. We provide functions that return the KG climate zone \n    for a given longitude and lattitude, or for a given United States zip code. In addition\n    the CZUncertainty() function will check climate zones nearby to check if the given location\n    is near a climate zone boundary. In addition an interactive shiny app is provided to define \n    the KG climate zone for a given longitude and lattitude, or United States zip code. \n    Digital data, as well as animated maps, showing the shift of the climate zones are provided \n    on the following website <http://koeppen-geiger.vu-wien.ac.at>.\n    This work was supported by the DOE-EERE SunShot award DE-EE-0007140.\n     [1] W. Koeppen, (2011) <doi:10.1127/0941-2948/2011/105>.\n     [2] F. Rubel and M. Kottek, (2010) <doi:10.1127/0941-2948/2010/0430>.\n     [3] F. Rubel, K. Brugger, K. Haslinger, and I. Auer, (2016) <doi:10.1127/metz/2016/0816>.",
    "version": "1.0.0.2",
    "maintainer": "Chelsey Bryant <clb117@case.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16068,
    "package_name": "kknn",
    "title": "Weighted k-Nearest Neighbors",
    "description": "Weighted k-Nearest Neighbors for Classification, Regression\n    and Clustering.",
    "version": "1.4.1",
    "maintainer": "Klaus Schliep <klaus.schliep@gmail.com>",
    "url": "https://github.com/KlausVigo/kknn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16070,
    "package_name": "klaR",
    "title": "Classification and Visualization",
    "description": "Miscellaneous functions for classification and visualization,\n     e.g. regularized discriminant analysis, sknn() kernel-density naive Bayes, \n     an interface to 'svmlight' and stepclass() wrapper variable selection \n     for supervised classification, partimat() visualization of classification rules \n         and shardsplot() of cluster results as well as kmodes() clustering for categorical data, \n     corclust() variable clustering, variable extraction from different variable clustering models \n         and weight of evidence preprocessing.",
    "version": "1.7-3",
    "maintainer": "Uwe Ligges <ligges@statistik.tu-dortmund.de>",
    "url": "https://statistik.tu-dortmund.de",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16071,
    "package_name": "klassR",
    "title": "Classifications for Statistics Norway",
    "description": "Functions to search, retrieve, apply and update classification \n  standards and code lists using Statistics Norway's API \n  <https://www.ssb.no/klass> from the system 'KLASS'. Retrieves classifications \n  by date with options to choose language, hierarchical level and formatting.",
    "version": "1.0.4",
    "maintainer": "Susie Jentoft <susie.jentoft@ssb.no>",
    "url": "https://statisticsnorway.github.io/ssb-klassr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16085,
    "package_name": "kmer",
    "title": "Fast K-Mer Counting and Clustering for Biological Sequence\nAnalysis",
    "description": "Contains tools for rapidly computing distance matrices \n    and clustering large sequence datasets using fast alignment-free \n    k-mer counting and recursive k-means partitioning. \n    See Vinga and Almeida (2003) <doi:10.1093/bioinformatics/btg005> \n    for a review of k-mer counting methods and applications for \n    biological sequence analysis.",
    "version": "1.1.2",
    "maintainer": "Shaun Wilkinson <shaunpwilkinson@gmail.com>",
    "url": "http://github.com/shaunpwilkinson/kmer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16114,
    "package_name": "kollaR",
    "title": "Event Classification, Visualization and Analysis of Eye Tracking\nData",
    "description": "Functions for analysing eye tracking data, including event detection, visualizations and area of interest (AOI) based analyses. \n  The package includes implementations of the IV-T, I-DT, adaptive velocity threshold, and Identification by two means clustering (I2MC) algorithms.\n  See separate documentation for each function. The principles underlying I-VT and I-DT algorithms are described in   Salvucci & Goldberg (2000,\\doi{10.1145/355017.355028}). \n  Two-means clustering is described in Hessels et al. (2017, \\doi{10.3758/s13428-016-0822-1}). \n  The adaptive velocity threshold algorithm is described in Nyström & Holmqvist (2010,\\doi{10.3758/BRM.42.1.188}).\n  See a demonstration in the URL.",
    "version": "1.1.2",
    "maintainer": "Johan Lundin Kleberg <johan.lundin.kleberg@su.se>",
    "url": "https://drjohanlk.github.io/kollaR/demo.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16123,
    "package_name": "kpodclustr",
    "title": "Method for Clustering Partially Observed Data",
    "description": "Software for k-means clustering of partially \n    observed data from Chi, Chi, and Baraniuk (2016) <doi:10.1080/00031305.2015.1086685>.",
    "version": "1.1",
    "maintainer": "Jocelyn T. Chi <jtchi@ncsu.edu>",
    "url": "http://jocelynchi.com/kpodclustr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16130,
    "package_name": "ks",
    "title": "Kernel Smoothing",
    "description": "Kernel smoothers for univariate and multivariate data, with comprehensive visualisation and bandwidth selection capabilities, including for densities, density derivatives, cumulative distributions, clustering, classification, density ridges, significant modal regions, and two-sample hypothesis tests. Chacon & Duong (2018) <doi:10.1201/9780429485572>.   ",
    "version": "1.15.1",
    "maintainer": "Tarn Duong <tarn.duong@gmail.com>",
    "url": "https://www.mvstat.net/mvksa/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16132,
    "package_name": "kselection",
    "title": "Selection of K in K-Means Clustering",
    "description": "Selection of k in k-means clustering based on Pham et al. paper\n    ``Selection of k in k-means clustering''.",
    "version": "0.2.1",
    "maintainer": "Daniel Rodriguez <daniel.rodriguez.perez@gmail.com>",
    "url": "https://github.com/drodriguezperez/kselection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16133,
    "package_name": "ksharp",
    "title": "Cluster Sharpening",
    "description": "Clustering typically assigns data points into discrete groups, but the clusters can sometimes be indistinct. Cluster sharpening adjusts an existing clustering to create contrast between groups. This package provides a general interface for cluster sharpening along with several implementations based on different excision criteria.",
    "version": "0.1.0.1",
    "maintainer": "Tomasz Konopka <tokonopka@gmail.com>",
    "url": "https://github.com/tkonopka/ksharp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16140,
    "package_name": "ktaucenters",
    "title": "Robust Clustering Procedures",
    "description": "A clustering algorithm similar to K-Means is implemented, it has two main advantages, \n    namely (a) The estimator is resistant to outliers, that means that results of estimator are still correct when\n    there are atypical values in the sample and (b) The estimator is efficient, roughly speaking, \n    if there are no outliers in the sample, results will be similar to those obtained by a classic algorithm (K-Means).\n    Clustering procedure is carried out by minimizing the overall robust scale so-called tau scale.\n    (see Gonzalez, Yohai and Zamar (2019) <arxiv:1906.08198>).",
    "version": "1.0.0",
    "maintainer": "Juan Domingo Gonzalez <juanrst@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16155,
    "package_name": "l1spectral",
    "title": "An L1-Version of the Spectral Clustering",
    "description": "Provides an l1-version of the spectral clustering algorithm devoted to robustly clustering highly perturbed graphs using l1-penalty. This algorithm is described with more details in the preprint C. Champion, M. Champion, M. Blazère, R. Burcelin and J.M. Loubes, \"l1-spectral clustering algorithm: a spectral clustering method using l1-regularization\" (2022).",
    "version": "0.99.6",
    "maintainer": "Magali Champion <magali.champion@u-paris.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16158,
    "package_name": "lab2clean",
    "title": "Automation and Standardization of Cleaning Clinical Laboratory\nData",
    "description": "Navigating the shift of clinical laboratory data from primary everyday clinical use to secondary research purposes presents a significant challenge. Given the substantial time and expertise required for lab data pre-processing and cleaning and the lack of all-in-one tools tailored for this need, we developed our algorithm 'lab2clean' as an open-source R-package. 'lab2clean' package is set to automate and standardize the intricate process of cleaning clinical laboratory results. With a keen focus on improving the data quality of laboratory result values and units, our goal is to equip researchers with a straightforward, plug-and-play tool, making it smoother for them to unlock the true potential of clinical laboratory data in clinical research and clinical machine learning (ML) model development. Functions to clean & validate result values (Version 1.0) are described in detail in 'Zayed et al. (2024)' <doi:10.1186/s12911-024-02652-7>. Functions to standardize & harmonize result units (added in Version 2.0) are described in detail in 'Zayed et al. (2025)' <doi:10.1016/j.ijmedinf.2025.106131>.",
    "version": "2.0.0",
    "maintainer": "Ahmed Zayed <ahmed.zayed@kuleuven.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16214,
    "package_name": "lars",
    "title": "Least Angle Regression, Lasso and Forward Stagewise",
    "description": "Efficient procedures for fitting an entire lasso\n\t\tsequence with the cost of a single least squares\n\t\tfit. Least angle regression and infinitesimal forward\n\t\tstagewise regression are related to the lasso, as\n\t\tdescribed in the paper below.",
    "version": "1.3",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "url": "https://doi.org/10.1214/009053604000000067",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16229,
    "package_name": "latrend",
    "title": "A Framework for Clustering Longitudinal Data",
    "description": "A framework for clustering longitudinal datasets in a standardized way. \n    The package provides an interface to existing R packages for clustering longitudinal univariate trajectories, facilitating reproducible and transparent analyses. \n    Additionally, standard tools are provided to support cluster analyses, including repeated estimation, model validation, and model assessment. \n    The interface enables users to compare results between methods, and to implement and evaluate new methods with ease.\n    The 'akmedoids' package is available from <https://github.com/MAnalytics/akmedoids>.",
    "version": "1.6.2",
    "maintainer": "Niek Den Teuling <niek.den.teuling@philips.com>",
    "url": "https://github.com/niekdt/latrend,\nhttps://niekdt.github.io/latrend/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16277,
    "package_name": "ldaPrototype",
    "title": "Prototype of Multiple Latent Dirichlet Allocation Runs",
    "description": "Determine a Prototype from a number of runs of Latent Dirichlet Allocation (LDA) measuring its similarities with S-CLOP: A procedure to select the LDA run with highest mean pairwise similarity, which is measured by S-CLOP (Similarity of multiple sets by Clustering with Local Pruning), to all other runs. LDA runs are specified by its assignments leading to estimators for distribution parameters. Repeated runs lead to different results, which we encounter by choosing the most representative LDA run as prototype.",
    "version": "0.3.1",
    "maintainer": "Jonas Rieger <jonas.rieger@tu-dortmund.de>",
    "url": "https://github.com/JonasRieger/ldaPrototype",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16283,
    "package_name": "ldhmm",
    "title": "Hidden Markov Model for Financial Time-Series Based on Lambda\nDistribution",
    "description": "Hidden Markov Model (HMM) based on symmetric lambda distribution\n    framework is implemented for the study of return time-series in the financial\n    market. Major features in the S&P500 index, such as regime identification,\n    volatility clustering, and anti-correlation between return and volatility,\n    can be extracted from HMM cleanly. Univariate symmetric lambda distribution\n    is essentially a location-scale family of exponential power distribution.\n    Such distribution is suitable for describing highly leptokurtic time series\n    obtained from the financial market. It provides a theoretically solid foundation\n    to explore such data where the normal distribution is not adequate. The HMM\n    implementation follows closely the book: \"Hidden Markov Models for Time Series\",\n    by Zucchini, MacDonald, Langrock (2016).",
    "version": "0.6.1",
    "maintainer": "Stephen H-T. Lihn <stevelihn@gmail.com>",
    "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2979516\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3435667",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16286,
    "package_name": "ldt",
    "title": "Automated Uncertainty Analysis",
    "description": "Methods and tools for model selection and multi-model inference (Burnham and Anderson (2002) <doi:10.1007/b97636>, among others). \n             'SUR' (for parameter estimation), 'logit'/'probit' (for binary classification), and 'VARMA' (for time-series forecasting) are implemented.\n             Evaluations are both in-sample and out-of-sample. \n             It is designed to be efficient in terms of CPU usage and memory consumption.",
    "version": "0.5.3",
    "maintainer": "Ramin Mojab <rmojab63@gmail.com>",
    "url": "https://github.com/rmojab63/LDT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16332,
    "package_name": "leiden",
    "title": "R Implementation of Leiden Clustering Algorithm",
    "description": "Implements the 'Python leidenalg' module to be called in R.\n    Enables clustering using the leiden algorithm for partition a graph into communities.\n    See the 'Python' repository for more details: <https://github.com/vtraag/leidenalg>\n    Traag et al (2018) From Louvain to Leiden: guaranteeing well-connected communities. <arXiv:1810.08473>.",
    "version": "0.4.3.1",
    "maintainer": "S. Thomas Kelly <tomkellygenetics@gmail.com>",
    "url": "https://github.com/TomKellyGenetics/leiden",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16379,
    "package_name": "lhmixr",
    "title": "Fit Sex-Specific Life History Models with Missing\nClassifications",
    "description": "Fits sex-specific life-history models for fish and other taxa where some of the individuals have unknown sex.",
    "version": "0.1.0",
    "maintainer": "Coilin Minto <coilin.minto@gmit.ie>",
    "url": "https://github.com/mintoc/lhmixr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16381,
    "package_name": "libbib",
    "title": "Various Utilities for Library Science/Assessment and Cataloging",
    "description": "Provides functions for validating and normalizing bibliographic\n   codes such as ISBN, ISSN, and LCCN. Also includes functions to communicate\n   with the WorldCat API, translate Call numbers (Library of Congress and\n   Dewey Decimal) to their subject classifications or subclassifications,\n   and provides various loadable data files such call number / subject\n   crosswalks and code tables.",
    "version": "1.6.4",
    "maintainer": "Tony Fischetti <tony.fischetti@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16405,
    "package_name": "lightgbm",
    "title": "Light Gradient Boosting Machine",
    "description": "Tree based algorithms can be improved by introducing boosting frameworks.\n    'LightGBM' is one such framework, based on Ke, Guolin et al. (2017) <https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision>.\n    This package offers an R interface to work with it.\n    It is designed to be distributed and efficient with the following advantages:\n        1. Faster training speed and higher efficiency.\n        2. Lower memory usage.\n        3. Better accuracy.\n        4. Parallel learning supported.\n        5. Capable of handling large-scale data.\n    In recognition of these advantages, 'LightGBM' has been widely-used in many winning solutions of machine learning competitions.\n    Comparison experiments on public datasets suggest that 'LightGBM' can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. In addition, parallel experiments suggest that in certain circumstances, 'LightGBM' can achieve a linear speed-up in training time by using multiple machines.",
    "version": "4.6.0",
    "maintainer": "James Lamb <jaylamb20@gmail.com>",
    "url": "https://github.com/Microsoft/LightGBM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16486,
    "package_name": "literanger",
    "title": "Fast Serializable Random Forests Based on 'ranger'",
    "description": "An updated implementation of R package 'ranger' by Wright et al,\n    (2017) <doi:10.18637/jss.v077.i01> for training and predicting from random\n    forests, particularly suited to high-dimensional data, and for embedding in\n    'Multiple Imputation by Chained Equations' (MICE) by van Buuren (2007)\n    <doi:10.1177/0962280206074463>. Ensembles of classification and regression\n    trees are currently supported. Sparse data of class 'dgCMatrix' (R package\n    'Matrix') can be directly analyzed. Conventional bagged predictions are\n    available alongside an efficient prediction for MICE via the algorithm\n    proposed by Doove et al (2014) <doi:10.1016/j.csda.2013.10.025>. Trained\n    forests can be written to and read from storage. Survival and probability\n    forests are not supported in the update, nor is data of class 'gwaa.data'\n    (R package 'GenABEL'); use the original 'ranger' package for these analyses.",
    "version": "0.2.0",
    "maintainer": "Stephen Wade <stephematician@gmail.com>",
    "url": "https://gitlab.com/stephematician/literanger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16550,
    "package_name": "localModel",
    "title": "LIME-Based Explanations with Interpretable Inputs Based on\nCeteris Paribus Profiles",
    "description": "Local explanations of machine learning models describe, how features contributed to a single prediction. \n    This package implements an explanation method based on LIME \n    (Local Interpretable Model-agnostic Explanations, \n    see Tulio Ribeiro, Singh, Guestrin (2016) <doi:10.1145/2939672.2939778>) in which interpretable\n    inputs are created based on local rather than global behaviour of each original feature.",
    "version": "0.5",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "url": "https://github.com/ModelOriented/localModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16609,
    "package_name": "lolR",
    "title": "Linear Optimal Low-Rank Projection",
    "description": "Supervised learning techniques designed for the situation when the dimensionality exceeds the sample size have a tendency to overfit as the dimensionality of the data increases. To remedy this High dimensionality; low sample size (HDLSS) situation, we attempt to learn a lower-dimensional representation of the data before learning a classifier. That is, we project the data to a situation where the dimensionality is more manageable, and then are able to better apply standard classification or clustering techniques since we will have fewer dimensions to overfit. A number of previous works have focused on how to strategically reduce dimensionality in the unsupervised case, yet in the supervised HDLSS regime, few works have attempted to devise dimensionality reduction techniques that leverage the labels associated with the data. In this package and the associated manuscript Vogelstein et al. (2017) <arXiv:1709.01233>, we provide several methods for feature extraction, some utilizing labels and some not, along with easily extensible utilities to simplify cross-validative efforts to identify the best feature extraction method. Additionally, we include a series of adaptable benchmark simulations to serve as a standard for future investigative efforts into supervised HDLSS. Finally, we produce a comprehensive comparison of the included algorithms across a range of benchmark simulations and real data applications.",
    "version": "2.1",
    "maintainer": "Eric Bridgeford <ericwb95@gmail.com>",
    "url": "https://github.com/neurodata/lol",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16612,
    "package_name": "long2lstmarray",
    "title": "Longitudinal Dataframes into Arrays for Machine Learning\nTraining",
    "description": "An easy tool to transform 2D longitudinal data into 3D arrays suitable for \n  Long short-term memory neural networks training. The array output can be\n  used by the 'keras' package. Long short-term memory neural networks are described\n  in: Hochreiter, S., & Schmidhuber, J. (1997) <doi:10.1162/neco.1997.9.8.1735>.",
    "version": "0.2.0",
    "maintainer": "Luis Garcez <luisgarcez1@gmail.com>",
    "url": "https://github.com/luisgarcez11/long2lstmarray",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16613,
    "package_name": "longROC",
    "title": "Time-Dependent Prognostic Accuracy with Multiply Evaluated Bio\nMarkers or Scores",
    "description": "Time-dependent Receiver Operating Characteristic curves, Area Under the Curve, and Net Reclassification Indexes for repeated measures. It is based on methods in Barbati and Farcomeni (2017) <doi:10.1007/s10260-017-0410-2>. ",
    "version": "1.0",
    "maintainer": "Alessio Farcomeni <alessio.farcomeni@uniroma1.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16615,
    "package_name": "longclust",
    "title": "Model-Based Clustering and Classification for Longitudinal Data",
    "description": "Clustering or classification of longitudinal data based on a mixture of multivariate t or Gaussian distributions with a Cholesky-decomposed covariance structure. Details in McNicholas and Murphy (2010) <doi:10.1002/cjs.10047> and McNicholas and Subedi (2012) <doi:10.1016/j.jspi.2011.11.026>.",
    "version": "1.5",
    "maintainer": "Paul D. McNicholas <mcnicholas@math.mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16624,
    "package_name": "longmixr",
    "title": "Longitudinal Consensus Clustering with 'flexmix'",
    "description": "An adaption of the consensus clustering approach from\n    'ConsensusClusterPlus' for longitudinal data. The longitudinal data is\n    clustered with flexible mixture models from 'flexmix', while the consensus\n    matrices are hierarchically clustered as in 'ConsensusClusterPlus'. By using\n    the flexibility from 'flexmix' and 'FactoMineR', one can use mixed data\n    types for the clustering.",
    "version": "1.0.0",
    "maintainer": "Jonas Hagenberg <jonas_hagenberg@psych.mpg.de>",
    "url": "https://cellmapslab.github.io/longmixr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16650,
    "package_name": "lowmemtkmeans",
    "title": "Low Memory Use Trimmed K-Means",
    "description": "Performs the trimmed k-means clustering algorithm with lower memory use. It also provides a number of utility functions such as BIC calculations.",
    "version": "0.1.4",
    "maintainer": "Andrew Thomas Jones <andrewthomasjones@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16677,
    "package_name": "lsdbc",
    "title": "Locally Scaled Density Based Clustering",
    "description": "Implementation of Locally Scaled Density Based Clustering (LSDBC) algorithm proposed by Bicici and Yuret (2007) <doi:10.1007/978-3-540-71618-1_82>. This package also contains some supporting functions such as betaCV() function and get_spectral() function.",
    "version": "0.1.0",
    "maintainer": "Fella Ulandari <16.9134@stis.ac.id>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16715,
    "package_name": "luz",
    "title": "Higher Level 'API' for 'torch'",
    "description": "A high level interface for 'torch' providing utilities to reduce the\n    the amount of code needed for common tasks, abstract away torch details and \n    make the same code work on both the 'CPU' and 'GPU'. It's flexible enough to\n    support expressing a large range of models. It's heavily inspired by 'fastai' by \n    Howard et al. (2020) <doi:10.48550/arXiv.2002.04688>, 'Keras' by Chollet et al. (2015) and \n    'PyTorch Lightning' by Falcon et al. (2019) <doi:10.5281/zenodo.3828935>.",
    "version": "0.5.1",
    "maintainer": "Daniel Falbel <daniel@rstudio.com>",
    "url": "https://mlverse.github.io/luz/, https://github.com/mlverse/luz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16722,
    "package_name": "lwc2022",
    "title": "Langa-Weir Classification of Cognitive Function for 2022 HRS\nData",
    "description": "Generates the Langa-Weir classification of cognitive function for \n    the 2022 Health and Retirement Study (HRS) cognition data. It is \n    particularly useful for researchers studying cognitive aging who wish to \n    work with the most recent release of HRS data. The package provides \n    user-friendly functions for data preprocessing, scoring, and classification\n    allowing users to easily apply the Langa-Weir classification system. \n    For details regarding the;\n    HRS <https://hrsdata.isr.umich.edu/> and\n    Langa-Weir classifications <https://hrsdata.isr.umich.edu/data-products/langa-weir-classification-cognitive-function-1995-2020>.",
    "version": "1.0.0",
    "maintainer": "Cormac Monaghan <cormacmonaghan@protonmail.com>",
    "url": "https://github.com/C-Monaghan/lwc2022,\nhttps://c-monaghan.github.io/lwc2022/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16726,
    "package_name": "m2b",
    "title": "Movement to Behaviour Inference using Random Forest",
    "description": "Prediction of behaviour from movement \n\tcharacteristics using observation and random forest for the analyses of movement\n\tdata in ecology.\n\tFrom movement information (speed, bearing...) the model predicts the\n\tobserved behaviour (movement, foraging...) using random forest. The\n\tmodel can then extrapolate behavioural information to movement data\n\twithout direct observation of behaviours.\n\tThe specificity of this method relies on the derivation of multiple predictor variables from the\n\tmovement data over a range of temporal windows. This procedure allows to capture\n\tas much information as possible on the changes and variations of movement and\n\tensures the use of the random forest algorithm to its best capacity. The method\n\tis very generic, applicable to any set of data providing movement data together with\n\tobservation of behaviour.",
    "version": "1.1.0",
    "maintainer": "Laurent Dubroca <laurent.dubroca@gmail.com>",
    "url": "https://github.com/ldbk/m2b",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16802,
    "package_name": "makeFlow",
    "title": "Visualizing Sequential Classifications",
    "description": "A user-friendly tool for visualizing categorical or group movement.",
    "version": "1.0.2",
    "maintainer": "Alex J. Krebs <Krebs.AlexJ@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16813,
    "package_name": "malani",
    "title": "Machine Learning Assisted Network Inference",
    "description": "Find dark genes. These genes are often disregarded due to no detected mutation or differential expression, but are important in coordinating the functionality in cancer networks.",
    "version": "1.0",
    "maintainer": "Mehrab Ghanat Bari <m.ghanatbari@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16821,
    "package_name": "mallet",
    "title": "An R Wrapper for the Java Mallet Topic Modeling Toolkit",
    "description": "\n  An R interface for the Java Machine Learning for Language Toolkit (mallet)\n  <http://mallet.cs.umass.edu/> to estimate probabilistic topic models, such\n  as Latent Dirichlet Allocation. We can use the R package to read textual \n  data into mallet from R objects, run the Java implementation of mallet \n  directly in R, and extract results as R objects. The Mallet toolkit \n  has many functions, this wrapper focuses on the topic modeling sub-package \n  written by David Mimno. The package uses the rJava package to connect to a \n  JVM.",
    "version": "1.3.0",
    "maintainer": "Måns Magnusson <mons.magnusson@gmail.com>",
    "url": "https://github.com/mimno/RMallet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16886,
    "package_name": "maptree",
    "title": "Mapping, Pruning, and Graphing Tree Models",
    "description": "Functions with example data for graphing, pruning, and\n        mapping models from hierarchical clustering, and classification\n        and regression trees.",
    "version": "1.4-9",
    "maintainer": "Robert B. Gramacy <rbg@vt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16912,
    "package_name": "markets",
    "title": "Estimation Methods for Markets in Equilibrium and Disequilibrium",
    "description": "Provides estimation methods for markets in equilibrium and\n    disequilibrium. Supports the estimation of an equilibrium and\n    four disequilibrium models with both correlated and independent shocks.\n    Also provides post-estimation analysis tools, such as aggregation,\n    marginal effect, and shortage calculations. See Karapanagiotis (2024)\n    <doi:10.18637/jss.v108.i02> for an overview of the functionality \n    and examples. The estimation methods are based on full information\n    maximum likelihood techniques given in\n    Maddala and Nelson (1974) <doi:10.2307/1914215>. They are implemented\n    using the analytic derivative expressions calculated in\n    Karapanagiotis (2020) <doi:10.2139/ssrn.3525622>. Standard\n    errors can be estimated by adjusting for heteroscedasticity or clustering.\n    The equilibrium estimation constitutes a case of a system of linear,\n    simultaneous equations. Instead, the disequilibrium models replace the\n    market-clearing condition with a non-linear,\n    short-side rule and allow for different specifications of price dynamics. ",
    "version": "1.1.6",
    "maintainer": "Pantelis Karapanagiotis <pikappa.devel@gmail.com>",
    "url": "https://github.com/pi-kappa-devel/markets/,\nhttps://markets.pikappa.eu/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17035,
    "package_name": "mcboost",
    "title": "Multi-Calibration Boosting",
    "description": "Implements 'Multi-Calibration Boosting' (2018) <https://proceedings.mlr.press/v80/hebert-johnson18a.html> and\n    'Multi-Accuracy Boosting' (2019) <doi:10.48550/arXiv.1805.12317> for the multi-calibration of a machine learning model's prediction.\n    'MCBoost' updates predictions for sub-groups in an iterative fashion in order to mitigate biases like poor calibration or large accuracy differences across subgroups.\n    Multi-Calibration works best in scenarios where the underlying data & labels are unbiased, but resulting models are.\n    This is often the case, e.g. when an algorithm fits a majority population while ignoring or under-fitting minority populations.",
    "version": "0.4.4",
    "maintainer": "Sebastian Fischer <sebf.fischer@gmail.com>",
    "url": "https://github.com/mlr-org/mcboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17036,
    "package_name": "mcca",
    "title": "Multi-Category Classification Accuracy",
    "description": "It contains six common multi-category classification accuracy evaluation measures.\n All of these measures could be found in Li and Ming (2019) <doi:10.1002/sim.8103>. Specifically,\n Hypervolume Under Manifold (HUM), described in\n Li and Fine (2008) <doi:10.1093/biostatistics/kxm050>.\n Correct Classification Percentage (CCP), Integrated Discrimination Improvement (IDI), Net Reclassification Improvement (NRI), R-Squared Value (RSQ), described in\n Li, Jiang and Fine (2013) <doi:10.1093/biostatistics/kxs047>.\n Polytomous Discrimination Index (PDI), described in\n Van Calster et al. (2012) <doi:10.1007/s10654-012-9733-3>.\n Li et al. (2018) <doi:10.1177/0962280217692830>.\n We described all these above measures and our mcca package in\n Li, Gao and D'Agostino (2019) <doi:10.1002/sim.8103>.",
    "version": "0.7.0",
    "maintainer": "Ming Gao <gaoming@umich.edu>",
    "url": "https://github.com/gaoming96/mcca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17038,
    "package_name": "mccf1",
    "title": "Creates the MCC-F1 Curve and Calculates the MCC-F1 Metric and\nthe Best Threshold",
    "description": "The MCC-F1 analysis is a method to evaluate the performance of binary classifications. \n    The MCC-F1 curve is more reliable than the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR)curve under imbalanced ground truth.\n    The MCC-F1 analysis also provides the MCC-F1 metric that integrates classifier performance over varying thresholds, and the best threshold of binary classification.",
    "version": "1.1",
    "maintainer": "Chang Cao <kirin.cao@mail.utoronto.ca>",
    "url": "https://bitbucket.org/hoffmanlab/mccf1/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17039,
    "package_name": "mcclust",
    "title": "Process an MCMC Sample of Clusterings",
    "description": "Implements methods for processing a sample of (hard)\n        clusterings, e.g. the MCMC output of a Bayesian clustering\n        model. Among them are methods that find a single best\n        clustering to represent the sample, which are based on the\n        posterior similarity matrix or a relabelling algorithm.",
    "version": "1.0.1",
    "maintainer": "Arno Fritsch <arno.fritsch@tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17044,
    "package_name": "mcen",
    "title": "Multivariate Cluster Elastic Net",
    "description": "Fits the Multivariate Cluster Elastic Net (MCEN) presented in Price & Sherwood (2018) <arXiv:1707.03530>. The MCEN model simultaneously estimates regression coefficients and a clustering of the responses for a multivariate response model. Currently accommodates the Gaussian and binomial likelihood. ",
    "version": "1.2.1",
    "maintainer": "Ben Sherwood <ben.sherwood@ku.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17052,
    "package_name": "mclust",
    "title": "Gaussian Mixture Modelling for Model-Based Clustering,\nClassification, and Density Estimation",
    "description": "Gaussian finite mixture models fitted via EM algorithm for\n  model-based clustering, classification, and density estimation, \n  including Bayesian regularization, dimension reduction for \n  visualisation, and resampling-based inference.",
    "version": "6.1.2",
    "maintainer": "Luca Scrucca <luca.scrucca@unibo.it>",
    "url": "https://mclust-org.github.io/mclust/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17053,
    "package_name": "mclustAddons",
    "title": "Addons for the 'mclust' Package",
    "description": "Extend the functionality of the 'mclust' package for Gaussian\n    finite mixture modeling by including: density estimation for data with\n    bounded support (Scrucca, 2019 <doi:10.1002/bimj.201800174>); modal\n    clustering using MEM (Modal EM) algorithm for Gaussian mixtures\n    (Scrucca, 2021 <doi:10.1002/sam.11527>); entropy estimation via\n    Gaussian mixture modeling (Robin & Scrucca, 2023\n    <doi:10.1016/j.csda.2022.107582>); Gaussian mixtures modeling of \n    financial log-returns (Scrucca, 2024 <doi:10.3390/e26110907>).",
    "version": "0.10",
    "maintainer": "Luca Scrucca <luca.scrucca@unibo.it>",
    "url": "https://mclust-org.github.io/mclustAddons/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17054,
    "package_name": "mclustcomp",
    "title": "Measures for Comparing Clusters",
    "description": "Given a set of data points, a clustering is defined as a disjoint partition\n    where each pair of sets in a partition has no overlapping elements. \n    This package provides 25 methods that play a role somewhat similar to \n    distance or metric that measures similarity of two clusterings - or partitions.\n    For a more detailed description, see Meila, M. (2005) <doi:10.1145/1102351.1102424>.",
    "version": "0.3.5",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17093,
    "package_name": "mdendro",
    "title": "Extended Agglomerative Hierarchical Clustering",
    "description": "A comprehensive collection of linkage methods for agglomerative\n  hierarchical clustering on a matrix of proximity data (distances or\n  similarities), returning a multifurcated dendrogram or multidendrogram.\n  Multidendrograms can group more than two clusters when ties in proximity data\n  occur, and therefore they do not depend on the order of the input data.\n  Descriptive measures to analyze the resulting dendrogram are additionally\n  provided. <doi:10.18637/jss.v114.i02>.",
    "version": "2.2.3",
    "maintainer": "Alberto Fernandez <alberto.fernandez@urv.cat>",
    "url": "https://webs-deim.urv.cat/~sergio.gomez/mdendro.php",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17107,
    "package_name": "meanShiftR",
    "title": "A Computationally Efficient Mean Shift Implementation",
    "description": "Performs mean shift classification using linear and \n  k-d tree based nearest neighbor implementations for the Gaussian,\n  Epanechnikov, and biweight product kernels. ",
    "version": "0.56",
    "maintainer": "Jonathan Lisic <jlisic@gmail.com>",
    "url": "http://meanmean.me/meanshift/r/cran/2016/08/28/meanShiftR.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17114,
    "package_name": "measures",
    "title": "Performance Measures for Statistical Learning",
    "description": "Provides the biggest amount of statistical measures in the whole R world. Includes measures of regression, (multiclass) classification and multilabel classification. The measures come mainly from the 'mlr' package and were programed by several 'mlr' developers. ",
    "version": "0.3",
    "maintainer": "Philipp Probst <philipp_probst@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17161,
    "package_name": "memoria",
    "title": "Quantifying Ecological Memory in Palaeoecological Datasets and\nOther Long Time-Series",
    "description": "Tools to quantify ecological memory in long time-series with Random Forest models (Breiman 2001 <doi:10.1023/A:1010933404324>) fitted with the 'ranger' library (Wright and Ziegler 2017 <doi:10.18637/jss.v077.i01>). Particularly oriented to palaeoecological datasets and simulated pollen curves produced by the 'virtualPollen' package, but also applicable to other long time-series involving a set of environmental drivers and a biotic response.",
    "version": "1.0.0",
    "maintainer": "Blas M. Benito <blasbenito@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17166,
    "package_name": "mergeTrees",
    "title": "Aggregating Trees",
    "description": "Aggregates a set of trees with the same leaves to create a consensus tree. The trees are typically \n  obtained via hierarchical clustering, hence the hclust format is used to encode both the aggregated trees and the final \n  consensus tree. The method is exact and proven to be O(nqlog(n)), n being the individuals and q being the number of trees to aggregate.",
    "version": "0.1.3",
    "maintainer": "Audrey Hulot <audrey.hulot@inra.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17215,
    "package_name": "metacart",
    "title": "Meta-CART: A Flexible Approach to Identify Moderators in\nMeta-Analysis",
    "description": "Meta-CART integrates classification and regression trees (CART) into meta-analysis. Meta-CART is a flexible approach to identify interaction effects between moderators in meta-analysis. The method is described in Dusseldorp et al. (2014) <doi:10.1037/hea0000018> and Li et al. (2017) <doi:10.1111/bmsp.12088>.",
    "version": "3.0.4",
    "maintainer": "Juan Claramunt <j.claramunt.gonzalez@fsw.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17226,
    "package_name": "metaforest",
    "title": "Exploring Heterogeneity in Meta-Analysis using Random Forests",
    "description": "Conduct random forests-based meta-analysis, obtain partial dependence plots for metaforest and classic meta-analyses, and cross-validate and tune metaforest- and classic meta-analyses in conjunction with the caret package. A requirement of classic meta-analysis is that the studies being aggregated are conceptually similar, and ideally, close replications. However, in many fields, there is substantial heterogeneity between studies on the same topic. Classic meta-analysis lacks the power to assess more than a handful of univariate moderators. MetaForest, by contrast, has substantial power to explore heterogeneity in meta-analysis. It can identify important moderators from a larger set of potential candidates (Van Lissa, 2020). This is an appealing quality, because many meta-analyses have small sample sizes. Moreover, MetaForest yields a measure of variable importance which can be used to identify important moderators, and offers partial prediction plots to explore the shape of the marginal relationship between moderators and effect size.",
    "version": "0.1.5",
    "maintainer": "Caspar J. Van Lissa <c.j.vanlissa@gmail.com>",
    "url": "https://cjvanlissa.github.io/metaforest/,\nhttps://github.com/cjvanlissa/metaforest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17227,
    "package_name": "metafuse",
    "title": "Fused Lasso Approach in Regression Coefficient Clustering",
    "description": "Fused lasso method to cluster and estimate regression coefficients\n    of the same covariate across different data sets when a large number of\n    independent data sets are combined. Package supports Gaussian, binomial,\n    Poisson and Cox PH models.",
    "version": "2.0-1",
    "maintainer": "Lu Tang <lutang@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17244,
    "package_name": "metamicrobiomeR",
    "title": "Microbiome Data Analysis & Meta-Analysis with GAMLSS-BEZI &\nRandom Effects",
    "description": "Generalized Additive Model for Location, Scale and Shape (GAMLSS) \n    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data \n    (with various options for data transformation/normalization to address compositional effects) and \n    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies \n    are implemented. \n    Random Forest model to predict microbiome age based on relative abundances of  \n    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), \n    comparison of multiple diversity indexes using linear/linear mixed effect models \n    and some data display/visualization are also implemented.\n    The reference paper is published by \n    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> . ",
    "version": "1.2",
    "maintainer": "Nhan Ho <nhanhocumc@gmail.com>",
    "url": "https://github.com/nhanhocu/metamicrobiomeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17245,
    "package_name": "metamisc",
    "title": "Meta-Analysis of Diagnosis and Prognosis Research Studies",
    "description": "Facilitate frequentist and Bayesian meta-analysis of diagnosis and prognosis research studies. It includes functions to  summarize multiple estimates of prediction model discrimination and calibration performance (Debray et al., 2019) <doi:10.1177/0962280218785504>. It also includes functions to evaluate funnel plot asymmetry (Debray et al., 2018) <doi:10.1002/jrsm.1266>. Finally, the package provides functions for developing multivariable prediction models from datasets with clustering (de Jong et al., 2021) <doi:10.1002/sim.8981>. ",
    "version": "0.4.0",
    "maintainer": "Thomas Debray <thomas.debray@gmail.com>",
    "url": "https://github.com/smartdata-analysis-and-statistics/metamisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17247,
    "package_name": "metan",
    "title": "Multi Environment Trials Analysis",
    "description": "Performs stability analysis of multi-environment trial data\n    using parametric and non-parametric methods. Parametric methods\n    includes Additive Main Effects and Multiplicative Interaction (AMMI)\n    analysis by Gauch (2013) <doi:10.2135/cropsci2013.04.0241>, Ecovalence\n    by Wricke (1965), Genotype plus Genotype-Environment (GGE) biplot\n    analysis by Yan & Kang (2003) <doi:10.1201/9781420040371>, geometric\n    adaptability index by Mohammadi & Amri (2008)\n    <doi:10.1007/s10681-007-9600-6>, joint regression analysis by Eberhart\n    & Russel (1966) <doi:10.2135/cropsci1966.0011183X000600010011x>,\n    genotypic confidence index by Annicchiarico (1992), Murakami & Cruz's\n    (2004) method, power law residuals (POLAR) statistics by Doring et al.\n    (2015) <doi:10.1016/j.fcr.2015.08.005>, scale-adjusted coefficient of\n    variation by Doring & Reckling (2018) <doi:10.1016/j.eja.2018.06.007>,\n    stability variance by Shukla (1972) <doi:10.1038/hdy.1972.87>,\n    weighted average of absolute scores by Olivoto et al. (2019a)\n    <doi:10.2134/agronj2019.03.0220>, and multi-trait stability index by\n    Olivoto et al. (2019b) <doi:10.2134/agronj2019.03.0221>.\n    Non-parametric methods includes superiority index by Lin & Binns\n    (1988) <doi:10.4141/cjps88-018>, nonparametric measures of phenotypic\n    stability by Huehn (1990) <doi:10.1007/BF00024241>, TOP third\n    statistic by Fox et al. (1990) <doi:10.1007/BF00040364>. Functions for\n    computing biometrical analysis such as path analysis, canonical\n    correlation, partial correlation, clustering analysis, and tools for\n    inspecting, manipulating, summarizing and plotting typical\n    multi-environment trial data are also provided.",
    "version": "1.19.0",
    "maintainer": "Tiago Olivoto <tiagoolivoto@gmail.com>",
    "url": "https://github.com/nepem-ufsc/metan,\nhttps://nepem-ufsc.github.io/metan/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17269,
    "package_name": "metaumbrella",
    "title": "Umbrella Review Package for R",
    "description": "A comprehensive range of facilities to perform umbrella reviews with stratification of the evidence in R. The package accomplishes this aim by building on three core functions that: (i) automatically perform all required calculations in an umbrella review (including but not limited to meta-analyses), (ii) stratify evidence according to various classification criteria, and (iii) generate a visual representation of the results. Note that if you are not familiar with R, the core features of this package are available from a web browser (<https://www.metaumbrella.org/>).",
    "version": "1.1.0",
    "maintainer": "Corentin J Gosling <cgosling@parisnanterre.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17298,
    "package_name": "metrica",
    "title": "Prediction Performance Metrics",
    "description": "A compilation of more than 80 functions designed to quantitatively and visually evaluate prediction performance of regression (continuous variables) and classification (categorical variables) of point-forecast models (e.g. APSIM, DSSAT, DNDC, supervised Machine Learning). For regression, it includes functions to generate plots (scatter, tiles, density, & Bland-Altman plot), and to estimate error metrics (e.g. MBE, MAE, RMSE), error decomposition (e.g. lack of accuracy-precision), model efficiency (e.g. NSE, E1, KGE), indices of agreement (e.g. d, RAC), goodness of fit (e.g. r, R2), adjusted correlation coefficients (e.g. CCC, dcorr), symmetric regression coefficients (intercept, slope), and mean absolute scaled error (MASE) for time series predictions. For classification (binomial and multinomial), it offers functions to generate and plot confusion matrices, and to estimate performance metrics such as accuracy, precision, recall, specificity, F-score, Cohen's Kappa, G-mean, and many more. For more details visit the vignettes <https://adriancorrendo.github.io/metrica/>.",
    "version": "2.1.0",
    "maintainer": "Adrian A. Correndo <acorrend@uoguelph.ca>",
    "url": "https://adriancorrendo.github.io/metrica/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17323,
    "package_name": "mgee2",
    "title": "Marginal Analysis of Misclassified Longitudinal Ordinal Data",
    "description": "Three estimating equation methods are provided in this package for marginal analysis of longitudinal ordinal data with misclassified responses and covariates. \n The naive analysis which is solely based on the observed data without adjustment may lead to bias.\n The corrected generalized estimating equations (GEE2) method which is unbiased requires the misclassification parameters to be known beforehand. \n The corrected generalized estimating equations (GEE2) with validation subsample method estimates the misclassification parameters based on a given\n validation set. This package is an implementation \n of Chen (2013) <doi:10.1002/bimj.201200195>.",
    "version": "0.6",
    "maintainer": "Yuliang Xu <yuliangx@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17326,
    "package_name": "mglasso",
    "title": "Multiscale Graphical Lasso",
    "description": "Inference of Multiscale graphical models with neighborhood\n    selection approach.  The method is based on solving a convex\n    optimization problem combining a Lasso and fused-group Lasso\n    penalties.  This allows to infer simultaneously a conditional\n    independence graph and a clustering partition. The optimization is\n    based on the Continuation with Nesterov smoothing in a\n    Shrinkage-Thresholding Algorithm solver (Hadj-Selem et al. 2018)\n    <doi:10.1109/TMI.2018.2829802> implemented in python.",
    "version": "0.1.2",
    "maintainer": "Edmond Sanou <doedmond.sanou@univ-evry.fr>",
    "url": "https://desanou.github.io/mglasso/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17376,
    "package_name": "micer",
    "title": "Map Image Classification Efficacy",
    "description": "Map image classification efficacy (MICE) adjusts the accuracy rate relative to a random classification baseline (Shao et al. (2021)<doi:10.1109/ACCESS.2021.3116526> and Tang et al. (2024)<doi:10.1109/TGRS.2024.3446950>). Only the proportions from the reference labels are considered, as opposed to the proportions from the reference and predictions, as is the case for the Kappa statistic. This package offers means to calculate MICE and adjusted versions of class-level user's accuracy (i.e., precision) and producer's accuracy (i.e., recall) and F1-scores. Class-level metrics are aggregated using macro-averaging. Functions are also made available to estimate confidence intervals using bootstrapping and statistically compare two classification results. ",
    "version": "0.2.1",
    "maintainer": "Aaron Maxwell <Aaron.Maxwell@mail.wvu.edu>",
    "url": "https://github.com/maxwell-geospatial/micer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17393,
    "package_name": "microeco",
    "title": "Microbial Community Ecology Data Analysis",
    "description": "A series of statistical and plotting approaches in microbial community ecology based on the R6 class. The classes are designed for data preprocessing, taxa abundance plotting, alpha diversity analysis, beta diversity analysis, differential abundance test, null model analysis, network analysis, machine learning, environmental data analysis and functional analysis.",
    "version": "1.16.0",
    "maintainer": "Chi Liu <liuchi0426@126.com>",
    "url": "https://github.com/ChiLiubio/microeco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17496,
    "package_name": "miscIC",
    "title": "Misclassified Interval Censored Time-to-Event Data",
    "description": "Estimation of the survivor function for interval censored time-to-event data subject to misclassification using nonparametric maximum likelihood estimation, implementing the methods of Titman (2017) <doi:10.1007/s11222-016-9705-7>. Misclassification probabilities can either be specified as fixed or estimated. Models with time dependent misclassification may also be fitted. ",
    "version": "0.1.0",
    "maintainer": "Andrew Titman <a.titman@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17498,
    "package_name": "misclassGLM",
    "title": "Computation of Generalized Linear Models with Misclassified\nCovariates Using Side Information",
    "description": "Estimates models that extend the standard GLM to take\n    misclassification into account. The models require side information from a secondary data set\n    on the misclassification process, i.e. some sort of misclassification\n    probabilities conditional on some common covariates.\n    A detailed description of the algorithm can be found in\n    Dlugosz, Mammen and Wilke (2015) <https://ftp.zew.de/pub/zew-docs/dp/dp15043.pdf>.",
    "version": "0.3.6",
    "maintainer": "Stephan Dlugosz <stephan.dlugosz@googlemail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17507,
    "package_name": "missForest",
    "title": "Nonparametric Missing Value Imputation using Random Forest",
    "description": "The function 'missForest' in this package is used to\n    impute missing values particularly in the case of mixed-type\n    data. It uses a random forest (via 'ranger' or 'randomForest') trained on the observed values of\n    a data matrix to predict the missing values. It can be used to\n    impute continuous and/or categorical data including complex\n    interactions and non-linear relations. It yields an out-of-bag\n    (OOB) imputation error estimate without the need of a test set\n    or elaborate cross-validation. It can be run in parallel to \n    save computation time.",
    "version": "1.6.1",
    "maintainer": "Daniel J. Stekhoven <stekhoven@nexus.ethz.ch>",
    "url": "https://www.r-project.org, https://github.com/stekhoven/missForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17508,
    "package_name": "missForestPredict",
    "title": "Missing Value Imputation using Random Forest for Prediction\nSettings",
    "description": "Missing data imputation based on the 'missForest' algorithm (Stekhoven, Daniel J (2012) <doi:10.1093/bioinformatics/btr597>)\n    with adaptations for prediction settings. The function missForest() is used \n    to impute a (training) dataset with missing values and to learn imputation \n    models that can be later used for imputing new observations. \n    The function missForestPredict() is used to impute one or multiple new \n    observations (test set) using the models learned on the training data. For more details see \n    Albu, E., Gao, S., Wynants, L., & Van Calster, B. (2024). missForestPredict--Missing data imputation for prediction settings <doi:10.48550/arXiv.2407.03379>.",
    "version": "1.0.1",
    "maintainer": "Elena Albu <elenaa.albu@gmail.com>",
    "url": "https://github.com/sibipx/missForestPredict",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17515,
    "package_name": "missSOM",
    "title": "Self-Organizing Maps with Built-in Missing Data Imputation",
    "description": "The Self-Organizing Maps with Built-in Missing Data Imputation. Missing values are imputed and regularly updated during the online Kohonen algorithm. Our method can be used for data visualisation, clustering or imputation of missing data. It is an extension of the online algorithm of the 'kohonen' package. The method is described\n    in the article \"Self-Organizing Maps for Exploration of Partially Observed Data and Imputation of Missing Values\" by S. Rejeb, C. Duveau, T. Rebafka (2022) <arXiv:2202.07963>.",
    "version": "1.0.1",
    "maintainer": "Sara Rejeb <sara.rejeb@live.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17519,
    "package_name": "misspi",
    "title": "Missing Value Imputation in Parallel",
    "description": "A framework that boosts the imputation of 'missForest' by Stekhoven, D.J. and Bühlmann, P. (2012) <doi:10.1093/bioinformatics/btr597> by harnessing parallel processing and through the fast Gradient Boosted Decision Trees (GBDT) implementation 'LightGBM' by Ke, Guolin et al.(2017) <https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision>. 'misspi' has the following main advantages:\n             1. Allows embrassingly parallel imputation on large scale data.\n             2. Accepts a variety of machine learning models as methods with friendly user portal.\n             3. Supports multiple initializations methods.\n             4. Supports early stopping that prohibits unnecessary iterations.",
    "version": "0.1.0",
    "maintainer": "Zhongli Jiang <jiang548@purdue.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17542,
    "package_name": "mixSPE",
    "title": "Mixtures of Power Exponential and Skew Power Exponential\nDistributions for Use in Model-Based Clustering and\nClassification",
    "description": "Mixtures of skewed and elliptical distributions are implemented using mixtures of multivariate skew \n    power exponential and power exponential distributions, respectively. A generalized expectation-maximization \n    framework is used for parameter estimation. See citation() for how to cite.",
    "version": "0.9.3",
    "maintainer": "Utkarsh J. Dang <utkarshdang@cunet.carleton.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17543,
    "package_name": "mixSSG",
    "title": "Clustering Using Mixtures of Sub Gaussian Stable Distributions",
    "description": "Developed for model-based clustering using the finite mixtures of skewed sub-Gaussian\n              stable distributions developed by Teimouri (2022) <arXiv:2205.14067> and estimating\n              parameters of the symmetric stable distribution within the Bayesian framework.",
    "version": "2.1.1",
    "maintainer": "Mahdi Teimouri <teimouri@aut.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17548,
    "package_name": "mixdir",
    "title": "Cluster High Dimensional Categorical Datasets",
    "description": "Scalable Bayesian clustering of categorical datasets. The package implements a hierarchical Dirichlet \n    (Process) mixture  of multinomial distributions. It is thus a probabilistic latent class model (LCM) and can be used\n    to reduce the  dimensionality of hierarchical data and cluster individuals into latent classes. It can automatically\n    infer an appropriate number of latent classes or find k classes, as defined by the user.  The model is based on a\n    paper by Dunson and Xing (2009) <doi:10.1198/jasa.2009.tm08439>, but implements a scalable variational inference algorithm so that it is\n    applicable to large datasets. It is described and tested in the accompanying paper by \n    Ahlmann-Eltze and Yau (2018) <doi:10.1109/DSAA.2018.00068>.",
    "version": "0.3.0",
    "maintainer": "Constantin Ahlmann-Eltze <artjom31415@googlemail.com>",
    "url": "https://github.com/const-ae/mixdir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17558,
    "package_name": "mixgb",
    "title": "Multiple Imputation Through 'XGBoost'",
    "description": "Multiple imputation using 'XGBoost', subsampling, and predictive mean \n    matching as described in Deng and Lumley (2023) \n    <doi:10.1080/10618600.2023.2252501>.  The package supports various types of \n    variables, offers flexible settings, and enables saving an imputation model to impute\n    new data. Data processing and memory usage have been optimised to speed up \n    the imputation process.",
    "version": "2.0.3",
    "maintainer": "Yongshi Deng <agnes.yongshideng@gmail.com>",
    "url": "https://github.com/agnesdeng/mixgb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17571,
    "package_name": "mixture",
    "title": "Mixture Models for Clustering and Classification",
    "description": "An implementation of 14 parsimonious mixture models for model-based clustering or model-based classification. Gaussian, Student's t, generalized hyperbolic, variance-gamma or skew-t mixtures are available. All approaches work with missing data. Celeux and Govaert (1995) <doi:10.1016/0031-3203(94)00125-6>, Browne and McNicholas (2014) <doi:10.1007/s11634-013-0139-1>, Browne and McNicholas (2015) <doi:10.1002/cjs.11246>.",
    "version": "2.2.0",
    "maintainer": "Paul D. McNicholas <mcnicholas@math.mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17579,
    "package_name": "mlapi",
    "title": "Abstract Classes for Building 'scikit-learn' Like API",
    "description": "Provides 'R6' abstract classes for building machine learning models \n    with 'scikit-learn' like API. <https://scikit-learn.org/> is a popular module \n    for 'Python' programming language which design became de facto a standard \n    in industry for machine learning tasks.",
    "version": "0.1.1",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17580,
    "package_name": "mlbench",
    "title": "Machine Learning Benchmark Problems",
    "description": "A collection of artificial and real-world machine learning\n        benchmark problems, including, e.g., several\n        data sets from the UCI repository.",
    "version": "2.1-6",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17593,
    "package_name": "mlf",
    "title": "Machine Learning Foundations",
    "description": "Offers a gentle introduction to machine learning concepts for practitioners with a statistical pedigree: decomposition of model error (bias-variance trade-off), nonlinear correlations, information theory and functional permutation/bootstrap simulations. Székely GJ, Rizzo ML, Bakirov NK. (2007). <doi:10.1214/009053607000000505>. Reshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ, Lander ES, Mitzenmacher M, Sabeti PC. (2011). <doi:10.1126/science.1205438>.",
    "version": "1.2.1",
    "maintainer": "Kyle Peterson <petersonkdon@gmail.com>",
    "url": "http://mlf-project.us/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17595,
    "package_name": "mlflow",
    "title": "Interface to 'MLflow'",
    "description": "R interface to 'MLflow', open source platform for\n    the complete machine learning life cycle, see <https://mlflow.org/>.\n    This package supports installing 'MLflow', tracking experiments,\n    creating and running projects, and saving and serving models.",
    "version": "3.6.0",
    "maintainer": "Ben Wilson <benjamin.wilson@databricks.com>",
    "url": "https://github.com/mlflow/mlflow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17606,
    "package_name": "mlmts",
    "title": "Machine Learning Algorithms for Multivariate Time Series",
    "description": "An implementation of several machine learning algorithms for \n    multivariate time series. The package includes functions allowing the\n    execution of clustering, classification or outlier detection methods,\n    among others. It also incorporates a collection of multivariate time\n    series datasets which can be used to analyse the performance of new\n    proposed algorithms. Some of these datasets are stored in GitHub data\n    packages 'ueadata1' to 'ueadata8'. To access these data packages, run\n    'install.packages(c('ueadata1', 'ueadata2', 'ueadata3', 'ueadata4', 'ueadata5', 'ueadata6', 'ueadata7', 'ueadata8'), repos='<https://anloor7.github.io/drat/>')'.\n    The installation takes a couple of minutes but we strongly encourage the\n    users to do it if they want to have available all datasets of mlmts.\n    Practitioners from a broad variety of fields could\n    benefit from the general framework provided by 'mlmts'.",
    "version": "1.1.2",
    "maintainer": "Angel Lopez-Oriona <oriona38@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17609,
    "package_name": "mlpack",
    "title": "'Rcpp' Integration for the 'mlpack' Library",
    "description": "A fast, flexible machine learning library, written in C++, that\n             aims to provide fast, extensible implementations of cutting-edge\n             machine learning algorithms.  See also Curtin et al. (2023)\n             <doi:10.21105/joss.05026>.",
    "version": "4.6.3",
    "maintainer": "Ryan Curtin <ryan@ratml.org>",
    "url": "https://www.mlpack.org/doc/user/bindings/r.html,\nhttps://github.com/mlpack/mlpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17620,
    "package_name": "mlr3fairness",
    "title": "Fairness Auditing and Debiasing for 'mlr3'",
    "description": "Integrates fairness auditing and bias mitigation methods for\n    the 'mlr3' ecosystem.  This includes fairness metrics, reporting\n    tools, visualizations and bias mitigation techniques such as\n    \"Reweighing\" described in 'Kamiran, Calders' (2012)\n    <doi:10.1007/s10115-011-0463-8> and \"Equalized Odds\" described in\n    'Hardt et al.' (2016)\n    <https://papers.nips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf>.\n    Integration with 'mlr3' allows for auditing of ML models as well as\n    convenient joint tuning of machine learning algorithms and debiasing\n    methods.",
    "version": "0.4.0",
    "maintainer": "Florian Pfisterer <pfistererf@googlemail.com>",
    "url": "https://mlr3fairness.mlr-org.com,\nhttps://github.com/mlr-org/mlr3fairness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17628,
    "package_name": "mlr3measures",
    "title": "Performance Measures for 'mlr3'",
    "description": "Implements multiple performance measures for supervised\n    learning.  Includes over 40 measures for regression and\n    classification. Additionally, meta information about the performance\n    measures can be queried, e.g. what the best and worst possible\n    performances scores are.",
    "version": "1.2.0",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "url": "https://mlr3measures.mlr-org.com,\nhttps://github.com/mlr-org/mlr3measures",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17637,
    "package_name": "mlr3summary",
    "title": "Model and Learner Summaries for 'mlr3'",
    "description": "Concise and interpretable summaries for machine learning models \n    and learners of the 'mlr3' ecosystem.\n    The package takes inspiration from the summary function for (generalized)\n    linear models but extends it to non-parametric machine learning models, \n    based on generalization performance, model complexity, feature importances \n    and effects, and fairness metrics. ",
    "version": "0.1.0",
    "maintainer": "Susanne Dandl <dandls.datascience@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17641,
    "package_name": "mlr3tuningspaces",
    "title": "Search Spaces for 'mlr3'",
    "description": "Collection of search spaces for hyperparameter optimization in the\n  'mlr3' ecosystem. It features ready-to-use search spaces for many popular\n  machine learning algorithms. The search spaces are from scientific articles\n  and work for a wide range of data sets.",
    "version": "0.6.0",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "url": "https://mlr3tuningspaces.mlr-org.com,\nhttps://github.com/mlr-org/mlr3tuningspaces",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17656,
    "package_name": "mltest",
    "title": "Classification Evaluation Metrics",
    "description": "\n    A fast, robust and easy-to-use calculation \n    of multi-class classification evaluation metrics based on confusion matrix.",
    "version": "1.0.3",
    "maintainer": "G. Dudnik <gl.dudnik@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17707,
    "package_name": "moc.gapbk",
    "title": "Multi-Objective Clustering Algorithm Guided by a-Priori\nBiological Knowledge",
    "description": "Implements the Multi-Objective Clustering Algorithm Guided by a-Priori Biological Knowledge (MOC-GaPBK) which was proposed by Parraga-Alava, J. et. al. (2018) <doi:10.1186/s13040-018-0178-4>.",
    "version": "0.1.3",
    "maintainer": "Jorge Parraga-Alava <jorge.parraga@usach.cl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17787,
    "package_name": "monmlp",
    "title": "Multi-Layer Perceptron Neural Network with Optional Monotonicity\nConstraints",
    "description": "Train and make predictions from a multi-layer perceptron neural\n        network with optional partial monotonicity constraints.",
    "version": "1.1.5-1",
    "maintainer": "Alex J. Cannon <alex.cannon@canada.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17832,
    "package_name": "mosclust",
    "title": "Model Order Selection for Clustering",
    "description": "Stability based methods for model order selection in clustering problems\n (Valentini, G (2007), <doi:10.1093/bioinformatics/btl600>).\n Using multiple perturbations of the data the stability of clustering solutions is assessed. Different\n perturbations may be used: resampling techniques, random projections and noise injection. Stability measures\n for the estimate of clustering solutions and statistical tests to assess their significance are provided.",
    "version": "1.0.2",
    "maintainer": "Jessica Gliozzo <jessica.gliozzo@unimi.it>",
    "url": "https://valentini.di.unimi.it/SW/mosclust/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17841,
    "package_name": "motifcluster",
    "title": "Motif-Based Spectral Clustering of Weighted Directed Networks",
    "description": "\n    Tools for spectral clustering of weighted directed networks using motif\n    adjacency matrices. Methods perform well on large and sparse networks, and\n    random sampling methods for generating weighted directed networks are also\n    provided. Based on methodology detailed in Underwood, Elliott and Cucuringu\n    (2020) <arXiv:2004.01293>.",
    "version": "0.2.3",
    "maintainer": "William George Underwood <wgu2@princeton.edu>",
    "url": "https://github.com/wgunderwood/motifcluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17845,
    "package_name": "motoRneuron",
    "title": "Analyzing Paired Neuron Discharge Times for Time-Domain\nSynchronization",
    "description": "The temporal relationship between motor neurons can offer \n    explanations for neural strategies. We combined functions to reduce neuron \n    action potential discharge data and analyze it for short-term, time-domain \n    synchronization. Even more so, motoRneuron combines most available methods \n    for the determining cross correlation histogram peaks and most available \n    indices for calculating synchronization into simple functions. See \n    Nordstrom, Fuglevand, and Enoka (1992) <doi:10.1113/jphysiol.1992.sp019244> \n    for a more thorough introduction.",
    "version": "1.0.0",
    "maintainer": "Andrew Tweedell <atweedell315@gmail.com>",
    "url": "http://github.com/tweedell/motoRneuron",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17860,
    "package_name": "movieROC",
    "title": "Visualizing the Decision Rules Underlying Binary Classification",
    "description": "Visualization of decision rules for binary classification and Receiver Operating Characteristic (ROC) curve estimation under different generalizations proposed in the literature:\n  - making the classification subsets flexible to cover those scenarios where both extremes of the\n  marker are associated with a higher risk of being positive, considering two thresholds \n  (gROC() function);\n  - transforming the marker by a proper function trying to improve the classification performance \n  (hROC() function);\n  - when dealing with multivariate markers, considering a proper transformation to univariate space \n  trying to maximize the resulting AUC of the TPR for each FPR (multiROC() function).\n  The classification regions behind each point of the ROC curve are displayed in both static \n  graphics (plot_buildROC(), plot_regions() or plot_funregions() function) or \n  videos (movieROC() function).",
    "version": "0.1.2",
    "maintainer": "Sonia Perez-Fernandez <perezsonia@uniovi.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17865,
    "package_name": "mpath",
    "title": "Regularized Linear Models",
    "description": "Algorithms compute robust estimators for loss functions in the concave convex (CC) family by the iteratively reweighted convex optimization (IRCO), an extension of the iteratively reweighted least squares (IRLS). The IRCO reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include robust (penalized) generalized linear models and robust support vector machines. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,\n      Wang et al. (2015) <doi:10.1002/bimj.201400143>,\n      Wang et al. (2016) <doi:10.1177/0962280214530608>,\n      Wang (2021) <doi:10.1007/s11749-021-00770-2>,\n      Wang (2024) <doi:10.1111/anzs.12409>.",
    "version": "0.4-2.26",
    "maintainer": "Zhu Wang <zwang145@uthsc.edu>",
    "url": "https://github.com/zhuwang46/mpath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17887,
    "package_name": "mrIML",
    "title": "Multi-Response (Multivariate) Interpretable Machine Learning",
    "description": "Builds and interprets multi-response machine learning models using 'tidymodels' syntax. Users can supply a tidy model, and 'mrIML' automates the process of fitting multiple response models to multivariate data and applying interpretable machine learning techniques across them. For more details see Fountain-Jones (2021) <doi:10.1111/1755-0998.13495> and Fountain-Jones et al. (2024) <doi:10.22541/au.172676147.77148600/v1>.",
    "version": "2.2.0",
    "maintainer": "Nick Fountain-Jones <nick.fountainjones@utas.edu.au>",
    "url": "https://github.com/nickfountainjones/mrIML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17893,
    "package_name": "mrbin",
    "title": "Metabolomics Data Analysis Functions",
    "description": "A collection of functions for processing and analyzing metabolite data. \n    The namesake function mrbin() converts 1D \n    or 2D Nuclear Magnetic Resonance data into a matrix of values suitable for further data analysis and\n    performs basic processing steps in a reproducible way. Negative values, a\n    common issue in such data, can be replaced by positive values (<doi:10.1021/acs.jproteome.0c00684>). All used\n    parameters are stored in a readable text file and can be restored from that\n    file to enable exact reproduction of the data at a later time. The function fia() ranks features according\n    to their impact on classifier models, especially artificial neural network models.",
    "version": "1.9.4",
    "maintainer": "Matthias Klein <matthias.s.klein@gmx.net>",
    "url": "https://github.com/kleinomicslab/mrbin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17900,
    "package_name": "mrfDepth",
    "title": "Depth Measures in Multivariate, Regression and Functional\nSettings",
    "description": "Tools to compute depth measures and implementations of related \n             tasks such as outlier detection, data exploration and \n            classification of multivariate, regression and functional data.",
    "version": "1.0.17",
    "maintainer": "Jakob Raymaekers <jakob.raymaekers@kuleuven.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17912,
    "package_name": "mritc",
    "title": "MRI Tissue Classification",
    "description": "Implements various methods for tissue classification in magnetic\n        resonance (MR) images of the brain, including normal mixture models\n        and hidden Markov normal mixture models, as outlined in Feng &\n        Tierney (2011) <doi:10.18637/jss.v044.i07>. These methods allow a\n        structural MR image to be classified into gray matter, white matter\n        and cerebrospinal fluid tissue types.",
    "version": "0.5-3",
    "maintainer": "Jon Clayden <code@clayden.org>",
    "url": "https://github.com/jonclayden/mritc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17957,
    "package_name": "mstclustering",
    "title": "\"MST-Based Clustering\"",
    "description": "Implements a minimum-spanning-tree-based heuristic for k-means clustering using a union-find disjoint set and the algorithm in Kruskal (1956) <doi:10.1090/S0002-9939-1956-0078686-7>.",
    "version": "1.0.0.0",
    "maintainer": "Kevin Michael Frick <kmfrick98@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17959,
    "package_name": "mstknnclust",
    "title": "MST-kNN Clustering Algorithm",
    "description": "Implements the MST-kNN clustering algorithm which was proposed by Inostroza-Ponta, M. (2008) <https://trove.nla.gov.au/work/28729389?selectedversion=NBD44634158>.  ",
    "version": "0.3.2",
    "maintainer": "Jorge Parraga-Alava <jorge.parraga@usach.cl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17962,
    "package_name": "mt",
    "title": "Metabolomics Data Analysis Toolbox",
    "description": "Functions for metabolomics data analysis: data preprocessing, \n  orthogonal signal correction, PCA analysis, PCA-DA analysis, \n\tPLS-DA analysis, classification, feature selection, correlation \n\tanalysis, data visualisation and re-sampling strategies.",
    "version": "2.0-1.21",
    "maintainer": "Wanchang Lin <wanchanglin@hotmail.com>",
    "url": "https://github.com/wanchanglin/mt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17968,
    "package_name": "mtlgmm",
    "title": "Unsupervised Multi-Task and Transfer Learning on Gaussian\nMixture Models",
    "description": "Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the Expectation-Maximization (EM) algorithm that not only can effectively utilize unknown similarity between related tasks but is also robust against a fraction of outlier tasks from arbitrary sources. The proposed procedure is shown to achieve minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Finally, we demonstrate the effectiveness of our methods through simulations and a real data analysis. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees. This package implements the algorithms proposed in Tian, Y., Weng, H., & Feng, Y. (2022) <arXiv:2209.15224>.",
    "version": "0.1.0",
    "maintainer": "Ye Tian <ye.t@columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17987,
    "package_name": "mult.latent.reg",
    "title": "Regression and Clustering in Multivariate Response Scenarios",
    "description": "Fitting multivariate response models with random effects on one or two levels; whereby the (one-dimensional) random effect represents a latent variable approximating the multivariate space of outcomes, after possible adjustment for covariates. The method is particularly useful for multivariate, highly correlated outcome variables with unobserved heterogeneities. Applications include regression with multivariate responses, as well as multivariate clustering or ranking problems. See Zhang and Einbeck (2024) <doi:10.1007/s42519-023-00357-0>.",
    "version": "0.2.2",
    "maintainer": "Yingjuan Zhang <yingjuan.zhang7@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18012,
    "package_name": "multiROC",
    "title": "Calculating and Visualizing ROC and PR Curves Across Multi-Class\nClassifications",
    "description": "Tools to solve real-world problems with multiple classes classifications by computing the areas under ROC and PR curve via micro-averaging and macro-averaging. The vignettes of this package can be found via <https://github.com/WandeRum/multiROC>. The methodology is described in V. Van Asch (2013) <https://www.clips.uantwerpen.be/~vincent/pdf/microaverage.pdf> and Pedregosa et al. (2011) <http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html>.",
    "version": "1.1.1",
    "maintainer": "Runmin Wei <runmin@hawaii.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18019,
    "package_name": "multiblock",
    "title": "Multiblock Data Fusion in Statistics and Machine Learning",
    "description": "Functions and datasets to support Smilde, Næs and Liland (2021, ISBN: 978-1-119-60096-1) \n   \"Multiblock Data Fusion in Statistics and Machine Learning - Applications in the Natural and Life Sciences\". \n   This implements and imports a large collection of methods for multiblock data analysis with common interfaces, result- and plotting \n   functions, several real data sets and six vignettes covering a range different applications.",
    "version": "0.8.10",
    "maintainer": "Kristian Hovde Liland <kristian.liland@nmbu.no>",
    "url": "https://khliland.github.io/multiblock/,\nhttps://github.com/khliland/multiblock/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18059,
    "package_name": "multinomialLogitMix",
    "title": "Clustering Multinomial Count Data under the Presence of\nCovariates",
    "description": "Methods for model-based clustering of multinomial counts under the presence of covariates using mixtures of multinomial logit models, as implemented in Papastamoulis (2023) <DOI:10.1007/s11634-023-00547-5>. These models are estimated under  a frequentist as well as a Bayesian setup using the Expectation-Maximization algorithm and Markov chain Monte Carlo sampling (MCMC), respectively. The (unknown) number of clusters is selected according to the Integrated Completed Likelihood criterion (for the frequentist model), and estimating the number of non-empty components using overfitting mixture models after imposing suitable sparse prior assumptions on the mixing proportions (in the Bayesian case), see Rousseau and Mengersen (2011) <DOI:10.1111/j.1467-9868.2011.00781.x>. In the latter case, various MCMC chains run in parallel and are allowed to switch states. The final MCMC output is suitably post-processed in order to undo label switching using the Equivalence Classes Representatives (ECR) algorithm, as described in Papastamoulis (2016) <DOI:10.18637/jss.v069.c01>. ",
    "version": "1.1",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18075,
    "package_name": "multisom",
    "title": "Clustering a Data Set using Multi-SOM Algorithm",
    "description": "Implements two versions of the algorithm namely: stochastic and batch. The package determines also the best number of clusters and offers to the user the best clustering scheme from different results.",
    "version": "1.3",
    "maintainer": "Sarra Chair <sarra.chair@gmail.com>",
    "url": "https://sites.google.com/site/malikacharrad/research/multisom-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18087,
    "package_name": "multiwayvcov",
    "title": "Multi-Way Standard Error Clustering",
    "description": "Exports two functions implementing\n    multi-way clustering using the method suggested by Cameron, Gelbach, &\n    Miller (2011) and cluster (or block)\n    bootstrapping for estimating variance-covariance matrices. Normal one and\n    two-way clustering matches the results of other common statistical\n    packages.  Missing values are handled transparently and rudimentary\n    parallelization support is provided.",
    "version": "1.2.3",
    "maintainer": "Nathaniel Graham <npgraham1@gmail.com>",
    "url": "http://sites.google.com/site/npgraham1/research/code",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18100,
    "package_name": "musclesyneRgies",
    "title": "Extract Muscle Synergies from Electromyography",
    "description": "Provides a framework to factorise electromyography (EMG) data.\n    Tools are provided for raw data pre-processing, non negative matrix factorisation,\n    classification of factorised data and plotting of obtained outcomes.\n    In particular, reading from ASCII files is supported, along with wide-used\n    filtering approaches to process EMG data. All steps include one or more sensible\n    defaults that aim at simplifying the workflow. Yet, all functions are largely\n    tunable at need. Example data sets are included.",
    "version": "1.2.5",
    "maintainer": "Alessandro Santuz <alessandro.santuz@gmail.com>",
    "url": "https://github.com/alesantuz/musclesyneRgies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18133,
    "package_name": "mvdalab",
    "title": "Multivariate Data Analysis Laboratory",
    "description": "An open-source implementation of latent variable methods and multivariate modeling tools. The focus is on exploratory analyses using dimensionality reduction methods including low dimensional embedding, classical multivariate statistical tools, and tools for enhanced interpretation of machine learning methods (i.e. intelligible models to provide important information for end-users).   Target domains include extension to dedicated applications e.g. for manufacturing process modeling, spectroscopic analyses, and data mining.",
    "version": "1.7",
    "maintainer": "Nelson Lee Afanador <nelson.afanador@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18191,
    "package_name": "nFunNN",
    "title": "Nonlinear Functional Principal Component Analysis using Neural\nNetworks",
    "description": "Implementation for 'nFunNN' method, which is a novel nonlinear functional principal component analysis method using neural networks. The crucial function of this package is nFunNNmodel().",
    "version": "1.0",
    "maintainer": "Rou Zhong <zhong_rou@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18235,
    "package_name": "nat.nblast",
    "title": "NeuroAnatomy Toolbox ('nat') Extension for Assessing Neuron\nSimilarity and Clustering",
    "description": "Extends package 'nat' (NeuroAnatomy Toolbox) by providing a\n    collection of NBLAST-related functions for neuronal morphology comparison (Costa et al. (2016) <doi: 10.1016/j.neuron.2016.06.012>).",
    "version": "1.6.8",
    "maintainer": "Gregory Jefferis <jefferis@gmail.com>",
    "url": "https://natverse.org/nat.nblast/, http://natverse.org/nat.nblast/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18274,
    "package_name": "ncvreg",
    "title": "Regularization Paths for SCAD and MCP Penalized Regression\nModels",
    "description": "Fits regularization paths for linear regression, GLM, and Cox\n  regression models using lasso or nonconvex penalties, in particular the\n  minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD)\n  penalty, with options for additional L2 penalties (the \"elastic net\" idea).\n  Utilities for carrying out cross-validation as well as post-fitting\n  visualization, summarization, inference, and prediction are also provided.\n  For more information, see Breheny and Huang (2011) <doi:10.1214/10-AOAS388>\n  or visit the ncvreg homepage <https://pbreheny.github.io/ncvreg/>.",
    "version": "3.16.0",
    "maintainer": "Patrick Breheny <patrick-breheny@uiowa.edu>",
    "url": "https://pbreheny.github.io/ncvreg/,\nhttps://github.com/pbreheny/ncvreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18279,
    "package_name": "ndl",
    "title": "Naive Discriminative Learning",
    "description": "Naive discriminative learning implements learning and\n    classification models based on the Rescorla-Wagner equations and their\n    equilibrium equations.",
    "version": "0.2.18",
    "maintainer": "Tino Sering <konstantin.sering@uni-tuebingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18287,
    "package_name": "neatmaps",
    "title": "Heatmaps for Multiple Network Data",
    "description": "Simplify the exploratory data analysis process for multiple network\n             data sets with the help of hierarchical clustering, consensus \n             clustering and heatmaps. Multiple network data consists of multiple\n             disjoint networks that have common variables (e.g. ego networks). \n             This package contains the necessary tools for exploring such data,\n             from the data pre-processing stage to the creation of dynamic\n             visualizations.",
    "version": "2.1.0",
    "maintainer": "Philippe Boileau <philippe_boileau@berkeley.edu>",
    "url": "https://github.com/PhilBoileau/neatmaps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18294,
    "package_name": "neighbr",
    "title": "Classification, Regression, Clustering with K Nearest Neighbors",
    "description": "Classification, regression, and clustering with k nearest neighbors\n    algorithm. Implements several distance and similarity measures, covering\n    continuous and logical features. Outputs ranked neighbors. Most features of\n    this package are directly based on the PMML specification for KNN.",
    "version": "1.0.3",
    "maintainer": "Dmitriy Bolotov <dmitriy.bolotov@softwareag.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18316,
    "package_name": "nestedcv",
    "title": "Nested Cross-Validation with 'glmnet' and 'caret'",
    "description": "Implements nested k*l-fold cross-validation for lasso and elastic-net regularised linear models via the 'glmnet' package and other machine learning models via the 'caret' package <doi:10.1093/bioadv/vbad048>. Cross-validation of 'glmnet' alpha mixing parameter and embedded fast filter functions for feature selection are provided. Described as double cross-validation by Stone (1977) <doi:10.1111/j.2517-6161.1977.tb01603.x>. Also implemented is a method using outer CV to measure unbiased model performance metrics when fitting Bayesian linear and logistic regression shrinkage models using the horseshoe prior over parameters to encourage a sparse model as described by Piironen & Vehtari (2017) <doi:10.1214/17-EJS1337SI>.",
    "version": "0.8.0",
    "maintainer": "Myles Lewis <myles.lewis@qmul.ac.uk>",
    "url": "https://github.com/myles-lewis/nestedcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18321,
    "package_name": "netClust",
    "title": "Model-Based Clustering of Network Data",
    "description": "Clustering unilayer and multilayer network data by means of finite mixtures is the main utility of 'netClust'.",
    "version": "1.0.1",
    "maintainer": "Shuchismita Sarkar <ssarkar@bgsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18326,
    "package_name": "netShiny",
    "title": "Tool for Comparison and Visualization of Multiple Networks",
    "description": "We developed a comprehensive tool that helps with visualization and analysis of networks with the same variables across multiple factor levels. The 'netShiny' contains most of the popular network features such as centrality measures, modularity, and other summary statistics (e.g. clustering coefficient). It also contains known tools to look at the (dis)similarities between two networks, such as pairwise distance measures between networks, set operations on the nodes of the networks, distribution of the weights of the edges and a network representing the difference between two correlation matrices. The package 'netShiny' also contains tools to perform bootstrapping and find clusters in networks. See the 'netShiny' manual for more information, documentation and examples.",
    "version": "1.0",
    "maintainer": "Pariya Behrouzi <pariya.behrouzi@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18332,
    "package_name": "netcom",
    "title": "NETwork COMparison Inference",
    "description": "Infer system functioning with empirical NETwork COMparisons. These methods are part of a growing paradigm in network science that uses relative comparisons of networks to infer mechanistic classifications and predict systemic interventions. They have been developed and applied in Langendorf and Burgess (2021) <doi:10.1038/s41598-021-99251-7>, Langendorf (2020) <doi:10.1201/9781351190831-6>, and Langendorf and Goldberg (2019) <doi:10.48550/arXiv.1912.12551>.",
    "version": "2.1.7",
    "maintainer": "Ryan Langendorf <ryan.langendorf@colorado.edu>",
    "url": "https://github.com/langendorfr/netcom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18371,
    "package_name": "neuralGAM",
    "title": "Interpretable Neural Network Based on Generalized Additive\nModels",
    "description": "Neural Additive Model framework based on Generalized Additive Models from Hastie & Tibshirani (1990, ISBN:9780412343902), which trains a different neural network to estimate the contribution of each feature to the response variable. The networks are trained independently leveraging the local scoring and backfitting algorithms to ensure that the Generalized Additive Model converges and it is additive. The resultant Neural Network is a highly accurate and interpretable deep learning model, which can be used for high-risk AI practices where decision-making should be based on accountable and interpretable algorithms. ",
    "version": "2.0.1",
    "maintainer": "Ines Ortega-Fernandez <iortega@gradiant.org>",
    "url": "https://inesortega.github.io/neuralGAM/,\nhttps://github.com/inesortega/neuralGAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18372,
    "package_name": "neuralnet",
    "title": "Training of Neural Networks",
    "description": "Training of neural networks using backpropagation,\n    resilient backpropagation with (Riedmiller, 1994) or without\n    weight backtracking (Riedmiller and Braun, 1993) or the\n    modified globally convergent version by Anastasiadis et al.\n    (2005). The package allows flexible settings through\n    custom-choice of error and activation function. Furthermore,\n    the calculation of generalized weights (Intrator O & Intrator\n    N, 1993) is implemented.",
    "version": "1.44.2",
    "maintainer": "Marvin N. Wright <wright@leibniz-bips.de>",
    "url": "https://github.com/bips-hb/neuralnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18381,
    "package_name": "neuromplex",
    "title": "Neural Multiplexing Analysis",
    "description": "Statistical methods for whole-trial and time-domain analysis of single cell neural response to multiple stimuli presented simultaneously. The package is based on the paper by C Glynn, ST Tokdar, A Zaman, VC Caruso, JT Mohl, SM Willett, and JM Groh (2021) \"Analyzing second order stochasticity of neural spiking under stimuli-bundle exposure\", is in press for publication by the Annals of Applied Statistics. A preprint may be found at <arXiv:1911.04387>.",
    "version": "1.0-1",
    "maintainer": "Surya Tokdar <surya.tokdar@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18406,
    "package_name": "nftbart",
    "title": "Nonparametric Failure Time Bayesian Additive Regression Trees",
    "description": "Nonparametric Failure Time (NFT) Bayesian Additive Regression Trees (BART): Time-to-event Machine Learning with Heteroskedastic Bayesian Additive Regression Trees (HBART) and Low Information Omnibus (LIO) Dirichlet Process Mixtures (DPM). An NFT BART model is of the form Y = mu + f(x) + sd(x) E where functions f and sd have BART and HBART priors, respectively, while E is a nonparametric error distribution due to a DPM LIO prior hierarchy. See the following for a description of the model at <doi:10.1111/biom.13857>.",
    "version": "2.3",
    "maintainer": "Rodney Sparapani <rsparapa@mcw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18417,
    "package_name": "nhm",
    "title": "Non-Homogeneous Markov and Hidden Markov Multistate Models",
    "description": "Fits non-homogeneous Markov multistate models and misclassification-type hidden Markov models in continuous time to intermittently observed data. Implements the methods in Titman (2011) <doi:10.1111/j.1541-0420.2010.01550.x>. Uses direct numerical solution of the Kolmogorov forward equations to calculate the transition probabilities.",
    "version": "0.1.2",
    "maintainer": "Andrew Titman <a.titman@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18430,
    "package_name": "nifti.io",
    "title": "Read and Write NIfTI Files",
    "description": "Tools for reading and writing NIfTI-1.1 (NII) files, including optimized voxelwise read/write operations and a simplified method to write dataframes to NII.\n    Specification of the NIfTI-1.1 format can be found here <https://nifti.nimh.nih.gov/nifti-1>.\n    Scientific publication first using these tools\n        Koscik TR, Man V, Jahn A, Lee CH, Cunningham WA (2020) <doi:10.1016/j.neuroimage.2020.116764> \"Decomposing the neural pathways in a simple, value-based choice.\" Neuroimage, 214, 116764. ",
    "version": "1.0.0",
    "maintainer": "Timothy Koscik <timothy-koscik@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18454,
    "package_name": "nlcv",
    "title": "Nested Loop Cross Validation",
    "description": "Nested loop cross validation for classification purposes for misclassification error rate estimation.\n  The package supports several methodologies for feature selection: random forest, Student t-test, limma, \n  and provides an interface to the following classification methods in the 'MLInterfaces' package: linear, \n  quadratic discriminant analyses, random forest, bagging, prediction analysis for microarray, generalized \n  linear model, support vector machine (svm and ksvm). Visualizations to assess the quality of\n  the classifier are included: plot of the ranks of the features, scores plot for a specific \n  classification algorithm and number of features, misclassification rate \n  for the different number of features and classification algorithms tested and ROC plot.\n  For further details about the methodology, please check:\n  Markus Ruschhaupt, Wolfgang Huber, Annemarie Poustka, and Ulrich Mansmann (2004) \n  <doi:10.2202/1544-6115.1078>.",
    "version": "0.3.6",
    "maintainer": "Laure Cougnaud <laure.cougnaud@openanalytics.eu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18506,
    "package_name": "nn2poly",
    "title": "Neural Network Weights Transformation into Polynomial\nCoefficients",
    "description": "Implements a method that builds the coefficients of a polynomial\n    model that performs almost equivalently as a given neural network\n    (densely connected). This is achieved using Taylor expansion at the\n    activation functions.  The obtained polynomial coefficients can be used\n    to explain features (and their interactions) importance  in the neural network,\n    therefore working as a tool for interpretability or eXplainable Artificial \n    Intelligence (XAI). See Morala et al. 2021 <doi:10.1016/j.neunet.2021.04.036>,\n    and 2023 <doi:10.1109/TNNLS.2023.3330328>.",
    "version": "0.1.3",
    "maintainer": "Pablo Morala <moralapablo@gmail.com>",
    "url": "https://ibidat.github.io/nn2poly/,\nhttps://github.com/IBiDat/nn2poly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18509,
    "package_name": "nnR",
    "title": "Neural Networks Made Algebraic",
    "description": "Do algebraic operations on neural networks. We seek here to implement\n  in R, operations on neural networks and their resulting approximations. Our operations derive\n  their descriptions mainly from\n  Rafi S., Padgett, J.L., and Nakarmi, U. (2024), \"Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials\", <doi:10.48550/arXiv.2402.01058>, \n  Grohs P., Hornung, F., Jentzen, A. et al. (2023), \"Space-time error estimates for deep neural network approximations for differential equations\", <doi:10.1007/s10444-022-09970-2>,\n  Jentzen A., Kuckuck B., von Wurstemberger, P. (2023), \"Mathematical Introduction to Deep Learning Methods, Implementations, and Theory\" <doi:10.48550/arXiv.2310.20360>.\n  Our implementation is meant mainly as a pedagogical tool, and proof of concept. Faster implementations with \n  deeper vectorizations may be made in future versions. ",
    "version": "0.1.0",
    "maintainer": "Shakil Rafi <sarafi@uark.edu>",
    "url": "https://github.com/2shakilrafi/nnR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18514,
    "package_name": "nndiagram",
    "title": "Generator of 'LaTeX' Code for Drawing Neural Network Diagrams\nwith 'TikZ'",
    "description": "Generates 'LaTeX' code for drawing well-formatted neural network diagrams with 'TikZ'. Users have to define number of neurons on each layer, and optionally define neuron connections they would like to keep or omit, layers they consider to be oversized and neurons they would like to draw with lighter color. They can also specify the title of diagram, color, opacity of figure, labels of layers, input and output neurons. In addition, this package helps to produce 'LaTeX' code for drawing activation functions which are crucial in neural network analysis. To make the code work in a 'LaTeX' editor, users need to install and import some 'TeX' packages including 'TikZ' in the setting of 'TeX' file.",
    "version": "1.0.0",
    "maintainer": "Chencheng Fang <ccfang@uni-bonn.de>",
    "url": "https://github.com/ccfang2/nndiagram",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18515,
    "package_name": "nnet",
    "title": "Feed-Forward Neural Networks and Multinomial Log-Linear Models",
    "description": "Software for feed-forward neural networks with a single\n  hidden layer, and for multinomial log-linear models.",
    "version": "7.3-20",
    "maintainer": "Brian Ripley <Brian.Ripley@R-project.org>",
    "url": "http://www.stats.ox.ac.uk/pub/MASS4/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18520,
    "package_name": "nnlib2Rcpp",
    "title": "A Tool for Creating Custom Neural Networks in C++ and using Them\nin R",
    "description": "Contains a module to define neural networks from custom components and versions of Autoencoder, BP, LVQ, MAM NN.",
    "version": "0.2.9",
    "maintainer": "Vasilis Nikolaidis <v.nikolaidis@uop.gr>",
    "url": "https://github.com/VNNikolaidis/nnlib2Rcpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18529,
    "package_name": "node2vec",
    "title": "Algorithmic Framework for Representational Learning on Graphs",
    "description": "Given any graph, the 'node2vec' algorithm can learn continuous feature representations for the nodes, which can then be used for various downstream machine learning tasks.The techniques are detailed in the paper \"node2vec: Scalable Feature Learning for Networks\" by Aditya Grover, Jure Leskovec(2016),available at <arXiv:1607.00653>.",
    "version": "0.1.0",
    "maintainer": "Yang Tian <tianyang1211@126.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18534,
    "package_name": "noisemodel",
    "title": "Noise Models for Classification Datasets",
    "description": "Implementation of models for the controlled introduction of errors in \n\tclassification datasets. This package contains the noise models described in \n\tSaez (2022) <doi:10.3390/math10203736> that allow corrupting class labels, \n\tattributes and both simultaneously.",
    "version": "1.0.2",
    "maintainer": "José A. Sáez <joseasaezm@ugr.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18536,
    "package_name": "noisySBM",
    "title": "Noisy Stochastic Block Mode: Graph Inference by Multiple Testing",
    "description": "Variational Expectation-Maximization algorithm to fit the noisy stochastic block model to an observed dense graph \n    and to perform a node clustering. Moreover, a graph inference procedure to recover the underlying \n    binary graph. This procedure comes with a control of the false discovery rate. The method is described\n    in the article \"Powerful graph inference with false discovery rate control\" by T. Rebafka, \n    E. Roquain, F. Villers (2020) <arXiv:1907.10176>.",
    "version": "0.1.4",
    "maintainer": "Tabea Rebafka <tabea.rebafka@sorbonne-universite.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18542,
    "package_name": "nomclust",
    "title": "Hierarchical Cluster Analysis of Nominal Data",
    "description": "Similarity measures for hierarchical clustering of objects characterized by\n    nominal (categorical) variables. Evaluation criteria for nominal data clustering.",
    "version": "2.8.1",
    "maintainer": "Zdenek Sulc <zdenek.sulc@vse.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18590,
    "package_name": "nose",
    "title": "Classification of Sparseness in 2-by-2 Categorical Data",
    "description": "Provides functions for classifying sparseness in 2 x 2 categorical data where one or more cells have zero counts. The classification uses three widely applied summary measures: Risk Difference (RD), Relative Risk (RR), and Odds Ratio (OR). Helps in selecting suitable continuity corrections for zero cells in multi-centre or meta-analysis studies. Also supports sensitivity analysis and can detect phenomena such as Simpson's paradox. The methodology is based on Subbiah and Srinivasan (2008) <doi:10.1016/j.spl.2008.06.023>.",
    "version": "1.0.5",
    "maintainer": "Subbiah M <sisufive@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18618,
    "package_name": "npcs",
    "title": "Neyman-Pearson Classification via Cost-Sensitive Learning",
    "description": "We connect the multi-class Neyman-Pearson classification (NP) problem to the cost-sensitive learning (CS) problem, and propose two algorithms (NPMC-CX and NPMC-ER) to solve the multi-class NP problem through cost-sensitive learning tools. Under certain conditions, the two algorithms are shown to satisfy multi-class NP properties. More details are available in the paper \"Neyman-Pearson Multi-class Classification via Cost-sensitive Learning\" (Ye Tian and Yang Feng, 2021).",
    "version": "0.1.1",
    "maintainer": "Ching-Tsung Tsai <tctsung@nyu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18652,
    "package_name": "nricens",
    "title": "NRI for Risk Prediction Models with Time to Event and Binary\nResponse Data",
    "description": "Calculating the net reclassification improvement (NRI) for risk prediction models with time to event and binary data.",
    "version": "1.6",
    "maintainer": "Eisuke Inoue <eisuke.inoue@marianna-u.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18722,
    "package_name": "obliqueRSF",
    "title": "Oblique Random Forests for Right-Censored Time-to-Event Data",
    "description": "Oblique random survival forests incorporate linear combinations of input variables into random survival forests (Ishwaran, 2008 <DOI:10.1214/08-AOAS169>). Regularized Cox proportional hazard models (Simon, 2016 <DOI:10.18637/jss.v039.i05>) are used to identify optimal linear combinations of input variables. ",
    "version": "0.1.2",
    "maintainer": "Byron Jaeger <bjaeger@wakehealth.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18730,
    "package_name": "occupationMeasurement",
    "title": "Interactively Measure Occupations in Interviews and Beyond",
    "description": "Perform interactive occupation coding during interviews as\n    described in Peycheva, D., Sakshaug, J., Calderwood, L. (2021) <doi:10.2478/jos-2021-0042>\n    and Schierholz, M., Gensicke, M., Tschersich, N., Kreuter, F. (2018) <doi:10.1111/rssa.12297>.\n    Generate suggestions for occupational categories based on free text\n    input, with pre-trained machine learning models in German and a ready-to-use\n    shiny application provided for quick and easy data collection.",
    "version": "0.3.2",
    "maintainer": "Jan Simson <jan.simson@lmu.de>",
    "url": "https://occupationMeasurement.github.io/occupationMeasurement/,\nhttps://github.com/occupationMeasurement/occupationMeasurement",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18740,
    "package_name": "ocf",
    "title": "Ordered Correlation Forest",
    "description": "Machine learning estimator specifically optimized for predictive modeling of ordered non-numeric outcomes. 'ocf' provides forest-based estimation of the \n    conditional choice probabilities and the covariates’ marginal effects. Under an \"honesty\" condition, the estimates are consistent and asymptotically normal \n    and standard errors can be obtained by leveraging the weight-based representation of the random forest predictions. Please reference the use as Di Francesco (2025)\n    <doi:10.1080/07474938.2024.2429596>.",
    "version": "1.0.3",
    "maintainer": "Riccardo Di Francesco <difrancesco.riccardo96@gmail.com>",
    "url": "https://riccardo-df.github.io/ocf/,\nhttps://github.com/riccardo-df/ocf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18741,
    "package_name": "ockc",
    "title": "Order Constrained Solutions in k-Means Clustering",
    "description": "Extends 'flexclust' with an R implementation of order constrained\n  solutions in k-means clustering (Steinley and Hubert, 2008, <doi:10.1007/s11336-008-9058-z>).",
    "version": "1.1.1",
    "maintainer": "Sebastian Krey <sebastian.dev@skrey.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18742,
    "package_name": "oclust",
    "title": "Gaussian Model-Based Clustering with Outliers",
    "description": "Provides a function to detect and trim outliers in Gaussian mixture model-based clustering using methods described in Clark and McNicholas (2024) <doi:10.1007/s00357-024-09473-3>.",
    "version": "1.0.0",
    "maintainer": "Katharine M. Clark <katclark@trentu.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18746,
    "package_name": "octopucs",
    "title": "Statistical Support for Hierarchical Clusters",
    "description": "Generates n hierarchical clustering hypotheses on subsets of classifiers (usually species in community ecology studies). The n clustering hypotheses are combined to generate a generalized cluster, and computes three metrics of support. 1) The average proportion of elements conforming the group in each of the n clusters (integrity). And 2) the contamination, i.e., the average proportion of elements from other groups that enter a focal group. 3) The probability of existence of the group gives the integrity and contamination in a Bayesian approach.",
    "version": "0.1.1",
    "maintainer": "Roger Guevara <roger.guevara@inecol.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18809,
    "package_name": "omicsTools",
    "title": "Omics Data Process Toolbox",
    "description": "Processing and analyzing omics data from genomics, transcriptomics, proteomics, and metabolomics platforms. It provides functions for preprocessing, normalization, visualization, and statistical analysis, as well as machine learning algorithms for predictive modeling. 'omicsTools' is an essential tool for researchers working with high-throughput omics data in fields such as biology, bioinformatics, and medicine.The QC-RLSC (quality control–based robust LOESS signal correction) algorithm is used for normalization. Dunn et al. (2011) <doi:10.1038/nprot.2011.335>.",
    "version": "1.1.7",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18832,
    "package_name": "oneclust",
    "title": "Maximum Homogeneity Clustering for Univariate Data",
    "description": "Maximum homogeneity clustering algorithm for one-dimensional data\n    described in W. D. Fisher (1958) <doi:10.1080/01621459.1958.10501479>\n    via dynamic programming.",
    "version": "0.3.0",
    "maintainer": "Nan Xiao <me@nanx.me>",
    "url": "https://nanx.me/oneclust/, https://github.com/nanxstats/oneclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18833,
    "package_name": "onehot",
    "title": "Fast Onehot Encoding for Data.frames",
    "description": "Quickly create numeric matrices for machine learning algorithms\n    that require them. It converts factor columns into onehot vectors.",
    "version": "0.1.1",
    "maintainer": "Eric E. Graves <gravcon5@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18877,
    "package_name": "openNLP",
    "title": "Apache OpenNLP Tools Interface",
    "description": "An interface to the Apache OpenNLP tools (version 1.5.3).\n  The Apache OpenNLP library is a machine learning based toolkit for the\n  processing of natural language text written in Java.\n  It supports the most common NLP tasks, such as tokenization, sentence\n  segmentation, part-of-speech tagging, named entity extraction, chunking,\n  parsing, and coreference resolution.\n  See <https://opennlp.apache.org/> for more information.",
    "version": "0.2-7",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18918,
    "package_name": "optBiomarker",
    "title": "Estimation of Optimal Number of Biomarkers for Two-Group\nMicroarray Based Classifications at a Given Error Tolerance\nLevel for Various Classification Rules",
    "description": "Estimates optimal number of biomarkers for two-group\n        classification based on microarray data.",
    "version": "1.0-28",
    "maintainer": "Mizanur Khondoker <mizanur.khondoker@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18932,
    "package_name": "opticskxi",
    "title": "OPTICS K-Xi Density-Based Clustering",
    "description": "Density-based clustering methods are well adapted to the clustering of high-dimensional data and enable the discovery of core groups of various shapes despite large amounts of noise. This package provides a novel density-based cluster extraction method, OPTICS k-Xi, and a framework to compare k-Xi models using distance-based metrics to investigate datasets with unknown number of clusters. The vignette first introduces density-based algorithms with simulated datasets, then presents and evaluates the k-Xi cluster extraction method. Finally, the models comparison framework is described and experimented on 2 genetic datasets to identify groups and their discriminating features. The k-Xi algorithm is a novel OPTICS cluster extraction method that specifies directly the number of clusters and does not require fine-tuning of the steepness parameter as the OPTICS Xi method. Combined with a framework that compares models with varying parameters, the OPTICS k-Xi method can identify groups in noisy datasets with unknown number of clusters. Results on summarized genetic data of 1,200 patients are in Charlon T. (2019) <doi:10.13097/archive-ouverte/unige:161795>. A short video tutorial can be found at <https://www.youtube.com/watch?v=P2XAjqI5Lc4/>.",
    "version": "1.2.1",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "url": "https://gitlab.com/thomaschln/opticskxi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18949,
    "package_name": "optimus",
    "title": "Model Based Diagnostics for Multivariate Cluster Analysis",
    "description": "Assessment and diagnostics for comparing competing\n    clustering solutions, using predictive models. The main intended\n    use is for comparing clustering/classification solutions of\n    ecological data (e.g. presence/absence, counts, ordinal scores) to\n    1) find an optimal partitioning solution, 2) identify\n    characteristic species and 3) refine a classification by merging\n    clusters that increase predictive performance. However, in a more\n    general sense, this package can do the above for any set of\n    clustering solutions for i observations of j variables.",
    "version": "0.2.0",
    "maintainer": "Mitchell Lyons <mitchell.lyons@gmail.com>",
    "url": "https://github.com/mitchest/optimus/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18962,
    "package_name": "opusminer",
    "title": "OPUS Miner Algorithm for Filtered Top-k Association Discovery",
    "description": "Provides a simple R interface to the OPUS Miner algorithm (implemented in C++) for finding the top-k productive, non-redundant itemsets from transaction data.  The OPUS Miner algorithm uses the OPUS search algorithm to efficiently discover the key associations in transaction data, in the form of self-sufficient itemsets, using either leverage or lift.  See <http://i.giwebb.com/index.php/research/association-discovery/> for more information in relation to the OPUS Miner algorithm.",
    "version": "0.1-1",
    "maintainer": "Christoph Bergmeir <christoph.bergmeir@monash.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18968,
    "package_name": "orclus",
    "title": "Subspace Clustering Based on Arbitrarily Oriented Projected\nCluster Generation",
    "description": "Functions to perform subspace clustering and classification. ",
    "version": "0.2-6",
    "maintainer": "Gero Szepannek <gero.szepannek@web.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18972,
    "package_name": "orderanalyzer",
    "title": "Extracting Order Position Tables from PDF-Based Order Documents",
    "description": "Functions for extracting text and tables from \n  PDF-based order documents. It provides an n-gram-based approach for identifying \n  the language of an order document. It furthermore uses R-package 'pdftools' to \n  extract the text from an order document. In the case that the PDF document is \n  only including an image (because it is scanned document), R package 'tesseract' \n  is used for OCR. Furthermore, the package provides functionality for identifying \n  and extracting order position tables in order documents based on a clustering approach.",
    "version": "1.0.0",
    "maintainer": "Michael Scholz <michael.scholz@th-deg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18987,
    "package_name": "ordinalLBM",
    "title": "Co-Clustering of Ordinal Data via Latent Continuous Random\nVariables",
    "description": "It implements functions for simulation and estimation of the ordinal latent block model (OLBM), as described in Corneli, Bouveyron and Latouche (2019).  ",
    "version": "1.0",
    "maintainer": "Marco Corneli <marcogenni@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18999,
    "package_name": "orf",
    "title": "Ordered Random Forests",
    "description": "An implementation of the Ordered Forest estimator as developed \n    in Lechner & Okasa (2019) <arXiv:1907.02436>. The Ordered Forest flexibly\n    estimates the conditional probabilities of models with ordered categorical\n    outcomes (so-called ordered choice models). Additionally to common machine \n    learning algorithms the 'orf' package provides functions for estimating\n    marginal effects as well as statistical inference thereof and thus provides\n    similar output as in standard econometric models for ordered choice. The\n    core forest algorithm relies on the fast C++ forest implementation from\n    the 'ranger' package (Wright & Ziegler, 2017) <arXiv:1508.04409>.",
    "version": "0.1.4",
    "maintainer": "Gabriel Okasa <okasa.gabriel@gmail.com>",
    "url": "https://github.com/okasag/orf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19053,
    "package_name": "otrimle",
    "title": "Robust Model-Based Clustering",
    "description": "Performs robust cluster analysis allowing for outliers and noise that cannot be fitted by any cluster. The data are modelled by a mixture of Gaussian distributions and a noise component, which is an improper uniform  distribution covering the whole Euclidean space. Parameters are estimated by  (pseudo) maximum likelihood. This is fitted by a EM-type algorithm. See Coretto and Hennig (2016) <doi:10.1080/01621459.2015.1100996>, and Coretto and Hennig (2017) <https://jmlr.org/papers/v18/16-382.html>.",
    "version": "2.0",
    "maintainer": "Pietro Coretto <pcoretto@unisa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19065,
    "package_name": "outlierMBC",
    "title": "Sequential Outlier Identification for Model-Based Clustering",
    "description": "Sequential outlier identification for Gaussian mixture models using\n    the distribution of Mahalanobis distances. The optimal number\n    of outliers is chosen based on the dissimilarity between the theoretical and\n    observed distributions of the scaled squared sample Mahalanobis distances.\n    Also includes an extension for Gaussian linear cluster-weighted models using\n    the distribution of studentized residuals. \n    Doherty, McNicholas, and White (2025) <doi:10.48550/arXiv.2505.11668>.",
    "version": "0.0.1",
    "maintainer": "Ultán P. Doherty <dohertyu@tcd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19104,
    "package_name": "pRF",
    "title": "Permutation Significance for Random Forests",
    "description": "Estimate False Discovery Rates (FDRs) for importance metrics from\n    random forest runs.",
    "version": "1.2",
    "maintainer": "Ankur Chakravarthy <ankur.chakravarthy.10@ucl.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19190,
    "package_name": "pandemonium",
    "title": "High Dimensional Analysis in Linked Spaces",
    "description": "A 'shiny' GUI that performs high dimensional cluster analysis. \n  This tool performs data preparation, clustering and visualisation within a dynamic GUI. \n  With interactive methods allowing the user to change settings all without having to to leave the GUI. \n  An earlier version of this package was described in Laa and Valencia (2022) <doi:10.1140/epjp/s13360-021-02310-1>.",
    "version": "0.2.4",
    "maintainer": "Gabriel McCoy <gabe.mccoy02@gmail.com>",
    "url": "https://gabrielmccoy.github.io/pandemonium/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19223,
    "package_name": "parallelpam",
    "title": "Parallel Partitioning-Around-Medoids (PAM) for Big Sets of Data",
    "description": "Application of the Partitioning-Around-Medoids (PAM) clustering algorithm described in Schubert, E. and Rousseeuw, P.J.:\n        \"Fast and eager k-medoids clustering: O(k) runtime improvement of the PAM, CLARA, and CLARANS algorithms.\" Information Systems,\n        vol. 101, p. 101804, (2021). <doi:10.1016/j.is.2021.101804>.\n\tIt uses a binary format for storing and retrieval of matrices developed for the 'jmatrix' package but the functionality of 'jmatrix'\n\tis included here, so you do not need to install it. Also, it is used by package 'scellpam', so if you have installed it, you do not need\n\tto install this package.\n\tPAM can be applied to sets of data whose dissimilarity matrix can be very big. It has been tested with up to 100.000 points.\n\tIt does this with the help of the code developed for other package, 'jmatrix', which allows the matrix not to be loaded in 'R' memory (which\n\twould force it to be of double type) but it gets from disk, which allows using float (or even smaller data types). Moreover, the\n\tdissimilarity matrix is calculated in parallel if the computer has several cores so it can open many threads. The initial part\n\tof the PAM algorithm can be done with the BUILD or LAB algorithms; the BUILD algorithm has been implemented in parallel. The optimization\n\tphase implements the FastPAM1 algorithm, also in parallel. Finally, calculation of silhouette is available and also implemented in parallel.",
    "version": "1.4.3",
    "maintainer": "Juan Domingo <Juan.Domingo@uv.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19235,
    "package_name": "paramtest",
    "title": "Run a Function Iteratively While Varying Parameters",
    "description": "Run simulations or other functions while easily varying parameters\n    from one iteration to the next. Some common use cases would be grid search\n    for machine learning algorithms, running sets of simulations (e.g.,\n    estimating statistical power for complex models), or bootstrapping under\n    various conditions. See the 'paramtest' documentation for more information\n    and examples.",
    "version": "0.1.1",
    "maintainer": "Jeffrey Hughes <jeff.hughes@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19267,
    "package_name": "partitionComparison",
    "title": "Implements Measures for the Comparison of Two Partitions",
    "description": "Provides several measures ((dis)similarity, distance/metric,\n    correlation, entropy) for comparing two partitions of the same set of\n    objects. The different measures can be assigned to three different\n    classes: Pair comparison (containing the famous Jaccard and Rand\n    indices), set based, and information theory based.\n    Many of the implemented measures can be found in\n    Albatineh AN, Niewiadomska-Bugaj M and Mihalko D (2006)\n    <doi:10.1007/s00357-006-0017-z> and\n    Meila M (2007) <doi:10.1016/j.jmva.2006.11.013>.\n    Partitions are represented by vectors of class labels which allow a\n    straightforward integration with existing clustering algorithms\n    (e.g. kmeans()). The package is mostly based on the S4 object system.",
    "version": "0.2.6",
    "maintainer": "Fabian Ball <mail@fabian-ball.de>",
    "url": "https://github.com/KIT-IISM-EM/partitionComparison",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19280,
    "package_name": "passt",
    "title": "Probability Associator Time (PASS-T)",
    "description": "Simulates judgments of frequency and duration based on\n    the Probability Associator Time (PASS-T) model. PASS-T is a memory\n    model based on a simple competitive artificial neural network. It \n    can imitate human judgments of frequency and duration, which have\n    been extensively studied in cognitive psychology\n    (e.g. Hintzman (1970) <doi:10.1037/h0028865>, Betsch et al. (2010)\n    <https://psycnet.apa.org/record/2010-18204-003>). The PASS-T model\n    is an extension of the PASS model (Sedlmeier, 2002,\n    ISBN:0198508638). The package provides an easy way to run\n    simulations, which can then be compared with empirical data in\n    human judgments of frequency and duration.",
    "version": "0.1.3",
    "maintainer": "Johannes Titz <johannes.titz@gmail.com>",
    "url": "https://github.com/johannes-titz/passt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19328,
    "package_name": "paws.machine.learning",
    "title": "'Amazon Web Services' Machine Learning Services",
    "description": "Interface to 'Amazon Web Services' machine learning services,\n    including 'SageMaker' managed machine learning service, natural\n    language processing, speech recognition, translation, and more\n    <https://aws.amazon.com/machine-learning/>.",
    "version": "0.9.0",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "url": "https://github.com/paws-r/paws,\nhttps://paws-r.r-universe.dev/paws.machine.learning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19365,
    "package_name": "pccc",
    "title": "Pediatric Complex Chronic Conditions",
    "description": "An implementation of the pediatric complex chronic conditions (CCC)\n    classification system using R and C++.",
    "version": "1.0.6",
    "maintainer": "Seth Russell <seth.russell@cuanschutz.edu>",
    "url": "https://github.com/CUD2V/pccc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19389,
    "package_name": "pdSpecEst",
    "title": "An Analysis Toolbox for Hermitian Positive Definite Matrices",
    "description": "An implementation of data analysis tools for samples of symmetric or \n  Hermitian positive definite matrices, such as collections of covariance matrices \n  or spectral density matrices. The tools in this package can be used to perform: (i) \n  intrinsic wavelet transforms for curves (1D) or surfaces (2D) of Hermitian positive \n  definite matrices with applications to dimension reduction, denoising and clustering in the \n  space of Hermitian positive definite matrices; and (ii) exploratory data analysis and inference \n  for samples of positive definite matrices by means of intrinsic data depth functions and \n  rank-based hypothesis tests in the space of Hermitian positive definite matrices.",
    "version": "1.2.6",
    "maintainer": "Joris Chau <joris.chau@openanalytics.eu>",
    "url": "https://github.com/JorisChau/pdSpecEst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19390,
    "package_name": "pda",
    "title": "Privacy-Preserving Distributed Algorithms",
    "description": "A collection of privacy-preserving distributed algorithms (PDAs) for conducting federated statistical learning across multiple data sites. The PDA framework includes models for various tasks such as regression, trial emulation, causal inference, design-specific analysis, and clustering. The PDA algorithms run on a lead site and only require summary statistics from collaborating sites, with one or few iterations. The package can be used together with the online data transfer system (<https://pda-ota.pdamethods.org/>) for safe and convenient collaboration. For more information, please visit our software websites: <https://github.com/Penncil/pda>, and <https://pdamethods.org/>.",
    "version": "1.3.0",
    "maintainer": "Chongliang Luo <luocl3009@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19394,
    "package_name": "pdfCluster",
    "title": "Cluster Analysis via Nonparametric Density Estimation",
    "description": "Cluster analysis via nonparametric density \n   estimation is performed. Operationally, the kernel method is used throughout to estimate\n   the density. Diagnostics methods for evaluating the quality of the clustering \n   are available. The package includes also a routine to estimate the \n   probability density function obtained by the kernel method, given a set of\n   data with arbitrary dimensions.",
    "version": "1.0-4",
    "maintainer": "Menardi Giovanna <menardi@stat.unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19446,
    "package_name": "penalizedSVM",
    "title": "Feature Selection SVM using Penalty Functions",
    "description": "Support Vector Machine (SVM) classification with simultaneous feature selection using penalty\n        functions is implemented. The smoothly clipped absolute deviation (SCAD),\n        'L1-norm', 'Elastic Net' ('L1-norm' and 'L2-norm') and 'Elastic\n        SCAD' (SCAD and 'L2-norm') penalties are available. The tuning\n        parameters can be found using either a fixed grid or a interval\n        search.",
    "version": "1.2.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://github.com/fbertran/penalizedSVM,\nhttps://fbertran.github.io/penalizedSVM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19468,
    "package_name": "peppm",
    "title": "Piecewise Exponential Distribution with Random Time Grids",
    "description": "Fits the Piecewise Exponential distribution with random time grids using the clustering structure of the Product Partition Models. Details of the implemented model can be found in Demarqui et al. (2008) <doi:10.1007/s10985-008-9086-0>.",
    "version": "0.0.1",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "url": "https://github.com/fndemarqui/peppm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19470,
    "package_name": "peptoolkit",
    "title": "A Toolkit for Using Peptide Sequences in Machine Learning",
    "description": "This toolkit is designed for manipulation and analysis of peptides. It provides functionalities to assist researchers in peptide engineering and proteomics. Users can manipulate peptides by adding amino acids at every position, count occurrences of each amino acid at each position, and transform amino acid counts based on probabilities. The package offers functionalities to select the best versus the worst peptides and analyze these peptides, which includes counting specific residues, reducing peptide sequences, extracting features through One Hot Encoding (OHE), and utilizing Quantitative Structure-Activity Relationship (QSAR) properties (based in the package 'Peptides' by Osorio et al. (2015) <doi:10.32614/RJ-2015-001>). This package is intended for both researchers and bioinformatics enthusiasts working on peptide-based projects, especially for their use with machine learning.",
    "version": "0.0.1",
    "maintainer": "Josep-Ramon Codina <jrc356@miami.edu>",
    "url": "https://github.com/jrcodina/peptoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19482,
    "package_name": "permimp",
    "title": "Conditional Permutation Importance",
    "description": "An add-on to the 'party' package, with a faster implementation \n   of the partial-conditional permutation importance for random forests. The \n   standard permutation importance is implemented exactly the same as in \n   the 'party' package. The conditional permutation importance can be \n   computed faster, with an option to be backward compatible to the 'party' \n   implementation. The package is compatible with random forests fit using the \n   'party' and the 'randomForest' package. The methods are described in\n   Strobl et al. (2007) <doi:10.1186/1471-2105-8-25> and \n   Debeer and Strobl (2020) <doi:10.1186/s12859-020-03622-2>.",
    "version": "1.1-0",
    "maintainer": "Dries Debeer <debeer.dries@gmail.com>",
    "url": "https://ddebeer.github.io/permimp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19526,
    "package_name": "pgmm",
    "title": "Parsimonious Gaussian Mixture Models",
    "description": "Carries out model-based clustering or classification using parsimonious Gaussian mixture models. McNicholas and Murphy (2008) <doi:10.1007/s11222-008-9056-0>, McNicholas (2010) <doi:10.1016/j.jspi.2009.11.006>, McNicholas and Murphy (2010) <doi:10.1093/bioinformatics/btq498>, McNicholas et al. (2010) <doi:10.1016/j.csda.2009.02.011>.",
    "version": "1.2.8",
    "maintainer": "Paul D. McNicholas <mcnicholas@math.mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19600,
    "package_name": "phscs",
    "title": "Philippine Statistical Classification Systems",
    "description": "A unified interface to access and manipulate various Philippine statistical classifications. It allows users to retrieve, filter, and harmonize classification data, making it easier to work with Philippine statistical data in R.",
    "version": "0.1.0",
    "maintainer": "Bhas Abdulsamad <aeabdulsamad@gmail.com>",
    "url": "https://yng-me.github.io/phscs/, https://github.com/yng-me/phscs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19604,
    "package_name": "phutil",
    "title": "Persistence Homology Utilities",
    "description": "A low-level package for hosting persistence data. It is part of the\n    'TDAverse' suite of packages, which is designed to provide a collection of\n    packages for enabling machine learning and data science tasks using\n    persistent homology. Implements a class for hosting persistence data, a\n    number of coercers from and to already existing and used data structures\n    from other packages and functions to compute distances between persistence\n    diagrams. A formal definition and study of bottleneck and Wasserstein\n    distances can be found in Bubenik, Scott and Stanley (2023)\n    <doi:10.1007/s41468-022-00103-8>. Their implementation in 'phutil' relies on\n    the 'C++' Hera library developed by Kerber, Morozov and Nigmetov (2017)\n    <doi:10.1145/3064175>.",
    "version": "0.0.1",
    "maintainer": "Aymeric Stamm <aymeric.stamm@cnrs.fr>",
    "url": "https://github.com/tdaverse/phutil,\nhttps://tdaverse.github.io/phutil/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19646,
    "package_name": "piecemaker",
    "title": "Tools for Preparing Text for Tokenizers",
    "description": "Tokenizers break text into pieces that are more usable by\n    machine learning models. Many tokenizers share some preparation steps.\n    This package provides those shared steps, along with a simple\n    tokenizer.",
    "version": "1.0.2",
    "maintainer": "Jon Harmon <jonthegeek@gmail.com>",
    "url": "https://github.com/macmillancontentscience/piecemaker,\nhttps://macmillancontentscience.github.io/piecemaker/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19683,
    "package_name": "pivmet",
    "title": "Pivotal Methods for Bayesian Relabelling and k-Means Clustering",
    "description": "Collection of pivotal algorithms \n             for: relabelling the MCMC chains in order to undo the label \n             switching problem in Bayesian mixture models;\n             fitting sparse finite mixtures;\n             initializing the centers of the classical k-means algorithm \n             in order to obtain a better clustering solution. \n             For further details see\n             Egidi, Pappadà, Pauli and Torelli (2018b)<ISBN:9788891910233>.",
    "version": "0.6.0",
    "maintainer": "Leonardo Egidi <legidi@units.it>",
    "url": "https://github.com/leoegidi/pivmet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19750,
    "package_name": "pleLMA",
    "title": "Pseudo-Likelihood Estimation of Log-Multiplicative Association\nModels",
    "description": "Log-multiplicative association models (LMA) are\n    models for cross-classifications of categorical variables\n    where interactions are represented by products of category\n    scale values and an association parameter. Maximum\n    likelihood estimation (MLE) fails for moderate to large\n    numbers of categorical variables. The 'pleLMA' package\n    overcomes this limitation of MLE by using pseudo-likelihood\n    estimation to fit the models to small or large\n    cross-classifications dichotomous or multi-category variables.\n    Originally proposed by Besag (1974,\n    <doi:10.1111/j.2517-6161.1974.tb00999.x>), pseudo-likelihood\n    estimation takes large complex models and breaks it down\n    into smaller ones. Rather than maximizing the likelihood\n    of the joint distribution of all the variables, a\n    pseudo-likelihood function, which is the product likelihoods\n    from conditional distributions, is maximized. LMA models can\n    be derived from a number of different frameworks including\n    (but not limited to) graphical models and uni-dimensional\n    and multi-dimensional item response theory models. More\n    details about the models and estimation can be found in\n    the vignette.",
    "version": "0.2.2",
    "maintainer": "Carolyn J. Anderson <cja@illinois.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19757,
    "package_name": "plgp",
    "title": "Particle Learning of Gaussian Processes",
    "description": "Sequential Monte Carlo (SMC) inference for fully Bayesian\n  Gaussian process (GP) regression and classification models by\n  particle learning (PL) following Gramacy & Polson (2011) <arXiv:0909.5262>.\n  The sequential nature of inference\n  and the active learning (AL) hooks provided facilitate thrifty \n  sequential design (by entropy) and optimization\n  (by improvement) for classification and\n  regression models, respectively.\n  This package essentially provides a generic\n  PL interface, and functions (arguments to the interface) which\n  implement the GP models and AL heuristics.  Functions for \n  a special, linked, regression/classification GP model and \n  an integrated expected conditional improvement (IECI) statistic \n  provide for optimization in the presence of unknown constraints.\n  Separable and isotropic Gaussian, and single-index correlation\n  functions are supported.\n  See the examples section of ?plgp and demo(package=\"plgp\") \n  for an index of demos.",
    "version": "1.1-12",
    "maintainer": "Robert B. Gramacy <rbg@vt.edu>",
    "url": "https://bobby.gramacy.com/r_packages/plgp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19795,
    "package_name": "plotmo",
    "title": "Plot a Model's Residuals, Response, and Partial Dependence Plots",
    "description": "Plot model surfaces for a wide variety of models\n        using partial dependence plots and other techniques.\n        Also plot model residuals and other information on the model.",
    "version": "3.6.4",
    "maintainer": "Stephen Milborrow <milbo@sonic.net>",
    "url": "http://www.milbo.users.sonic.net",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19810,
    "package_name": "plsVarSel",
    "title": "Variable Selection in Partial Least Squares",
    "description": "Interfaces and methods for variable selection in Partial Least\n    Squares. The methods include filter methods, wrapper methods and embedded\n    methods. Both regression and classification is supported.",
    "version": "0.9.13",
    "maintainer": "Kristian Hovde Liland <kristian.liland@nmbu.no>",
    "url": "https://github.com/khliland/plsVarSel/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19838,
    "package_name": "pmclust",
    "title": "Parallel Model-Based Clustering using\nExpectation-Gathering-Maximization Algorithm for Finite Mixture\nGaussian Model",
    "description": "Aims to utilize model-based clustering (unsupervised)\n        for high dimensional and ultra large data, especially in a distributed\n        manner. The code employs 'pbdMPI' to perform a\n        expectation-gathering-maximization algorithm\n        for finite mixture Gaussian\n        models. The unstructured dispersion matrices are assumed in the\n        Gaussian models. The implementation is default in the single program\n        multiple data programming model. The code can be executed\n        through 'pbdMPI' and MPI' implementations such as 'OpenMPI'\n        and 'MPICH'.\n        See the High Performance Statistical Computing website\n\t<https://snoweye.github.io/hpsc/>\n\tfor more information, documents and examples.",
    "version": "0.2-1",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "url": "https://pbdr.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19845,
    "package_name": "pmlbr",
    "title": "Interface to the Penn Machine Learning Benchmarks Data\nRepository",
    "description": "Check available classification and regression data sets from the PMLB repository and download them.\n    The PMLB repository (<https://github.com/EpistasisLab/pmlbr>) contains a curated collection of data sets for evaluating and comparing machine learning algorithms.\n    These data sets cover a range of applications, and include binary/multi-class classification problems and \n    regression problems, as well as combinations of categorical, ordinal, and continuous features.\n    There are currently over 150 datasets included in the PMLB repository.",
    "version": "0.3.0",
    "maintainer": "Trang Le <grixor@gmail.com>",
    "url": "https://github.com/EpistasisLab/pmlbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19849,
    "package_name": "pmmlTransformations",
    "title": "Transforms Input Data from a PMML Perspective",
    "description": "Allows for data to be transformed before using it to construct models. Builds structures to allow functions in the PMML package to\n    output transformation details in addition to the model in the resulting PMML file. The Predictive Model Markup Language (PMML) is an XML-based language which provides a way for applications to define machine learning, statistical and data mining models and to share models between PMML compliant applications. More information about the PMML industry standard and the Data Mining Group can be found at <http://www.dmg.org>. The generated PMML can be imported into any PMML consuming application, such as Zementis Predictive Analytics products, which integrate with web services, relational database systems and deploy natively on Hadoop in conjunction with Hive, Spark or Storm, as well as allow predictive analytics to be executed for IBM z Systems mainframe applications and real-time, streaming analytics platforms.",
    "version": "1.3.3",
    "maintainer": "Dmitriy Bolotov <rpmmlsupport@softwareag.com>",
    "url": "https://www.softwareag.com/zementis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19861,
    "package_name": "pmxNODE",
    "title": "Application of NODEs in 'Monolix', 'NONMEM', and 'nlmixr2'",
    "description": "An easy-to-use tool for implementing Neural Ordinary Differential Equations (NODEs) in pharmacometric software such as 'Monolix', 'NONMEM', and 'nlmixr2', see Bräm et al. (2024) <doi:10.1007/s10928-023-09886-4> and Bräm et al. (2025) <doi:10.1002/psp4.13265>. The main functionality is to automatically generate structural model code describing computations within a neural network. Additionally, parameters and software settings can be initialized automatically. For using these additional functionalities with 'Monolix', 'pmxNODE' interfaces with 'MonolixSuite' via the 'lixoftConnectors' package. The 'lixoftConnectors' package is distributed with 'MonolixSuite' (<https://monolixsuite.slp-software.com/r-functions/2024R1/package-lixoftconnectors>) and is not available from public repositories.",
    "version": "0.1.0",
    "maintainer": "Dominic Bräm <domi.braem@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19882,
    "package_name": "poisDoubleSamp",
    "title": "Confidence Intervals with Poisson Double Sampling",
    "description": "Functions to create confidence intervals for ratios of Poisson\n    rates under misclassification using double sampling. Implementations of the \n    methods described in Kahle, D., P. Young, B. Greer, and D. Young (2016). \n    \"Confidence Intervals for the Ratio of Two Poisson Rates Under One-Way \n    Differential Misclassification Using Double Sampling.\" Computational \n    Statistics & Data Analysis, 95:122–132.",
    "version": "1.1.1",
    "maintainer": "David Kahle <david@kahle.io>",
    "url": "https://github.com/dkahle/poisDoubleSamp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19887,
    "package_name": "poisson.glm.mix",
    "title": "Fit High Dimensional Mixtures of Poisson GLMs",
    "description": "Mixtures of Poisson Generalized Linear Models for high dimensional count data clustering. The (multivariate) responses can be partitioned into set of blocks. Three different parameterizations of the linear predictor are considered. The models are estimated according to the EM algorithm with an efficient initialization scheme <doi:10.1016/j.csda.2014.07.005>. ",
    "version": "1.4",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19961,
    "package_name": "popsom7",
    "title": "A Fast, User-Friendly Implementation of Self-Organizing Maps\n(SOMs)",
    "description": "Methods for building self-organizing maps (SOMs) with a number of distinguishing features such automatic centroid detection and cluster visualization using starbursts.  For more details see the paper \"Improved Interpretability of the Unified Distance Matrix with Connected Components\" by Hamel and Brown (2011) in <ISBN:1-60132-168-6>.  The package provides user-friendly access to two models we construct: (a) a SOM model and (b) a centroid based clustering model. The package also exposes a number of quality metrics for the quantitative evaluation of the map, Hamel (2016) <doi:10.1007/978-3-319-28518-4_4>.  Finally, we reintroduced our fast, vectorized training algorithm for SOM with substantial improvements. It is about an order of magnitude faster than the canonical, stochastic C implementation <doi:10.1007/978-3-030-01057-7_60>.",
    "version": "7.1.0",
    "maintainer": "Lutz Hamel <lutzhamel@uri.edu>",
    "url": "https://github.com/lutzhamel/popsom7",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19985,
    "package_name": "postHoc",
    "title": "Tools for Post-Hoc Analysis",
    "description": "Implements a range of facilities for post-hoc analysis and\n             summarizing linear models, generalized linear models and \n             generalized linear mixed models, including grouping and clustering \n             via pairwise comparisons using graph representations and efficient\n             algorithms for finding maximal cliques of a graph. \n             Includes also non-parametric toos for post-hoc analysis.\n             It has S3 methods for printing summarizing, and producing plots, \n             line and barplots suitable for post-hoc analyses. ",
    "version": "0.1.3",
    "maintainer": "Rodrigo Labouriau <rodrigo.labouriau@math.au.dk>",
    "url": "https://tildeweb.au.dk/au33031/astatlab/software/posthoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20015,
    "package_name": "powerPLS",
    "title": "Power Analysis for PLS Classification",
    "description": "It estimates power and sample size for Partial Least Squares-based methods described in Andreella, et al., (2024), <doi:10.48550/arXiv.2403.10289>. ",
    "version": "0.2.1",
    "maintainer": "Angela Andreella <angela.andreella@unitn.it>",
    "url": "https://github.com/angeella/powerPLS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20028,
    "package_name": "ppRank",
    "title": "Classification of Algorithms",
    "description": "Implements the Bi-objective Lexicographical Classification method and Performance Assessment Ratio at 10% metric for algorithm classification. Constructs matrices representing algorithm performance under multiple criteria, facilitating decision-making in algorithm selection and evaluation. Analyzes and compares algorithm performance based on various metrics to identify the most suitable algorithms for specific tasks. This package includes methods for algorithm classification and evaluation, with examples provided in the documentation. Carvalho (2019) presents a statistical evaluation of algorithmic computational experimentation with infeasible solutions <doi:10.48550/arXiv.1902.00101>. Moreira and Carvalho (2023) analyze power in preprocessing methodologies for datasets with missing values <doi:10.1080/03610918.2023.2234683>.",
    "version": "0.1.1",
    "maintainer": "Iago Augusto de Carvalho <iago.carvalho@unifal-mg.edu.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20046,
    "package_name": "ppmf",
    "title": "Read Census Privacy Protected Microdata Files",
    "description": "Implements data processing described in <doi:10.1126/sciadv.abk3283>\n    to align modern differentially private data with formatting of older US Census\n    data releases. The primary goal is to read in Census Privacy Protected Microdata\n    Files data in a reproducible way. This includes tools for aggregating to relevant\n    levels of geography by creating geographic identifiers which match the US Census\n    Bureau's numbering. Additionally, there are tools for grouping race numeric\n    identifiers into categories, consistent with OMB (Office of Management and Budget)\n    classifications. Functions exist for downloading and linking to existing\n    sources of privacy protected microdata.",
    "version": "0.2.1",
    "maintainer": "Christopher T. Kenny <ctkenny@proton.me>",
    "url": "https://github.com/christopherkenny/ppmf/,\nhttps://christophertkenny.com/ppmf/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20071,
    "package_name": "prclust",
    "title": "Penalized Regression-Based Clustering Method",
    "description": "Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit. In this package, we provide two algorithms for fitting the penalized regression-based clustering (PRclust) with non-convex grouping penalties, such as group truncated lasso, MCP and SCAD. One algorithm is based on quadratic penalty and difference convex method. Another algorithm is based on difference convex and ADMM, called DC-ADD, which is more efficient. Generalized cross validation and stability based method were provided to select the tuning parameters. Rand index, adjusted Rand index and Jaccard index were provided to estimate the agreement between estimated cluster memberships and the truth.",
    "version": "1.3",
    "maintainer": "Chong Wu <wuxx0845@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20072,
    "package_name": "prcr",
    "title": "Person-Centered Analysis",
    "description": "Provides an easy-to-use yet adaptable set of tools to conduct person-center analysis using a two-step clustering procedure. As described in Bergman and El-Khouri (1999) <DOI:10.1002/(SICI)1521-4036(199910)41:6%3C753::AID-BIMJ753%3E3.0.CO;2-K>, hierarchical clustering is performed to determine the initial partition for the subsequent k-means clustering procedure.",
    "version": "0.2.1",
    "maintainer": "Joshua M Rosenberg <jmichaelrosenberg@gmail.com>",
    "url": "https://github.com/jrosen48/prcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20073,
    "package_name": "pre",
    "title": "Prediction Rule Ensembles",
    "description": "Derives prediction rule ensembles (PREs). Largely follows the\n    procedure for deriving PREs as described in Friedman & Popescu (2008; \n    <DOI:10.1214/07-AOAS148>), with adjustments and improvements described in \n    Fokkema (2020; <DOI:10.18637/jss.v092.i12>) and Fokkema & Strobl \n    (2020; <DOI:10.1037/met0000256>). The main function pre() derives \n    prediction rule ensembles consisting of rules and/or linear terms for \n    continuous, binary, count, multinomial, survival and multivariate \n    continuous responses. Function gpe() derives generalized prediction \n    ensembles, consisting of rules, hinge and linear functions of the \n    predictor variables.",
    "version": "1.0.8",
    "maintainer": "Marjolein Fokkema <m.fokkema@fsw.leidenuniv.nl>",
    "url": "https://github.com/marjoleinF/pre",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20085,
    "package_name": "predhy",
    "title": "Genomic Prediction of Hybrid Performance",
    "description": "Performs genomic prediction of hybrid performance using eight statistical methods including GBLUP, BayesB, RKHS, PLS, LASSO, EN, LightGBM and XGBoost along with additive and additive-dominance models. Users are able to incorporate parental phenotypic information in all methods based on their specific needs. (Xu S et al(2017) <doi:10.1534/g3.116.038059>; Xu Y et al (2021) <doi: 10.1111/pbi.13458>).",
    "version": "2.1.2",
    "maintainer": "Yang Xu <xuyang_89@126.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20086,
    "package_name": "predhy.GUI",
    "title": "Genomic Prediction of Hybrid Performance with Graphical User\nInterface",
    "description": "Performs genomic prediction of hybrid performance using eight GS methods including GBLUP, BayesB, RKHS, PLS, LASSO, Elastic net, XGBoost and LightGBM.\n             GBLUP: genomic best liner unbiased prediction, RKHS: reproducing kernel Hilbert space, PLS: partial least squares regression, LASSO: least absolute shrinkage and selection operator, XGBoost: extreme gradient boosting, LightGBM: light gradient boosting machine.\n             It also provides fast cross-validation and mating design scheme for training population (Xu S et al (2016) <doi:10.1111/tpj.13242>; Xu S (2017) <doi:10.1534/g3.116.038059>).\n\t\t\t A complete manual for this package is provided in the manual folder of the package installation directory. \n\t\t\t You can locate the manual by running the following command in R: system.file(\"manual\", package = \"predhy.GUI\").",
    "version": "2.1.1",
    "maintainer": "Yuxiang Zhang <yuxiangzhang_99@foxmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20093,
    "package_name": "predictoR",
    "title": "Predictive Data Analysis System",
    "description": "Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning, Support Vector Machines and Bayesian Methods.",
    "version": "4.1.6",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "url": "https://promidat.website/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20177,
    "package_name": "pro",
    "title": "Point-Process Response Model for Optogenetics",
    "description": "Optogenetics is a new tool to study neuronal circuits that have been genetically modified to allow stimulation by flashes of light.  This package implements the methodological framework, Point-process Response model for Optogenetics (PRO), for analyzing data from these experiments.  This method provides explicit nonlinear transformations to link the flash point-process with the spiking point-process.  Such response functions can be used to provide important and interpretable scientific insights into the properties of the biophysical process that governs neural spiking in response to optogenetic stimulation.",
    "version": "0.1.1",
    "maintainer": "Xi (Rossi) LUO <xi.rossi.luo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20240,
    "package_name": "promptr",
    "title": "Format and Complete Few-Shot LLM Prompts",
    "description": "Format and submit few-shot prompts to OpenAI's Large Language Models (LLMs). Designed to be particularly useful for text classification problems in the social sciences. Methods are described in Ornstein, Blasingame, and Truscott (2024) <https://joeornstein.github.io/publications/ornstein-blasingame-truscott.pdf>.",
    "version": "1.0.0",
    "maintainer": "Joe Ornstein <jornstein@uga.edu>",
    "url": "https://github.com/joeornstein/promptr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20243,
    "package_name": "propOverlap",
    "title": "Feature (gene) selection based on the Proportional Overlapping\nScores",
    "description": "A package for selecting the most relevant features (genes) in the high-dimensional binary classification problems. The discriminative features are identified using analyzing the overlap between the expression values across both classes. The package includes functions for measuring the proportional overlapping score for each gene avoiding the outliers effect. The used measure for the overlap is the one defined in the \"Proportional Overlapping Score (POS)\" technique for feature selection. A gene mask which represents a gene's classification power can also be produced for each gene (feature). The set size of the selected genes might be set by the user. The minimum set of genes that correctly classify the maximum number of the given tissue samples (observations) can be also produced.",
    "version": "1.0",
    "maintainer": "Osama Mahmoud <ofamah@essex.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20263,
    "package_name": "protoshiny",
    "title": "Interactive Dendrograms for Visualizing Hierarchical Clusters\nwith Prototypes",
    "description": "Shiny app to interactively visualize hierarchical clustering with prototypes. \n    For details on hierarchical clustering with prototypes, see \n    Bien and Tibshirani (2011) <doi:10.1198/jasa.2011.tm10183>. This package currently launches the application.",
    "version": "0.1.1",
    "maintainer": "Andee Kaplan <andee.kaplan@colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20308,
    "package_name": "psica",
    "title": "Decision Tree Analysis for Probabilistic Subgroup Identification\nwith Multiple Treatments",
    "description": "In the situation when multiple alternative treatments or\n    interventions available, different population groups may respond differently\n    to different treatments. This package implements a method that discovers\n    the population subgroups in which a certain treatment has a better effect\n    than the other alternative treatments. This is done by first estimating the\n    treatment effect for a given treatment and its uncertainty by computing random\n    forests, and the resulting model is summarized by a decision tree in which the\n    probabilities that the given treatment is best for a given subgroup is shown in\n    the corresponding terminal node of the tree.",
    "version": "1.0.2",
    "maintainer": "Oleg Sysoev <Oleg.Sysoev@liu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20330,
    "package_name": "psych",
    "title": "Procedures for Psychological, Psychometric, and Personality\nResearch",
    "description": "A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <https://personality-project.org/r/> web page.",
    "version": "2.5.6",
    "maintainer": "William Revelle <revelle@northwestern.edu>",
    "url": "https://personality-project.org/r/psych/\nhttps://personality-project.org/r/psych-manual.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20371,
    "package_name": "pullword",
    "title": "R Interface to Pullword Service",
    "description": "R Interface to Pullword Service for natural language processing\n    in Chinese. It enables users to extract valuable words from text by deep learning models. \n    For more details please visit the official site (in Chinese) <http://www.pullword.com/>.",
    "version": "0.3",
    "maintainer": "Tong He <hetong007@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20372,
    "package_name": "puls",
    "title": "Partitioning Using Local Subregions",
    "description": "A method of clustering functional data using\n    subregion information of the curves. It is intended to supplement the\n    'fda' and 'fda.usc' packages in functional data object clustering. It\n    also facilitates the printing and plotting of the results in a tree\n    format and limits the partitioning candidates into a specific set of\n    subregions.",
    "version": "0.1.3",
    "maintainer": "Tan Tran <vinhtantran@gmail.com>",
    "url": "https://vinhtantran.github.io/puls/,\nhttps://github.com/vinhtantran/puls",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20397,
    "package_name": "pvclust",
    "title": "Hierarchical Clustering with P-Values via Multiscale Bootstrap\nResampling",
    "description": "An implementation of multiscale bootstrap resampling for\n             assessing the uncertainty in hierarchical cluster analysis.\n             It provides SI (selective inference) p-value, AU (approximately unbiased)\n             p-value and BP (bootstrap probability) value for each cluster in a dendrogram.",
    "version": "2.2-0",
    "maintainer": "Ryota Suzuki <suzuki@ef-prime.com>",
    "url": "http://stat.sys.i.kyoto-u.ac.jp/prog/pvclust/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20411,
    "package_name": "pwrRasch",
    "title": "Statistical Power Simulation for Testing the Rasch Model",
    "description": "Statistical power simulation for testing the Rasch Model based on a three-way analysis of variance design with mixed classification.",
    "version": "0.1-2",
    "maintainer": "Takuya Yanagida <takuya.yanagida@univie.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20429,
    "package_name": "qCBA",
    "title": "Postprocessing of Rule Classification Models Learnt on Quantized\nData",
    "description": "Implements the Quantitative Classification-based on\n    Association Rules (QCBA)  algorithm (<doi:10.1007/s10489-022-04370-x>). \n    QCBA postprocesses rule classification models making them typically smaller and in some cases more accurate. \n    Supported are 'CBA' implementations from 'rCBA', 'arulesCBA' and 'arc' packages, and 'CPAR', 'CMAR', 'FOIL2' and 'PRM' implementations \n    from 'arulesCBA' package and 'SBRL' implementation from the 'sbrl' package. The result of the post-processing is an ordered CBA-like rule list. ",
    "version": "1.0.2",
    "maintainer": "Tomáš Kliegr <kliegr@gmail.com>",
    "url": "https://github.com/kliegr/QCBA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20436,
    "package_name": "qVarSel",
    "title": "Select Variables for Optimal Clustering",
    "description": "Finding hidden clusters in structured data can be hindered\n  by the presence of masking variables. If not detected,\n  masking variables are used to calculate the overall similarities between units, \n  and therefore the cluster attribution is more imprecise.\n  The algorithm q-vars implements an optimization method to find the variables\n  that most separate units between clusters. In this way, masking variables can be \n  discarded from the data frame and the clustering is more accurate.\n  Tests can be found in Benati et al.(2017) <doi:10.1080/01605682.2017.1398206>.",
    "version": "1.2",
    "maintainer": "Stefano Benati <stefano.benati@unitn.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20453,
    "package_name": "qcluster",
    "title": "Clustering via Quadratic Scoring",
    "description": "Performs tuning of clustering models, methods and algorithms including the problem of determining an appropriate number of clusters. Validation of cluster analysis results is performed via quadratic scoring using resampling methods, as in Coraggio, L. and Coretto, P. (2023) <doi:10.1016/j.jmva.2023.105181>.",
    "version": "1.2.1",
    "maintainer": "Luca Coraggio <luca.coraggio@unina.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20466,
    "package_name": "qeML",
    "title": "Quick and Easy Machine Learning Tools",
    "description": "The letters 'qe' in the package title stand for \"quick and\n   easy,\" alluding to the convenience goal of the package. We bring\n   together a variety of machine learning (ML) tools from standard R\n   packages, providing wrappers with a simple, convenient, \n   and uniform interface.",
    "version": "1.1",
    "maintainer": "Norm Matloff <nsmatloff@ucdavis.edu>",
    "url": "https://github.com/matloff/qeML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20522,
    "package_name": "qrnn",
    "title": "Quantile Regression Neural Network",
    "description": "Fit quantile regression neural network models with optional\n    left censoring, partial monotonicity constraints, generalized additive\n    model constraints, and the ability to fit multiple non-crossing quantile\n    functions following Cannon (2011) <doi:10.1016/j.cageo.2010.07.005>\n    and Cannon (2018) <doi:10.1007/s00477-018-1573-6>.",
    "version": "2.1.1",
    "maintainer": "Alex J. Cannon <alex.cannon@ec.gc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20574,
    "package_name": "quanteda",
    "title": "Quantitative Analysis of Textual Data",
    "description": "A fast, flexible, and comprehensive framework for \n    quantitative text analysis in R.  Provides functionality for corpus management,\n    creating and manipulating tokens and n-grams, exploring keywords in context, \n    forming and manipulating sparse matrices\n    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and\n    distances, applying content dictionaries, applying supervised and unsupervised machine learning, \n    visually representing text and text analyses, and more. ",
    "version": "4.3.1",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "url": "https://quanteda.io",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20590,
    "package_name": "quantregForest",
    "title": "Quantile Regression Forests",
    "description": "Quantile Regression Forests is a tree-based ensemble\n        method for estimation of conditional quantiles. It is\n        particularly well suited for high-dimensional data. Predictor\n        variables of mixed classes can be handled. The package is\n        dependent on the package 'randomForest', written by Andy Liaw.",
    "version": "1.3-7.1",
    "maintainer": "Loris Michel <michel@stat.math.ethz.ch>",
    "url": "https://github.com/lorismichel/quantregForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20592,
    "package_name": "quantregRanger",
    "title": "Quantile Regression Forests for 'ranger'",
    "description": "This is the implementation of quantile regression forests for the fast random forest package 'ranger'.",
    "version": "1.0",
    "maintainer": "Philipp Probst <philipp_probst@gmx.de>",
    "url": "https://github.com/PhilippPro/quantregRanger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20598,
    "package_name": "quarrint",
    "title": "Interaction Prediction Between Groundwater and Quarry Extension\nUsing Discrete Choice Models and Artificial Neural Networks",
    "description": "An implementation of two interaction indices between extractive\n    activity and groundwater resources based on hazard and vulnerability\n    parameters used in the assessment of natural hazards. One index is based\n    on a discrete choice model and the other is relying on an artificial\n    neural network.",
    "version": "1.0.0",
    "maintainer": "Johan Barthelemy <johan@uow.edu.au>",
    "url": "https://github.com/jojo-/quarrint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20631,
    "package_name": "quint",
    "title": "Qualitative Interaction Trees",
    "description": "Grows a qualitative interaction tree. Quint is a tool for subgroup analysis, suitable for data from a two-arm randomized controlled trial. More information in Dusseldorp, E., Doove, L., & Van Mechelen, I. (2016) <doi:10.3758/s13428-015-0594-z>.",
    "version": "2.2.2",
    "maintainer": "Elise Dusseldorp <elise.dusseldorp@fsw.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20700,
    "package_name": "rCBA",
    "title": "CBA Classifier",
    "description": "Provides implementations of a classifier based on the\n    \"Classification Based on Associations\" (CBA). It can be used for building\n    classification models from association rules. Rules are pruned in the order of\n    precedence given by the sort criteria and a default rule is added. The final\n    classifier labels provided instances. CBA was originally proposed by Liu,\n    B. Hsu, W. and Ma, Y. Integrating Classification and Association Rule\n    Mining. Proceedings KDD-98, New York, 27-31 August. AAAI. pp80-86 (1998, ISBN:1-57735-070-7).",
    "version": "0.4.3",
    "maintainer": "Jaroslav Kuchar <jaroslav.kuchar@gmail.com>",
    "url": "https://github.com/jaroslav-kuchar/rCBA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20719,
    "package_name": "rFerns",
    "title": "Random Ferns Classifier",
    "description": "Provides the random ferns classifier by Ozuysal, Calonder, Lepetit and Fua (2009) <doi:10.1109/TPAMI.2009.23>, modified for generic and multi-label classification and featuring OOB error approximation and importance measure as introduced in Kursa (2014) <doi:10.18637/jss.v061.i10>.",
    "version": "5.0.0",
    "maintainer": "Miron Bartosz Kursa <M.Kursa@icm.edu.pl>",
    "url": "https://gitlab.com/mbq/rFerns",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20763,
    "package_name": "rQSAR",
    "title": "QSAR Modeling with Multiple Algorithms: MLR, PLS, and Random\nForest",
    "description": "Quantitative Structure-Activity Relationship (QSAR) modeling is a valuable tool in computational chemistry and drug design, where it aims to predict the activity or property of chemical compounds based on their molecular structure. In this vignette, we present the 'rQSAR' package, which provides functions for variable selection and QSAR modeling using Multiple Linear Regression (MLR), Partial Least Squares (PLS), and Random Forest algorithms.",
    "version": "1.0.0",
    "maintainer": "Oche Ambrose George <ocheab1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20766,
    "package_name": "rSCA",
    "title": "An R Package for Stepwise Cluster Analysis",
    "description": "A statistical tool for multivariate modeling and clustering using stepwise cluster analysis. The modeling output of rSCA is constructed as a cluster tree to represent the complicated relationships between multiple dependent and independent variables. A free tool (named rSCA Tree Generator) for visualizing the cluster tree from rSCA is also released and it can be downloaded at <https://rscatree.weebly.com/>.",
    "version": "3.1",
    "maintainer": "Xiuquan (Xander) Wang <xiuquan.wang@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20779,
    "package_name": "rTG",
    "title": "Methods to Analyse Seasonal Radial Tree Growth Data",
    "description": "Methods for comparing different regression algorithms for \n    describing the temporal dynamics of secondary tree growth (xylem and \n    phloem). Users can compare the accuracy of the most common fitting methods \n    usually used to analyse xylem and phloem data, i.e., Gompertz function, \n    Double Gompertz function, General Additive Models (GAMs); and an algorithm \n    newly introduced to the field, i.e., Bayesian Regularised Neural Networks \n    (brnn). The core function of the package is XPSgrowth(), while the results \n    can be interpreted using implemented generic S3 methods, such as plot() and \n    summary().",
    "version": "1.0.4",
    "maintainer": "Jernej Jevsenak <jernej.jevsenak@gmail.com>",
    "url": "https://github.com/jernejjevsenak/rTG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20780,
    "package_name": "rTLsDeep",
    "title": "Post-Hurricane Damage Severity Classification from TLS and AI",
    "description": "Terrestrial laser scanning (TLS) data processing and post-hurricane damage severity classification at the individual tree level using deep Learning. Further details were published in Klauberg et al. (2023) <doi:10.3390/rs15041165>.",
    "version": "0.0.5",
    "maintainer": "Caio Hamamura <caiohamamura@gmail.com>",
    "url": "https://github.com/carlos-alberto-silva/rTLsDeep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20803,
    "package_name": "radarBoxplot",
    "title": "Implementation of the Radar-Boxplot",
    "description": "Creates the radar-boxplot, a plot that was created by the \n    author during his Ph.D. in forest resources. \n    The radar-boxplot is a visualization feature suited for  \n    multivariate classification/clustering. It provides an intuitive \n    deep understanding of the data.",
    "version": "1.0.5",
    "maintainer": "Caio Hamamura <caiohamamura@gmail.com>",
    "url": "https://github.com/caiohamamura/radarBoxplot-R,\nhttps://radarboxplot.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20825,
    "package_name": "rainette",
    "title": "The Reinert Method for Textual Data Clustering",
    "description": "An R implementation of the Reinert text clustering method. For more \n    details about the algorithm see the included vignettes or Reinert (1990) \n    <doi:10.1177/075910639002600103>.",
    "version": "0.3.1.1",
    "maintainer": "Julien Barnier <julien.barnier@cnrs.fr>",
    "url": "https://juba.github.io/rainette/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20851,
    "package_name": "randomForest",
    "title": "Breiman and Cutlers Random Forests for Classification and\nRegression",
    "description": "Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) <DOI:10.1023/A:1010933404324>.",
    "version": "4.7-1.2",
    "maintainer": "Andy Liaw <andy_liaw@merck.com>",
    "url": "https://www.stat.berkeley.edu/~breiman/RandomForests/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20852,
    "package_name": "randomForestExplainer",
    "title": "Explaining and Visualizing Random Forests in Terms of Variable\nImportance",
    "description": "A set of tools to help explain which variables are most important in a random forests. Various variable importance measures are calculated and visualized in different settings in order to get an idea on how their importance changes depending on our criteria (Hemant Ishwaran and Udaya B. Kogalur and Eiran Z. Gorodeski and Andy J. Minn and Michael S. Lauer (2010) <doi:10.1198/jasa.2009.tm08622>, Leo Breiman (2001) <doi:10.1023/A:1010933404324>).",
    "version": "0.10.1",
    "maintainer": "Yue Jiang <rivehill@gmail.com>",
    "url": "https://github.com/ModelOriented/randomForestExplainer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20854,
    "package_name": "randomForestVIP",
    "title": "Tune Random Forests Based on Variable Importance & Plot Results",
    "description": "Functions for assessing variable relations and associations \n    prior to modeling with a Random Forest algorithm (although these are \n    relevant for any predictive model).\n    Metrics such as partial correlations and variance inflation factors\n    are tabulated as well as plotted for the user. A function is available\n    for tuning the main Random Forest hyper-parameter based on model performance \n    and variable importance metrics. This grid-search technique provides\n    tables and plots showing the effect of the main hyper-parameter on each \n    of the assessment metrics. It also returns each of the evaluated models \n    to the user. The package also provides superior variable importance plots \n    for individual models. All of the plots are developed so that the \n    user has the ability to edit and improve further upon the \n    plots. Derivations and methodology are described in Bladen (2022) \n    <https://digitalcommons.usu.edu/etd/8587/>.",
    "version": "0.1.3",
    "maintainer": "Kelvyn Bladen <kelvyn.bladen@usu.edu>",
    "url": "https://github.com/KelvynBladen/randomForestVIP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20858,
    "package_name": "randomMachines",
    "title": "An Ensemble Modeling using Random Machines",
    "description": "A novel ensemble method employing Support Vector Machines (SVMs) as base learners. This powerful ensemble model is designed for both classification (Ara A., et. al, 2021) <doi:10.6339/21-JDS1014>, and regression (Ara A., et. al, 2021) <doi:10.1016/j.eswa.2022.117107> problems, offering versatility and robust performance across different datasets and compared with other consolidated methods as Random Forests (Maia M, et. al, 2021) <doi:10.6339/21-JDS1025>.",
    "version": "0.1.1",
    "maintainer": "Mateus Maia <mateus.maiamarques@glasgow.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20861,
    "package_name": "randomUniformForest",
    "title": "Random Uniform Forests for Classification, Regression and\nUnsupervised Learning",
    "description": "Ensemble model, for classification, regression\n\tand unsupervised learning, based on a forest of unpruned \n\tand randomized binary decision trees. Each tree is grown \n\tby sampling, with replacement, a set of variables at each node. \n\tEach cut-point is generated randomly, according to the continuous \n\tUniform distribution. For each tree, data are either bootstrapped \n\tor subsampled. The unsupervised mode introduces clustering, dimension reduction\n\tand variable importance, using a three-layer engine. Random Uniform Forests are mainly \n\taimed to lower correlation between trees (or trees residuals), to provide a deep analysis \n\tof variable importance and to allow native distributed and incremental learning.",
    "version": "1.1.6",
    "maintainer": "Saip Ciss <saip.ciss@wanadoo.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20975,
    "package_name": "rbooster",
    "title": "AdaBoost Framework for Any Classifier",
    "description": "This is a simple package which provides a function\n      that boosts pre-ready or custom-made classifiers. Package\n      uses Discrete AdaBoost (<doi:10.1006/jcss.1997.1504>) and Real AdaBoost\n      (<doi:10.1214/aos/1016218223>) for two class,\n      SAMME (<doi:10.4310/SII.2009.v2.n3.a8>) and\n      SAMME.R (<doi:10.4310/SII.2009.v2.n3.a8>)\n      for multiclass classification. ",
    "version": "1.1.0",
    "maintainer": "Fatih Saglam <fatih.saglam@omu.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20993,
    "package_name": "rcccd",
    "title": "Class Cover Catch Digraph Classification",
    "description": "Fit Class Cover Catch Digraph Classification models that can be \n    used in machine learning. Pure and proper and random walk approaches are \n    available. Methods are explained in Priebe et al. (2001) \n    <doi:10.1016/S0167-7152(01)00129-8>, Priebe et al. (2003) \n    <doi:10.1007/s00357-003-0003-7>, and Manukyan and Ceyhan (2016) \n    <doi:10.48550/arXiv.1904.04564>.",
    "version": "0.3.2",
    "maintainer": "Fatih Saglam <saglamf89@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21035,
    "package_name": "rda",
    "title": "Shrunken Centroids Regularized Discriminant Analysis",
    "description": "Provides functions implementing the shrunken centroids regularized discriminant analysis for\n        classification purpose in high dimensional data. The method is described in \n        Guo at al. (2013) <doi:10.1093/biostatistics/kxj035>.",
    "version": "1.2-1",
    "maintainer": "Valentin Todorov <valentin@todorov.at>",
    "url": "https://github.com/valentint/rda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21063,
    "package_name": "rdomains",
    "title": "Get the Category of Content Hosted by a Domain",
    "description": "Get the category of content hosted by a domain. Use Shallalist (service discontinued),",
    "version": "0.3.0",
    "maintainer": "",
    "url": "https://github.com/themains/rdomains",
    "exports": [],
    "topics": ["classify-domains", "cran", "dmoz", "domain-classifier", "machine-learning", "r", "r-package", "shallalist"],
    "score": "NA",
    "stars": 58
  },
  {
    "id": 21092,
    "package_name": "readMLData",
    "title": "Reading Machine Learning Benchmark Data Sets in Different\nFormats",
    "description": "Functions for reading data sets in different formats\n  for testing machine learning tools are provided. This allows to run\n  a loop over several data sets in their original form, for example\n  if they are downloaded from UCI Machine Learning Repository.\n  The data are not part of the package and have to be downloaded\n  separately.",
    "version": "0.9-7",
    "maintainer": "Petr Savicky <savicky@cs.cas.cz>",
    "url": "http://www.cs.cas.cz/~savicky/readMLData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21094,
    "package_name": "readNSx",
    "title": "Read 'Blackrock-Microsystems' Files ('NEV', 'NSx')",
    "description": "Loads 'Blackrock' <https://blackrockneurotech.com> neural signal \n    data files into the memory, provides utility tools to extract the data into \n    common formats such as plain-text 'tsv' and 'HDF5'.",
    "version": "0.0.6",
    "maintainer": "Zhengjia Wang <dipterix.wang@gmail.com>",
    "url": "http://dipterix.org/readNSx/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21115,
    "package_name": "readsparse",
    "title": "Read and Write Sparse Matrices in 'SVMLight' and 'LibSVM'\nFormats",
    "description": "Read and write labelled sparse matrices in text format as used by\n    software such as 'SVMLight', 'LibSVM', 'ThunderSVM', 'LibFM', 'xLearn', 'XGBoost', 'LightGBM',\n    and others. Supports labelled data for regression, classification (binary, multi-class, multi-label),\n    and ranking (with 'qid' field), and can handle header metadata and comments in files.",
    "version": "0.1.5-8",
    "maintainer": "David Cortes <david.cortes.rivera@gmail.com>",
    "url": "https://github.com/david-cortes/readsparse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21130,
    "package_name": "rebmix",
    "title": "Finite Mixture Modeling, Clustering & Classification",
    "description": "Random univariate and multivariate finite mixture model generation, estimation, clustering, latent class analysis and classification. Variables can be continuous, discrete, independent or dependent and may follow normal, lognormal, Weibull, gamma, Gumbel, binomial, Poisson, Dirac, uniform or circular von Mises parametric families.",
    "version": "2.17.1",
    "maintainer": "Marko Nagode <marko.nagode@fs.uni-lj.si>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21147,
    "package_name": "reclin2",
    "title": "Record Linkage Toolkit",
    "description": "Functions to assist in performing probabilistic record linkage and\n    deduplication: generating pairs, comparing records, em-algorithm for\n    estimating m- and u-probabilities\n    (I. Fellegi & A. Sunter (1969) <doi:10.1080/01621459.1969.10501049>, \n    T.N. Herzog, F.J. Scheuren, & W.E. Winkler (2007), \n    \"Data Quality and Record Linkage Techniques\", ISBN:978-0-387-69502-0),\n    forcing one-to-one matching. Can also be\n    used for pre- and post-processing for machine learning methods for record\n    linkage. Focus is on memory, CPU performance and flexibility. ",
    "version": "0.6.0",
    "maintainer": "Jan van der Laan <r@eoos.dds.nl>",
    "url": "https://github.com/djvanderlaan/reclin2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21148,
    "package_name": "recluster",
    "title": "Ordination Methods for the Analysis of Beta-Diversity Indices",
    "description": "The analysis of different aspects of biodiversity requires specific algorithms. \n\tFor example, in regionalisation analyses, the high frequency of ties and zero values in \n\tdissimilarity matrices produced by Beta-diversity turnover produces hierarchical \n\tcluster dendrograms whose topology and bootstrap supports are affected by the order of \n\trows in the original matrix. Moreover, visualisation of biogeographical regionalisation \n\tcan be facilitated by a combination of hierarchical clustering and multi-dimensional \n\tscaling. The recluster package provides robust techniques to visualise and analyse \n\tpattern of biodiversity and to improve occurrence data for cryptic taxa. \n\tOther functions \trelated to recluster (e.g. the biodecrypt family) are currently \n\tavailable in GitHub at <https://github.com/leondap/recluster>.",
    "version": "2.9",
    "maintainer": "Leonardo Dapporto <leondap@gmail.com>",
    "url": "https://github.com/leondap/recluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21197,
    "package_name": "refinr",
    "title": "Cluster and Merge Similar Values Within a Character Vector",
    "description": "These functions take a character vector as input, identify and \n  cluster similar values, and then merge clusters together so their values \n  become identical. The functions are an implementation of the key collision \n  and ngram fingerprint algorithms from the open source tool Open Refine \n  <https://openrefine.org/>. More info on key collision and ngram fingerprint \n  can be found here <https://openrefine.org/docs/technical-reference/clustering-in-depth>.",
    "version": "0.3.3",
    "maintainer": "Chris Muir <chrismuirRVA@gmail.com>",
    "url": "https://github.com/ChrisMuir/refinr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21242,
    "package_name": "regressoR",
    "title": "Regression Data Analysis System",
    "description": "Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as linear regression, penalized regression, k-nearest neighbors, decision trees, ada boosting, extreme gradient boosting, random forest, neural networks, deep learning and support vector machines.",
    "version": "4.0.7",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "url": "https://promidat.website/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21249,
    "package_name": "regtools",
    "title": "Regression and Classification Tools",
    "description": "Tools for linear, nonlinear and nonparametric regression\n             and classification.  Novel graphical methods for assessment \n             of parametric models using nonparametric methods. One \n             vs. All and All vs. All multiclass classification, optional\n             class probabilities adjustment.  Nonparametric regression \n             (k-NN) for general dimension, local-linear option.  Nonlinear \n             regression with Eickert-White method for dealing with \n             heteroscedasticity.  Utilities for converting time series\n             to rectangular form.  Utilities for conversion between\n             factors and indicator variables.  Some code related to\n             \"Statistical Regression and Classification: from Linear\n             Models to Machine Learning\", N. Matloff, 2017, CRC,\n             ISBN 9781498710916.",
    "version": "1.7.0",
    "maintainer": "Norm Matloff <matloff@cs.ucdavis.edu>",
    "url": "https://github.com/matloff/regtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21327,
    "package_name": "repsim",
    "title": "Measures of Representational Similarity Across Models",
    "description": "Provides a collection of methods for quantifying representational similarity between learned features or multivariate data. The package offers an efficient 'C++' backend, designed for applications in machine learning, computational neuroscience, and multivariate statistics. See Klabunde et al. (2025) <doi:10.1145/3728458> for a comprehensive overview of the topic.",
    "version": "0.1.0",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21328,
    "package_name": "reptiledb.data",
    "title": "Reptile Database Data",
    "description": "Provides easy access to 'The Reptile Database', a comprehensive catalogue of all living reptile species and their classification. This package includes taxonomic data for over 10,000 reptile species, approximately 2,800 of which are subspecies, covering all extant reptiles. The dataset features taxonomic names, synonyms, distribution data, type specimens, and literature references, making it ready for research and analysis. Data is sourced from 'The Reptile Database' <http://www.reptile-database.org/>.",
    "version": "0.0.0.1",
    "maintainer": "Paul Efren Santos Andrade <paulefrens@gmail.com>",
    "url": "https://github.com/PaulESantos/reptiledb.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21346,
    "package_name": "reservr",
    "title": "Fit Distributions and Neural Networks to Censored and Truncated\nData",
    "description": "Define distribution families and fit them to\n    interval-censored and interval-truncated data, where the truncation\n    bounds may depend on the individual observation. The defined\n    distributions feature density, probability, sampling and fitting\n    methods as well as efficient implementations of the log-density log\n    f(x) and log-probability log P(x0 <= X <= x1) for use in 'TensorFlow'\n    neural networks via the 'tensorflow' package. Allows training\n    parametric neural networks on interval-censored and interval-truncated\n    data with flexible parameterization. Applications include Claims\n    Development in Non-Life Insurance, e.g. modelling reporting delay\n    distributions from incomplete data, see Bücher, Rosenstock (2022)\n    <doi:10.1007/s13385-022-00314-4>.",
    "version": "0.0.3",
    "maintainer": "Alexander Rosenstock <alexander.rosenstock@web.de>",
    "url": "https://ashesitr.github.io/reservr/,\nhttps://github.com/AshesITR/reservr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21407,
    "package_name": "rfPermute",
    "title": "Estimate Permutation p-Values for Random Forest Importance\nMetrics",
    "description": "Estimate significance of importance metrics\n    for a Random Forest model by permuting the response\n    variable. Produces null distribution of importance\n    metrics for each predictor variable and p-value of\n    observed. Provides summary and visualization functions for 'randomForest' \n    results.",
    "version": "2.5.5",
    "maintainer": "Eric Archer <eric.ivan.archer@gmail.com>",
    "url": "https://github.com/SWFSC/rfPermute",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21409,
    "package_name": "rfVarImpOOB",
    "title": "Unbiased Variable Importance for Random Forests",
    "description": "Computes a novel variable importance for random forests: Impurity reduction importance scores for out-of-bag (OOB) data complementing the existing inbag Gini importance, see also <doi: 10.1080/03610926.2020.1764042>. \n    The Gini impurities for inbag and OOB data are combined in three different ways, after which the information gain is computed at each split.\n    This gain is aggregated for each split variable in a tree and averaged across trees.",
    "version": "1.0.3",
    "maintainer": "Markus Loecher <Markus.Loecher@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21417,
    "package_name": "rfinterval",
    "title": "Predictive Inference for Random Forests",
    "description": "An integrated package for constructing random forest prediction intervals using a fast implementation package 'ranger'. This package can apply the following three methods described in Haozhe Zhang, Joshua Zimmerman, Dan Nettleton, and Daniel J. Nordman (2019) <doi:10.1080/00031305.2019.1585288>: the out-of-bag prediction interval, the split conformal method, and the quantile regression forest.",
    "version": "1.0.0",
    "maintainer": "Haozhe Zhang <haozhe.stat@gmail.com>",
    "url": "http://github.com/haozhestat/rfinterval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21429,
    "package_name": "rfviz",
    "title": "Interactive Visualization Tool for Random Forests",
    "description": "An interactive data visualization and exploration toolkit\n    that implements Breiman and Cutler's original random forest Java based        \n    visualization tools in R, for supervised and unsupervised classification and \n    regression within the algorithm random forest. ",
    "version": "1.0.1",
    "maintainer": "Chris Kuchar <chrisjkuchar@gmail.com>",
    "url": "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_graphics.htm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21438,
    "package_name": "rgenoud",
    "title": "R Version of GENetic Optimization Using Derivatives",
    "description": "A genetic algorithm plus derivative optimizer.",
    "version": "5.9-0.11",
    "maintainer": "Jasjeet Singh Sekhon <jas.sekhon@yale.edu>",
    "url": "https://github.com/JasjeetSekhon/rgenoud",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21465,
    "package_name": "rhcoclust",
    "title": "Robust Hierarchical Co-Clustering to Identify Significant\nCo-Cluster",
    "description": "Here we performs robust hierarchical co-clustering between row and column entities of a data matrix in \n             absence and presence of outlying observations. It can be used to explore important co-clusters consisting of  \n             important samples and their regulatory significant features. Please see Hasan, Badsha and Mollah (2020) \n             <doi:10.1101/2020.05.13.094946>.",
    "version": "2.0.0",
    "maintainer": "Md. Bahadur Badsha <mbbadshar@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21493,
    "package_name": "richCluster",
    "title": "Fast, Robust Clustering Algorithms for Gene Enrichment Data",
    "description": "Fast 'C++' agglomerative hierarchical clustering \n\talgorithm packaged into easily callable R functions, designed \n\tto help cluster biological terms based on how\n    similar of genes are expressed in their activation.",
    "version": "1.0.2",
    "maintainer": "Junguk Hur <hurlabshared@gmail.com>",
    "url": "https://github.com/hurlab/richCluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21522,
    "package_name": "ripc",
    "title": "Download and Tidy IPC and CH Data",
    "description": "Utilities to access Integrated Food Security Phase Classification\n    (IPC) and Cadre Harmonisé (CH) food security data. Wrapper functions are\n    available for all of the 'IPC-CH' Public API (<https://docs.api.ipcinfo.org>)\n    simplified and advanced endpoints to easily download the data in a clean and\n    tidy format.",
    "version": "0.3.1",
    "maintainer": "Seth Caldwell <caldwellst@gmail.com>",
    "url": "https://github.com/ocha-dap/ripc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21551,
    "package_name": "rjaf",
    "title": "Regularized Joint Assignment Forest with Treatment Arm\nClustering",
    "description": "Personalized assignment to one of many treatment arms via regularized and clustered joint assignment forests as described in Ladhania, Spiess, Ungar, and Wu (2023) <doi:10.48550/arXiv.2311.00577>. The algorithm pools information across treatment arms: it considers a regularized forest-based assignment algorithm based on greedy recursive partitioning that shrinks effect estimates across arms; and it incorporates a clustering scheme that combines treatment arms with consistently similar outcomes.",
    "version": "0.1.3",
    "maintainer": "Xinyi Zhang <zhang.xinyi@nyu.edu>",
    "url": "https://github.com/wustat/rjaf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21623,
    "package_name": "rminer",
    "title": "Machine Learning Classification and Regression Methods",
    "description": "Facilitates the use of machine learning algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.5.0 improved mparheuristic function (new hyperparameter heuristics); 1.4.9 / 1.4.8 improved help, several warning and error code fixes (more stable version, all examples run correctly); 1.4.7 - improved Importance function and examples, minor error fixes; 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new \"lssvm\" model and improved mparheuristic() function; 1.4.2 new \"NMAE\" metric, \"xgboost\" and \"cv.glmnet\" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version.",
    "version": "1.5.0",
    "maintainer": "Paulo Cortez <pcortez@dsi.uminho.pt>",
    "url": "https://cran.r-project.org/package=rminer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21661,
    "package_name": "rnn",
    "title": "Recurrent Neural Network",
    "description": "Implementation of a Recurrent Neural Network architectures in native R, including Long Short-Term Memory (Hochreiter and Schmidhuber, <doi:10.1162/neco.1997.9.8.1735>), Gated Recurrent Unit (Chung et al., <arXiv:1412.3555>) and vanilla RNN.",
    "version": "1.9.0",
    "maintainer": "Bastiaan Quast <bquast@gmail.com>",
    "url": "https://qua.st/rnn/, https://github.com/bquast/rnn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21721,
    "package_name": "robustmeta",
    "title": "Robust Inference for Meta-Analysis with Influential Outlying\nStudies",
    "description": "Robust inference methods for fixed-effect and random-effects models of meta-analysis are implementable. The robust methods are developed using the density power divergence that is a robust estimating criterion developed in machine learning theory, and can effectively circumvent biases and misleading results caused by influential outliers. The density power divergence is originally introduced by Basu et al. (1998) <doi:10.1093/biomet/85.3.549>, and the meta-analysis methods are developed by Noma et al. (2022) <forthcoming>.",
    "version": "1.2-1",
    "maintainer": "Hisashi Noma <noma@ism.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21723,
    "package_name": "robustrao",
    "title": "An Extended Rao-Stirling Diversity Index to Handle Missing Data",
    "description": "A collection of functions to compute the Rao-Stirling diversity index\n\t(Porter and Rafols, 2009) <DOI:10.1007/s11192-008-2197-2> and its extension to\n\tacknowledge missing data (i.e.,\tuncategorized references) by calculating its\n\tinterval of uncertainty using\tmathematical optimization as proposed in Calatrava\n\tet al. (2016) <DOI:10.1007/s11192-016-1842-4>.\n\tThe Rao-Stirling diversity index is a well-established bibliometric indicator\n\tto measure the interdisciplinarity of scientific publications. Apart from the\n\tobligatory dataset of publications with their respective references and\ta\n\ttaxonomy of disciplines that categorizes references as well as a measure of\n\tsimilarity between the disciplines, the Rao-Stirling diversity index requires\n\ta complete categorization of all references of a publication into disciplines.\n\tThus, it fails for a incomplete categorization; in this case, the robust\n\textension has to be used, which encodes the uncertainty caused by missing\n\tbibliographic data as an uncertainty interval.\n\tClassification / ACM - 2012: Information systems ~ Similarity measures,\n\tTheory of computation ~ Quadratic\tprogramming, Applied computing ~ Digital\n\tlibraries and archives.",
    "version": "1.0-5",
    "maintainer": "Maria del Carmen Calatrava Moreno\n<mc.calatrava.moreno@gmail.com>",
    "url": "https://gitlab.com/mc.calatrava.moreno/robustrao.git",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21777,
    "package_name": "roseRF",
    "title": "ROSE Random Forests for Robust Semiparametric Efficient\nEstimation",
    "description": "ROSE (RObust Semiparametric Efficient) random forests for robust \n  semiparametric efficient estimation in partially parametric models (containing \n  generalised partially linear models).\n  Details can be found in the paper by Young and Shah (2024) <doi:10.48550/arXiv.2410.03471>.",
    "version": "0.1.0",
    "maintainer": "Elliot H. Young <ey244@cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21783,
    "package_name": "rotationForest",
    "title": "Fit and Deploy Rotation Forest Models",
    "description": "Fit and deploy rotation forest models (\"Rodriguez, J.J., Kuncheva,\n    L.I., 2006. Rotation forest: A new classifier ensemble method. IEEE Trans.\n    Pattern Anal. Mach. Intell. 28, 1619-1630 <doi:10.1109/TPAMI.2006.211>\") for binary classification.\n    Rotation forest is an ensemble method where each base classifier (tree) is\n    fit on the principal components of the variables of random partitions of\n    the feature set.",
    "version": "0.1.3",
    "maintainer": "Michel Ballings <michel.ballings@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21810,
    "package_name": "rpartScore",
    "title": "Classification Trees for Ordinal Responses",
    "description": "Recursive partitioning methods to build\n        classification trees for ordinal responses within the CART\n        framework. Trees are grown using the Generalized Gini\n        impurity function, where the misclassification costs are given\n        by the absolute or squared differences in scores assigned to\n        the categories of the response. Pruning is based on the total\n        misclassification rate or on the total misclassification cost.",
    "version": "1.0-2",
    "maintainer": "Giuliano Galimberti <giuliano.galimberti@unibo.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21827,
    "package_name": "rpms",
    "title": "Recursive Partitioning for Modeling Survey Data",
    "description": "Functions to allow users to build and analyze design consistent \n    tree and random forest models using survey data from a complex sample \n    design.  The tree model algorithm can fit a linear model to survey data \n    in each node obtained by recursively partitioning the data.  The splitting \n    variables and selected splits are obtained using a randomized permutation \n    test procedure which adjusted for complex sample design features used to \n    obtain the data. Likewise the model fitting algorithm produces \n    design-consistent coefficients to any specified least squares linear model \n    between the dependent and independent variables used in the end nodes.\n    The main functions return the resulting binary tree or random forest as \n    an object of \"rpms\" or \"rpms_forest\" type. The package also provides methods\n    modeling a \"boosted\" tree or forest model and a tree model for zero-inflated\n    data as well as a number of functions and methods available for use with \n    these object types.",
    "version": "0.5.1",
    "maintainer": "Daniell Toth <danielltoth@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21862,
    "package_name": "rrcovHD",
    "title": "Robust Multivariate Methods for High Dimensional Data",
    "description": "Robust multivariate methods for high dimensional data including\n        outlier detection (Filzmoser and Todorov (2013) <doi:10.1016/j.ins.2012.10.017>), \n        robust sparse PCA (Croux et al. (2013) <doi:10.1080/00401706.2012.727746>, Todorov and Filzmoser (2013) <doi:10.1007/978-3-642-33042-1_31>), \n        robust PLS (Todorov and Filzmoser (2014) <doi:10.17713/ajs.v43i4.44>), \n        and robust sparse classification (Ortner et al. (2020) <doi:10.1007/s10618-019-00666-8>).",
    "version": "0.3-1",
    "maintainer": "Valentin Todorov <valentin.todorov@chello.at>",
    "url": "https://github.com/valentint/rrcovHD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21924,
    "package_name": "rsparkling",
    "title": "R Interface for H2O Sparkling Water",
    "description": "An extension package for 'sparklyr' that provides an R interface to",
    "version": "0.2.4",
    "maintainer": "",
    "url": "https://github.com/h2oai/rsparkling",
    "exports": [],
    "topics": ["big-data", "data-science", "deep-learning", "h2o", "machine-learning", "r", "spark", "sparklyr", "water"],
    "score": "NA",
    "stars": 62
  },
  {
    "id": 21983,
    "package_name": "rtkore",
    "title": "'STK++' Core Library Integration to 'R' using 'Rcpp'",
    "description": "'STK++' <http://www.stkpp.org> is a collection of C++ classes\n    for statistics, clustering, linear algebra, arrays (with an 'Eigen'-like API),\n    regression, dimension reduction, etc. The integration of the library to 'R' is\n    using 'Rcpp'. The 'rtkore' package includes the header files from the 'STK++'\n    core library. All files contain only template classes and/or inline functions.\n    'STK++' is licensed under the GNU LGPL version 2 or later. 'rtkore' (the 'stkpp'\n    integration into 'R') is licensed under the GNU GPL version 2 or later. See file\n    LICENSE.note for details.",
    "version": "1.6.13",
    "maintainer": "Serge Iovleff <serge.iovleff@stkpp.org>",
    "url": "http://www.stkpp.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22083,
    "package_name": "sClust",
    "title": "R Toolbox for Unsupervised Spectral Clustering",
    "description": "Toolbox containing a variety of spectral clustering tools functions. Among the tools available are the hierarchical spectral clustering algorithm, the Shi and Malik clustering algorithm, the Perona and Freeman algorithm, the non-normalized clustering, the Von Luxburg algorithm, the Partition Around Medoids clustering algorithm, a multi-level clustering algorithm, recursive clustering and the fast method for all clustering algorithm. As well as other tools needed to run these algorithms or useful for unsupervised spectral clustering. This toolbox aims to gather the main tools for unsupervised spectral classification. See <http://mawenzi.univ-littoral.fr/> for more information and documentation. ",
    "version": "1.0",
    "maintainer": "Emilie Poisson-Caillault <emilie.caillault@univ-littoral.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22141,
    "package_name": "safetensors",
    "title": "Safetensors File Format",
    "description": "A file format for storing tensors that is secure (doesn't allow for\n    code execution), fast and simple to implement. 'safetensors' also enables cross\n    language and cross frameworks compatibility making it an ideal format for\n    storing machine learning model weights.",
    "version": "0.2.0",
    "maintainer": "Daniel Falbel <daniel@posit.co>",
    "url": "https://github.com/mlverse/safetensors,\nhttps://mlverse.github.io/safetensors/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22145,
    "package_name": "sageR",
    "title": "Applied Statistics for Economics and Management with R",
    "description": "Datasets and functions for the book \"Statistiques pour l’économie et la gestion\", \"Théorie et applications en entreprise\", F. Bertrand, Ch. Derquenne, G. Dufrénot, F. Jawadi and M. Maumy, C. Borsenberger editor, (2021, ISBN:9782807319448, De Boeck Supérieur, Louvain-la-Neuve). \n    The first chapter of the book is dedicated to an introduction to statistics and their world. \n    The second chapter deals with univariate exploratory statistics and graphics. \n    The third chapter deals with bivariate and multivariate exploratory statistics and graphics. \n    The fourth chapter is dedicated to data exploration with Principal Component Analysis. \n    The fifth chapter is dedicated to data exploration with Correspondance Analysis.\n    The sixth chapter is dedicated to data exploration with Multiple Correspondance Analysis. \n    The seventh chapter is dedicated to data exploration with automatic clustering. \n    The eighth chapter is dedicated to an introduction to probability theory and classical probability distributions.\n    The ninth chapter is dedicated to an estimation theory, one-sample and two-sample tests.\n    The tenth chapter is dedicated to an Gaussian linear model.\n    The eleventh chapter is dedicated to an introduction to time series.\n    The twelfth chapter is dedicated to an introduction to probit and logit models.\n    Various example datasets are shipped with the package as well as some new functions.",
    "version": "0.7.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/homepage/,\nhttps://fbertran.github.io/sageR/,\nhttps://github.com/fbertran/sageR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22152,
    "package_name": "salso",
    "title": "Search Algorithms and Loss Functions for Bayesian Clustering",
    "description": "The SALSO algorithm is an efficient randomized greedy search method to find a point estimate for a random partition based on a loss function and posterior Monte Carlo samples. The algorithm is implemented for many loss functions, including the Binder loss and a generalization of the variation of information loss, both of which allow for unequal weights on the two types of clustering mistakes. Efficient implementations are also provided for Monte Carlo estimation of the posterior expected loss of a given clustering estimate. See Dahl, Johnson, Müller (2022) <doi:10.1080/10618600.2022.2069779>.",
    "version": "0.3.57",
    "maintainer": "David B. Dahl <dahl@stat.byu.edu>",
    "url": "https://github.com/dbdahl/salso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22196,
    "package_name": "sansa",
    "title": "Synthetic Data Generation for Imbalanced Learning in 'R'",
    "description": "Machine learning is widely used in information-systems design. Yet, training algorithms on imbalanced datasets may severely affect performance on unseen data. For example, in some cases in healthcare, financial, or internet-security contexts, certain sub-classes are difficult to learn because they are underrepresented in training data. This 'R' package offers a flexible and efficient solution based on a new synthetic average neighborhood sampling algorithm ('SANSA'), which, in contrast to other solutions, introduces a novel “placement” parameter that can be tuned to adapt to each datasets unique manifestation of the imbalance. More information about the algorithm's parameters can be found at Nasir et al. (2022) <https://murtaza.cc/SANSA/>. ",
    "version": "0.0.1",
    "maintainer": "Murtaza Nasir <mail@murtaza.cc>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22212,
    "package_name": "sarp.snowprofile.alignment",
    "title": "Snow Profile Alignment, Aggregation, and Clustering",
    "description": "Snow profiles describe the vertical (1D) stratigraphy of layered \n    snow with different layer characteristics, such as grain type, hardness, \n    deposition date, and many more. Hence, they represent a data format similar \n    to multivariate time series containing categorical, ordinal, and numerical \n    data types. Use this package to align snow profiles by matching their \n    individual layers based on Dynamic Time Warping (DTW). The aligned profiles \n    can then be assessed with an independent, global similarity measure that is \n    geared towards avalanche hazard assessment. Finally, through exploiting data\n    aggregation and clustering methods, the similarity measure provides the\n    foundation for grouping and summarizing snow profiles according to similar\n    hazard conditions. In particular, this package allows for averaging large\n    numbers of snow profiles with DTW Barycenter Averaging and thereby \n    facilitates the computation of individual layer distributions and summary \n    statistics that are relevant for avalanche forecasting purposes. \n    For more background information refer to Herla, Horton, Mair,\n    and Haegeli (2021) <doi:10.5194/gmd-14-239-2021>, Herla, Mair, and Haegeli \n    (2022) <doi:10.5194/tc-16-3149-2022>, and Horton, Herla, and Haegeli (2024)\n    <doi:10.5194/egusphere-2024-1609>.",
    "version": "2.0.2",
    "maintainer": "Florian Herla <fherla@sfu.ca>",
    "url": "https://avalancheresearch.ca/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22237,
    "package_name": "sbfc",
    "title": "Selective Bayesian Forest Classifier",
    "description": "An MCMC algorithm for simultaneous feature selection and classification, \n    and visualization of the selected features and feature interactions. \n    An implementation of SBFC by Krakovna, Du and Liu (2015), <arXiv:1506.02371>.",
    "version": "1.0.3",
    "maintainer": "Viktoriya Krakovna <vkrakovna@gmail.com>",
    "url": "https://github.com/vkrakovna/sbfc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22243,
    "package_name": "sboost",
    "title": "Machine Learning with AdaBoost on Decision Stumps",
    "description": "Creates classifier for binary outcomes using Adaptive Boosting \n    (AdaBoost) algorithm on decision stumps with a fast C++ implementation. \n    For a description of AdaBoost, see Freund and Schapire (1997) \n    <doi:10.1006/jcss.1997.1504>. This type of classifier is nonlinear, but\n    easy to interpret and visualize. Feature vectors may be a combination of\n    continuous (numeric) and categorical (string, factor) elements. Methods \n    for classifier assessment, predictions, and cross-validation also included.",
    "version": "0.1.2",
    "maintainer": "Jadon Wagstaff <jadonw@gmail.com>",
    "url": "https://github.com/jadonwagstaff/sboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22262,
    "package_name": "scDHA",
    "title": "Single-Cell Decomposition using Hierarchical Autoencoder",
    "description": "Provides a fast and accurate pipeline for single-cell analyses. \n    The 'scDHA' software package can perform clustering, dimension reduction and visualization, classification, and time-trajectory inference on single-cell data (Tran et.al. (2021) <DOI:10.1038/s41467-021-21312-2>).",
    "version": "1.2.3",
    "maintainer": "Ha Nguyen <hvn0006@auburn.edu>",
    "url": "https://github.com/duct317/scDHA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22326,
    "package_name": "scapGNN",
    "title": "Graph Neural Network-Based Framework for Single Cell Active\nPathways and Gene Modules Analysis",
    "description": "It is a single cell active pathway analysis tool based on the graph neural network (F. Scarselli (2009) <doi:10.1109/TNN.2008.2005605>; Thomas N. Kipf (2017) <arXiv:1609.02907v4>) to construct the gene-cell association network, infer pathway activity scores from different single cell modalities data, integrate multiple modality data on the same cells into one pathway activity score matrix, identify cell phenotype activated gene modules and parse association networks of gene modules under multiple cell phenotype. In addition, abundant visualization programs are provided to display the results.",
    "version": "0.1.4",
    "maintainer": "Xudong Han <hanxd1217@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22342,
    "package_name": "scclust",
    "title": "Size-Constrained Clustering",
    "description": "\n    Provides wrappers for 'scclust', a C library for computationally efficient\n    size-constrained clustering with near-optimal performance.\n    See <https://github.com/fsavje/scclust> for more information.",
    "version": "0.2.5",
    "maintainer": "Fredrik Savje <rpackages@fredriksavje.com>",
    "url": "https://github.com/fsavje/scclust-R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22373,
    "package_name": "scientific",
    "title": "Two Highly Customizable 'rmarkdown' Themes for Scientific\nReports",
    "description": "Offers 'markdown' output formats designed with various styles, allowing users to generate HTML reports tailored for scientific or machine learning showcase. The output has a contemporary appearance with vibrant visuals, providing numerous styles for effective highlighting. Created using the 'tufte' <https://rstudio.github.io/tufte/> package code as a starting point.",
    "version": "2025.1",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "url": "https://scientific.obi.obianom.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22388,
    "package_name": "scorecard",
    "title": "Credit Risk Scorecard",
    "description": "\n  The `scorecard` package makes the development of credit risk scorecard \n  easier and efficient by providing functions for some common tasks, \n  such as data partition, variable selection, woe binning, scorecard scaling,\n  performance evaluation and report generation. These functions can also used\n  in the development of machine learning models.\n    The references including: \n  1. Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: \n  Development and Implementation Using SAS. \n  2. Siddiqi, N. (2006, ISBN: 9780471754510). Credit risk scorecards. \n  Developing and Implementing Intelligent Credit Scoring.",
    "version": "0.4.5",
    "maintainer": "Shichen Xie <xie@shichen.name>",
    "url": "https://github.com/ShichenXie/scorecard,\nhttp://shichen.name/scorecard/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22389,
    "package_name": "scorecardModelUtils",
    "title": "Credit Scorecard Modelling Utils",
    "description": "Provides infrastructure functionalities such as missing value treatment, information value calculation, GINI calculation etc. which are used for developing a traditional credit scorecard as well as a machine learning based model. The functionalities defined are standard steps for any credit underwriting scorecard development, extensively used in financial domain.",
    "version": "0.0.1.0",
    "maintainer": "Arya Poddar <aryapoddar290990@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22436,
    "package_name": "scutr",
    "title": "Balancing Multiclass Datasets for Classification Tasks",
    "description": "Imbalanced training datasets impede many popular classifiers. To balance training data, a combination of oversampling minority classes and undersampling majority classes is useful. This package implements the SCUT (SMOTE and Cluster-based Undersampling Technique) algorithm as described in Agrawal et. al. (2015) <doi:10.5220/0005595502260234>. Their paper uses model-based clustering and synthetic oversampling to balance multiclass training datasets, although other resampling methods are provided in this package.",
    "version": "0.2.0",
    "maintainer": "Keenan Ganz <ganzkeenan1@gmail.com>",
    "url": "https://github.com/s-kganz/scutr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22440,
    "package_name": "sda",
    "title": "Shrinkage Discriminant Analysis and CAT Score Variable Selection",
    "description": "Provides an efficient framework for \n   high-dimensional linear and diagonal discriminant analysis with \n   variable selection.  The classifier is trained using James-Stein-type \n   shrinkage estimators and predictor variables are ranked using \n   correlation-adjusted t-scores (CAT scores).  Variable selection error \n   is controlled using false non-discovery rates or higher criticism.",
    "version": "1.3.9",
    "maintainer": "Korbinian Strimmer <strimmerlab@gmail.com>",
    "url": "https://strimmerlab.github.io/software/sda/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22465,
    "package_name": "sdwd",
    "title": "Sparse Distance Weighted Discrimination",
    "description": "Formulates a sparse distance weighted discrimination (SDWD) for high-dimensional classification and implements a very fast algorithm for computing its solution path with the L1, the elastic-net, and the adaptive elastic-net penalties. More details about the methodology SDWD is seen on Wang and Zou (2016) (<doi:10.1080/10618600.2015.1049700>).",
    "version": "1.0.5",
    "maintainer": "Boxiang Wang <boxiang-wang@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22550,
    "package_name": "semiArtificial",
    "title": "Generator of Semi-Artificial Data",
    "description": "Contains methods to generate and evaluate semi-artificial data sets. \n Based on a given data set different methods learn data properties using machine learning algorithms and\n generate new data with the same properties.\n The package currently includes the following data generators:\n  i) a RBF network based generator using rbfDDA() from package 'RSNNS',\n  ii) a Random Forest based generator for both classification and regression problems\n  iii) a density forest based generator for unsupervised data\n Data evaluation support tools include:\n  a) single attribute based statistical evaluation: mean, median, standard deviation, skewness, kurtosis, medcouple, L/RMC, KS test, Hellinger distance\n  b) evaluation based on clustering using Adjusted Rand Index (ARI) and FM\n  c) evaluation based on classification performance with various learning models, e.g., random forests.",
    "version": "2.4.1",
    "maintainer": "Marko Robnik-Sikonja <marko.robnik@fri.uni-lj.si>",
    "url": "http://lkm.fri.uni-lj.si/rmarko/software/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22553,
    "package_name": "semidist",
    "title": "Measure Dependence Between Categorical and Continuous Variables",
    "description": "Semi-distance and mean-variance (MV) index are proposed to measure the dependence between a categorical random variable and a continuous variable.\n    Test of independence and feature screening for classification problems can be implemented via the two dependence measures.\n    For the details of the methods, see Zhong et al. (2023) <doi:10.1080/01621459.2023.2284988>;\n    Cui and Zhong (2019) <doi:10.1016/j.csda.2019.05.004>;\n    Cui, Li and Zhong (2015) <doi:10.1080/01621459.2014.920256>.",
    "version": "0.1.0",
    "maintainer": "Zhuoxi Li <chainchei@gmail.com>",
    "url": "https://github.com/wzhong41/semidist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22565,
    "package_name": "semtree",
    "title": "Recursive Partitioning for Structural Equation Models",
    "description": "SEM Trees and SEM Forests -- an extension of model-based decision\n    trees and forests to Structural Equation Models (SEM). SEM trees hierarchically\n    split empirical data into homogeneous groups each sharing similar data patterns\n    with respect to a SEM by recursively selecting optimal predictors of these\n    differences. SEM forests are an extension of SEM trees. They are ensembles of\n    SEM trees each built on a random sample of the original data. By aggregating\n    over a forest, we obtain measures of variable importance that are more robust\n    than measures from single trees. A description of the method was published by\n    Brandmaier, von Oertzen, McArdle, & Lindenberger (2013) <doi:10.1037/a0030001> \n    and Arnold, Voelkle, & Brandmaier (2020) <doi:10.3389/fpsyg.2020.564403>.",
    "version": "0.9.23",
    "maintainer": "Andreas M. Brandmaier <andy@brandmaier.de>",
    "url": "https://github.com/brandmaier/semtree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22577,
    "package_name": "sensitivity",
    "title": "Global Sensitivity Analysis of Model Outputs and Importance\nMeasures",
    "description": "A collection of functions for sensitivity analysis of model outputs (factor screening, global sensitivity analysis and robustness analysis), for variable importance measures of data, as well as for interpretability of machine learning models. Most of the functions have to be applied on scalar output, but several functions support multi-dimensional outputs.",
    "version": "1.30.2",
    "maintainer": "Bertrand Iooss <biooss@yahoo.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22587,
    "package_name": "sensory",
    "title": "Simultaneous Model-Based Clustering and Imputation via a\nProgressive Expectation-Maximization Algorithm",
    "description": "Contains the function CUUimpute() which performs model-based clustering and imputation simultaneously.",
    "version": "1.1",
    "maintainer": "Brian C. Franczak <bfrancza@math.mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22589,
    "package_name": "sentencepiece",
    "title": "Text Tokenization using Byte Pair Encoding and Unigram Modelling",
    "description": "Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. \n    Wraps the 'sentencepiece' library <https://github.com/google/sentencepiece> which provides a language independent tokenizer to split text in words and smaller subword units. \n    The techniques are explained in the paper \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\" by Taku Kudo and John Richardson (2018) <doi:10.18653/v1/D18-2012>.\n    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia using 'word2vec', \n    as described in \"BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages\" by Benjamin Heinzerling and Michael Strube (2018) <http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf>.",
    "version": "0.2.4",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "https://github.com/bnosac/sentencepiece",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22590,
    "package_name": "sentiment.ai",
    "title": "Simple Sentiment Analysis Using Deep Learning",
    "description": "Sentiment Analysis via deep learning and gradient boosting models with a lot of the underlying hassle taken care of to make the process as simple as possible. \n  In addition to out-performing traditional, lexicon-based sentiment analysis (see <https://benwiseman.github.io/sentiment.ai/#Benchmarks>),\n  it also allows the user to create embedding vectors for text which can be used in other analyses.\n  GPU acceleration is supported on Windows and Linux.",
    "version": "0.1.1",
    "maintainer": "Ben Wiseman <benjamin.h.wiseman@gmail.com>",
    "url": "https://benwiseman.github.io/sentiment.ai/,\nhttps://github.com/BenWiseman/sentiment.ai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22614,
    "package_name": "seqimpute",
    "title": "Imputation of Missing Data in Sequence Analysis",
    "description": "Multiple imputation of missing data in a dataset using MICT or \n    MICT-timing methods. The core idea of the algorithms is to fill gaps of \n    missing data, which is the typical form of missing data in a longitudinal \n    setting, recursively from their edges. Prediction is based on either a \n    multinomial or random forest regression model. Covariates and \n    time-dependent covariates can be included in the model.",
    "version": "2.2.0",
    "maintainer": "Kevin Emery <kevin.emery@unige.ch>",
    "url": "https://github.com/emerykevin/seqimpute",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22624,
    "package_name": "sequoia",
    "title": "Pedigree Inference from SNPs",
    "description": "Multi-generational pedigree inference from incomplete data on\n    hundreds of SNPs, including parentage assignment and sibship clustering.\n    See Huisman (2017) (<DOI:10.1111/1755-0998.12665>) for more information.",
    "version": "3.1.3",
    "maintainer": "Jisca Huisman <jisca.huisman@gmail.com>",
    "url": "https://jiscah.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22710,
    "package_name": "shadowVIMP",
    "title": "Covariate Selection Based on VIMP Permutation-Like Testing",
    "description": "A statistical method for reducing the number of covariates in\n    an analysis by evaluating Variable Importance Measures (VIMPs) derived\n    from the Random Forest algorithm. It performs statistical tests on the\n    VIMPs and outputs whether the covariate is significant along with the\n    p-values.",
    "version": "1.0.2",
    "maintainer": "Oktawia Miluch <oktawia.miluch@staburo.de>",
    "url": "https://github.com/OktawiaStaburo/shadowVIMP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22714,
    "package_name": "shapFlex",
    "title": "Stochastic Shapley Values with Causal Constraints in Machine Learning",
    "description": "The purpose of 'shapFlex' is to compute stochastic Shapley values which can be used to interpret and assess the fairness of any machine learning model while incorporating causal constraints into the trained model's feature space.",
    "version": "0.3.0",
    "maintainer": "Nickalus Redell <nickalusredell@gmail.com>",
    "url": "https://github.com/nredell/shapFlex",
    "exports": [],
    "topics": ["causal-inference", "causal-networks", "causality", "ensemble", "feature-importance", "iml", "interpretable-machine-learning", "machine-learning", "package", "r", "r-package", "shap", "shapley", "shapley-value", "shapley-values"],
    "score": "NA",
    "stars": 74
  },
  {
    "id": 22721,
    "package_name": "shapper",
    "title": "Wrapper of Python Library 'shap'",
    "description": "Provides SHAP explanations of machine learning models. In applied machine learning, there is a strong belief that we need to strike a balance between interpretability and accuracy. However, in field of the Interpretable Machine Learning, there are more and more new ideas for explaining black-box models. One of the best known method for local explanations is SHapley Additive exPlanations (SHAP) introduced by Lundberg, S., et al., (2016) <arXiv:1705.07874> The SHAP method is used to calculate influences of variables on the particular observation. This method is based on Shapley values, a technique used in game theory. The R package 'shapper' is a port of the Python library 'shap'. ",
    "version": "0.1.3",
    "maintainer": "Szymon Maksymiuk <sz.maksymiuk@gmail.com>",
    "url": "https://github.com/ModelOriented/shapper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22722,
    "package_name": "shapr",
    "title": "Prediction Explanation with Dependence-Aware Shapley Values",
    "description": "Complex machine learning models are often hard to interpret. However, in\n  many situations it is crucial to understand and explain why a model made a specific\n  prediction. Shapley values is the only method for such prediction explanation framework\n  with a solid theoretical foundation. Previously known methods for estimating the Shapley\n  values do, however, assume feature independence. This package implements methods which accounts for any feature\n  dependence, and thereby produces more accurate estimates of the true Shapley values.\n  An accompanying 'Python' wrapper ('shaprpy') is available through PyPI.",
    "version": "1.0.7",
    "maintainer": "Martin Jullum <Martin.Jullum@nr.no>",
    "url": "https://norskregnesentral.github.io/shapr/,\nhttps://github.com/NorskRegnesentral/shapr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22725,
    "package_name": "sharp",
    "title": "Stability-enHanced Approaches using Resampling Procedures",
    "description": "In stability selection (N Meinshausen, P Bühlmann (2010) <doi:10.1111/j.1467-9868.2010.00740.x>) and consensus clustering (S Monti et al (2003) <doi:10.1023/A:1023949509487>), resampling techniques are used to enhance the reliability of the results. In this package (B Bodinier et al (2025) <doi:10.18637/jss.v112.i05>), hyper-parameters are calibrated by maximising model stability, which is measured under the null hypothesis that all selection (or co-membership) probabilities are identical (B Bodinier et al (2023a) <doi:10.1093/jrsssc/qlad058> and B Bodinier et al (2023b) <doi:10.1093/bioinformatics/btad635>). Functions are readily implemented for the use of LASSO regression, sparse PCA, sparse (group) PLS or graphical LASSO in stability selection, and hierarchical clustering, partitioning around medoids, K means or Gaussian mixture models in consensus clustering. ",
    "version": "1.4.8",
    "maintainer": "Barbara Bodinier <barbara.bodinier@gmail.com>",
    "url": "https://github.com/barbarabodinier/sharp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22789,
    "package_name": "shinyML",
    "title": "Compare Supervised Machine Learning Models Using Shiny App",
    "description": "Implementation of a shiny app to easily compare supervised machine learning model performances. \n    You provide the data and configure each model parameter directly on the shiny app.  \n    Different supervised learning algorithms can be tested either on Spark or H2O frameworks to suit your regression and classification tasks.\n    Implementation of available machine learning models on R has been done by Lantz (2013, ISBN:9781782162148).",
    "version": "1.0.1",
    "maintainer": "Jean Bertin <jean.bertin@mines-paris.org>",
    "url": "https://jeanbertinr.github.io/shinyMLpackage/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22797,
    "package_name": "shinyNORRRM",
    "title": "The Ultimate Igneous Norm",
    "description": "The computer program is an efficient igneous norm algorithm and rock classification system written in R but run as shiny app.",
    "version": "0.8.6",
    "maintainer": "Reneé González-Guzmán <rguzman@geociencias.unam.mx>",
    "url": "https://github.com/TheRFrog/shinyNORRRM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22867,
    "package_name": "shinyr",
    "title": "Data Insights Through Inbuilt R Shiny App",
    "description": "It builds dynamic R shiny based dashboards to analyze any CSV files. It provides simple dashboard design to subset the data, perform exploratory data analysis and preliminary machine learning (supervised and unsupervised). It also provides filters based on columns of interest. ",
    "version": "0.4.2",
    "maintainer": "Jayachandra N <itsjay510@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22884,
    "package_name": "shortIRT",
    "title": "Procedures Based on Item Response Theory Models for the\nDevelopment of Short Test Forms",
    "description": "Implement different Item Response Theory (IRT) based procedures for the development of static short test forms (STFs) from a test. Two main procedures are considered, specifically the typical IRT-based procedure for the development of STF, and a recently introduced procedure (Epifania, Anselmi & Robusto, 2022 <doi:10.1007/978-3-031-27781-8_7>).\n        The procedures differ in how the most informative items are selected for the inclusion in the STF, either by considering their item information functions without considering any specific level of the latent trait (typical procedure) or by considering their informativeness with respect to specific levels of the latent trait, denoted as theta targets (the newly introduced procedure). Regarding the latter procedure, three methods are implemented for the definition of the theta targets: (i) theta targets are defined by segmenting the latent trait in equal intervals and considering the midpoint of each interval (equal interval procedure, eip), (ii) by clustering the latent trait to obtain unequal intervals and considering the centroids of the clusters as the theta targets (unequal intervals procedure, uip), and (iii) by letting the user set the specific theta targets of interest (user-defined procedure, udp).\n        For further details on the procedure, please refer to Epifania, Anselmi & Robusto (2022) <doi:10.1007/978-3-031-27781-8_7>.",
    "version": "0.1.4",
    "maintainer": "Ottavia M. Epifania <ottavia.epifania@unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22917,
    "package_name": "sigclust",
    "title": "Statistical Significance of Clustering",
    "description": "SigClust is a statistical method for testing the\n        significance of clustering results. SigClust can be applied to\n        assess the statistical significance of splitting a data set\n        into two clusters. For more than two clusters, SigClust can be\n        used iteratively.",
    "version": "1.1.0.1",
    "maintainer": "Hanwen Huang <hanwenh.unc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22922,
    "package_name": "sigmoid",
    "title": "Sigmoid Functions for Machine Learning",
    "description": "Several different sigmoid functions are implemented, including a wrapper function, SoftMax preprocessing and inverse functions.",
    "version": "1.4.0",
    "maintainer": "Bastiaan Quast <bquast@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23002,
    "package_name": "simpleNeural",
    "title": "An Easy to Use Multilayer Perceptron",
    "description": "Trains neural networks (multilayer perceptrons with one hidden layer) for bi- or multi-class classification.",
    "version": "0.1.3",
    "maintainer": "David Dernoncourt <me@daviddernoncourt.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23023,
    "package_name": "simrel",
    "title": "Simulation of Multivariate Linear Model Data",
    "description": "Researchers have been using simulated data from a multivariate linear model to compare and evaluate different methods, ideas and models. Additionally, teachers and educators have been using a simulation tool to demonstrate and teach various statistical and machine learning concepts.\n    This package helps users to simulate linear model data with a wide range of properties by tuning few parameters such as relevant latent components. In addition, a shiny app as an 'RStudio' gadget gives users a simple interface for using the simulation function. See more on: Sæbø, S., Almøy, T., Helland, I.S. (2015) <doi:10.1016/j.chemolab.2015.05.012> and Rimal, R., Almøy, T., Sæbø, S. (2018) <doi:10.1016/j.chemolab.2018.02.009>.",
    "version": "2.1.0",
    "maintainer": "Raju Rimal <raju.rimal@medisin.uio.no>",
    "url": "https://simulatr.github.io/simrel/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23035,
    "package_name": "simuclustfactor",
    "title": "Simultaneous Clustering and Factorial Decomposition of Three-Way\nDatasets",
    "description": "Implements two iterative techniques called T3Clus and 3Fkmeans, aimed at simultaneously clustering objects and a factorial dimensionality reduction of variables and occasions on three-mode datasets developed by Vichi et al. (2007) <doi:10.1007/s00357-007-0006-x>. Also, we provide a convex combination of these two simultaneous procedures called CT3Clus and based on a hyperparameter alpha (alpha in [0,1], with 3FKMeans for alpha=0 and T3Clus for alpha= 1) also developed by Vichi et al. (2007) <doi:10.1007/s00357-007-0006-x>. Furthermore, we implemented the traditional tandem procedures of T3Clus (TWCFTA) and 3FKMeans (TWFCTA) for sequential clustering-factorial decomposition (TWCFTA), and vice-versa (TWFCTA) proposed by P. Arabie and L. Hubert (1996) <doi:10.1007/978-3-642-79999-0_1>.",
    "version": "0.0.3",
    "maintainer": "Prosper Ablordeppey <pablordeppey@ua.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23061,
    "package_name": "sirus",
    "title": "Stable and Interpretable RUle Set",
    "description": "A regression and classification algorithm based on random forests, which takes the form of a short list of rules. SIRUS combines the simplicity of decision trees with a predictivity close to random forests. The core aggregation principle of random forests is kept, but instead of aggregating predictions, SIRUS aggregates the forest structure: the most frequent nodes of the forest are selected to form a stable rule ensemble model. The algorithm is fully described in the following articles: Benard C., Biau G., da Veiga S., Scornet E. (2021), Electron. J. Statist., 15:427-505 <DOI:10.1214/20-EJS1792> for classification, and Benard C., Biau G., da Veiga S., Scornet E. (2021), AISTATS, PMLR 130:937-945 <http://proceedings.mlr.press/v130/benard21a>, for regression. This R package is a fork from the project ranger (<https://github.com/imbs-hl/ranger>). ",
    "version": "0.3.3",
    "maintainer": "Clement Benard <clement.benard5@gmail.com>",
    "url": "https://gitlab.com/drti/sirus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23091,
    "package_name": "skater",
    "title": "Utilities for SNP-Based Kinship Analysis",
    "description": "Utilities for single nucleotide polymorphism (SNP) based kinship analysis\n    testing and evaluation. The 'skater' package contains functions for importing, parsing, \n    and analyzing pedigree data, performing relationship degree inference, benchmarking \n    relationship degree classification, and summarizing identity by descent (IBD) segment data.\n    Package functions and methods are described in Turner et al. (2021) \"skater: An R package \n    for SNP-based Kinship Analysis, Testing, and Evaluation\" <doi:10.1101/2021.07.21.453083>.",
    "version": "0.1.2",
    "maintainer": "Stephen Turner <vustephen@gmail.com>",
    "url": "https://github.com/signaturescience/skater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23110,
    "package_name": "skmeans",
    "title": "Spherical k-Means Clustering",
    "description": "Algorithms to compute spherical k-means partitions.\n  Features several methods, including a genetic and a fixed-point\n  algorithm and an interface to the CLUTO vcluster program.",
    "version": "0.2-18",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23120,
    "package_name": "slanter",
    "title": "Slanted Matrices and Ordered Clustering",
    "description": "Slanted matrices and ordered clustering for better visualization of similarity data.",
    "version": "0.2-0",
    "maintainer": "Oren Ben-Kiki <oren@ben-kiki.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23221,
    "package_name": "snap",
    "title": "Simple Neural Application",
    "description": "A simple wrapper to easily design vanilla deep neural networks using 'Tensorflow'/'Keras' backend for regression, classification and multi-label tasks, with some tweaks and tricks (skip shortcuts, embedding, feature selection and anomaly detection).",
    "version": "1.1.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/snap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23236,
    "package_name": "snn",
    "title": "Stabilized Nearest Neighbor Classifier",
    "description": "Implement K-nearest neighbor classifier, weighted nearest neighbor classifier, bagged nearest neighbor classifier, optimal weighted nearest neighbor classifier and stabilized nearest neighbor classifier, and perform model selection via 5 fold cross-validation for them. This package also provides functions for computing the classification error and classification instability of a classification procedure.",
    "version": "1.1",
    "maintainer": "Wei Sun <sunweisurrey8@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23288,
    "package_name": "som",
    "title": "Self-Organizing Map",
    "description": "Self-Organizing Map (with application in gene clustering).",
    "version": "0.3-5.2",
    "maintainer": "Jun Yan <jyan@stat.uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23293,
    "package_name": "somhca",
    "title": "Self-Organising Maps Coupled with Hierarchical Cluster Analysis",
    "description": "Implements self-organising maps combined with hierarchical cluster analysis (SOM-HCA) for clustering and visualization of high-dimensional data.\n    The package includes functions to estimate the optimal map size based on various quality measures\n    and to generate a model using the selected dimensions.\n    It also performs hierarchical clustering on the map nodes to group similar units.\n    Documentation about the SOM-HCA method is provided in Pastorelli et al. (2024)\n    <doi:10.1002/xrs.3388>.",
    "version": "0.2.0",
    "maintainer": "Gianluca Pastorelli <gianluca.pastorelli@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23332,
    "package_name": "spFSR",
    "title": "Feature Selection and Ranking via Simultaneous Perturbation\nStochastic Approximation",
    "description": "An implementation of feature selection, weighting and ranking via simultaneous perturbation\n    stochastic approximation (SPSA). The SPSA-FSR algorithm searches for a locally optimal set of\n    features that yield the best predictive performance using some error measures such as mean \n    squared error (for regression problems) and accuracy rate (for classification problems).",
    "version": "2.0.4",
    "maintainer": "David Akman <david.v.akman@gmail.com>",
    "url": "https://www.featureranking.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23373,
    "package_name": "sparktf",
    "title": "Interface for 'TensorFlow' 'TFRecord' Files with 'Apache Spark'",
    "description": "A 'sparklyr' extension that enables reading and writing 'TensorFlow'\n  TFRecord files via 'Apache Spark'.",
    "version": "0.1.0",
    "maintainer": "Kevin Kuo <kevin.kuo@rstudio.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23374,
    "package_name": "sparkxgb",
    "title": "Interface for 'XGBoost' on 'Apache Spark'",
    "description": "A 'sparklyr' <https://spark.posit.co/> extension that provides an R\n  interface for 'XGBoost' <https://github.com/dmlc/xgboost> on 'Apache Spark'. \n  'XGBoost' is an optimized distributed gradient boosting library.",
    "version": "0.2.1",
    "maintainer": "Edgar Ruiz <edgar@posit.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23474,
    "package_name": "speakeasyR",
    "title": "Fast and Robust Multi-Scale Graph Clustering",
    "description": "A graph community detection algorithm that aims to be performant\n    on large graphs and robust, returning consistent results across runs.\n    SpeakEasy 2 (SE2), the underlying algorithm, is described in Chris Gaiteri,\n    David R. Connell & Faraz A. Sultan et al. (2023)\n    <doi:10.1186/s13059-023-03062-0>. The core algorithm is written in 'C',\n    providing speed and keeping the memory requirements low. This implementation\n    can take advantage of multiple computing cores without increasing memory\n    usage. SE2 can detect community structure across scales, making it a good\n    choice for biological data, which often has hierarchical structure. Graphs\n    can be passed to the algorithm as adjacency matrices using base 'R'\n    matrices, the 'Matrix' library, 'igraph' graphs, or any data that can be\n    coerced into a matrix.",
    "version": "0.1.8",
    "maintainer": "David Connell <david32@dcon.addy.io>",
    "url": "https://github.com/SpeakEasy-2/speakeasyR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23487,
    "package_name": "spect",
    "title": "Survival Prediction Ensemble Classification Tool",
    "description": "A tool for survival analysis using a discrete time approach with ensemble binary classification. 'spect' provides a\n\tsimple interface consistent with commonly used R data analysis packages, such as 'caret', a variety of parameter options\n\tto help facilitate search automation, a high degree of transparency to the end-user - all intermediate data sets and\n\tparameters are made available for further analysis and useful, out-of-the-box visualizations of model performance. Methods\n\tfor transforming survival data into discrete-time are adapted from the 'autosurv' package by Suresh et al., (2022)\n\t<doi:10.1186/s12874-022-01679-6>.",
    "version": "1.0",
    "maintainer": "Stephen Abrams <stephen.abrams@gmail.com>",
    "url": "https://github.com/dawdawdo/spect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23495,
    "package_name": "spectralGraphTopology",
    "title": "Learning Graphs from Data via Spectral Constraints",
    "description": "In the era of big data and hyperconnectivity, learning",
    "version": "0.2.3",
    "maintainer": "Ze Vinicius <jvmirca@gmail.com>",
    "url": "https://github.com/convexfi/spectralGraphTopology",
    "exports": [],
    "topics": ["clustering", "machine-learning"],
    "score": "NA",
    "stars": 61
  },
  {
    "id": 23506,
    "package_name": "speedytax",
    "title": "Rapidly Import Classifier Results into 'phyloseq'",
    "description": "Import classification results from the 'RDP Classifier' (Ribosomal Database Project),' 'USEARCH sintax,' 'vsearch sintax' and the 'QIIME2' (Quantitative Insights into Microbial Ecology) classifiers into 'phyloseq' tax_table objects.",
    "version": "1.0.4",
    "maintainer": "John Quensen <quensenj@msu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23521,
    "package_name": "sphereML",
    "title": "Analyzing Students' Performance Dataset in Physics Education\nResearch (SPHERE) using Machine Learning (ML)",
    "description": "A simple package facilitating ML based analysis for physics education research (PER) purposes. The implemented machine learning technique is random forest optimized by item response theory (IRT) for feature selection and genetic algorithm (GA) for hyperparameter tuning. The data analyzed here has been made available in the CRAN repository through the 'spheredata' package. The SPHERE stands for Students' Performance in Physics Education Research (PER). The students are the eleventh graders learning physics at the high school curriculum. We follow the stream of multidimensional students' assessment as probed by some research based assessments in PER. The goal is to predict the students' performance at the end of the learning process. Three learning domains are measured including conceptual understanding, scientific ability, and scientific attitude. Furthermore, demographic backgrounds and potential variables predicting students' performance on physics are also demonstrated.",
    "version": "0.1.1",
    "maintainer": "Purwoko Haryadi Santoso <purwokoharyadisantoso@unsulbar.ac.id>",
    "url": "https://github.com/santosoph/sphereML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23558,
    "package_name": "splinetree",
    "title": "Longitudinal Regression Trees and Forests",
    "description": "Builds regression trees and random forests for longitudinal or functional data using a spline projection method. Implements and extends the work of Yu and Lambert (1999) <doi:10.1080/10618600.1999.10474847>. This method allows trees and forests to be built while considering either level and shape or only shape of response trajectories. ",
    "version": "0.2.0",
    "maintainer": "Anna Neufeld <aneufeld@uw.edu>",
    "url": "https://github.com/anna-neufeld/splinetree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23567,
    "package_name": "splmm",
    "title": "Simultaneous Penalized Linear Mixed Effects Models",
    "description": "Contains functions that fit linear mixed-effects models\n        for high-dimensional data (p>>n) with penalty for both the fixed effects and random effects for variable selection. \n        The details of the algorithm can be found in Luoying Yang PhD thesis (Yang and Wu 2020). The algorithm implementation\n        is based on the R package 'lmmlasso'. \n        Reference: Yang L, Wu TT (2020). Model-Based Clustering of Longitudinal Data in High-Dimensionality. Unpublished thesis.",
    "version": "1.2.0",
    "maintainer": "Eli Sun <eli_sun@urmc.rochester.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23570,
    "package_name": "spls",
    "title": "Sparse Partial Least Squares (SPLS) Regression and\nClassification",
    "description": "Provides functions for fitting a sparse\n        partial least squares (SPLS) regression and classification\n        (Chun and Keles (2010) <doi:10.1111/j.1467-9868.2009.00723.x>).",
    "version": "2.3-2",
    "maintainer": "Valentin Todorov <valentin@todorov.at>",
    "url": "https://github.com/valentint/spls",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23632,
    "package_name": "sr",
    "title": "Smooth Regression - The Gamma Test and Tools",
    "description": "Finds causal connections in precision data, finds lags and embeddings in \n  time series, guides training of neural networks and other smooth models, evaluates \n  their performance, gives a mathematically grounded answer to the over-training \n  problem.  Smooth regression is based on the Gamma test, which measures smoothness\n  in a multivariate relationship.  Causal relations are smooth, noise is not.  \n  'sr' includes the Gamma test and search techniques that use it. \n  References: Evans & Jones (2002) <doi:10.1098/rspa.2002.1010>, \n  AJ Jones (2004) <doi:10.1007/s10287-003-0006-1>.",
    "version": "0.1.0",
    "maintainer": "Wayne Haythorn <support@smoothregression.com>",
    "url": "https://smoothregression.com, https://github.com/haythorn/sr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23654,
    "package_name": "ssc",
    "title": "Semi-Supervised Classification Methods",
    "description": "Provides a collection of self-labeled techniques for semi-supervised \n    classification. In semi-supervised classification, both labeled and unlabeled\n    data are used to train a classifier. This learning paradigm has obtained promising\n    results, specifically in the presence of a reduced set of labeled examples. \n    This package implements a collection of self-labeled techniques to construct a\n    classification model. This family of techniques enlarges the original labeled set \n\tusing the most confident predictions to classify unlabeled data. The techniques \n\timplemented can be applied to classification problems in several domains by the \n\tspecification of a supervised base classifier. At low ratios of labeled data, it \n\tcan be shown to perform better than classical supervised classifiers.",
    "version": "2.1-0",
    "maintainer": "Christoph Bergmeir <c.bergmeir@decsai.ugr.es>",
    "url": "https://github.com/mabelc/SSC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23715,
    "package_name": "stacking",
    "title": "Building Predictive Models with Stacking",
    "description": "Building predictive models with stacking which is a type of ensemble learning. Learners can be specified from those implemented in 'caret'. For more information of the package, see Nukui and Onogi (2023) <doi:10.1101/2023.06.06.543970>.",
    "version": "0.2.1",
    "maintainer": "Akio Onogi <onogiakio@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23819,
    "package_name": "stepmixr",
    "title": "Interface to 'Python' Package 'StepMix'",
    "description": "This is an interface for the 'Python' package\n  'StepMix'. It is a 'Python' package following the scikit-learn API for\n  model-based clustering and generalized mixture modeling (latent class/profile\n  analysis) of continuous and categorical data. 'StepMix' handles missing values\n  through Full Information Maximum Likelihood (FIML) and provides multiple stepwise\n  Expectation-Maximization (EM) estimation methods based on pseudolikelihood\n  theory. Additional features include support for covariates and distal outcomes,\n  various simulation utilities, and non-parametric bootstrapping, which allows\n  inference in semi-supervised and unsupervised settings. Software paper available\n  at <doi:10.18637/jss.v113.i08>.",
    "version": "0.1.3",
    "maintainer": "Charles-Édouard Giguère <ce.giguere@gmail.com>",
    "url": "https://github.com/Labo-Lacourse/StepMixr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23822,
    "package_name": "steprf",
    "title": "Stepwise Predictive Variable Selection for Random Forest",
    "description": "An introduction to several novel predictive variable selection methods for random forest. They are based on various variable importance methods (i.e., averaged variable importance (AVI), and knowledge informed AVI (i.e., KIAVI, and KIAVI2)) and predictive accuracy in stepwise algorithms. For details of the variable selection methods, please see: Li, J., Siwabessy, J., Huang, Z. and Nichol, S. (2019) <doi:10.3390/geosciences9040180>. Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R., Radke, L., Howard, F., Nichol, S. (2017). <DOI: 10.13140/RG.2.2.27686.22085>.",
    "version": "1.0.2",
    "maintainer": "Jin Li <jinli68@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23841,
    "package_name": "stlTDNN",
    "title": "STL Decomposition and TDNN Hybrid Time Series Forecasting",
    "description": "Implementation of hybrid STL decomposition based time delay neural network model for univariate time series forecasting. For method details see Jha G K, Sinha, K (2014). <doi:10.1007/s00521-012-1264-z>, Xiong T, Li C, Bao Y (2018). <doi:10.1016/j.neucom.2017.11.053>. ",
    "version": "0.1.0",
    "maintainer": "Girish Kumar Jha <girish.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23857,
    "package_name": "stockR",
    "title": "Identifying Stocks in Genetic Data",
    "description": "Provides a mixture model for clustering individuals (or sampling groups) into stocks based on their genetic profile. Here, sampling groups are individuals that are sure to come from the same stock (e.g. breeding adults or larvae). The mixture (log-)likelihood is maximised using the EM-algorithm after finding good starting values via a K-means clustering of the genetic data. Details can be found in: Foster, S. D.; Feutry, P.; Grewe, P. M.; Berry, O.; Hui, F. K. C. & Davies (2020) <doi:10.1111/1755-0998.12920>.",
    "version": "1.0.76",
    "maintainer": "Scott D. Foster <scott.foster@data61.csiro.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23898,
    "package_name": "stream",
    "title": "Infrastructure for Data Stream Mining",
    "description": "A framework for data stream modeling and associated data\n    mining tasks such as clustering and classification. The development of\n    this package was supported in part by NSF IIS-0948893, NSF CMMI\n    1728612, and NIH R21HG005912. Hahsler et al (2017)\n    <doi:10.18637/jss.v076.i14>.",
    "version": "2.0-3",
    "maintainer": "Michael Hahsler <mhahsler@lyle.smu.edu>",
    "url": "https://github.com/mhahsler/stream",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23908,
    "package_name": "stressor",
    "title": "Algorithms for Testing Models under Stress",
    "description": "Traditional model evaluation metrics fail to capture model \n    performance under less than ideal conditions. This package employs \n    techniques to evaluate models \"under-stress\". This includes testing \n    models' extrapolation ability, or testing accuracy on specific \n    sub-samples of the overall model space. Details describing stress-testing \n    methods in this package are provided in \n    Haycock (2023) <doi:10.26076/2am5-9f67>. The other primary contribution of\n    this package is provided to R users access to the 'Python' library 'PyCaret'\n    <https://pycaret.org/> for quick and easy access to auto-tuned \n    machine learning models. ",
    "version": "0.2.0",
    "maintainer": "Sam Haycock <haycock.sam@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23930,
    "package_name": "structree",
    "title": "Tree-Structured Clustering",
    "description": "Tree-structured modelling of categorical predictors (Tutz and Berger (2018), <doi:10.1007/s11634-017-0298-6>) or measurement\n    units (Berger and Tutz (2018), <doi:10.1080/10618600.2017.1371030>).",
    "version": "1.1.7",
    "maintainer": "Moritz Berger <Moritz.Berger@imbie.uni-bonn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23935,
    "package_name": "studyStrap",
    "title": "Study Strap and Multi-Study Learning Algorithms",
    "description": "Implements multi-study learning algorithms such as \n\t\tmerging, the study-specific ensemble (trained-on-observed-studies ensemble) the study strap, \n\t\tthe covariate-matched study strap, covariate-profile similarity weighting, and stacking weights. \n\t\tEmbedded within the 'caret' framework, this package allows for a wide range of \n\t\tsingle-study learners (e.g., neural networks, lasso, random forests). \n\t\tThe package offers over 20 default similarity measures and allows for specification of custom \n\t\tsimilarity measures for covariate-profile similarity weighting and an accept/reject step. \n\t\tThis implements methods described in Loewinger, Kishida, Patil, and Parmigiani. (2019)\n\t\t<doi:10.1101/856385>.",
    "version": "1.0.0",
    "maintainer": "Gabriel Loewinger <gloewinger@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23979,
    "package_name": "supclust",
    "title": "Supervised Clustering of Predictor Variables Such as Genes",
    "description": "Methodology for supervised grouping aka \"clustering\" of\n   potentially many predictor variables, such as genes etc, implementing\n   algorithms 'PELORA' and 'WILMA'.",
    "version": "1.1-1",
    "maintainer": "Martin Maechler <maechler@stat.math.ethz.ch>",
    "url": "https://github.com/mmaechler/supclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23990,
    "package_name": "supervisedPRIM",
    "title": "Supervised Classification Learning and Prediction using Patient\nRule Induction Method (PRIM)",
    "description": "The Patient Rule Induction Method (PRIM) is typically\n  used for \"bump hunting\" data mining to identify regions with abnormally\n  high concentrations of data with large or small values. This package\n  extends this methodology so that it can be applied to binary classification\n  problems and used for prediction.",
    "version": "2.0.0",
    "maintainer": "David Shaub <davidshaub@gmx.com>",
    "url": "https://github.com/dashaub/supervisedPRIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24022,
    "package_name": "survML",
    "title": "Tools for Flexible Survival Analysis Using Machine Learning",
    "description": "Statistical tools for analyzing time-to-event data using\n             machine learning. Implements survival stacking for conditional \n             survival estimation, standardized survival function estimation for \n             current status data, and methods for algorithm-agnostic variable\n             importance. See Wolock CJ, Gilbert PB, Simon N, \n             and Carone M (2024) <doi:10.1080/10618600.2024.2304070>. ",
    "version": "1.2.0",
    "maintainer": "Charles Wolock <cwolock@gmail.com>",
    "url": "https://github.com/cwolock/survML,\nhttps://cwolock.github.io/survML/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24052,
    "package_name": "surveynnet",
    "title": "Neural Network for Complex Survey Data",
    "description": "The goal of 'surveynnet' is to extend the functionality of 'nnet', \n  which already supports survey weights, by enabling it to handle clustered\n  and stratified data. It achieves this by incorporating design effects \n  through the use of effective sample sizes as outlined by Chen\n  and Rust (2017), <doi:10.1093/jssam/smw036>, and performed by  \n  'deffCR' in the package 'PracTools' (Valliant, Dever, and Kreuter (2018), \n  <doi:10.1007/978-3-319-93632-1>).",
    "version": "1.0.0",
    "maintainer": "Aaron Cohen <cohenaa@iu.edu>",
    "url": "https://github.com/237triangle/surveynnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24067,
    "package_name": "survivalmodels",
    "title": "Models for Survival Analysis",
    "description": "Implementations of classical and machine learning models for survival analysis, including deep neural networks via 'keras' and 'tensorflow'. Each model includes a separated fit and predict interface with consistent prediction types for predicting risk or survival probabilities. Models are either implemented from 'Python' via 'reticulate' <https://CRAN.R-project.org/package=reticulate>, from code in GitHub packages, or novel implementations using 'Rcpp' <https://CRAN.R-project.org/package=Rcpp>. Neural networks are implemented from the 'Python' package 'pycox' <https://github.com/havakv/pycox>.",
    "version": "0.1.191",
    "maintainer": "Yohann Foucher <yohann.foucher@univ-poitiers.fr>",
    "url": "https://github.com/RaphaelS1/survivalmodels/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24106,
    "package_name": "svmpath",
    "title": "The SVM Path Algorithm",
    "description": "Computes the entire regularization path for the two-class svm classifier\n\t\twith essentially the same cost as a single SVM fit.",
    "version": "0.970",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "url": "http://www.jmlr.org/papers/volume5/hastie04a/hastie04a.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24110,
    "package_name": "svs",
    "title": "Tools for Semantic Vector Spaces",
    "description": "Various tools for semantic vector spaces, such as\n    correspondence analysis (simple, multiple and discriminant), latent\n    semantic analysis, probabilistic latent semantic analysis, non-negative\n    matrix factorization, latent class analysis, EM clustering, logratio\n\tanalysis and log-multiplicative (association) analysis. Furthermore,\n    there are specialized distance measures, plotting functions and some helper\n    functions.",
    "version": "3.1.1",
    "maintainer": "Koen Plevoets <koen.plevoets@ugent.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24111,
    "package_name": "svyROC",
    "title": "Estimation of the ROC Curve and the AUC for Complex Survey Data",
    "description": "Estimate the receiver operating characteristic (ROC) curve, area under the curve (AUC) and optimal cut-off points for individual classification taking into account complex sampling designs when working with complex survey data. Methods implemented in this package are described in: A. Iparragirre, I. Barrio, I. Arostegui (2024) <doi:10.1002/sta4.635>; A. Iparragirre, I. Barrio, J. Aramendi, I. Arostegui (2022) <doi:10.2436/20.8080.02.121>; A. Iparragirre, I. Barrio (2024) <doi:10.1007/978-3-031-65723-8_7>.",
    "version": "1.0.0",
    "maintainer": "Amaia Iparragirre <amaia.iparragirre@ehu.eus>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24121,
    "package_name": "swa",
    "title": "Subsampling Winner Algorithm for Classification",
    "description": "This algorithm conducts variable selection in the classification setting. It repeatedly subsamples variables and runs linear discriminant analysis (LDA) on the subsampled variables. Variables are scored based on the AUC and the t-statistics. Variables then enter a competition and the semi-finalist variables will be evaluated in a final round of LDA classification. The algorithm then outputs a list of variable selected. Qiao, Sun and Fan (2017) <http://people.math.binghamton.edu/qiao/swa.html>.",
    "version": "0.8.1",
    "maintainer": "Xingye Qiao <qiao@math.binghamton.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24127,
    "package_name": "swamp",
    "title": "Visualization, Analysis and Adjustment of High-Dimensional Data\nin Respect to Sample Annotations",
    "description": "Collection of functions to connect the structure of the data with the information on the samples. Three types of associations are covered: 1. linear model of principal components. 2. hierarchical clustering analysis. 3. distribution of features-sample annotation associations. Additionally, the inter-relation between sample annotations can be analyzed. Simple methods are provided for the correction of batch effects and removal of principal components.",
    "version": "1.5.1",
    "maintainer": "Martin Lauss <martin.lauss@med.lu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24161,
    "package_name": "symbolicDA",
    "title": "Analysis of Symbolic Data",
    "description": "Symbolic data analysis methods: importing/exporting data from ASSO XML Files, distance calculation for symbolic data (Ichino-Yaguchi, de Carvalho measure), zoom star plot, 3d interval plot, multidimensional scaling for symbolic interval data, dynamic clustering based on distance matrix, HINoV method for symbolic data, Ichino's feature selection method, principal component analysis for symbolic interval data, decision trees for symbolic data based on optimal split with bagging, boosting and random forest approach (+visualization), kernel discriminant analysis for symbolic data, Kohonen's self-organizing maps for symbolic data, replication and profiling, artificial symbolic data generation.\n (Milligan, G.W., Cooper, M.C. (1985) <doi:10.1007/BF02294245>,\n Breiman, L. (1996), <doi:10.1007/BF00058655>,\n Hubert, L., Arabie, P. (1985), <doi:10.1007%2FBF01908075>,\n Ichino, M., & Yaguchi, H. (1994), <doi:10.1109/21.286391>,\n Rand, W.M. (1971) <doi:10.1080/01621459.1971.10482356>,\n Breckenridge, J.N. (2000) <doi:10.1207/S15327906MBR3502_5>,\n Groenen, P.J.F, Winsberg, S., Rodriguez, O., Diday, E. (2006) <doi:10.1016/j.csda.2006.04.003>,\n Dudek, A. (2007), <doi:10.1007/978-3-540-70981-7_4>).",
    "version": "0.7-2",
    "maintainer": "Andrzej Dudek <andrzej.dudek@ue.wroc.pl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24185,
    "package_name": "synthpop",
    "title": "Generating Synthetic Versions of Sensitive Microdata for\nStatistical Disclosure Control",
    "description": "A tool for producing synthetic versions of microdata containing confidential information so that they are safe to be released to users for exploratory analysis. The key objective of generating synthetic data is to replace sensitive original values with synthetic ones causing minimal distortion of the statistical information contained in the data set. Variables, which can be categorical or continuous, are synthesised one-by-one using sequential modelling. Replacements are generated by drawing from conditional distributions fitted to the original data using parametric or classification and regression trees models. Data are synthesised via the function syn() which can be largely automated, if default settings are used, or with methods defined by the user. Optional parameters can be used to influence the disclosure risk and the analytical quality of the synthesised data. For a description of the implemented method see Nowok, Raab and Dibben (2016) <doi:10.18637/jss.v074.i11>. Functions to assess identity and attribute disclosure for the original and for the synthetic data are included in the package, and their use is illustrated in a vignette on disclosure (Practical Privacy Metrics for Synthetic Data).",
    "version": "1.9-2",
    "maintainer": "Beata Nowok <beata.nowok@gmail.com>",
    "url": "<https://www.synthpop.org.uk/>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24234,
    "package_name": "tabnet",
    "title": "Fit 'TabNet' Models for Classification and Regression",
    "description": "Implements the 'TabNet' model by Sercan O. Arik et al. (2019)\n    <doi:10.48550/arXiv.1908.07442> with 'Coherent Hierarchical Multi-label\n    Classification Networks' by Giunchiglia et al.  <doi:10.48550/arXiv.2010.10151> and\n    provides a consistent interface for fitting and creating predictions.\n    It's also fully compatible with the 'tidymodels' ecosystem.",
    "version": "0.7.0",
    "maintainer": "Christophe Regouby <christophe.regouby@free.fr>",
    "url": "https://mlverse.github.io/tabnet/,\nhttps://github.com/mlverse/tabnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24265,
    "package_name": "tame",
    "title": "Timing, Anatomical, Therapeutic and Chemical Based Medication\nClustering",
    "description": "Agglomerative hierarchical clustering with a bespoke distance \n  measure based on medication similarities in the Anatomical Therapeutic \n  Chemical Classification System, medication timing and \n  medication amount or dosage. Tools for summarizing, illustrating and \n  manipulating the cluster objects are also available.",
    "version": "0.2.0",
    "maintainer": "Anna Laksafoss <adls@ssi.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24316,
    "package_name": "tclust",
    "title": "Robust Trimmed Clustering",
    "description": "Provides functions for robust trimmed clustering. The methods are \n    described in Garcia-Escudero (2008) <doi:10.1214/07-AOS515>, \n    Fritz et al. (2012) <doi:10.18637/jss.v047.i12>, \n    Garcia-Escudero et al. (2011)  <doi:10.1007/s11222-010-9194-z> and others.",
    "version": "2.1-2",
    "maintainer": "Valentin Todorov <valentin@todorov.at>",
    "url": "https://github.com/valentint/tclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24327,
    "package_name": "tdcmStan",
    "title": "Automating the Creation of Stan Code for TDCMs",
    "description": "A collection of functions for automatically creating 'Stan'\n      code for transition diagnostic classification models (TDCMs) as they are\n      defined by Madison and Bradshaw (2018) <DOI:10.1007/s11336-018-9638-5>.\n      This package supports automating the creation of 'Stan' code for TDCMs,\n      fungible TDCMs (i.e., TDCMs with item parameters constrained to be equal\n      across all items), and multi-threaded TDCMs.",
    "version": "3.0.0",
    "maintainer": "Jeffrey Hoover <jeffrey.c.hoover@gmail.com>",
    "url": "https://github.com/atlas-aai/tdcmStan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24350,
    "package_name": "teigen",
    "title": "Model-Based Clustering and Classification with the Multivariate\nt Distribution",
    "description": "Fits mixtures of multivariate t-distributions (with eigen-decomposed covariance structure) via the expectation conditional-maximization algorithm under a clustering or classification paradigm.",
    "version": "2.2.2",
    "maintainer": "Jeffrey L. Andrews <jeff.andrews@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24358,
    "package_name": "temper",
    "title": "Temporal Encoder-Masked Probabilistic Ensemble Regressor",
    "description": "Implements a probabilistic ensemble time-series forecaster that combines an auto-encoder with a neural decision forest whose split variables are learned through a differentiable feature-mask layer. Functions are written with 'torch' tensors and provide CRPS (Continuous Ranked Probability Scores) training plus mixture-distribution post-processing.",
    "version": "1.1.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/temper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24376,
    "package_name": "tensorsparse",
    "title": "Multiway Clustering via Tensor Block Models",
    "description": "Implements the multiway sparse clustering approach of M. Wang and Y. Zeng, \"Multiway clustering via tensor block models\". Advances in Neural Information Processing System 32 (NeurIPS), 715-725, 2019.",
    "version": "3.0",
    "maintainer": "Yuchen Zeng <yzeng58@wisc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24437,
    "package_name": "textdata",
    "title": "Download and Load Various Text Datasets",
    "description": "Provides a framework to download, parse, and store text\n    datasets on the disk and load them when needed. Includes various\n    sentiment lexicons and labeled text data sets for classification and\n    analysis.",
    "version": "0.4.5",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "url": "https://emilhvitfeldt.github.io/textdata/,\nhttps://github.com/EmilHvitfeldt/textdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24440,
    "package_name": "textfeatures",
    "title": "Extracts Features from Text",
    "description": "A tool for extracting some generic features (e.g., number of",
    "version": "0.3.3",
    "maintainer": "",
    "url": "https://github.com/mkearney/textfeatures",
    "exports": [],
    "topics": ["feature-extraction", "machine-learning", "mkearney-r-package", "neural-network", "neural-networks", "r", "rstats", "text-mining", "word2vec"],
    "score": "NA",
    "stars": 166
  },
  {
    "id": 24444,
    "package_name": "textplot",
    "title": "Text Plots",
    "description": "Visualise complex relations in texts. This is done by providing functionalities for displaying \n    text co-occurrence networks, text correlation networks, dependency relationships as well as text clustering and semantic text 'embeddings'. \n    Feel free to join the effort of providing interesting text visualisations.",
    "version": "0.2.2",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "url": "https://github.com/bnosac/textplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24457,
    "package_name": "tfNeuralODE",
    "title": "Create Neural Ordinary Differential Equations with 'tensorflow'",
    "description": "Provides a framework for the creation and use of Neural ordinary\n    differential equations with the 'tensorflow' and 'keras' packages. \n    The idea of Neural ordinary differential equations comes from \n    Chen et al. (2018) <doi:10.48550/arXiv.1806.07366>, and \n    presents a novel way of learning and solving differential systems. ",
    "version": "0.1.0",
    "maintainer": "Shayaan Emran <shayaan.emran@gmail.com>",
    "url": "https://github.com/semran9/tfNeuralODE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24460,
    "package_name": "tfautograph",
    "title": "Autograph R for 'Tensorflow'",
    "description": "Translate R control flow expressions into 'Tensorflow' graphs.",
    "version": "0.3.2",
    "maintainer": "Tomasz Kalinowski <kalinowskit@gmail.com>",
    "url": "https://t-kalinowski.github.io/tfautograph/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24462,
    "package_name": "tfdeploy",
    "title": "Deploy 'TensorFlow' Models",
    "description": "Tools to deploy 'TensorFlow' <https://www.tensorflow.org/> models across \n  multiple services. Currently, it provides a local server for testing 'cloudml' \n  compatible services.",
    "version": "0.6.1",
    "maintainer": "Daniel Falbel <daniel@rstudio.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24465,
    "package_name": "tfevents",
    "title": "Write Events for 'TensorBoard'",
    "description": "Provides a convenient way to log scalars, images, audio, and histograms in the 'tfevent' record file format. \n  Logged data can be visualized on the fly using 'TensorBoard', a web based tool that focuses on visualizing the training \n  progress of machine learning models.",
    "version": "0.0.4",
    "maintainer": "Daniel Falbel <daniel@posit.co>",
    "url": "https://github.com/mlverse/tfevents,\nhttps://mlverse.github.io/tfevents/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24467,
    "package_name": "tfio",
    "title": "Interface to 'TensorFlow IO'",
    "description": "Interface to 'TensorFlow IO', Datasets and filesystem extensions maintained by `TensorFlow SIG-IO` <https://github.com/tensorflow/community/blob/master/sigs/io/CHARTER.md>.",
    "version": "0.4.1",
    "maintainer": "Yuan Tang <terrytangyuan@gmail.com>",
    "url": "https://github.com/tensorflow/io",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24478,
    "package_name": "tgml",
    "title": "Tree Guided Machine Learning for Personalized Predictions and\nPrecision Diagnostics",
    "description": "Generalization of the classification and regression tree (CART) model that partitions subjects into terminal nodes and tailors machine learning model to each terminal node.",
    "version": "0.3.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24551,
    "package_name": "tidycode",
    "title": "Analyze Lines of R Code the Tidy Way",
    "description": "Analyze lines of R code using tidy principles. This allows you to \n    input lines of R code and output a data frame with one row per function \n    included. Additionally, it facilitates code classification via included lexicons.",
    "version": "0.1.1",
    "maintainer": "Lucy D'Agostino McGowan <lucydagostino@gmail.com>",
    "url": "https://github.com/LucyMcGowan/tidycode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24578,
    "package_name": "tidyhte",
    "title": "Tidy Estimation of Heterogeneous Treatment Effects",
    "description": "Estimates heterogeneous treatment effects using tidy semantics \n    on experimental or observational data.  Methods are based on the doubly-robust\n    learner of Kennedy (2023) <doi:10.1214/23-EJS2157>. You provide a simple\n    recipe for what machine learning algorithms to use in estimating the nuisance\n    functions and 'tidyhte' will take care of cross-validation, estimation, model\n    selection, diagnostics and construction of relevant quantities of interest about\n    the variability of treatment effects.",
    "version": "1.0.4",
    "maintainer": "Drew Dimmery <cran@ddimmery.com>",
    "url": "https://github.com/ddimmery/tidyhte\nhttps://ddimmery.github.io/tidyhte/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24648,
    "package_name": "tightClust",
    "title": "Tight Clustering",
    "description": "The functions needed to perform tight clustering Algorithm.",
    "version": "1.1",
    "maintainer": "Chi Song <song.1188@osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24721,
    "package_name": "titanic",
    "title": "Titanic Passenger Survival Data Set",
    "description": "This data set provides information on the fate of passengers on\n    the fatal maiden voyage of the ocean liner \"Titanic\", summarized according\n    to economic status (class), sex, age and survival. Whereas the base R\n    Titanic data found by calling data(\"Titanic\") is an array resulting from\n    cross-tabulating 2201 observations, these data sets are the individual\n    non-aggregated observations and formatted in a machine learning context\n    with a training sample, a testing sample, and two additional data sets\n    that can be used for deeper machine learning analysis. These data sets\n    are also the data sets downloaded from the Kaggle competition and thus\n    lowers the barrier to entry for users new to R or machine learing.",
    "version": "0.1.0",
    "maintainer": "Paul Hendricks <paul.hendricks.2013@owu.edu>",
    "url": "https://github.com/paulhendricks/titanic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24820,
    "package_name": "torchvision",
    "title": "Models, Datasets and Transformations for Images",
    "description": "Provides access to datasets, models and preprocessing\n    facilities for deep learning with images. Integrates seamlessly\n    with the 'torch' package and it's 'API' borrows heavily from\n    'PyTorch' vision package.",
    "version": "0.8.0",
    "maintainer": "Daniel Falbel <daniel@posit.co>",
    "url": "https://torchvision.mlverse.org,\nhttps://github.com/mlverse/torchvision",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24821,
    "package_name": "torchvisionlib",
    "title": "Additional Operators for Image Models",
    "description": "Implements additional operators for computer vision models, including\n    operators necessary for image segmentation and object detection deep learning\n    models.",
    "version": "0.6.0",
    "maintainer": "Daniel Falbel <daniel@rstudio.com>",
    "url": "https://github.com/mlverse/torchvisionlib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24857,
    "package_name": "trackdem",
    "title": "Particle Tracking and Demography",
    "description": "Obtain population density and body size structure, using video material or image sequences as input. Functions assist in the creation of image sequences from videos, background detection and subtraction, particle identification and tracking. An artificial neural network can be trained for noise filtering. The goal is to supply accurate estimates of population size, structure and/or individual behavior, for use in  evolutionary and ecological studies.",
    "version": "0.7.2",
    "maintainer": "Marjolein Bruijning <m.bruijning@uva.nl>",
    "url": "https://github.com/marjoleinbruijning/trackdem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24877,
    "package_name": "traj",
    "title": "Clustering of Functional Data Based on Measures of Change",
    "description": "Implements a three-step procedure in the spirit of Leffondre et al. (2004) to identify clusters of individual longitudinal trajectories. The procedure involves (1) computing a number of \"measures of change\" capturing various features of the trajectories; (2) using a Principal Component Analysis based dimension reduction algorithm to select a subset of measures and (3) using the k-medoids or k-means algorithm to identify clusters of trajectories.",
    "version": "2.2.1",
    "maintainer": "Laurence Boulanger <laurence.boulanger@umontreal.ca>",
    "url": "https://CRAN.R-project.org/package=traj",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24897,
    "package_name": "transforEmotion",
    "title": "Sentiment Analysis for Text, Image and Video using Transformer\nModels",
    "description": "Implements sentiment analysis using huggingface <https://huggingface.co> transformer zero-shot classification model pipelines for text and image data. The default text pipeline is Cross-Encoder's DistilRoBERTa <https://huggingface.co/cross-encoder/nli-distilroberta-base> and default image/video pipeline is Open AI's CLIP  <https://huggingface.co/openai/clip-vit-base-patch32>. All other zero-shot classification model pipelines can be implemented using their model name from <https://huggingface.co/models?pipeline_tag=zero-shot-classification>.",
    "version": "0.1.6",
    "maintainer": "Aleksandar Tomašević <atomashevic@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24900,
    "package_name": "transformer",
    "title": "Implementation of Transformer Deep Neural Network with Vignettes",
    "description": "Transformer is a Deep Neural Network Architecture based i.a. on the Attention mechanism (Vaswani et al. (2017) <doi:10.48550/arXiv.1706.03762>). ",
    "version": "0.2.0",
    "maintainer": "Bastiaan Quast <bquast@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24901,
    "package_name": "transformerForecasting",
    "title": "Transformer Deep Learning Model for Time Series Forecasting",
    "description": "Time series forecasting faces challenges due to the non-stationarity, nonlinearity, and chaotic nature of the data. Traditional deep learning models like Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) process data sequentially but are inefficient for long sequences. To overcome the limitations of these models, we proposed a transformer-based deep learning architecture utilizing an attention mechanism for parallel processing, enhancing prediction accuracy and efficiency. This paper presents user-friendly code for the implementation of the proposed transformer-based deep learning architecture utilizing an attention mechanism for parallel processing. References:  Nayak et al. (2024) <doi:10.1007/s40808-023-01944-7> and Nayak et al. (2024) <doi:10.1016/j.simpa.2024.100716>.",
    "version": "0.1.0",
    "maintainer": "G H Harish Nayak <harishnayak626@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24929,
    "package_name": "tree",
    "title": "Classification and Regression Trees",
    "description": "Classification and regression trees.",
    "version": "1.0-45",
    "maintainer": "Brian Ripley <Brian.Ripley@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24932,
    "package_name": "treeClust",
    "title": "Cluster Distances Through Trees",
    "description": "Create a measure of inter-point dissimilarity useful \n for clustering mixed data, and, optionally, perform the clustering.",
    "version": "1.1-7.1",
    "maintainer": "Sam Buttrey <buttrey@nps.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24981,
    "package_name": "trimcluster",
    "title": "Cluster Analysis with Trimming",
    "description": "Trimmed k-means clustering. The method is described in Cuesta-Albertos et al. (1997) <doi:10.1214/aos/1031833664>.",
    "version": "0.2-0",
    "maintainer": "Valentin Todorov <valentin@todorov.at>",
    "url": "https://github.com/valentint/trimcluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24985,
    "package_name": "trinROC",
    "title": "Statistical Tests for Assessing Trinormal ROC Data",
    "description": "Several statistical test functions as well as a function for exploratory data\n  analysis to investigate classifiers allocating individuals to one of three disjoint and\n  ordered classes. In a single classifier assessment the discriminatory power is compared\n  to classification by chance. In a comparison of two classifiers the null hypothesis\n  corresponds to equal discriminatory power of the two classifiers.\n  See also \"ROC Analysis for Classification and Prediction in Practice\" by Nakas, Bantis\n  and Gatsonis (2023), ISBN 9781482233704.",
    "version": "0.7",
    "maintainer": "Reinhard Furrer <reinhard.furrer@uzh.ch>",
    "url": "https://www.math.uzh.ch/pages/trinROC/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25070,
    "package_name": "tspredit",
    "title": "Time Series Prediction with Integrated Tuning",
    "description": "\n  Time series prediction is a critical task in data analysis, requiring not only the selection of appropriate models, but also suitable data preprocessing and tuning strategies. \n  TSPredIT (Time Series Prediction with Integrated Tuning) is a framework that provides a seamless integration of data preprocessing, decomposition, model training, hyperparameter optimization, and evaluation. \n  Unlike other frameworks, TSPredIT emphasizes the co-optimization of both preprocessing and modeling steps, improving predictive performance. \n  It supports a variety of statistical and machine learning models, filtering techniques, outlier detection, data augmentation, and ensemble strategies. \n  More information is available in Salles et al. <doi:10.1007/978-3-662-68014-8_2>.",
    "version": "1.2.747",
    "maintainer": "Eduardo Ogasawara <eogasawara@ieee.org>",
    "url": "https://cefet-rj-dal.github.io/tspredit/,\nhttps://github.com/cefet-rj-dal/tspredit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25108,
    "package_name": "tuneRanger",
    "title": "Tune Random Forest of the 'ranger' Package",
    "description": "Tuning random forest with one line. The package is mainly based on the packages 'ranger' and 'mlrMBO'.",
    "version": "0.8.1",
    "maintainer": "Philipp Probst <philipp_probst@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25138,
    "package_name": "tweetbotornot2",
    "title": "Detect Twitter Bots",
    "description": "Providing an out-of-the-box classifier for detecting Twitter bots that",
    "version": "0.0.1",
    "maintainer": "",
    "url": "https://github.com/mkearney/tweetbotornot2",
    "exports": [],
    "topics": ["bot-detection", "bot-detector", "classification", "data-science", "machine-learning", "r", "r-package", "rstats", "rtweet", "twitter", "twitter-api", "twitter-bot-detection", "twitter-bots", "xgboost"],
    "score": "NA",
    "stars": 93
  },
  {
    "id": 25150,
    "package_name": "twl",
    "title": "Two-Way Latent Structure Clustering Model",
    "description": "Implementation of a Bayesian two-way latent structure model for integrative genomic clustering.  The model clusters samples in relation to distinct data sources, with each subject-dataset receiving a latent cluster label, though cluster labels have across-dataset meaning because of the model formulation.  A common scaling across data sources is unneeded, and inference is obtained by a Gibbs Sampler.  The model can fit multivariate Gaussian distributed clusters or a heavier-tailed modification of a Gaussian density.  Uniquely among integrative clustering models, the formulation makes no nestedness assumptions of samples across data sources -- the user can still fit the model if a study subject only has information from one data source. The package provides a variety of post-processing functions for model examination including ones for quantifying observed alignment of clusterings across genomic data sources.  Run time is optimized so that analyses of datasets on the order of thousands of features on fewer than 5 datasets and hundreds of subjects can converge in 1 or 2 days on a single CPU.  See \"Swanson DM, Lien T, Bergholtz H, Sorlie T, Frigessi A, Investigating Coordinated Architectures Across Clusters in Integrative Studies: a Bayesian Two-Way Latent Structure Model, 2018, <doi:10.1101/387076>, Cold Spring Harbor Laboratory\" at <https://www.biorxiv.org/content/early/2018/08/07/387076.full.pdf> for model details.",
    "version": "1.0",
    "maintainer": "Michael Swanson <dms866@mail.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25155,
    "package_name": "twomodeclusteringGA",
    "title": "Genetic Algorithm Based Two-Mode Clustering",
    "description": "Implements two-mode clustering (biclustering) using \n    genetic algorithms. The method was first introduced in \n    Hageman et al. (2008) <doi:10.1007/s11306-008-0105-7>. The \n    package provides tools for fitting, visualization, and \n    validation of two-mode cluster structures in data matrices.",
    "version": "1.0.0",
    "maintainer": "Jos Hageman <jos.hageman@wur.nl>",
    "url": "https://github.com/joshageman/twomodeclusteringGA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25180,
    "package_name": "uHMM",
    "title": "Construct an Unsupervised Hidden Markov Model",
    "description": "Construct a Hidden Markov Model with states learnt by unsupervised classification.",
    "version": "1.0",
    "maintainer": "Paul TERNYNCK <ternynck@lisic.univ-littoral.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25190,
    "package_name": "uclust",
    "title": "Clustering and Classification Inference with U-Statistics",
    "description": "Clustering and classification inference for high dimension low sample size (HDLSS)\n    data with U-statistics. The package contains implementations of nonparametric statistical\n    tests for sample homogeneity, group separation, clustering, and classification of \n    multivariate data. The methods have high statistical power and are tailored for data\n    in which the dimension L is much larger than sample size n. See Gabriela B. Cybis,\n    Marcio Valk and Sílvia RC Lopes (2018) <doi:10.1080/00949655.2017.1374387>, Marcio \n    Valk and Gabriela B. Cybis (2020) <doi:10.1080/10618600.2020.1796398>, Debora Z. Bello, Marcio \n    Valk and Gabriela B. Cybis (2021) <arXiv:2106.09115>. ",
    "version": "1.0.0",
    "maintainer": "Gabriela Cybis <gcybis@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25235,
    "package_name": "uni.survival.tree",
    "title": "A Survival Tree Based on Stabilized Score Tests for\nHigh-dimensional Covariates",
    "description": "A classification (decision) tree is constructed from survival data with high-dimensional covariates.\n The method is a robust version of the logrank tree, where the variance is stabilized.\n The main function \"uni.tree\" returns a classification tree for a given survival dataset.\n The inner nodes (splitting criterion) are selected by minimizing the P-value of the two-sample the score tests.\n The decision of declaring terminal nodes (stopping criterion) is the P-value threshold given by an argument (specified by user).\n This tree construction algorithm is proposed by Emura et al. (2021, in review).",
    "version": "1.5",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25242,
    "package_name": "unifiedml",
    "title": "Unified Interface for Machine Learning Models",
    "description": "Provides a unified R6-based interface for various machine learning models with automatic interface detection, consistent cross-validation, model interpretations via numerical derivatives, and visualization. Supports both regression and classification tasks with any model function that follows R's standard modeling conventions (formula or matrix interface).",
    "version": "0.1.0",
    "maintainer": "T. Moudiki <thierry.moudiki@gmail.com>",
    "url": "https://github.com/Techtonique/unifiedml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25276,
    "package_name": "unvotes",
    "title": "United Nations General Assembly Voting Data",
    "description": "Historical voting data of the United Nations General Assembly. This\n    includes votes for each country in each roll call, as well as descriptions and\n    topic classifications for each vote.",
    "version": "0.3.0",
    "maintainer": "David Robinson <admiral.david@gmail.com>",
    "url": "https://github.com/dgrtwo/unvotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25313,
    "package_name": "useful",
    "title": "A Collection of Handy, Useful Functions",
    "description": "A set of little functions that have been found useful to do little\n    odds and ends such as plotting the results of K-means clustering, substituting\n    special text characters, viewing parts of a data.frame, constructing formulas\n    from text and building design and response matrices.",
    "version": "1.2.6.1",
    "maintainer": "Jared P. Lander <packages@jaredlander.com>",
    "url": "https://github.com/jaredlander/useful",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25320,
    "package_name": "uskewFactors",
    "title": "Model-Based Clustering via Mixtures of Unrestricted Skew-t\nSactor Analyzer Models",
    "description": "Implements mixtures of unrestricted skew-t factor analyzer models via the EM algorithm.",
    "version": "2.0",
    "maintainer": "Paula M. Murray <paula.murray@math.mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25362,
    "package_name": "validann",
    "title": "Validation Tools for Artificial Neural Networks",
    "description": "Methods and tools for analysing and validating the outputs\n    and modelled functions of artificial neural networks (ANNs) in terms\n    of predictive, replicative and structural validity. Also provides a\n    method for fitting feed-forward ANNs with a single hidden layer.",
    "version": "1.2.1",
    "maintainer": "Greer B. Humphrey <greer.humphrey@student.adelaide.edu.au>",
    "url": "http://github.com/gbhumphrey1/validann",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25384,
    "package_name": "varImp",
    "title": "RF Variable Importance for Arbitrary Measures",
    "description": "Computes the random forest variable importance (VIMP) for the conditional inference random forest (cforest) of the 'party' package. Includes a function (varImp) that computes the VIMP for arbitrary measures from the 'measures' package. For calculating the VIMP regarding the measures accuracy and AUC two extra functions exist (varImpACC and varImpAUC).",
    "version": "0.4",
    "maintainer": "Philipp Probst <philipp_probst@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25386,
    "package_name": "varSel",
    "title": "Sequential Forward Floating Selection using Jeffries-Matusita\nDistance",
    "description": "Feature selection using Sequential Forward Floating feature Selection and Jeffries-Matusita distance. It returns a suboptimal set of features to use for image classification. Reference: Dalponte, M., Oerka, H.O., Gobakken, T., Gianelle, D. & Naesset, E. (2013). Tree Species Classification in Boreal Forests With Hyperspectral Data. IEEE Transactions on Geoscience and Remote Sensing, 51, 2632-2645, <DOI:10.1109/TGRS.2012.2216272>.",
    "version": "0.2",
    "maintainer": "Michele Dalponte <michele.dalponte@fmach.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25387,
    "package_name": "varSelRF",
    "title": "Variable Selection using Random Forests",
    "description": "Variable selection from random forests using both\n        backwards variable elimination (for the selection of small sets\n        of non-redundant variables) and selection based on the\n        importance spectrum (somewhat similar to scree plots; for the\n        selection of large, potentially highly-correlated variables).\n        Main applications in high-dimensional data (e.g., microarray\n        data, and other genomics and proteomics applications). ",
    "version": "0.7-8",
    "maintainer": "Ramon Diaz-Uriarte <rdiaz02@gmail.com>",
    "url": "http://ligarto.org/rdiaz/Software/Software.html,\nhttp://ligarto.org/rdiaz/Papers/rfVS/randomForestVarSel.html,\nhttps://github.com/rdiaz02/varSelRF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25392,
    "package_name": "varclust",
    "title": "Variables Clustering",
    "description": "Performs clustering of quantitative variables,\n    assuming that clusters lie in low-dimensional subspaces. Segmentation of\n    variables, number of clusters and their dimensions are selected based on\n    BIC. Candidate models are identified based on many runs of K-means\n    algorithm with different random initializations of cluster centers.",
    "version": "0.9.4",
    "maintainer": "Piotr Sobczyk <pj.sobczyk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25406,
    "package_name": "varoc",
    "title": "Value Added Receiver Operating Characteristics Curve",
    "description": "A continuous version of the receiver operating characteristics (ROC) curve to assess both classification and continuity performances of biomarkers, diagnostic tests, or risk prediction models.",
    "version": "1.0.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25445,
    "package_name": "veesa",
    "title": "Pipeline for Explainable Machine Learning with Functional Data",
    "description": "Implements the Variable importance Explainable Elastic Shape Analysis pipeline for explainable machine learning with functional data inputs. Converts training and testing data functional inputs to elastic shape analysis principal components that account for vertical and/or horizontal variability. Computes feature importance to identify important principal components and visualizes variability captured by functional principal components. See Goode et al. (2025) <doi:10.48550/arXiv.2501.07602> for technical details about the methodology.",
    "version": "0.1.7",
    "maintainer": "Katherine Goode <kjgoode@sandia.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25451,
    "package_name": "vegclust",
    "title": "Fuzzy Clustering of Vegetation Data",
    "description": "A set of functions to: (1) perform fuzzy clustering of vegetation data (De Caceres et al, 2010) <doi:10.1111/j.1654-1103.2010.01211.x>; (2) to assess ecological community similarity on the basis of structure and composition (De Caceres et al, 2013) <doi:10.1111/2041-210X.12116>.",
    "version": "2.0.3",
    "maintainer": "Miquel De Cáceres <miquelcaceres@gmail.com>",
    "url": "https://emf-creaf.github.io/vegclust/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25502,
    "package_name": "vimpclust",
    "title": "Variable Importance in Clustering",
    "description": "An implementation of methods related to sparse clustering and variable importance \n    in clustering. The package currently allows to perform sparse k-means clustering with a group \n    penalty, so that it automatically selects groups of numerical features. It also allows to \n    perform sparse clustering and variable selection on mixed data (categorical and numerical \n    features), by preprocessing each categorical feature as a group of numerical features.\n    Several methods for visualizing and exploring the results are also provided. \n    M. Chavent, J. Lacaille, A. Mourer and M. Olteanu (2020)<https://www.esann.org/sites/default/files/proceedings/2020/ES2020-103.pdf>.",
    "version": "0.1.0",
    "maintainer": "Madalina Olteanu <madalina.olteanu@dauphine.psl.eu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25540,
    "package_name": "visualpred",
    "title": "Visualization 2D of Binary Classification Models",
    "description": "Visual contour and 2D point and contour plots for binary classification modeling under algorithms such as 'glm', 'rf', 'gbm', 'nnet' and 'svm', presented over two dimensions generated by 'famd' and 'mca' methods. Package 'FactoMineR' for multivariate reduction functions and package 'MBA' for interpolation functions are used. The package can be used to visualize the discriminant power of input variables and algorithmic modeling, explore outliers, compare algorithm behaviour, etc. It has been created initially for teaching purposes, but it has also many practical uses under the 'XAI' paradigm.",
    "version": "0.1.2",
    "maintainer": "Javier Portela <javipgm@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25543,
    "package_name": "visxhclust",
    "title": "A Shiny App for Visual Exploration of Hierarchical Clustering",
    "description": "A Shiny application and functions for visual exploration of hierarchical clustering with numeric datasets. Allows users to iterative set hyperparameters, select features and evaluate results through various plots and computation of evaluation criteria.",
    "version": "1.1.0",
    "maintainer": "Rafael Henkin <r.henkin@qmul.ac.uk>",
    "url": "https://github.com/rhenkin/visxhclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25544,
    "package_name": "vita",
    "title": "Variable Importance Testing Approaches",
    "description": "Implements the novel testing approach by Janitza et al.(2015)\n    <http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-25587-4>\n    for the permutation variable importance measure in a random forest and the\n    PIMP-algorithm by Altmann et al.(2010) <doi:10.1093/bioinformatics/btq134>.\n    Janitza et al.(2015) <http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-25587-4>\n    do not use the \"standard\" permutation variable\n    importance but the cross-validated permutation variable\n    importance for the novel test approach. The cross-validated\n    permutation variable importance is not based on the out-of-bag\n    observations but uses a similar strategy which is inspired by\n    the cross-validation procedure. The novel test approach can be\n    applied for classification trees as well as for regression\n    trees. However, the use of the novel testing approach has not\n    been tested for regression trees so far, so this routine is\n    meant for the expert user only and its current state is rather\n    experimental.",
    "version": "1.0.0",
    "maintainer": "Ender Celik <celik.p.ender@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25558,
    "package_name": "vmdTDNN",
    "title": "VMD Based Time Delay Neural Network Model",
    "description": "Forecasting univariate time series with Variational Mode Decomposition (VMD) based time delay neural network models.For method details see Konstantin, D.and Dominique, Z. (2014). <doi:10.1109/TSP.2013.2288675>. ",
    "version": "0.1.1",
    "maintainer": "Kapil Choudhary <kapiliasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25563,
    "package_name": "vocaldia",
    "title": "Create and Manipulate Vocalisation Diagrams",
    "description": "Create adjacency matrices of vocalisation graphs from\n  dataframes containing sequences of speech and silence intervals,\n  transforming these matrices into Markov diagrams, and generating\n  datasets for classification of these diagrams by 'flattening' them\n  and adding global properties (functionals) etc.  Vocalisation\n  diagrams date back to early work in psychiatry (Jaffe and Feldstein,\n  1970) and social psychology (Dabbs and Ruback, 1987) but have only\n  recently been employed as a data representation method for machine\n  learning tasks including meeting segmentation (Luz, 2012)\n  <doi:10.1145/2328967.2328970> and classification (Luz,\n  2013) <doi:10.1145/2522848.2533788>.",
    "version": "0.8.4",
    "maintainer": "Saturnino Luz <luzs@acm.org>",
    "url": "https://git.ecdf.ed.ac.uk/sluzfil/vocaldia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25594,
    "package_name": "vscc",
    "title": "Variable Selection for Clustering and Classification",
    "description": "Performs variable selection/feature reduction under a clustering or \n  classification framework. In particular, it can be used in an automated fashion \n  using mixture model-based methods ('teigen' and 'mclust' are currently supported).\n  Can account for mixtures of non-Gaussian distributions via Manly transform (via 'ManlyMix').\n  See Andrews and McNicholas (2014) <doi:10.1007/s00357-013-9139-2> and Neal and McNicholas (2023)\n  <doi:10.48550/arXiv.2305.16464>. ",
    "version": "0.8",
    "maintainer": "Mackenzie R. Neal <nealm6@mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25607,
    "package_name": "vtype",
    "title": "Estimates the Variable Type in Error Afflicted Data",
    "description": "Estimates the type of variables in non-quality controlled data. The prediction is based on a random forest model, trained on over 5000 medical variables with accuracy of 99%. The accuracy can hardy depend on type and coding style of data.",
    "version": "0.8",
    "maintainer": "Andreas Schulz <ades-s@web.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25663,
    "package_name": "waterYearType",
    "title": "Sacramento and San Joaquin Valley Water Year Types",
    "description": "Provides Water Year Hydrologic Classification Indices based on measured \n    unimpaired runoff (in million acre-feet). Data is provided by California Department of Water Resources\n    and subject to revision.",
    "version": "1.0.1",
    "maintainer": "Sadie Gill <sgill@flowwest.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25690,
    "package_name": "wconf",
    "title": "Weighted Confusion Matrix",
    "description": "Allows users to create weighted confusion matrices and accuracy\n    metrics that help with the model selection process for classification\n    problems, where distance from the correct category is important. The\n    package includes several weighting schemes which can be parameterized, as\n    well as custom configuration options. Furthermore, users can decide\n    whether they wish to positively or negatively affect the accuracy score\n    as a result of applying weights to the confusion matrix. Functions are\n    included to calculate accuracy metrics for imbalanced data. Finally,\n    'wconf' integrates well with the 'caret' package, but it can also work\n    standalone when provided data in matrix form.\n    References:\n    Kuhn, M. (2008) \"Building Perspective Models in R Using the caret Package\"\n    <doi:10.18637/jss.v028.i05>\n    Monahov, A. (2021) \"Model Evaluation with Weighted Threshold Optimization\n    (and the mewto R package)\" <doi:10.2139/ssrn.3805911>\n    Monahov, A. (2024) \"Improved Accuracy Metrics for Classification with\n    Imbalanced Data and Where Distance from the Truth Matters, with the wconf R\n    Package\" <doi:10.2139/ssrn.4802336>\n    Starovoitov, V., Golub, Y. (2020). New Function for Estimating Imbalanced\n    Data Classification Results. Pattern Recognition and Image Analysis, 295–302\n    Van de Velden, M., Iodice D'Enza, A., Markos, A., Cavicchia, C. (2023)\n    \"A general framework for implementing distances for categorical variables\"\n    <doi:10.48550/arXiv.2301.02190>.",
    "version": "1.2.0",
    "maintainer": "Alexandru Monahov <alexandru.monahov@proton.me>",
    "url": "https://www.alexandrumonahov.eu.org/projects",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25696,
    "package_name": "wdnet",
    "title": "Weighted and Directed Networks",
    "description": "Assortativity coefficients, centrality measures, \n    and clustering coefficients for weighted and directed networks.\n    Rewiring unweighted networks with given assortativity coefficients.\n    Generating general preferential attachment networks.",
    "version": "1.2.3",
    "maintainer": "Yelie Yuan <yelie.yuan@uconn.edu>",
    "url": "https://gitlab.com/wdnetwork/wdnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25705,
    "package_name": "weatherindices",
    "title": "Calculate Weather Indices",
    "description": "Weather indices represent the overall weekly effect of a weather variable on crop yield throughout the cropping season. This package contains functions that can convert the weekly weather data into yearly weighted Weather indices with weights being the correlation coefficient between weekly weather data over the years and crop yield over the years. This can be done for an individual weather variable and for two weather variables at a time as the interaction effect. This method was first devised by Jain, RC, Agrawal R, and Jha, MP (1980), \"Effect of climatic variables on rice yield and its forecast\",MAUSAM, 31(4), 591–596, <doi:10.54302/mausam.v31i4.3477>. Later, the method have been used by various researchers and the latest can found in Gupta, AK, Sarkar, KA, Dhakre, DS, & Bhattacharya, D (2022), \"Weather Based Potato Yield Modelling using Statistical and Machine Learning Technique\",Environment and Ecology, 40(3B), 1444–1449,<https://www.environmentandecology.com/volume-40-2022>.",
    "version": "0.1.0",
    "maintainer": "Akhilesh Kumar Gupta <akhileshgupta.ouat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25765,
    "package_name": "wheatmap",
    "title": "Incrementally Build Complex Plots using Natural Semantics",
    "description": "Builds complex plots, heatmaps in particular, using natural semantics. Bigger plots can be assembled using directives such as 'LeftOf', 'RightOf', 'TopOf', and 'Beneath' and more. Other features include clustering, dendrograms and integration with 'ggplot2' generated grid objects. This package is particularly designed for bioinformaticians to assemble complex plots for publication.",
    "version": "0.2.0",
    "maintainer": "Wanding Zhou <zhouwanding@gmail.com>",
    "url": "https://github.com/zwdzwd/wheatmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25786,
    "package_name": "widyr",
    "title": "Widen, Process, then Re-Tidy Data",
    "description": "Encapsulates the pattern of untidying data into a wide\n    matrix, performing some processing, then turning it back into a tidy\n    form. This is useful for several operations such as co-occurrence\n    counts, correlations, or clustering that are mathematically convenient\n    on wide matrices.",
    "version": "0.1.5",
    "maintainer": "Julia Silge <julia.silge@gmail.com>",
    "url": "https://github.com/juliasilge/widyr,\nhttps://juliasilge.github.io/widyr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25840,
    "package_name": "wordmap",
    "title": "Feature Extraction and Document Classification with Noisy Labels",
    "description": "Extract features and classify documents with noisy labels given by document-meta data or keyword matching Watanabe & Zhou (2020) <doi:10.1177/0894439320907027>.",
    "version": "0.9.5",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "url": "https://github.com/koheiw/wordmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25894,
    "package_name": "wskm",
    "title": "Weighted k-Means Clustering",
    "description": "Entropy weighted k-means (ewkm) by Liping Jing, Michael\n        K. Ng and Joshua Zhexue Huang (2007)\n        <doi:10.1109/TKDE.2007.1048> is a weighted subspace clustering\n        algorithm that is well suited to very high dimensional data.\n        Weights are calculated as the importance of a variable with\n        regard to cluster membership.  The two-level variable\n        weighting clustering algorithm tw-k-means (twkm) by Xiaojun\n        Chen, Xiaofei Xu, Joshua Zhexue Huang and Yunming Ye (2013)\n        <doi:10.1109/TKDE.2011.262> introduces two types of weights,\n        the weights on individual variables and the weights on\n        variable groups, and they are calculated during the clustering\n        process.  The feature group weighted k-means (fgkm) by Xiaojun\n        Chen, Yunminng Ye, Xiaofei Xu and Joshua Zhexue Huang (2012)\n        <doi:10.1016/j.patcog.2011.06.004> extends this concept by\n        grouping features and weighting the group in addition to\n        weighting individual features.",
    "version": "1.4.40",
    "maintainer": "He Zhao <Simon.Yansen.Zhao@gmail.com>",
    "url": "https://github.com/SimonYansenZhao/wskm,\nhttp://english.siat.cas.cn/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25896,
    "package_name": "wsrf",
    "title": "Weighted Subspace Random Forest for Classification",
    "description": "\n    A parallel implementation of Weighted Subspace Random Forest.  The\n    Weighted Subspace Random Forest algorithm was proposed in the\n    International Journal of Data Warehousing and Mining by Baoxun Xu,\n    Joshua Zhexue Huang, Graham Williams, Qiang Wang, and Yunming Ye\n    (2012) <DOI:10.4018/jdwm.2012040103>.  The algorithm can classify\n    very high-dimensional data with random forests built using small\n    subspaces.  A novel variable weighting method is used for variable\n    subspace selection in place of the traditional random variable\n    sampling.This new approach is particularly useful in building\n    models from high-dimensional data.",
    "version": "1.7.31",
    "maintainer": "He Zhao <Simon.Yansen.Zhao@gmail.com>",
    "url": "https://github.com/SimonYansenZhao/wsrf, https://togaware.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25938,
    "package_name": "xgb2sql",
    "title": "Convert Trained 'XGBoost' Model to SQL Query",
    "description": "This tool enables in-database scoring of 'XGBoost' models built in R, by translating trained model objects into SQL query. \n  'XGBoost' <https://github.com/dmlc/xgboost> provides parallel tree boosting (also known as gradient boosting machine, or GBM) algorithms\n  in a highly efficient, flexible and portable way. GBM algorithm is introduced by Friedman (2001) <doi:10.1214/aos/1013203451>, \n  and more details on 'XGBoost' can be found in Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>.",
    "version": "0.1.3",
    "maintainer": "Chengjun Hou <chengjun.hou@gmail.com>",
    "url": "https://github.com/chengjunhou/xgb2sql",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25941,
    "package_name": "xgrove",
    "title": "Explanation Groves",
    "description": "Compute surrogate explanation groves for predictive machine learning models and analyze complexity vs. explanatory power of an explanation according to Szepannek, G. and von Holt, B. (2023) <doi:10.1007/s41237-023-00205-2>.",
    "version": "0.1-15",
    "maintainer": "Gero Szepannek <gero.szepannek@web.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25953,
    "package_name": "xmap",
    "title": "Transforming Data Between Statistical Classifications",
    "description": "Provides support for transformations of numeric aggregates\n   between statistical classifications (e.g. occupation or industry categorisations) using the 'Crossmaps' framework. \n   Implements classes for representing transformations between a source and target classification\n   as graph structures, and methods for validating and applying crossmaps to transform\n   data collected under the source classification into data indexed using the target classification codes.\n   Documentation about the 'Crossmaps' framework is provided in the included vignettes\n   and in Huang (2024, <doi:10.48550/arXiv.2406.14163>).",
    "version": "0.1.0",
    "maintainer": "Cynthia A. Huang <cynthiahqy@gmail.com>",
    "url": "https://github.com/cynthiahqy/xmap,\nhttps://cynthiahqy.github.io/xmap/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25979,
    "package_name": "xrf",
    "title": "eXtreme RuleFit",
    "description": "An implementation of the RuleFit algorithm as described in\n    Friedman & Popescu (2008) <doi:10.1214/07-AOAS148>. eXtreme Gradient\n    Boosting ('XGBoost') is used to build rules, and 'glmnet' is used to\n    fit a sparse linear model on the raw and rule features. The result is\n    a model that learns similarly to a tree ensemble, while often offering\n    improved interpretability and achieving improved scoring runtime in\n    live applications. Several algorithms for reducing rule complexity are\n    provided, most notably hyperrectangle de-overlapping. All algorithms\n    scale to several million rows and support sparse representations to\n    handle tens of thousands of dimensions.",
    "version": "0.3.1",
    "maintainer": "Karl Holub <karljholub@gmail.com>",
    "url": "https://github.com/holub008/xrf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25984,
    "package_name": "xtdml",
    "title": "Double Machine Learning for Static Panel Models with Fixed\nEffects",
    "description": "The 'xtdml' package implements partially linear panel regression (PLPR) models with high-dimensional confounding variables and an exogenous treatment variable within the double machine learning framework. The package is used to estimate the structural parameter (treatment effect) in static panel data models with fixed effects using the approaches established in Clarke and Polselli (2025) <doi:10.1093/ectj/utaf011>. 'xtdml' is built on the object-oriented package 'DoubleML' (Bach et al., 2024) <doi:10.18637/jss.v108.i03> using the 'mlr3' ecosystem.",
    "version": "0.1.11",
    "maintainer": "Annalivia Polselli <apolselli.econ@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25995,
    "package_name": "yaConsensus",
    "title": "Consensus Clustering of Omic Data",
    "description": "Procedures to perform consensus clustering starting from a dissimilarity matrix or a data matrix. It's allowed to select if the subsampling has to be by samples or features. In case of computational heavy load, the procedures can run in parallel.",
    "version": "1.1",
    "maintainer": "Stefano Maria Pagnotta <pagnotta@unisannio.it>",
    "url": "https://github.com/stefanoMP/yaConsensus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25998,
    "package_name": "yager",
    "title": "Yet Another General Regression Neural Network",
    "description": "Another implementation of general regression neural network in R\n    based on Specht (1991) <DOI:10.1109/72.97934>. It is applicable to the \n    functional approximation or the classification. ",
    "version": "0.1.1",
    "maintainer": "WenSui Liu <liuwensui@gmail.com>",
    "url": "https://github.com/statcompute/yager",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26080,
    "package_name": "zooimage",
    "title": "Analysis of Numerical Plankton Images",
    "description": "A free (open source) solution for analyzing digital\n  images of plankton. In combination with ImageJ, a free image analysis\n  system, it processes digital images, measures individuals, trains for\n  automatic classification of taxa, and finally, measures plankton samples\n  (abundances, total and partial size spectra or biomasses, etc.).",
    "version": "5.5.2",
    "maintainer": "Philippe Grosjean <phgrosjean@sciviews.org>",
    "url": "http://www.sciviews.org/zooimage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  }
]
