[
  {
    "id": 1365,
    "package_name": "tokenizers",
    "title": "Fast, Consistent Tokenization of Natural Language Text",
    "description": "Convert natural language text into tokens. Includes\ntokenizers for shingled n-grams, skip n-grams, words, word\nstems, sentences, paragraphs, characters, shingled characters,\nlines, Penn Treebank, regular expressions, as well as functions\nfor counting characters, words, and sentences, and a function\nfor splitting longer texts into separate documents, each with\nthe same number of words.  The tokenizers have a consistent\ninterface, and the package is built on the 'stringi' and 'Rcpp'\npackages for fast yet correct tokenization in 'UTF-8'.",
    "version": "0.3.1",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "author": "Thomas Charlon [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-7497-0470>),\nLincoln Mullen [aut] (ORCID: <https://orcid.org/0000-0001-5103-6917>),\nOs Keyes [ctb] (ORCID: <https://orcid.org/0000-0001-5196-609X>),\nDmitriy Selivanov [ctb],\nJeffrey Arnold [ctb] (ORCID: <https://orcid.org/0000-0001-9953-3904>),\nKenneth Benoit [ctb] (ORCID: <https://orcid.org/0000-0002-0797-564X>)",
    "url": "https://docs.ropensci.org/tokenizers/,\nhttps://github.com/ropensci/tokenizers",
    "bug_reports": "https://github.com/ropensci/tokenizers/issues",
    "repository": "",
    "exports": [
      [
        "chunk_text"
      ],
      [
        "count_characters"
      ],
      [
        "count_sentences"
      ],
      [
        "count_words"
      ],
      [
        "tokenize_character_shingles"
      ],
      [
        "tokenize_characters"
      ],
      [
        "tokenize_lines"
      ],
      [
        "tokenize_ngrams"
      ],
      [
        "tokenize_paragraphs"
      ],
      [
        "tokenize_ptb"
      ],
      [
        "tokenize_regex"
      ],
      [
        "tokenize_sentences"
      ],
      [
        "tokenize_skip_ngrams"
      ],
      [
        "tokenize_word_stems"
      ],
      [
        "tokenize_words"
      ]
    ],
    "topics": [
      [
        "nlp"
      ],
      [
        "peer-reviewed"
      ],
      [
        "text-mining"
      ],
      [
        "tokenizer"
      ],
      [
        "cpp"
      ]
    ],
    "score": 13.4746,
    "stars": 186,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tokenizers Fast, Consistent Tokenization of Natural Language Text Convert natural language text into tokens. Includes\ntokenizers for shingled n-grams, skip n-grams, words, word\nstems, sentences, paragraphs, characters, shingled characters,\nlines, Penn Treebank, regular expressions, as well as functions\nfor counting characters, words, and sentences, and a function\nfor splitting longer texts into separate documents, each with\nthe same number of words.  The tokenizers have a consistent\ninterface, and the package is built on the 'stringi' and 'Rcpp'\npackages for fast yet correct tokenization in 'UTF-8'. chunk_text count_characters count_sentences count_words tokenize_character_shingles tokenize_characters tokenize_lines tokenize_ngrams tokenize_paragraphs tokenize_ptb tokenize_regex tokenize_sentences tokenize_skip_ngrams tokenize_word_stems tokenize_words nlp peer-reviewed text-mining tokenizer cpp"
  },
  {
    "id": 695,
    "package_name": "hunspell",
    "title": "High-Performance Stemmer, Tokenizer, and Spell Checker",
    "description": "Low level spell checker and morphological analyzer based\non the famous 'hunspell' library <https://hunspell.github.io>.\nThe package can analyze or check individual words as well as\nparse text, latex, html or xml documents. For a more\nuser-friendly interface use the 'spelling' package which builds\non this package to automate checking of files, documentation\nand vignettes in all common formats.",
    "version": "3.0.7",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre],\nAuthors of libhunspell [cph] (see AUTHORS file)",
    "url": "https://docs.ropensci.org/hunspell/\nhttps://ropensci.r-universe.dev/hunspell",
    "bug_reports": "https://github.com/ropensci/hunspell/issues",
    "repository": "",
    "exports": [
      [
        "dicpath"
      ],
      [
        "dictionary"
      ],
      [
        "en_stats"
      ],
      [
        "hunspell"
      ],
      [
        "hunspell_analyze"
      ],
      [
        "hunspell_check"
      ],
      [
        "hunspell_find"
      ],
      [
        "hunspell_info"
      ],
      [
        "hunspell_parse"
      ],
      [
        "hunspell_stem"
      ],
      [
        "hunspell_suggest"
      ],
      [
        "list_dictionaries"
      ]
    ],
    "topics": [
      [
        "hunspell"
      ],
      [
        "spell-check"
      ],
      [
        "spellchecker"
      ],
      [
        "stemmer"
      ],
      [
        "tokenizer"
      ],
      [
        "cpp"
      ]
    ],
    "score": 13.3629,
    "stars": 113,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "hunspell High-Performance Stemmer, Tokenizer, and Spell Checker Low level spell checker and morphological analyzer based\non the famous 'hunspell' library <https://hunspell.github.io>.\nThe package can analyze or check individual words as well as\nparse text, latex, html or xml documents. For a more\nuser-friendly interface use the 'spelling' package which builds\non this package to automate checking of files, documentation\nand vignettes in all common formats. dicpath dictionary en_stats hunspell hunspell_analyze hunspell_check hunspell_find hunspell_info hunspell_parse hunspell_stem hunspell_suggest list_dictionaries hunspell spell-check spellchecker stemmer tokenizer cpp"
  },
  {
    "id": 1362,
    "package_name": "tm",
    "title": "Text Mining Package",
    "description": "A framework for text mining applications within R.",
    "version": "0.7-17",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Ingo Feinerer [aut] (ORCID: <https://orcid.org/0000-0001-7656-8338>),\nKurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\nArtifex Software, Inc. [ctb, cph] (pdf_info.ps taken from GPL\nGhostscript)",
    "url": "https://tm.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "as.DocumentTermMatrix"
      ],
      [
        "as.TermDocumentMatrix"
      ],
      [
        "as.VCorpus"
      ],
      [
        "Boost_tokenizer"
      ],
      [
        "content_transformer"
      ],
      [
        "Corpus"
      ],
      [
        "DataframeSource"
      ],
      [
        "DirSource"
      ],
      [
        "Docs"
      ],
      [
        "DocumentTermMatrix"
      ],
      [
        "DublinCore"
      ],
      [
        "DublinCore<-"
      ],
      [
        "eoi"
      ],
      [
        "findAssocs"
      ],
      [
        "findFreqTerms"
      ],
      [
        "findMostFreqTerms"
      ],
      [
        "FunctionGenerator"
      ],
      [
        "getElem"
      ],
      [
        "getMeta"
      ],
      [
        "getReaders"
      ],
      [
        "getSources"
      ],
      [
        "getTokenizers"
      ],
      [
        "getTransformations"
      ],
      [
        "Heaps_plot"
      ],
      [
        "inspect"
      ],
      [
        "MC_tokenizer"
      ],
      [
        "nDocs"
      ],
      [
        "nTerms"
      ],
      [
        "PCorpus"
      ],
      [
        "pGetElem"
      ],
      [
        "PlainTextDocument"
      ],
      [
        "read_dtm_Blei_et_al"
      ],
      [
        "read_dtm_MC"
      ],
      [
        "readDataframe"
      ],
      [
        "readDOC"
      ],
      [
        "reader"
      ],
      [
        "readPDF"
      ],
      [
        "readPlain"
      ],
      [
        "readRCV1"
      ],
      [
        "readRCV1asPlain"
      ],
      [
        "readReut21578XML"
      ],
      [
        "readReut21578XMLasPlain"
      ],
      [
        "readTagged"
      ],
      [
        "readXML"
      ],
      [
        "removeNumbers"
      ],
      [
        "removePunctuation"
      ],
      [
        "removeSparseTerms"
      ],
      [
        "removeWords"
      ],
      [
        "scan_tokenizer"
      ],
      [
        "SimpleCorpus"
      ],
      [
        "SimpleSource"
      ],
      [
        "stemCompletion"
      ],
      [
        "stemDocument"
      ],
      [
        "stepNext"
      ],
      [
        "stopwords"
      ],
      [
        "stripWhitespace"
      ],
      [
        "TermDocumentMatrix"
      ],
      [
        "termFreq"
      ],
      [
        "Terms"
      ],
      [
        "tm_filter"
      ],
      [
        "tm_index"
      ],
      [
        "tm_map"
      ],
      [
        "tm_parLapply"
      ],
      [
        "tm_parLapply_engine"
      ],
      [
        "tm_reduce"
      ],
      [
        "tm_term_score"
      ],
      [
        "URISource"
      ],
      [
        "VCorpus"
      ],
      [
        "VectorSource"
      ],
      [
        "weightBin"
      ],
      [
        "WeightFunction"
      ],
      [
        "weightSMART"
      ],
      [
        "weightTf"
      ],
      [
        "weightTfIdf"
      ],
      [
        "writeCorpus"
      ],
      [
        "XMLSource"
      ],
      [
        "XMLTextDocument"
      ],
      [
        "Zipf_plot"
      ],
      [
        "ZipSource"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 12.8457,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "tm Text Mining Package A framework for text mining applications within R. as.DocumentTermMatrix as.TermDocumentMatrix as.VCorpus Boost_tokenizer content_transformer Corpus DataframeSource DirSource Docs DocumentTermMatrix DublinCore DublinCore<- eoi findAssocs findFreqTerms findMostFreqTerms FunctionGenerator getElem getMeta getReaders getSources getTokenizers getTransformations Heaps_plot inspect MC_tokenizer nDocs nTerms PCorpus pGetElem PlainTextDocument read_dtm_Blei_et_al read_dtm_MC readDataframe readDOC reader readPDF readPlain readRCV1 readRCV1asPlain readReut21578XML readReut21578XMLasPlain readTagged readXML removeNumbers removePunctuation removeSparseTerms removeWords scan_tokenizer SimpleCorpus SimpleSource stemCompletion stemDocument stepNext stopwords stripWhitespace TermDocumentMatrix termFreq Terms tm_filter tm_index tm_map tm_parLapply tm_parLapply_engine tm_reduce tm_term_score URISource VCorpus VectorSource weightBin WeightFunction weightSMART weightTf weightTfIdf writeCorpus XMLSource XMLTextDocument Zipf_plot ZipSource cpp"
  },
  {
    "id": 651,
    "package_name": "googleLanguageR",
    "title": "Call Google's 'Natural Language', 'Cloud Translation', 'Cloud\nSpeech', and 'Cloud Text-to-Speech' APIs",
    "description": "Access Google Cloud machine learning APIs for text and\nspeech tasks. Use the Cloud Translation API for text detection\nand translation, the Natural Language API to analyze sentiment,\nentities, and syntax, the Cloud Speech API to transcribe audio\nto text, and the Cloud Text-to-Speech API to synthesize text\ninto audio files.",
    "version": "0.3.1.1",
    "maintainer": "Cheryl Isabella Lim <cheryl.academic@gmail.com>",
    "author": "Aleksander Dietrichson [ctb], Mark Edmondson [aut], John\nMuschelli [ctb], Neal Richardson [rev] (Reviewed package for\nropensci), Julia Gustavsen [rev] (Reviewed package for\nropensci), Cheryl Isabella Lim [aut, cre]",
    "url": "https://github.com/ropensci/googleLanguageR",
    "bug_reports": "https://github.com/ropensci/googleLanguageR/issues",
    "repository": "",
    "exports": [
      [
        "gl_auth"
      ],
      [
        "gl_auto_auth"
      ],
      [
        "gl_nlp"
      ],
      [
        "gl_speech"
      ],
      [
        "gl_speech_op"
      ],
      [
        "gl_talk"
      ],
      [
        "gl_talk_languages"
      ],
      [
        "gl_talk_player"
      ],
      [
        "gl_talk_shiny"
      ],
      [
        "gl_talk_shinyUI"
      ],
      [
        "gl_translate"
      ],
      [
        "gl_translate_detect"
      ],
      [
        "gl_translate_document"
      ],
      [
        "gl_translate_languages"
      ]
    ],
    "topics": [
      [
        "cloud-speech-api"
      ],
      [
        "cloud-translation-api"
      ],
      [
        "google-api-client"
      ],
      [
        "google-cloud"
      ],
      [
        "google-cloud-speech"
      ],
      [
        "google-nlp"
      ],
      [
        "googleauthr"
      ],
      [
        "natural-language-processing"
      ],
      [
        "peer-reviewed"
      ],
      [
        "sentiment-analysis"
      ],
      [
        "speech-api"
      ],
      [
        "translation-api"
      ]
    ],
    "score": 11.1888,
    "stars": 202,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "googleLanguageR Call Google's 'Natural Language', 'Cloud Translation', 'Cloud\nSpeech', and 'Cloud Text-to-Speech' APIs Access Google Cloud machine learning APIs for text and\nspeech tasks. Use the Cloud Translation API for text detection\nand translation, the Natural Language API to analyze sentiment,\nentities, and syntax, the Cloud Speech API to transcribe audio\nto text, and the Cloud Text-to-Speech API to synthesize text\ninto audio files. gl_auth gl_auto_auth gl_nlp gl_speech gl_speech_op gl_talk gl_talk_languages gl_talk_player gl_talk_shiny gl_talk_shinyUI gl_translate gl_translate_detect gl_translate_document gl_translate_languages cloud-speech-api cloud-translation-api google-api-client google-cloud google-cloud-speech google-nlp googleauthr natural-language-processing peer-reviewed sentiment-analysis speech-api translation-api"
  },
  {
    "id": 1320,
    "package_name": "textrecipes",
    "title": "Extra 'Recipes' for Text Processing",
    "description": "Converting text to numerical features requires\nspecifically created procedures, which are implemented as steps\naccording to the 'recipes' package. These steps allows for\ntokenization, filtering, counting (tf and tfidf) and feature\nhashing.",
    "version": "1.1.0.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nMichael W. Kearney [cph] (author of count_functions),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/tidymodels/textrecipes,\nhttps://textrecipes.tidymodels.org/",
    "bug_reports": "https://github.com/tidymodels/textrecipes/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "all_tokenized"
      ],
      [
        "all_tokenized_predictors"
      ],
      [
        "count_functions"
      ],
      [
        "ngram"
      ],
      [
        "required_pkgs"
      ],
      [
        "show_tokens"
      ],
      [
        "step_clean_levels"
      ],
      [
        "step_clean_names"
      ],
      [
        "step_dummy_hash"
      ],
      [
        "step_lda"
      ],
      [
        "step_lemma"
      ],
      [
        "step_ngram"
      ],
      [
        "step_pos_filter"
      ],
      [
        "step_sequence_onehot"
      ],
      [
        "step_stem"
      ],
      [
        "step_stopwords"
      ],
      [
        "step_text_normalization"
      ],
      [
        "step_textfeature"
      ],
      [
        "step_texthash"
      ],
      [
        "step_tf"
      ],
      [
        "step_tfidf"
      ],
      [
        "step_tokenfilter"
      ],
      [
        "step_tokenize"
      ],
      [
        "step_tokenize_bpe"
      ],
      [
        "step_tokenize_sentencepiece"
      ],
      [
        "step_tokenize_wordpiece"
      ],
      [
        "step_tokenmerge"
      ],
      [
        "step_untokenize"
      ],
      [
        "step_word_embeddings"
      ],
      [
        "tidy"
      ],
      [
        "tokenlist"
      ],
      [
        "tunable"
      ]
    ],
    "topics": [],
    "score": 11.1367,
    "stars": 164,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "textrecipes Extra 'Recipes' for Text Processing Converting text to numerical features requires\nspecifically created procedures, which are implemented as steps\naccording to the 'recipes' package. These steps allows for\ntokenization, filtering, counting (tf and tfidf) and feature\nhashing. %>% all_tokenized all_tokenized_predictors count_functions ngram required_pkgs show_tokens step_clean_levels step_clean_names step_dummy_hash step_lda step_lemma step_ngram step_pos_filter step_sequence_onehot step_stem step_stopwords step_text_normalization step_textfeature step_texthash step_tf step_tfidf step_tokenfilter step_tokenize step_tokenize_bpe step_tokenize_sentencepiece step_tokenize_wordpiece step_tokenmerge step_untokenize step_word_embeddings tidy tokenlist tunable "
  },
  {
    "id": 1321,
    "package_name": "textreuse",
    "title": "Detect Text Reuse and Document Similarity",
    "description": "Tools for measuring similarity among documents and\ndetecting passages which have been reused. Implements shingled\nn-gram, skip n-gram, and other tokenizers;\nsimilarity/dissimilarity functions; pairwise comparisons;\nminhash and locality sensitive hashing algorithms; and a\nversion of the Smith-Waterman local alignment algorithm\nsuitable for natural language.",
    "version": "0.1.5",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-9200-1016>),\nLincoln Mullen [aut] (ORCID: <https://orcid.org/0000-0001-5103-6917>)",
    "url": "https://docs.ropensci.org/textreuse (website)\nhttps://github.com/ropensci/textreuse",
    "bug_reports": "https://github.com/ropensci/textreuse/issues",
    "repository": "",
    "exports": [
      [
        "align_local"
      ],
      [
        "content"
      ],
      [
        "content<-"
      ],
      [
        "filenames"
      ],
      [
        "has_content"
      ],
      [
        "has_hashes"
      ],
      [
        "has_minhashes"
      ],
      [
        "has_tokens"
      ],
      [
        "hash_string"
      ],
      [
        "hashes"
      ],
      [
        "hashes<-"
      ],
      [
        "is.TextReuseCorpus"
      ],
      [
        "is.TextReuseTextDocument"
      ],
      [
        "jaccard_bag_similarity"
      ],
      [
        "jaccard_dissimilarity"
      ],
      [
        "jaccard_similarity"
      ],
      [
        "lsh"
      ],
      [
        "lsh_candidates"
      ],
      [
        "lsh_compare"
      ],
      [
        "lsh_probability"
      ],
      [
        "lsh_query"
      ],
      [
        "lsh_subset"
      ],
      [
        "lsh_threshold"
      ],
      [
        "meta"
      ],
      [
        "meta<-"
      ],
      [
        "minhash_generator"
      ],
      [
        "minhashes"
      ],
      [
        "minhashes<-"
      ],
      [
        "pairwise_candidates"
      ],
      [
        "pairwise_compare"
      ],
      [
        "ratio_of_matches"
      ],
      [
        "rehash"
      ],
      [
        "skipped"
      ],
      [
        "TextReuseCorpus"
      ],
      [
        "TextReuseTextDocument"
      ],
      [
        "tokenize"
      ],
      [
        "tokenize_ngrams"
      ],
      [
        "tokenize_sentences"
      ],
      [
        "tokenize_skip_ngrams"
      ],
      [
        "tokenize_words"
      ],
      [
        "tokens"
      ],
      [
        "tokens<-"
      ],
      [
        "wordcount"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ],
      [
        "cpp"
      ]
    ],
    "score": 9.3355,
    "stars": 202,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "textreuse Detect Text Reuse and Document Similarity Tools for measuring similarity among documents and\ndetecting passages which have been reused. Implements shingled\nn-gram, skip n-gram, and other tokenizers;\nsimilarity/dissimilarity functions; pairwise comparisons;\nminhash and locality sensitive hashing algorithms; and a\nversion of the Smith-Waterman local alignment algorithm\nsuitable for natural language. align_local content content<- filenames has_content has_hashes has_minhashes has_tokens hash_string hashes hashes<- is.TextReuseCorpus is.TextReuseTextDocument jaccard_bag_similarity jaccard_dissimilarity jaccard_similarity lsh lsh_candidates lsh_compare lsh_probability lsh_query lsh_subset lsh_threshold meta meta<- minhash_generator minhashes minhashes<- pairwise_candidates pairwise_compare ratio_of_matches rehash skipped TextReuseCorpus TextReuseTextDocument tokenize tokenize_ngrams tokenize_sentences tokenize_skip_ngrams tokenize_words tokens tokens<- wordcount peer-reviewed cpp"
  },
  {
    "id": 941,
    "package_name": "pangoling",
    "title": "Access to Large Language Model Predictions",
    "description": "Provides access to word predictability estimates using\nlarge language models (LLMs) based on 'transformer'\narchitectures via integration with the 'Hugging Face' ecosystem\n<https://huggingface.co/>. The package interfaces with\npre-trained neural networks and supports both\ncausal/auto-regressive LLMs (e.g., 'GPT-2') and\nmasked/bidirectional LLMs (e.g., 'BERT') to compute the\nprobability of words, phrases, or tokens given their linguistic\ncontext. For details on GPT-2 and causal models, see Radford et\nal. (2019)\n<https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf>,\nfor details on BERT and masked models, see Devlin et al. (2019)\n<doi:10.48550/arXiv.1810.04805>. By enabling a straightforward\nestimation of word predictability, the package facilitates\nresearch in psycholinguistics, computational linguistics, and\nnatural language processing (NLP).",
    "version": "1.0.3",
    "maintainer": "Bruno Nicenboim <b.nicenboim@tilburguniversity.edu>",
    "author": "Bruno Nicenboim [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5176-3943>),\nChris Emmerly [ctb],\nGiovanni Cassani [ctb],\nLisa Levinson [rev],\nUtku Turk [rev]",
    "url": "https://docs.ropensci.org/pangoling/,\nhttps://github.com/ropensci/pangoling",
    "bug_reports": "https://github.com/ropensci/pangoling/issues",
    "repository": "",
    "exports": [
      [
        "causal_config"
      ],
      [
        "causal_lp"
      ],
      [
        "causal_lp_mats"
      ],
      [
        "causal_next_tokens_pred_tbl"
      ],
      [
        "causal_next_tokens_tbl"
      ],
      [
        "causal_pred_mats"
      ],
      [
        "causal_preload"
      ],
      [
        "causal_targets_pred"
      ],
      [
        "causal_tokens_lp_tbl"
      ],
      [
        "causal_tokens_pred_lst"
      ],
      [
        "causal_words_pred"
      ],
      [
        "install_py_pangoling"
      ],
      [
        "installed_py_pangoling"
      ],
      [
        "masked_config"
      ],
      [
        "masked_lp"
      ],
      [
        "masked_preload"
      ],
      [
        "masked_targets_pred"
      ],
      [
        "masked_tokens_pred_tbl"
      ],
      [
        "masked_tokens_tbl"
      ],
      [
        "ntokens"
      ],
      [
        "perplexity_calc"
      ],
      [
        "set_cache_folder"
      ],
      [
        "tokenize_lst"
      ],
      [
        "transformer_vocab"
      ]
    ],
    "topics": [
      [
        "nlp"
      ],
      [
        "psycholinguistics"
      ],
      [
        "transformers"
      ]
    ],
    "score": 6.2833,
    "stars": 12,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pangoling Access to Large Language Model Predictions Provides access to word predictability estimates using\nlarge language models (LLMs) based on 'transformer'\narchitectures via integration with the 'Hugging Face' ecosystem\n<https://huggingface.co/>. The package interfaces with\npre-trained neural networks and supports both\ncausal/auto-regressive LLMs (e.g., 'GPT-2') and\nmasked/bidirectional LLMs (e.g., 'BERT') to compute the\nprobability of words, phrases, or tokens given their linguistic\ncontext. For details on GPT-2 and causal models, see Radford et\nal. (2019)\n<https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf>,\nfor details on BERT and masked models, see Devlin et al. (2019)\n<doi:10.48550/arXiv.1810.04805>. By enabling a straightforward\nestimation of word predictability, the package facilitates\nresearch in psycholinguistics, computational linguistics, and\nnatural language processing (NLP). causal_config causal_lp causal_lp_mats causal_next_tokens_pred_tbl causal_next_tokens_tbl causal_pred_mats causal_preload causal_targets_pred causal_tokens_lp_tbl causal_tokens_pred_lst causal_words_pred install_py_pangoling installed_py_pangoling masked_config masked_lp masked_preload masked_targets_pred masked_tokens_pred_tbl masked_tokens_tbl ntokens perplexity_calc set_cache_folder tokenize_lst transformer_vocab nlp psycholinguistics transformers"
  },
  {
    "id": 537,
    "package_name": "epubr",
    "title": "Read EPUB File Metadata and Text",
    "description": "Provides functions supporting the reading and parsing of\ninternal e-book content from EPUB files. The 'epubr' package\nprovides functions supporting the reading and parsing of\ninternal e-book content from EPUB files. E-book metadata and\ntext content are parsed separately and joined together in a\ntidy, nested tibble data frame. E-book formatting is not\ncompletely standardized across all literature. It can be\nchallenging to curate parsed e-book content across an arbitrary\ncollection of e-books perfectly and in completely general form,\nto yield a singular, consistently formatted output. Many EPUB\nfiles do not even contain all the same pieces of information in\ntheir respective metadata. EPUB file parsing functionality in\nthis package is intended for relatively general application to\narbitrary EPUB e-books. However, poorly formatted e-books or\ne-books with highly uncommon formatting may not work with this\npackage. There may even be cases where an EPUB file has DRM or\nsome other property that makes it impossible to read with\n'epubr'. Text is read 'as is' for the most part. The only\nnominal changes are minor substitutions, for example curly\nquotes changed to straight quotes. Substantive changes are\nexpected to be performed subsequently by the user as part of\ntheir text analysis. Additional text cleaning can be performed\nat the user's discretion, such as with functions from packages\nlike 'tm' or 'qdap'.",
    "version": "0.6.5",
    "maintainer": "Matthew Leonawicz <rpkgs@pm.me>",
    "author": "Matthew Leonawicz [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-9452-2771>)",
    "url": "https://docs.ropensci.org/epubr/,\nhttps://github.com/ropensci/epubr",
    "bug_reports": "https://github.com/ropensci/epubr/issues",
    "repository": "",
    "exports": [
      [
        "count_words"
      ],
      [
        "epub"
      ],
      [
        "epub_cat"
      ],
      [
        "epub_head"
      ],
      [
        "epub_meta"
      ],
      [
        "epub_recombine"
      ],
      [
        "epub_reorder"
      ],
      [
        "epub_sift"
      ],
      [
        "epub_unzip"
      ]
    ],
    "topics": [
      [
        "epub"
      ],
      [
        "epub-files"
      ],
      [
        "epub-format"
      ],
      [
        "peer-reviewed"
      ]
    ],
    "score": 5.8573,
    "stars": 24,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "epubr Read EPUB File Metadata and Text Provides functions supporting the reading and parsing of\ninternal e-book content from EPUB files. The 'epubr' package\nprovides functions supporting the reading and parsing of\ninternal e-book content from EPUB files. E-book metadata and\ntext content are parsed separately and joined together in a\ntidy, nested tibble data frame. E-book formatting is not\ncompletely standardized across all literature. It can be\nchallenging to curate parsed e-book content across an arbitrary\ncollection of e-books perfectly and in completely general form,\nto yield a singular, consistently formatted output. Many EPUB\nfiles do not even contain all the same pieces of information in\ntheir respective metadata. EPUB file parsing functionality in\nthis package is intended for relatively general application to\narbitrary EPUB e-books. However, poorly formatted e-books or\ne-books with highly uncommon formatting may not work with this\npackage. There may even be cases where an EPUB file has DRM or\nsome other property that makes it impossible to read with\n'epubr'. Text is read 'as is' for the most part. The only\nnominal changes are minor substitutions, for example curly\nquotes changed to straight quotes. Substantive changes are\nexpected to be performed subsequently by the user as part of\ntheir text analysis. Additional text cleaning can be performed\nat the user's discretion, such as with functions from packages\nlike 'tm' or 'qdap'. count_words epub epub_cat epub_head epub_meta epub_recombine epub_reorder epub_sift epub_unzip epub epub-files epub-format peer-reviewed"
  },
  {
    "id": 366,
    "package_name": "citecorp",
    "title": "Client for the Open Citations Corpus",
    "description": "Client for the Open Citations Corpus\n(<http://opencitations.net/>). Includes a set of functions for\ngetting one identifier type from another, as well as getting\nreferences and citations for a given identifier.",
    "version": "0.3.2.9000",
    "maintainer": "David Selby <david_antony.selby@dfki.de>",
    "author": "Scott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nDavid Selby [cre, aut] (ORCID: <https://orcid.org/0000-0001-8026-5663>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/citecorp (devel),\nhttps://docs.ropensci.org/citecorp/ (docs)",
    "bug_reports": "https://github.com/ropensci/citecorp/issues",
    "repository": "",
    "exports": [
      [
        "oc_coci_citation"
      ],
      [
        "oc_coci_cites"
      ],
      [
        "oc_coci_meta"
      ],
      [
        "oc_coci_refs"
      ],
      [
        "oc_doi2ids"
      ],
      [
        "oc_lookup_check"
      ],
      [
        "oc_pmcid2ids"
      ],
      [
        "oc_pmid2ids"
      ]
    ],
    "topics": [
      [
        "doi"
      ],
      [
        "metadata"
      ],
      [
        "citation"
      ],
      [
        "opencitations"
      ],
      [
        "bibtex"
      ],
      [
        "citations"
      ],
      [
        "pmcid"
      ],
      [
        "pmid"
      ],
      [
        "sparql"
      ]
    ],
    "score": 4.3936,
    "stars": 11,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "citecorp Client for the Open Citations Corpus Client for the Open Citations Corpus\n(<http://opencitations.net/>). Includes a set of functions for\ngetting one identifier type from another, as well as getting\nreferences and citations for a given identifier. oc_coci_citation oc_coci_cites oc_coci_meta oc_coci_refs oc_doi2ids oc_lookup_check oc_pmcid2ids oc_pmid2ids doi metadata citation opencitations bibtex citations pmcid pmid sparql"
  },
  {
    "id": 1353,
    "package_name": "tif",
    "title": "Text Interchange Format",
    "description": "Provides validation functions for common interchange\nformats for representing text data in R. Includes formats for\ncorpus objects, document term matrices, and tokens. Other\nannotations can be stored by overloading the tokens structure.",
    "version": "0.4",
    "maintainer": "Taylor B. Arnold <tarnold2@richmond.edu>",
    "author": "Taylor Arnold [aut, cre],\nKen Benoit [aut],\nLincoln Mullen [aut],\nAdam Obeng [aut],\nrOpenSci Text Workshop Participants (2017) [aut]",
    "url": "https://docs.ropensci.org/tif, https://github.com/ropensci/tif",
    "bug_reports": "http://github.com/ropensci/tif/issues",
    "repository": "",
    "exports": [
      [
        "tif_as_corpus_character"
      ],
      [
        "tif_as_corpus_df"
      ],
      [
        "tif_as_tokens_df"
      ],
      [
        "tif_as_tokens_list"
      ],
      [
        "tif_is_corpus_character"
      ],
      [
        "tif_is_corpus_df"
      ],
      [
        "tif_is_dtm"
      ],
      [
        "tif_is_tokens_df"
      ],
      [
        "tif_is_tokens_list"
      ]
    ],
    "topics": [
      [
        "corpus"
      ],
      [
        "natural-language-processing"
      ],
      [
        "term-frequency"
      ],
      [
        "text-processing"
      ],
      [
        "tokenizer"
      ]
    ],
    "score": 3.9106,
    "stars": 37,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tif Text Interchange Format Provides validation functions for common interchange\nformats for representing text data in R. Includes formats for\ncorpus objects, document term matrices, and tokens. Other\nannotations can be stored by overloading the tokens structure. tif_as_corpus_character tif_as_corpus_df tif_as_tokens_df tif_as_tokens_list tif_is_corpus_character tif_is_corpus_df tif_is_dtm tif_is_tokens_df tif_is_tokens_list corpus natural-language-processing term-frequency text-processing tokenizer"
  },
  {
    "id": 405,
    "package_name": "corpora",
    "title": "Statistics and Data Sets for Corpus Frequency Data",
    "description": "Utility functions for the statistical analysis of corpus\nfrequency data. This package is a companion to the open-source\ncourse \"Statistical Inference: A Gentle Introduction for\nComputational Linguists and Similar Creatures\" ('SIGIL').",
    "version": "0.7",
    "maintainer": "Stephanie Evert <stephanie.evert@fau.de>",
    "author": "Stephanie Evert [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-4192-2437>)",
    "url": "http://SIGIL.R-Forge.R-Project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "alpha.col"
      ],
      [
        "am.score"
      ],
      [
        "binom.pval"
      ],
      [
        "builtin.am"
      ],
      [
        "chisq"
      ],
      [
        "chisq.pval"
      ],
      [
        "colVector"
      ],
      [
        "cont.table"
      ],
      [
        "corpora.palette"
      ],
      [
        "fisher.pval"
      ],
      [
        "keyness"
      ],
      [
        "prop.cint"
      ],
      [
        "qw"
      ],
      [
        "rowVector"
      ],
      [
        "sample.df"
      ],
      [
        "simulated.census"
      ],
      [
        "simulated.language.course"
      ],
      [
        "simulated.wikipedia"
      ],
      [
        "stars.pval"
      ],
      [
        "z.score"
      ],
      [
        "z.score.pval"
      ]
    ],
    "topics": [],
    "score": 3.1584,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "corpora Statistics and Data Sets for Corpus Frequency Data Utility functions for the statistical analysis of corpus\nfrequency data. This package is a companion to the open-source\ncourse \"Statistical Inference: A Gentle Introduction for\nComputational Linguists and Similar Creatures\" ('SIGIL'). alpha.col am.score binom.pval builtin.am chisq chisq.pval colVector cont.table corpora.palette fisher.pval keyness prop.cint qw rowVector sample.df simulated.census simulated.language.course simulated.wikipedia stars.pval z.score z.score.pval "
  },
  {
    "id": 55,
    "package_name": "FMAT",
    "title": "The Fill-Mask Association Test",
    "description": "\n    The Fill-Mask Association Test ('FMAT')\n    <doi:10.1037/pspa0000396>\n    is an integrative and probability-based method using\n    Masked Language Models to measure conceptual associations\n    (e.g., attitudes, biases, stereotypes, social norms, cultural values)\n    as propositions in natural language.\n    Supported language models include 'BERT'\n    <doi:10.48550/arXiv.1810.04805> and its variants available at 'Hugging Face'\n    <https://huggingface.co/models?pipeline_tag=fill-mask>.\n    Methodological references and installation guidance are provided at\n    <https://psychbruce.github.io/FMAT/>.",
    "version": "2025.12",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/FMAT/",
    "bug_reports": "https://github.com/psychbruce/FMAT/issues",
    "repository": "https://cran.r-project.org/package=FMAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FMAT The Fill-Mask Association Test \n    The Fill-Mask Association Test ('FMAT')\n    <doi:10.1037/pspa0000396>\n    is an integrative and probability-based method using\n    Masked Language Models to measure conceptual associations\n    (e.g., attitudes, biases, stereotypes, social norms, cultural values)\n    as propositions in natural language.\n    Supported language models include 'BERT'\n    <doi:10.48550/arXiv.1810.04805> and its variants available at 'Hugging Face'\n    <https://huggingface.co/models?pipeline_tag=fill-mask>.\n    Methodological references and installation guidance are provided at\n    <https://psychbruce.github.io/FMAT/>.  "
  },
  {
    "id": 231,
    "package_name": "akc",
    "title": "Automatic Knowledge Classification",
    "description": "A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well.",
    "version": "0.9.9.2",
    "maintainer": "Tian-Yuan Huang <huang.tian-yuan@qq.com>",
    "author": "Tian-Yuan Huang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4151-3764>)",
    "url": "https://github.com/hope-data-science/akc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=akc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "akc Automatic Knowledge Classification A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well.  "
  },
  {
    "id": 399,
    "package_name": "contentanalysis",
    "title": "Scientific Content and Citation Analysis from PDF Documents",
    "description": "Provides comprehensive tools for extracting and analyzing scientific \n    content from PDF documents, including citation extraction, reference matching, \n    text analysis, and bibliometric indicators. Supports multi-column PDF layouts,\n    'CrossRef' API <https://www.crossref.org/documentation/retrieve-metadata/rest-api/> integration, and advanced citation parsing.",
    "version": "0.2.1",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "author": "Massimo Aria [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-8517-9411>),\n  Corrado Cuccurullo [aut] (ORCID:\n    <https://orcid.org/0000-0002-7401-8575>)",
    "url": "https://github.com/massimoaria/contentanalysis,",
    "bug_reports": "https://github.com/massimoaria/contentanalysis/issues",
    "repository": "https://cran.r-project.org/package=contentanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "contentanalysis Scientific Content and Citation Analysis from PDF Documents Provides comprehensive tools for extracting and analyzing scientific \n    content from PDF documents, including citation extraction, reference matching, \n    text analysis, and bibliometric indicators. Supports multi-column PDF layouts,\n    'CrossRef' API <https://www.crossref.org/documentation/retrieve-metadata/rest-api/> integration, and advanced citation parsing.  "
  },
  {
    "id": 1042,
    "package_name": "quanteda.tidy",
    "title": "'tidyverse' Extensions for 'quanteda'",
    "description": "Enables 'tidyverse' operations on 'quanteda' corpus objects by extending 'dplyr' verbs to work directly with corpus objects and their document-level variables ('docvars'). Implements row operations for 'subsetting' and reordering documents; column operations for managing document variables; grouped operations; and two-table verbs for merging external data. For more on 'quanteda' see 'Benoit et al.' (2018) <doi:10.21105/joss.00774>. For 'dplyr' see 'Wickham et al.' (2023) <doi:10.32614/CRAN.package.dplyr>.",
    "version": "0.4",
    "maintainer": "Kenneth Benoit <kbenoit@smu.edu.sg>",
    "author": "Kenneth Benoit [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quanteda.tidy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda.tidy 'tidyverse' Extensions for 'quanteda' Enables 'tidyverse' operations on 'quanteda' corpus objects by extending 'dplyr' verbs to work directly with corpus objects and their document-level variables ('docvars'). Implements row operations for 'subsetting' and reordering documents; column operations for managing document variables; grouped operations; and two-table verbs for merging external data. For more on 'quanteda' see 'Benoit et al.' (2018) <doi:10.21105/joss.00774>. For 'dplyr' see 'Wickham et al.' (2023) <doi:10.32614/CRAN.package.dplyr>.  "
  },
  {
    "id": 1300,
    "package_name": "tall",
    "title": "Text Analysis for All",
    "description": "An R 'shiny' app designed for diverse text analysis tasks, offering a wide range of methodologies tailored to Natural Language Processing (NLP) needs. \n             It is a versatile, general-purpose tool for analyzing textual data. \n             'tall' features a comprehensive workflow, including data cleaning, preprocessing, statistical analysis, and visualization, all integrated for effective text analysis.",
    "version": "0.5.1",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "author": "Massimo Aria [aut, cre, cph] (0000-0002-8517-9411),\n  Maria Spano [aut] (ORCID: <https://orcid.org/0000-0002-3103-2342>),\n  Luca D'Aniello [aut] (ORCID: <https://orcid.org/0000-0003-1019-9212>),\n  Corrado Cuccurullo [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7401-8575>),\n  Michelangelo Misuraca [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8794-966X>)",
    "url": "https://github.com/massimoaria/tall, https://www.k-synth.com/tall/",
    "bug_reports": "https://github.com/massimoaria/tall/issues",
    "repository": "https://cran.r-project.org/package=tall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tall Text Analysis for All An R 'shiny' app designed for diverse text analysis tasks, offering a wide range of methodologies tailored to Natural Language Processing (NLP) needs. \n             It is a versatile, general-purpose tool for analyzing textual data. \n             'tall' features a comprehensive workflow, including data cleaning, preprocessing, statistical analysis, and visualization, all integrated for effective text analysis.  "
  },
  {
    "id": 1364,
    "package_name": "tokenbrowser",
    "title": "Create Full Text Browsers from Annotated Token Lists",
    "description": "Create browsers for reading full texts from a token list format.\n    Information obtained from text analyses (e.g., topic modeling, word scaling)\n    can be used to annotate the texts.",
    "version": "0.1.6",
    "maintainer": "Kasper Welbers <kasperwelbers@gmail.com>",
    "author": "Kasper Welbers [aut, cre],\n  Wouter van Atteveldt [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tokenbrowser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tokenbrowser Create Full Text Browsers from Annotated Token Lists Create browsers for reading full texts from a token list format.\n    Information obtained from text analyses (e.g., topic modeling, word scaling)\n    can be used to annotate the texts.  "
  },
  {
    "id": 1655,
    "package_name": "AgeTopicModels",
    "title": "Inferring Age-Dependent Disease Topic from Diagnosis Data",
    "description": "We propose an age-dependent topic modelling (ATM) model,\n    providing a low-rank representation of longitudinal records of\n    hundreds of distinct diseases in large electronic health record data sets. The model\n    assigns to each individual topic weights for several disease topics;\n    each disease topic reflects a set of diseases that tend to co-occur as\n    a function of age, quantified by age-dependent topic loadings for each\n    disease. The model assumes that for each disease diagnosis, a topic is\n    sampled based on the individual\u2019s topic weights (which sum to 1 across\n    topics, for a given individual), and a disease is sampled based on the\n    individual\u2019s age and the age-dependent topic loadings (which sum to 1\n    across diseases, for a given topic at a given age). The model\n    generalises the Latent Dirichlet Allocation (LDA) model by allowing\n    topic loadings for each topic to vary with age.\n    References: Jiang (2023) <doi:10.1038/s41588-023-01522-8>.",
    "version": "0.1.0",
    "maintainer": "Xilin Jiang <jiangxilin1@gmail.com>",
    "author": "Xilin Jiang [aut, cre] (ORCID: <https://orcid.org/0000-0001-6773-9182>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AgeTopicModels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AgeTopicModels Inferring Age-Dependent Disease Topic from Diagnosis Data We propose an age-dependent topic modelling (ATM) model,\n    providing a low-rank representation of longitudinal records of\n    hundreds of distinct diseases in large electronic health record data sets. The model\n    assigns to each individual topic weights for several disease topics;\n    each disease topic reflects a set of diseases that tend to co-occur as\n    a function of age, quantified by age-dependent topic loadings for each\n    disease. The model assumes that for each disease diagnosis, a topic is\n    sampled based on the individual\u2019s topic weights (which sum to 1 across\n    topics, for a given individual), and a disease is sampled based on the\n    individual\u2019s age and the age-dependent topic loadings (which sum to 1\n    across diseases, for a given topic at a given age). The model\n    generalises the Latent Dirichlet Allocation (LDA) model by allowing\n    topic loadings for each topic to vary with age.\n    References: Jiang (2023) <doi:10.1038/s41588-023-01522-8>.  "
  },
  {
    "id": 1918,
    "package_name": "BTM",
    "title": "Biterm Topic Models for Short Text",
    "description": "Biterm Topic Models find topics in collections of short texts. \n    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.\n    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis \n    which are word-document co-occurrence topic models.\n    A biterm consists of two words co-occurring in the same short text window.  \n    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. \n    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf>.",
    "version": "0.3.8",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Xiaohui Yan [ctb, cph] (BTM C++ library)",
    "url": "https://github.com/bnosac/BTM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BTM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BTM Biterm Topic Models for Short Text Biterm Topic Models find topics in collections of short texts. \n    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.\n    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis \n    which are word-document co-occurrence topic models.\n    A biterm consists of two words co-occurring in the same short text window.  \n    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. \n    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf>.  "
  },
  {
    "id": 2045,
    "package_name": "BetaBit",
    "title": "Mini Games from Adventures of Beta and Bit",
    "description": "Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.",
    "version": "2.2",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek [aut, cre],\n  Witold Chodor [trl],\n  Katarzyna Fak [aut],\n  Tomasz Zoltak [aut],\n  Foundation SmarterPoland.pl [cph]",
    "url": "https://github.com/BetaAndBit/Charts",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BetaBit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BetaBit Mini Games from Adventures of Beta and Bit Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.  "
  },
  {
    "id": 2617,
    "package_name": "ConversationAlign",
    "title": "Process Text and Compute Linguistic Alignment in Conversation\nTranscripts",
    "description": "Imports conversation transcripts into R, concatenates them into a single dataframe appending event identifiers, cleans and formats the text, then yokes user-specified psycholinguistic database values to each word.  'ConversationAlign' then computes alignment indices between two interlocutors across each transcript for >40 possible semantic, lexical, and affective dimensions. In addition to alignment, 'ConversationAlign' also produces a table of analytics (e.g., token count, type-token-ratio) in a summary table describing your particular text corpus.",
    "version": "0.4.0",
    "maintainer": "Jamie Reilly <jamie_reilly@temple.edu>",
    "author": "Jamie Reilly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0891-438X>),\n  Virginia Ulichney [aut],\n  Ben Sacks [aut],\n  Sarah Weinstein [ctb],\n  Chelsea Helion [ctb],\n  Gus Cooney [ctb]",
    "url": "https://github.com/Reilly-ConceptsCognitionLab/ConversationAlign",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConversationAlign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConversationAlign Process Text and Compute Linguistic Alignment in Conversation\nTranscripts Imports conversation transcripts into R, concatenates them into a single dataframe appending event identifiers, cleans and formats the text, then yokes user-specified psycholinguistic database values to each word.  'ConversationAlign' then computes alignment indices between two interlocutors across each transcript for >40 possible semantic, lexical, and affective dimensions. In addition to alignment, 'ConversationAlign' also produces a table of analytics (e.g., token count, type-token-ratio) in a summary table describing your particular text corpus.  "
  },
  {
    "id": 2777,
    "package_name": "DICEM",
    "title": "Directness and Intensity of Conflict Expression",
    "description": "A Natural Language Processing Model trained to detect directness and intensity during conflict. See <https://www.mikeyeomans.info>.",
    "version": "0.1.0",
    "maintainer": "Michael Yeomans <mk.yeomans@gmail.com>",
    "author": "Michael Yeomans [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DICEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DICEM Directness and Intensity of Conflict Expression A Natural Language Processing Model trained to detect directness and intensity during conflict. See <https://www.mikeyeomans.info>.  "
  },
  {
    "id": 2992,
    "package_name": "DisasterAlert",
    "title": "Disaster Alert and Sentiment Analysis",
    "description": "By systematically aggregating and processing textual reports from earthquakes, floods, storms,\n  wildfires, and other natural disasters, the framework enables a holistic assessment of crisis narratives.  \n  Intelligent cleaning and normalization techniques transform raw commentary into structured data, ensuring\n  precise extraction of disaster-specific insights. Collective sentiments of affected communities are  \n  quantitatively scored and qualitatively categorized, providing a multifaceted view of societal responses  \n  under duress. Interactive geographic maps and temporal charts illustrate the evolution and spatial dispersion\n  of emotional reactions and impact indicators. ",
    "version": "1.0.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Hossein Hassani [aut],\n  Nadejda Komendantova [aut],\n  Leila Marvian Mashhad [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DisasterAlert",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DisasterAlert Disaster Alert and Sentiment Analysis By systematically aggregating and processing textual reports from earthquakes, floods, storms,\n  wildfires, and other natural disasters, the framework enables a holistic assessment of crisis narratives.  \n  Intelligent cleaning and normalization techniques transform raw commentary into structured data, ensuring\n  precise extraction of disaster-specific insights. Collective sentiments of affected communities are  \n  quantitatively scored and qualitatively categorized, providing a multifaceted view of societal responses  \n  under duress. Interactive geographic maps and temporal charts illustrate the evolution and spatial dispersion\n  of emotional reactions and impact indicators.   "
  },
  {
    "id": 3253,
    "package_name": "EpidigiR",
    "title": "Digital Epidemiological Analysis and Visualization Tools",
    "description": "Integrates methods for epidemiological analysis, modeling, and visualization, including functions for summary statistics, SIR (Susceptible-Infectious-Recovered) modeling, DALY (Disability-Adjusted Life Years) estimation, age standardization, diagnostic test evaluation, NLP (Natural Language Processing) keyword extraction, clinical trial power analysis, survival analysis, SNP (Single Nucleotide Polymorphism) association, and machine learning methods such as logistic regression, k-means clustering, Random Forest, and Support Vector Machine (SVM). Includes datasets for prevalence estimation, SIR modeling, genomic analysis, clinical trials, DALY, diagnostic tests, and survival analysis. Methods are based on Gelman et al. (2013) <doi:10.1201/b16018> and Wickham et al. (2019, ISBN:9781492052040>.",
    "version": "0.1.2",
    "maintainer": "Esther Atsabina Wanjala <digitalepidemiologist23@gmail.com>",
    "author": "Esther Atsabina Wanjala [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpidigiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpidigiR Digital Epidemiological Analysis and Visualization Tools Integrates methods for epidemiological analysis, modeling, and visualization, including functions for summary statistics, SIR (Susceptible-Infectious-Recovered) modeling, DALY (Disability-Adjusted Life Years) estimation, age standardization, diagnostic test evaluation, NLP (Natural Language Processing) keyword extraction, clinical trial power analysis, survival analysis, SNP (Single Nucleotide Polymorphism) association, and machine learning methods such as logistic regression, k-means clustering, Random Forest, and Support Vector Machine (SVM). Includes datasets for prevalence estimation, SIR modeling, genomic analysis, clinical trials, DALY, diagnostic tests, and survival analysis. Methods are based on Gelman et al. (2013) <doi:10.1201/b16018> and Wickham et al. (2019, ISBN:9781492052040>.  "
  },
  {
    "id": 3793,
    "package_name": "GenAI",
    "title": "Generative Artificial Intelligence",
    "description": "Utilizing Generative Artificial Intelligence models like 'GPT-4' and 'Gemini Pro' as coding and writing assistants for 'R' users. Through these models, 'GenAI' offers a variety of functions, encompassing text generation, code optimization, natural language processing, chat, and image interpretation. The goal is to aid 'R' users in streamlining laborious coding and language processing tasks.",
    "version": "0.2.0",
    "maintainer": "Li Yuan <lyuan@gd.edu.kg>",
    "author": "Li Yuan [aut, cre] (ORCID: <https://orcid.org/0009-0008-1075-9922>)",
    "url": "https://genai.gd.edu.kg/",
    "bug_reports": "https://github.com/GitData-GA/GenAI/issues",
    "repository": "https://cran.r-project.org/package=GenAI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GenAI Generative Artificial Intelligence Utilizing Generative Artificial Intelligence models like 'GPT-4' and 'Gemini Pro' as coding and writing assistants for 'R' users. Through these models, 'GenAI' offers a variety of functions, encompassing text generation, code optimization, natural language processing, chat, and image interpretation. The goal is to aid 'R' users in streamlining laborious coding and language processing tasks.  "
  },
  {
    "id": 3876,
    "package_name": "Goodreader",
    "title": "Scrape and Analyze 'Goodreads' Book Data",
    "description": "A comprehensive toolkit for scraping and analyzing book data from <https://www.goodreads.com/>. This package provides functions to search for books, scrape book details and reviews, perform sentiment analysis on reviews, and conduct topic modeling. It's designed for researchers, data analysts, and book enthusiasts who want to gain insights from 'Goodreads' data. ",
    "version": "0.1.2",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/Goodreader,\nhttp://liu-chao.site/Goodreader/",
    "bug_reports": "https://github.com/chaoliu-cl/Goodreader/issues",
    "repository": "https://cran.r-project.org/package=Goodreader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Goodreader Scrape and Analyze 'Goodreads' Book Data A comprehensive toolkit for scraping and analyzing book data from <https://www.goodreads.com/>. This package provides functions to search for books, scrape book details and reviews, perform sentiment analysis on reviews, and conduct topic modeling. It's designed for researchers, data analysts, and book enthusiasts who want to gain insights from 'Goodreads' data.   "
  },
  {
    "id": 4443,
    "package_name": "LDAvis",
    "title": "Interactive Visualization of Topic Models",
    "description": "Tools to create an interactive web-based visualization of a\n    topic model that has been fit to a corpus of text data using\n    Latent Dirichlet Allocation (LDA). Given the estimated parameters of\n    the topic model, it computes various summary statistics as input to\n    an interactive visualization built with D3.js that is accessed via\n    a browser. The goal is to help users interpret the topics in their\n    LDA topic model.",
    "version": "0.3.2",
    "maintainer": "Carson Sievert <cpsievert1@gmail.com>",
    "author": "Carson Sievert [aut, cre],\n  Kenny Shirley [aut]",
    "url": "https://github.com/cpsievert/LDAvis",
    "bug_reports": "https://github.com/cpsievert/LDAvis/issues",
    "repository": "https://cran.r-project.org/package=LDAvis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LDAvis Interactive Visualization of Topic Models Tools to create an interactive web-based visualization of a\n    topic model that has been fit to a corpus of text data using\n    Latent Dirichlet Allocation (LDA). Given the estimated parameters of\n    the topic model, it computes various summary statistics as input to\n    an interactive visualization built with D3.js that is accessed via\n    a browser. The goal is to help users interpret the topics in their\n    LDA topic model.  "
  },
  {
    "id": 4469,
    "package_name": "LLMing",
    "title": "Large Language Model (LLM) Tools for Psychological Text Analysis",
    "description": "A collection of large language model (LLM) text analysis methods\n  designed with psychological data in mind. Currently, LLMing (aka \"lemming\")\n  includes a text anomaly detection method based on the angle-based subspace\n  approach described by Zhang, Lin, and Karim (2015)\n  <doi:10.1016/j.ress.2015.05.025>.",
    "version": "1.0.0",
    "maintainer": "Lindley Slipetz <ddj6tu@virginia.edu>",
    "author": "Lindley Slipetz [aut, cre],\n  Teague Henry [aut],\n  Siqi Sun [ctb]",
    "url": "https://github.com/sliplr19/LLMing",
    "bug_reports": "https://github.com/sliplr19/LLMing/issues",
    "repository": "https://cran.r-project.org/package=LLMing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMing Large Language Model (LLM) Tools for Psychological Text Analysis A collection of large language model (LLM) text analysis methods\n  designed with psychological data in mind. Currently, LLMing (aka \"lemming\")\n  includes a text anomaly detection method based on the angle-based subspace\n  approach described by Zhang, Lin, and Karim (2015)\n  <doi:10.1016/j.ress.2015.05.025>.  "
  },
  {
    "id": 4523,
    "package_name": "LSX",
    "title": "Semi-Supervised Algorithm for Document Scaling",
    "description": "A word embeddings-based semi-supervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.\n    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).\n    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.",
    "version": "1.5.1",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph]",
    "url": "https://koheiw.github.io/LSX/",
    "bug_reports": "https://github.com/koheiw/LSX/issues",
    "repository": "https://cran.r-project.org/package=LSX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSX Semi-Supervised Algorithm for Document Scaling A word embeddings-based semi-supervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.\n    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).\n    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.  "
  },
  {
    "id": 4587,
    "package_name": "LilRhino",
    "title": "For Implementation of Feed Reduction, Learning Examples, NLP and\nCode Management",
    "description": "This is for code management functions, NLP tools, a Monty Hall simulator, and for implementing my own variable reduction technique called Feed Reduction. The Feed Reduction technique is not yet published, but is merely a tool for implementing a series of binary neural networks meant for reducing data into N dimensions, where N is the number of possible values of the response variable.",
    "version": "1.2.2",
    "maintainer": "Travis Barton <travisdatabarton@gmail.com>",
    "author": "Travis Barton (2018) ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LilRhino",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LilRhino For Implementation of Feed Reduction, Learning Examples, NLP and\nCode Management This is for code management functions, NLP tools, a Monty Hall simulator, and for implementing my own variable reduction technique called Feed Reduction. The Feed Reduction technique is not yet published, but is merely a tool for implementing a series of binary neural networks meant for reducing data into N dimensions, where N is the number of possible values of the response variable.  "
  },
  {
    "id": 4653,
    "package_name": "MAP",
    "title": "Multimodal Automated Phenotyping",
    "description": "Electronic health records (EHR) linked with biorepositories are \n    a powerful platform for translational studies. A major bottleneck exists \n    in the ability to phenotype patients accurately and efficiently. \n    Towards that end, we developed an automated high-throughput \n    phenotyping method integrating International \n    Classification of Diseases (ICD) codes and narrative data extracted \n    using natural language processing (NLP). Specifically, our proposed method, \n    called MAP (Map Automated Phenotyping algorithm), fits an ensemble of latent \n    mixture models on aggregated ICD and NLP counts along with healthcare \n    utilization. The MAP algorithm yields a predicted probability of phenotype \n    for each patient and a threshold for classifying subjects with phenotype \n    yes/no (See Katherine P. Liao, et al. (2019) <doi:10.1093/jamia/ocz066>.).",
    "version": "1.0.0",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "author": "Thomas Charlon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7497-0470>),\n  Chuan Hong [aut],\n  Jiehuan Sun [aut],\n  Katherine Liao [aut],\n  Sheng Yu [aut],\n  Tianxi Cai [aut],\n  PARSE Health [aut] (<https://parse-health.org>),\n  CELEHS [aut] (<https://celehs.hms.harvard.edu>)",
    "url": "https://celehs.github.io/MAP/",
    "bug_reports": "https://github.com/celehs/MAP/issues",
    "repository": "https://cran.r-project.org/package=MAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MAP Multimodal Automated Phenotyping Electronic health records (EHR) linked with biorepositories are \n    a powerful platform for translational studies. A major bottleneck exists \n    in the ability to phenotype patients accurately and efficiently. \n    Towards that end, we developed an automated high-throughput \n    phenotyping method integrating International \n    Classification of Diseases (ICD) codes and narrative data extracted \n    using natural language processing (NLP). Specifically, our proposed method, \n    called MAP (Map Automated Phenotyping algorithm), fits an ensemble of latent \n    mixture models on aggregated ICD and NLP counts along with healthcare \n    utilization. The MAP algorithm yields a predicted probability of phenotype \n    for each patient and a threshold for classifying subjects with phenotype \n    yes/no (See Katherine P. Liao, et al. (2019) <doi:10.1093/jamia/ocz066>.).  "
  },
  {
    "id": 4939,
    "package_name": "MadanText",
    "title": "Persian Text Mining Tool for Frequency Analysis, Statistical\nAnalysis, and Word Clouds",
    "description": "This is an open-source software designed specifically for text mining in the Persian language. It allows users to examine word frequencies, download data for analysis, and generate word clouds. This tool is particularly useful for researchers and analysts working with Persian language data.\n    This package mainly makes use of the 'PersianStemmer' (Safshekan, R., et al. (2019). <https://CRAN.R-project.org/package=PersianStemmer>),\n    'udpipe' (Wijffels, J., et al. (2023). <https://CRAN.R-project.org/package=udpipe>),\n    and 'shiny' (Chang, W., et al. (2023). <https://CRAN.R-project.org/package=shiny>) packages.",
    "version": "0.1.0",
    "maintainer": "Kido Ishikawa <kido.ishikawa6@gmail.com>",
    "author": "Kido Ishikawa [aut, cre],\n  Hasan Khosravi [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MadanText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MadanText Persian Text Mining Tool for Frequency Analysis, Statistical\nAnalysis, and Word Clouds This is an open-source software designed specifically for text mining in the Persian language. It allows users to examine word frequencies, download data for analysis, and generate word clouds. This tool is particularly useful for researchers and analysts working with Persian language data.\n    This package mainly makes use of the 'PersianStemmer' (Safshekan, R., et al. (2019). <https://CRAN.R-project.org/package=PersianStemmer>),\n    'udpipe' (Wijffels, J., et al. (2023). <https://CRAN.R-project.org/package=udpipe>),\n    and 'shiny' (Chang, W., et al. (2023). <https://CRAN.R-project.org/package=shiny>) packages.  "
  },
  {
    "id": 4940,
    "package_name": "MadanTextNetwork",
    "title": "Persian Text Mining Tool for Co-Occurrence Network",
    "description": "Provides an extension to 'MadanText' for creating and analyzing co-occurrence networks in Persian text data.\n             This package mainly makes use of the 'PersianStemmer' (Safshekan, R., et al. (2019). <https://CRAN.R-project.org/package=PersianStemmer>),\n             'udpipe' (Wijffels, J., et al. (2023). <https://CRAN.R-project.org/package=udpipe>),\n             and 'shiny' (Chang, W., et al. (2023). <https://CRAN.R-project.org/package=shiny>) packages.",
    "version": "0.1.0",
    "maintainer": "Kido Ishikawa <kido.ishikawa6@gmail.com>",
    "author": "Kido Ishikawa [aut, cre],\n  Hasan Khosravi [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MadanTextNetwork",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MadanTextNetwork Persian Text Mining Tool for Co-Occurrence Network Provides an extension to 'MadanText' for creating and analyzing co-occurrence networks in Persian text data.\n             This package mainly makes use of the 'PersianStemmer' (Safshekan, R., et al. (2019). <https://CRAN.R-project.org/package=PersianStemmer>),\n             'udpipe' (Wijffels, J., et al. (2023). <https://CRAN.R-project.org/package=udpipe>),\n             and 'shiny' (Chang, W., et al. (2023). <https://CRAN.R-project.org/package=shiny>) packages.  "
  },
  {
    "id": 5003,
    "package_name": "MediaNews",
    "title": "Media News Extraction for Text Analysis",
    "description": "Extract textual data from different media channels \n    through its source based on users choice of keywords. \n    These data can be used to perform text analysis to \n    identify patterns in respective media reporting.\n    The media channels used in this package are print media.\n    The data (or news) used are publicly available to consumers.",
    "version": "0.2.1",
    "maintainer": "Vatsal Aima <vaima75@hotmail.com>",
    "author": "Vatsal Aima [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MediaNews",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MediaNews Media News Extraction for Text Analysis Extract textual data from different media channels \n    through its source based on users choice of keywords. \n    These data can be used to perform text analysis to \n    identify patterns in respective media reporting.\n    The media channels used in this package are print media.\n    The data (or news) used are publicly available to consumers.  "
  },
  {
    "id": 5020,
    "package_name": "MetaNLP",
    "title": "Natural Language Processing for Meta Analysis",
    "description": "Given a CSV file with titles and abstracts, the package creates a\n    document-term matrix that is lemmatized and stemmed and can directly be used to\n    train machine learning methods for automatic title-abstract screening in the\n    preparation of a meta analysis.",
    "version": "0.1.4",
    "maintainer": "Maximilian Pilz <maximilian.pilz@itwm.fraunhofer.de>",
    "author": "Nico Bruder [aut] (ORCID: <https://orcid.org/0009-0004-9522-2075>),\n  Samuel Zimmermann [aut] (ORCID:\n    <https://orcid.org/0009-0000-4828-9294>),\n  Johannes Vey [aut] (ORCID: <https://orcid.org/0000-0002-2610-9667>),\n  Maximilian Pilz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9685-1613>),\n  Institute of Medical Biometry - University of Heidelberg [cph]",
    "url": "https://github.com/imbi-heidelberg/MetaNLP",
    "bug_reports": "https://github.com/imbi-heidelberg/MetaNLP/issues",
    "repository": "https://cran.r-project.org/package=MetaNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MetaNLP Natural Language Processing for Meta Analysis Given a CSV file with titles and abstracts, the package creates a\n    document-term matrix that is lemmatized and stemmed and can directly be used to\n    train machine learning methods for automatic title-abstract screening in the\n    preparation of a meta analysis.  "
  },
  {
    "id": 5234,
    "package_name": "NLP",
    "title": "Natural Language Processing Infrastructure",
    "description": "Basic classes and methods for Natural Language Processing.",
    "version": "0.3-2",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NLP Natural Language Processing Infrastructure Basic classes and methods for Natural Language Processing.  "
  },
  {
    "id": 5235,
    "package_name": "NLPclient",
    "title": "Stanford 'CoreNLP' Annotation Client",
    "description": "Stanford 'CoreNLP' annotation client.\n  Stanford 'CoreNLP' <https://stanfordnlp.github.io/CoreNLP/index.html> integrates all \n  NLP tools from the Stanford Natural Language Processing Group, \n  including a part-of-speech (POS) tagger, a named entity recognizer (NER), \n  a parser, and a coreference resolution system, and provides model files \n  for the analysis of English. More information can be found in the README.",
    "version": "1.0",
    "maintainer": "Florian Schwendinger <FlorianSchwendinger@gmx.at>",
    "author": "Florian Schwendinger [aut, cre],\n  Kurt Hornik [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NLPclient",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NLPclient Stanford 'CoreNLP' Annotation Client Stanford 'CoreNLP' annotation client.\n  Stanford 'CoreNLP' <https://stanfordnlp.github.io/CoreNLP/index.html> integrates all \n  NLP tools from the Stanford Natural Language Processing Group, \n  including a part-of-speech (POS) tagger, a named entity recognizer (NER), \n  a parser, and a coreference resolution system, and provides model files \n  for the analysis of English. More information can be found in the README.  "
  },
  {
    "id": 5236,
    "package_name": "NLPutils",
    "title": "Natural Language Processing Utilities",
    "description": "Utilities for Natural Language Processing.",
    "version": "0.0-5.1",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NLPutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NLPutils Natural Language Processing Utilities Utilities for Natural Language Processing.  "
  },
  {
    "id": 5285,
    "package_name": "NUSS",
    "title": "Mixed N-Grams and Unigram Sequence Segmentation",
    "description": "Segmentation of short text sequences - like hashtags - into the\n    separated words sequence, done with the use of dictionary, which may be\n    built on custom corpus of texts. Unigram dictionary is used to find most\n    probable sequence, and n-grams approach is used to determine possible\n    segmentation given the text corpus.",
    "version": "0.1.0",
    "maintainer": "Oskar Kosch <contact@oskarkosch.com>",
    "author": "Oskar Kosch [aut, cre] (ORCID: <https://orcid.org/0000-0003-2697-1393>)",
    "url": "https://github.com/theogrost/NUSS",
    "bug_reports": "https://github.com/theogrost/NUSS/issues",
    "repository": "https://cran.r-project.org/package=NUSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NUSS Mixed N-Grams and Unigram Sequence Segmentation Segmentation of short text sequences - like hashtags - into the\n    separated words sequence, done with the use of dictionary, which may be\n    built on custom corpus of texts. Unigram dictionary is used to find most\n    probable sequence, and n-grams approach is used to determine possible\n    segmentation given the text corpus.  "
  },
  {
    "id": 5733,
    "package_name": "PaLMr",
    "title": "Interface for 'Google Pathways Language Model 2 (PaLM 2)'",
    "description": "'Google Pathways Language Model 2 (PaLM 2)' as a coding and writing assistant designed for 'R'. With a range of functions, including natural language processing and coding optimization, to assist 'R' developers in simplifying tedious coding tasks and content searching.",
    "version": "0.2.0",
    "maintainer": "Li Yuan <lyuan@gd.edu.kg>",
    "author": "Li Yuan [aut, cre] (ORCID: <https://orcid.org/0009-0008-1075-9922>)",
    "url": "https://palmr.ly.gd.edu.kg/",
    "bug_reports": "https://github.com/lygitdata/PaLMr/issues",
    "repository": "https://cran.r-project.org/package=PaLMr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PaLMr Interface for 'Google Pathways Language Model 2 (PaLM 2)' 'Google Pathways Language Model 2 (PaLM 2)' as a coding and writing assistant designed for 'R'. With a range of functions, including natural language processing and coding optimization, to assist 'R' developers in simplifying tedious coding tasks and content searching.  "
  },
  {
    "id": 5803,
    "package_name": "PersianStemmer",
    "title": "Persian Stemmer for Text Analysis",
    "description": "Allows users to stem Persian texts for text analysis.",
    "version": "1.0",
    "maintainer": "Roozbeh Safshekan <rse@mit.edu>",
    "author": "Roozbeh Safshekan and Rich Nielsen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PersianStemmer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PersianStemmer Persian Stemmer for Text Analysis Allows users to stem Persian texts for text analysis.  "
  },
  {
    "id": 6056,
    "package_name": "R.temis",
    "title": "Integrated Text Mining Solution",
    "description": "An integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, lexical summary, terms\n    co-occurrences and documents similarity measures, graphs of terms,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",
    "version": "0.1.4",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre],\n  Gilles Bastin [aut],\n  Antoine Chollet [aut]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=R.temis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R.temis Integrated Text Mining Solution An integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, lexical summary, terms\n    co-occurrences and documents similarity measures, graphs of terms,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.  "
  },
  {
    "id": 6261,
    "package_name": "RKorAPClient",
    "title": "'KorAP' Web Service Client Package",
    "description": "A client package that makes the 'KorAP' web service API accessible from R. The corpus analysis platform 'KorAP' has been developed as a scientific tool to make potentially large, stratified and multiply annotated corpora, such as the 'German Reference Corpus DeReKo' or the 'Corpus of the Contemporary Romanian Language CoRoLa', accessible for linguists to let them verify hypotheses and to find interesting patterns in real language use. The 'RKorAPClient' package provides access to 'KorAP' and the corpora behind it for user-created R code, as a programmatic alternative to the 'KorAP' web user-interface. You can learn more about 'KorAP' and use it directly on 'DeReKo' at <https://korap.ids-mannheim.de/>.",
    "version": "1.2.1",
    "maintainer": "Marc Kupietz <kupietz@ids-mannheim.de>",
    "author": "Marc Kupietz [aut, cre],\n  Nils Diewald [ctb],\n  Leibniz Institute for the German Language [cph, fnd]",
    "url": "https://github.com/KorAP/RKorAPClient/,\nhttps://korap.ids-mannheim.de/,\nhttps://www.ids-mannheim.de/digspra/kl/projekte/korap",
    "bug_reports": "https://github.com/KorAP/RKorAPClient/issues",
    "repository": "https://cran.r-project.org/package=RKorAPClient",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RKorAPClient 'KorAP' Web Service Client Package A client package that makes the 'KorAP' web service API accessible from R. The corpus analysis platform 'KorAP' has been developed as a scientific tool to make potentially large, stratified and multiply annotated corpora, such as the 'German Reference Corpus DeReKo' or the 'Corpus of the Contemporary Romanian Language CoRoLa', accessible for linguists to let them verify hypotheses and to find interesting patterns in real language use. The 'RKorAPClient' package provides access to 'KorAP' and the corpora behind it for user-created R code, as a programmatic alternative to the 'KorAP' web user-interface. You can learn more about 'KorAP' and use it directly on 'DeReKo' at <https://korap.ids-mannheim.de/>.  "
  },
  {
    "id": 6410,
    "package_name": "RSentiment",
    "title": "Analyse Sentiment of English Sentences",
    "description": "Analyses sentiment of a sentence in English and assigns score to it. It can classify sentences to the following categories of sentiments:- Positive, Negative, very Positive, very negative, \n              Neutral. For a vector of sentences, it counts the number of sentences in each\n              category of sentiment.In calculating the score, negation and various degrees\n              of adjectives are taken into consideration. It deals only with English sentences.",
    "version": "2.2.2",
    "maintainer": "Subhasree Bose <subhasree10.7@gmail.com>",
    "author": "Subhasree Bose <subhasree10.7@gmail.com> with contributons from Saptarsi Goswami.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSentiment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSentiment Analyse Sentiment of English Sentences Analyses sentiment of a sentence in English and assigns score to it. It can classify sentences to the following categories of sentiments:- Positive, Negative, very Positive, very negative, \n              Neutral. For a vector of sentences, it counts the number of sentences in each\n              category of sentiment.In calculating the score, negation and various degrees\n              of adjectives are taken into consideration. It deals only with English sentences.  "
  },
  {
    "id": 6486,
    "package_name": "RapidFuzz",
    "title": "String Similarity Computation Using 'RapidFuzz'",
    "description": "Provides a high-performance interface for calculating string similarities and distances, leveraging the efficient library 'RapidFuzz' <https://github.com/rapidfuzz/rapidfuzz-cpp>. This package integrates the 'C++' implementation, allowing 'R' users to access cutting-edge algorithms for fuzzy matching and text analysis.",
    "version": "1.0",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Hugo Vaconcelos [aut],\n  Max Bachmann [ctb],\n  Adam Cohen [ctb]",
    "url": "<https://github.com/StrategicProjects/RapidFuzz>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RapidFuzz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RapidFuzz String Similarity Computation Using 'RapidFuzz' Provides a high-performance interface for calculating string similarities and distances, leveraging the efficient library 'RapidFuzz' <https://github.com/rapidfuzz/rapidfuzz-cpp>. This package integrates the 'C++' implementation, allowing 'R' users to access cutting-edge algorithms for fuzzy matching and text analysis.  "
  },
  {
    "id": 6547,
    "package_name": "RcmdrPlugin.temis",
    "title": "Graphical Integrated Text Mining Solution",
    "description": "An 'R Commander' plug-in providing an integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, vocabulary tables, terms\n    co-occurrences and documents similarity measures, time series analysis,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",
    "version": "0.7.12",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre],\n  Gilles Bastin [aut]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.temis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.temis Graphical Integrated Text Mining Solution An 'R Commander' plug-in providing an integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, vocabulary tables, terms\n    co-occurrences and documents similarity measures, time series analysis,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.  "
  },
  {
    "id": 6561,
    "package_name": "RcppCWB",
    "title": "'Rcpp' Bindings for the 'Corpus Workbench' ('CWB')",
    "description": "'Rcpp' Bindings for the C code of the 'Corpus Workbench' ('CWB'), an indexing and query \n  engine to efficiently analyze large corpora (<https://cwb.sourceforge.io>). 'RcppCWB' is licensed\n  under the GNU GPL-3, in line with the GPL-3 license of the 'CWB' (<https://www.r-project.org/Licenses/GPL-3>).\n  The 'CWB' relies on 'pcre2' (BSD license, see <https://github.com/PCRE2Project/pcre2/blob/master/LICENCE.md>)\n  and 'GLib' (LGPL license, see <https://www.gnu.org/licenses/lgpl-3.0.en.html>).\n  See the file LICENSE.note for further information. The package includes modified code of the\n  'rcqp' package (GPL-2, see <https://cran.r-project.org/package=rcqp>). The original work of the authors\n  of the 'rcqp' package is acknowledged with great respect, and they are listed as authors of this\n  package. To achieve cross-platform portability (including Windows), using 'Rcpp' for wrapper code\n  is the approach used by 'RcppCWB'.",
    "version": "0.6.10",
    "maintainer": "Andreas Blaette <andreas.blaette@uni-due.de>",
    "author": "Andreas Blaette [aut, cre],\n  Bernard Desgraupes [aut],\n  Sylvain Loiseau [aut],\n  Oliver Christ [ctb],\n  Bruno Maximilian Schulze [ctb],\n  Stephanie Evert [ctb],\n  Arne Fitschen [ctb],\n  Jeroen Ooms [ctb],\n  Marius Bertram [ctb],\n  Tomas Kalibera [ctb]",
    "url": "https://github.com/PolMine/RcppCWB",
    "bug_reports": "https://github.com/PolMine/RcppCWB/issues",
    "repository": "https://cran.r-project.org/package=RcppCWB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppCWB 'Rcpp' Bindings for the 'Corpus Workbench' ('CWB') 'Rcpp' Bindings for the C code of the 'Corpus Workbench' ('CWB'), an indexing and query \n  engine to efficiently analyze large corpora (<https://cwb.sourceforge.io>). 'RcppCWB' is licensed\n  under the GNU GPL-3, in line with the GPL-3 license of the 'CWB' (<https://www.r-project.org/Licenses/GPL-3>).\n  The 'CWB' relies on 'pcre2' (BSD license, see <https://github.com/PCRE2Project/pcre2/blob/master/LICENCE.md>)\n  and 'GLib' (LGPL license, see <https://www.gnu.org/licenses/lgpl-3.0.en.html>).\n  See the file LICENSE.note for further information. The package includes modified code of the\n  'rcqp' package (GPL-2, see <https://cran.r-project.org/package=rcqp>). The original work of the authors\n  of the 'rcqp' package is acknowledged with great respect, and they are listed as authors of this\n  package. To achieve cross-platform portability (including Windows), using 'Rcpp' for wrapper code\n  is the approach used by 'RcppCWB'.  "
  },
  {
    "id": 6585,
    "package_name": "RcppJagger",
    "title": "An R Wrapper for Jagger",
    "description": "A wrapper for Jagger, a morphological analyzer proposed in Yoshinaga (2023) <arXiv:2305.19045>. Jagger uses patterns derived from morphological dictionaries and training data sets and applies them from the beginning of the input. This simultaneous and deterministic process enables it to effectively perform tokenization, POS tagging, and lemmatization.",
    "version": "0.0.2",
    "maintainer": "Shusei Eshima <shuseieshima@gmail.com>",
    "author": "Shusei Eshima [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3613-4046>),\n  Naoki Yoshinaga [ctb]",
    "url": "https://shusei-e.github.io/RcppJagger/,\nhttps://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jagger/index.en.html",
    "bug_reports": "https://github.com/Shusei-E/RcppJagger/issues",
    "repository": "https://cran.r-project.org/package=RcppJagger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppJagger An R Wrapper for Jagger A wrapper for Jagger, a morphological analyzer proposed in Yoshinaga (2023) <arXiv:2305.19045>. Jagger uses patterns derived from morphological dictionaries and training data sets and applies them from the beginning of the input. This simultaneous and deterministic process enables it to effectively perform tokenization, POS tagging, and lemmatization.  "
  },
  {
    "id": 6740,
    "package_name": "RmecabKo",
    "title": "An 'Rcpp' Interface for Eunjeon Project",
    "description": "An 'Rcpp' interface for Eunjeon project <http://eunjeon.blogspot.com/>.\n    The 'mecab-ko' and 'mecab-ko-dic' is based on a C++ library,\n    and part-of-speech tagging with them is useful when the spacing of source Korean text is not correct.\n    This package provides part-of-speech tagging and tokenization function for Korean text.",
    "version": "0.1.6.2",
    "maintainer": "Junhewk Kim <junhewk.kim@gmail.com>",
    "author": "Junhewk Kim",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RmecabKo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RmecabKo An 'Rcpp' Interface for Eunjeon Project An 'Rcpp' interface for Eunjeon project <http://eunjeon.blogspot.com/>.\n    The 'mecab-ko' and 'mecab-ko-dic' is based on a C++ library,\n    and part-of-speech tagging with them is useful when the spacing of source Korean text is not correct.\n    This package provides part-of-speech tagging and tokenization function for Korean text.  "
  },
  {
    "id": 7209,
    "package_name": "Sejong",
    "title": "KoNLP static dictionaries and Sejong project resources",
    "description": "Sejong(http://www.sejong.or.kr/) corpus and\n        Hannanum(http://semanticweb.kaist.ac.kr/home/index.php/HanNanum)\n        dictionaries for KoNLP",
    "version": "0.01",
    "maintainer": "Heewon Jeon <madjakarta@gmail.com>",
    "author": "Heewon Jeon",
    "url": "https://github.com/haven-jeon/Sejong",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Sejong",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Sejong KoNLP static dictionaries and Sejong project resources Sejong(http://www.sejong.or.kr/) corpus and\n        Hannanum(http://semanticweb.kaist.ac.kr/home/index.php/HanNanum)\n        dictionaries for KoNLP  "
  },
  {
    "id": 7232,
    "package_name": "SentimentAnalysis",
    "title": "Dictionary-Based Sentiment Analysis",
    "description": "Performs a sentiment analysis of textual contents in R. This implementation\n    utilizes various existing dictionaries, such as Harvard IV, or finance-specific \n    dictionaries. Furthermore, it can also create customized dictionaries. The latter \n    uses LASSO regularization as a statistical approach to select relevant terms based on \n    an exogenous response variable. ",
    "version": "1.3-5",
    "maintainer": "Nicolas Proellochs <nicolas@nproellochs.com>",
    "author": "Nicolas Proellochs [aut, cre],\n  Stefan Feuerriegel [aut]",
    "url": "https://github.com/sfeuerriegel/SentimentAnalysis",
    "bug_reports": "https://github.com/sfeuerriegel/SentimentAnalysis/issues",
    "repository": "https://cran.r-project.org/package=SentimentAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SentimentAnalysis Dictionary-Based Sentiment Analysis Performs a sentiment analysis of textual contents in R. This implementation\n    utilizes various existing dictionaries, such as Harvard IV, or finance-specific \n    dictionaries. Furthermore, it can also create customized dictionaries. The latter \n    uses LASSO regularization as a statistical approach to select relevant terms based on \n    an exogenous response variable.   "
  },
  {
    "id": 7360,
    "package_name": "SouthParkRshiny",
    "title": "Data and 'Shiny' Application for the Show 'SouthPark'",
    "description": "Ratings, votes, swear words and sentiments are analysed for\n    the show 'SouthPark' through a 'Shiny' application after web scraping \n    from 'IMDB' and the website <https://southpark.fandom.com/wiki/South_Park_Archives>.",
    "version": "1.0.0",
    "maintainer": "Amalan Mahendran <amalan0595@gmail.com>",
    "author": "Amalan Mahendran [cre, aut]",
    "url": "https://github.com/Amalan-ConStat/SouthParkRshiny,https://amalan-con-stat.shinyapps.io/SouthParkRshiny/",
    "bug_reports": "https://github.com/Amalan-ConStat/SouthParkRshiny/issues",
    "repository": "https://cran.r-project.org/package=SouthParkRshiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SouthParkRshiny Data and 'Shiny' Application for the Show 'SouthPark' Ratings, votes, swear words and sentiments are analysed for\n    the show 'SouthPark' through a 'Shiny' application after web scraping \n    from 'IMDB' and the website <https://southpark.fandom.com/wiki/South_Park_Archives>.  "
  },
  {
    "id": 7372,
    "package_name": "SpaTopic",
    "title": "Topic Inference to Identify Tissue Architecture in Multiplexed\nImages",
    "description": "A novel spatial topic model to integrate both cell type and spatial information to identify the complex spatial tissue architecture on multiplexed tissue images without human intervention. The Package implements a collapsed Gibbs sampling algorithm for inference. 'SpaTopic' is scalable to large-scale image datasets without extracting neighborhood information for every single cell. For more details on the methodology, see <https://xiyupeng.github.io/SpaTopic/>.",
    "version": "1.2.0",
    "maintainer": "Xiyu Peng <pansypeng124@gmail.com>",
    "author": "Xiyu Peng [aut, cre] (ORCID: <https://orcid.org/0000-0003-4232-0910>)",
    "url": "https://github.com/xiyupeng/SpaTopic",
    "bug_reports": "https://github.com/xiyupeng/SpaTopic/issues",
    "repository": "https://cran.r-project.org/package=SpaTopic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpaTopic Topic Inference to Identify Tissue Architecture in Multiplexed\nImages A novel spatial topic model to integrate both cell type and spatial information to identify the complex spatial tissue architecture on multiplexed tissue images without human intervention. The Package implements a collapsed Gibbs sampling algorithm for inference. 'SpaTopic' is scalable to large-scale image datasets without extracting neighborhood information for every single cell. For more details on the methodology, see <https://xiyupeng.github.io/SpaTopic/>.  "
  },
  {
    "id": 7748,
    "package_name": "TextMiningGUI",
    "title": "Text Mining GUI Interface",
    "description": "Graphic interface for text analysis, implement a few methods such as biplots, correspondence analysis, co-occurrence, clustering, topic models, correlations and sentiments.",
    "version": "0.3",
    "maintainer": "Conrado Reyes <coreyes@gmail.com>",
    "author": "Conrado Reyes [aut, cre],\n  Purificacion Galindo [tch]",
    "url": "https://c0reyes.github.io/TextMiningGUI/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TextMiningGUI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TextMiningGUI Text Mining GUI Interface Graphic interface for text analysis, implement a few methods such as biplots, correspondence analysis, co-occurrence, clustering, topic models, correlations and sentiments.  "
  },
  {
    "id": 7786,
    "package_name": "TopicScore",
    "title": "The Topic SCORE Algorithm to Fit Topic Models",
    "description": "Provides implementation of the \"Topic SCORE\" algorithm that is\n\t     proposed by Tracy Ke and Minzhe Wang. The singular value decomposition\n\t     step is optimized through the usage of svds() function in 'RSpectra'\n\t     package, on a 'dgRMatrix' sparse matrix. Also provides a column-wise\n\t     error measure in the word-topic matrix A, and an algorithm for\n\t     recovering the topic-document matrix W given A and D based on\n\t     quadratic programming.\n\t\tThe details about the techniques are explained in the paper \"A new SVD approach to optimal topic estimation\" by Tracy Ke and Minzhe Wang (2017) <arXiv:1704.07016>.",
    "version": "0.0.1",
    "maintainer": "Minzhe Wang <minzhew@uchicago.edu>",
    "author": "Minzhe Wang [aut, cre],\n  Tracy Ke [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TopicScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TopicScore The Topic SCORE Algorithm to Fit Topic Models Provides implementation of the \"Topic SCORE\" algorithm that is\n\t     proposed by Tracy Ke and Minzhe Wang. The singular value decomposition\n\t     step is optimized through the usage of svds() function in 'RSpectra'\n\t     package, on a 'dgRMatrix' sparse matrix. Also provides a column-wise\n\t     error measure in the word-topic matrix A, and an algorithm for\n\t     recovering the topic-document matrix W given A and D based on\n\t     quadratic programming.\n\t\tThe details about the techniques are explained in the paper \"A new SVD approach to optimal topic estimation\" by Tracy Ke and Minzhe Wang (2017) <arXiv:1704.07016>.  "
  },
  {
    "id": 8053,
    "package_name": "WeatherSentiment",
    "title": "Comprehensive Analysis of Tweet Sentiments and Weather Data",
    "description": "A comprehensive suite of functions for processing,\n  analyzing, and visualizing textual data from tweets is offered.  \n  Users can clean tweets, analyze their sentiments, visualize data, \n  and examine the correlation between sentiments and environmental \n  data such as weather conditions. Main features include text processing,  \n  sentiment analysis, data visualization, correlation analysis, and \n  synthetic data generation. Text processing involves cleaning and preparing  \n  tweets by removing textual noise and irrelevant words. Sentiment analysis \n  extracts and accurately analyzes sentiments from tweet texts using advanced \n  algorithms. Data visualization creates various charts like word clouds\n  and sentiment polarity graphs for visual representation of data. Correlation \n  analysis examines and calculates the correlation between tweet sentiments \n  and environmental variables such as weather conditions. Additionally, \n  random tweets can be generated for testing and evaluating the performance \n  of analyses, empowering users to effectively analyze and interpret 'Twitter' \n  data for research and commercial purposes.",
    "version": "1.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Andriette Bekker [aut],\n  Mohammad Arashi [aut],\n  Leila Marvian Mashhad [aut, cre],\n  Priyanka Nagar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WeatherSentiment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeatherSentiment Comprehensive Analysis of Tweet Sentiments and Weather Data A comprehensive suite of functions for processing,\n  analyzing, and visualizing textual data from tweets is offered.  \n  Users can clean tweets, analyze their sentiments, visualize data, \n  and examine the correlation between sentiments and environmental \n  data such as weather conditions. Main features include text processing,  \n  sentiment analysis, data visualization, correlation analysis, and \n  synthetic data generation. Text processing involves cleaning and preparing  \n  tweets by removing textual noise and irrelevant words. Sentiment analysis \n  extracts and accurately analyzes sentiments from tweet texts using advanced \n  algorithms. Data visualization creates various charts like word clouds\n  and sentiment polarity graphs for visual representation of data. Correlation \n  analysis examines and calculates the correlation between tweet sentiments \n  and environmental variables such as weather conditions. Additionally, \n  random tweets can be generated for testing and evaluating the performance \n  of analyses, empowering users to effectively analyze and interpret 'Twitter' \n  data for research and commercial purposes.  "
  },
  {
    "id": 8109,
    "package_name": "Xplortext",
    "title": "Statistical Analysis of Textual Data",
    "description": "Provides a set of functions devoted to multivariate exploratory statistics on textual data. Classical methods such as correspondence analysis and agglomerative hierarchical clustering are available. Chronologically constrained agglomerative hierarchical clustering enriched with labelled-by-words trees is offered. Given a division of the corpus into parts, their characteristic words and documents are identified. Further, accessing to 'FactoMineR' functions is very easy. Two of them are relevant in textual domain. MFA() addresses multiple lexical table allowing applications such as dealing with multilingual corpora as well as simultaneously analyzing both open-ended and closed questions in surveys. See <http://xplortext.unileon.es> for examples.",
    "version": "1.5.5",
    "maintainer": "Ram\u00f3n Alvarez-Esteban <ramon.alvarez@unileon.es>",
    "author": "Ram\u00f3n Alvarez-Esteban [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4751-2797>),\n  M\u00f3nica B\u00e9cue-Bertaut [aut] (ORCID:\n    <https://orcid.org/0000-0002-6027-3655>),\n  Josep-Anton S\u00e1nchez-Espigares [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8195-1913>),\n  Belchin Adriyanov Kostov [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2126-3892>)",
    "url": "https://xplortext.unileon.es",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Xplortext",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Xplortext Statistical Analysis of Textual Data Provides a set of functions devoted to multivariate exploratory statistics on textual data. Classical methods such as correspondence analysis and agglomerative hierarchical clustering are available. Chronologically constrained agglomerative hierarchical clustering enriched with labelled-by-words trees is offered. Given a division of the corpus into parts, their characteristic words and documents are identified. Further, accessing to 'FactoMineR' functions is very easy. Two of them are relevant in textual domain. MFA() addresses multiple lexical table allowing applications such as dealing with multilingual corpora as well as simultaneously analyzing both open-ended and closed questions in surveys. See <http://xplortext.unileon.es> for examples.  "
  },
  {
    "id": 8221,
    "package_name": "act",
    "title": "Aligned Corpus Toolkit",
    "description": "The Aligned Corpus Toolkit (act) is designed for linguists that work with time aligned transcription data. It offers functions to import and export various annotation file formats ('ELAN' .eaf, 'EXMARaLDA .exb and 'Praat' .TextGrid files), create print transcripts in the style of conversation analysis, search transcripts (span searches across multiple annotations, search in normalized annotations, make concordances etc.), export and re-import search results (.csv and 'Excel' .xlsx format), create cuts for the search results (print transcripts, audio/video cuts using 'FFmpeg' and video sub titles in 'Subrib title' .srt format), modify the data in a corpus (search/replace, delete, filter etc.), interact with 'Praat' using 'Praat'-scripts, and exchange data with the 'rPraat' package. The package is itself written in R and may be expanded by other users.",
    "version": "1.3.1",
    "maintainer": "Oliver Ehmer <oliver.ehmer@uos.de>",
    "author": "Oliver Ehmer [aut, cre]",
    "url": "http://www.oliverehmer.de, https://github.com/oliverehmer/act",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=act",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "act Aligned Corpus Toolkit The Aligned Corpus Toolkit (act) is designed for linguists that work with time aligned transcription data. It offers functions to import and export various annotation file formats ('ELAN' .eaf, 'EXMARaLDA .exb and 'Praat' .TextGrid files), create print transcripts in the style of conversation analysis, search transcripts (span searches across multiple annotations, search in normalized annotations, make concordances etc.), export and re-import search results (.csv and 'Excel' .xlsx format), create cuts for the search results (print transcripts, audio/video cuts using 'FFmpeg' and video sub titles in 'Subrib title' .srt format), modify the data in a corpus (search/replace, delete, filter etc.), interact with 'Praat' using 'Praat'-scripts, and exchange data with the 'rPraat' package. The package is itself written in R and may be expanded by other users.  "
  },
  {
    "id": 8390,
    "package_name": "aifeducation",
    "title": "Artificial Intelligence for Education",
    "description": "In social and educational settings, the use of Artificial\n    Intelligence (AI) is a challenging task. Relevant data is often only\n    available in handwritten forms, or the use of data is restricted by\n    privacy policies. This often leads to small data sets. Furthermore, in\n    the educational and social sciences, data is often unbalanced in terms\n    of frequencies. To support educators as well as educational and social\n    researchers in using the potentials of AI for their work, this package\n    provides a unified interface for neural nets in 'PyTorch' to deal with\n    natural language problems. In addition, the package ships with a shiny\n    app, providing a graphical user interface.  This allows the usage of\n    AI for people without skills in writing python/R scripts.  The tools\n    integrate existing mathematical and statistical methods for dealing\n    with small data sets via pseudo-labeling (e.g. Cascante-Bonilla et al.\n    (2020) <doi:10.48550/arXiv.2001.06001>) and imbalanced data via the\n    creation of synthetic cases (e.g.  Islam et al. (2012)\n    <doi:10.1016/j.asoc.2021.108288>).  Performance evaluation of AI is\n    connected to measures from content analysis which educational and\n    social researchers are generally more familiar with (e.g. Berding &\n    Pargmann (2022) <doi:10.30819/5581>, Gwet (2014)\n    <ISBN:978-0-9708062-8-4>, Krippendorff (2019)\n    <doi:10.4135/9781071878781>). Estimation of energy consumption and CO2\n    emissions during model training is done with the 'python' library\n    'codecarbon'.  Finally, all objects created with this package allow to\n    share trained AI models with other people.",
    "version": "1.1.3",
    "maintainer": "Berding Florian <florian.berding@uni-hamburg.de>",
    "author": "Berding Florian [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3593-1695>),\n  Tykhonova Yuliia [aut] (ORCID: <https://orcid.org/0009-0006-9015-1006>),\n  Pargmann Julia [ctb] (ORCID: <https://orcid.org/0000-0003-3616-0172>),\n  Leube Anna [ctb] (ORCID: <https://orcid.org/0009-0001-6949-1608>),\n  Riebenbauer Elisabeth [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8535-3694>),\n  Rebmann Karin [ctb],\n  Slopinski Andreas [ctb]",
    "url": "https://fberding.github.io/aifeducation/",
    "bug_reports": "https://github.com/FBerding/aifeducation/issues",
    "repository": "https://cran.r-project.org/package=aifeducation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aifeducation Artificial Intelligence for Education In social and educational settings, the use of Artificial\n    Intelligence (AI) is a challenging task. Relevant data is often only\n    available in handwritten forms, or the use of data is restricted by\n    privacy policies. This often leads to small data sets. Furthermore, in\n    the educational and social sciences, data is often unbalanced in terms\n    of frequencies. To support educators as well as educational and social\n    researchers in using the potentials of AI for their work, this package\n    provides a unified interface for neural nets in 'PyTorch' to deal with\n    natural language problems. In addition, the package ships with a shiny\n    app, providing a graphical user interface.  This allows the usage of\n    AI for people without skills in writing python/R scripts.  The tools\n    integrate existing mathematical and statistical methods for dealing\n    with small data sets via pseudo-labeling (e.g. Cascante-Bonilla et al.\n    (2020) <doi:10.48550/arXiv.2001.06001>) and imbalanced data via the\n    creation of synthetic cases (e.g.  Islam et al. (2012)\n    <doi:10.1016/j.asoc.2021.108288>).  Performance evaluation of AI is\n    connected to measures from content analysis which educational and\n    social researchers are generally more familiar with (e.g. Berding &\n    Pargmann (2022) <doi:10.30819/5581>, Gwet (2014)\n    <ISBN:978-0-9708062-8-4>, Krippendorff (2019)\n    <doi:10.4135/9781071878781>). Estimation of energy consumption and CO2\n    emissions during model training is done with the 'python' library\n    'codecarbon'.  Finally, all objects created with this package allow to\n    share trained AI models with other people.  "
  },
  {
    "id": 8586,
    "package_name": "arabicStemR",
    "title": "Arabic Stemmer for Text Analysis",
    "description": "Allows users to stem Arabic texts for text analysis.",
    "version": "1.3",
    "maintainer": "Rich Nielsen <rnielsen@mit.edu>",
    "author": "Rich Nielsen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=arabicStemR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arabicStemR Arabic Stemmer for Text Analysis Allows users to stem Arabic texts for text analysis.  "
  },
  {
    "id": 8787,
    "package_name": "aws.comprehend",
    "title": "Client for 'AWS Comprehend'",
    "description": "Client for 'AWS Comprehend' <https://aws.amazon.com/comprehend>, a cloud natural language processing service that can perform a number of quantitative text analyses, including language detection, sentiment analysis, and feature extraction.",
    "version": "0.2.1",
    "maintainer": "Antoine Sachet <antoine.sac@gmail.com>",
    "author": "Thomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\n  Antoine Sachet [aut, cre],\n  Dave Kincaid [ctb]",
    "url": "https://github.com/cloudyr/aws.comprehend",
    "bug_reports": "https://github.com/cloudyr/aws.comprehend/issues",
    "repository": "https://cran.r-project.org/package=aws.comprehend",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aws.comprehend Client for 'AWS Comprehend' Client for 'AWS Comprehend' <https://aws.amazon.com/comprehend>, a cloud natural language processing service that can perform a number of quantitative text analyses, including language detection, sentiment analysis, and feature extraction.  "
  },
  {
    "id": 9401,
    "package_name": "brassica",
    "title": "1970s BASIC Interpreter",
    "description": "Executes BASIC programs from the 1970s, for historical and\n    educational purposes. This enables famous examples of early machine\n    learning, artificial intelligence, natural language processing, cellular\n    automata, and so on, to be run in their original form.",
    "version": "1.0.2",
    "maintainer": "Mike Lee <random.deviate@gmail.com>",
    "author": "Mike Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=brassica",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brassica 1970s BASIC Interpreter Executes BASIC programs from the 1970s, for historical and\n    educational purposes. This enables famous examples of early machine\n    learning, artificial intelligence, natural language processing, cellular\n    automata, and so on, to be run in their original form.  "
  },
  {
    "id": 9957,
    "package_name": "cleanNLP",
    "title": "A Tidy Data Model for Natural Language Processing",
    "description": "Provides a set of fast tools for converting a textual corpus into\n  a set of normalized tables. Users may make use of the 'udpipe' back end with\n  no external dependencies, or a Python back ends with 'spaCy' <https://spacy.io>.\n  Exposed annotation tasks include tokenization, part of speech tagging, named\n  entity recognition, and dependency parsing.",
    "version": "3.1.0",
    "maintainer": "Taylor B. Arnold <tarnold2@richmond.edu>",
    "author": "Taylor B. Arnold [aut, cre]",
    "url": "https://statsmaths.github.io/cleanNLP/",
    "bug_reports": "https://github.com/statsmaths/cleanNLP/issues",
    "repository": "https://cran.r-project.org/package=cleanNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cleanNLP A Tidy Data Model for Natural Language Processing Provides a set of fast tools for converting a textual corpus into\n  a set of normalized tables. Users may make use of the 'udpipe' back end with\n  no external dependencies, or a Python back ends with 'spaCy' <https://spacy.io>.\n  Exposed annotation tasks include tokenization, part of speech tagging, named\n  entity recognition, and dependency parsing.  "
  },
  {
    "id": 9996,
    "package_name": "clinspacy",
    "title": "Clinical Natural Language Processing using 'spaCy', 'scispaCy',\nand 'medspaCy'",
    "description": "Performs biomedical named entity recognition,\n    Unified Medical Language System (UMLS) concept mapping, and negation\n    detection using the Python 'spaCy', 'scispaCy', and 'medspaCy' packages, and \n    transforms extracted data into a wide format for inclusion in machine\n    learning models. The development of the 'scispaCy' package is described by\n    Neumann (2019) <doi:10.18653/v1/W19-5034>. The 'medspacy' package uses\n    'ConText', an algorithm for determining the context of clinical statements\n    described by Harkema (2009) <doi:10.1016/j.jbi.2009.05.002>. Clinspacy\n    also supports entity embeddings from 'scispaCy' and UMLS 'cui2vec' concept\n    embeddings developed by Beam (2018) <arXiv:1804.01486>.",
    "version": "1.0.2",
    "maintainer": "Karandeep Singh <kdpsingh@umich.edu>",
    "author": "Karandeep Singh [aut, cre],\n  Benjamin Kompa [aut],\n  Andrew Beam [aut],\n  Allen Schmaltz [aut]",
    "url": "https://github.com/ML4LHS/clinspacy",
    "bug_reports": "https://github.com/ML4LHS/clinspacy/issues",
    "repository": "https://cran.r-project.org/package=clinspacy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinspacy Clinical Natural Language Processing using 'spaCy', 'scispaCy',\nand 'medspaCy' Performs biomedical named entity recognition,\n    Unified Medical Language System (UMLS) concept mapping, and negation\n    detection using the Python 'spaCy', 'scispaCy', and 'medspaCy' packages, and \n    transforms extracted data into a wide format for inclusion in machine\n    learning models. The development of the 'scispaCy' package is described by\n    Neumann (2019) <doi:10.18653/v1/W19-5034>. The 'medspacy' package uses\n    'ConText', an algorithm for determining the context of clinical statements\n    described by Harkema (2009) <doi:10.1016/j.jbi.2009.05.002>. Clinspacy\n    also supports entity embeddings from 'scispaCy' and UMLS 'cui2vec' concept\n    embeddings developed by Beam (2018) <arXiv:1804.01486>.  "
  },
  {
    "id": 10129,
    "package_name": "codecountR",
    "title": "Counting Codes in a Text and Preparing Data for Analysis",
    "description": "Data analysis often requires coding, especially when data are collected through interviews, observations, or questionnaires. As a result, code counting and data preparation are essential steps in the analysis process. Analysts may need to count the codes in a text (Tokenization, counting of pre-established codes, computing the co-occurrence matrix by line) and prepare the data (e.g., min-max normalization, Z-score, robust scaling, Box-Cox transformation, and non-parametric bootstrap). For the Box-Cox transformation (Box & Cox, 1964, <https://www.jstor.org/stable/2984418>), the optimal Lambda is determined using the log-likelihood method. Non-parametric bootstrap involves randomly sampling data with replacement. Two random number generators are also integrated: a Lehmer congruential generator for uniform distribution and a Box-Muller generator for normal distribution. Package for educational purposes. ",
    "version": "0.0.4.8",
    "maintainer": "Philippe Cohard <p.cohard@laposte.net>",
    "author": "Philippe Cohard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=codecountR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "codecountR Counting Codes in a Text and Preparing Data for Analysis Data analysis often requires coding, especially when data are collected through interviews, observations, or questionnaires. As a result, code counting and data preparation are essential steps in the analysis process. Analysts may need to count the codes in a text (Tokenization, counting of pre-established codes, computing the co-occurrence matrix by line) and prepare the data (e.g., min-max normalization, Z-score, robust scaling, Box-Cox transformation, and non-parametric bootstrap). For the Box-Cox transformation (Box & Cox, 1964, <https://www.jstor.org/stable/2984418>), the optimal Lambda is determined using the log-likelihood method. Non-parametric bootstrap involves randomly sampling data with replacement. Two random number generators are also integrated: a Lehmer congruential generator for uniform distribution and a Box-Muller generator for normal distribution. Package for educational purposes.   "
  },
  {
    "id": 10366,
    "package_name": "conversim",
    "title": "Conversation Similarity Analysis",
    "description": "Analyze and compare conversations using various similarity measures\n    including topic, lexical, semantic, structural, stylistic, sentiment,\n    participant, and timing similarities. Supports both pairwise conversation\n    comparisons and analysis of multiple dyads. Methods are based on established research:\n    Topic modeling: Blei et al. (2003) <doi:10.1162/jmlr.2003.3.4-5.993>;\n    Landauer et al. (1998) <doi:10.1080/01638539809545028>;\n    Lexical similarity: Jaccard (1912) <doi:10.1111/j.1469-8137.1912.tb05611.x>;\n    Semantic similarity: Salton & Buckley (1988) <doi:10.1016/0306-4573(88)90021-0>;\n    Mikolov et al. (2013) <doi:10.48550/arXiv.1301.3781>;\n    Pennington et al. (2014) <doi:10.3115/v1/D14-1162>;\n    Structural and stylistic analysis: Graesser et al. (2004) <doi:10.1075/target.21131.ryu>;\n    Sentiment analysis: Rinker (2019) <https://github.com/trinker/sentimentr>.",
    "version": "0.1.0",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre, cph]",
    "url": "https://github.com/chaoliu-cl/conversim,\nhttp://liu-chao.site/conversim/",
    "bug_reports": "https://github.com/chaoliu-cl/conversim/issues",
    "repository": "https://cran.r-project.org/package=conversim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conversim Conversation Similarity Analysis Analyze and compare conversations using various similarity measures\n    including topic, lexical, semantic, structural, stylistic, sentiment,\n    participant, and timing similarities. Supports both pairwise conversation\n    comparisons and analysis of multiple dyads. Methods are based on established research:\n    Topic modeling: Blei et al. (2003) <doi:10.1162/jmlr.2003.3.4-5.993>;\n    Landauer et al. (1998) <doi:10.1080/01638539809545028>;\n    Lexical similarity: Jaccard (1912) <doi:10.1111/j.1469-8137.1912.tb05611.x>;\n    Semantic similarity: Salton & Buckley (1988) <doi:10.1016/0306-4573(88)90021-0>;\n    Mikolov et al. (2013) <doi:10.48550/arXiv.1301.3781>;\n    Pennington et al. (2014) <doi:10.3115/v1/D14-1162>;\n    Structural and stylistic analysis: Graesser et al. (2004) <doi:10.1075/target.21131.ryu>;\n    Sentiment analysis: Rinker (2019) <https://github.com/trinker/sentimentr>.  "
  },
  {
    "id": 10400,
    "package_name": "coreNLP",
    "title": "Wrappers Around Stanford CoreNLP Tools",
    "description": "Provides a minimal interface for applying\n    annotators from the 'Stanford CoreNLP' java library. Methods\n    are provided for tasks such as tokenisation, part of speech\n    tagging, lemmatisation, named entity recognition, coreference\n    detection and sentiment analysis.",
    "version": "0.4-3",
    "maintainer": "Taylor Arnold <tarnold2@richmond.edu>",
    "author": "Taylor Arnold, Lauren Tilton",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=coreNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "coreNLP Wrappers Around Stanford CoreNLP Tools Provides a minimal interface for applying\n    annotators from the 'Stanford CoreNLP' java library. Methods\n    are provided for tasks such as tokenisation, part of speech\n    tagging, lemmatisation, named entity recognition, coreference\n    detection and sentiment analysis.  "
  },
  {
    "id": 10415,
    "package_name": "corpustools",
    "title": "Managing, Querying and Analyzing Tokenized Text",
    "description": "Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).\n    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),\n    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,\n    and the possibility to reconstruct original text from tokens to facilitate interpretation.",
    "version": "0.5.2",
    "maintainer": "Kasper Welbers <kasperwelbers@gmail.com>",
    "author": "Kasper Welbers [aut, cre],\n  Wouter van Atteveldt [aut]",
    "url": "https://github.com/kasperwelbers/corpustools",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corpustools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corpustools Managing, Querying and Analyzing Tokenized Text Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).\n    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),\n    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,\n    and the possibility to reconstruct original text from tokens to facilitate interpretation.  "
  },
  {
    "id": 10620,
    "package_name": "cryptoQuotes",
    "title": "Open Access to Cryptocurrency Market Data, Sentiment Indicators\nand Interactive Charts",
    "description": "\n  This high-level API client provides open access to cryptocurrency market data, sentiment indicators, and interactive charting tools. \n  The data is sourced from major cryptocurrency exchanges via 'curl' and returned in 'xts'-format. The data comes in open, high, low, and close (OHLC) format with flexible granularity, ranging from seconds to months. \n  This flexibility makes it ideal for developing and backtesting trading strategies or conducting detailed market analysis.",
    "version": "1.3.3",
    "maintainer": "Serkan Korkmaz <serkor1@duck.com>",
    "author": "Serkan Korkmaz [cre, aut, ctb, cph] (ORCID:\n    <https://orcid.org/0000-0002-5052-0982>),\n  Jonas Cuzulan Hirani [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9512-1993>)",
    "url": "https://serkor1.github.io/cryptoQuotes/,\nhttps://github.com/serkor1/cryptoQuotes",
    "bug_reports": "https://github.com/serkor1/cryptoQuotes/issues",
    "repository": "https://cran.r-project.org/package=cryptoQuotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cryptoQuotes Open Access to Cryptocurrency Market Data, Sentiment Indicators\nand Interactive Charts \n  This high-level API client provides open access to cryptocurrency market data, sentiment indicators, and interactive charting tools. \n  The data is sourced from major cryptocurrency exchanges via 'curl' and returned in 'xts'-format. The data comes in open, high, low, and close (OHLC) format with flexible granularity, ranging from seconds to months. \n  This flexibility makes it ideal for developing and backtesting trading strategies or conducting detailed market analysis.  "
  },
  {
    "id": 10687,
    "package_name": "cucumber",
    "title": "Behavior-Driven Development for R",
    "description": "Write executable specifications in a natural language that describes how your code should behave.\n    Write specifications in feature files using 'Gherkin' language and execute them using functions implemented in R.\n    Use them as an extension to your 'testthat' tests to provide a high level description of how your code works.",
    "version": "2.1.1",
    "maintainer": "Jakub Sobolewski <jakupsob@gmail.com>",
    "author": "Jakub Sobolewski [aut, cre]",
    "url": "https://github.com/jakubsob/cucumber,\nhttps://jakubsobolewski.com/cucumber/",
    "bug_reports": "https://github.com/jakubsob/cucumber/issues",
    "repository": "https://cran.r-project.org/package=cucumber",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cucumber Behavior-Driven Development for R Write executable specifications in a natural language that describes how your code should behave.\n    Write specifications in feature files using 'Gherkin' language and execute them using functions implemented in R.\n    Use them as an extension to your 'testthat' tests to provide a high level description of how your code works.  "
  },
  {
    "id": 11195,
    "package_name": "discoverableresearch",
    "title": "Checks Title, Abstract and Keywords to Optimise Discoverability",
    "description": "A suite of tools are provided here to support authors in making their research more discoverable. \n    check_keywords() - this function checks the keywords to assess whether they are already represented in the \n    title and abstract. check_fields() - this function compares terminology used across the title, abstract \n    and keywords to assess where terminological diversity (i.e. the use of synonyms) could increase the likelihood \n    of the record being identified in a search. The function looks for terms in the title and abstract that also \n    exist in other fields and highlights these as needing attention. suggest_keywords() - this function takes a \n    full text document and produces a list of unigrams, bigrams and trigrams (1-, 2- or 2-word phrases) \n    present in the full text after removing stop words (words with a low utility in natural language processing) \n    that do not occur in the title or abstract that may be suitable candidates for keywords. suggest_title() - \n    this function takes a full text document and produces a list of the most frequently used unigrams, bigrams \n    and trigrams after removing stop words that do not occur in the abstract or keywords that may be suitable \n    candidates for title words. check_title() - this function carries out a number of sub tasks:  1) it compares \n    the length (number of words) of the title with the mean length of titles in major bibliographic databases to \n    assess whether the title is likely to be too short; 2) it assesses the proportion of stop words in the title \n    to highlight titles with low utility in search engines that strip out stop words; 3) it compares the title \n    with a given sample of record titles from an .ris import and calculates a similarity score based on phrase \n    overlap. This highlights the level of uniqueness of the title. This version of the package also contains \n    functions currently in a non-CRAN package called 'litsearchr' <https://github.com/elizagrames/litsearchr>.",
    "version": "0.0.1",
    "maintainer": "Neal Haddaway <nealhaddaway@gmail.com>",
    "author": "Neal Haddaway [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3902-2234>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=discoverableresearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "discoverableresearch Checks Title, Abstract and Keywords to Optimise Discoverability A suite of tools are provided here to support authors in making their research more discoverable. \n    check_keywords() - this function checks the keywords to assess whether they are already represented in the \n    title and abstract. check_fields() - this function compares terminology used across the title, abstract \n    and keywords to assess where terminological diversity (i.e. the use of synonyms) could increase the likelihood \n    of the record being identified in a search. The function looks for terms in the title and abstract that also \n    exist in other fields and highlights these as needing attention. suggest_keywords() - this function takes a \n    full text document and produces a list of unigrams, bigrams and trigrams (1-, 2- or 2-word phrases) \n    present in the full text after removing stop words (words with a low utility in natural language processing) \n    that do not occur in the title or abstract that may be suitable candidates for keywords. suggest_title() - \n    this function takes a full text document and produces a list of the most frequently used unigrams, bigrams \n    and trigrams after removing stop words that do not occur in the abstract or keywords that may be suitable \n    candidates for title words. check_title() - this function carries out a number of sub tasks:  1) it compares \n    the length (number of words) of the title with the mean length of titles in major bibliographic databases to \n    assess whether the title is likely to be too short; 2) it assesses the proportion of stop words in the title \n    to highlight titles with low utility in search engines that strip out stop words; 3) it compares the title \n    with a given sample of record titles from an .ris import and calculates a similarity score based on phrase \n    overlap. This highlights the level of uniqueness of the title. This version of the package also contains \n    functions currently in a non-CRAN package called 'litsearchr' <https://github.com/elizagrames/litsearchr>.  "
  },
  {
    "id": 11300,
    "package_name": "doc2concrete",
    "title": "Measuring Concreteness in Natural Language",
    "description": "Models for detecting concreteness in natural language. This package is built in support of Yeomans (2021) <doi:10.1016/j.obhdp.2020.10.008>, which reviews linguistic models of concreteness in several domains. Here, we provide an implementation of the best-performing domain-general model (from Brysbaert et al., (2014) <doi:10.3758/s13428-013-0403-5>) as well as two pre-trained models for the feedback and plan-making domains.",
    "version": "0.6.0",
    "maintainer": "Mike Yeomans <mk.yeomans@gmail.com>",
    "author": "Mike Yeomans",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=doc2concrete",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doc2concrete Measuring Concreteness in Natural Language Models for detecting concreteness in natural language. This package is built in support of Yeomans (2021) <doi:10.1016/j.obhdp.2020.10.008>, which reviews linguistic models of concreteness in several domains. Here, we provide an implementation of the best-performing domain-general model (from Brysbaert et al., (2014) <doi:10.3758/s13428-013-0403-5>) as well as two pre-trained models for the feedback and plan-making domains.  "
  },
  {
    "id": 11661,
    "package_name": "edgar",
    "title": "Tool for the U.S. SEC EDGAR Retrieval and Parsing of Corporate\nFilings",
    "description": "In the USA, companies file different forms with the U.S. \n    Securities and Exchange Commission (SEC) through EDGAR (Electronic \n    Data Gathering, Analysis, and Retrieval system). The EDGAR \n    database automated system collects all the different necessary \n    filings and makes it publicly available. This package facilitates\n    retrieving, storing, searching, and parsing of all the available \n    filings on the EDGAR server. It downloads filings from SEC \n    server in bulk with a single query. Additionally, it provides \n    various useful functions: extracts 8-K triggering events, extract\n    \"Business (Item 1)\" and \"Management's Discussion and Analysis(Item 7)\"\n    sections of annual statements, searches filings for desired \n    keywords, provides sentiment measures, parses filing header \n    information, and provides HTML view of SEC filings.  ",
    "version": "2.0.8",
    "maintainer": "Gunratan Lonare <lonare.gunratan@gmail.com>",
    "author": "Gunratan Lonare [aut, cre],\n  Bharat Patil [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=edgar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edgar Tool for the U.S. SEC EDGAR Retrieval and Parsing of Corporate\nFilings In the USA, companies file different forms with the U.S. \n    Securities and Exchange Commission (SEC) through EDGAR (Electronic \n    Data Gathering, Analysis, and Retrieval system). The EDGAR \n    database automated system collects all the different necessary \n    filings and makes it publicly available. This package facilitates\n    retrieving, storing, searching, and parsing of all the available \n    filings on the EDGAR server. It downloads filings from SEC \n    server in bulk with a single query. Additionally, it provides \n    various useful functions: extracts 8-K triggering events, extract\n    \"Business (Item 1)\" and \"Management's Discussion and Analysis(Item 7)\"\n    sections of annual statements, searches filings for desired \n    keywords, provides sentiment measures, parses filing header \n    information, and provides HTML view of SEC filings.    "
  },
  {
    "id": 11872,
    "package_name": "epos",
    "title": "Epilepsy Ontologies' Similarities",
    "description": "Analysis and visualization of similarities between epilepsy ontologies based on text mining results by comparing ranked lists of co-occurring drug terms in the BioASQ corpus. The ranked result lists of neurological drug terms co-occurring with terms from the epilepsy ontologies EpSO, ESSO, EPILONT, EPISEM and FENICS undergo further analysis. The source data to create the ranked lists of drug names is produced using the text mining workflows described in Mueller, Bernd and Hagelstein, Alexandra (2016) <doi:10.4126/FRL01-006408558>, Mueller, Bernd et al. (2017) <doi:10.1007/978-3-319-58694-6_22>, Mueller, Bernd and Rebholz-Schuhmann, Dietrich (2020) <doi:10.1007/978-3-030-43887-6_52>, and Mueller, Bernd et al. (2022) <doi:10.1186/s13326-021-00258-w>.",
    "version": "1.2",
    "maintainer": "Bernd Mueller <bernd.mueller@zbmed.de>",
    "author": "Bernd Mueller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3062-8192>)",
    "url": "https://github.com/bernd-mueller/epos",
    "bug_reports": "https://github.com/bernd-mueller/epos/issues",
    "repository": "https://cran.r-project.org/package=epos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epos Epilepsy Ontologies' Similarities Analysis and visualization of similarities between epilepsy ontologies based on text mining results by comparing ranked lists of co-occurring drug terms in the BioASQ corpus. The ranked result lists of neurological drug terms co-occurring with terms from the epilepsy ontologies EpSO, ESSO, EPILONT, EPISEM and FENICS undergo further analysis. The source data to create the ranked lists of drug names is produced using the text mining workflows described in Mueller, Bernd and Hagelstein, Alexandra (2016) <doi:10.4126/FRL01-006408558>, Mueller, Bernd et al. (2017) <doi:10.1007/978-3-319-58694-6_22>, Mueller, Bernd and Rebholz-Schuhmann, Dietrich (2020) <doi:10.1007/978-3-030-43887-6_52>, and Mueller, Bernd et al. (2022) <doi:10.1186/s13326-021-00258-w>.  "
  },
  {
    "id": 12708,
    "package_name": "friends",
    "title": "The Entire Transcript from Friends in Tidy Format",
    "description": "The complete scripts from the American sitcom Friends in tibble \n    format. Use this package to practice data wrangling, text analysis and \n    network analysis.",
    "version": "0.1.0",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0679-1945>)",
    "url": "https://github.com/EmilHvitfeldt/friends",
    "bug_reports": "https://github.com/EmilHvitfeldt/friends/issues",
    "repository": "https://cran.r-project.org/package=friends",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "friends The Entire Transcript from Friends in Tidy Format The complete scripts from the American sitcom Friends in tibble \n    format. Use this package to practice data wrangling, text analysis and \n    network analysis.  "
  },
  {
    "id": 13132,
    "package_name": "getwiki",
    "title": "R Wrapper for Wikipedia Data",
    "description": "A simple wrapper for 'Wikipedia' data. Specifically, this \n    package looks to fill a gap in retrieving text data in a tidy format that can be used for Natural Language Processing. ",
    "version": "0.9.0",
    "maintainer": "Corydon Baylor <cwbaylor1@gmail.com>",
    "author": "Corydon Baylor [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=getwiki",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "getwiki R Wrapper for Wikipedia Data A simple wrapper for 'Wikipedia' data. Specifically, this \n    package looks to fill a gap in retrieving text data in a tidy format that can be used for Natural Language Processing.   "
  },
  {
    "id": 13359,
    "package_name": "ggx",
    "title": "A Natural Language Interface to 'ggplot2'",
    "description": "The 'ggplot2' package is the state-of-the-art toolbox for creating and formatting graphs. However, it is easy to forget how certain formatting commands are named and sometimes users find themselves asking: How do you rotate the x-axis labels again? Or how do you hide the legend...? This package allows users to issue natural language commands related to theme-related styling of plots (colors, font size and such), which then are translated into valid 'ggplot2' commands.",
    "version": "0.1.1",
    "maintainer": "Andreas M. Brandmaier <andy@brandmaier.de>",
    "author": "Andreas M. Brandmaier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggx A Natural Language Interface to 'ggplot2' The 'ggplot2' package is the state-of-the-art toolbox for creating and formatting graphs. However, it is easy to forget how certain formatting commands are named and sometimes users find themselves asking: How do you rotate the x-axis labels again? Or how do you hide the legend...? This package allows users to issue natural language commands related to theme-related styling of plots (colors, font size and such), which then are translated into valid 'ggplot2' commands.  "
  },
  {
    "id": 13366,
    "package_name": "gibasa",
    "title": "An Alternative 'Rcpp' Wrapper of 'MeCab'",
    "description": "A plain 'Rcpp' wrapper for 'MeCab' that can segment Chinese,\n    Japanese, and Korean text into tokens. The main goal of this package\n    is to provide an alternative to 'tidytext' using morphological\n    analysis.",
    "version": "1.1.2",
    "maintainer": "Akiru Kato <paithiov909@gmail.com>",
    "author": "Akiru Kato [aut, cre],\n  Shogo Ichinose [aut],\n  Taku Kudo [aut],\n  Jorge Nocedal [ctb],\n  Nippon Telegraph and Telephone Corporation [cph]",
    "url": "https://paithiov909.github.io/gibasa/",
    "bug_reports": "https://github.com/paithiov909/gibasa/issues",
    "repository": "https://cran.r-project.org/package=gibasa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gibasa An Alternative 'Rcpp' Wrapper of 'MeCab' A plain 'Rcpp' wrapper for 'MeCab' that can segment Chinese,\n    Japanese, and Korean text into tokens. The main goal of this package\n    is to provide an alternative to 'tidytext' using morphological\n    analysis.  "
  },
  {
    "id": 13542,
    "package_name": "googlenlp",
    "title": "An Interface to Google's Cloud Natural Language API",
    "description": "Interact with Google's Cloud Natural Language API\n    <https://cloud.google.com/natural-language/> (v1) via R. The API has\n    four main features, all of which are available through this\n    R package: syntax analysis and part-of-speech tagging, entity\n    analysis, sentiment analysis, and language identification.",
    "version": "0.2.0",
    "maintainer": "Brian Weinstien <bweinstein02@gmail.com>",
    "author": "Brian Weinstien [aut, cre]",
    "url": "https://github.com/BrianWeinstein/googlenlp",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=googlenlp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "googlenlp An Interface to Google's Cloud Natural Language API Interact with Google's Cloud Natural Language API\n    <https://cloud.google.com/natural-language/> (v1) via R. The API has\n    four main features, all of which are available through this\n    R package: syntax analysis and part-of-speech tagging, entity\n    analysis, sentiment analysis, and language identification.  "
  },
  {
    "id": 13574,
    "package_name": "gptr",
    "title": "A Convenient R Interface with the OpenAI 'ChatGPT' API",
    "description": "A convenient interface with the OpenAI 'ChatGPT' API <https://openai.com/api>. 'gptr' allows you to interact with 'ChatGPT', a powerful language model, for various natural language processing tasks. The 'gptr' R package makes talking to 'ChatGPT' in R super easy. It helps researchers and data folks by simplifying the complicated stuff, like asking questions and getting answers. With 'gptr', you can use 'ChatGPT' in R without any hassle, making it simpler for everyone to do cool things with language!",
    "version": "0.7.0",
    "maintainer": "Wanjun Gu <wanjun.gu@ucsf.edu>",
    "author": "Wanjun Gu [aut, cre] (ORCID: <https://orcid.org/0000-0002-7342-7000>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gptr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gptr A Convenient R Interface with the OpenAI 'ChatGPT' API A convenient interface with the OpenAI 'ChatGPT' API <https://openai.com/api>. 'gptr' allows you to interact with 'ChatGPT', a powerful language model, for various natural language processing tasks. The 'gptr' R package makes talking to 'ChatGPT' in R super easy. It helps researchers and data folks by simplifying the complicated stuff, like asking questions and getting answers. With 'gptr', you can use 'ChatGPT' in R without any hassle, making it simpler for everyone to do cool things with language!  "
  },
  {
    "id": 13586,
    "package_name": "grafzahl",
    "title": "Supervised Machine Learning for Textual Data Using Transformers\nand 'Quanteda'",
    "description": "Duct tape the 'quanteda' ecosystem (Benoit et al., 2018) <doi:10.21105/joss.00774> to modern Transformer-based text classification models (Wolf et al., 2020) <doi:10.18653/v1/2020.emnlp-demos.6>, in order to facilitate supervised machine learning for textual data. This package mimics the behaviors of 'quanteda.textmodels' and provides a function to setup the 'Python' environment to use the pretrained models from 'Hugging Face' <https://huggingface.co/>. More information: <doi:10.5117/CCR2023.1.003.CHAN>.",
    "version": "0.0.12",
    "maintainer": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "author": "Chung-hong Chan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6232-7530>)",
    "url": "https://gesistsa.github.io/grafzahl/,\nhttps://github.com/gesistsa/grafzahl",
    "bug_reports": "https://github.com/gesistsa/grafzahl/issues",
    "repository": "https://cran.r-project.org/package=grafzahl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grafzahl Supervised Machine Learning for Textual Data Using Transformers\nand 'Quanteda' Duct tape the 'quanteda' ecosystem (Benoit et al., 2018) <doi:10.21105/joss.00774> to modern Transformer-based text classification models (Wolf et al., 2020) <doi:10.18653/v1/2020.emnlp-demos.6>, in order to facilitate supervised machine learning for textual data. This package mimics the behaviors of 'quanteda.textmodels' and provides a function to setup the 'Python' environment to use the pretrained models from 'Hugging Face' <https://huggingface.co/>. More information: <doi:10.5117/CCR2023.1.003.CHAN>.  "
  },
  {
    "id": 13833,
    "package_name": "hcandersenr",
    "title": "H.C. Andersens Fairy Tales",
    "description": "Texts for H.C. Andersens fairy tales, ready for\n    text analysis. Fairy tales in German, Danish, English, Spanish and\n    French.",
    "version": "0.2.0",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "author": "Emil Hvitfeldt [aut, cre]",
    "url": "https://github.com/EmilHvitfeldt/hcandersenr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hcandersenr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hcandersenr H.C. Andersens Fairy Tales Texts for H.C. Andersens fairy tales, ready for\n    text analysis. Fairy tales in German, Danish, English, Spanish and\n    French.  "
  },
  {
    "id": 14437,
    "package_name": "ingredients",
    "title": "Effects and Importances of Model Ingredients",
    "description": "Collection of tools for assessment of feature importance and feature effects.\n    Key functions are:\n    feature_importance() for assessment of global level feature importance,\n    ceteris_paribus() for calculation of the what-if plots,\n    partial_dependence() for partial dependence plots,\n    conditional_dependence() for conditional dependence plots,\n    accumulated_dependence() for accumulated local effects plots,\n    aggregate_profiles() and cluster_profiles() for aggregation of ceteris paribus profiles,\n    generic print() and plot() for better usability of selected explainers,\n    generic plotD3() for interactive, D3 based explanations, and\n    generic describe() for explanations in natural language.\n    The package 'ingredients' is a part of the 'DrWhy.AI' universe (Biecek 2018) <arXiv:1806.08915>.",
    "version": "2.3.0",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8423-1823>),\n  Hubert Baniecki [aut] (ORCID: <https://orcid.org/0000-0001-6661-5364>),\n  Adam Izdebski [ctb]",
    "url": "https://ModelOriented.github.io/ingredients/,\nhttps://github.com/ModelOriented/ingredients",
    "bug_reports": "https://github.com/ModelOriented/ingredients/issues",
    "repository": "https://cran.r-project.org/package=ingredients",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ingredients Effects and Importances of Model Ingredients Collection of tools for assessment of feature importance and feature effects.\n    Key functions are:\n    feature_importance() for assessment of global level feature importance,\n    ceteris_paribus() for calculation of the what-if plots,\n    partial_dependence() for partial dependence plots,\n    conditional_dependence() for conditional dependence plots,\n    accumulated_dependence() for accumulated local effects plots,\n    aggregate_profiles() and cluster_profiles() for aggregation of ceteris paribus profiles,\n    generic print() and plot() for better usability of selected explainers,\n    generic plotD3() for interactive, D3 based explanations, and\n    generic describe() for explanations in natural language.\n    The package 'ingredients' is a part of the 'DrWhy.AI' universe (Biecek 2018) <arXiv:1806.08915>.  "
  },
  {
    "id": 14450,
    "package_name": "inpdfr",
    "title": "Analyse Text Documents Using Ecological Tools",
    "description": "A set of functions to analyse and compare texts, using classical \n  text mining\tfunctions, as well as those from theoretical ecology.",
    "version": "0.1.12",
    "maintainer": "Rebaudo Francois <francois.rebaudo@ird.fr>",
    "author": "Rebaudo Francois (IRD, UMR EGCE, IRD, CNRS, Univ. ParisSaclay)",
    "url": "https://github.com/frareb/inpdfr/",
    "bug_reports": "https://github.com/frareb/inpdfr/issues",
    "repository": "https://cran.r-project.org/package=inpdfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inpdfr Analyse Text Documents Using Ecological Tools A set of functions to analyse and compare texts, using classical \n  text mining\tfunctions, as well as those from theoretical ecology.  "
  },
  {
    "id": 14693,
    "package_name": "janeaustenr",
    "title": "Jane Austen's Complete Novels",
    "description": "Full texts for Jane Austen's 6 completed novels, ready for\n    text analysis. These novels are \"Sense and Sensibility\", \"Pride and\n    Prejudice\", \"Mansfield Park\", \"Emma\", \"Northanger Abbey\", and\n    \"Persuasion\".",
    "version": "1.0.0",
    "maintainer": "Julia Silge <julia.silge@gmail.com>",
    "author": "Julia Silge [aut, cre] (ORCID: <https://orcid.org/0000-0002-3671-836X>)",
    "url": "https://github.com/juliasilge/janeaustenr",
    "bug_reports": "https://github.com/juliasilge/janeaustenr/issues",
    "repository": "https://cran.r-project.org/package=janeaustenr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "janeaustenr Jane Austen's Complete Novels Full texts for Jane Austen's 6 completed novels, ready for\n    text analysis. These novels are \"Sense and Sensibility\", \"Pride and\n    Prejudice\", \"Mansfield Park\", \"Emma\", \"Northanger Abbey\", and\n    \"Persuasion\".  "
  },
  {
    "id": 14767,
    "package_name": "js",
    "title": "Tools for Working with JavaScript in R",
    "description": "A set of utilities for working with JavaScript syntax in R.\n    Includes tools to parse, tokenize, compile, validate, reformat, optimize \n    and analyze JavaScript code.",
    "version": "1.2.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://jeroen.r-universe.dev/js",
    "bug_reports": "https://github.com/jeroen/js/issues",
    "repository": "https://cran.r-project.org/package=js",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "js Tools for Working with JavaScript in R A set of utilities for working with JavaScript syntax in R.\n    Includes tools to parse, tokenize, compile, validate, reformat, optimize \n    and analyze JavaScript code.  "
  },
  {
    "id": 14855,
    "package_name": "keyATM",
    "title": "Keyword Assisted Topic Models",
    "description": "Fits keyword assisted topic models (keyATM) using collapsed Gibbs samplers. The keyATM combines the latent dirichlet allocation (LDA) models with a small number of keywords selected by researchers in order to improve the interpretability and topic classification of the LDA. The keyATM can also incorporate covariates and directly model time trends. The keyATM is proposed in Eshima, Imai, and Sasaki (2024) <doi:10.1111/ajps.12779>.",
    "version": "0.5.4",
    "maintainer": "Shusei Eshima <shuseieshima@gmail.com>",
    "author": "Shusei Eshima [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3613-4046>),\n  Tomoya Sasaki [aut],\n  Kosuke Imai [aut],\n  Chung-hong Chan [ctb] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Romain Fran\u00e7ois [ctb] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Martin Feldkircher [ctb] (ORCID:\n    <https://orcid.org/0000-0002-5511-9215>),\n  William Lowe [ctb],\n  Seo-young Silvia Kim [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8801-9210>)",
    "url": "https://keyatm.github.io/keyATM/",
    "bug_reports": "https://github.com/keyATM/keyATM/issues",
    "repository": "https://cran.r-project.org/package=keyATM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "keyATM Keyword Assisted Topic Models Fits keyword assisted topic models (keyATM) using collapsed Gibbs samplers. The keyATM combines the latent dirichlet allocation (LDA) models with a small number of keywords selected by researchers in order to improve the interpretability and topic classification of the LDA. The keyATM can also incorporate covariates and directly model time trends. The keyATM is proposed in Eshima, Imai, and Sasaki (2024) <doi:10.1111/ajps.12779>.  "
  },
  {
    "id": 14859,
    "package_name": "keyperm",
    "title": "Keyword Analysis Using Permutation Tests",
    "description": "Efficient implementation of permutation tests for keyword analysis in corpus linguistics as described in Mildenberger (2023) <arXiv:2308.13383>.",
    "version": "0.1.1",
    "maintainer": "Thoralf Mildenberger <mild@zhaw.ch>",
    "author": "Thoralf Mildenberger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7242-1873>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=keyperm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "keyperm Keyword Analysis Using Permutation Tests Efficient implementation of permutation tests for keyword analysis in corpus linguistics as described in Mildenberger (2023) <arXiv:2308.13383>.  "
  },
  {
    "id": 14931,
    "package_name": "koRpus",
    "title": "Text Analysis with Emphasis on POS Tagging, Readability, and\nLexical Diversity",
    "description": "A set of tools to analyze texts. Includes, amongst others, functions for\n          automatic language detection, hyphenation, several indices of lexical diversity\n          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,\n          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also\n          provided, to enable frequency analyses (supports Celex and Leipzig Corpora\n          Collection file formats) and measures like tf-idf. Note: For full functionality\n          a local installation of TreeTagger is recommended. It is also recommended to\n          not load this package directly, but by loading one of the available language\n          support packages from the 'l10n' repository\n          <https://undocumeantit.github.io/repos/l10n/>. 'koRpus' also includes a plugin\n          for the R GUI and IDE RKWard, providing graphical dialogs for its basic\n          features. The respective R package 'rkward' cannot be installed directly from a\n          repository, as it is a part of RKWard. To make full use of this feature, please\n          install RKWard from <https://rkward.kde.org> (plugins are detected\n          automatically). Due to some restrictions on CRAN, the full package sources are\n          only available from the project homepage. To ask for help, report bugs, request\n          features, or discuss the development of the package, please subscribe to the\n          koRpus-dev mailing list (<https://korpusml.reaktanz.de>).",
    "version": "0.13-8",
    "maintainer": "Meik Michalke <meik.michalke@hhu.de>",
    "author": "Meik Michalke [aut, cre],\n  Earl Brown [ctb],\n  Alberto Mirisola [ctb],\n  Alexandre Brulet [ctb],\n  Laura Hauser [ctb]",
    "url": "https://reaktanz.de/?c=hacking&s=koRpus",
    "bug_reports": "https://github.com/unDocUMeantIt/koRpus/issues",
    "repository": "https://cran.r-project.org/package=koRpus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "koRpus Text Analysis with Emphasis on POS Tagging, Readability, and\nLexical Diversity A set of tools to analyze texts. Includes, amongst others, functions for\n          automatic language detection, hyphenation, several indices of lexical diversity\n          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,\n          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also\n          provided, to enable frequency analyses (supports Celex and Leipzig Corpora\n          Collection file formats) and measures like tf-idf. Note: For full functionality\n          a local installation of TreeTagger is recommended. It is also recommended to\n          not load this package directly, but by loading one of the available language\n          support packages from the 'l10n' repository\n          <https://undocumeantit.github.io/repos/l10n/>. 'koRpus' also includes a plugin\n          for the R GUI and IDE RKWard, providing graphical dialogs for its basic\n          features. The respective R package 'rkward' cannot be installed directly from a\n          repository, as it is a part of RKWard. To make full use of this feature, please\n          install RKWard from <https://rkward.kde.org> (plugins are detected\n          automatically). Due to some restrictions on CRAN, the full package sources are\n          only available from the project homepage. To ask for help, report bugs, request\n          features, or discuss the development of the package, please subscribe to the\n          koRpus-dev mailing list (<https://korpusml.reaktanz.de>).  "
  },
  {
    "id": 15024,
    "package_name": "languagelayeR",
    "title": "Access the 'languagelayer' API",
    "description": "Improve your text analysis with languagelayer\n        <https://languagelayer.com>, a powerful language detection\n        API.",
    "version": "1.2.4",
    "maintainer": "Colin FAY <contact@colinfay.me>",
    "author": "Colin FAY [aut, cre]",
    "url": "https://github.com/ColinFay/languagelayer",
    "bug_reports": "https://github.com/ColinFay/languagelayer/issues",
    "repository": "https://cran.r-project.org/package=languagelayeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "languagelayeR Access the 'languagelayer' API Improve your text analysis with languagelayer\n        <https://languagelayer.com>, a powerful language detection\n        API.  "
  },
  {
    "id": 15090,
    "package_name": "lda",
    "title": "Collapsed Gibbs Sampling Methods for Topic Models",
    "description": "Implements latent Dirichlet allocation (LDA)\n\t     and related models.  This includes (but is not limited\n\t     to) sLDA, corrLDA, and the mixed-membership stochastic\n\t     blockmodel.  Inference for all of these models is\n\t     implemented via a fast collapsed Gibbs sampler written\n\t     in C.  Utility functions for reading/writing data\n\t     typically used in topic models, as well as tools for\n\t     examining posterior distributions are also included.",
    "version": "1.5.2",
    "maintainer": "Santiago Olivella <olivella@unc.edu>",
    "author": "Jonathan Chang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lda Collapsed Gibbs Sampling Methods for Topic Models Implements latent Dirichlet allocation (LDA)\n\t     and related models.  This includes (but is not limited\n\t     to) sLDA, corrLDA, and the mixed-membership stochastic\n\t     blockmodel.  Inference for all of these models is\n\t     implemented via a fast collapsed Gibbs sampler written\n\t     in C.  Utility functions for reading/writing data\n\t     typically used in topic models, as well as tools for\n\t     examining posterior distributions are also included.  "
  },
  {
    "id": 15153,
    "package_name": "lexicon",
    "title": "Lexicons for Text Analysis",
    "description": "A collection of lexical hash tables, dictionaries, and word lists.",
    "version": "1.2.1",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker [aut, cre, cph],\n  University of Notre Dame [dtc, cph],\n  Department of Knowledge Technologies [dtc, cph],\n  Unicode, Inc. [dtc, cph],\n  John Higgins [dtc, cph],\n  Grady Ward [dtc],\n  Heiko Possel [dtc],\n  Michal Boleslav Mechura [dtc, cph],\n  Bing Liu [dtc],\n  Minqing Hu [dtc],\n  Saif M. Mohammad [dtc],\n  Peter Turney [dtc],\n  Erik Cambria [dtc],\n  Soujanya Poria [dtc],\n  Rajiv Bajpai [dtc],\n  Bjoern Schuller [dtc],\n  SentiWordNet [dtc, cph],\n  Liang Wu [dtc, cph],\n  Fred Morstatter [dtc, cph],\n  Huan Liu [dtc, cph],\n  Grammar Revolution [dtc, cph],\n  Vidar Holen [dtc, cph],\n  Alejandro U. Alvarez [dtc, cph],\n  Stackoverflow User user2592414 [dtc, cph],\n  BannedWordList.com [dtc, cph],\n  Apache Software Foundation [dtc, cph],\n  Andrew Kachites McCallum [dtc, cph],\n  Alireza Savand [dtc, cph],\n  Zact Anger [dtc, cph],\n  Titus Wormer [dtc, cph],\n  Colin Martindale [dtc, cph],\n  John Wiseman [dtc, cph],\n  Nadra Pencle [dtc, cph],\n  Irina Malaescu [dtc, cph]",
    "url": "https://github.com/trinker/lexicon",
    "bug_reports": "https://github.com/trinker/lexicon/issues?state=open",
    "repository": "https://cran.r-project.org/package=lexicon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lexicon Lexicons for Text Analysis A collection of lexical hash tables, dictionaries, and word lists.  "
  },
  {
    "id": 15154,
    "package_name": "lexiconPT",
    "title": "Lexicons for Portuguese Text Analysis",
    "description": "Provides easy access for sentiment lexicons for those who want to do text analysis in Portuguese texts.\n    As of now, two Portuguese lexicons are available: 'SentiLex-PT02' and 'OpLexicon' (v2.1 and v3.0).",
    "version": "0.1.0",
    "maintainer": "Sillas Gonzaga <sillas.gonzaga@gmail.com>",
    "author": "Sillas Gonzaga [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lexiconPT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lexiconPT Lexicons for Portuguese Text Analysis Provides easy access for sentiment lexicons for those who want to do text analysis in Portuguese texts.\n    As of now, two Portuguese lexicons are available: 'SentiLex-PT02' and 'OpLexicon' (v2.1 and v3.0).  "
  },
  {
    "id": 15551,
    "package_name": "malaytextr",
    "title": "Text Mining for Bahasa Malaysia",
    "description": "It is designed to work with text written in Bahasa Malaysia. We provide \n    functions and data sets that will make working with Bahasa Malaysia text much easier. \n    For word stemming in particular, we will look up the Malay words in a dictionary and\n    then proceed to remove \"extra suffix\" as explained in Khan, Rehman Ullah, \n    Fitri Suraya Mohamad, Muh Inam UlHaq, Shahren Ahmad Zadi Adruce, \n    Philip Nuli Anding, Sajjad Nawaz Khan, and Abdulrazak Yahya Saleh Al-Hababi\n    (2017) <https://ijrest.net/vol-4-issue-12.html> . This package includes\n    a dictionary of Malay words that may be used to perform word stemming, \n    a dataset of Malay stop words, a dataset of sentiment words\n    and a dataset of normalized words.",
    "version": "0.1.3",
    "maintainer": "Zahier Nasrudin <zahiernasrudin@gmail.com>",
    "author": "Zahier Nasrudin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7060-776X>)",
    "url": "https://github.com/zahiernasrudin/malaytextr",
    "bug_reports": "https://github.com/zahiernasrudin/malaytextr/issues",
    "repository": "https://cran.r-project.org/package=malaytextr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "malaytextr Text Mining for Bahasa Malaysia It is designed to work with text written in Bahasa Malaysia. We provide \n    functions and data sets that will make working with Bahasa Malaysia text much easier. \n    For word stemming in particular, we will look up the Malay words in a dictionary and\n    then proceed to remove \"extra suffix\" as explained in Khan, Rehman Ullah, \n    Fitri Suraya Mohamad, Muh Inam UlHaq, Shahren Ahmad Zadi Adruce, \n    Philip Nuli Anding, Sajjad Nawaz Khan, and Abdulrazak Yahya Saleh Al-Hababi\n    (2017) <https://ijrest.net/vol-4-issue-12.html> . This package includes\n    a dictionary of Malay words that may be used to perform word stemming, \n    a dataset of Malay stop words, a dataset of sentiment words\n    and a dataset of normalized words.  "
  },
  {
    "id": 15554,
    "package_name": "mallet",
    "title": "An R Wrapper for the Java Mallet Topic Modeling Toolkit",
    "description": "\n  An R interface for the Java Machine Learning for Language Toolkit (mallet)\n  <http://mallet.cs.umass.edu/> to estimate probabilistic topic models, such\n  as Latent Dirichlet Allocation. We can use the R package to read textual \n  data into mallet from R objects, run the Java implementation of mallet \n  directly in R, and extract results as R objects. The Mallet toolkit \n  has many functions, this wrapper focuses on the topic modeling sub-package \n  written by David Mimno. The package uses the rJava package to connect to a \n  JVM.",
    "version": "1.3.0",
    "maintainer": "M\u00e5ns Magnusson <mons.magnusson@gmail.com>",
    "author": "M\u00e5ns Magnusson [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-0296-2719>),\n  David Mimno [aut, cph] (ORCID: <https://orcid.org/0000-0001-7510-9404>)",
    "url": "https://github.com/mimno/RMallet",
    "bug_reports": "https://github.com/mimno/RMallet/issues",
    "repository": "https://cran.r-project.org/package=mallet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mallet An R Wrapper for the Java Mallet Topic Modeling Toolkit \n  An R interface for the Java Machine Learning for Language Toolkit (mallet)\n  <http://mallet.cs.umass.edu/> to estimate probabilistic topic models, such\n  as Latent Dirichlet Allocation. We can use the R package to read textual \n  data into mallet from R objects, run the Java implementation of mallet \n  directly in R, and extract results as R objects. The Mallet toolkit \n  has many functions, this wrapper focuses on the topic modeling sub-package \n  written by David Mimno. The package uses the rJava package to connect to a \n  JVM.  "
  },
  {
    "id": 15562,
    "package_name": "manifestoR",
    "title": "Access and Process Data and Documents of the Manifesto Project",
    "description": "Provides access to coded election programmes from the Manifesto\n    Corpus and to the Manifesto Project's Main Dataset and routines to analyse this\n    data. The Manifesto Project <https://manifesto-project.wzb.eu> collects and\n    analyses election programmes across time and space to measure the political\n    preferences of parties. The Manifesto Corpus contains the collected and\n    annotated election programmes in the Corpus format of the package 'tm' to enable\n    easy use of text processing and text mining functionality. Specific functions\n    for scaling of coded political texts are included.",
    "version": "1.6.1",
    "maintainer": "Pola Lehmann <pola.lehmann@wzb.eu>",
    "author": "Jirka Lewandowski [aut],\n  Nicolas Merz [aut],\n  Sven Regel [aut],\n  Pola Lehmann [cre, ctb],\n  Paul Muscat [ctb]",
    "url": "https://manifesto-project.wzb.eu/manifestoR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=manifestoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "manifestoR Access and Process Data and Documents of the Manifesto Project Provides access to coded election programmes from the Manifesto\n    Corpus and to the Manifesto Project's Main Dataset and routines to analyse this\n    data. The Manifesto Project <https://manifesto-project.wzb.eu> collects and\n    analyses election programmes across time and space to measure the political\n    preferences of parties. The Manifesto Corpus contains the collected and\n    annotated election programmes in the Corpus format of the package 'tm' to enable\n    easy use of text processing and text mining functionality. Specific functions\n    for scaling of coded political texts are included.  "
  },
  {
    "id": 15607,
    "package_name": "maptpx",
    "title": "MAP Estimation of Topic Models",
    "description": "Maximum a posteriori (MAP) estimation for topic models (i.e., Latent Dirichlet Allocation) in text analysis,\n\tas described in Taddy (2012) 'On estimation and selection for topic models'.  Previous versions of this code were included as part of the 'textir' package.  If you want to take advantage of openmp parallelization, uncomment the relevant flags in src/MAKEVARS before compiling.",
    "version": "1.9-7",
    "maintainer": "Matt Taddy <mataddy@gmail.com>",
    "author": "Matt Taddy <mataddy@gmail.com>",
    "url": "http://taddylab.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=maptpx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maptpx MAP Estimation of Topic Models Maximum a posteriori (MAP) estimation for topic models (i.e., Latent Dirichlet Allocation) in text analysis,\n\tas described in Taddy (2012) 'On estimation and selection for topic models'.  Previous versions of this code were included as part of the 'textir' package.  If you want to take advantage of openmp parallelization, uncomment the relevant flags in src/MAKEVARS before compiling.  "
  },
  {
    "id": 15748,
    "package_name": "mclm",
    "title": "Mastering Corpus Linguistics Methods",
    "description": "Read, inspect and process corpus files for quantitative corpus linguistics.\n  Obtain concordances via regular expressions, tokenize texts,\n  and compute frequencies and association measures. Useful for collocation analysis,\n  keywords analysis and variationist studies (comparison of linguistic variants\n  and of linguistic varieties).",
    "version": "0.2.7",
    "maintainer": "Mariana Montes <mariana.montes@kuleuven.be>",
    "author": "Dirk Speelman [aut],\n  Mariana Montes [aut, cre]",
    "url": "https://github.com/masterclm/mclm,\nhttps://masterclm.github.io/mclm/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mclm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mclm Mastering Corpus Linguistics Methods Read, inspect and process corpus files for quantitative corpus linguistics.\n  Obtain concordances via regular expressions, tokenize texts,\n  and compute frequencies and association measures. Useful for collocation analysis,\n  keywords analysis and variationist studies (comparison of linguistic variants\n  and of linguistic varieties).  "
  },
  {
    "id": 15801,
    "package_name": "meanr",
    "title": "Sentiment Analysis Scorer",
    "description": "Sentiment analysis is a popular technique in text mining that\n    attempts to determine the emotional state of some text. We provide a new\n    implementation of a common method for computing sentiment, whereby words are\n    scored as positive or negative according to a dictionary lookup. Then the\n    sum of those scores is returned for the document. We use the 'Hu' and 'Liu'\n    sentiment dictionary ('Hu' and 'Liu', 2004) <doi:10.1145/1014052.1014073>\n    for determining sentiment. The scoring function is 'vectorized' by document,\n    and scores for multiple documents are computed in parallel via 'OpenMP'.",
    "version": "0.1-6",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre]",
    "url": "https://github.com/wrathematics/meanr",
    "bug_reports": "https://github.com/wrathematics/meanr/issues",
    "repository": "https://cran.r-project.org/package=meanr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meanr Sentiment Analysis Scorer Sentiment analysis is a popular technique in text mining that\n    attempts to determine the emotional state of some text. We provide a new\n    implementation of a common method for computing sentiment, whereby words are\n    scored as positive or negative according to a dictionary lookup. Then the\n    sum of those scores is returned for the document. We use the 'Hu' and 'Liu'\n    sentiment dictionary ('Hu' and 'Liu', 2004) <doi:10.1145/1014052.1014073>\n    for determining sentiment. The scoring function is 'vectorized' by document,\n    and scores for multiple documents are computed in parallel via 'OpenMP'.  "
  },
  {
    "id": 16091,
    "package_name": "minorparties",
    "title": "Quantitatively Analyze Minor Political Parties",
    "description": "Tools for calculating I-Scores, a simple way to measure how successful minor political parties are at influencing the major parties in their environment. I-Scores are designed to be a more comprehensive measurement of minor party success than vote share and legislative seats won, the current standard measurements, which do not reflect the strategies that most minor parties employ. The procedure leverages the Manifesto Project's NLP model to identify the issue areas that sentences discuss, see Burst et al. (2024) <doi:10.25522/manifesto.manifestoberta.56topics.context.2024.1.1>, and the Wordfish algorithm to estimate the relative positions that platforms take on those issue areas, see Slapin and Proksch (2008) <doi:10.1111/j.1540-5907.2008.00338.x>.",
    "version": "1.0.0",
    "maintainer": "Theodore Gercken <tgercken@hamilton.edu>",
    "author": "Theodore Gercken [aut, cre, cph]",
    "url": "https://gerckentheodore.github.io/minorparties/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minorparties",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minorparties Quantitatively Analyze Minor Political Parties Tools for calculating I-Scores, a simple way to measure how successful minor political parties are at influencing the major parties in their environment. I-Scores are designed to be a more comprehensive measurement of minor party success than vote share and legislative seats won, the current standard measurements, which do not reflect the strategies that most minor parties employ. The procedure leverages the Manifesto Project's NLP model to identify the issue areas that sentences discuss, see Burst et al. (2024) <doi:10.25522/manifesto.manifestoberta.56topics.context.2024.1.1>, and the Wordfish algorithm to estimate the relative positions that platforms take on those issue areas, see Slapin and Proksch (2008) <doi:10.1111/j.1540-5907.2008.00338.x>.  "
  },
  {
    "id": 16387,
    "package_name": "morestopwords",
    "title": "All Stop Words in One Place",
    "description": "A standalone package combining several stop-word lists for 65 languages with a median of 329 stop words for language and over 1,000 entries for English, Breton, Latin, Slovenian, and Ancient Greek! The user automatically gets access to all the unique stop words contained in: the 'StopwordISO' repository; the 'Natural Language Toolkit' for 'python'; the 'Snowball' stop-word list; the R package 'quanteda'; the 'marimo' repository; the 'Perseus' project; and A. Berra's list of stop words for Ancient Greek and Latin.",
    "version": "0.2.0",
    "maintainer": "Fabio Ashtar Telarico <Fabio-Ashtar.Telarico@fdv.uni-lj.si>",
    "author": "Fabio Ashtar Telarico [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>),\n  Kohei Watanabe [aut]",
    "url": "https://fatelarico.github.io/morestopwords.html",
    "bug_reports": "https://github.com/FATelarico/morestopwords/issues",
    "repository": "https://cran.r-project.org/package=morestopwords",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morestopwords All Stop Words in One Place A standalone package combining several stop-word lists for 65 languages with a median of 329 stop words for language and over 1,000 entries for English, Breton, Latin, Slovenian, and Ancient Greek! The user automatically gets access to all the unique stop words contained in: the 'StopwordISO' repository; the 'Natural Language Toolkit' for 'python'; the 'Snowball' stop-word list; the R package 'quanteda'; the 'marimo' repository; the 'Perseus' project; and A. Berra's list of stop words for Ancient Greek and Latin.  "
  },
  {
    "id": 16389,
    "package_name": "morphemepiece",
    "title": "Morpheme Tokenization",
    "description": "Tokenize text into morphemes. The morphemepiece algorithm uses a \n  lookup table to determine the morpheme breakdown of words, and falls back on a \n  modified wordpiece tokenization algorithm for words not found in the lookup \n  table.",
    "version": "1.2.3",
    "maintainer": "Jonathan Bratt <jonathan.bratt@macmillan.com>",
    "author": "Jonathan Bratt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2859-0076>),\n  Jon Harmon [aut] (ORCID: <https://orcid.org/0000-0003-4781-4346>),\n  Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning [cph]",
    "url": "https://github.com/macmillancontentscience/morphemepiece",
    "bug_reports": "https://github.com/macmillancontentscience/morphemepiece/issues",
    "repository": "https://cran.r-project.org/package=morphemepiece",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morphemepiece Morpheme Tokenization Tokenize text into morphemes. The morphemepiece algorithm uses a \n  lookup table to determine the morpheme breakdown of words, and falls back on a \n  modified wordpiece tokenization algorithm for words not found in the lookup \n  table.  "
  },
  {
    "id": 16390,
    "package_name": "morphemepiece.data",
    "title": "Data for Morpheme Tokenization",
    "description": "Provides data about morphemes, the smallest units of meaning in a \n    language.",
    "version": "1.2.0",
    "maintainer": "Jon Harmon <jonthegeek@gmail.com>",
    "author": "Jonathan Bratt [aut] (ORCID: <https://orcid.org/0000-0003-2859-0076>),\n  Jon Harmon [aut, cre] (ORCID: <https://orcid.org/0000-0003-4781-4346>),\n  Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning [cph]",
    "url": "https://github.com/macmillancontentscience/morphemepiece.data",
    "bug_reports": "https://github.com/macmillancontentscience/morphemepiece.data/issues",
    "repository": "https://cran.r-project.org/package=morphemepiece.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morphemepiece.data Data for Morpheme Tokenization Provides data about morphemes, the smallest units of meaning in a \n    language.  "
  },
  {
    "id": 16476,
    "package_name": "mscstexta4r",
    "title": "R Client for the Microsoft Cognitive Services Text Analytics\nREST API",
    "description": "R Client for the Microsoft Cognitive Services Text Analytics\n    REST API, including Sentiment Analysis, Topic Detection, Language Detection,\n    and Key Phrase Extraction. An account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly.",
    "version": "0.1.2",
    "maintainer": "Phil Ferriere <pferriere@hotmail.com>",
    "author": "Phil Ferriere [aut, cre]",
    "url": "https://github.com/philferriere/mscstexta4r",
    "bug_reports": "http://www.github.com/philferriere/mscstexta4r/issues",
    "repository": "https://cran.r-project.org/package=mscstexta4r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mscstexta4r R Client for the Microsoft Cognitive Services Text Analytics\nREST API R Client for the Microsoft Cognitive Services Text Analytics\n    REST API, including Sentiment Analysis, Topic Detection, Language Detection,\n    and Key Phrase Extraction. An account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly.  "
  },
  {
    "id": 16556,
    "package_name": "multicastR",
    "title": "A Companion to the Multi-CAST Collection",
    "description": "Provides a basic interface for accessing annotation data from\n   the Multi-CAST collection, a database of spoken natural language texts\n   edited by Geoffrey Haig and Stefan Schnell. The collection draws from a\n   diverse set of languages and has been annotated across multiple levels.\n   Annotation data is downloaded on request from the servers of the\n   University of Bamberg. See the Multi-CAST website\n   <https://multicast.aspra.uni-bamberg.de/> for more information and a list\n   of related publications.",
    "version": "2.0.0",
    "maintainer": "Nils Norman Schiborr <nils-norman.schiborr@uni-bamberg.de>",
    "author": "Nils Norman Schiborr [aut, cre]",
    "url": "https://multicast.aspra.uni-bamberg.de/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multicastR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multicastR A Companion to the Multi-CAST Collection Provides a basic interface for accessing annotation data from\n   the Multi-CAST collection, a database of spoken natural language texts\n   edited by Geoffrey Haig and Stefan Schnell. The collection draws from a\n   diverse set of languages and has been annotated across multiple levels.\n   Annotation data is downloaded on request from the servers of the\n   University of Bamberg. See the Multi-CAST website\n   <https://multicast.aspra.uni-bamberg.de/> for more information and a list\n   of related publications.  "
  },
  {
    "id": 16679,
    "package_name": "mvrsquared",
    "title": "Compute the Coefficient of Determination for Vector or Matrix\nOutcomes",
    "description": "Compute the coefficient of determination for outcomes in n-dimensions. \n  May be useful for multidimensional predictions (such as a multinomial model) or\n  calculating goodness of fit from latent variable models such as probabilistic\n  topic models like latent Dirichlet allocation or deterministic topic models \n  like latent semantic analysis. Based on Jones (2019) <arXiv:1911.11061>.",
    "version": "0.1.5",
    "maintainer": "Tommy Jones <jones.thos.w@gmail.com>",
    "author": "Tommy Jones [aut, cre] (ORCID: <https://orcid.org/0000-0001-6457-2452>),\n  Thomas Nagler [ctb] (ORCID: <https://orcid.org/0000-0003-1855-0046>)",
    "url": "https://github.com/TommyJones/mvrsquared",
    "bug_reports": "https://github.com/TommyJones/mvrsquared/issues",
    "repository": "https://cran.r-project.org/package=mvrsquared",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvrsquared Compute the Coefficient of Determination for Vector or Matrix\nOutcomes Compute the coefficient of determination for outcomes in n-dimensions. \n  May be useful for multidimensional predictions (such as a multinomial model) or\n  calculating goodness of fit from latent variable models such as probabilistic\n  topic models like latent Dirichlet allocation or deterministic topic models \n  like latent semantic analysis. Based on Jones (2019) <arXiv:1911.11061>.  "
  },
  {
    "id": 16891,
    "package_name": "ngram",
    "title": "Fast n-Gram 'Tokenization'",
    "description": "An n-gram is a sequence of n \"words\" taken, in order, from a\n    body of text.  This is a collection of utilities for creating,\n    displaying, summarizing, and \"babbling\" n-grams.  The\n    'tokenization' and \"babbling\" are handled by very efficient C\n    code, which can even be built as its own standalone library.\n    The babbler is a simple Markov chain.  The package also offers\n    a vignette with complete example 'workflows' and information about\n    the utilities offered in the package.",
    "version": "3.2.3",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre],\n  Christian Heckendorf [aut]",
    "url": "https://github.com/wrathematics/ngram",
    "bug_reports": "https://github.com/wrathematics/ngram/issues",
    "repository": "https://cran.r-project.org/package=ngram",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ngram Fast n-Gram 'Tokenization' An n-gram is a sequence of n \"words\" taken, in order, from a\n    body of text.  This is a collection of utilities for creating,\n    displaying, summarizing, and \"babbling\" n-grams.  The\n    'tokenization' and \"babbling\" are handled by very efficient C\n    code, which can even be built as its own standalone library.\n    The babbler is a simple Markov chain.  The package also offers\n    a vignette with complete example 'workflows' and information about\n    the utilities offered in the package.  "
  },
  {
    "id": 16893,
    "package_name": "ngramrr",
    "title": "A Simple General Purpose N-Gram Tokenizer",
    "description": "A simple n-gram (contiguous sequences of n items from a\n    given sequence of text) tokenizer to be used with the 'tm' package with no\n    'rJava'/'RWeka' dependency.",
    "version": "0.2.0",
    "maintainer": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "author": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "url": "https://github.com/chainsawriot/ngramrr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ngramrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ngramrr A Simple General Purpose N-Gram Tokenizer A simple n-gram (contiguous sequences of n items from a\n    given sequence of text) tokenizer to be used with the 'tm' package with no\n    'rJava'/'RWeka' dependency.  "
  },
  {
    "id": 16954,
    "package_name": "nlpembeds",
    "title": "Natural Language Processing Embeddings",
    "description": "Provides efficient methods to compute co-occurrence matrices, pointwise mutual information (PMI) and singular value decomposition (SVD). In the biomedical and clinical settings, one challenge is the huge size of databases, e.g. when analyzing data of millions of patients over tens of years. To address this, this package provides functions to efficiently compute monthly co-occurrence matrices, which is the computational bottleneck of the analysis, by using the 'RcppAlgos' package and sparse matrices. Furthermore, the functions can be called on 'SQL' databases, enabling the computation of co-occurrence matrices of tens of gigabytes of data, representing millions of patients over tens of years. Partly based on Hong C. (2021) <doi:10.1038/s41746-021-00519-z>.",
    "version": "1.0.0",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "author": "Thomas Charlon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7497-0470>),\n  Doudou Zhou [ctb] (ORCID: <https://orcid.org/0000-0002-0830-2287>),\n  CELEHS [aut] (<https://celehs.hms.harvard.edu>)",
    "url": "https://gitlab.com/thomaschln/nlpembeds",
    "bug_reports": "https://gitlab.com/thomaschln/nlpembeds/-/issues",
    "repository": "https://cran.r-project.org/package=nlpembeds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlpembeds Natural Language Processing Embeddings Provides efficient methods to compute co-occurrence matrices, pointwise mutual information (PMI) and singular value decomposition (SVD). In the biomedical and clinical settings, one challenge is the huge size of databases, e.g. when analyzing data of millions of patients over tens of years. To address this, this package provides functions to efficiently compute monthly co-occurrence matrices, which is the computational bottleneck of the analysis, by using the 'RcppAlgos' package and sparse matrices. Furthermore, the functions can be called on 'SQL' databases, enabling the computation of co-occurrence matrices of tens of gigabytes of data, representing millions of patients over tens of years. Partly based on Hong C. (2021) <doi:10.1038/s41746-021-00519-z>.  "
  },
  {
    "id": 17149,
    "package_name": "nzilbb.labbcat",
    "title": "Accessing Data Stored in 'LaBB-CAT' Instances",
    "description": "'LaBB-CAT' is a web-based language corpus management\n system developed by the New Zealand Institute of Language, Brain\n and Behaviour (NZILBB) - see <https://labbcat.canterbury.ac.nz>.\n This package defines functions for accessing corpus data in a 'LaBB-CAT'\n instance. You must have at least version 20230818.1400 of 'LaBB-CAT'\n to use this package.\n For more information about 'LaBB-CAT', see\n Robert Fromont and Jennifer Hay (2008) <doi:10.3366/E1749503208000142>\n or \n Robert Fromont (2017) <doi:10.1016/j.csl.2017.01.004>.",
    "version": "1.5-0",
    "maintainer": "Robert Fromont <robert.fromont@canterbury.ac.nz>",
    "author": "Robert Fromont [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5271-5487>)",
    "url": "https://nzilbb.github.io/labbcat-R/,\nhttps://labbcat.canterbury.ac.nz",
    "bug_reports": "https://github.com/nzilbb/labbcat-R/issues",
    "repository": "https://cran.r-project.org/package=nzilbb.labbcat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nzilbb.labbcat Accessing Data Stored in 'LaBB-CAT' Instances 'LaBB-CAT' is a web-based language corpus management\n system developed by the New Zealand Institute of Language, Brain\n and Behaviour (NZILBB) - see <https://labbcat.canterbury.ac.nz>.\n This package defines functions for accessing corpus data in a 'LaBB-CAT'\n instance. You must have at least version 20230818.1400 of 'LaBB-CAT'\n to use this package.\n For more information about 'LaBB-CAT', see\n Robert Fromont and Jennifer Hay (2008) <doi:10.3366/E1749503208000142>\n or \n Robert Fromont (2017) <doi:10.1016/j.csl.2017.01.004>.  "
  },
  {
    "id": 17279,
    "package_name": "oolong",
    "title": "Create Validation Tests for Automated Content Analysis",
    "description": "Intended to create standard human-in-the-loop validity tests for typical automated content analysis such as topic modeling and dictionary-based methods. This package offers a standard workflow with functions to prepare, administer and evaluate a human-in-the-loop validity test. This package provides functions for validating topic models using word intrusion, topic intrusion (Chang et al. 2009,  <https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models>) and word set intrusion (Ying et al. 2021) <doi:10.1017/pan.2021.33> tests. This package also provides functions for generating gold-standard data which are useful for validating dictionary-based methods. The default settings of all generated tests match those suggested in Chang et al. (2009) and Song et al. (2020) <doi:10.1080/10584609.2020.1723752>.",
    "version": "0.6.1",
    "maintainer": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "author": "Chung-hong Chan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6232-7530>),\n  Marius S\u00e4ltzer [aut] (ORCID: <https://orcid.org/0000-0002-8604-4666>)",
    "url": "https://gesistsa.github.io/oolong/,\nhttps://github.com/gesistsa/oolong",
    "bug_reports": "https://github.com/gesistsa/oolong/issues",
    "repository": "https://cran.r-project.org/package=oolong",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oolong Create Validation Tests for Automated Content Analysis Intended to create standard human-in-the-loop validity tests for typical automated content analysis such as topic modeling and dictionary-based methods. This package offers a standard workflow with functions to prepare, administer and evaluate a human-in-the-loop validity test. This package provides functions for validating topic models using word intrusion, topic intrusion (Chang et al. 2009,  <https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models>) and word set intrusion (Ying et al. 2021) <doi:10.1017/pan.2021.33> tests. This package also provides functions for generating gold-standard data which are useful for validating dictionary-based methods. The default settings of all generated tests match those suggested in Chang et al. (2009) and Song et al. (2020) <doi:10.1080/10584609.2020.1723752>.  "
  },
  {
    "id": 17292,
    "package_name": "openNLP",
    "title": "Apache OpenNLP Tools Interface",
    "description": "An interface to the Apache OpenNLP tools (version 1.5.3).\n  The Apache OpenNLP library is a machine learning based toolkit for the\n  processing of natural language text written in Java.\n  It supports the most common NLP tasks, such as tokenization, sentence\n  segmentation, part-of-speech tagging, named entity extraction, chunking,\n  parsing, and coreference resolution.\n  See <https://opennlp.apache.org/> for more information.",
    "version": "0.2-7",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=openNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openNLP Apache OpenNLP Tools Interface An interface to the Apache OpenNLP tools (version 1.5.3).\n  The Apache OpenNLP library is a machine learning based toolkit for the\n  processing of natural language text written in Java.\n  It supports the most common NLP tasks, such as tokenization, sentence\n  segmentation, part-of-speech tagging, named entity extraction, chunking,\n  parsing, and coreference resolution.\n  See <https://opennlp.apache.org/> for more information.  "
  },
  {
    "id": 17293,
    "package_name": "openNLPdata",
    "title": "Apache OpenNLP Jars and Basic English Language Models",
    "description": "Apache OpenNLP jars and basic English language models.",
    "version": "1.5.3-5",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  The Apache Software Foundation [ctb, cph] (Apache OpenNLP Java\n    libraries),\n  JWNL development team [ctb, cph] (JWNL Java WordNet Library)",
    "url": "https://opennlp.apache.org/,\nhttps://opennlp.sourceforge.net/models-1.5/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=openNLPdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openNLPdata Apache OpenNLP Jars and Basic English Language Models Apache OpenNLP jars and basic English language models.  "
  },
  {
    "id": 17900,
    "package_name": "phrases",
    "title": "Phrasal Verbs in English Club Website",
    "description": "Contains all phrasal verbs listed in <https://www.englishclub.com/ref/Phrasal_Verbs/>\n            as data frame. Useful for educational purpose as well as for text mining.",
    "version": "0.1",
    "maintainer": "Suman Khanal <suman81765@gmail.com>",
    "author": "Suman Khanal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6563-6294>)",
    "url": "https://github.com/sumanstats/phrases",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phrases",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phrases Phrasal Verbs in English Club Website Contains all phrasal verbs listed in <https://www.englishclub.com/ref/Phrasal_Verbs/>\n            as data frame. Useful for educational purpose as well as for text mining.  "
  },
  {
    "id": 17939,
    "package_name": "piecemaker",
    "title": "Tools for Preparing Text for Tokenizers",
    "description": "Tokenizers break text into pieces that are more usable by\n    machine learning models. Many tokenizers share some preparation steps.\n    This package provides those shared steps, along with a simple\n    tokenizer.",
    "version": "1.0.2",
    "maintainer": "Jon Harmon <jonthegeek@gmail.com>",
    "author": "Jon Harmon [aut, cre] (ORCID: <https://orcid.org/0000-0003-4781-4346>),\n  Jonathan Bratt [aut] (ORCID: <https://orcid.org/0000-0003-2859-0076>),\n  Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning [cph]",
    "url": "https://github.com/macmillancontentscience/piecemaker,\nhttps://macmillancontentscience.github.io/piecemaker/",
    "bug_reports": "https://github.com/macmillancontentscience/piecemaker/issues",
    "repository": "https://cran.r-project.org/package=piecemaker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "piecemaker Tools for Preparing Text for Tokenizers Tokenizers break text into pieces that are more usable by\n    machine learning models. Many tokenizers share some preparation steps.\n    This package provides those shared steps, along with a simple\n    tokenizer.  "
  },
  {
    "id": 18072,
    "package_name": "plu",
    "title": "Dynamically Pluralize Phrases",
    "description": "Converts English phrases to singular or plural form based on\n    the length of an associated vector.  Contains helper functions to\n    create natural language lists from vectors and to include the length\n    of a vector in natural language.",
    "version": "0.3.0",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>)",
    "url": "https://pkg.rossellhayes.com/plu/,\nhttps://github.com/rossellhayes/plu",
    "bug_reports": "https://github.com/rossellhayes/plu/issues",
    "repository": "https://cran.r-project.org/package=plu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plu Dynamically Pluralize Phrases Converts English phrases to singular or plural form based on\n    the length of an associated vector.  Contains helper functions to\n    create natural language lists from vectors and to include the length\n    of a vector in natural language.  "
  },
  {
    "id": 18123,
    "package_name": "poismf",
    "title": "Factorization of Sparse Counts Matrices Through Poisson\nLikelihood",
    "description": "Creates a non-negative low-rank approximate factorization of a sparse counts matrix by maximizing Poisson\n    likelihood with L1/L2 regularization (e.g. for implicit-feedback recommender systems or bag-of-words-based topic modeling)\n    (Cortes, (2018) <arXiv:1811.01908>), which usually leads to very sparse user and item factors (over 90% zero-valued).\n    Similar to hierarchical Poisson factorization (HPF), but follows an optimization-based approach with regularization\n    instead of a hierarchical prior, and is fit through gradient-based methods instead of variational inference.",
    "version": "0.4.0-4",
    "maintainer": "David Cortes <david.cortes.rivera@gmail.com>",
    "author": "David Cortes [aut, cre, cph],\n  Jean-Sebastien Roy [cph] (Copyright holder of included tnc library),\n  Stephen Nash [cph] (Copyright holder of included tnc library)",
    "url": "https://github.com/david-cortes/poismf",
    "bug_reports": "https://github.com/david-cortes/poismf/issues",
    "repository": "https://cran.r-project.org/package=poismf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "poismf Factorization of Sparse Counts Matrices Through Poisson\nLikelihood Creates a non-negative low-rank approximate factorization of a sparse counts matrix by maximizing Poisson\n    likelihood with L1/L2 regularization (e.g. for implicit-feedback recommender systems or bag-of-words-based topic modeling)\n    (Cortes, (2018) <arXiv:1811.01908>), which usually leads to very sparse user and item factors (over 90% zero-valued).\n    Similar to hierarchical Poisson factorization (HPF), but follows an optimization-based approach with regularization\n    instead of a hierarchical prior, and is fit through gradient-based methods instead of variational inference.  "
  },
  {
    "id": 18138,
    "package_name": "politeness",
    "title": "Detecting Politeness Features in Text",
    "description": "Detecting markers of politeness in English natural language. This package allows researchers to easily visualize and quantify politeness between groups of documents. This package combines prior research on the linguistic markers of politeness. We thank the Spencer Foundation, the Hewlett Foundation, and Harvard's Institute for Quantitative Social Science for support.",
    "version": "0.9.4",
    "maintainer": "Mike Yeomans <mk.yeomans@gmail.com>",
    "author": "Mike Yeomans [aut, cre],\n  Alejandro Kantor [aut],\n  Dustin Tingley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=politeness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "politeness Detecting Politeness Features in Text Detecting markers of politeness in English natural language. This package allows researchers to easily visualize and quantify politeness between groups of documents. This package combines prior research on the linguistic markers of politeness. We thank the Spencer Foundation, the Hewlett Foundation, and Harvard's Institute for Quantitative Social Science for support.  "
  },
  {
    "id": 18142,
    "package_name": "polmineR",
    "title": "Verbs and Nouns for Corpus Analysis",
    "description": "Package for corpus analysis using the Corpus Workbench \n    ('CWB', <https://cwb.sourceforge.io>) as an efficient back end for indexing\n    and querying large corpora. The package offers functionality to flexibly create\n    subcorpora and to carry out basic statistical operations (count, co-occurrences\n    etc.). The original full text of documents can be reconstructed and inspected at\n    any time. Beyond that, the package is intended to serve as an interface to \n    packages implementing advanced statistical procedures. Respective data structures\n    (document-term matrices, term-co-occurrence matrices etc.) can be created based \n    on the indexed corpora.",
    "version": "0.8.9",
    "maintainer": "Andreas Blaette <andreas.blaette@uni-due.de>",
    "author": "Andreas Blaette [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8970-8010>),\n  Christoph Leonhardt [ctb],\n  Marius Bertram [ctb]",
    "url": "https://github.com/PolMine/polmineR",
    "bug_reports": "https://github.com/PolMine/polmineR/issues",
    "repository": "https://cran.r-project.org/package=polmineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "polmineR Verbs and Nouns for Corpus Analysis Package for corpus analysis using the Corpus Workbench \n    ('CWB', <https://cwb.sourceforge.io>) as an efficient back end for indexing\n    and querying large corpora. The package offers functionality to flexibly create\n    subcorpora and to carry out basic statistical operations (count, co-occurrences\n    etc.). The original full text of documents can be reconstructed and inspected at\n    any time. Beyond that, the package is intended to serve as an interface to \n    packages implementing advanced statistical procedures. Respective data structures\n    (document-term matrices, term-co-occurrence matrices etc.) can be created based \n    on the indexed corpora.  "
  },
  {
    "id": 18454,
    "package_name": "proustr",
    "title": "Tools for Natural Language Processing in French",
    "description": "Tools for Natural Language Processing in French and texts from Marcel Proust's collection \n  \"A La Recherche Du Temps Perdu\". The novels contained in this collection are \n  \"Du cote de chez Swann \", \"A l'ombre des jeunes filles en fleurs\",\"Le Cote de Guermantes\", \n  \"Sodome et Gomorrhe I et II\", \"La Prisonniere\", \"Albertine disparue\", and \"Le Temps retrouve\".",
    "version": "0.4.0",
    "maintainer": "Colin Fay <contact@colinfay.me>",
    "author": "Colin Fay [aut, cre] (ORCID: <https://orcid.org/0000-0001-7343-1846>)",
    "url": "https://github.com/ColinFay/proustr",
    "bug_reports": "https://github.com/ColinFay/proustr/issues",
    "repository": "https://cran.r-project.org/package=proustr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "proustr Tools for Natural Language Processing in French Tools for Natural Language Processing in French and texts from Marcel Proust's collection \n  \"A La Recherche Du Temps Perdu\". The novels contained in this collection are \n  \"Du cote de chez Swann \", \"A l'ombre des jeunes filles en fleurs\",\"Le Cote de Guermantes\", \n  \"Sodome et Gomorrhe I et II\", \"La Prisonniere\", \"Albertine disparue\", and \"Le Temps retrouve\".  "
  },
  {
    "id": 18543,
    "package_name": "pubmed.mineR",
    "title": "Text Mining of PubMed Abstracts",
    "description": "Text mining of PubMed Abstracts (text and XML) from <https://pubmed.ncbi.nlm.nih.gov/>.",
    "version": "1.0.21",
    "maintainer": "S. Ramachandran <ramuigib@gmail.com>",
    "author": "Jyoti Rani [aut],\n  S.Ramachandran [aut],\n  Ab Rauf Shah [aut],\n  S. Ramachandran [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pubmed.mineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pubmed.mineR Text Mining of PubMed Abstracts Text mining of PubMed Abstracts (text and XML) from <https://pubmed.ncbi.nlm.nih.gov/>.  "
  },
  {
    "id": 18548,
    "package_name": "pullword",
    "title": "R Interface to Pullword Service",
    "description": "R Interface to Pullword Service for natural language processing\n    in Chinese. It enables users to extract valuable words from text by deep learning models. \n    For more details please visit the official site (in Chinese) <http://www.pullword.com/>.",
    "version": "0.3",
    "maintainer": "Tong He <hetong007@gmail.com>",
    "author": "Tong He <hetong007@gmail.com>",
    "url": "",
    "bug_reports": "https://github.com/hetong007/pullword/issues",
    "repository": "https://cran.r-project.org/package=pullword",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pullword R Interface to Pullword Service R Interface to Pullword Service for natural language processing\n    in Chinese. It enables users to extract valuable words from text by deep learning models. \n    For more details please visit the official site (in Chinese) <http://www.pullword.com/>.  "
  },
  {
    "id": 18626,
    "package_name": "qdap",
    "title": "Bridging the Gap Between Qualitative Data and Quantitative\nAnalysis",
    "description": "Automates many of the tasks associated with quantitative discourse analysis of transcripts containing discourse\n              including frequency counts of sentence types, words, sentences, turns of talk, syllables and other assorted\n              analysis tasks. The package provides parsing tools for preparing transcript data. Many functions enable the user\n              to aggregate data by any number of grouping variables, providing analysis and seamless integration with other R\n              packages that undertake higher level analysis and visualization of text. This affords the user a more efficient\n              and targeted analysis. 'qdap' is designed for transcript analysis, however, many functions are applicable to other\n              areas of Text Mining/ Natural Language Processing.",
    "version": "2.4.6.1",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker [aut, cre],\n  Bryan Goodrich [ctb],\n  Dason Kurkiewicz [ctb]",
    "url": "https://trinker.github.io/qdap/",
    "bug_reports": "https://github.com/trinker/qdap/issues",
    "repository": "https://cran.r-project.org/package=qdap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qdap Bridging the Gap Between Qualitative Data and Quantitative\nAnalysis Automates many of the tasks associated with quantitative discourse analysis of transcripts containing discourse\n              including frequency counts of sentence types, words, sentences, turns of talk, syllables and other assorted\n              analysis tasks. The package provides parsing tools for preparing transcript data. Many functions enable the user\n              to aggregate data by any number of grouping variables, providing analysis and seamless integration with other R\n              packages that undertake higher level analysis and visualization of text. This affords the user a more efficient\n              and targeted analysis. 'qdap' is designed for transcript analysis, however, many functions are applicable to other\n              areas of Text Mining/ Natural Language Processing.  "
  },
  {
    "id": 18627,
    "package_name": "qdapDictionaries",
    "title": "Dictionaries and Word Lists for the 'qdap' Package",
    "description": "A collection of text analysis dictionaries and word lists for use with\n        the 'qdap' package.",
    "version": "1.0.7",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker",
    "url": "http://trinker.github.com/qdapDictionaries/",
    "bug_reports": "http://github.com/trinker/qdapDictionaries/issues",
    "repository": "https://cran.r-project.org/package=qdapDictionaries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qdapDictionaries Dictionaries and Word Lists for the 'qdap' Package A collection of text analysis dictionaries and word lists for use with\n        the 'qdap' package.  "
  },
  {
    "id": 18629,
    "package_name": "qdapTools",
    "title": "Tools for the 'qdap' Package",
    "description": "A collection of tools associated with the 'qdap' package that may be useful outside of the\n            context of text analysis.",
    "version": "1.3.7",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Bryan Goodrich [ctb],\n  Dason Kurkiewicz [ctb],\n  Kirill Muller [ctb],\n  Tyler Rinker [aut, cre]",
    "url": "https://github.com/trinker/qdapTools",
    "bug_reports": "https://github.com/trinker/qdapTools/issues",
    "repository": "https://cran.r-project.org/package=qdapTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qdapTools Tools for the 'qdap' Package A collection of tools associated with the 'qdap' package that may be useful outside of the\n            context of text analysis.  "
  },
  {
    "id": 18692,
    "package_name": "qtkit",
    "title": "Quantitative Text Kit",
    "description": "Support package for the textbook \"An Introduction to\n    Quantitative Text Analysis for Linguists: Reproducible Research Using\n    R\" (Francom, 2024) <doi:10.4324/9781003393764>. Includes functions to\n    acquire, clean, and analyze text data as well as functions to document\n    and share the results of text analysis. The package is designed to be\n    used in conjunction with the book, but can also be used as a standalone\n    package for text analysis.",
    "version": "1.1.1",
    "maintainer": "Jerid Francom <francojc@wfu.edu>",
    "author": "Jerid Francom [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5972-6330>)",
    "url": "https://cran.r-project.org/package=qtkit",
    "bug_reports": "https://github.com/qtalr/qtkit/issues",
    "repository": "https://cran.r-project.org/package=qtkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qtkit Quantitative Text Kit Support package for the textbook \"An Introduction to\n    Quantitative Text Analysis for Linguists: Reproducible Research Using\n    R\" (Francom, 2024) <doi:10.4324/9781003393764>. Includes functions to\n    acquire, clean, and analyze text data as well as functions to document\n    and share the results of text analysis. The package is designed to be\n    used in conjunction with the book, but can also be used as a standalone\n    package for text analysis.  "
  },
  {
    "id": 18707,
    "package_name": "quRan",
    "title": "Complete Text of the Qur'an",
    "description": "Full text, in data frames containing one row per verse, of the \n    Qur'an in Arabic (with and without vowels) and in English (the Yusuf Ali \n    and Saheeh International translations), formatted to be convenient for \n    text analysis.",
    "version": "0.1.0",
    "maintainer": "Andrew Heiss <andrew@andrewheiss.com>",
    "author": "Andrew Heiss [aut, cre]",
    "url": "https://github.com/andrewheiss/quRan",
    "bug_reports": "https://github.com/andrewheiss/quRan/issues",
    "repository": "https://cran.r-project.org/package=quRan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quRan Complete Text of the Qur'an Full text, in data frames containing one row per verse, of the \n    Qur'an in Arabic (with and without vowels) and in English (the Yusuf Ali \n    and Saheeh International translations), formatted to be convenient for \n    text analysis.  "
  },
  {
    "id": 18725,
    "package_name": "quanteda",
    "title": "Quantitative Analysis of Textual Data",
    "description": "A fast, flexible, and comprehensive framework for \n    quantitative text analysis in R.  Provides functionality for corpus management,\n    creating and manipulating tokens and n-grams, exploring keywords in context, \n    forming and manipulating sparse matrices\n    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and\n    distances, applying content dictionaries, applying supervised and unsupervised machine learning, \n    visually representing text and text analyses, and more. ",
    "version": "4.3.1",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  Haiyan Wang [aut] (ORCID: <https://orcid.org/0000-0003-4992-4311>),\n  Paul Nulty [aut] (ORCID: <https://orcid.org/0000-0002-7214-4666>),\n  Adam Obeng [aut] (ORCID: <https://orcid.org/0000-0002-2906-4775>),\n  Stefan M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-6315-4125>),\n  Akitaka Matsuo [aut] (ORCID: <https://orcid.org/0000-0002-3323-6330>),\n  William Lowe [aut] (ORCID: <https://orcid.org/0000-0002-1549-6163>),\n  Christian M\u00fcller [ctb],\n  Olivier Delmarcelle [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4347-070X>),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "https://quanteda.io",
    "bug_reports": "https://github.com/quanteda/quanteda/issues",
    "repository": "https://cran.r-project.org/package=quanteda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda Quantitative Analysis of Textual Data A fast, flexible, and comprehensive framework for \n    quantitative text analysis in R.  Provides functionality for corpus management,\n    creating and manipulating tokens and n-grams, exploring keywords in context, \n    forming and manipulating sparse matrices\n    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and\n    distances, applying content dictionaries, applying supervised and unsupervised machine learning, \n    visually representing text and text analyses, and more.   "
  },
  {
    "id": 18727,
    "package_name": "quanteda.textplots",
    "title": "Plots for the Quantitative Analysis of Textual Data",
    "description": "Plotting functions for visualising textual data.  Extends 'quanteda' and \n   related packages with plot methods designed specifically for text data, textual statistics, \n   and models fit to textual data. Plot types include word clouds, lexical dispersion plots, \n   scaling plots, network visualisations, and word 'keyness' plots.",
    "version": "0.96.1",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  Haiyan Wang [aut] (ORCID: <https://orcid.org/0000-0003-4992-4311>),\n  Adam Obeng [aut] (ORCID: <https://orcid.org/0000-0002-2906-4775>),\n  Stefan M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-6315-4125>),\n  Akitaka Matsuo [aut] (ORCID: <https://orcid.org/0000-0002-3323-6330>),\n  Ian Fellows [cph] (authored wordcloud C source code (modified)),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "",
    "bug_reports": "https://github.com/quanteda/quanteda.textplots/issues",
    "repository": "https://cran.r-project.org/package=quanteda.textplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda.textplots Plots for the Quantitative Analysis of Textual Data Plotting functions for visualising textual data.  Extends 'quanteda' and \n   related packages with plot methods designed specifically for text data, textual statistics, \n   and models fit to textual data. Plot types include word clouds, lexical dispersion plots, \n   scaling plots, network visualisations, and word 'keyness' plots.  "
  },
  {
    "id": 18728,
    "package_name": "quanteda.textstats",
    "title": "Textual Statistics for the Quantitative Analysis of Textual Data",
    "description": "Textual statistics functions formerly in the 'quanteda' package.\n    Textual statistics for characterizing and comparing textual data. Includes \n    functions for measuring term and document frequency, the co-occurrence of \n    words, similarity and distance between features and documents, feature entropy, \n    keyword occurrence, readability, and lexical diversity.  These functions \n    extend the 'quanteda' package and are specially designed for sparse textual data.",
    "version": "0.97.2",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  Haiyan Wang [aut] (ORCID: <https://orcid.org/0000-0003-4992-4311>),\n  Jiong Wei Lua [aut],\n  Jouni Kuha [aut] (ORCID: <https://orcid.org/0000-0002-1156-8465>),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "https://quanteda.io",
    "bug_reports": "https://github.com/quanteda/quanteda.textstats/issues",
    "repository": "https://cran.r-project.org/package=quanteda.textstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda.textstats Textual Statistics for the Quantitative Analysis of Textual Data Textual statistics functions formerly in the 'quanteda' package.\n    Textual statistics for characterizing and comparing textual data. Includes \n    functions for measuring term and document frequency, the co-occurrence of \n    words, similarity and distance between features and documents, feature entropy, \n    keyword occurrence, readability, and lexical diversity.  These functions \n    extend the 'quanteda' package and are specially designed for sparse textual data.  "
  },
  {
    "id": 18857,
    "package_name": "rJavaEnv",
    "title": "'Java' Environments for R Projects",
    "description": "Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.",
    "version": "0.3.0",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6690-5345>),\n  Chung-hong Chan [aut] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Mauricio Vargas [ctb] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\n  Hadley Wickham [ctb] (use_java feature suggestion and PR review),\n  Enrique Mondragon-Estrada [ctb] (ORCID:\n    <https://orcid.org/0009-0004-5592-1728>),\n  Jonas Lieth [ctb] (ORCID: <https://orcid.org/0000-0002-3451-3176>)",
    "url": "https://github.com/e-kotov/rJavaEnv,\nhttps://www.ekotov.pro/rJavaEnv/",
    "bug_reports": "https://github.com/e-kotov/rJavaEnv/issues",
    "repository": "https://cran.r-project.org/package=rJavaEnv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rJavaEnv 'Java' Environments for R Projects Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.  "
  },
  {
    "id": 18864,
    "package_name": "rLTP",
    "title": "R Interface to the 'LTP'-Cloud Service",
    "description": "R interface to the 'LTP'-Cloud service for Natural Language Processing\n    in Chinese (http://www.ltp-cloud.com/).",
    "version": "0.1.4",
    "maintainer": "Tong He <hetong007@gmail.com>",
    "author": "Tong He [aut, cre], Oliver Keyes [ctb]",
    "url": "https://github.com/hetong007/rLTP",
    "bug_reports": "https://github.com/hetong007/rLTP/issues",
    "repository": "https://cran.r-project.org/package=rLTP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rLTP R Interface to the 'LTP'-Cloud Service R interface to the 'LTP'-Cloud service for Natural Language Processing\n    in Chinese (http://www.ltp-cloud.com/).  "
  },
  {
    "id": 19215,
    "package_name": "receptiviti",
    "title": "Text Analysis Through the 'Receptiviti' API",
    "description": "Sends texts to the <https://www.receptiviti.com> API to be scored,\n    and facilitates the creation of custom norms and local results databases.",
    "version": "0.2.0",
    "maintainer": "Kent English <kenglish@receptiviti.com>",
    "author": "Receptiviti Inc. [fnd, cph],\n  Kent English [cre],\n  Micah Iserman [aut, ctr]",
    "url": "https://receptiviti.github.io/receptiviti-r/,\nhttps://github.com/Receptiviti/receptiviti-r",
    "bug_reports": "https://github.com/Receptiviti/receptiviti-r/issues",
    "repository": "https://cran.r-project.org/package=receptiviti",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "receptiviti Text Analysis Through the 'Receptiviti' API Sends texts to the <https://www.receptiviti.com> API to be scored,\n    and facilitates the creation of custom norms and local results databases.  "
  },
  {
    "id": 19433,
    "package_name": "revtools",
    "title": "Tools to Support Evidence Synthesis",
    "description": "Researchers commonly need to summarize scientific information, a process known as 'evidence synthesis'. The first stage of a synthesis process (such as a systematic review or meta-analysis) is to download a list of references from academic search engines such as 'Web of Knowledge' or 'Scopus'. The traditional approach to systematic review is then to sort these data manually, first by locating and removing duplicated entries, and then screening to remove irrelevant content by viewing titles and abstracts (in that order). 'revtools' provides interfaces for each of these tasks. An alternative approach, however, is to draw on tools from machine learning to visualise patterns in the corpus. In this case, you can use 'revtools' to render ordinations of text drawn from article titles, keywords and abstracts, and interactively select or exclude individual references, words or topics.",
    "version": "0.4.1",
    "maintainer": "Martin J. Westgate <martinjwestgate@gmail.com>",
    "author": "Martin J. Westgate [aut, cre]",
    "url": "https://revtools.net",
    "bug_reports": "https://github.com/mjwestgate/revtools/issues",
    "repository": "https://cran.r-project.org/package=revtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "revtools Tools to Support Evidence Synthesis Researchers commonly need to summarize scientific information, a process known as 'evidence synthesis'. The first stage of a synthesis process (such as a systematic review or meta-analysis) is to download a list of references from academic search engines such as 'Web of Knowledge' or 'Scopus'. The traditional approach to systematic review is then to sort these data manually, first by locating and removing duplicated entries, and then screening to remove irrelevant content by viewing titles and abstracts (in that order). 'revtools' provides interfaces for each of these tasks. An alternative approach, however, is to draw on tools from machine learning to visualise patterns in the corpus. In this case, you can use 'revtools' to render ordinations of text drawn from article titles, keywords and abstracts, and interactively select or exclude individual references, words or topics.  "
  },
  {
    "id": 19726,
    "package_name": "rollinglda",
    "title": "Construct Consistent Time Series from Textual Data",
    "description": "A rolling version of the Latent Dirichlet Allocation, see Rieger et al. (2021) <doi:10.18653/v1/2021.findings-emnlp.201>. By a sequential approach, it enables the construction of LDA-based time series of topics that are consistent with previous states of LDA models. After an initial modeling, updates can be computed efficiently, allowing for real-time monitoring and detection of events or structural breaks.",
    "version": "0.1.4",
    "maintainer": "Jonas Rieger <jonas.rieger@tu-dortmund.de>",
    "author": "Jonas Rieger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0007-4478>)",
    "url": "https://github.com/JonasRieger/rollinglda",
    "bug_reports": "https://github.com/JonasRieger/rollinglda/issues",
    "repository": "https://cran.r-project.org/package=rollinglda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rollinglda Construct Consistent Time Series from Textual Data A rolling version of the Latent Dirichlet Allocation, see Rieger et al. (2021) <doi:10.18653/v1/2021.findings-emnlp.201>. By a sequential approach, it enables the construction of LDA-based time series of topics that are consistent with previous states of LDA models. After an initial modeling, updates can be computed efficiently, allowing for real-time monitoring and detection of events or structural breaks.  "
  },
  {
    "id": 19910,
    "package_name": "rtiktoken",
    "title": "A Byte-Pair-Encoding (BPE) Tokenizer for OpenAI's Large Language\nModels",
    "description": "A thin wrapper around the tiktoken-rs crate, allowing to encode text into Byte-Pair-Encoding (BPE) tokens and decode tokens back to text. This is useful to understand how Large Language Models (LLMs) perceive text. ",
    "version": "0.0.7",
    "maintainer": "David Zimmermann-Kollenda <david_j_zimmermann@hotmail.com>",
    "author": "David Zimmermann-Kollenda [aut, cre],\n  Roger Zurawicki [aut] (tiktoken-rs Rust library),\n  Authors of the dependent Rust crates [aut] (see AUTHORS file)",
    "url": "https://davzim.github.io/rtiktoken/,\nhttps://github.com/DavZim/rtiktoken/",
    "bug_reports": "https://github.com/DavZim/rtiktoken/issues",
    "repository": "https://cran.r-project.org/package=rtiktoken",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtiktoken A Byte-Pair-Encoding (BPE) Tokenizer for OpenAI's Large Language\nModels A thin wrapper around the tiktoken-rs crate, allowing to encode text into Byte-Pair-Encoding (BPE) tokens and decode tokens back to text. This is useful to understand how Large Language Models (LLMs) perceive text.   "
  },
  {
    "id": 20309,
    "package_name": "seasonalityPlot",
    "title": "Seasonality Variation Plots of Stock Prices and Cryptocurrencies",
    "description": "The price action at any given time is determined by investor \n  sentiment and market conditions. Although there is no established principle, \n  over a long period of time, things often move with a certain periodicity.\n  This is sometimes referred to as anomaly. \n  The seasonPlot() function in this package calculates and visualizes the \n  average value of price movements over a year for any given period. \n  In addition, the monthly increase or decrease in price movement is \n  represented with a colored background. \n  This seasonPlot() function can use the same symbols as the 'quantmod' package \n  (e.g. ^IXIC, ^DJI, SPY, BTC-USD, and ETH-USD etc). ",
    "version": "1.3.1",
    "maintainer": "Satoshi Kume <satoshi.kume.1984@gmail.com>",
    "author": "Satoshi Kume [aut, cre]",
    "url": "https://github.com/kumeS/seasonalityPlot,\nhttps://kumes.github.io/seasonalityPlot/,\nhttps://skume-seasonalityplot.hf.space/__docs__/#/",
    "bug_reports": "https://github.com/kumeS/seasonalityPlot/issues",
    "repository": "https://cran.r-project.org/package=seasonalityPlot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seasonalityPlot Seasonality Variation Plots of Stock Prices and Cryptocurrencies The price action at any given time is determined by investor \n  sentiment and market conditions. Although there is no established principle, \n  over a long period of time, things often move with a certain periodicity.\n  This is sometimes referred to as anomaly. \n  The seasonPlot() function in this package calculates and visualizes the \n  average value of price movements over a year for any given period. \n  In addition, the monthly increase or decrease in price movement is \n  represented with a colored background. \n  This seasonPlot() function can use the same symbols as the 'quantmod' package \n  (e.g. ^IXIC, ^DJI, SPY, BTC-USD, and ETH-USD etc).   "
  },
  {
    "id": 20326,
    "package_name": "seededlda",
    "title": "Seeded Sequential LDA for Topic Modeling",
    "description": "Seeded Sequential LDA can classify sentences of texts into pre-define topics with a small number of seed words (Watanabe & Baturo, 2023) <doi:10.1177/08944393231178605>.\n    Implements Seeded LDA (Lu et al., 2010) <doi:10.1109/ICDMW.2011.125> and Sequential LDA (Du et al., 2012) <doi:10.1007/s10115-011-0425-1> with the distributed LDA algorithm (Newman, et al., 2009) for parallel computing.",
    "version": "1.4.3",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph],\n  Phan Xuan-Hieu [aut, cph] (GibbsLDA++)",
    "url": "https://github.com/koheiw/seededlda,\nhttps://koheiw.github.io/seededlda/",
    "bug_reports": "https://github.com/koheiw/seededlda/issues",
    "repository": "https://cran.r-project.org/package=seededlda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seededlda Seeded Sequential LDA for Topic Modeling Seeded Sequential LDA can classify sentences of texts into pre-define topics with a small number of seed words (Watanabe & Baturo, 2023) <doi:10.1177/08944393231178605>.\n    Implements Seeded LDA (Lu et al., 2010) <doi:10.1109/ICDMW.2011.125> and Sequential LDA (Du et al., 2012) <doi:10.1007/s10115-011-0425-1> with the distributed LDA algorithm (Newman, et al., 2009) for parallel computing.  "
  },
  {
    "id": 20414,
    "package_name": "sentencepiece",
    "title": "Text Tokenization using Byte Pair Encoding and Unigram Modelling",
    "description": "Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. \n    Wraps the 'sentencepiece' library <https://github.com/google/sentencepiece> which provides a language independent tokenizer to split text in words and smaller subword units. \n    The techniques are explained in the paper \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\" by Taku Kudo and John Richardson (2018) <doi:10.18653/v1/D18-2012>.\n    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia using 'word2vec', \n    as described in \"BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages\" by Benjamin Heinzerling and Michael Strube (2018) <http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf>.",
    "version": "0.2.4",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Google Inc. [ctb, cph] (Files at src/sentencepiece/src (Apache License,\n    Version 2.0),\n  The Abseil Authors [ctb, cph] (Files at src/third_party/absl (Apache\n    License, Version 2.0),\n  Google Inc. [ctb, cph] (Files at src/third_party/protobuf-lite (BSD-3\n    License)),\n  Kenton Varda (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: coded_stream.cc, extension_set.cc,\n    generated_message_util.cc, generated_message_util.cc,\n    message_lite.cc, repeated_field.cc, wire_format_lite.cc,\n    zero_copy_stream.cc, zero_copy_stream_impl_lite.cc,\n    google/protobuf/extension_set.h,\n    google/protobuf/generated_message_util.h,\n    google/protobuf/wire_format_lite.h,\n    google/protobuf/wire_format_lite_inl.h,\n    google/protobuf/message_lite.h, google/protobuf/repeated_field.h,\n    google/protobuf/io/coded_stream.h,\n    google/protobuf/io/zero_copy_stream_impl_lite.h,\n    google/protobuf/io/zero_copy_stream.h,\n    google/protobuf/stubs/common.h, google/protobuf/stubs/hash.h,\n    google/protobuf/stubs/once.h, google/protobuf/stubs/once.h.org\n    (BSD-3 License)),\n  Sanjay Ghemawat (Google Inc.) [ctb, cph] (Design of files at\n    src/third_party/protobuf-lite: coded_stream.cc, extension_set.cc,\n    generated_message_util.cc, generated_message_util.cc,\n    message_lite.cc, repeated_field.cc, wire_format_lite.cc,\n    zero_copy_stream.cc, zero_copy_stream_impl_lite.cc,\n    google/protobuf/extension_set.h,\n    google/protobuf/generated_message_util.h,\n    google/protobuf/wire_format_lite.h,\n    google/protobuf/wire_format_lite_inl.h,\n    google/protobuf/message_lite.h, google/protobuf/repeated_field.h,\n    google/protobuf/io/coded_stream.h,\n    google/protobuf/io/zero_copy_stream_impl_lite.h,\n    google/protobuf/io/zero_copy_stream.h (BSD-3 License)),\n  Jeff Dean (Google Inc.) [ctb, cph] (Design of files at\n    src/third_party/protobuf-lite: coded_stream.cc, extension_set.cc,\n    generated_message_util.cc, generated_message_util.cc,\n    message_lite.cc, repeated_field.cc, wire_format_lite.cc,\n    zero_copy_stream.cc, zero_copy_stream_impl_lite.cc,\n    google/protobuf/extension_set.h,\n    google/protobuf/generated_message_util.h,\n    google/protobuf/wire_format_lite.h,\n    google/protobuf/wire_format_lite_inl.h,\n    google/protobuf/message_lite.h, google/protobuf/repeated_field.h,\n    google/protobuf/io/coded_stream.h,\n    google/protobuf/io/zero_copy_stream_impl_lite.h,\n    google/protobuf/io/zero_copy_stream.h (BSD-3 License)),\n  Laszlo Csomor (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: io_win32.cc,\n    google/protobuf/stubs/io_win32.h (BSD-3 License)),\n  Wink Saville (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: message_lite.cc,\n    google/protobuf/wire_format_lite.h,\n    google/protobuf/wire_format_lite_inl.h,\n    google/protobuf/message_lite.h (BSD-3 License)),\n  Jim Meehan (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: structurally_valid.cc (BSD-3\n    License)),\n  Chris Atenasio (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: google/protobuf/wire_format_lite.h\n    (BSD-3 License)),\n  Jason Hsueh (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite:\n    google/protobuf/io/coded_stream_inl.h (BSD-3 License)),\n  Anton Carver (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: google/protobuf/stubs/map_util.h\n    (BSD-3 License)),\n  Maxim Lifantsev (Google Inc.) [ctb, cph] (Files at\n    src/third_party/protobuf-lite: google/protobuf/stubs/mathlimits.h\n    (BSD-3 License)),\n  Susumu Yata [ctb, cph] (Files at src/third_party/darts_clone (BSD-3\n    License),\n  Daisuke Okanohara [ctb, cph] (File src/third_party/esaxx/esa.hxx (MIT\n    License)),\n  Yuta Mori [ctb, cph] (File src/third_party/esaxx/sais.hxx (MIT\n    License)),\n  Benjamin Heinzerling [ctb, cph] (Files\n    data/models/nl.wiki.bpe.vs1000.d25.w2v.txt,\n    data/models/nl.wiki.bpe.vs1000.d25.w2v.bin and\n    data/models/nl.wiki.bpe.vs1000.model (MIT License))",
    "url": "https://github.com/bnosac/sentencepiece",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sentencepiece",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sentencepiece Text Tokenization using Byte Pair Encoding and Unigram Modelling Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. \n    Wraps the 'sentencepiece' library <https://github.com/google/sentencepiece> which provides a language independent tokenizer to split text in words and smaller subword units. \n    The techniques are explained in the paper \"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\" by Taku Kudo and John Richardson (2018) <doi:10.18653/v1/D18-2012>.\n    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia using 'word2vec', \n    as described in \"BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages\" by Benjamin Heinzerling and Michael Strube (2018) <http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf>.  "
  },
  {
    "id": 20415,
    "package_name": "sentiment.ai",
    "title": "Simple Sentiment Analysis Using Deep Learning",
    "description": "Sentiment Analysis via deep learning and gradient boosting models with a lot of the underlying hassle taken care of to make the process as simple as possible. \n  In addition to out-performing traditional, lexicon-based sentiment analysis (see <https://benwiseman.github.io/sentiment.ai/#Benchmarks>),\n  it also allows the user to create embedding vectors for text which can be used in other analyses.\n  GPU acceleration is supported on Windows and Linux.",
    "version": "0.1.1",
    "maintainer": "Ben Wiseman <benjamin.h.wiseman@gmail.com>",
    "author": "Ben Wiseman [cre, aut, ccp],\n  Steven Nydick [aut] (ORCID: <https://orcid.org/0000-0002-2908-1188>),\n  Tristan Wisner [aut],\n  Fiona Lodge [ctb],\n  Yu-Ann Wang [ctb],\n  Veronica Ge [art],\n  Korn Ferry Institute [fnd]",
    "url": "https://benwiseman.github.io/sentiment.ai/,\nhttps://github.com/BenWiseman/sentiment.ai",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sentiment.ai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sentiment.ai Simple Sentiment Analysis Using Deep Learning Sentiment Analysis via deep learning and gradient boosting models with a lot of the underlying hassle taken care of to make the process as simple as possible. \n  In addition to out-performing traditional, lexicon-based sentiment analysis (see <https://benwiseman.github.io/sentiment.ai/#Benchmarks>),\n  it also allows the user to create embedding vectors for text which can be used in other analyses.\n  GPU acceleration is supported on Windows and Linux.  "
  },
  {
    "id": 20416,
    "package_name": "sentimentr",
    "title": "Calculate Text Polarity Sentiment",
    "description": "Calculate text polarity sentiment at the sentence level and\n         optionally aggregate by rows or grouping variable(s).",
    "version": "2.9.0",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker [aut, cre]",
    "url": "https://github.com/trinker/sentimentr",
    "bug_reports": "https://github.com/trinker/sentimentr/issues",
    "repository": "https://cran.r-project.org/package=sentimentr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sentimentr Calculate Text Polarity Sentiment Calculate text polarity sentiment at the sentence level and\n         optionally aggregate by rows or grouping variable(s).  "
  },
  {
    "id": 20417,
    "package_name": "sentometrics",
    "title": "An Integrated Framework for Textual Sentiment Time Series\nAggregation and Prediction",
    "description": "Optimized prediction based on textual sentiment, accounting for the intrinsic challenge that sentiment can be computed and pooled across texts and time in various ways. See Ardia et al. (2021) <doi:10.18637/jss.v099.i02>.",
    "version": "1.0.1",
    "maintainer": "Samuel Borms <borms_sam@hotmail.com>",
    "author": "Samuel Borms [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9533-1870>),\n  David Ardia [aut] (ORCID: <https://orcid.org/0000-0003-2823-782X>),\n  Keven Bluteau [aut] (ORCID: <https://orcid.org/0000-0003-2990-4807>),\n  Kris Boudt [aut] (ORCID: <https://orcid.org/0000-0002-1000-5142>),\n  Jeroen Van Pelt [ctb],\n  Andres Algaba [ctb]",
    "url": "https://sentometrics-research.com/sentometrics/",
    "bug_reports": "https://github.com/SentometricsResearch/sentometrics/issues",
    "repository": "https://cran.r-project.org/package=sentometrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sentometrics An Integrated Framework for Textual Sentiment Time Series\nAggregation and Prediction Optimized prediction based on textual sentiment, accounting for the intrinsic challenge that sentiment can be computed and pooled across texts and time in various ways. See Ardia et al. (2021) <doi:10.18637/jss.v099.i02>.  "
  },
  {
    "id": 20418,
    "package_name": "sentopics",
    "title": "Tools for Joint Sentiment and Topic Analysis of Textual Data",
    "description": "A framework that joins topic modeling and sentiment analysis of\n  textual data. The package implements a fast Gibbs sampling estimation of\n  Latent Dirichlet Allocation (Griffiths and Steyvers (2004)\n  <doi:10.1073/pnas.0307752101>) and Joint Sentiment/Topic Model (Lin, He,\n  Everson and Ruger (2012) <doi:10.1109/TKDE.2011.48>). It offers a variety of\n  helpers and visualizations to analyze the result of topic modeling. The\n  framework also allows enriching topic models with dates and externally\n  computed sentiment measures. A flexible aggregation scheme enables the\n  creation of time series of sentiment or topical proportions from the enriched\n  topic models. Moreover, a novel method jointly aggregates topic proportions\n  and sentiment measures to derive time series of topical sentiment.",
    "version": "0.7.6",
    "maintainer": "Olivier Delmarcelle <delmarcelle.olivier@gmail.com>",
    "author": "Olivier Delmarcelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4347-070X>),\n  Samuel Borms [ctb] (ORCID: <https://orcid.org/0000-0001-9533-1870>),\n  Chengua Lin [cph] (Original JST implementation),\n  Yulan He [cph] (Original JST implementation),\n  Jose Bernardo [cph] (Original JST implementation),\n  David Robinson [cph] (Implementation of reorder_within()),\n  Julia Silge [cph] (Implementation of reorder_within(), ORCID:\n    <https://orcid.org/0000-0002-3671-836X>)",
    "url": "https://github.com/odelmarcelle/sentopics",
    "bug_reports": "https://github.com/odelmarcelle/sentopics/issues",
    "repository": "https://cran.r-project.org/package=sentopics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sentopics Tools for Joint Sentiment and Topic Analysis of Textual Data A framework that joins topic modeling and sentiment analysis of\n  textual data. The package implements a fast Gibbs sampling estimation of\n  Latent Dirichlet Allocation (Griffiths and Steyvers (2004)\n  <doi:10.1073/pnas.0307752101>) and Joint Sentiment/Topic Model (Lin, He,\n  Everson and Ruger (2012) <doi:10.1109/TKDE.2011.48>). It offers a variety of\n  helpers and visualizations to analyze the result of topic modeling. The\n  framework also allows enriching topic models with dates and externally\n  computed sentiment measures. A flexible aggregation scheme enables the\n  creation of time series of sentiment or topical proportions from the enriched\n  topic models. Moreover, a novel method jointly aggregates topic proportions\n  and sentiment measures to derive time series of topical sentiment.  "
  },
  {
    "id": 21011,
    "package_name": "sotu",
    "title": "United States Presidential State of the Union Addresses",
    "description": "The President of the United States is constitutionally obligated to provide\n  a report known as the 'State of the Union'. The report summarizes the current challenges\n  facing the country and the president's upcoming legislative agenda. While historically\n  the State of the Union was often a written document, in recent decades it has always\n  taken the form of an oral address to a joint session of the United States Congress.\n  This package provides the raw text from every such address with the intention of\n  being used for meaningful examples of text analysis in R. The corpus is well suited\n  to the task as it is historically important, includes material intended to be read\n  and material intended to be spoken, and it falls in the public domain. As the corpus\n  spans over two centuries it is also a good test of how well various methods hold up\n  to the idiosyncrasies of historical texts. Associated data about each address, such\n  as the year, president, party, and format, are also included.",
    "version": "1.0.4",
    "maintainer": "Taylor B. Arnold <tarnold2@richmond.edu>",
    "author": "Taylor B. Arnold [aut, cre]",
    "url": "https://github.com/statsmaths/sotu/",
    "bug_reports": "https://github.com/statsmaths/sotu/issues/",
    "repository": "https://cran.r-project.org/package=sotu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sotu United States Presidential State of the Union Addresses The President of the United States is constitutionally obligated to provide\n  a report known as the 'State of the Union'. The report summarizes the current challenges\n  facing the country and the president's upcoming legislative agenda. While historically\n  the State of the Union was often a written document, in recent decades it has always\n  taken the form of an oral address to a joint session of the United States Congress.\n  This package provides the raw text from every such address with the intention of\n  being used for meaningful examples of text analysis in R. The corpus is well suited\n  to the task as it is historically important, includes material intended to be read\n  and material intended to be spoken, and it falls in the public domain. As the corpus\n  spans over two centuries it is also a good test of how well various methods hold up\n  to the idiosyncrasies of historical texts. Associated data about each address, such\n  as the year, president, party, and format, are also included.  "
  },
  {
    "id": 21015,
    "package_name": "sourcetools",
    "title": "Tools for Reading, Tokenizing and Parsing R Code",
    "description": "Tools for the reading and tokenization of R code. The\n    'sourcetools' package provides both an R and C++ interface for the tokenization\n    of R code, and helpers for interacting with the tokenized representation of R\n    code.",
    "version": "0.1.7-1",
    "maintainer": "Kevin Ushey <kevinushey@gmail.com>",
    "author": "Kevin Ushey",
    "url": "",
    "bug_reports": "https://github.com/kevinushey/sourcetools/issues",
    "repository": "https://cran.r-project.org/package=sourcetools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sourcetools Tools for Reading, Tokenizing and Parsing R Code Tools for the reading and tokenization of R code. The\n    'sourcetools' package provides both an R and C++ interface for the tokenization\n    of R code, and helpers for interacting with the tokenized representation of R\n    code.  "
  },
  {
    "id": 21053,
    "package_name": "spacyr",
    "title": "Wrapper to the 'spaCy' 'NLP' Library",
    "description": "An R wrapper to the 'Python' 'spaCy' 'NLP' library,\n    from <https://spacy.io>.",
    "version": "1.3.0",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Akitaka Matsuo [aut] (ORCID: <https://orcid.org/0000-0002-3323-6330>),\n  Johannes Gruber [ctb] (ORCID: <https://orcid.org/0000-0001-9177-1772>),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "https://spacyr.quanteda.io",
    "bug_reports": "https://github.com/quanteda/spacyr/issues",
    "repository": "https://cran.r-project.org/package=spacyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spacyr Wrapper to the 'spaCy' 'NLP' Library An R wrapper to the 'Python' 'spaCy' 'NLP' library,\n    from <https://spacy.io>.  "
  },
  {
    "id": 21187,
    "package_name": "spell.replacer",
    "title": "Probabilistic Spelling Correction in a Character Vector",
    "description": "Automatically replaces \"misspelled\" words in a character vector\n    based on their string distance from a list of words sorted by their frequency\n    in a corpus. The default word list provided in the package comes from \n    the Corpus of Contemporary American English. Uses the Jaro-Winkler distance\n    metric for string similarity as implemented in van der Loo (2014) \n    <doi:10.32614/RJ-2014-011>. The word frequency data is derived from \n    Davies (2008-) \"The Corpus of Contemporary American English (COCA)\" \n    <https://www.english-corpora.org/coca/>.",
    "version": "1.0.1",
    "maintainer": "David Brown <dwb2@andrew.cmu.edu>",
    "author": "David Brown [aut, cre] (ORCID: <https://orcid.org/0000-0001-7745-6354>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spell.replacer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spell.replacer Probabilistic Spelling Correction in a Character Vector Automatically replaces \"misspelled\" words in a character vector\n    based on their string distance from a list of words sorted by their frequency\n    in a corpus. The default word list provided in the package comes from \n    the Corpus of Contemporary American English. Uses the Jaro-Winkler distance\n    metric for string similarity as implemented in van der Loo (2014) \n    <doi:10.32614/RJ-2014-011>. The word frequency data is derived from \n    Davies (2008-) \"The Corpus of Contemporary American English (COCA)\" \n    <https://www.english-corpora.org/coca/>.  "
  },
  {
    "id": 21303,
    "package_name": "srt",
    "title": "Read Subtitle Files as Tabular Data",
    "description": "Read 'SubRip' <https://sourceforge.net/projects/subrip/>\n    subtitle files as data frames for easy text analysis or manipulation.\n    Easily shift numeric timings and export subtitles back into valid\n    'SubRip' timestamp format to sync subtitles and audio.",
    "version": "1.0.4",
    "maintainer": "Kiernan Nicholls <k5cents@gmail.com>",
    "author": "Kiernan Nicholls [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9229-7897>)",
    "url": "https://k5cents.github.io/srt/, https://github.com/k5cents/srt",
    "bug_reports": "https://github.com/k5cents/srt/issues",
    "repository": "https://cran.r-project.org/package=srt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "srt Read Subtitle Files as Tabular Data Read 'SubRip' <https://sourceforge.net/projects/subrip/>\n    subtitle files as data frames for easy text analysis or manipulation.\n    Easily shift numeric timings and export subtitles back into valid\n    'SubRip' timestamp format to sync subtitles and audio.  "
  },
  {
    "id": 21371,
    "package_name": "stanza",
    "title": "'Stanza' - A 'R' NLP Package for Many Human Languages",
    "description": "\n    An interface to the 'Python' package 'stanza' <https://stanfordnlp.github.io/stanza/index.html>.\n    'stanza' is a 'Python' 'NLP' library for many human languages.\n    It contains support for running various accurate natural language processing tools on 60+ languages.",
    "version": "1.0-3",
    "maintainer": "Florian Schwendinger <FlorianSchwendinger@gmx.at>",
    "author": "Kurt Hornik [aut],\n  Florian Schwendinger [aut, cre],\n  Julian Amon [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stanza",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stanza 'Stanza' - A 'R' NLP Package for Many Human Languages \n    An interface to the 'Python' package 'stanza' <https://stanfordnlp.github.io/stanza/index.html>.\n    'stanza' is a 'Python' 'NLP' library for many human languages.\n    It contains support for running various accurate natural language processing tools on 60+ languages.  "
  },
  {
    "id": 21414,
    "package_name": "statlingua",
    "title": "Explain Statistical Output with Large Language Models",
    "description": "Transform complex statistical output into straightforward, understandable, and context-aware natural language descriptions using Large Language Models (LLMs), making complex analyses more accessible to individuals with varying statistical expertise. It relies on the 'ellmer' package to interface with LLM providers including OpenAI <https://openai.com/>, Google AI Studio <https://aistudio.google.com/>, and Anthropic <https://www.anthropic.com/> (API keys are required and managed via 'ellmer').",
    "version": "0.1.0",
    "maintainer": "Brandon M. Greenwell <greenwell.brandon@gmail.com>",
    "author": "Brandon M. Greenwell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8120-0084>)",
    "url": "https://github.com/bgreenwell/statlingua,\nhttps://bgreenwell.github.io/statlingua/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=statlingua",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statlingua Explain Statistical Output with Large Language Models Transform complex statistical output into straightforward, understandable, and context-aware natural language descriptions using Large Language Models (LLMs), making complex analyses more accessible to individuals with varying statistical expertise. It relies on the 'ellmer' package to interface with LLM providers including OpenAI <https://openai.com/>, Google AI Studio <https://aistudio.google.com/>, and Anthropic <https://www.anthropic.com/> (API keys are required and managed via 'ellmer').  "
  },
  {
    "id": 21476,
    "package_name": "stm",
    "title": "Estimation of the Structural Topic Model",
    "description": "The Structural Topic Model (STM) allows researchers \n  to estimate topic models with document-level covariates. \n  The package also includes tools for model selection, visualization,\n  and estimation of topic-covariate regressions. Methods developed in\n  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and \n  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette\n  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>.",
    "version": "1.3.8",
    "maintainer": "Brandon Stewart <bms4@princeton.edu>",
    "author": "Margaret Roberts [aut],\n  Brandon Stewart [aut, cre],\n  Dustin Tingley [aut],\n  Kenneth Benoit [ctb]",
    "url": "http://www.structuraltopicmodel.com/",
    "bug_reports": "https://github.com/bstewart/stm/issues",
    "repository": "https://cran.r-project.org/package=stm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stm Estimation of the Structural Topic Model The Structural Topic Model (STM) allows researchers \n  to estimate topic models with document-level covariates. \n  The package also includes tools for model selection, visualization,\n  and estimation of topic-covariate regressions. Methods developed in\n  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and \n  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette\n  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>.  "
  },
  {
    "id": 21478,
    "package_name": "stmgui",
    "title": "Shiny Application for Creating STM Models",
    "description": "Provides an application that acts as a GUI for the 'stm' text analysis package.",
    "version": "0.1.6",
    "maintainer": "Dan Zangri <dzangri@gmail.com>",
    "author": "Dan Zangri [aut, cre], Dustin Tingley [ctb], Brandon Stewart [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stmgui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stmgui Shiny Application for Creating STM Models Provides an application that acts as a GUI for the 'stm' text analysis package.  "
  },
  {
    "id": 21479,
    "package_name": "stminsights",
    "title": "A 'Shiny' Application for Inspecting Structural Topic Models",
    "description": "This app enables interactive validation, interpretation and visualization of structural topic models from the 'stm' package by Roberts and others (2014) <doi:10.1111/ajps.12103>. It also includes helper functions for model diagnostics and extracting data from effect estimates.",
    "version": "0.4.3",
    "maintainer": "Carsten Schwemmer <c.schwem2er@gmail.com>",
    "author": "Carsten Schwemmer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9084-946X>),\n  Jonne Guyt [ctb]",
    "url": "https://github.com/cschwem2er/stminsights",
    "bug_reports": "https://github.com/cschwem2er/stminsights/issues",
    "repository": "https://cran.r-project.org/package=stminsights",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stminsights A 'Shiny' Application for Inspecting Structural Topic Models This app enables interactive validation, interpretation and visualization of structural topic models from the 'stm' package by Roberts and others (2014) <doi:10.1111/ajps.12103>. It also includes helper functions for model diagnostics and extracting data from effect estimates.  "
  },
  {
    "id": 21495,
    "package_name": "stopwords",
    "title": "Multilingual Stopword Lists",
    "description": "Provides multiple sources of stopwords, for use in text analysis and natural language processing.",
    "version": "2.3",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [aut, cre],\n  David Muhr [aut],\n  Kohei Watanabe [aut]",
    "url": "https://github.com/quanteda/stopwords",
    "bug_reports": "https://github.com/quanteda/stopwords/issues",
    "repository": "https://cran.r-project.org/package=stopwords",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stopwords Multilingual Stopword Lists Provides multiple sources of stopwords, for use in text analysis and natural language processing.  "
  },
  {
    "id": 21542,
    "package_name": "stringi",
    "title": "Fast and Portable Character String Processing Facilities",
    "description": "A collection of character string/text/natural language\n    processing tools for pattern searching (e.g., with 'Java'-like regular\n    expressions or the 'Unicode' collation algorithm), random string generation,\n    case mapping, string transliteration, concatenation, sorting, padding,\n    wrapping, Unicode normalisation, date-time formatting and parsing,\n    and many more. They are fast, consistent, convenient, and -\n    thanks to 'ICU' (International Components for Unicode) -\n    portable across all locales and platforms. Documentation about 'stringi' is\n    provided via its website at <https://stringi.gagolewski.com/> and\n    the paper by Gagolewski (2022, <doi:10.18637/jss.v103.i02>).",
    "version": "1.8.7",
    "maintainer": "Marek Gagolewski <marek@gagolewski.com>",
    "author": "Marek Gagolewski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0637-6028>),\n  Bartek Tartanus [ctb],\n  Unicode, Inc. and others [ctb] (ICU4C source code, Unicode Character\n    Database)",
    "url": "https://stringi.gagolewski.com/,\nhttps://github.com/gagolews/stringi, https://icu.unicode.org/",
    "bug_reports": "https://github.com/gagolews/stringi/issues",
    "repository": "https://cran.r-project.org/package=stringi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stringi Fast and Portable Character String Processing Facilities A collection of character string/text/natural language\n    processing tools for pattern searching (e.g., with 'Java'-like regular\n    expressions or the 'Unicode' collation algorithm), random string generation,\n    case mapping, string transliteration, concatenation, sorting, padding,\n    wrapping, Unicode normalisation, date-time formatting and parsing,\n    and many more. They are fast, consistent, convenient, and -\n    thanks to 'ICU' (International Components for Unicode) -\n    portable across all locales and platforms. Documentation about 'stringi' is\n    provided via its website at <https://stringi.gagolewski.com/> and\n    the paper by Gagolewski (2022, <doi:10.18637/jss.v103.i02>).  "
  },
  {
    "id": 21545,
    "package_name": "stringx",
    "title": "Replacements for Base String Functions Powered by 'stringi'",
    "description": "English is the native language for only 5% of the World population.\n    Also, only 17% of us can understand this text. Moreover, the Latin alphabet\n    is the main one for merely 36% of the total.\n    The early computer era, now a very long time ago, was dominated by the US.\n    Due to the proliferation of the internet, smartphones, social media, and\n    other technologies and communication platforms, this is no longer the case.\n    This package replaces base R string functions (such as grep(),\n    tolower(), sprintf(), and strptime()) with ones that fully\n    support the Unicode standards related to natural language and\n    date-time processing. It also fixes some long-standing inconsistencies,\n    and introduces some new, useful features.\n    Thanks to 'ICU' (International Components for Unicode) and 'stringi',\n    they are fast, reliable, and portable across different platforms.",
    "version": "0.2.9",
    "maintainer": "Marek Gagolewski <marek@gagolewski.com>",
    "author": "Marek Gagolewski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0637-6028>)",
    "url": "https://stringx.gagolewski.com/,\nhttps://github.com/gagolews/stringx",
    "bug_reports": "https://github.com/gagolews/stringx/issues",
    "repository": "https://cran.r-project.org/package=stringx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stringx Replacements for Base String Functions Powered by 'stringi' English is the native language for only 5% of the World population.\n    Also, only 17% of us can understand this text. Moreover, the Latin alphabet\n    is the main one for merely 36% of the total.\n    The early computer era, now a very long time ago, was dominated by the US.\n    Due to the proliferation of the internet, smartphones, social media, and\n    other technologies and communication platforms, this is no longer the case.\n    This package replaces base R string functions (such as grep(),\n    tolower(), sprintf(), and strptime()) with ones that fully\n    support the Unicode standards related to natural language and\n    date-time processing. It also fixes some long-standing inconsistencies,\n    and introduces some new, useful features.\n    Thanks to 'ICU' (International Components for Unicode) and 'stringi',\n    they are fast, reliable, and portable across different platforms.  "
  },
  {
    "id": 21556,
    "package_name": "sts",
    "title": "Estimation of the Structural Topic and Sentiment-Discourse Model\nfor Text Analysis",
    "description": "The Structural Topic and Sentiment-Discourse (STS) model allows researchers to estimate topic models with document-level metadata that determines both topic prevalence and sentiment-discourse. The sentiment-discourse is modeled as a document-level latent variable for each topic that modulates the word frequency within a topic. These latent topic sentiment-discourse variables are controlled by the document-level metadata. The STS model can be useful for regression analysis with text data in addition to topic modeling\u2019s traditional use of descriptive analysis. The method was developed in Chen and Mankad (2024) <doi:10.1287/mnsc.2022.00261>. ",
    "version": "1.4",
    "maintainer": "Shawn Mankad <smankad@ncsu.edu>",
    "author": "Shawn Mankad [aut, cre],\n  Li Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sts Estimation of the Structural Topic and Sentiment-Discourse Model\nfor Text Analysis The Structural Topic and Sentiment-Discourse (STS) model allows researchers to estimate topic models with document-level metadata that determines both topic prevalence and sentiment-discourse. The sentiment-discourse is modeled as a document-level latent variable for each topic that modulates the word frequency within a topic. These latent topic sentiment-discourse variables are controlled by the document-level metadata. The STS model can be useful for regression analysis with text data in addition to topic modeling\u2019s traditional use of descriptive analysis. The method was developed in Chen and Mankad (2024) <doi:10.1287/mnsc.2022.00261>.   "
  },
  {
    "id": 21561,
    "package_name": "stylest2",
    "title": "Estimating Speakers of Texts",
    "description": "Estimates the authors or speakers of texts. Methods developed in Huang, Perry, and Spirling (2020) <doi:10.1017/pan.2019.49>. The model is built on a Bayesian framework in which the distinctiveness of each speaker is defined by how different, on average, the speaker's terms are to everyone else in the corpus of texts. An optional cross-validation method is implemented to select the subset of terms that generate the most accurate speaker predictions. Once a set of terms is selected, the model can be estimated. Speaker distinctiveness and term influence can be recovered from parameters in the model using package functions. Once fitted, the model can be used to predict authorship of new texts.",
    "version": "0.1",
    "maintainer": "Christian Baehr <cbaehr@princeton.edu>",
    "author": "Christian Baehr [aut, cre, cph],\n  Arthur Spirling [aut, cph],\n  Leslie Huang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stylest2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stylest2 Estimating Speakers of Texts Estimates the authors or speakers of texts. Methods developed in Huang, Perry, and Spirling (2020) <doi:10.1017/pan.2019.49>. The model is built on a Bayesian framework in which the distinctiveness of each speaker is defined by how different, on average, the speaker's terms are to everyone else in the corpus of texts. An optional cross-validation method is implemented to select the subset of terms that generate the most accurate speaker predictions. Once a set of terms is selected, the model can be estimated. Speaker distinctiveness and term influence can be recovered from parameters in the model using package functions. Once fitted, the model can be used to predict authorship of new texts.  "
  },
  {
    "id": 21596,
    "package_name": "sumup",
    "title": "Utilizing Automated Text Analysis to Support Interpretation of\nNarrative Feedback",
    "description": "Combine topic modeling and sentiment analysis to identify individual students' gaps, and highlight their strengths and weaknesses across predefined competency domains and professional activities.",
    "version": "1.0.0",
    "maintainer": "Joyce Moonen - van Loon <j.moonen@maastrichtuniversity.nl>",
    "author": "Joyce Moonen - van Loon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8883-8822>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sumup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sumup Utilizing Automated Text Analysis to Support Interpretation of\nNarrative Feedback Combine topic modeling and sentiment analysis to identify individual students' gaps, and highlight their strengths and weaknesses across predefined competency domains and professional activities.  "
  },
  {
    "id": 21621,
    "package_name": "sureLDA",
    "title": "A Novel Multi-Disease Automated Phenotyping Method for the EHR",
    "description": "A statistical learning method to simultaneously predict a range of target phenotypes using codified and natural language processing (NLP)-derived Electronic Health Record (EHR) data. See Ahuja et al (2020) JAMIA <doi:10.1093/jamia/ocaa079> for details.",
    "version": "0.1.0-1",
    "maintainer": "Yuri Ahuja <Yuri_Ahuja@hms.harvard.edu>",
    "author": "Yuri Ahuja [aut, cre],\n  Tianxi Cai [aut],\n  PARSE LTD [aut]",
    "url": "https://github.com/celehs/sureLDA",
    "bug_reports": "https://github.com/celehs/sureLDA/issues",
    "repository": "https://cran.r-project.org/package=sureLDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sureLDA A Novel Multi-Disease Automated Phenotyping Method for the EHR A statistical learning method to simultaneously predict a range of target phenotypes using codified and natural language processing (NLP)-derived Electronic Health Record (EHR) data. See Ahuja et al (2020) JAMIA <doi:10.1093/jamia/ocaa079> for details.  "
  },
  {
    "id": 21737,
    "package_name": "sweater",
    "title": "Speedy Word Embedding Association Test and Extras Using R",
    "description": "Conduct various tests for evaluating implicit biases in word embeddings: Word Embedding Association Test (Caliskan et al., 2017), <doi:10.1126/science.aal4230>, Relative Norm Distance (Garg et al., 2018), <doi:10.1073/pnas.1720347115>, Mean Average Cosine Similarity (Mazini et al., 2019) <arXiv:1904.04047>, SemAxis (An et al., 2018) <arXiv:1806.05521>, Relative Negative Sentiment Bias (Sweeney & Najafian, 2019) <doi:10.18653/v1/P19-1162>, and Embedding Coherence Test (Dev & Phillips, 2019) <arXiv:1901.07656>.",
    "version": "0.1.8",
    "maintainer": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "author": "Chung-hong Chan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6232-7530>)",
    "url": "https://github.com/gesistsa/sweater",
    "bug_reports": "https://github.com/gesistsa/sweater/issues",
    "repository": "https://cran.r-project.org/package=sweater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sweater Speedy Word Embedding Association Test and Extras Using R Conduct various tests for evaluating implicit biases in word embeddings: Word Embedding Association Test (Caliskan et al., 2017), <doi:10.1126/science.aal4230>, Relative Norm Distance (Garg et al., 2018), <doi:10.1073/pnas.1720347115>, Mean Average Cosine Similarity (Mazini et al., 2019) <arXiv:1904.04047>, SemAxis (An et al., 2018) <arXiv:1806.05521>, Relative Negative Sentiment Bias (Sweeney & Najafian, 2019) <doi:10.18653/v1/P19-1162>, and Embedding Coherence Test (Dev & Phillips, 2019) <arXiv:1901.07656>.  "
  },
  {
    "id": 21754,
    "package_name": "sylly",
    "title": "Hyphenation and Syllable Counting for Text Analysis",
    "description": "Provides the hyphenation algorithm used for 'TeX'/'LaTeX' and similar software, as proposed by Liang (1983, <https://tug.org/docs/liang/>). Mainly contains the\n                    function hyphen() to be used for hyphenation/syllable counting of text objects. It was originally developed for and part of the 'koRpus' package, but later\n                    released as a separate package so it's lighter to have this particular functionality available for other packages. Support for various languages needs be added\n                    on-the-fly or by plugin packages (<https://undocumeantit.github.io/repos/>); this package does not include any language specific data. Due to some restrictions\n                    on CRAN, the full package sources are only available from the project homepage. To ask for help, report bugs, request features, or discuss the development of\n                    the package, please subscribe to the koRpus-dev mailing list (<http://korpusml.reaktanz.de>).",
    "version": "0.1-6",
    "maintainer": "Meik Michalke <meik.michalke@hhu.de>",
    "author": "Meik Michalke [aut, cre]",
    "url": "https://reaktanz.de/?c=hacking&s=sylly",
    "bug_reports": "https://github.com/unDocUMeantIt/sylly/issues",
    "repository": "https://cran.r-project.org/package=sylly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sylly Hyphenation and Syllable Counting for Text Analysis Provides the hyphenation algorithm used for 'TeX'/'LaTeX' and similar software, as proposed by Liang (1983, <https://tug.org/docs/liang/>). Mainly contains the\n                    function hyphen() to be used for hyphenation/syllable counting of text objects. It was originally developed for and part of the 'koRpus' package, but later\n                    released as a separate package so it's lighter to have this particular functionality available for other packages. Support for various languages needs be added\n                    on-the-fly or by plugin packages (<https://undocumeantit.github.io/repos/>); this package does not include any language specific data. Due to some restrictions\n                    on CRAN, the full package sources are only available from the project homepage. To ask for help, report bugs, request features, or discuss the development of\n                    the package, please subscribe to the koRpus-dev mailing list (<http://korpusml.reaktanz.de>).  "
  },
  {
    "id": 21786,
    "package_name": "syuzhet",
    "title": "Extracts Sentiment and Sentiment-Derived Plot Arcs from Text",
    "description": "Extracts sentiment and sentiment-derived plot arcs\n    from text using a variety of sentiment dictionaries conveniently\n    packaged for consumption by R users.  Implemented dictionaries include\n    \"syuzhet\" (default) developed in the Nebraska Literary Lab\n    \"afinn\" developed by Finn \u00c5rup Nielsen, \"bing\" developed by Minqing Hu\n    and Bing Liu, and \"nrc\" developed by Mohammad, Saif M. and Turney, Peter D.\n    Applicable references are available in README.md and in the documentation\n    for the \"get_sentiment\" function.  The package also provides a hack for\n    implementing Stanford's coreNLP sentiment parser. The package provides\n    several methods for plot arc normalization.",
    "version": "1.0.7",
    "maintainer": "Matthew Jockers <mjockers@gmail.com>",
    "author": "Matthew Jockers [aut, cre]",
    "url": "https://github.com/mjockers/syuzhet",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=syuzhet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "syuzhet Extracts Sentiment and Sentiment-Derived Plot Arcs from Text Extracts sentiment and sentiment-derived plot arcs\n    from text using a variety of sentiment dictionaries conveniently\n    packaged for consumption by R users.  Implemented dictionaries include\n    \"syuzhet\" (default) developed in the Nebraska Literary Lab\n    \"afinn\" developed by Finn \u00c5rup Nielsen, \"bing\" developed by Minqing Hu\n    and Bing Liu, and \"nrc\" developed by Mohammad, Saif M. and Turney, Peter D.\n    Applicable references are available in README.md and in the documentation\n    for the \"get_sentiment\" function.  The package also provides a hack for\n    implementing Stanford's coreNLP sentiment parser. The package provides\n    several methods for plot arc normalization.  "
  },
  {
    "id": 21848,
    "package_name": "tardis",
    "title": "Text Analysis with Rules and Dictionaries for Inferring\nSentiment",
    "description": "Measure text's sentiment with dictionaries and simple rules covering\n    negations and modifiers. User-supplied dictionaries are supported, including\n    Unicode emojis and multi-word tokens, so this package can also be used to\n    study constructs beyond sentiment.",
    "version": "0.1.5",
    "maintainer": "Christopher Belanger <christopher.a.belanger@gmail.com>",
    "author": "Christopher Belanger [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2070-5721>)",
    "url": "https://github.com/chris31415926535/tardis",
    "bug_reports": "https://github.com/chris31415926535/tardis/issues",
    "repository": "https://cran.r-project.org/package=tardis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tardis Text Analysis with Rules and Dictionaries for Inferring\nSentiment Measure text's sentiment with dictionaries and simple rules covering\n    negations and modifiers. User-supplied dictionaries are supported, including\n    Unicode emojis and multi-word tokens, so this package can also be used to\n    study constructs beyond sentiment.  "
  },
  {
    "id": 21857,
    "package_name": "tau",
    "title": "Text Analysis Utilities",
    "description": "Utilities for text analysis.",
    "version": "0.0-26",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Christian Buchta [aut],\n  Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  Ingo Feinerer [aut],\n  David Meyer [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tau",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tau Text Analysis Utilities Utilities for text analysis.  "
  },
  {
    "id": 21879,
    "package_name": "tcv",
    "title": "Determining the Number of Factors in Poisson Factor Models via\nThinning Cross-Validation",
    "description": "Implements methods for selecting the number of factors in Poisson\n  factor models, with a primary focus on Thinning Cross-Validation (TCV). The\n  TCV method is based on the 'data thinning' technique, which probabilistically\n  partitions each count observation into training and test sets while preserving\n  the underlying factor structure. The Poisson factor model is then fit on the\n  training set, and model selection is performed by comparing predictive\n  performance on the test set. This toolkit is designed for researchers working\n  with high-dimensional count data in fields such as genomics, text mining, and\n  social sciences. The data thinning methodology is detailed in Dharamshi et al.\n  (2025) <doi:10.1080/01621459.2024.2353948> and Wang et al. (2025)\n  <doi:10.1080/01621459.2025.2546577>.",
    "version": "0.1.0",
    "maintainer": "Zhijing Wang <wangzhijing@sjtu.edu.cn>",
    "author": "Zhijing Wang [aut, cre],\n  Heng Peng [aut],\n  Peirong Xu [aut]",
    "url": "https://github.com/Wangzhijingwzj/tcv",
    "bug_reports": "https://github.com/Wangzhijingwzj/tcv/issues",
    "repository": "https://cran.r-project.org/package=tcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tcv Determining the Number of Factors in Poisson Factor Models via\nThinning Cross-Validation Implements methods for selecting the number of factors in Poisson\n  factor models, with a primary focus on Thinning Cross-Validation (TCV). The\n  TCV method is based on the 'data thinning' technique, which probabilistically\n  partitions each count observation into training and test sets while preserving\n  the underlying factor structure. The Poisson factor model is then fit on the\n  training set, and model selection is performed by comparing predictive\n  performance on the test set. This toolkit is designed for researchers working\n  with high-dimensional count data in fields such as genomics, text mining, and\n  social sciences. The data thinning methodology is detailed in Dharamshi et al.\n  (2025) <doi:10.1080/01621459.2024.2353948> and Wang et al. (2025)\n  <doi:10.1080/01621459.2025.2546577>.  "
  },
  {
    "id": 21969,
    "package_name": "text",
    "title": "Analyses of Text using Transformers Models from HuggingFace,\nNatural Language Processing and Machine Learning",
    "description": "Link R with Transformers from Hugging Face to transform text variables to word embeddings; where the word embeddings are used to statistically test the mean difference between set of texts, compute semantic similarity scores between texts, predict numerical variables, and visual statistically significant words according to various dimensions etc. For more information see  <https://www.r-text.org>.",
    "version": "1.7.0",
    "maintainer": "Oscar Kjell <oscar.kjell@psy.lu.se>",
    "author": "Oscar Kjell [aut, cre] (ORCID: <https://orcid.org/0000-0002-2728-6278>),\n  Salvatore Giorgi [aut] (ORCID: <https://orcid.org/0000-0001-7381-6295>),\n  Andrew Schwartz [aut] (ORCID: <https://orcid.org/0000-0002-6383-3339>)",
    "url": "https://r-text.org/, https://github.com/OscarKjell/text/",
    "bug_reports": "https://github.com/OscarKjell/text/issues/",
    "repository": "https://cran.r-project.org/package=text",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text Analyses of Text using Transformers Models from HuggingFace,\nNatural Language Processing and Machine Learning Link R with Transformers from Hugging Face to transform text variables to word embeddings; where the word embeddings are used to statistically test the mean difference between set of texts, compute semantic similarity scores between texts, predict numerical variables, and visual statistically significant words according to various dimensions etc. For more information see  <https://www.r-text.org>.  "
  },
  {
    "id": 21971,
    "package_name": "text2emotion",
    "title": "Emotion Analysis and Emoji Mapping for Text",
    "description": "Allows users to analyze text and classify emotions\n    such as happiness, sadness, anger, fear, and neutrality.\n    It combines text preprocessing, TF-IDF (Term Frequency-Inverse Document Frequency) \n    feature extraction, and Random Forest classification to predict emotions\n    and map them to corresponding emojis for enhanced sentiment visualization.",
    "version": "0.1.0",
    "maintainer": "Fangyi Wang <123090550@link.cuhk.edu.cn>",
    "author": "Yusong Zhao [aut],\n  Fangyi Wang [aut, cre],\n  Zisheng Qu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=text2emotion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2emotion Emotion Analysis and Emoji Mapping for Text Allows users to analyze text and classify emotions\n    such as happiness, sadness, anger, fear, and neutrality.\n    It combines text preprocessing, TF-IDF (Term Frequency-Inverse Document Frequency) \n    feature extraction, and Random Forest classification to predict emotions\n    and map them to corresponding emojis for enhanced sentiment visualization.  "
  },
  {
    "id": 21972,
    "package_name": "text2map",
    "title": "R Tools for Text Matrices, Embeddings, and Networks",
    "description": "This is a collection of functions optimized for working with\n             with various kinds of text matrices. Focusing on \n             the text matrix as the primary object - represented \n             either as a base R dense matrix or a 'Matrix' package sparse \n             matrix - allows for a consistent and intuitive interface \n             that stays close to the underlying mathematical foundation \n             of computational text analysis. In particular, the package\n             includes functions for working with word embeddings, \n             text networks, and document-term matrices. Methods developed in \n             Stoltz and Taylor (2019) <doi:10.1007/s42001-019-00048-6>, \n             Taylor and Stoltz (2020) <doi:10.1007/s42001-020-00075-8>, \n             Taylor and Stoltz (2020) <doi:10.15195/v7.a23>, and\n             Stoltz and Taylor (2021) <doi:10.1016/j.poetic.2021.101567>.",
    "version": "0.2.0",
    "maintainer": "Dustin Stoltz <dss219@lehigh.edu>",
    "author": "Dustin Stoltz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4774-0765>),\n  Marshall Taylor [aut] (ORCID: <https://orcid.org/0000-0002-7440-0723>)",
    "url": "https://gitlab.com/culturalcartography/text2map",
    "bug_reports": "https://gitlab.com/culturalcartography/text2map/-/issues",
    "repository": "https://cran.r-project.org/package=text2map",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2map R Tools for Text Matrices, Embeddings, and Networks This is a collection of functions optimized for working with\n             with various kinds of text matrices. Focusing on \n             the text matrix as the primary object - represented \n             either as a base R dense matrix or a 'Matrix' package sparse \n             matrix - allows for a consistent and intuitive interface \n             that stays close to the underlying mathematical foundation \n             of computational text analysis. In particular, the package\n             includes functions for working with word embeddings, \n             text networks, and document-term matrices. Methods developed in \n             Stoltz and Taylor (2019) <doi:10.1007/s42001-019-00048-6>, \n             Taylor and Stoltz (2020) <doi:10.1007/s42001-020-00075-8>, \n             Taylor and Stoltz (2020) <doi:10.15195/v7.a23>, and\n             Stoltz and Taylor (2021) <doi:10.1016/j.poetic.2021.101567>.  "
  },
  {
    "id": 21976,
    "package_name": "text2vec",
    "title": "Modern Text Mining Framework for R",
    "description": "Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines.",
    "version": "0.6.6",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "author": "Dmitriy Selivanov [aut, cre, cph],\n  Manuel Bickel [aut, cph] (Coherence measures for topic models),\n  Qing Wang [aut, cph] (Author of the WaprLDA C++ code)",
    "url": "http://text2vec.org",
    "bug_reports": "https://github.com/dselivanov/text2vec/issues",
    "repository": "https://cran.r-project.org/package=text2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2vec Modern Text Mining Framework for R Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines.  "
  },
  {
    "id": 21979,
    "package_name": "textTinyR",
    "title": "Text Processing for Small or Big Data Files",
    "description": "It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.",
    "version": "1.1.8",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>)",
    "url": "https://github.com/mlampros/textTinyR",
    "bug_reports": "https://github.com/mlampros/textTinyR/issues",
    "repository": "https://cran.r-project.org/package=textTinyR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textTinyR Text Processing for Small or Big Data Files It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.  "
  },
  {
    "id": 21980,
    "package_name": "textTools",
    "title": "Functions for Text Cleansing and Text Analysis",
    "description": "A framework for text cleansing and analysis. Conveniently prepare and process large amounts of text for analysis. \n  Includes various metrics for word counts/frequencies that scale efficiently. Quickly \n  analyze large amounts of text data using a text.table (a data.table created with one word (or unit of text analysis) per row, similar to the tidytext format). \n  Offers flexibility to efficiently work with text data stored in vectors as well as text data formatted as a text.table.",
    "version": "0.1.0",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textTools Functions for Text Cleansing and Text Analysis A framework for text cleansing and analysis. Conveniently prepare and process large amounts of text for analysis. \n  Includes various metrics for word counts/frequencies that scale efficiently. Quickly \n  analyze large amounts of text data using a text.table (a data.table created with one word (or unit of text analysis) per row, similar to the tidytext format). \n  Offers flexibility to efficiently work with text data stored in vectors as well as text data formatted as a text.table.  "
  },
  {
    "id": 21985,
    "package_name": "textdata",
    "title": "Download and Load Various Text Datasets",
    "description": "Provides a framework to download, parse, and store text\n    datasets on the disk and load them when needed. Includes various\n    sentiment lexicons and labeled text data sets for classification and\n    analysis.",
    "version": "0.4.5",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0679-1945>),\n  Julia Silge [ctb] (ORCID: <https://orcid.org/0000-0002-3671-836X>)",
    "url": "https://emilhvitfeldt.github.io/textdata/,\nhttps://github.com/EmilHvitfeldt/textdata",
    "bug_reports": "https://github.com/EmilHvitfeldt/textdata/issues",
    "repository": "https://cran.r-project.org/package=textdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textdata Download and Load Various Text Datasets Provides a framework to download, parse, and store text\n    datasets on the disk and load them when needed. Includes various\n    sentiment lexicons and labeled text data sets for classification and\n    analysis.  "
  },
  {
    "id": 21986,
    "package_name": "texteffect",
    "title": "Discovering Latent Treatments in Text Corpora and Estimating\nTheir Causal Effects",
    "description": "Implements the approach described in Fong and Grimmer (2016) <https://aclweb.org/anthology/P/P16/P16-1151.pdf> for \n\tautomatically discovering latent treatments from a corpus and estimating the average marginal component effect (AMCE) of \n\teach treatment.  The data is divided into a training and test set.  The supervised Indian Buffet Process (sibp) is used \n\tto discover latent treatments in the training set.  The fitted model is then applied to the test set to infer the values \n\tof the latent treatments in the test set.  Finally, Y is regressed on the latent treatments in the test set to estimate \n\tthe causal effect of each treatment.",
    "version": "0.3",
    "maintainer": "Christian Fong <christianfong@stanford.edu>",
    "author": "Christian Fong <christianfong@stanford.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=texteffect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "texteffect Discovering Latent Treatments in Text Corpora and Estimating\nTheir Causal Effects Implements the approach described in Fong and Grimmer (2016) <https://aclweb.org/anthology/P/P16/P16-1151.pdf> for \n\tautomatically discovering latent treatments from a corpus and estimating the average marginal component effect (AMCE) of \n\teach treatment.  The data is divided into a training and test set.  The supervised Indian Buffet Process (sibp) is used \n\tto discover latent treatments in the training set.  The fitted model is then applied to the test set to infer the values \n\tof the latent treatments in the test set.  Finally, Y is regressed on the latent treatments in the test set to estimate \n\tthe causal effect of each treatment.  "
  },
  {
    "id": 21987,
    "package_name": "texter",
    "title": "An Easy Text and Sentiment Analysis Library",
    "description": "Implement text and sentiment analysis with 'texter'. \n             Generate sentiment scores on text data and also visualize sentiments.\n             'texter' allows you to quickly generate insights on your data.\n             It includes support for lexicons such as 'NRC' and 'Bing'.",
    "version": "0.1.9",
    "maintainer": "Simi Kafaru <kafarusimileoluwa@gmail.com>",
    "author": "Simi Kafaru [aut, cre]",
    "url": "https://github.com/simmieyungie/texter",
    "bug_reports": "https://github.com/simmieyungie/texter/issues",
    "repository": "https://cran.r-project.org/package=texter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "texter An Easy Text and Sentiment Analysis Library Implement text and sentiment analysis with 'texter'. \n             Generate sentiment scores on text data and also visualize sentiments.\n             'texter' allows you to quickly generate insights on your data.\n             It includes support for lexicons such as 'NRC' and 'Bing'.  "
  },
  {
    "id": 21988,
    "package_name": "textir",
    "title": "Inverse Regression for Text Analysis",
    "description": "Multinomial (inverse) regression inference for text documents and associated attributes.  For details see: Taddy (2013 JASA) Multinomial Inverse Regression for Text Analysis <arXiv:1012.2098> and Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>. A minimalist partial least squares routine is also included.  Note that the topic modeling capability of earlier 'textir' is now a separate package, 'maptpx'.",
    "version": "2.0-5",
    "maintainer": "Matt Taddy <mataddy@gmail.com>",
    "author": "Matt Taddy <mataddy@gmail.com>",
    "url": "http://taddylab.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textir Inverse Regression for Text Analysis Multinomial (inverse) regression inference for text documents and associated attributes.  For details see: Taddy (2013 JASA) Multinomial Inverse Regression for Text Analysis <arXiv:1012.2098> and Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>. A minimalist partial least squares routine is also included.  Note that the topic modeling capability of earlier 'textir' is now a separate package, 'maptpx'.  "
  },
  {
    "id": 21989,
    "package_name": "textmineR",
    "title": "Functions for Text Mining and Topic Modeling",
    "description": "An aid for text mining in R, with a syntax that\n    should be familiar to experienced R users. Provides a wrapper for several \n    topic models that take similarly-formatted input and give similarly-formatted\n    output. Has additional functionality for analyzing and diagnostics for\n    topic models.",
    "version": "3.0.6",
    "maintainer": "Tommy Jones <jones.thos.w@gmail.com>",
    "author": "Tommy Jones [aut, cre],\n  William Doane [ctb],\n  Mattias Attbom [ctb]",
    "url": "https://www.rtextminer.com/",
    "bug_reports": "https://github.com/TommyJones/textmineR/issues",
    "repository": "https://cran.r-project.org/package=textmineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textmineR Functions for Text Mining and Topic Modeling An aid for text mining in R, with a syntax that\n    should be familiar to experienced R users. Provides a wrapper for several \n    topic models that take similarly-formatted input and give similarly-formatted\n    output. Has additional functionality for analyzing and diagnostics for\n    topic models.  "
  },
  {
    "id": 21992,
    "package_name": "textpress",
    "title": "A Lightweight and Versatile NLP Toolkit",
    "description": "A simple Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the 'HuggingFace API' <https://huggingface.co/docs/api-inference/index>.",
    "version": "1.0.0",
    "maintainer": "Jason Timm <JaTimm@salud.unm.edu>",
    "author": "Jason Timm [aut, cre]",
    "url": "https://github.com/jaytimm/textpress,\nhttps://jaytimm.github.io/textpress/",
    "bug_reports": "https://github.com/jaytimm/textpress/issues",
    "repository": "https://cran.r-project.org/package=textpress",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textpress A Lightweight and Versatile NLP Toolkit A simple Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the 'HuggingFace API' <https://huggingface.co/docs/api-inference/index>.  "
  },
  {
    "id": 22130,
    "package_name": "tidytext",
    "title": "Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools",
    "description": "Using tidy data principles can make many text mining tasks\n    easier, more effective, and consistent with tools already in wide use.\n    Much of the infrastructure needed for text mining with tidy data\n    frames already exists in packages like 'dplyr', 'broom', 'tidyr', and\n    'ggplot2'. In this package, we provide functions and supporting data\n    sets to allow conversion of text to and from tidy formats, and to\n    switch seamlessly between tidy tools and existing text mining\n    packages.",
    "version": "0.4.3",
    "maintainer": "Julia Silge <julia.silge@gmail.com>",
    "author": "Gabriela De Queiroz [ctb],\n  Colin Fay [ctb] (ORCID: <https://orcid.org/0000-0001-7343-1846>),\n  Emil Hvitfeldt [ctb],\n  Os Keyes [ctb] (ORCID: <https://orcid.org/0000-0001-5196-609X>),\n  Kanishka Misra [ctb],\n  Tim Mastny [ctb],\n  Jeff Erickson [ctb],\n  David Robinson [aut],\n  Julia Silge [aut, cre] (ORCID: <https://orcid.org/0000-0002-3671-836X>)",
    "url": "https://juliasilge.github.io/tidytext/,\nhttps://github.com/juliasilge/tidytext",
    "bug_reports": "https://github.com/juliasilge/tidytext/issues",
    "repository": "https://cran.r-project.org/package=tidytext",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidytext Text Mining using 'dplyr', 'ggplot2', and Other Tidy Tools Using tidy data principles can make many text mining tasks\n    easier, more effective, and consistent with tools already in wide use.\n    Much of the infrastructure needed for text mining with tidy data\n    frames already exists in packages like 'dplyr', 'broom', 'tidyr', and\n    'ggplot2'. In this package, we provide functions and supporting data\n    sets to allow conversion of text to and from tidy formats, and to\n    switch seamlessly between tidy tools and existing text mining\n    packages.  "
  },
  {
    "id": 22141,
    "package_name": "tidyxl",
    "title": "Read Untidy Excel Files",
    "description": "Imports non-tabular from Excel files into R.  Exposes cell content,\n    position and formatting in a tidy structure for further manipulation.\n    Tokenizes Excel formulas.  Supports '.xlsx' and '.xlsm' via the embedded\n    'RapidXML' C++ library <https://rapidxml.sourceforge.net>.  Does not support\n    '.xlsb' or '.xls'.",
    "version": "1.0.10",
    "maintainer": "Duncan Garmonsway <nacnudus@gmail.com>",
    "author": "Duncan Garmonsway [aut, cre],\n  Hadley Wickham [ctb] (Author of included readxl fragments),\n  Jenny Bryan [ctb] (Author of included readxl fragments),\n  RStudio [cph] (Copyright holder of included readxl fragments),\n  Marcin Kalicinski [ctb, cph] (Author of included RapidXML code)",
    "url": "https://github.com/nacnudus/tidyxl,\nhttps://nacnudus.github.io/tidyxl/",
    "bug_reports": "https://github.com/nacnudus/tidyxl/issues",
    "repository": "https://cran.r-project.org/package=tidyxl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyxl Read Untidy Excel Files Imports non-tabular from Excel files into R.  Exposes cell content,\n    position and formatting in a tidy structure for further manipulation.\n    Tokenizes Excel formulas.  Supports '.xlsx' and '.xlsm' via the embedded\n    'RapidXML' C++ library <https://rapidxml.sourceforge.net>.  Does not support\n    '.xlsb' or '.xls'.  "
  },
  {
    "id": 22212,
    "package_name": "tlda",
    "title": "Tools for Language Data Analysis",
    "description": "Support functions and datasets to facilitate the analysis of linguistic\n    data. The current focus is on the calculation of corpus-linguistic dispersion\n    measures as described in Gries (2021) <doi:10.1007/978-3-030-46216-1_5> and \n    Soenning (2025) <doi:10.3366/cor.2025.0326>. The most commonly used parts-based \n    indices are implemented, including different formulas and modifications that are \n    found in the literature, with the additional option to obtain frequency-adjusted \n    scores. Dispersion scores can be computed based on individual count variables or \n    a term-document matrix.  ",
    "version": "0.1.0",
    "maintainer": "Lukas Soenning <lukas.soenning@uni-bamberg.de>",
    "author": "Lukas Soenning [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2705-395X>),\n  German Research Foundation (DFG) [fnd] (ROR:\n    <https://ror.org/018mejw64>, Grant number 548274092)",
    "url": "https://github.com/lsoenning/tlda",
    "bug_reports": "https://github.com/lsoenning/tlda/issues",
    "repository": "https://cran.r-project.org/package=tlda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tlda Tools for Language Data Analysis Support functions and datasets to facilitate the analysis of linguistic\n    data. The current focus is on the calculation of corpus-linguistic dispersion\n    measures as described in Gries (2021) <doi:10.1007/978-3-030-46216-1_5> and \n    Soenning (2025) <doi:10.3366/cor.2025.0326>. The most commonly used parts-based \n    indices are implemented, including different formulas and modifications that are \n    found in the literature, with the additional option to obtain frequency-adjusted \n    scores. Dispersion scores can be computed based on individual count variables or \n    a term-document matrix.    "
  },
  {
    "id": 22218,
    "package_name": "tm.plugin.alceste",
    "title": "Import Texts from Files in the 'Alceste' Format Using the 'tm'\nText Mining Framework",
    "description": "Provides a 'tm' Source to create corpora from\n  a corpus prepared in the format used by the 'Alceste' application (i.e.\n  a single text file with inline meta-data). It is able to import both\n  text contents and meta-data (starred) variables.",
    "version": "1.1.2",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=tm.plugin.alceste",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.alceste Import Texts from Files in the 'Alceste' Format Using the 'tm'\nText Mining Framework Provides a 'tm' Source to create corpora from\n  a corpus prepared in the format used by the 'Alceste' application (i.e.\n  a single text file with inline meta-data). It is able to import both\n  text contents and meta-data (starred) variables.  "
  },
  {
    "id": 22219,
    "package_name": "tm.plugin.dc",
    "title": "Text Mining Distributed Corpus Plug-in",
    "description": "A plug-in for the text mining framework tm to support text mining \n             in a distributed way. The package provides a convenient interface for\n             handling distributed corpus objects based on distributed list objects.",
    "version": "0.2-10",
    "maintainer": "Stefan Theussl <Stefan.Theussl@R-project.org>",
    "author": "Ingo Feinerer [aut],\n  Stefan Theussl [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tm.plugin.dc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.dc Text Mining Distributed Corpus Plug-in A plug-in for the text mining framework tm to support text mining \n             in a distributed way. The package provides a convenient interface for\n             handling distributed corpus objects based on distributed list objects.  "
  },
  {
    "id": 22220,
    "package_name": "tm.plugin.europresse",
    "title": "Import Articles from 'Europresse' Using the 'tm' Text Mining\nFramework",
    "description": "Provides a 'tm' Source to create corpora from\n  articles exported from the 'Europresse' content provider as\n  HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author and pages).",
    "version": "1.4.1",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=tm.plugin.europresse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.europresse Import Articles from 'Europresse' Using the 'tm' Text Mining\nFramework Provides a 'tm' Source to create corpora from\n  articles exported from the 'Europresse' content provider as\n  HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author and pages).  "
  },
  {
    "id": 22221,
    "package_name": "tm.plugin.factiva",
    "title": "Import Articles from 'Factiva' Using the 'tm' Text Mining\nFramework",
    "description": "Provides a 'tm' Source to create corpora from\n  articles exported from the Dow Jones 'Factiva' content provider as\n  XML or HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author, subject,\n  geographical coverage, company, industry, and various\n  provider-specific fields).",
    "version": "1.8.1",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre],\n  Grigorij Ljubownikow [ctb],\n  Juliane Krueger [ctb],\n  Tom Nicholls [ctb]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=tm.plugin.factiva",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.factiva Import Articles from 'Factiva' Using the 'tm' Text Mining\nFramework Provides a 'tm' Source to create corpora from\n  articles exported from the Dow Jones 'Factiva' content provider as\n  XML or HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author, subject,\n  geographical coverage, company, industry, and various\n  provider-specific fields).  "
  },
  {
    "id": 22222,
    "package_name": "tm.plugin.koRpus",
    "title": "Full Corpus Support for the 'koRpus' Package",
    "description": "Enhances 'koRpus' text object classes and methods to also support large\n          corpora. Hierarchical ordering of corpus texts into arbitrary categories will\n          be preserved. Provided classes and methods also improve the ability of using\n          the 'koRpus' package together with the 'tm' package. To ask for help, report\n          bugs, suggest feature improvements, or discuss the global development of the\n          package, please subscribe to the koRpus-dev mailing list\n          (<https://korpusml.reaktanz.de>).",
    "version": "0.4-2",
    "maintainer": "m.eik michalke <meik.michalke@hhu.de>",
    "author": "m.eik michalke [aut, cre]",
    "url": "https://reaktanz.de/?c=hacking&s=koRpus",
    "bug_reports": "https://github.com/unDocUMeantIt/tm.plugin.koRpus/issues",
    "repository": "https://cran.r-project.org/package=tm.plugin.koRpus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.koRpus Full Corpus Support for the 'koRpus' Package Enhances 'koRpus' text object classes and methods to also support large\n          corpora. Hierarchical ordering of corpus texts into arbitrary categories will\n          be preserved. Provided classes and methods also improve the ability of using\n          the 'koRpus' package together with the 'tm' package. To ask for help, report\n          bugs, suggest feature improvements, or discuss the global development of the\n          package, please subscribe to the koRpus-dev mailing list\n          (<https://korpusml.reaktanz.de>).  "
  },
  {
    "id": 22223,
    "package_name": "tm.plugin.lexisnexis",
    "title": "Import Articles from 'LexisNexis' Using the 'tm' Text Mining\nFramework",
    "description": "Provides a 'tm' Source to create corpora from\n  articles exported from the 'LexisNexis' content provider as\n  HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author and pages).\n  Note that the file format is highly unstable: there is no warranty\n  that this package will work for your corpus, and you may have\n  to adjust the code to adapt it to your particular format.",
    "version": "1.4.2",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre],\n  Tom Nicholls [ctb]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=tm.plugin.lexisnexis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.lexisnexis Import Articles from 'LexisNexis' Using the 'tm' Text Mining\nFramework Provides a 'tm' Source to create corpora from\n  articles exported from the 'LexisNexis' content provider as\n  HTML files. It is able to read both text content and meta-data\n  information (including source, date, title, author and pages).\n  Note that the file format is highly unstable: there is no warranty\n  that this package will work for your corpus, and you may have\n  to adjust the code to adapt it to your particular format.  "
  },
  {
    "id": 22224,
    "package_name": "tm.plugin.mail",
    "title": "Text Mining E-Mail Plug-in",
    "description": "A plug-in for the tm text mining framework providing mail handling\n  functionality.",
    "version": "0.3-1",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Ingo Feinerer [aut] (ORCID: <https://orcid.org/0000-0001-7656-8338>),\n  Wolfgang Mauerer [aut],\n  Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tm.plugin.mail",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.mail Text Mining E-Mail Plug-in A plug-in for the tm text mining framework providing mail handling\n  functionality.  "
  },
  {
    "id": 22235,
    "package_name": "tmcn",
    "title": "A Text Mining Toolkit for Chinese",
    "description": "A Text mining toolkit for Chinese, which includes facilities for \n    Chinese string processing, Chinese NLP supporting, encoding detecting and \n    converting. Moreover, it provides some functions to support 'tm' package \n    in Chinese.",
    "version": "0.2-13",
    "maintainer": "Jian Li <rweibo@sina.com>",
    "author": "Jian Li",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmcn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmcn A Text Mining Toolkit for Chinese A Text mining toolkit for Chinese, which includes facilities for \n    Chinese string processing, Chinese NLP supporting, encoding detecting and \n    converting. Moreover, it provides some functions to support 'tm' package \n    in Chinese.  "
  },
  {
    "id": 22253,
    "package_name": "tok",
    "title": "Fast Text Tokenization",
    "description": "\n  Interfaces with the 'Hugging Face' tokenizers library to provide implementations\n  of today's most used tokenizers such as the 'Byte-Pair Encoding' algorithm \n  <https://huggingface.co/docs/tokenizers/index>. It's extremely fast for both \n  training new vocabularies and tokenizing texts.",
    "version": "0.2.1",
    "maintainer": "Daniel Falbel <daniel@posit.co>",
    "author": "Daniel Falbel [aut, cre],\n  Regouby Christophe [ctb],\n  Posit [cph]",
    "url": "https://github.com/mlverse/tok",
    "bug_reports": "https://github.com/mlverse/tok/issues",
    "repository": "https://cran.r-project.org/package=tok",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tok Fast Text Tokenization \n  Interfaces with the 'Hugging Face' tokenizers library to provide implementations\n  of today's most used tokenizers such as the 'Byte-Pair Encoding' algorithm \n  <https://huggingface.co/docs/tokenizers/index>. It's extremely fast for both \n  training new vocabularies and tokenizing texts.  "
  },
  {
    "id": 22254,
    "package_name": "tokenizers.bpe",
    "title": "Byte Pair Encoding Text Tokenization",
    "description": "Unsupervised text tokenizer focused on computational efficiency. Wraps the 'YouTokenToMe' library <https://github.com/VKCOM/YouTokenToMe> which is an implementation of fast Byte Pair Encoding (BPE) <https://aclanthology.org/P16-1162/>.",
    "version": "0.1.4",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  VK.com [cph],\n  Gregory Popovitch [ctb, cph] (Files at src/parallel_hashmap (Apache\n    License, Version 2.0),\n  The Abseil Authors [ctb, cph] (Files at src/parallel_hashmap (Apache\n    License, Version 2.0),\n  Ivan Belonogov [ctb, cph] (Files at src/youtokentome (MIT License))",
    "url": "https://github.com/bnosac/tokenizers.bpe",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tokenizers.bpe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tokenizers.bpe Byte Pair Encoding Text Tokenization Unsupervised text tokenizer focused on computational efficiency. Wraps the 'YouTokenToMe' library <https://github.com/VKCOM/YouTokenToMe> which is an implementation of fast Byte Pair Encoding (BPE) <https://aclanthology.org/P16-1162/>.  "
  },
  {
    "id": 22266,
    "package_name": "topicdoc",
    "title": "Topic-Specific Diagnostics for LDA and CTM Topic Models",
    "description": "Calculates topic-specific diagnostics (e.g. mean token length, exclusivity) for \n    Latent Dirichlet Allocation and Correlated Topic Models fit using the 'topicmodels' package.\n    For more details, see Chapter 12 in Airoldi et al. (2014, ISBN:9781466504080), \n    pp 262-272 Mimno et al. (2011, ISBN:9781937284114), and Bischof et al. (2014) <arXiv:1206.4631v1>.",
    "version": "0.1.1",
    "maintainer": "Doug Friedman <doug.nhp@gmail.com>",
    "author": "Doug Friedman [aut, cre]",
    "url": "https://github.com/doug-friedman/topicdoc",
    "bug_reports": "https://github.com/doug-friedman/topicdoc/issues",
    "repository": "https://cran.r-project.org/package=topicdoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topicdoc Topic-Specific Diagnostics for LDA and CTM Topic Models Calculates topic-specific diagnostics (e.g. mean token length, exclusivity) for \n    Latent Dirichlet Allocation and Correlated Topic Models fit using the 'topicmodels' package.\n    For more details, see Chapter 12 in Airoldi et al. (2014, ISBN:9781466504080), \n    pp 262-272 Mimno et al. (2011, ISBN:9781937284114), and Bischof et al. (2014) <arXiv:1206.4631v1>.  "
  },
  {
    "id": 22267,
    "package_name": "topiclabels",
    "title": "Automated Topic Labeling with Language Models",
    "description": "Leveraging (large) language models for automatic topic labeling. The main function converts a list of top terms into a label for each topic. Hence, it is complementary to any topic modeling package that produces a list of top terms for each topic. While human judgement is indispensable for topic validation (i.e., inspecting top terms and most representative documents), automatic topic labeling can be a valuable tool for researchers in various scenarios.",
    "version": "0.3.0",
    "maintainer": "Jonas Rieger <rieger@statistik.tu-dortmund.de>",
    "author": "Jonas Rieger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0007-4478>),\n  Fritz Peters [aut] (ORCID: <https://orcid.org/0009-0003-8471-4931>),\n  Andreas Fischer [aut] (ORCID: <https://orcid.org/0009-0006-0748-6076>),\n  Tim Lauer [aut] (ORCID: <https://orcid.org/0009-0003-1625-1672>),\n  Andr\u00e9 Bittermann [aut] (ORCID: <https://orcid.org/0000-0003-2942-9831>)",
    "url": "https://github.com/PetersFritz/topiclabels",
    "bug_reports": "https://github.com/PetersFritz/topiclabels/issues",
    "repository": "https://cran.r-project.org/package=topiclabels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topiclabels Automated Topic Labeling with Language Models Leveraging (large) language models for automatic topic labeling. The main function converts a list of top terms into a label for each topic. Hence, it is complementary to any topic modeling package that produces a list of top terms for each topic. While human judgement is indispensable for topic validation (i.e., inspecting top terms and most representative documents), automatic topic labeling can be a valuable tool for researchers in various scenarios.  "
  },
  {
    "id": 22268,
    "package_name": "topicmodels",
    "title": "Topic Models",
    "description": "Provides an interface to the C code for Latent Dirichlet\n\t     Allocation (LDA) models and Correlated Topics Models\n\t     (CTM) by David M. Blei and co-authors and the C++ code\n\t     for fitting LDA models using Gibbs sampling by Xuan-Hieu\n\t     Phan and co-authors.",
    "version": "0.2-17",
    "maintainer": "Bettina Gr\u00fcn <Bettina.Gruen@R-project.org>",
    "author": "Bettina Gr\u00fcn [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7265-4773>),\n  Kurt Hornik [aut] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  David M Blei [ctb, cph] (VEM estimation of LDA and CTM),\n  John D Lafferty [ctb, cph] (VEM estimation of CTM),\n  Xuan-Hieu Phan [ctb, cph] (MCMC estimation of LDA),\n  Makoto Matsumoto [ctb, cph] (Mersenne Twister RNG),\n  Takuji Nishimura [ctb, cph] (Mersenne Twister RNG),\n  Shawn Cokus [ctb] (Mersenne Twister RNG)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=topicmodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topicmodels Topic Models Provides an interface to the C code for Latent Dirichlet\n\t     Allocation (LDA) models and Correlated Topics Models\n\t     (CTM) by David M. Blei and co-authors and the C++ code\n\t     for fitting LDA models using Gibbs sampling by Xuan-Hieu\n\t     Phan and co-authors.  "
  },
  {
    "id": 22269,
    "package_name": "topicmodels.etm",
    "title": "Topic Modelling in Embedding Spaces",
    "description": "Find topics in texts which are semantically embedded using techniques like word2vec or Glove. \n    This topic modelling technique models each word with a categorical distribution whose natural parameter is the inner product between a word embedding and an embedding of its assigned topic.\n    The techniques are explained in detail in the paper 'Topic Modeling in Embedding Spaces' by Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei (2019), available at <doi:10.48550/arXiv.1907.04907>. ",
    "version": "0.1.1",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R implementation),\n  BNOSAC [cph] (R implementation),\n  Adji B. Dieng [ctb, cph] (original Python implementation in inst/orig),\n  Francisco J. R. Ruiz [ctb, cph] (original Python implementation in\n    inst/orig),\n  David M. Blei [ctb, cph] (original Python implementation in inst/orig)",
    "url": "https://github.com/bnosac/ETM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=topicmodels.etm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topicmodels.etm Topic Modelling in Embedding Spaces Find topics in texts which are semantically embedded using techniques like word2vec or Glove. \n    This topic modelling technique models each word with a categorical distribution whose natural parameter is the inner product between a word embedding and an embedding of its assigned topic.\n    The techniques are explained in detail in the paper 'Topic Modeling in Embedding Spaces' by Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei (2019), available at <doi:10.48550/arXiv.1907.04907>.   "
  },
  {
    "id": 22288,
    "package_name": "tosca",
    "title": "Tools for Statistical Content Analysis",
    "description": "A framework for statistical analysis in content analysis. In addition to a pipeline for preprocessing text corpora and linking to the latent Dirichlet allocation from the 'lda' package, plots are offered for the descriptive analysis of text corpora and topic models. In addition, an implementation of Chang's intruder words and intruder topics is provided. Sample data for the vignette is included in the toscaData package, which is available on gitHub: <https://github.com/Docma-TU/toscaData>.",
    "version": "0.3-4",
    "maintainer": "Lars Koppers <koppers@statistik.tu-dortmund.de>",
    "author": "Lars Koppers [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-9616>),\n  Jonas Rieger [aut] (ORCID: <https://orcid.org/0000-0002-0007-4478>),\n  Karin Boczek [ctb] (ORCID: <https://orcid.org/0000-0003-1516-4094>),\n  Gerret von Nordheim [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7553-3838>)",
    "url": "https://github.com/Docma-TU/tosca,\nhttps://doi.org/10.5281/zenodo.3591068",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tosca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tosca Tools for Statistical Content Analysis A framework for statistical analysis in content analysis. In addition to a pipeline for preprocessing text corpora and linking to the latent Dirichlet allocation from the 'lda' package, plots are offered for the descriptive analysis of text corpora and topic models. In addition, an implementation of Chang's intruder words and intruder topics is provided. Sample data for the vignette is included in the toscaData package, which is available on gitHub: <https://github.com/Docma-TU/toscaData>.  "
  },
  {
    "id": 22343,
    "package_name": "transforEmotion",
    "title": "Sentiment Analysis for Text, Image and Video using Transformer\nModels",
    "description": "Implements sentiment analysis using huggingface <https://huggingface.co> transformer zero-shot classification model pipelines for text and image data. The default text pipeline is Cross-Encoder's DistilRoBERTa <https://huggingface.co/cross-encoder/nli-distilroberta-base> and default image/video pipeline is Open AI's CLIP  <https://huggingface.co/openai/clip-vit-base-patch32>. All other zero-shot classification model pipelines can be implemented using their model name from <https://huggingface.co/models?pipeline_tag=zero-shot-classification>.",
    "version": "0.1.6",
    "maintainer": "Aleksandar Toma\u0161evi\u0107 <atomashevic@gmail.com>",
    "author": "Alexander Christensen [aut] (ORCID:\n    <https://orcid.org/0000-0002-9798-7037>),\n  Hudson Golino [aut] (ORCID: <https://orcid.org/0000-0002-1601-1447>),\n  Aleksandar Toma\u0161evi\u0107 [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4863-6051>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=transforEmotion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transforEmotion Sentiment Analysis for Text, Image and Video using Transformer\nModels Implements sentiment analysis using huggingface <https://huggingface.co> transformer zero-shot classification model pipelines for text and image data. The default text pipeline is Cross-Encoder's DistilRoBERTa <https://huggingface.co/cross-encoder/nli-distilroberta-base> and default image/video pipeline is Open AI's CLIP  <https://huggingface.co/openai/clip-vit-base-patch32>. All other zero-shot classification model pipelines can be implemented using their model name from <https://huggingface.co/models?pipeline_tag=zero-shot-classification>.  "
  },
  {
    "id": 22458,
    "package_name": "tsentiment",
    "title": "Fetching Tweet Data for Sentiment Analysis",
    "description": "Which uses Twitter APIs for the necessary data in sentiment analysis, acts as a middleware with the approved Twitter Application.\n    A special access key is given to users who subscribe to the application with their Twitter account. With this special access key, the user defined keyword for sentiment analysis can be searched in twitter recent searches and results can be obtained( more information <https://github.com/hakkisabah/tsentiment> ).\n    In addition, a service named tsentiment-services has been developed to provide all these operations ( for more information <https://github.com/hakkisabah/tsentiment-services> ).\n    After the successful results obtained and in line with the permissions given by the user, the results of the analysis of the word cloud and bar graph saved in the user folder directory can be seen. In each analysis performed, the previous analysis visual result is deleted and this is the basic information you need to know as a practice rule.\n    'tsentiment' package provides a free service that acts as a middleware for easy data extraction from Twitter, and in return, the user rate limit is reduced by 30 requests from the total limit and the remaining requests are used. These 30 requests are reserved for use in application analytics. For information about endpoints, you can refer to the limit information in the \"GET search/tweets\" row in the Endpoints column in the list at <https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits>.",
    "version": "1.0.5",
    "maintainer": "Hakki Sabah <hakkisabah@hotmail.com>",
    "author": "Hakki Sabah <hakkisabah@hotmail.com>",
    "url": "https://github.com/hakkisabah/tsentiment,\nhttps://www.tsentiment.com",
    "bug_reports": "https://github.com/hakkisabah/tsentiment/issues",
    "repository": "https://cran.r-project.org/package=tsentiment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsentiment Fetching Tweet Data for Sentiment Analysis Which uses Twitter APIs for the necessary data in sentiment analysis, acts as a middleware with the approved Twitter Application.\n    A special access key is given to users who subscribe to the application with their Twitter account. With this special access key, the user defined keyword for sentiment analysis can be searched in twitter recent searches and results can be obtained( more information <https://github.com/hakkisabah/tsentiment> ).\n    In addition, a service named tsentiment-services has been developed to provide all these operations ( for more information <https://github.com/hakkisabah/tsentiment-services> ).\n    After the successful results obtained and in line with the permissions given by the user, the results of the analysis of the word cloud and bar graph saved in the user folder directory can be seen. In each analysis performed, the previous analysis visual result is deleted and this is the basic information you need to know as a practice rule.\n    'tsentiment' package provides a free service that acts as a middleware for easy data extraction from Twitter, and in return, the user rate limit is reduced by 30 requests from the total limit and the remaining requests are used. These 30 requests are reserved for use in application analytics. For information about endpoints, you can refer to the limit information in the \"GET search/tweets\" row in the Endpoints column in the list at <https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits>.  "
  },
  {
    "id": 22587,
    "package_name": "udpipe",
    "title": "Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit",
    "description": "This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.",
    "version": "0.8.15",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Institute of Formal and Applied Linguistics, Faculty of Mathematics and\n    Physics, Charles University in Prague, Czech Republic [cph]\n    (src/udpipe.cpp & src/udpipe.h),\n  Milan Straka [aut, cph] (src/udpipe.cpp & src/udpipe.h),\n  Jana Strakov\u00e1 [ctb, cph] (src/udpipe.cpp & src/udpipe.h)",
    "url": "https://bnosac.github.io/udpipe/en/index.html,\nhttps://github.com/bnosac/udpipe",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=udpipe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "udpipe Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.  "
  },
  {
    "id": 22728,
    "package_name": "vader",
    "title": "Valence Aware Dictionary and sEntiment Reasoner (VADER)",
    "description": "A lexicon and rule-based sentiment analysis tool that is specifically \n    attuned to sentiments expressed in social media, and works well on texts from other \n    domains. Hutto & Gilbert (2014) <https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109/8122>. ",
    "version": "0.2.1",
    "maintainer": "Katherine Roehrick <kr.gitcode@gmail.com>",
    "author": "Katherine Roehrick [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vader Valence Aware Dictionary and sEntiment Reasoner (VADER) A lexicon and rule-based sentiment analysis tool that is specifically \n    attuned to sentiments expressed in social media, and works well on texts from other \n    domains. Hutto & Gilbert (2014) <https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109/8122>.   "
  },
  {
    "id": 22742,
    "package_name": "validateIt",
    "title": "Validating Topic Coherence and Topic Labels",
    "description": "By creating crowd-sourcing tasks that can be easily posted and results retrieved using Amazon's Mechanical Turk (MTurk) API, researchers can use this solution to validate the quality of topics obtained from unsupervised or semi-supervised learning methods, and the relevance of topic labels assigned. This helps ensure that the topic modeling results are accurate and useful for research purposes. See Ying and others (2022) <doi:10.1101/2023.05.02.538599>. For more information, please visit <https://github.com/Triads-Developer/Topic_Model_Validation>.  ",
    "version": "1.2.1",
    "maintainer": "Luwei Ying <triads.developers@wustl.edu>",
    "author": "Luwei Ying [aut, cre] (ORCID: <https://orcid.org/0000-0001-7307-4834>),\n  Jacob Montgomery [aut],\n  Brandon Stewart [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=validateIt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "validateIt Validating Topic Coherence and Topic Labels By creating crowd-sourcing tasks that can be easily posted and results retrieved using Amazon's Mechanical Turk (MTurk) API, researchers can use this solution to validate the quality of topics obtained from unsupervised or semi-supervised learning methods, and the relevance of topic labels assigned. This helps ensure that the topic modeling results are accurate and useful for research purposes. See Ying and others (2022) <doi:10.1101/2023.05.02.538599>. For more information, please visit <https://github.com/Triads-Developer/Topic_Model_Validation>.    "
  },
  {
    "id": 23136,
    "package_name": "word.alignment",
    "title": "Computing Word Alignment Using IBM Model 1 (and Symmetrization)\nfor a Given Parallel Corpus and Its Evaluation",
    "description": "For a given Sentence-Aligned Parallel Corpus, it aligns words for each sentence pair. It considers one-to-many and symmetrization alignments. Moreover, it evaluates the quality of word alignment based on this package and some other software. It also builds an automatic dictionary of two languages based on given parallel corpus.",
    "version": "1.1",
    "maintainer": "Neda Daneshgar<ne_da978@stu-mail.um.ac.ir>",
    "author": "Neda Daneshagr and Majid Sarmad.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=word.alignment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "word.alignment Computing Word Alignment Using IBM Model 1 (and Symmetrization)\nfor a Given Parallel Corpus and Its Evaluation For a given Sentence-Aligned Parallel Corpus, it aligns words for each sentence pair. It considers one-to-many and symmetrization alignments. Moreover, it evaluates the quality of word alignment based on this package and some other software. It also builds an automatic dictionary of two languages based on given parallel corpus.  "
  },
  {
    "id": 23144,
    "package_name": "wordpiece",
    "title": "R Implementation of Wordpiece Tokenization",
    "description": "Apply 'Wordpiece' (<arXiv:1609.08144>) tokenization to input text, \n given an appropriate vocabulary. The 'BERT' (<arXiv:1810.04805>) tokenization \n conventions are used by default.",
    "version": "2.1.3",
    "maintainer": "Jonathan Bratt <jonathan.bratt@macmillan.com>",
    "author": "Jonathan Bratt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2859-0076>),\n  Jon Harmon [aut] (ORCID: <https://orcid.org/0000-0003-4781-4346>),\n  Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning [cph]",
    "url": "https://github.com/macmillancontentscience/wordpiece",
    "bug_reports": "https://github.com/macmillancontentscience/wordpiece/issues",
    "repository": "https://cran.r-project.org/package=wordpiece",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordpiece R Implementation of Wordpiece Tokenization Apply 'Wordpiece' (<arXiv:1609.08144>) tokenization to input text, \n given an appropriate vocabulary. The 'BERT' (<arXiv:1810.04805>) tokenization \n conventions are used by default.  "
  },
  {
    "id": 23145,
    "package_name": "wordpiece.data",
    "title": "Data for Wordpiece-Style Tokenization",
    "description": "Provides data to be used by the wordpiece algorithm in order to \n    tokenize text into somewhat meaningful chunks. Included vocabularies were \n    retrieved from \n    <https://huggingface.co/bert-base-cased/resolve/main/vocab.txt> and \n    <https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt> and parsed\n    into an R-friendly format.",
    "version": "2.0.0",
    "maintainer": "Jon Harmon <jonthegeek@gmail.com>",
    "author": "Jonathan Bratt [aut] (ORCID: <https://orcid.org/0000-0003-2859-0076>),\n  Jon Harmon [aut, cre] (ORCID: <https://orcid.org/0000-0003-4781-4346>),\n  Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning [cph],\n  Google, Inc [cph] (original BERT vocabularies)",
    "url": "https://github.com/macmillancontentscience/wordpiece.data",
    "bug_reports": "https://github.com/macmillancontentscience/wordpiece.data/issues",
    "repository": "https://cran.r-project.org/package=wordpiece.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordpiece.data Data for Wordpiece-Style Tokenization Provides data to be used by the wordpiece algorithm in order to \n    tokenize text into somewhat meaningful chunks. Included vocabularies were \n    retrieved from \n    <https://huggingface.co/bert-base-cased/resolve/main/vocab.txt> and \n    <https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt> and parsed\n    into an R-friendly format.  "
  },
  {
    "id": 23149,
    "package_name": "wordvector",
    "title": "Word and Document Vector Models",
    "description": "Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) <doi:10.48550/arXiv.1310.4546> and Latent Semantic Analysis (Deerwester et al., 1990) <doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9>.",
    "version": "0.6.0",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6519-5265>),\n  Jan Wijffels [aut] (Original R code),\n  BNOSAC [cph] (Original R code),\n  Max Fomichev [ctb, cph] (Original C++ code)",
    "url": "https://github.com/koheiw/wordvector",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordvector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordvector Word and Document Vector Models Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) <doi:10.48550/arXiv.1310.4546> and Latent Semantic Analysis (Deerwester et al., 1990) <doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9>.  "
  },
  {
    "id": 23159,
    "package_name": "wpa",
    "title": "Tools for Analysing and Visualising Viva Insights Data",
    "description": "Opinionated functions that enable easier and faster\n    analysis of Viva Insights data. There are three main types of functions in 'wpa':\n    (i) Standard functions create a 'ggplot' visual or a summary table based on a specific\n    Viva Insights metric; (2) Report Generation functions generate HTML reports on\n    a specific analysis area, e.g. Collaboration; (3) Other miscellaneous functions cover\n    more specific applications (e.g. Subject Line text mining) of Viva Insights data.\n    This package adheres to 'tidyverse' principles and works well with the pipe syntax.\n    'wpa' is built with the beginner-to-intermediate R users in mind, and is optimised for\n    simplicity. ",
    "version": "1.10.0",
    "maintainer": "Martin Chan <martin.chan@microsoft.com>",
    "author": "Martin Chan [aut, cre],\n  Carlos Morales [aut],\n  Mark Powers [ctb],\n  Ainize Cidoncha [ctb],\n  Rosamary Ochoa Vargas [ctb],\n  Tannaz Sattari [ctb],\n  Lucas Hogner [ctb],\n  Jasminder Thind [ctb],\n  Simone Liebal [ctb],\n  Aleksey Ashikhmin [ctb],\n  Ellen Trinklein [ctb],\n  Microsoft Corporation [cph]",
    "url": "https://github.com/microsoft/wpa/,\nhttps://microsoft.github.io/wpa/",
    "bug_reports": "https://github.com/microsoft/wpa/issues/",
    "repository": "https://cran.r-project.org/package=wpa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wpa Tools for Analysing and Visualising Viva Insights Data Opinionated functions that enable easier and faster\n    analysis of Viva Insights data. There are three main types of functions in 'wpa':\n    (i) Standard functions create a 'ggplot' visual or a summary table based on a specific\n    Viva Insights metric; (2) Report Generation functions generate HTML reports on\n    a specific analysis area, e.g. Collaboration; (3) Other miscellaneous functions cover\n    more specific applications (e.g. Subject Line text mining) of Viva Insights data.\n    This package adheres to 'tidyverse' principles and works well with the pipe syntax.\n    'wpa' is built with the beginner-to-intermediate R users in mind, and is optimised for\n    simplicity.   "
  }
]