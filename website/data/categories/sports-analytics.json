[
  {
    "id": 321,
    "package_name": "broom",
    "title": "Convert Statistical Objects into Tidy Tibbles",
    "description": "Summarizes key information about statistical objects in\ntidy tibbles. This makes it easy to report results, create\nplots and consistently work with large numbers of models at\nonce.  Broom provides three verbs that each provide different\ntypes of information about a model. tidy() summarizes\ninformation about model components such as coefficients of a\nregression. glance() reports information about an entire model,\nsuch as goodness of fit measures like AIC and BIC. augment()\nadds information about individual observations to a dataset,\nsuch as fitted values or influence measures.",
    "version": "1.0.11.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "David Robinson [aut],\nAlex Hayes [aut] (ORCID: <https://orcid.org/0000-0002-4985-5160>),\nSimon Couch [aut] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\nEmil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\nIndrajeet Patil [ctb] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\nDerek Chiu [ctb],\nMatthieu Gomez [ctb],\nBoris Demeshev [ctb],\nDieter Menne [ctb],\nBenjamin Nutter [ctb],\nLuke Johnston [ctb],\nBen Bolker [ctb],\nFrancois Briatte [ctb],\nJeffrey Arnold [ctb],\nJonah Gabry [ctb],\nLuciano Selzer [ctb],\nGavin Simpson [ctb],\nJens Preussner [ctb],\nJay Hesselberth [ctb],\nHadley Wickham [ctb],\nMatthew Lincoln [ctb],\nAlessandro Gasparini [ctb],\nLukasz Komsta [ctb],\nFrederick Novometsky [ctb],\nWilson Freitas [ctb],\nMichelle Evans [ctb],\nJason Cory Brunson [ctb],\nSimon Jackson [ctb],\nBen Whalley [ctb],\nKarissa Whiting [ctb],\nYves Rosseel [ctb],\nMichael Kuehn [ctb],\nJorge Cimentada [ctb],\nErle Holgersen [ctb],\nKarl Dunkle Werner [ctb] (ORCID:\n<https://orcid.org/0000-0003-0523-7309>),\nEthan Christensen [ctb],\nSteven Pav [ctb],\nPaul PJ [ctb],\nBen Schneider [ctb],\nPatrick Kennedy [ctb],\nLily Medina [ctb],\nBrian Fannin [ctb],\nJason Muhlenkamp [ctb],\nMatt Lehman [ctb],\nBill Denney [ctb] (ORCID: <https://orcid.org/0000-0002-5759-428X>),\nNic Crane [ctb],\nAndrew Bates [ctb],\nVincent Arel-Bundock [ctb] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>),\nHideaki Hayashi [ctb],\nLuis Tobalina [ctb],\nAnnie Wang [ctb],\nWei Yang Tham [ctb],\nClara Wang [ctb],\nAbby Smith [ctb] (ORCID: <https://orcid.org/0000-0002-3207-0375>),\nJasper Cooper [ctb] (ORCID: <https://orcid.org/0000-0002-8639-3188>),\nE Auden Krauska [ctb] (ORCID: <https://orcid.org/0000-0002-1466-5850>),\nAlex Wang [ctb],\nMalcolm Barrett [ctb] (ORCID: <https://orcid.org/0000-0003-0299-5825>),\nCharles Gray [ctb] (ORCID: <https://orcid.org/0000-0002-9978-011X>),\nJared Wilber [ctb],\nVilmantas Gegzna [ctb] (ORCID: <https://orcid.org/0000-0002-9500-5167>),\nEduard Szoecs [ctb],\nFrederik Aust [ctb] (ORCID: <https://orcid.org/0000-0003-4900-788X>),\nAngus Moore [ctb],\nNick Williams [ctb],\nMarius Barth [ctb] (ORCID: <https://orcid.org/0000-0002-3421-6665>),\nBruna Wundervald [ctb] (ORCID: <https://orcid.org/0000-0001-8163-220X>),\nJoyce Cahoon [ctb] (ORCID: <https://orcid.org/0000-0001-7217-4702>),\nGrant McDermott [ctb] (ORCID: <https://orcid.org/0000-0001-7883-8573>),\nKevin Zarca [ctb],\nShiro Kuriwaki [ctb] (ORCID: <https://orcid.org/0000-0002-5687-2647>),\nLukas Wallrich [ctb] (ORCID: <https://orcid.org/0000-0003-2121-5177>),\nJames Martherus [ctb] (ORCID: <https://orcid.org/0000-0002-8285-3300>),\nChuliang Xiao [ctb] (ORCID: <https://orcid.org/0000-0002-8466-9398>),\nJoseph Larmarange [ctb],\nMax Kuhn [ctb],\nMichal Bojanowski [ctb],\nHakon Malmedal [ctb],\nClara Wang [ctb],\nSergio Oller [ctb],\nLuke Sonnet [ctb],\nJim Hester [ctb],\nBen Schneider [ctb],\nBernie Gray [ctb] (ORCID: <https://orcid.org/0000-0001-9190-6032>),\nMara Averick [ctb],\nAaron Jacobs [ctb],\nAndreas Bender [ctb],\nSven Templer [ctb],\nPaul-Christian Buerkner [ctb],\nMatthew Kay [ctb],\nErwan Le Pennec [ctb],\nJohan Junkka [ctb],\nHao Zhu [ctb],\nBenjamin Soltoff [ctb],\nZoe Wilkinson Saldana [ctb],\nTyler Littlefield [ctb],\nCharles T. Gray [ctb],\nShabbh E. Banks [ctb],\nSerina Robinson [ctb],\nRoger Bivand [ctb],\nRiinu Ots [ctb],\nNicholas Williams [ctb],\nNina Jakobsen [ctb],\nMichael Weylandt [ctb],\nLisa Lendway [ctb],\nKarl Hailperin [ctb],\nJosue Rodriguez [ctb],\nJenny Bryan [ctb],\nChris Jarvis [ctb],\nGreg Macfarlane [ctb],\nBrian Mannakee [ctb],\nDrew Tyre [ctb],\nShreyas Singh [ctb],\nLaurens Geffert [ctb],\nHong Ooi [ctb],\nHenrik Bengtsson [ctb],\nEduard Szocs [ctb],\nDavid Hugh-Jones [ctb],\nMatthieu Stigler [ctb],\nHugo Tavares [ctb] (ORCID: <https://orcid.org/0000-0001-9373-2726>),\nR. Willem Vervoort [ctb],\nBrenton M. Wiernik [ctb],\nJosh Yamamoto [ctb],\nJasme Lee [ctb],\nTaren Sanders [ctb] (ORCID: <https://orcid.org/0000-0002-4504-6008>),\nIlaria Prosdocimi [ctb] (ORCID:\n<https://orcid.org/0000-0001-8565-094X>),\nDaniel D. Sjoberg [ctb] (ORCID:\n<https://orcid.org/0000-0003-0862-2018>),\nAlex Reinhart [ctb] (ORCID: <https://orcid.org/0000-0002-6658-514X>)",
    "url": "https://broom.tidymodels.org/, https://github.com/tidymodels/broom",
    "bug_reports": "https://github.com/tidymodels/broom/issues",
    "repository": "",
    "exports": [
      [
        "augment"
      ],
      [
        "augment_columns"
      ],
      [
        "bootstrap"
      ],
      [
        "confint_tidy"
      ],
      [
        "finish_glance"
      ],
      [
        "fix_data_frame"
      ],
      [
        "glance"
      ],
      [
        "tidy"
      ],
      [
        "tidy_irlba"
      ]
    ],
    "topics": [
      [
        "modeling"
      ],
      [
        "tidy-data"
      ]
    ],
    "score": 21.9333,
    "stars": 1503,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "broom Convert Statistical Objects into Tidy Tibbles Summarizes key information about statistical objects in\ntidy tibbles. This makes it easy to report results, create\nplots and consistently work with large numbers of models at\nonce.  Broom provides three verbs that each provide different\ntypes of information about a model. tidy() summarizes\ninformation about model components such as coefficients of a\nregression. glance() reports information about an entire model,\nsuch as goodness of fit measures like AIC and BIC. augment()\nadds information about individual observations to a dataset,\nsuch as fitted values or influence measures. augment augment_columns bootstrap confint_tidy finish_glance fix_data_frame glance tidy tidy_irlba modeling tidy-data"
  },
  {
    "id": 319,
    "package_name": "brms",
    "title": "Bayesian Regression Models using 'Stan'",
    "description": "Fit Bayesian generalized (non-)linear multivariate\nmultilevel models using 'Stan' for full Bayesian inference. A\nwide range of distributions and link functions are supported,\nallowing users to fit -- among others -- linear, robust linear,\ncount data, survival, response times, ordinal, zero-inflated,\nhurdle, and even self-defined mixture models all in a\nmultilevel context. Further modeling options include both\ntheory-driven and data-driven non-linear terms,\nauto-correlation structures, censoring and truncation,\nmeta-analytic standard errors, and quite a few more. In\naddition, all parameters of the response distribution can be\npredicted in order to perform distributional regression. Prior\nspecifications are flexible and explicitly encourage users to\napply prior distributions that actually reflect their prior\nknowledge. Models can easily be evaluated and compared using\nseveral methods assessing posterior or prior predictions.\nReferences: B\u00fcrkner (2017) <doi:10.18637/jss.v080.i01>; B\u00fcrkner\n(2018) <doi:10.32614/RJ-2018-017>; B\u00fcrkner (2021)\n<doi:10.18637/jss.v100.i05>; Carpenter et al. (2017)\n<doi:10.18637/jss.v076.i01>.",
    "version": "2.23.1",
    "maintainer": "Paul-Christian B\u00fcrkner <paul.buerkner@gmail.com>",
    "author": "Paul-Christian B\u00fcrkner [aut, cre],\nJonah Gabry [ctb],\nSebastian Weber [ctb],\nAndrew Johnson [ctb],\nMartin Modrak [ctb],\nHamada S. Badr [ctb],\nFrank Weber [ctb],\nAki Vehtari [ctb],\nMattan S. Ben-Shachar [ctb],\nHayden Rabel [ctb],\nSimon C. Mills [ctb],\nStephen Wild [ctb],\nVen Popov [ctb],\nIoannis Kosmidis [ctb],\nBen Schneider [ctb],\nNoa Kallioinen [ctb],\nWellington J. Silva [ctb],\nLuiz Carvalho [ctb],\nSermet Pekin [ctb]",
    "url": "https://github.com/paul-buerkner/brms,\nhttps://discourse.mc-stan.org/, https://paulbuerkner.com/brms/",
    "bug_reports": "https://github.com/paul-buerkner/brms/issues",
    "repository": "",
    "exports": [
      [
        "acat"
      ],
      [
        "acformula"
      ],
      [
        "add_criterion"
      ],
      [
        "add_ic"
      ],
      [
        "add_ic<-"
      ],
      [
        "add_loo"
      ],
      [
        "add_rstan_model"
      ],
      [
        "add_waic"
      ],
      [
        "ar"
      ],
      [
        "arma"
      ],
      [
        "as_draws"
      ],
      [
        "as_draws_array"
      ],
      [
        "as_draws_df"
      ],
      [
        "as_draws_list"
      ],
      [
        "as_draws_matrix"
      ],
      [
        "as_draws_rvars"
      ],
      [
        "as.brmsprior"
      ],
      [
        "as.mcmc"
      ],
      [
        "asym_laplace"
      ],
      [
        "autocor"
      ],
      [
        "bayes_factor"
      ],
      [
        "bayes_R2"
      ],
      [
        "bernoulli"
      ],
      [
        "Beta"
      ],
      [
        "beta_binomial"
      ],
      [
        "bf"
      ],
      [
        "bridge_sampler"
      ],
      [
        "brm"
      ],
      [
        "brm_multiple"
      ],
      [
        "brmsfamily"
      ],
      [
        "brmsfit_needs_refit"
      ],
      [
        "brmsformula"
      ],
      [
        "brmsterms"
      ],
      [
        "car"
      ],
      [
        "categorical"
      ],
      [
        "combine_models"
      ],
      [
        "compare_ic"
      ],
      [
        "conditional_effects"
      ],
      [
        "conditional_smooths"
      ],
      [
        "constant"
      ],
      [
        "control_params"
      ],
      [
        "cor_ar"
      ],
      [
        "cor_arma"
      ],
      [
        "cor_arr"
      ],
      [
        "cor_bsts"
      ],
      [
        "cor_car"
      ],
      [
        "cor_cosy"
      ],
      [
        "cor_errorsar"
      ],
      [
        "cor_fixed"
      ],
      [
        "cor_icar"
      ],
      [
        "cor_lagsar"
      ],
      [
        "cor_ma"
      ],
      [
        "cor_sar"
      ],
      [
        "cosy"
      ],
      [
        "cox"
      ],
      [
        "cratio"
      ],
      [
        "cs"
      ],
      [
        "cse"
      ],
      [
        "cumulative"
      ],
      [
        "custom_family"
      ],
      [
        "dasym_laplace"
      ],
      [
        "data_predictor"
      ],
      [
        "data_response"
      ],
      [
        "dbeta_binomial"
      ],
      [
        "ddirichlet"
      ],
      [
        "default_prior"
      ],
      [
        "density_ratio"
      ],
      [
        "dexgaussian"
      ],
      [
        "dfrechet"
      ],
      [
        "dgen_extreme_value"
      ],
      [
        "dhurdle_gamma"
      ],
      [
        "dhurdle_lognormal"
      ],
      [
        "dhurdle_negbinomial"
      ],
      [
        "dhurdle_poisson"
      ],
      [
        "dinv_gaussian"
      ],
      [
        "dirichlet"
      ],
      [
        "dirichlet_multinomial"
      ],
      [
        "dlogistic_normal"
      ],
      [
        "dmulti_normal"
      ],
      [
        "dmulti_student_t"
      ],
      [
        "do_call"
      ],
      [
        "dshifted_lnorm"
      ],
      [
        "dskew_normal"
      ],
      [
        "dstudent_t"
      ],
      [
        "dvon_mises"
      ],
      [
        "dwiener"
      ],
      [
        "dzero_inflated_beta"
      ],
      [
        "dzero_inflated_beta_binomial"
      ],
      [
        "dzero_inflated_binomial"
      ],
      [
        "dzero_inflated_negbinomial"
      ],
      [
        "dzero_inflated_poisson"
      ],
      [
        "empty_prior"
      ],
      [
        "exgaussian"
      ],
      [
        "exponential"
      ],
      [
        "expose_functions"
      ],
      [
        "expp1"
      ],
      [
        "extract_draws"
      ],
      [
        "fcor"
      ],
      [
        "fixef"
      ],
      [
        "frechet"
      ],
      [
        "gen_extreme_value"
      ],
      [
        "geometric"
      ],
      [
        "get_dpar"
      ],
      [
        "get_prior"
      ],
      [
        "get_y"
      ],
      [
        "gp"
      ],
      [
        "gr"
      ],
      [
        "horseshoe"
      ],
      [
        "hurdle_cumulative"
      ],
      [
        "hurdle_gamma"
      ],
      [
        "hurdle_lognormal"
      ],
      [
        "hurdle_negbinomial"
      ],
      [
        "hurdle_poisson"
      ],
      [
        "hypothesis"
      ],
      [
        "inits"
      ],
      [
        "inv_logit_scaled"
      ],
      [
        "is.brmsfit"
      ],
      [
        "is.brmsfit_multiple"
      ],
      [
        "is.brmsformula"
      ],
      [
        "is.brmsprior"
      ],
      [
        "is.brmsterms"
      ],
      [
        "is.cor_arma"
      ],
      [
        "is.cor_brms"
      ],
      [
        "is.cor_car"
      ],
      [
        "is.cor_cosy"
      ],
      [
        "is.cor_fixed"
      ],
      [
        "is.cor_sar"
      ],
      [
        "is.mvbrmsformula"
      ],
      [
        "is.mvbrmsterms"
      ],
      [
        "kfold"
      ],
      [
        "kfold_predict"
      ],
      [
        "lasso"
      ],
      [
        "lf"
      ],
      [
        "log_lik"
      ],
      [
        "log_posterior"
      ],
      [
        "logistic_normal"
      ],
      [
        "logit_scaled"
      ],
      [
        "logm1"
      ],
      [
        "lognormal"
      ],
      [
        "loo"
      ],
      [
        "LOO"
      ],
      [
        "loo_compare"
      ],
      [
        "loo_epred"
      ],
      [
        "loo_linpred"
      ],
      [
        "loo_model_weights"
      ],
      [
        "loo_moment_match"
      ],
      [
        "loo_predict"
      ],
      [
        "loo_predictive_interval"
      ],
      [
        "loo_R2"
      ],
      [
        "loo_subsample"
      ],
      [
        "ma"
      ],
      [
        "make_conditions"
      ],
      [
        "make_stancode"
      ],
      [
        "make_standata"
      ],
      [
        "marginal_effects"
      ],
      [
        "marginal_smooths"
      ],
      [
        "mcmc_plot"
      ],
      [
        "me"
      ],
      [
        "mi"
      ],
      [
        "mixture"
      ],
      [
        "mm"
      ],
      [
        "mmc"
      ],
      [
        "mo"
      ],
      [
        "model_weights"
      ],
      [
        "multinomial"
      ],
      [
        "mvbf"
      ],
      [
        "mvbind"
      ],
      [
        "mvbrmsformula"
      ],
      [
        "nchains"
      ],
      [
        "ndraws"
      ],
      [
        "neff_ratio"
      ],
      [
        "negbinomial"
      ],
      [
        "ngrps"
      ],
      [
        "niterations"
      ],
      [
        "nlf"
      ],
      [
        "nsamples"
      ],
      [
        "nuts_params"
      ],
      [
        "nvariables"
      ],
      [
        "opencl"
      ],
      [
        "parnames"
      ],
      [
        "parse_bf"
      ],
      [
        "pasym_laplace"
      ],
      [
        "pbeta_binomial"
      ],
      [
        "pexgaussian"
      ],
      [
        "pfrechet"
      ],
      [
        "pgen_extreme_value"
      ],
      [
        "phurdle_gamma"
      ],
      [
        "phurdle_lognormal"
      ],
      [
        "phurdle_negbinomial"
      ],
      [
        "phurdle_poisson"
      ],
      [
        "pinv_gaussian"
      ],
      [
        "post_prob"
      ],
      [
        "posterior_average"
      ],
      [
        "posterior_epred"
      ],
      [
        "posterior_interval"
      ],
      [
        "posterior_linpred"
      ],
      [
        "posterior_predict"
      ],
      [
        "posterior_samples"
      ],
      [
        "posterior_smooths"
      ],
      [
        "posterior_summary"
      ],
      [
        "posterior_table"
      ],
      [
        "pp_average"
      ],
      [
        "pp_check"
      ],
      [
        "pp_expect"
      ],
      [
        "pp_mixture"
      ],
      [
        "predictive_error"
      ],
      [
        "predictive_interval"
      ],
      [
        "prepare_predictions"
      ],
      [
        "prior"
      ],
      [
        "prior_"
      ],
      [
        "prior_draws"
      ],
      [
        "prior_samples"
      ],
      [
        "prior_string"
      ],
      [
        "prior_summary"
      ],
      [
        "pshifted_lnorm"
      ],
      [
        "psis"
      ],
      [
        "pskew_normal"
      ],
      [
        "pstudent_t"
      ],
      [
        "pvon_mises"
      ],
      [
        "pzero_inflated_beta"
      ],
      [
        "pzero_inflated_beta_binomial"
      ],
      [
        "pzero_inflated_binomial"
      ],
      [
        "pzero_inflated_negbinomial"
      ],
      [
        "pzero_inflated_poisson"
      ],
      [
        "qasym_laplace"
      ],
      [
        "qfrechet"
      ],
      [
        "qgen_extreme_value"
      ],
      [
        "qshifted_lnorm"
      ],
      [
        "qskew_normal"
      ],
      [
        "qstudent_t"
      ],
      [
        "R2D2"
      ],
      [
        "ranef"
      ],
      [
        "rasym_laplace"
      ],
      [
        "rbeta_binomial"
      ],
      [
        "rdirichlet"
      ],
      [
        "read_csv_as_stanfit"
      ],
      [
        "recompile_model"
      ],
      [
        "reloo"
      ],
      [
        "rename_pars"
      ],
      [
        "resp_bhaz"
      ],
      [
        "resp_cat"
      ],
      [
        "resp_cens"
      ],
      [
        "resp_dec"
      ],
      [
        "resp_index"
      ],
      [
        "resp_mi"
      ],
      [
        "resp_rate"
      ],
      [
        "resp_se"
      ],
      [
        "resp_subset"
      ],
      [
        "resp_thres"
      ],
      [
        "resp_trials"
      ],
      [
        "resp_trunc"
      ],
      [
        "resp_vint"
      ],
      [
        "resp_vreal"
      ],
      [
        "resp_weights"
      ],
      [
        "restructure"
      ],
      [
        "rexgaussian"
      ],
      [
        "rfrechet"
      ],
      [
        "rgen_extreme_value"
      ],
      [
        "rhat"
      ],
      [
        "rinv_gaussian"
      ],
      [
        "rlogistic_normal"
      ],
      [
        "rmulti_normal"
      ],
      [
        "rmulti_student_t"
      ],
      [
        "rows2labels"
      ],
      [
        "rshifted_lnorm"
      ],
      [
        "rskew_normal"
      ],
      [
        "rstudent_t"
      ],
      [
        "rvon_mises"
      ],
      [
        "rwiener"
      ],
      [
        "s"
      ],
      [
        "sar"
      ],
      [
        "save_pars"
      ],
      [
        "set_mecor"
      ],
      [
        "set_nl"
      ],
      [
        "set_prior"
      ],
      [
        "set_rescor"
      ],
      [
        "shifted_lognormal"
      ],
      [
        "skew_normal"
      ],
      [
        "sratio"
      ],
      [
        "stancode"
      ],
      [
        "standata"
      ],
      [
        "stanplot"
      ],
      [
        "stanvar"
      ],
      [
        "student"
      ],
      [
        "t2"
      ],
      [
        "theme_black"
      ],
      [
        "theme_default"
      ],
      [
        "threading"
      ],
      [
        "unstr"
      ],
      [
        "update_adterms"
      ],
      [
        "validate_newdata"
      ],
      [
        "validate_prior"
      ],
      [
        "VarCorr"
      ],
      [
        "variables"
      ],
      [
        "von_mises"
      ],
      [
        "waic"
      ],
      [
        "WAIC"
      ],
      [
        "weibull"
      ],
      [
        "wiener"
      ],
      [
        "xbeta"
      ],
      [
        "zero_inflated_beta"
      ],
      [
        "zero_inflated_beta_binomial"
      ],
      [
        "zero_inflated_binomial"
      ],
      [
        "zero_inflated_negbinomial"
      ],
      [
        "zero_inflated_poisson"
      ],
      [
        "zero_one_inflated_beta"
      ]
    ],
    "topics": [
      [
        "bayesian-inference"
      ],
      [
        "brms"
      ],
      [
        "multilevel-models"
      ],
      [
        "stan"
      ],
      [
        "statistical-models"
      ]
    ],
    "score": 16.9006,
    "stars": 1369,
    "primary_category": "statistics",
    "source_universe": "paul-buerkner",
    "search_text": "brms Bayesian Regression Models using 'Stan' Fit Bayesian generalized (non-)linear multivariate\nmultilevel models using 'Stan' for full Bayesian inference. A\nwide range of distributions and link functions are supported,\nallowing users to fit -- among others -- linear, robust linear,\ncount data, survival, response times, ordinal, zero-inflated,\nhurdle, and even self-defined mixture models all in a\nmultilevel context. Further modeling options include both\ntheory-driven and data-driven non-linear terms,\nauto-correlation structures, censoring and truncation,\nmeta-analytic standard errors, and quite a few more. In\naddition, all parameters of the response distribution can be\npredicted in order to perform distributional regression. Prior\nspecifications are flexible and explicitly encourage users to\napply prior distributions that actually reflect their prior\nknowledge. Models can easily be evaluated and compared using\nseveral methods assessing posterior or prior predictions.\nReferences: B\u00fcrkner (2017) <doi:10.18637/jss.v080.i01>; B\u00fcrkner\n(2018) <doi:10.32614/RJ-2018-017>; B\u00fcrkner (2021)\n<doi:10.18637/jss.v100.i05>; Carpenter et al. (2017)\n<doi:10.18637/jss.v076.i01>. acat acformula add_criterion add_ic add_ic<- add_loo add_rstan_model add_waic ar arma as_draws as_draws_array as_draws_df as_draws_list as_draws_matrix as_draws_rvars as.brmsprior as.mcmc asym_laplace autocor bayes_factor bayes_R2 bernoulli Beta beta_binomial bf bridge_sampler brm brm_multiple brmsfamily brmsfit_needs_refit brmsformula brmsterms car categorical combine_models compare_ic conditional_effects conditional_smooths constant control_params cor_ar cor_arma cor_arr cor_bsts cor_car cor_cosy cor_errorsar cor_fixed cor_icar cor_lagsar cor_ma cor_sar cosy cox cratio cs cse cumulative custom_family dasym_laplace data_predictor data_response dbeta_binomial ddirichlet default_prior density_ratio dexgaussian dfrechet dgen_extreme_value dhurdle_gamma dhurdle_lognormal dhurdle_negbinomial dhurdle_poisson dinv_gaussian dirichlet dirichlet_multinomial dlogistic_normal dmulti_normal dmulti_student_t do_call dshifted_lnorm dskew_normal dstudent_t dvon_mises dwiener dzero_inflated_beta dzero_inflated_beta_binomial dzero_inflated_binomial dzero_inflated_negbinomial dzero_inflated_poisson empty_prior exgaussian exponential expose_functions expp1 extract_draws fcor fixef frechet gen_extreme_value geometric get_dpar get_prior get_y gp gr horseshoe hurdle_cumulative hurdle_gamma hurdle_lognormal hurdle_negbinomial hurdle_poisson hypothesis inits inv_logit_scaled is.brmsfit is.brmsfit_multiple is.brmsformula is.brmsprior is.brmsterms is.cor_arma is.cor_brms is.cor_car is.cor_cosy is.cor_fixed is.cor_sar is.mvbrmsformula is.mvbrmsterms kfold kfold_predict lasso lf log_lik log_posterior logistic_normal logit_scaled logm1 lognormal loo LOO loo_compare loo_epred loo_linpred loo_model_weights loo_moment_match loo_predict loo_predictive_interval loo_R2 loo_subsample ma make_conditions make_stancode make_standata marginal_effects marginal_smooths mcmc_plot me mi mixture mm mmc mo model_weights multinomial mvbf mvbind mvbrmsformula nchains ndraws neff_ratio negbinomial ngrps niterations nlf nsamples nuts_params nvariables opencl parnames parse_bf pasym_laplace pbeta_binomial pexgaussian pfrechet pgen_extreme_value phurdle_gamma phurdle_lognormal phurdle_negbinomial phurdle_poisson pinv_gaussian post_prob posterior_average posterior_epred posterior_interval posterior_linpred posterior_predict posterior_samples posterior_smooths posterior_summary posterior_table pp_average pp_check pp_expect pp_mixture predictive_error predictive_interval prepare_predictions prior prior_ prior_draws prior_samples prior_string prior_summary pshifted_lnorm psis pskew_normal pstudent_t pvon_mises pzero_inflated_beta pzero_inflated_beta_binomial pzero_inflated_binomial pzero_inflated_negbinomial pzero_inflated_poisson qasym_laplace qfrechet qgen_extreme_value qshifted_lnorm qskew_normal qstudent_t R2D2 ranef rasym_laplace rbeta_binomial rdirichlet read_csv_as_stanfit recompile_model reloo rename_pars resp_bhaz resp_cat resp_cens resp_dec resp_index resp_mi resp_rate resp_se resp_subset resp_thres resp_trials resp_trunc resp_vint resp_vreal resp_weights restructure rexgaussian rfrechet rgen_extreme_value rhat rinv_gaussian rlogistic_normal rmulti_normal rmulti_student_t rows2labels rshifted_lnorm rskew_normal rstudent_t rvon_mises rwiener s sar save_pars set_mecor set_nl set_prior set_rescor shifted_lognormal skew_normal sratio stancode standata stanplot stanvar student t2 theme_black theme_default threading unstr update_adterms validate_newdata validate_prior VarCorr variables von_mises waic WAIC weibull wiener xbeta zero_inflated_beta zero_inflated_beta_binomial zero_inflated_binomial zero_inflated_negbinomial zero_inflated_poisson zero_one_inflated_beta bayesian-inference brms multilevel-models stan statistical-models"
  },
  {
    "id": 958,
    "package_name": "performance",
    "title": "Assessment of Regression Models Performance",
    "description": "Utilities for computing measures to assess model quality,\nwhich are not directly provided by R's 'base' or 'stats'\npackages. These include e.g. measures like r-squared,\nintraclass correlation coefficient (Nakagawa, Johnson &\nSchielzeth (2017) <doi:10.1098/rsif.2017.0213>), root mean\nsquared error or functions to check models for overdispersion,\nsingularity or zero-inflation and more. Functions apply to a\nlarge variety of regression models, including generalized\nlinear models, mixed effects models and Bayesian models.\nReferences: L\u00fcdecke et al. (2021) <doi:10.21105/joss.03139>.",
    "version": "0.15.3.1",
    "maintainer": "Daniel L\u00fcdecke <officialeasystats@gmail.com>",
    "author": "Daniel L\u00fcdecke [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-8895-3206>),\nDominique Makowski [aut, ctb] (ORCID:\n<https://orcid.org/0000-0001-5375-9967>),\nMattan S. Ben-Shachar [aut, ctb] (ORCID:\n<https://orcid.org/0000-0002-4287-4801>),\nIndrajeet Patil [aut, ctb] (ORCID:\n<https://orcid.org/0000-0003-1995-6531>),\nPhilip Waggoner [aut, ctb] (ORCID:\n<https://orcid.org/0000-0002-7825-7573>),\nBrenton M. Wiernik [aut, ctb] (ORCID:\n<https://orcid.org/0000-0001-9560-6336>),\nR\u00e9mi Th\u00e9riault [aut, ctb] (ORCID:\n<https://orcid.org/0000-0003-4315-6788>),\nVincent Arel-Bundock [ctb] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>),\nMartin Jullum [rev],\ngjo11 [rev],\nEtienne Bacher [ctb] (ORCID: <https://orcid.org/0000-0002-9271-5075>),\nJoseph Luchman [ctb] (ORCID: <https://orcid.org/0000-0002-8886-9717>)",
    "url": "https://easystats.github.io/performance/",
    "bug_reports": "https://github.com/easystats/performance/issues",
    "repository": "",
    "exports": [
      [
        "as.dag"
      ],
      [
        "binned_residuals"
      ],
      [
        "check_autocorrelation"
      ],
      [
        "check_clusterstructure"
      ],
      [
        "check_collinearity"
      ],
      [
        "check_concurvity"
      ],
      [
        "check_convergence"
      ],
      [
        "check_dag"
      ],
      [
        "check_distribution"
      ],
      [
        "check_factorstructure"
      ],
      [
        "check_group_variation"
      ],
      [
        "check_heterogeneity_bias"
      ],
      [
        "check_heteroscedasticity"
      ],
      [
        "check_heteroskedasticity"
      ],
      [
        "check_homogeneity"
      ],
      [
        "check_itemscale"
      ],
      [
        "check_kmo"
      ],
      [
        "check_model"
      ],
      [
        "check_multimodal"
      ],
      [
        "check_normality"
      ],
      [
        "check_outliers"
      ],
      [
        "check_overdispersion"
      ],
      [
        "check_predictions"
      ],
      [
        "check_residuals"
      ],
      [
        "check_singularity"
      ],
      [
        "check_sphericity"
      ],
      [
        "check_sphericity_bartlett"
      ],
      [
        "check_symmetry"
      ],
      [
        "check_zeroinflation"
      ],
      [
        "compare_performance"
      ],
      [
        "cronbachs_alpha"
      ],
      [
        "display"
      ],
      [
        "icc"
      ],
      [
        "item_alpha"
      ],
      [
        "item_difficulty"
      ],
      [
        "item_discrimination"
      ],
      [
        "item_intercor"
      ],
      [
        "item_omega"
      ],
      [
        "item_reliability"
      ],
      [
        "item_split_half"
      ],
      [
        "item_totalcor"
      ],
      [
        "looic"
      ],
      [
        "mae"
      ],
      [
        "model_performance"
      ],
      [
        "mse"
      ],
      [
        "multicollinearity"
      ],
      [
        "performance"
      ],
      [
        "performance_accuracy"
      ],
      [
        "performance_aic"
      ],
      [
        "performance_aicc"
      ],
      [
        "performance_cv"
      ],
      [
        "performance_dvour"
      ],
      [
        "performance_hosmer"
      ],
      [
        "performance_logloss"
      ],
      [
        "performance_mae"
      ],
      [
        "performance_mse"
      ],
      [
        "performance_pcp"
      ],
      [
        "performance_reliability"
      ],
      [
        "performance_rmse"
      ],
      [
        "performance_roc"
      ],
      [
        "performance_rse"
      ],
      [
        "performance_score"
      ],
      [
        "print_html"
      ],
      [
        "print_md"
      ],
      [
        "r2"
      ],
      [
        "r2_bayes"
      ],
      [
        "r2_coxsnell"
      ],
      [
        "r2_efron"
      ],
      [
        "r2_ferrari"
      ],
      [
        "r2_kullback"
      ],
      [
        "r2_loo"
      ],
      [
        "r2_loo_posterior"
      ],
      [
        "r2_mcfadden"
      ],
      [
        "r2_mckelvey"
      ],
      [
        "r2_mlm"
      ],
      [
        "r2_nagelkerke"
      ],
      [
        "r2_nakagawa"
      ],
      [
        "r2_posterior"
      ],
      [
        "r2_somers"
      ],
      [
        "r2_tjur"
      ],
      [
        "r2_xu"
      ],
      [
        "r2_zeroinflated"
      ],
      [
        "rmse"
      ],
      [
        "simulate_residuals"
      ],
      [
        "test_bf"
      ],
      [
        "test_likelihoodratio"
      ],
      [
        "test_lrt"
      ],
      [
        "test_performance"
      ],
      [
        "test_vuong"
      ],
      [
        "test_wald"
      ],
      [
        "variance_decomposition"
      ]
    ],
    "topics": [
      [
        "aic"
      ],
      [
        "easystats"
      ],
      [
        "hacktoberfest"
      ],
      [
        "loo"
      ],
      [
        "machine-learning"
      ],
      [
        "mixed-models"
      ],
      [
        "models"
      ],
      [
        "performance"
      ],
      [
        "r2"
      ],
      [
        "statistics"
      ]
    ],
    "score": 16.6888,
    "stars": 1115,
    "primary_category": "statistics",
    "source_universe": "easystats",
    "search_text": "performance Assessment of Regression Models Performance Utilities for computing measures to assess model quality,\nwhich are not directly provided by R's 'base' or 'stats'\npackages. These include e.g. measures like r-squared,\nintraclass correlation coefficient (Nakagawa, Johnson &\nSchielzeth (2017) <doi:10.1098/rsif.2017.0213>), root mean\nsquared error or functions to check models for overdispersion,\nsingularity or zero-inflation and more. Functions apply to a\nlarge variety of regression models, including generalized\nlinear models, mixed effects models and Bayesian models.\nReferences: L\u00fcdecke et al. (2021) <doi:10.21105/joss.03139>. as.dag binned_residuals check_autocorrelation check_clusterstructure check_collinearity check_concurvity check_convergence check_dag check_distribution check_factorstructure check_group_variation check_heterogeneity_bias check_heteroscedasticity check_heteroskedasticity check_homogeneity check_itemscale check_kmo check_model check_multimodal check_normality check_outliers check_overdispersion check_predictions check_residuals check_singularity check_sphericity check_sphericity_bartlett check_symmetry check_zeroinflation compare_performance cronbachs_alpha display icc item_alpha item_difficulty item_discrimination item_intercor item_omega item_reliability item_split_half item_totalcor looic mae model_performance mse multicollinearity performance performance_accuracy performance_aic performance_aicc performance_cv performance_dvour performance_hosmer performance_logloss performance_mae performance_mse performance_pcp performance_reliability performance_rmse performance_roc performance_rse performance_score print_html print_md r2 r2_bayes r2_coxsnell r2_efron r2_ferrari r2_kullback r2_loo r2_loo_posterior r2_mcfadden r2_mckelvey r2_mlm r2_nagelkerke r2_nakagawa r2_posterior r2_somers r2_tjur r2_xu r2_zeroinflated rmse simulate_residuals test_bf test_likelihoodratio test_lrt test_performance test_vuong test_wald variance_decomposition aic easystats hacktoberfest loo machine-learning mixed-models models performance r2 statistics"
  },
  {
    "id": 1085,
    "package_name": "rentrez",
    "title": "'Entrez' in R",
    "description": "Provides an R interface to the NCBI's 'EUtils' API,\nallowing users to search databases like 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and 'PubMed'\n<https://pubmed.ncbi.nlm.nih.gov/>, process the results of\nthose searches and pull data into their R sessions.",
    "version": "1.2.4",
    "maintainer": "Indraneel Chakraborty <hello.indraneel@gmail.com>",
    "author": "David Winter [aut] (ORCID: <https://orcid.org/0000-0002-6165-0029>),\nScott Chamberlain [ctb] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nHan Guangchun [ctb] (ORCID: <https://orcid.org/0000-0001-9277-2507>),\nIndraneel Chakraborty [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6958-8269>)",
    "url": "https://github.com/ropensci/rentrez/",
    "bug_reports": "https://github.com/ropensci/rentrez/issues/",
    "repository": "",
    "exports": [
      [
        "entrez_citmatch"
      ],
      [
        "entrez_db_links"
      ],
      [
        "entrez_db_searchable"
      ],
      [
        "entrez_db_summary"
      ],
      [
        "entrez_dbs"
      ],
      [
        "entrez_fetch"
      ],
      [
        "entrez_global_query"
      ],
      [
        "entrez_info"
      ],
      [
        "entrez_link"
      ],
      [
        "entrez_post"
      ],
      [
        "entrez_search"
      ],
      [
        "entrez_summary"
      ],
      [
        "extract_from_esummary"
      ],
      [
        "linkout_urls"
      ],
      [
        "parse_pubmed_xml"
      ],
      [
        "set_entrez_key"
      ]
    ],
    "topics": [],
    "score": 14.6938,
    "stars": 212,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rentrez 'Entrez' in R Provides an R interface to the NCBI's 'EUtils' API,\nallowing users to search databases like 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and 'PubMed'\n<https://pubmed.ncbi.nlm.nih.gov/>, process the results of\nthose searches and pull data into their R sessions. entrez_citmatch entrez_db_links entrez_db_searchable entrez_db_summary entrez_dbs entrez_fetch entrez_global_query entrez_info entrez_link entrez_post entrez_search entrez_summary extract_from_esummary linkout_urls parse_pubmed_xml set_entrez_key "
  },
  {
    "id": 604,
    "package_name": "generics",
    "title": "Common S3 Generics not Provided by Base R Methods Related to\nModel Fitting",
    "description": "In order to reduce potential package dependencies and\nconflicts, generics provides a number of commonly used S3\ngenerics.",
    "version": "0.1.4.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4757-117X>),\nMax Kuhn [aut],\nDavis Vaughan [aut],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://generics.r-lib.org, https://github.com/r-lib/generics",
    "bug_reports": "https://github.com/r-lib/generics/issues",
    "repository": "",
    "exports": [
      [
        "accuracy"
      ],
      [
        "as.difftime"
      ],
      [
        "as.factor"
      ],
      [
        "as.ordered"
      ],
      [
        "augment"
      ],
      [
        "calculate"
      ],
      [
        "compile"
      ],
      [
        "components"
      ],
      [
        "equation"
      ],
      [
        "estfun"
      ],
      [
        "evaluate"
      ],
      [
        "explain"
      ],
      [
        "explore"
      ],
      [
        "fit"
      ],
      [
        "fit_xy"
      ],
      [
        "forecast"
      ],
      [
        "generate"
      ],
      [
        "glance"
      ],
      [
        "hypothesize"
      ],
      [
        "interpolate"
      ],
      [
        "intersect"
      ],
      [
        "is.element"
      ],
      [
        "learn"
      ],
      [
        "min_grid"
      ],
      [
        "prune"
      ],
      [
        "rank_results"
      ],
      [
        "refit"
      ],
      [
        "required_pkgs"
      ],
      [
        "setdiff"
      ],
      [
        "setequal"
      ],
      [
        "specify"
      ],
      [
        "tidy"
      ],
      [
        "train"
      ],
      [
        "tunable"
      ],
      [
        "tune_args"
      ],
      [
        "union"
      ],
      [
        "var_imp"
      ],
      [
        "varying_args"
      ],
      [
        "visualize"
      ]
    ],
    "topics": [],
    "score": 14.2047,
    "stars": 61,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "generics Common S3 Generics not Provided by Base R Methods Related to\nModel Fitting In order to reduce potential package dependencies and\nconflicts, generics provides a number of commonly used S3\ngenerics. accuracy as.difftime as.factor as.ordered augment calculate compile components equation estfun evaluate explain explore fit fit_xy forecast generate glance hypothesize interpolate intersect is.element learn min_grid prune rank_results refit required_pkgs setdiff setequal specify tidy train tunable tune_args union var_imp varying_args visualize "
  },
  {
    "id": 396,
    "package_name": "conflicted",
    "title": "An Alternative Conflict Resolution Strategy",
    "description": "R's default conflict management system gives the most\nrecently loaded package precedence. This can make it hard to\ndetect conflicts, particularly when they arise because a\npackage update creates ambiguity that did not previously exist.\n'conflicted' takes a different approach, making every conflict\nan error and forcing you to choose which function to use.",
    "version": "1.2.0.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://conflicted.r-lib.org/, https://github.com/r-lib/conflicted",
    "bug_reports": "https://github.com/r-lib/conflicted/issues",
    "repository": "",
    "exports": [
      [
        "conflict_prefer"
      ],
      [
        "conflict_prefer_all"
      ],
      [
        "conflict_prefer_matching"
      ],
      [
        "conflict_scout"
      ],
      [
        "conflicts_prefer"
      ]
    ],
    "topics": [
      [
        "conflicts"
      ]
    ],
    "score": 14.1014,
    "stars": 254,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "conflicted An Alternative Conflict Resolution Strategy R's default conflict management system gives the most\nrecently loaded package precedence. This can make it hard to\ndetect conflicts, particularly when they arise because a\npackage update creates ambiguity that did not previously exist.\n'conflicted' takes a different approach, making every conflict\nan error and forcing you to choose which function to use. conflict_prefer conflict_prefer_all conflict_prefer_matching conflict_scout conflicts_prefer conflicts"
  },
  {
    "id": 937,
    "package_name": "pak",
    "title": "Another Approach to Package Installation",
    "description": "The goal of 'pak' is to make package installation faster\nand more reliable. In particular, it performs all HTTP\noperations in parallel, so metadata resolution and package\ndownloads are fast. Metadata and package files are cached on\nthe local disk as well. 'pak' has a dependency solver, so it\nfinds version conflicts before performing the installation.\nThis version of 'pak' supports CRAN, 'Bioconductor' and\n'GitHub' packages as well.",
    "version": "0.9.1.9000",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nJim Hester [aut],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\nWinston Chang [ctb] (R6, callr, processx),\nAscent Digital Services [cph, fnd] (callr, processx),\nHadley Wickham [ctb, cph] (cli, curl, pkgbuild, yaml),\nJeroen Ooms [ctb] (curl, jsonlite),\nMa\u00eblle Salmon [ctb] (desc, pkgsearch),\nDuncan Temple Lang [ctb] (jsonlite),\nLloyd Hilaiel [cph] (jsonlite),\nAlec Wong [ctb] (keyring),\nMichel Berkelaar and lpSolve authors [ctb] (lpSolve),\nR Consortium [fnd] (pkgsearch),\nJay Loden [ctb] (ps),\nDave Daeschler [ctb] (ps),\nGiampaolo Rodola [ctb] (ps),\nShawn Garbett [ctb] (yaml),\nJeremy Stephens [ctb] (yaml),\nKirill Simonov [ctb] (yaml),\nYihui Xie [ctb] (yaml),\nZhuoer Dong [ctb] (yaml),\nJeffrey Horner [ctb] (yaml),\nWill Beasley [ctb] (yaml),\nBrendan O'Connor [ctb] (yaml),\nGregory Warnes [ctb] (yaml),\nMichael Quinn [ctb] (yaml),\nZhian Kamvar [ctb] (yaml),\nCharlie Gao [ctb] (yaml),\nKuba Podg\u00f3rski [ctb] (zip),\nRich Geldreich [ctb] (zip)",
    "url": "https://pak.r-lib.org/, https://github.com/r-lib/pak",
    "bug_reports": "https://github.com/r-lib/pak/issues",
    "repository": "",
    "exports": [
      [
        "cache_clean"
      ],
      [
        "cache_delete"
      ],
      [
        "cache_list"
      ],
      [
        "cache_summary"
      ],
      [
        "handle_package_not_found"
      ],
      [
        "lib_status"
      ],
      [
        "local_deps"
      ],
      [
        "local_deps_explain"
      ],
      [
        "local_deps_tree"
      ],
      [
        "local_dev_deps"
      ],
      [
        "local_dev_deps_explain"
      ],
      [
        "local_dev_deps_tree"
      ],
      [
        "local_install"
      ],
      [
        "local_install_deps"
      ],
      [
        "local_install_dev_deps"
      ],
      [
        "local_system_requirements"
      ],
      [
        "lockfile_create"
      ],
      [
        "lockfile_install"
      ],
      [
        "meta_clean"
      ],
      [
        "meta_list"
      ],
      [
        "meta_summary"
      ],
      [
        "meta_update"
      ],
      [
        "pak"
      ],
      [
        "pak_cleanup"
      ],
      [
        "pak_install_extra"
      ],
      [
        "pak_setup"
      ],
      [
        "pak_sitrep"
      ],
      [
        "pak_update"
      ],
      [
        "pkg_deps"
      ],
      [
        "pkg_deps_explain"
      ],
      [
        "pkg_deps_tree"
      ],
      [
        "pkg_download"
      ],
      [
        "pkg_history"
      ],
      [
        "pkg_install"
      ],
      [
        "pkg_list"
      ],
      [
        "pkg_name_check"
      ],
      [
        "pkg_remove"
      ],
      [
        "pkg_search"
      ],
      [
        "pkg_status"
      ],
      [
        "pkg_sysreqs"
      ],
      [
        "pkg_system_requirements"
      ],
      [
        "ppm_has_binaries"
      ],
      [
        "ppm_platforms"
      ],
      [
        "ppm_r_versions"
      ],
      [
        "ppm_repo_url"
      ],
      [
        "ppm_snapshots"
      ],
      [
        "repo_add"
      ],
      [
        "repo_auth"
      ],
      [
        "repo_auth_key_get"
      ],
      [
        "repo_auth_key_set"
      ],
      [
        "repo_auth_unlock"
      ],
      [
        "repo_get"
      ],
      [
        "repo_ping"
      ],
      [
        "repo_resolve"
      ],
      [
        "repo_status"
      ],
      [
        "scan_deps"
      ],
      [
        "sysreqs_check_installed"
      ],
      [
        "sysreqs_db_list"
      ],
      [
        "sysreqs_db_match"
      ],
      [
        "sysreqs_db_update"
      ],
      [
        "sysreqs_fix_installed"
      ],
      [
        "sysreqs_is_supported"
      ],
      [
        "sysreqs_list_system_packages"
      ],
      [
        "sysreqs_platforms"
      ],
      [
        "system_r_platform"
      ],
      [
        "system_r_platform_data"
      ]
    ],
    "topics": [],
    "score": 13.3618,
    "stars": 774,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "pak Another Approach to Package Installation The goal of 'pak' is to make package installation faster\nand more reliable. In particular, it performs all HTTP\noperations in parallel, so metadata resolution and package\ndownloads are fast. Metadata and package files are cached on\nthe local disk as well. 'pak' has a dependency solver, so it\nfinds version conflicts before performing the installation.\nThis version of 'pak' supports CRAN, 'Bioconductor' and\n'GitHub' packages as well. cache_clean cache_delete cache_list cache_summary handle_package_not_found lib_status local_deps local_deps_explain local_deps_tree local_dev_deps local_dev_deps_explain local_dev_deps_tree local_install local_install_deps local_install_dev_deps local_system_requirements lockfile_create lockfile_install meta_clean meta_list meta_summary meta_update pak pak_cleanup pak_install_extra pak_setup pak_sitrep pak_update pkg_deps pkg_deps_explain pkg_deps_tree pkg_download pkg_history pkg_install pkg_list pkg_name_check pkg_remove pkg_search pkg_status pkg_sysreqs pkg_system_requirements ppm_has_binaries ppm_platforms ppm_r_versions ppm_repo_url ppm_snapshots repo_add repo_auth repo_auth_key_get repo_auth_key_set repo_auth_unlock repo_get repo_ping repo_resolve repo_status scan_deps sysreqs_check_installed sysreqs_db_list sysreqs_db_match sysreqs_db_update sysreqs_fix_installed sysreqs_is_supported sysreqs_list_system_packages sysreqs_platforms system_r_platform system_r_platform_data "
  },
  {
    "id": 864,
    "package_name": "nanonext",
    "title": "NNG (Nanomsg Next Gen) Lightweight Messaging Library",
    "description": "R binding for NNG (Nanomsg Next Gen), a successor to\nZeroMQ. NNG is a socket library for reliable, high-performance\nmessaging over in-process, IPC, TCP, WebSocket and secure TLS\ntransports. Implements 'Scalability Protocols', a standard for\ncommon communications patterns including publish/subscribe,\nrequest/reply and service discovery. As its own threaded\nconcurrency framework, provides a toolkit for asynchronous\nprogramming and distributed computing. Intuitive 'aio' objects\nresolve automatically when asynchronous operations complete,\nand synchronisation primitives allow R to wait upon events\nsignalled by concurrent threads.",
    "version": "1.7.2.9000",
    "maintainer": "Charlie Gao <charlie.gao@posit.co>",
    "author": "Charlie Gao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0750-061X>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\nHibiki AI Limited [cph],\nR Consortium [fnd]",
    "url": "https://nanonext.r-lib.org, https://github.com/r-lib/nanonext",
    "bug_reports": "https://github.com/r-lib/nanonext/issues",
    "repository": "",
    "exports": [
      [
        ".advance"
      ],
      [
        ".context"
      ],
      [
        ".keep"
      ],
      [
        ".mark"
      ],
      [
        ".read_header"
      ],
      [
        ".read_marker"
      ],
      [
        ".unresolved"
      ],
      [
        "%~>%"
      ],
      [
        "call_aio"
      ],
      [
        "call_aio_"
      ],
      [
        "collect_aio"
      ],
      [
        "collect_aio_"
      ],
      [
        "context"
      ],
      [
        "cv"
      ],
      [
        "cv_reset"
      ],
      [
        "cv_signal"
      ],
      [
        "cv_value"
      ],
      [
        "dial"
      ],
      [
        "ip_addr"
      ],
      [
        "is_aio"
      ],
      [
        "is_error_value"
      ],
      [
        "is_nano"
      ],
      [
        "is_ncurl_session"
      ],
      [
        "is_nul_byte"
      ],
      [
        "listen"
      ],
      [
        "mclock"
      ],
      [
        "messenger"
      ],
      [
        "monitor"
      ],
      [
        "msleep"
      ],
      [
        "nano"
      ],
      [
        "ncurl"
      ],
      [
        "ncurl_aio"
      ],
      [
        "ncurl_session"
      ],
      [
        "nng_error"
      ],
      [
        "nng_version"
      ],
      [
        "opt"
      ],
      [
        "opt<-"
      ],
      [
        "parse_url"
      ],
      [
        "pipe_id"
      ],
      [
        "pipe_notify"
      ],
      [
        "random"
      ],
      [
        "read_monitor"
      ],
      [
        "read_stdin"
      ],
      [
        "reap"
      ],
      [
        "recv"
      ],
      [
        "recv_aio"
      ],
      [
        "reply"
      ],
      [
        "request"
      ],
      [
        "send"
      ],
      [
        "send_aio"
      ],
      [
        "serial_config"
      ],
      [
        "socket"
      ],
      [
        "stat"
      ],
      [
        "status_code"
      ],
      [
        "stop_aio"
      ],
      [
        "stop_request"
      ],
      [
        "stream"
      ],
      [
        "subscribe"
      ],
      [
        "survey_time"
      ],
      [
        "tls_config"
      ],
      [
        "transact"
      ],
      [
        "unresolved"
      ],
      [
        "unsubscribe"
      ],
      [
        "until"
      ],
      [
        "until_"
      ],
      [
        "wait"
      ],
      [
        "wait_"
      ],
      [
        "write_cert"
      ],
      [
        "write_stdout"
      ]
    ],
    "topics": [
      [
        "concurrency"
      ],
      [
        "https"
      ],
      [
        "ipc-message"
      ],
      [
        "messaging-library"
      ],
      [
        "nng"
      ],
      [
        "rpc"
      ],
      [
        "socket-communication"
      ],
      [
        "synchronization-primitives"
      ],
      [
        "tcp-protocol"
      ],
      [
        "websocket"
      ],
      [
        "mbedtls"
      ]
    ],
    "score": 12.3829,
    "stars": 75,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "nanonext NNG (Nanomsg Next Gen) Lightweight Messaging Library R binding for NNG (Nanomsg Next Gen), a successor to\nZeroMQ. NNG is a socket library for reliable, high-performance\nmessaging over in-process, IPC, TCP, WebSocket and secure TLS\ntransports. Implements 'Scalability Protocols', a standard for\ncommon communications patterns including publish/subscribe,\nrequest/reply and service discovery. As its own threaded\nconcurrency framework, provides a toolkit for asynchronous\nprogramming and distributed computing. Intuitive 'aio' objects\nresolve automatically when asynchronous operations complete,\nand synchronisation primitives allow R to wait upon events\nsignalled by concurrent threads. .advance .context .keep .mark .read_header .read_marker .unresolved %~>% call_aio call_aio_ collect_aio collect_aio_ context cv cv_reset cv_signal cv_value dial ip_addr is_aio is_error_value is_nano is_ncurl_session is_nul_byte listen mclock messenger monitor msleep nano ncurl ncurl_aio ncurl_session nng_error nng_version opt opt<- parse_url pipe_id pipe_notify random read_monitor read_stdin reap recv recv_aio reply request send send_aio serial_config socket stat status_code stop_aio stop_request stream subscribe survey_time tls_config transact unresolved unsubscribe until until_ wait wait_ write_cert write_stdout concurrency https ipc-message messaging-library nng rpc socket-communication synchronization-primitives tcp-protocol websocket mbedtls"
  },
  {
    "id": 296,
    "package_name": "biomartr",
    "title": "Genomic Data Retrieval",
    "description": "Perform large scale genomic data retrieval and functional\nannotation retrieval. This package aims to provide users with a\nstandardized way to automate genome, proteome, 'RNA', coding\nsequence ('CDS'), 'GFF', and metagenome retrieval from 'NCBI\nRefSeq', 'NCBI Genbank', 'ENSEMBL', and 'UniProt' databases.\nFurthermore, an interface to the 'BioMart' database (Smedley et\nal. (2009) <doi:10.1186/1471-2164-10-22>) allows users to\nretrieve functional annotation for genomic loci. In addition,\nusers can download entire databases such as 'NCBI RefSeq'\n(Pruitt et al. (2007) <doi:10.1093/nar/gkl842>), 'NCBI nr',\n'NCBI nt', 'NCBI Genbank' (Benson et al. (2013)\n<doi:10.1093/nar/gks1195>), etc. with only one command.",
    "version": "1.0.11",
    "maintainer": "Hajk-Georg Drost <hajk-georg.drost@tuebingen.mpg.de>",
    "author": "Hajk-Georg Drost [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-1567-306X>),\nHaakon Tjeldnes [aut, ctb]",
    "url": "https://docs.ropensci.org/biomartr/,\nhttps://github.com/ropensci/biomartr",
    "bug_reports": "https://github.com/ropensci/biomartr/issues",
    "repository": "",
    "exports": [
      [
        "biomart"
      ],
      [
        "cachedir"
      ],
      [
        "cachedir_set"
      ],
      [
        "check_annotation_biomartr"
      ],
      [
        "clean.retrieval"
      ],
      [
        "download.database"
      ],
      [
        "download.database.all"
      ],
      [
        "ensembl_divisions"
      ],
      [
        "get.ensembl.info"
      ],
      [
        "getAssemblyStats"
      ],
      [
        "getAttributes"
      ],
      [
        "getBioSet"
      ],
      [
        "getCDS"
      ],
      [
        "getCDSSet"
      ],
      [
        "getCollection"
      ],
      [
        "getCollectionSet"
      ],
      [
        "getDatasets"
      ],
      [
        "getENSEMBLGENOMESInfo"
      ],
      [
        "getENSEMBLInfo"
      ],
      [
        "getFilters"
      ],
      [
        "getGenome"
      ],
      [
        "getGENOMEREPORT"
      ],
      [
        "getGenomeSet"
      ],
      [
        "getGFF"
      ],
      [
        "getGFFSet"
      ],
      [
        "getGO"
      ],
      [
        "getGroups"
      ],
      [
        "getGTF"
      ],
      [
        "getKingdomAssemblySummary"
      ],
      [
        "getKingdoms"
      ],
      [
        "getMarts"
      ],
      [
        "getMetaGenomeAnnotations"
      ],
      [
        "getMetaGenomes"
      ],
      [
        "getMetaGenomeSummary"
      ],
      [
        "getProteome"
      ],
      [
        "getProteomeSet"
      ],
      [
        "getReleases"
      ],
      [
        "getRepeatMasker"
      ],
      [
        "getRNA"
      ],
      [
        "getRNASet"
      ],
      [
        "getSummaryFile"
      ],
      [
        "getUniProtInfo"
      ],
      [
        "getUniProtSTATS"
      ],
      [
        "is.genome.available"
      ],
      [
        "listDatabases"
      ],
      [
        "listGenomes"
      ],
      [
        "listGroups"
      ],
      [
        "listKingdoms"
      ],
      [
        "listMetaGenomes"
      ],
      [
        "listNCBIDatabases"
      ],
      [
        "meta.retrieval"
      ],
      [
        "meta.retrieval.all"
      ],
      [
        "organismAttributes"
      ],
      [
        "organismBM"
      ],
      [
        "organismFilters"
      ],
      [
        "read_assemblystats"
      ],
      [
        "read_cds"
      ],
      [
        "read_genome"
      ],
      [
        "read_gff"
      ],
      [
        "read_proteome"
      ],
      [
        "read_rm"
      ],
      [
        "read_rna"
      ],
      [
        "refseqOrganisms"
      ],
      [
        "summary_cds"
      ],
      [
        "summary_genome"
      ]
    ],
    "topics": [
      [
        "biomart"
      ],
      [
        "genomic-data-retrieval"
      ],
      [
        "annotation-retrieval"
      ],
      [
        "database-retrieval"
      ],
      [
        "ncbi"
      ],
      [
        "ensembl"
      ],
      [
        "biological-data-retrieval"
      ],
      [
        "ensembl-servers"
      ],
      [
        "genome"
      ],
      [
        "genome-annotation"
      ],
      [
        "genome-retrieval"
      ],
      [
        "genomics"
      ],
      [
        "meta-analysis"
      ],
      [
        "metagenomics"
      ],
      [
        "ncbi-genbank"
      ],
      [
        "peer-reviewed"
      ],
      [
        "proteome"
      ],
      [
        "sequenced-genomes"
      ]
    ],
    "score": 11.7795,
    "stars": 222,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "biomartr Genomic Data Retrieval Perform large scale genomic data retrieval and functional\nannotation retrieval. This package aims to provide users with a\nstandardized way to automate genome, proteome, 'RNA', coding\nsequence ('CDS'), 'GFF', and metagenome retrieval from 'NCBI\nRefSeq', 'NCBI Genbank', 'ENSEMBL', and 'UniProt' databases.\nFurthermore, an interface to the 'BioMart' database (Smedley et\nal. (2009) <doi:10.1186/1471-2164-10-22>) allows users to\nretrieve functional annotation for genomic loci. In addition,\nusers can download entire databases such as 'NCBI RefSeq'\n(Pruitt et al. (2007) <doi:10.1093/nar/gkl842>), 'NCBI nr',\n'NCBI nt', 'NCBI Genbank' (Benson et al. (2013)\n<doi:10.1093/nar/gks1195>), etc. with only one command. biomart cachedir cachedir_set check_annotation_biomartr clean.retrieval download.database download.database.all ensembl_divisions get.ensembl.info getAssemblyStats getAttributes getBioSet getCDS getCDSSet getCollection getCollectionSet getDatasets getENSEMBLGENOMESInfo getENSEMBLInfo getFilters getGenome getGENOMEREPORT getGenomeSet getGFF getGFFSet getGO getGroups getGTF getKingdomAssemblySummary getKingdoms getMarts getMetaGenomeAnnotations getMetaGenomes getMetaGenomeSummary getProteome getProteomeSet getReleases getRepeatMasker getRNA getRNASet getSummaryFile getUniProtInfo getUniProtSTATS is.genome.available listDatabases listGenomes listGroups listKingdoms listMetaGenomes listNCBIDatabases meta.retrieval meta.retrieval.all organismAttributes organismBM organismFilters read_assemblystats read_cds read_genome read_gff read_proteome read_rm read_rna refseqOrganisms summary_cds summary_genome biomart genomic-data-retrieval annotation-retrieval database-retrieval ncbi ensembl biological-data-retrieval ensembl-servers genome genome-annotation genome-retrieval genomics meta-analysis metagenomics ncbi-genbank peer-reviewed proteome sequenced-genomes"
  },
  {
    "id": 1301,
    "package_name": "tarchetypes",
    "title": "Archetypes for Targets",
    "description": "Function-oriented Make-like declarative pipelines for\nStatistics and data science are supported in the 'targets' R\npackage. As an extension to 'targets', the 'tarchetypes'\npackage provides convenient user-side functions to make\n'targets' easier to use. By establishing reusable archetypes\nfor common kinds of targets and pipelines, these functions help\nexpress complicated reproducible pipelines concisely and\ncompactly. The methods in this package were influenced by the\n'targets' R package. by Will Landau (2018)\n<doi:10.21105/joss.00550>.",
    "version": "0.13.2.9004",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nRudolf Siegel [ctb] (ORCID: <https://orcid.org/0000-0002-6021-804X>),\nSamantha Oliver [rev] (ORCID: <https://orcid.org/0000-0001-5668-1165>),\nTristan Mahr [rev] (ORCID: <https://orcid.org/0000-0002-8890-5116>),\nEli Lilly and Company [cph, fnd]",
    "url": "https://docs.ropensci.org/tarchetypes/,\nhttps://github.com/ropensci/tarchetypes",
    "bug_reports": "https://github.com/ropensci/tarchetypes/issues",
    "repository": "",
    "exports": [
      [
        "all_of"
      ],
      [
        "any_of"
      ],
      [
        "contains"
      ],
      [
        "counter_init"
      ],
      [
        "counter_set_names"
      ],
      [
        "ends_with"
      ],
      [
        "everything"
      ],
      [
        "last_col"
      ],
      [
        "matches"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "starts_with"
      ],
      [
        "tar_age"
      ],
      [
        "tar_append_static_values"
      ],
      [
        "tar_arrow_feather"
      ],
      [
        "tar_assign"
      ],
      [
        "tar_aws_file"
      ],
      [
        "tar_aws_fst"
      ],
      [
        "tar_aws_fst_dt"
      ],
      [
        "tar_aws_fst_tbl"
      ],
      [
        "tar_aws_keras"
      ],
      [
        "tar_aws_parquet"
      ],
      [
        "tar_aws_qs"
      ],
      [
        "tar_aws_rds"
      ],
      [
        "tar_aws_torch"
      ],
      [
        "tar_change"
      ],
      [
        "tar_combine"
      ],
      [
        "tar_combine_raw"
      ],
      [
        "tar_cue_age"
      ],
      [
        "tar_cue_age_raw"
      ],
      [
        "tar_cue_force"
      ],
      [
        "tar_cue_skip"
      ],
      [
        "tar_download"
      ],
      [
        "tar_download_run"
      ],
      [
        "tar_eval"
      ],
      [
        "tar_eval_raw"
      ],
      [
        "tar_file"
      ],
      [
        "tar_file_fast"
      ],
      [
        "tar_file_read"
      ],
      [
        "tar_files"
      ],
      [
        "tar_files_input"
      ],
      [
        "tar_files_input_raw"
      ],
      [
        "tar_files_raw"
      ],
      [
        "tar_force"
      ],
      [
        "tar_force_change"
      ],
      [
        "tar_format_aws_feather"
      ],
      [
        "tar_format_feather"
      ],
      [
        "tar_format_nanoparquet"
      ],
      [
        "tar_fst"
      ],
      [
        "tar_fst_dt"
      ],
      [
        "tar_fst_tbl"
      ],
      [
        "tar_group_by"
      ],
      [
        "tar_group_by_run"
      ],
      [
        "tar_group_count"
      ],
      [
        "tar_group_count_index"
      ],
      [
        "tar_group_count_run"
      ],
      [
        "tar_group_select"
      ],
      [
        "tar_group_select_run"
      ],
      [
        "tar_group_size"
      ],
      [
        "tar_group_size_index"
      ],
      [
        "tar_group_size_run"
      ],
      [
        "tar_hook_before"
      ],
      [
        "tar_hook_before_raw"
      ],
      [
        "tar_hook_inner"
      ],
      [
        "tar_hook_inner_raw"
      ],
      [
        "tar_hook_outer"
      ],
      [
        "tar_hook_outer_raw"
      ],
      [
        "tar_keras"
      ],
      [
        "tar_knit"
      ],
      [
        "tar_knit_raw"
      ],
      [
        "tar_knit_run"
      ],
      [
        "tar_knitr_deps"
      ],
      [
        "tar_knitr_deps_expr"
      ],
      [
        "tar_map"
      ],
      [
        "tar_map_rep"
      ],
      [
        "tar_map_rep_raw"
      ],
      [
        "tar_map2"
      ],
      [
        "tar_map2_count"
      ],
      [
        "tar_map2_count_raw"
      ],
      [
        "tar_map2_group"
      ],
      [
        "tar_map2_raw"
      ],
      [
        "tar_map2_run"
      ],
      [
        "tar_map2_run_rep"
      ],
      [
        "tar_map2_size"
      ],
      [
        "tar_map2_size_raw"
      ],
      [
        "tar_nanoparquet"
      ],
      [
        "tar_nanoparquet_convert"
      ],
      [
        "tar_nanoparquet_read"
      ],
      [
        "tar_nanoparquet_write"
      ],
      [
        "tar_parquet"
      ],
      [
        "tar_plan"
      ],
      [
        "tar_qs"
      ],
      [
        "tar_quarto"
      ],
      [
        "tar_quarto_files"
      ],
      [
        "tar_quarto_raw"
      ],
      [
        "tar_quarto_rep"
      ],
      [
        "tar_quarto_rep_raw"
      ],
      [
        "tar_quarto_rep_rep"
      ],
      [
        "tar_quarto_rep_run"
      ],
      [
        "tar_quarto_rep_run_params"
      ],
      [
        "tar_quarto_run"
      ],
      [
        "tar_rds"
      ],
      [
        "tar_render"
      ],
      [
        "tar_render_raw"
      ],
      [
        "tar_render_rep"
      ],
      [
        "tar_render_rep_raw"
      ],
      [
        "tar_render_rep_rep"
      ],
      [
        "tar_render_rep_run"
      ],
      [
        "tar_render_rep_run_params"
      ],
      [
        "tar_render_run"
      ],
      [
        "tar_rep"
      ],
      [
        "tar_rep_index"
      ],
      [
        "tar_rep_map"
      ],
      [
        "tar_rep_map_raw"
      ],
      [
        "tar_rep_raw"
      ],
      [
        "tar_rep_run"
      ],
      [
        "tar_rep_run_map_rep"
      ],
      [
        "tar_rep2"
      ],
      [
        "tar_rep2_raw"
      ],
      [
        "tar_rep2_run"
      ],
      [
        "tar_rep2_run_rep"
      ],
      [
        "tar_select_names"
      ],
      [
        "tar_select_targets"
      ],
      [
        "tar_skip"
      ],
      [
        "tar_sub"
      ],
      [
        "tar_sub_raw"
      ],
      [
        "tar_tangle"
      ],
      [
        "tar_torch"
      ],
      [
        "tar_url"
      ],
      [
        "walk_ast"
      ],
      [
        "walk_call_knitr"
      ]
    ],
    "topics": [
      [
        "data-science"
      ],
      [
        "high-performance-computing"
      ],
      [
        "peer-reviewed"
      ],
      [
        "pipeline"
      ],
      [
        "r-targetopia"
      ],
      [
        "reproducibility"
      ],
      [
        "targets"
      ],
      [
        "workflow"
      ]
    ],
    "score": 11.7048,
    "stars": 146,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tarchetypes Archetypes for Targets Function-oriented Make-like declarative pipelines for\nStatistics and data science are supported in the 'targets' R\npackage. As an extension to 'targets', the 'tarchetypes'\npackage provides convenient user-side functions to make\n'targets' easier to use. By establishing reusable archetypes\nfor common kinds of targets and pipelines, these functions help\nexpress complicated reproducible pipelines concisely and\ncompactly. The methods in this package were influenced by the\n'targets' R package. by Will Landau (2018)\n<doi:10.21105/joss.00550>. all_of any_of contains counter_init counter_set_names ends_with everything last_col matches num_range one_of starts_with tar_age tar_append_static_values tar_arrow_feather tar_assign tar_aws_file tar_aws_fst tar_aws_fst_dt tar_aws_fst_tbl tar_aws_keras tar_aws_parquet tar_aws_qs tar_aws_rds tar_aws_torch tar_change tar_combine tar_combine_raw tar_cue_age tar_cue_age_raw tar_cue_force tar_cue_skip tar_download tar_download_run tar_eval tar_eval_raw tar_file tar_file_fast tar_file_read tar_files tar_files_input tar_files_input_raw tar_files_raw tar_force tar_force_change tar_format_aws_feather tar_format_feather tar_format_nanoparquet tar_fst tar_fst_dt tar_fst_tbl tar_group_by tar_group_by_run tar_group_count tar_group_count_index tar_group_count_run tar_group_select tar_group_select_run tar_group_size tar_group_size_index tar_group_size_run tar_hook_before tar_hook_before_raw tar_hook_inner tar_hook_inner_raw tar_hook_outer tar_hook_outer_raw tar_keras tar_knit tar_knit_raw tar_knit_run tar_knitr_deps tar_knitr_deps_expr tar_map tar_map_rep tar_map_rep_raw tar_map2 tar_map2_count tar_map2_count_raw tar_map2_group tar_map2_raw tar_map2_run tar_map2_run_rep tar_map2_size tar_map2_size_raw tar_nanoparquet tar_nanoparquet_convert tar_nanoparquet_read tar_nanoparquet_write tar_parquet tar_plan tar_qs tar_quarto tar_quarto_files tar_quarto_raw tar_quarto_rep tar_quarto_rep_raw tar_quarto_rep_rep tar_quarto_rep_run tar_quarto_rep_run_params tar_quarto_run tar_rds tar_render tar_render_raw tar_render_rep tar_render_rep_raw tar_render_rep_rep tar_render_rep_run tar_render_rep_run_params tar_render_run tar_rep tar_rep_index tar_rep_map tar_rep_map_raw tar_rep_raw tar_rep_run tar_rep_run_map_rep tar_rep2 tar_rep2_raw tar_rep2_run tar_rep2_run_rep tar_select_names tar_select_targets tar_skip tar_sub tar_sub_raw tar_tangle tar_torch tar_url walk_ast walk_call_knitr data-science high-performance-computing peer-reviewed pipeline r-targetopia reproducibility targets workflow"
  },
  {
    "id": 1332,
    "package_name": "themis",
    "title": "Extra Recipes Steps for Dealing with Unbalanced Data",
    "description": "A dataset with an uneven number of cases in each class is\nsaid to be unbalanced. Many models produce a subpar performance\non unbalanced datasets. A dataset can be balanced by increasing\nthe number of minority cases using SMOTE 2011\n<doi:10.48550/arXiv.1106.1813>, BorderlineSMOTE 2005\n<doi:10.1007/11538059_91> and ADASYN 2008\n<https://ieeexplore.ieee.org/document/4633969>. Or by\ndecreasing the number of majority cases using NearMiss 2003\n<https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf> or\nTomek link removal 1976\n<https://ieeexplore.ieee.org/document/4309452>.",
    "version": "1.0.3.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/tidymodels/themis,\nhttps://themis.tidymodels.org",
    "bug_reports": "https://github.com/tidymodels/themis/issues",
    "repository": "",
    "exports": [
      [
        "adasyn"
      ],
      [
        "bsmote"
      ],
      [
        "nearmiss"
      ],
      [
        "required_pkgs"
      ],
      [
        "smote"
      ],
      [
        "smotenc"
      ],
      [
        "step_adasyn"
      ],
      [
        "step_bsmote"
      ],
      [
        "step_downsample"
      ],
      [
        "step_nearmiss"
      ],
      [
        "step_rose"
      ],
      [
        "step_smote"
      ],
      [
        "step_smotenc"
      ],
      [
        "step_tomek"
      ],
      [
        "step_upsample"
      ],
      [
        "tidy"
      ],
      [
        "tomek"
      ],
      [
        "tunable"
      ]
    ],
    "topics": [],
    "score": 10.5555,
    "stars": 142,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "themis Extra Recipes Steps for Dealing with Unbalanced Data A dataset with an uneven number of cases in each class is\nsaid to be unbalanced. Many models produce a subpar performance\non unbalanced datasets. A dataset can be balanced by increasing\nthe number of minority cases using SMOTE 2011\n<doi:10.48550/arXiv.1106.1813>, BorderlineSMOTE 2005\n<doi:10.1007/11538059_91> and ADASYN 2008\n<https://ieeexplore.ieee.org/document/4633969>. Or by\ndecreasing the number of majority cases using NearMiss 2003\n<https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf> or\nTomek link removal 1976\n<https://ieeexplore.ieee.org/document/4309452>. adasyn bsmote nearmiss required_pkgs smote smotenc step_adasyn step_bsmote step_downsample step_nearmiss step_rose step_smote step_smotenc step_tomek step_upsample tidy tomek tunable "
  },
  {
    "id": 999,
    "package_name": "poissonreg",
    "title": "Model Wrappers for Poisson Regression",
    "description": "Bindings for Poisson regression models for use with the\n'parsnip' package. Models include simple generalized linear\nmodels, Bayesian models, and zero-inflated Poisson models\n(Zeileis, Kleiber, and Jackman (2008)\n<doi:10.18637/jss.v027.i08>).",
    "version": "1.0.1.9001",
    "maintainer": "Hannah Frick <hannah@posit.co>",
    "author": "Max Kuhn [aut] (ORCID: <https://orcid.org/0000-0003-2402-136X>),\nHannah Frick [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6049-5258>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/tidymodels/poissonreg,\nhttps://poissonreg.tidymodels.org/",
    "bug_reports": "https://github.com/tidymodels/poissonreg/issues",
    "repository": "",
    "exports": [
      [
        "tidy"
      ]
    ],
    "topics": [],
    "score": 7.4084,
    "stars": 22,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "poissonreg Model Wrappers for Poisson Regression Bindings for Poisson regression models for use with the\n'parsnip' package. Models include simple generalized linear\nmodels, Bayesian models, and zero-inflated Poisson models\n(Zeileis, Kleiber, and Jackman (2008)\n<doi:10.18637/jss.v027.i08>). tidy "
  },
  {
    "id": 254,
    "package_name": "asciicast",
    "title": "Create 'Ascii' Screen Casts from R Scripts",
    "description": "Record 'asciicast' screen casts from R scripts. Convert\nthem to animated SVG images, to be used in 'README' files, or\nblog posts. Includes 'asciinema-player' as an 'HTML' widget,\nand an 'asciicast' 'knitr' engine, to embed 'ascii' screen\ncasts in 'Rmarkdown' documents.",
    "version": "2.3.1.9000",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nRomain Francois [aut],\nMario Nebl [aut] (https://github.com/marionebl/svg-term author),\nMarcin Kulik [aut] (https://github.com/asciinema/asciinema-player\nauthor),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://asciicast.r-lib.org/, https://github.com/r-lib/asciicast",
    "bug_reports": "https://github.com/r-lib/asciicast/issues",
    "repository": "",
    "exports": [
      [
        "asciicast_options"
      ],
      [
        "asciicast_start_process"
      ],
      [
        "asciinema_player"
      ],
      [
        "clear_screen"
      ],
      [
        "default_theme"
      ],
      [
        "expect_snapshot_r_process"
      ],
      [
        "get_locales"
      ],
      [
        "init_knitr_engine"
      ],
      [
        "install_phantomjs"
      ],
      [
        "merge_casts"
      ],
      [
        "pause"
      ],
      [
        "play"
      ],
      [
        "read_cast"
      ],
      [
        "record"
      ],
      [
        "record_output"
      ],
      [
        "write_gif"
      ],
      [
        "write_html"
      ],
      [
        "write_json"
      ],
      [
        "write_svg"
      ]
    ],
    "topics": [],
    "score": 7.3286,
    "stars": 242,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "asciicast Create 'Ascii' Screen Casts from R Scripts Record 'asciicast' screen casts from R scripts. Convert\nthem to animated SVG images, to be used in 'README' files, or\nblog posts. Includes 'asciinema-player' as an 'HTML' widget,\nand an 'asciicast' 'knitr' engine, to embed 'ascii' screen\ncasts in 'Rmarkdown' documents. asciicast_options asciicast_start_process asciinema_player clear_screen default_theme expect_snapshot_r_process get_locales init_knitr_engine install_phantomjs merge_casts pause play read_cast record record_output write_gif write_html write_json write_svg "
  },
  {
    "id": 1094,
    "package_name": "restez",
    "title": "Create and Query a Local Copy of 'GenBank' in R",
    "description": "Download large sections of 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and generate a local\nSQL-based database. A user can then query this database using\n'restez' functions or through 'rentrez'\n<https://CRAN.R-project.org/package=rentrez> wrappers.",
    "version": "2.1.5.9000",
    "maintainer": "Joel H. Nitta <joelnitta@gmail.com>",
    "author": "Joel H. Nitta [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4719-7472>),\nDom Bennett [aut] (ORCID: <https://orcid.org/0000-0003-2722-1359>)",
    "url": "https://github.com/ropensci/restez,\nhttps://docs.ropensci.org/restez/",
    "bug_reports": "https://github.com/ropensci/restez/issues",
    "repository": "",
    "exports": [
      [
        "count_db_ids"
      ],
      [
        "db_create"
      ],
      [
        "db_delete"
      ],
      [
        "db_download"
      ],
      [
        "demo_db_create"
      ],
      [
        "entrez_fetch"
      ],
      [
        "gb_definition_get"
      ],
      [
        "gb_extract"
      ],
      [
        "gb_fasta_get"
      ],
      [
        "gb_organism_get"
      ],
      [
        "gb_record_get"
      ],
      [
        "gb_sequence_get"
      ],
      [
        "gb_version_get"
      ],
      [
        "is_in_db"
      ],
      [
        "list_db_ids"
      ],
      [
        "ncbi_acc_get"
      ],
      [
        "restez_connect"
      ],
      [
        "restez_disconnect"
      ],
      [
        "restez_path_get"
      ],
      [
        "restez_path_set"
      ],
      [
        "restez_path_unset"
      ],
      [
        "restez_ready"
      ],
      [
        "restez_status"
      ]
    ],
    "topics": [
      [
        "dna"
      ],
      [
        "entrez"
      ],
      [
        "genbank"
      ],
      [
        "sequence"
      ]
    ],
    "score": 7.0459,
    "stars": 27,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "restez Create and Query a Local Copy of 'GenBank' in R Download large sections of 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and generate a local\nSQL-based database. A user can then query this database using\n'restez' functions or through 'rentrez'\n<https://CRAN.R-project.org/package=rentrez> wrappers. count_db_ids db_create db_delete db_download demo_db_create entrez_fetch gb_definition_get gb_extract gb_fasta_get gb_organism_get gb_record_get gb_sequence_get gb_version_get is_in_db list_db_ids ncbi_acc_get restez_connect restez_disconnect restez_path_get restez_path_set restez_path_unset restez_ready restez_status dna entrez genbank sequence"
  },
  {
    "id": 1400,
    "package_name": "unifir",
    "title": "A Unifying API for Calling the 'Unity' '3D' Video Game Engine",
    "description": "Functions for the creation and manipulation of scenes and\nobjects within the 'Unity' '3D' video game engine\n(<https://unity.com/>). Specific focuses include the creation\nand import of terrain data and 'GameObjects' as well as scene\nmanagement.",
    "version": "0.2.4.9000",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Michael Mahoney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2402-304X>),\nWill Jones [rev] (Will reviewed the package (v. 0.2.0) for rOpenSci,\nsee <https://github.com/ropensci/software-review/issues/521>),\nTan Tran [rev] (Tan reviewed the package (v. 0.2.0) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/521>)",
    "url": "https://docs.ropensci.org/unifir/,\nhttps://github.com/ropensci/unifir",
    "bug_reports": "https://github.com/ropensci/unifir/issues",
    "repository": "",
    "exports": [
      [
        "action"
      ],
      [
        "add_default_player"
      ],
      [
        "add_default_tree"
      ],
      [
        "add_light"
      ],
      [
        "add_prop"
      ],
      [
        "add_texture"
      ],
      [
        "associate_coordinates"
      ],
      [
        "create_terrain"
      ],
      [
        "create_unity_project"
      ],
      [
        "find_unity"
      ],
      [
        "get_asset"
      ],
      [
        "import_asset"
      ],
      [
        "instantiate_prefab"
      ],
      [
        "load_png"
      ],
      [
        "load_scene"
      ],
      [
        "make_script"
      ],
      [
        "new_scene"
      ],
      [
        "read_raw"
      ],
      [
        "save_scene"
      ],
      [
        "set_active_scene"
      ],
      [
        "unifir_prop"
      ],
      [
        "unity_version"
      ],
      [
        "validate_path"
      ],
      [
        "validate_single_path"
      ],
      [
        "waiver"
      ]
    ],
    "topics": [
      [
        "unifir"
      ],
      [
        "unity"
      ],
      [
        "unity3d"
      ],
      [
        "visualization"
      ]
    ],
    "score": 6.3197,
    "stars": 29,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "unifir A Unifying API for Calling the 'Unity' '3D' Video Game Engine Functions for the creation and manipulation of scenes and\nobjects within the 'Unity' '3D' video game engine\n(<https://unity.com/>). Specific focuses include the creation\nand import of terrain data and 'GameObjects' as well as scene\nmanagement. action add_default_player add_default_tree add_light add_prop add_texture associate_coordinates create_terrain create_unity_project find_unity get_asset import_asset instantiate_prefab load_png load_scene make_script new_scene read_raw save_scene set_active_scene unifir_prop unity_version validate_path validate_single_path waiver unifir unity unity3d visualization"
  },
  {
    "id": 966,
    "package_name": "phylotaR",
    "title": "Automated Phylogenetic Sequence Cluster Identification from\n'GenBank'",
    "description": "A pipeline for the identification, within taxonomic\ngroups, of orthologous sequence clusters from 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> as the first step in a\nphylogenetic analysis. The pipeline depends on a local\nalignment search tool and is, therefore, not dependent on\ndifferences in gene naming conventions and naming errors.",
    "version": "1.3.0",
    "maintainer": "Shixiang Wang <w_shixiang@163.com>",
    "author": "Shixiang Wang [aut, cre],\nHannes Hettling [aut],\nRutger Vos [aut],\nAlexander Zizka [aut],\nDom Bennett [aut],\nAlexandre Antonelli [aut]",
    "url": "https://docs.ropensci.org/phylotaR/,\nhttps://github.com/ropensci/phylotaR#readme",
    "bug_reports": "https://github.com/ropensci/phylotaR/issues",
    "repository": "",
    "exports": [
      [
        "addClade"
      ],
      [
        "addNdmtrx"
      ],
      [
        "addTip"
      ],
      [
        "blncdTree"
      ],
      [
        "calc_mad"
      ],
      [
        "calc_wrdfrq"
      ],
      [
        "calcDstBLD"
      ],
      [
        "calcDstMtrx"
      ],
      [
        "calcDstRF"
      ],
      [
        "calcDstTrp"
      ],
      [
        "calcFrPrp"
      ],
      [
        "calcNdBlnc"
      ],
      [
        "calcNdsBlnc"
      ],
      [
        "calcOvrlp"
      ],
      [
        "calcPhyDv"
      ],
      [
        "calcPrtFrPrp"
      ],
      [
        "checkNdlst"
      ],
      [
        "checkTreeMen"
      ],
      [
        "clusters_run"
      ],
      [
        "clusters2_run"
      ],
      [
        "cTrees"
      ],
      [
        "download_run"
      ],
      [
        "drop_by_rank"
      ],
      [
        "drop_clstrs"
      ],
      [
        "drop_sqs"
      ],
      [
        "fastCheckTreeMan"
      ],
      [
        "get_clstr_slot"
      ],
      [
        "get_nsqs"
      ],
      [
        "get_ntaxa"
      ],
      [
        "get_sq_slot"
      ],
      [
        "get_stage_times"
      ],
      [
        "get_tx_slot"
      ],
      [
        "get_txids"
      ],
      [
        "getAge"
      ],
      [
        "getBiprts"
      ],
      [
        "getCnnctdNds"
      ],
      [
        "getDcsd"
      ],
      [
        "getLvng"
      ],
      [
        "getNdAge"
      ],
      [
        "getNdKids"
      ],
      [
        "getNdLng"
      ],
      [
        "getNdPD"
      ],
      [
        "getNdPrdst"
      ],
      [
        "getNdPrids"
      ],
      [
        "getNdPtids"
      ],
      [
        "getNdsAge"
      ],
      [
        "getNdsFrmTxnyms"
      ],
      [
        "getNdsKids"
      ],
      [
        "getNdsLng"
      ],
      [
        "getNdSlt"
      ],
      [
        "getNdsPD"
      ],
      [
        "getNdsPrdst"
      ],
      [
        "getNdsPrids"
      ],
      [
        "getNdsPtids"
      ],
      [
        "getNdsSlt"
      ],
      [
        "getNdsSstr"
      ],
      [
        "getNdSstr"
      ],
      [
        "getOtgrp"
      ],
      [
        "getPath"
      ],
      [
        "getPrnt"
      ],
      [
        "getSpnAge"
      ],
      [
        "getSpnsAge"
      ],
      [
        "getSubtree"
      ],
      [
        "getUnqNds"
      ],
      [
        "is_txid_in_clstr"
      ],
      [
        "is_txid_in_sq"
      ],
      [
        "isUltrmtrc"
      ],
      [
        "list_clstrrec_slots"
      ],
      [
        "list_ncbi_ranks"
      ],
      [
        "list_seqrec_slots"
      ],
      [
        "list_taxrec_slots"
      ],
      [
        "loadTreeMan"
      ],
      [
        "outfmt_get"
      ],
      [
        "parameters"
      ],
      [
        "parameters_reset"
      ],
      [
        "pinTips"
      ],
      [
        "plot_phylota_pa"
      ],
      [
        "plot_phylota_treemap"
      ],
      [
        "print"
      ],
      [
        "pstMnp"
      ],
      [
        "randTree"
      ],
      [
        "read_phylota"
      ],
      [
        "readTree"
      ],
      [
        "readTrmn"
      ],
      [
        "reset"
      ],
      [
        "restart"
      ],
      [
        "rmClade"
      ],
      [
        "rmNdmtrx"
      ],
      [
        "rmNodes"
      ],
      [
        "rmOtherSlt"
      ],
      [
        "rmTips"
      ],
      [
        "run"
      ],
      [
        "saveTreeMan"
      ],
      [
        "searchTxnyms"
      ],
      [
        "setAge"
      ],
      [
        "setNdID"
      ],
      [
        "setNdOther"
      ],
      [
        "setNdsID"
      ],
      [
        "setNdsOther"
      ],
      [
        "setNdSpn"
      ],
      [
        "setNdsSpn"
      ],
      [
        "setPD"
      ],
      [
        "setTxnyms"
      ],
      [
        "setup"
      ],
      [
        "show"
      ],
      [
        "str"
      ],
      [
        "summary"
      ],
      [
        "taxaResolve"
      ],
      [
        "taxise_run"
      ],
      [
        "twoer"
      ],
      [
        "ultrTree"
      ],
      [
        "unblncdTree"
      ],
      [
        "updateSlts"
      ],
      [
        "write_sqs"
      ],
      [
        "writeTree"
      ],
      [
        "writeTrmn"
      ]
    ],
    "topics": [
      [
        "blastn"
      ],
      [
        "genbank"
      ],
      [
        "peer-reviewed"
      ],
      [
        "phylogenetics"
      ],
      [
        "sequence-alignment"
      ]
    ],
    "score": 5.9165,
    "stars": 25,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "phylotaR Automated Phylogenetic Sequence Cluster Identification from\n'GenBank' A pipeline for the identification, within taxonomic\ngroups, of orthologous sequence clusters from 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> as the first step in a\nphylogenetic analysis. The pipeline depends on a local\nalignment search tool and is, therefore, not dependent on\ndifferences in gene naming conventions and naming errors. addClade addNdmtrx addTip blncdTree calc_mad calc_wrdfrq calcDstBLD calcDstMtrx calcDstRF calcDstTrp calcFrPrp calcNdBlnc calcNdsBlnc calcOvrlp calcPhyDv calcPrtFrPrp checkNdlst checkTreeMen clusters_run clusters2_run cTrees download_run drop_by_rank drop_clstrs drop_sqs fastCheckTreeMan get_clstr_slot get_nsqs get_ntaxa get_sq_slot get_stage_times get_tx_slot get_txids getAge getBiprts getCnnctdNds getDcsd getLvng getNdAge getNdKids getNdLng getNdPD getNdPrdst getNdPrids getNdPtids getNdsAge getNdsFrmTxnyms getNdsKids getNdsLng getNdSlt getNdsPD getNdsPrdst getNdsPrids getNdsPtids getNdsSlt getNdsSstr getNdSstr getOtgrp getPath getPrnt getSpnAge getSpnsAge getSubtree getUnqNds is_txid_in_clstr is_txid_in_sq isUltrmtrc list_clstrrec_slots list_ncbi_ranks list_seqrec_slots list_taxrec_slots loadTreeMan outfmt_get parameters parameters_reset pinTips plot_phylota_pa plot_phylota_treemap print pstMnp randTree read_phylota readTree readTrmn reset restart rmClade rmNdmtrx rmNodes rmOtherSlt rmTips run saveTreeMan searchTxnyms setAge setNdID setNdOther setNdsID setNdsOther setNdSpn setNdsSpn setPD setTxnyms setup show str summary taxaResolve taxise_run twoer ultrTree unblncdTree updateSlts write_sqs writeTree writeTrmn blastn genbank peer-reviewed phylogenetics sequence-alignment"
  },
  {
    "id": 818,
    "package_name": "mitey",
    "title": "Serial Interval and Case Reproduction Number Estimation",
    "description": "Provides methods to estimate serial intervals and\ntime-varying case reproduction numbers from infectious disease\noutbreak data. Serial intervals measure the time between\nsymptom onset in linked transmission pairs, while case\nreproduction numbers quantify how many secondary cases each\ninfected individual generates over time. These parameters are\nessential for understanding transmission dynamics, evaluating\ncontrol measures, and informing public health responses. The\npackage implements the maximum likelihood framework from Vink\net al. (2014) <doi:10.1093/aje/kwu209> for serial interval\nestimation and the retrospective method from Wallinga &\nLipsitch (2007) <doi:10.1098/rspb.2006.3754> for reproduction\nnumber estimation. Originally developed for scabies\ntransmission analysis but applicable to other infectious\ndiseases including influenza, COVID-19, and emerging pathogens.\nDesigned for epidemiologists, public health researchers, and\ninfectious disease modelers working with outbreak surveillance\ndata.",
    "version": "0.2.0.9000",
    "maintainer": "Kylie Ainslie <ainslie.kylie@gmail.com>",
    "author": "Kylie Ainslie [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0001-5419-7206>)",
    "url": "https://github.com/kylieainslie/mitey,\nhttps://kylieainslie.github.io/mitey/",
    "bug_reports": "https://github.com/kylieainslie/mitey/issues",
    "repository": "",
    "exports": [
      [
        "calculate_si_probability_matrix"
      ],
      [
        "calculate_truncation_correction"
      ],
      [
        "conv_tri_dist"
      ],
      [
        "create_day_diff_matrix"
      ],
      [
        "f_gam"
      ],
      [
        "f_norm"
      ],
      [
        "f0"
      ],
      [
        "flower"
      ],
      [
        "fupper"
      ],
      [
        "generate_synthetic_epidemic"
      ],
      [
        "integrate_component"
      ],
      [
        "integrate_components_wrapper"
      ],
      [
        "plot_si_fit"
      ],
      [
        "si_estim"
      ],
      [
        "wallinga_lipsitch"
      ],
      [
        "weighted_var"
      ],
      [
        "wt_loglik"
      ]
    ],
    "topics": [],
    "score": 5.7559,
    "stars": 6,
    "primary_category": "epidemiology",
    "source_universe": "kylieainslie",
    "search_text": "mitey Serial Interval and Case Reproduction Number Estimation Provides methods to estimate serial intervals and\ntime-varying case reproduction numbers from infectious disease\noutbreak data. Serial intervals measure the time between\nsymptom onset in linked transmission pairs, while case\nreproduction numbers quantify how many secondary cases each\ninfected individual generates over time. These parameters are\nessential for understanding transmission dynamics, evaluating\ncontrol measures, and informing public health responses. The\npackage implements the maximum likelihood framework from Vink\net al. (2014) <doi:10.1093/aje/kwu209> for serial interval\nestimation and the retrospective method from Wallinga &\nLipsitch (2007) <doi:10.1098/rspb.2006.3754> for reproduction\nnumber estimation. Originally developed for scabies\ntransmission analysis but applicable to other infectious\ndiseases including influenza, COVID-19, and emerging pathogens.\nDesigned for epidemiologists, public health researchers, and\ninfectious disease modelers working with outbreak surveillance\ndata. calculate_si_probability_matrix calculate_truncation_correction conv_tri_dist create_day_diff_matrix f_gam f_norm f0 flower fupper generate_synthetic_epidemic integrate_component integrate_components_wrapper plot_si_fit si_estim wallinga_lipsitch weighted_var wt_loglik "
  },
  {
    "id": 759,
    "package_name": "lightsout",
    "title": "Implementation of the 'Lights Out' Puzzle Game",
    "description": "Lights Out is a puzzle game consisting of a grid of lights\nthat are either on or off. Pressing any light will toggle it\nand its adjacent lights. The goal of the game is to switch all\nthe lights off. This package provides an interface to play the\ngame on different board sizes, both through the command line or\nwith a visual application. Puzzles can also be solved using the\nautomatic solver included. View a demo online at\n<https://daattali.com/shiny/lightsout/>.",
    "version": "0.3.2",
    "maintainer": "Dean Attali <daattali@gmail.com>",
    "author": "Dean Attali [aut, cre] (ORCID: <https://orcid.org/0000-0002-5645-3493>)",
    "url": "https://github.com/daattali/lightsout,\nhttps://daattali.com/shiny/lightsout/",
    "bug_reports": "https://github.com/daattali/lightsout/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "board_classic"
      ],
      [
        "board_entries"
      ],
      [
        "board_entries<-"
      ],
      [
        "board_size"
      ],
      [
        "board_toggle_matrix"
      ],
      [
        "empty_board"
      ],
      [
        "is_solvable"
      ],
      [
        "is_solved"
      ],
      [
        "launch"
      ],
      [
        "new_board"
      ],
      [
        "play"
      ],
      [
        "random_board"
      ],
      [
        "solve_board"
      ]
    ],
    "topics": [],
    "score": 5.7226,
    "stars": 44,
    "primary_category": "visualization",
    "source_universe": "daattali",
    "search_text": "lightsout Implementation of the 'Lights Out' Puzzle Game Lights Out is a puzzle game consisting of a grid of lights\nthat are either on or off. Pressing any light will toggle it\nand its adjacent lights. The goal of the game is to switch all\nthe lights off. This package provides an interface to play the\ngame on different board sizes, both through the command line or\nwith a visual application. Puzzles can also be solved using the\nautomatic solver included. View a demo online at\n<https://daattali.com/shiny/lightsout/>. %>% board_classic board_entries board_entries<- board_size board_toggle_matrix empty_board is_solvable is_solved launch new_board play random_board solve_board "
  },
  {
    "id": 394,
    "package_name": "concstats",
    "title": "Market Structure, Concentration and Inequality Measures",
    "description": "Based on individual market shares of all participants in a\nmarket or space, the package offers a set of different\nstructural and concentration measures frequently - and not so\nfrequently - used in research and in practice. Measures can be\ncalculated in groups or individually. The calculated measure or\nthe resulting vector in table format should help practitioners\nmake more informed decisions. Methods used in this package are\nfrom: 1.  Chang, E. J., Guerra, S. M., de Souza Penaloza, R. A.\n& Tabak, B. M. (2005) \"Banking concentration: the Brazilian\ncase\". 2.  Cobham, A. and A. Summer (2013). \"Is It All About\nthe Tails? The Palma Measure of Income Inequality\". 3.  Garcia\nAlba Idunate, P. (1994). \"Un Indice de dominancia para el\nanalisis de la estructura de los mercados\". 4.  Ginevicius, R.\nand S. Cirba (2009). \"Additive measurement of market\nconcentration\" <doi:10.3846/1611-1699.2009.10.191-198>. 5.\nHerfindahl, O. C. (1950), \"Concentration in the steel industry\"\n(PhD thesis). 6.  Hirschmann, A. O. (1945), \"National power and\nstructure of foreign trade\". 7.  Melnik, A., O. Shy, and R.\nStenbacka (2008), \"Assessing market dominance\"\n<doi:10.1016/j.jebo.2008.03.010>. 8.  Palma, J. G. (2006).\n\"Globalizing Inequality: 'Centrifugal' and 'Centripetal' Forces\nat Work\". 9.  Shannon, C. E. (1948). \"A Mathematical Theory of\nCommunication\". 10.  Simpson, E. H. (1949). \"Measurement of\nDiversity\" <doi:10.1038/163688a0>.",
    "version": "0.2.1",
    "maintainer": "Andreas Schneider <schneiderconsultingpy@gmail.com>",
    "author": "Andreas Schneider [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-5630-1097>),\nSebastian Wojcik [rev],\nChristopher T. Kenny [rev]",
    "url": "https://github.com/ropensci/concstats/,\nhttps://docs.ropensci.org/concstats/ (website)",
    "bug_reports": "https://github.com/ropensci/concstats/issues/",
    "repository": "",
    "exports": [
      [
        "concstats_all_comp"
      ],
      [
        "concstats_all_inequ"
      ],
      [
        "concstats_all_mstruct"
      ],
      [
        "concstats_comp"
      ],
      [
        "concstats_concstats"
      ],
      [
        "concstats_dom"
      ],
      [
        "concstats_entropy"
      ],
      [
        "concstats_firm"
      ],
      [
        "concstats_gini"
      ],
      [
        "concstats_grs"
      ],
      [
        "concstats_hhi"
      ],
      [
        "concstats_hhi_d"
      ],
      [
        "concstats_hhi_min"
      ],
      [
        "concstats_inequ"
      ],
      [
        "concstats_mstruct"
      ],
      [
        "concstats_nrs_eq"
      ],
      [
        "concstats_palma"
      ],
      [
        "concstats_shares"
      ],
      [
        "concstats_simpson"
      ],
      [
        "concstats_sten"
      ],
      [
        "concstats_top"
      ],
      [
        "concstats_top_df"
      ],
      [
        "concstats_top3"
      ],
      [
        "concstats_top3_df"
      ],
      [
        "concstats_top5"
      ],
      [
        "concstats_top5_df"
      ]
    ],
    "topics": [
      [
        "business-analytics"
      ],
      [
        "competition"
      ],
      [
        "concentration"
      ],
      [
        "diversity"
      ],
      [
        "inequality"
      ],
      [
        "package-development"
      ]
    ],
    "score": 5.4683,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "concstats Market Structure, Concentration and Inequality Measures Based on individual market shares of all participants in a\nmarket or space, the package offers a set of different\nstructural and concentration measures frequently - and not so\nfrequently - used in research and in practice. Measures can be\ncalculated in groups or individually. The calculated measure or\nthe resulting vector in table format should help practitioners\nmake more informed decisions. Methods used in this package are\nfrom: 1.  Chang, E. J., Guerra, S. M., de Souza Penaloza, R. A.\n& Tabak, B. M. (2005) \"Banking concentration: the Brazilian\ncase\". 2.  Cobham, A. and A. Summer (2013). \"Is It All About\nthe Tails? The Palma Measure of Income Inequality\". 3.  Garcia\nAlba Idunate, P. (1994). \"Un Indice de dominancia para el\nanalisis de la estructura de los mercados\". 4.  Ginevicius, R.\nand S. Cirba (2009). \"Additive measurement of market\nconcentration\" <doi:10.3846/1611-1699.2009.10.191-198>. 5.\nHerfindahl, O. C. (1950), \"Concentration in the steel industry\"\n(PhD thesis). 6.  Hirschmann, A. O. (1945), \"National power and\nstructure of foreign trade\". 7.  Melnik, A., O. Shy, and R.\nStenbacka (2008), \"Assessing market dominance\"\n<doi:10.1016/j.jebo.2008.03.010>. 8.  Palma, J. G. (2006).\n\"Globalizing Inequality: 'Centrifugal' and 'Centripetal' Forces\nat Work\". 9.  Shannon, C. E. (1948). \"A Mathematical Theory of\nCommunication\". 10.  Simpson, E. H. (1949). \"Measurement of\nDiversity\" <doi:10.1038/163688a0>. concstats_all_comp concstats_all_inequ concstats_all_mstruct concstats_comp concstats_concstats concstats_dom concstats_entropy concstats_firm concstats_gini concstats_grs concstats_hhi concstats_hhi_d concstats_hhi_min concstats_inequ concstats_mstruct concstats_nrs_eq concstats_palma concstats_shares concstats_simpson concstats_sten concstats_top concstats_top_df concstats_top3 concstats_top3_df concstats_top5 concstats_top5_df business-analytics competition concentration diversity inequality package-development"
  },
  {
    "id": 66,
    "package_name": "GNE",
    "title": "Computation of Generalized Nash Equilibria",
    "description": "Compute standard and generalized Nash Equilibria of\nnon-cooperative games. Optimization methods available are\nnonsmooth reformulation, fixed-point formulation, minimization\nproblem and constrained-equation reformulation. See e.g. Kanzow\nand Facchinei (2010), <doi:10.1007/s10479-009-0653-x>.",
    "version": "0.99-7",
    "maintainer": "Christophe Dutang <dutangc@gmail.com>",
    "author": "Christophe Dutang [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6732-1501>)",
    "url": "https://r-forge.r-project.org/projects/optimizer/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "bench.GNE.ceq"
      ],
      [
        "bench.GNE.fpeq"
      ],
      [
        "bench.GNE.minpb"
      ],
      [
        "bench.GNE.nseq"
      ],
      [
        "compl.par"
      ],
      [
        "decrstep"
      ],
      [
        "decrstep10"
      ],
      [
        "decrstep20"
      ],
      [
        "decrstep5"
      ],
      [
        "fpNIR"
      ],
      [
        "fpVIR"
      ],
      [
        "funCER"
      ],
      [
        "funSSR"
      ],
      [
        "gapNIR"
      ],
      [
        "gapVIR"
      ],
      [
        "GNE"
      ],
      [
        "GNE.ceq"
      ],
      [
        "GNE.fpeq"
      ],
      [
        "GNE.minpb"
      ],
      [
        "GNE.nseq"
      ],
      [
        "gradpotential.ce"
      ],
      [
        "gradpsi.ce"
      ],
      [
        "gradxgapNIR"
      ],
      [
        "gradxgapVIR"
      ],
      [
        "gradygapNIR"
      ],
      [
        "gradygapVIR"
      ],
      [
        "GrAphiFB"
      ],
      [
        "GrAphiKK"
      ],
      [
        "GrAphiLT"
      ],
      [
        "GrAphiMan"
      ],
      [
        "GrAphiMin"
      ],
      [
        "GrAphipFB"
      ],
      [
        "GrBphiFB"
      ],
      [
        "GrBphiKK"
      ],
      [
        "GrBphiLT"
      ],
      [
        "GrBphiMan"
      ],
      [
        "GrBphiMin"
      ],
      [
        "GrBphipFB"
      ],
      [
        "jacCER"
      ],
      [
        "jacSSR"
      ],
      [
        "phiFB"
      ],
      [
        "phiKK"
      ],
      [
        "phiLT"
      ],
      [
        "phiMan"
      ],
      [
        "phiMin"
      ],
      [
        "phipFB"
      ],
      [
        "potential.ce"
      ],
      [
        "projector"
      ],
      [
        "psi.ce"
      ],
      [
        "purestep"
      ],
      [
        "rejection"
      ]
    ],
    "topics": [],
    "score": 5.0492,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "GNE Computation of Generalized Nash Equilibria Compute standard and generalized Nash Equilibria of\nnon-cooperative games. Optimization methods available are\nnonsmooth reformulation, fixed-point formulation, minimization\nproblem and constrained-equation reformulation. See e.g. Kanzow\nand Facchinei (2010), <doi:10.1007/s10479-009-0653-x>. bench.GNE.ceq bench.GNE.fpeq bench.GNE.minpb bench.GNE.nseq compl.par decrstep decrstep10 decrstep20 decrstep5 fpNIR fpVIR funCER funSSR gapNIR gapVIR GNE GNE.ceq GNE.fpeq GNE.minpb GNE.nseq gradpotential.ce gradpsi.ce gradxgapNIR gradxgapVIR gradygapNIR gradygapVIR GrAphiFB GrAphiKK GrAphiLT GrAphiMan GrAphiMin GrAphipFB GrBphiFB GrBphiKK GrBphiLT GrBphiMan GrBphiMin GrBphipFB jacCER jacSSR phiFB phiKK phiLT phiMan phiMin phipFB potential.ce projector psi.ce purestep rejection "
  },
  {
    "id": 1278,
    "package_name": "stops",
    "title": "Structure Optimized Proximity Scaling",
    "description": "Methods that use flexible variants of multidimensional\nscaling (MDS) which incorporate parametric nonlinear distance\ntransformations and trade-off the goodness-of-fit fit with\nstructure considerations to find optimal hyperparameters, also\nknown as structure optimized proximity scaling (STOPS) (Rusch,\nMair & Hornik, 2023,<doi:10.1007/s11222-022-10197-w>). The\npackage contains various functions, wrappers, methods and\nclasses for fitting, plotting and displaying different 1-way\nMDS models with ratio, interval, ordinal optimal scaling in a\nSTOPS framework. These cover essentially the functionality of\nthe package smacofx, including Torgerson (classical) scaling\nwith power transformations of dissimilarities, SMACOF MDS with\npowers of dissimilarities, Sammon mapping with powers of\ndissimilarities, elastic scaling with powers of\ndissimilarities, spherical SMACOF with powers of\ndissimilarities, (ALSCAL) s-stress MDS with powers of\ndissimilarities, r-stress MDS, MDS with powers of\ndissimilarities and configuration distances, elastic scaling\npowers of dissimilarities and configuration distances, Sammon\nmapping powers of dissimilarities and configuration distances,\npower stress MDS (POST-MDS), approximate power stress, Box-Cox\nMDS, local MDS, Isomap, curvilinear component analysis (CLCA),\ncurvilinear distance analysis (CLDA) and sparsified (power)\nmultidimensional scaling and (power) multidimensional distance\nanalysis (experimental models from smacofx influenced by CLCA).\nAll of these models can also be fit by optimizing over\nhyperparameters based on goodness-of-fit fit only (i.e., no\nstructure considerations). The package further contains\nfunctions for optimization, specifically the adaptive\nLuus-Jaakola algorithm and a wrapper for Bayesian optimization\nwith treed Gaussian process with jumps to linear models, and\nfunctions for various c-structuredness indices. Hyperparameter\noptimization can be done with a number of techniques but we\nrecommend either Bayesian optimization or particle swarm. For\nusing \"Kriging\", users need to install a version of the\narchived 'DiceOptim' R package.",
    "version": "1.10-1",
    "maintainer": "Thomas Rusch <thomas.rusch@wu.ac.at>",
    "author": "Thomas Rusch [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7773-2096>),\nPatrick Mair [aut] (ORCID: <https://orcid.org/0000-0003-0100-6511>),\nKurt Hornik [ctb] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "https://r-forge.r-project.org/projects/stops/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "c_association"
      ],
      [
        "c_clumpiness"
      ],
      [
        "c_clusteredness"
      ],
      [
        "c_complexity"
      ],
      [
        "c_convexity"
      ],
      [
        "c_dependence"
      ],
      [
        "c_faithfulness"
      ],
      [
        "c_functionality"
      ],
      [
        "c_hierarchy"
      ],
      [
        "c_inequality"
      ],
      [
        "c_linearity"
      ],
      [
        "c_manifoldness"
      ],
      [
        "c_nonmonotonicity"
      ],
      [
        "c_outlying"
      ],
      [
        "c_regularity"
      ],
      [
        "c_shepardness"
      ],
      [
        "c_skinniness"
      ],
      [
        "c_sparsity"
      ],
      [
        "c_striatedness"
      ],
      [
        "c_stringiness"
      ],
      [
        "ljoptim"
      ],
      [
        "stop_bcmds"
      ],
      [
        "stop_clca"
      ],
      [
        "stop_cldae"
      ],
      [
        "stop_cldak"
      ],
      [
        "stop_elastic"
      ],
      [
        "stop_isomap1"
      ],
      [
        "stop_isomap2"
      ],
      [
        "stop_lmds"
      ],
      [
        "stop_powerelastic"
      ],
      [
        "stop_powermds"
      ],
      [
        "stop_powersammon"
      ],
      [
        "stop_powerstress"
      ],
      [
        "stop_rstress"
      ],
      [
        "stop_sammon"
      ],
      [
        "stop_sammon2"
      ],
      [
        "stop_smacofSphere"
      ],
      [
        "stop_smacofSym"
      ],
      [
        "stop_smddae"
      ],
      [
        "stop_smddak"
      ],
      [
        "stop_smds"
      ],
      [
        "stop_spmddae"
      ],
      [
        "stop_spmddak"
      ],
      [
        "stop_spmds"
      ],
      [
        "stop_sstress"
      ],
      [
        "stoploss"
      ],
      [
        "stops"
      ],
      [
        "tgpoptim"
      ]
    ],
    "topics": [
      [
        "openjdk"
      ]
    ],
    "score": 4.4886,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "stops Structure Optimized Proximity Scaling Methods that use flexible variants of multidimensional\nscaling (MDS) which incorporate parametric nonlinear distance\ntransformations and trade-off the goodness-of-fit fit with\nstructure considerations to find optimal hyperparameters, also\nknown as structure optimized proximity scaling (STOPS) (Rusch,\nMair & Hornik, 2023,<doi:10.1007/s11222-022-10197-w>). The\npackage contains various functions, wrappers, methods and\nclasses for fitting, plotting and displaying different 1-way\nMDS models with ratio, interval, ordinal optimal scaling in a\nSTOPS framework. These cover essentially the functionality of\nthe package smacofx, including Torgerson (classical) scaling\nwith power transformations of dissimilarities, SMACOF MDS with\npowers of dissimilarities, Sammon mapping with powers of\ndissimilarities, elastic scaling with powers of\ndissimilarities, spherical SMACOF with powers of\ndissimilarities, (ALSCAL) s-stress MDS with powers of\ndissimilarities, r-stress MDS, MDS with powers of\ndissimilarities and configuration distances, elastic scaling\npowers of dissimilarities and configuration distances, Sammon\nmapping powers of dissimilarities and configuration distances,\npower stress MDS (POST-MDS), approximate power stress, Box-Cox\nMDS, local MDS, Isomap, curvilinear component analysis (CLCA),\ncurvilinear distance analysis (CLDA) and sparsified (power)\nmultidimensional scaling and (power) multidimensional distance\nanalysis (experimental models from smacofx influenced by CLCA).\nAll of these models can also be fit by optimizing over\nhyperparameters based on goodness-of-fit fit only (i.e., no\nstructure considerations). The package further contains\nfunctions for optimization, specifically the adaptive\nLuus-Jaakola algorithm and a wrapper for Bayesian optimization\nwith treed Gaussian process with jumps to linear models, and\nfunctions for various c-structuredness indices. Hyperparameter\noptimization can be done with a number of techniques but we\nrecommend either Bayesian optimization or particle swarm. For\nusing \"Kriging\", users need to install a version of the\narchived 'DiceOptim' R package. c_association c_clumpiness c_clusteredness c_complexity c_convexity c_dependence c_faithfulness c_functionality c_hierarchy c_inequality c_linearity c_manifoldness c_nonmonotonicity c_outlying c_regularity c_shepardness c_skinniness c_sparsity c_striatedness c_stringiness ljoptim stop_bcmds stop_clca stop_cldae stop_cldak stop_elastic stop_isomap1 stop_isomap2 stop_lmds stop_powerelastic stop_powermds stop_powersammon stop_powerstress stop_rstress stop_sammon stop_sammon2 stop_smacofSphere stop_smacofSym stop_smddae stop_smddak stop_smds stop_spmddae stop_spmddak stop_spmds stop_sstress stoploss stops tgpoptim openjdk"
  },
  {
    "id": 172,
    "package_name": "RobLox",
    "title": "Optimally Robust Influence Curves and Estimators for Location\nand Scale",
    "description": "Functions for the determination of optimally robust\ninfluence curves and estimators in case of normal location\nand/or scale (see Chapter 8 in Kohl (2005)\n<https://epub.uni-bayreuth.de/839/2/DissMKohl.pdf>).",
    "version": "1.2.3",
    "maintainer": "Matthias Kohl <Matthias.Kohl@stamats.de>",
    "author": "Matthias Kohl [cre, cph],\nPeter Ruckdeschel [aut, cph]",
    "url": "http://robast.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "colRoblox"
      ],
      [
        "finiteSampleCorrection"
      ],
      [
        "rlOptIC"
      ],
      [
        "rlsOptIC.AL"
      ],
      [
        "rlsOptIC.An1"
      ],
      [
        "rlsOptIC.An2"
      ],
      [
        "rlsOptIC.AnMad"
      ],
      [
        "rlsOptIC.BM"
      ],
      [
        "rlsOptIC.Ha3"
      ],
      [
        "rlsOptIC.Ha4"
      ],
      [
        "rlsOptIC.HaMad"
      ],
      [
        "rlsOptIC.Hu1"
      ],
      [
        "rlsOptIC.Hu2"
      ],
      [
        "rlsOptIC.Hu2a"
      ],
      [
        "rlsOptIC.Hu3"
      ],
      [
        "rlsOptIC.HuMad"
      ],
      [
        "rlsOptIC.M"
      ],
      [
        "rlsOptIC.MM2"
      ],
      [
        "rlsOptIC.Tu1"
      ],
      [
        "rlsOptIC.Tu2"
      ],
      [
        "rlsOptIC.TuMad"
      ],
      [
        "roblox"
      ],
      [
        "rowRoblox"
      ],
      [
        "rsOptIC"
      ],
      [
        "showdown"
      ]
    ],
    "topics": [],
    "score": 4.1004,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "RobLox Optimally Robust Influence Curves and Estimators for Location\nand Scale Functions for the determination of optimally robust\ninfluence curves and estimators in case of normal location\nand/or scale (see Chapter 8 in Kohl (2005)\n<https://epub.uni-bayreuth.de/839/2/DissMKohl.pdf>). colRoblox finiteSampleCorrection rlOptIC rlsOptIC.AL rlsOptIC.An1 rlsOptIC.An2 rlsOptIC.AnMad rlsOptIC.BM rlsOptIC.Ha3 rlsOptIC.Ha4 rlsOptIC.HaMad rlsOptIC.Hu1 rlsOptIC.Hu2 rlsOptIC.Hu2a rlsOptIC.Hu3 rlsOptIC.HuMad rlsOptIC.M rlsOptIC.MM2 rlsOptIC.Tu1 rlsOptIC.Tu2 rlsOptIC.TuMad roblox rowRoblox rsOptIC showdown "
  },
  {
    "id": 592,
    "package_name": "frontier",
    "title": "Stochastic Frontier Analysis",
    "description": "Maximum Likelihood Estimation of Stochastic Frontier\nProduction and Cost Functions. Two specifications are\navailable: the error components specification with time-varying\nefficiencies (Battese and Coelli, 1992,\n<doi:10.1007/BF00158774>) and a model specification in which\nthe firm effects are directly influenced by a number of\nvariables (Battese and Coelli, 1995, <doi:10.1007/BF01205442>).",
    "version": "1.1-9",
    "maintainer": "Arne Henningsen <arne.henningsen@gmail.com>",
    "author": "Tim Coelli, Arne Henningsen",
    "url": "http://frontier.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "efficiencies"
      ],
      [
        "front41Est"
      ],
      [
        "front41ReadOutput"
      ],
      [
        "front41WriteInput"
      ],
      [
        "frontier"
      ],
      [
        "frontierQuad"
      ],
      [
        "frontierTranslogRay"
      ],
      [
        "resettestFrontier"
      ],
      [
        "sfa"
      ]
    ],
    "topics": [
      [
        "fortran"
      ]
    ],
    "score": 3.9669,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "frontier Stochastic Frontier Analysis Maximum Likelihood Estimation of Stochastic Frontier\nProduction and Cost Functions. Two specifications are\navailable: the error components specification with time-varying\nefficiencies (Battese and Coelli, 1992,\n<doi:10.1007/BF00158774>) and a model specification in which\nthe firm effects are directly influenced by a number of\nvariables (Battese and Coelli, 1995, <doi:10.1007/BF01205442>). efficiencies front41Est front41ReadOutput front41WriteInput frontier frontierQuad frontierTranslogRay resettestFrontier sfa fortran"
  },
  {
    "id": 173,
    "package_name": "RobLoxBioC",
    "title": "Infinitesimally Robust Estimators for Preprocessing -Omics Data",
    "description": "Functions for the determination of optimally robust\ninfluence curves and estimators for preprocessing omics data,\nin particular gene expression data (Kohl and Deigner (2010),\n<doi:10.1186/1471-2105-11-583>).",
    "version": "1.2.3",
    "maintainer": "Matthias Kohl <Matthias.Kohl@stamats.de>",
    "author": "Matthias Kohl [aut, cre, cph]",
    "url": "https://r-forge.r-project.org/projects/robast/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "AffySimStudy"
      ],
      [
        "IlluminaSimStudy"
      ],
      [
        "KolmogorovMinDist"
      ],
      [
        "robloxbioc"
      ]
    ],
    "topics": [],
    "score": 3.7324,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "RobLoxBioC Infinitesimally Robust Estimators for Preprocessing -Omics Data Functions for the determination of optimally robust\ninfluence curves and estimators for preprocessing omics data,\nin particular gene expression data (Kohl and Deigner (2010),\n<doi:10.1186/1471-2105-11-583>). AffySimStudy IlluminaSimStudy KolmogorovMinDist robloxbioc "
  },
  {
    "id": 1,
    "package_name": "ALERT",
    "title": "The Above Local Elevated Respiratory illness Threshold (ALERT) Algorithm",
    "description": "A program that helps hospitals prospectively determine the start and end to a period of elevated influenza incidence in a community.",
    "version": "0.1",
    "maintainer": "Nicholas G Reich <nick@umass.edu>",
    "author": "Nicholas G Reich, Stephen A Lauer",
    "url": "https://github.com/reichlab/ALERT",
    "bug_reports": "https://github.com/reichlab/ALERT/issues",
    "repository": "https://github.com/reichlab/ALERT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 9,
    "primary_category": "epidemiology",
    "source_universe": "github:reichlab",
    "search_text": "ALERT The Above Local Elevated Respiratory illness Threshold (ALERT) Algorithm A program that helps hospitals prospectively determine the start and end to a period of elevated influenza incidence in a community.  "
  },
  {
    "id": 52,
    "package_name": "FAfA",
    "title": "Factor Analysis for All",
    "description": "Provides a comprehensive Shiny-based graphical user interface\n    for conducting a wide range of factor analysis procedures. 'FAfA'\n    (Factor Analysis for All) guides users through data uploading,\n    assumption checking (descriptives, collinearity, multivariate\n    normality, outliers), data wrangling (variable exclusion, data\n    splitting), factor retention analysis (e.g., Parallel Analysis, Hull\n    method, EGA), Exploratory Factor Analysis (EFA) with various rotation\n    and extraction methods, Confirmatory Factor Analysis (CFA) for model\n    testing, Reliability Analysis (e.g., Cronbach's Alpha, McDonald's\n    Omega), Measurement Invariance testing across groups, and item\n    weighting techniques. The application leverages established R packages\n    such as 'lavaan' and 'psych' to perform these analyses, offering an\n    accessible platform for researchers and students. Results are\n    presented in user-friendly tables and plots, with options for\n    downloading outputs.",
    "version": "0.5",
    "maintainer": "Abdullah Faruk KILIC <afarukkilic@trakya.edu.tr>",
    "author": "Abdullah Faruk KILIC [aut, cre],\n  Ahmet Caliskan [aut]",
    "url": "https://github.com/AFarukKILIC/FAfA",
    "bug_reports": "https://github.com/AFarukKILIC/FAfA/issues",
    "repository": "https://cran.r-project.org/package=FAfA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAfA Factor Analysis for All Provides a comprehensive Shiny-based graphical user interface\n    for conducting a wide range of factor analysis procedures. 'FAfA'\n    (Factor Analysis for All) guides users through data uploading,\n    assumption checking (descriptives, collinearity, multivariate\n    normality, outliers), data wrangling (variable exclusion, data\n    splitting), factor retention analysis (e.g., Parallel Analysis, Hull\n    method, EGA), Exploratory Factor Analysis (EFA) with various rotation\n    and extraction methods, Confirmatory Factor Analysis (CFA) for model\n    testing, Reliability Analysis (e.g., Cronbach's Alpha, McDonald's\n    Omega), Measurement Invariance testing across groups, and item\n    weighting techniques. The application leverages established R packages\n    such as 'lavaan' and 'psych' to perform these analyses, offering an\n    accessible platform for researchers and students. Results are\n    presented in user-friendly tables and plots, with options for\n    downloading outputs.  "
  },
  {
    "id": 77,
    "package_name": "HOIFCar",
    "title": "Covariate Adjustment in RCT by Higher-Order Influence Functions",
    "description": "Estimates treatment effects  using covariate adjustment methods in Randomized Clinical Trials (RCT) motivated by higher-order influence functions (HOIF). Provides point estimates, oracle bias, variance, and approximate variance for HOIF-adjusted estimators. For methodology details, see Zhao et al. (2024) <doi:10.48550/arXiv.2411.08491>.",
    "version": "1.1.0",
    "maintainer": "Xinbo Wang <cinbo_w@sjtu.edu.cn>",
    "author": "Sihui Zhao [aut],\n  Xinbo Wang [cre, aut],\n  Liu Liu [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HOIFCar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HOIFCar Covariate Adjustment in RCT by Higher-Order Influence Functions Estimates treatment effects  using covariate adjustment methods in Randomized Clinical Trials (RCT) motivated by higher-order influence functions (HOIF). Provides point estimates, oracle bias, variance, and approximate variance for HOIF-adjusted estimators. For methodology details, see Zhao et al. (2024) <doi:10.48550/arXiv.2411.08491>.  "
  },
  {
    "id": 151,
    "package_name": "RPEIF",
    "title": "Computation and Plots of Influence Functions for Risk and\nPerformance Measures",
    "description": "Computes the influence functions time series of the returns for the risk and \n             performance measures as mentioned in Chen and Martin (2018) \n             <https://www.ssrn.com/abstract=3085672>, as well as in Zhang et al. (2019)\n             <https://www.ssrn.com/abstract=3415903>. Also evaluates estimators influence\n             functions at a set of parameter values and plots them to display the shapes of \n             the influence functions.",
    "version": "1.2.5",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "author": "Anthony Christidis [aut, cre],\n  Shengyu Zhang [aut],\n  Douglas Martin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RPEIF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPEIF Computation and Plots of Influence Functions for Risk and\nPerformance Measures Computes the influence functions time series of the returns for the risk and \n             performance measures as mentioned in Chen and Martin (2018) \n             <https://www.ssrn.com/abstract=3085672>, as well as in Zhang et al. (2019)\n             <https://www.ssrn.com/abstract=3415903>. Also evaluates estimators influence\n             functions at a set of parameter values and plots them to display the shapes of \n             the influence functions.  "
  },
  {
    "id": 206,
    "package_name": "TUvalues",
    "title": "Tools for Calculating Allocations in Game Theory using Exact and\nApproximated Methods",
    "description": "The main objective of cooperative Transferable-Utility games (TU-games) \n  is to allocate a good among the agents involved. The package implements major \n  solution concepts including the Shapley value, Banzhaf value, and egalitarian\n  rules, alongside their extensions for structured games: \n  the Owen value and Banzhaf-Owen value for games with a priori unions, and the\n  Myerson value for communication games on networks. To address the inherent exponential \n  computational complexity of exact evaluation, the package offers both exact \n  algorithms and linear approximation methods based on sampling, enabling the \n  analysis of large-scale games. Additionally, it supports core set-based \n  solutions, allowing computation of the vertices and the centroid of the core.",
    "version": "1.1.0",
    "maintainer": "Maria D. Guillen <maria.guilleng@umh.es>",
    "author": "Maria D. Guillen [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-2445-5654>),\n  Juan Carlos Gon\u00e7alves [aut] (ORCID:\n    <https://orcid.org/0000-0002-0867-0004>)",
    "url": "https://github.com/mariaguilleng/TUvalues",
    "bug_reports": "https://github.com/mariaguilleng/TUvalues/issues",
    "repository": "https://cran.r-project.org/package=TUvalues",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TUvalues Tools for Calculating Allocations in Game Theory using Exact and\nApproximated Methods The main objective of cooperative Transferable-Utility games (TU-games) \n  is to allocate a good among the agents involved. The package implements major \n  solution concepts including the Shapley value, Banzhaf value, and egalitarian\n  rules, alongside their extensions for structured games: \n  the Owen value and Banzhaf-Owen value for games with a priori unions, and the\n  Myerson value for communication games on networks. To address the inherent exponential \n  computational complexity of exact evaluation, the package offers both exact \n  algorithms and linear approximation methods based on sampling, enabling the \n  analysis of large-scale games. Additionally, it supports core set-based \n  solutions, allowing computation of the vertices and the centroid of the core.  "
  },
  {
    "id": 550,
    "package_name": "extraSuperpower",
    "title": "Power Calculation for Two-Way Factorial Designs",
    "description": "The basic use of this package is with 3 sequential functions. First to generate a cell mean matrix. In case of a repeated measurements design also generate correlation and covariance matrices. This is followed by iterative experiment simulation. Finally, power is calculated from the simulated data. Features that may be considered in the model are interaction, measure correlation, non-normal and unbalanced designs distributions.",
    "version": "1.6.2",
    "maintainer": "Louis Macias <luisrmacias@gmail.com>",
    "author": "Louis Macias [aut, cre, cph] (ORCID =\n    <https://orcid.org/0000-0002-3080-2835>),\n  Silke Szymczak [aut] (ORCID = <https://orcid.org/0000-0002-8897-9035>)",
    "url": "https://github.com/luisrmacias/extraSuperpower",
    "bug_reports": "https://github.com/luisrmacias/extraSuperpower/issues",
    "repository": "https://cran.r-project.org/package=extraSuperpower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extraSuperpower Power Calculation for Two-Way Factorial Designs The basic use of this package is with 3 sequential functions. First to generate a cell mean matrix. In case of a repeated measurements design also generate correlation and covariance matrices. This is followed by iterative experiment simulation. Finally, power is calculated from the simulated data. Features that may be considered in the model are interaction, measure correlation, non-normal and unbalanced designs distributions.  "
  },
  {
    "id": 754,
    "package_name": "leidenbase",
    "title": "R and C/C++ Wrappers to Run the Leiden find_partition() Function",
    "description": "An R to C/C++ interface that runs the Leiden community\n    detection algorithm to find a basic partition (). It runs the\n    equivalent of the 'leidenalg' find_partition() function, which is\n    given in the 'leidenalg' distribution file\n    'leiden/src/functions.py'. This package includes the\n    required source code files from the official 'leidenalg'\n    distribution and functions from the R 'igraph'\n    package.  The 'leidenalg' distribution is available from\n    <https://github.com/vtraag/leidenalg/>\n    and the R 'igraph' package is available from\n    <https://igraph.org/r/>.\n    The Leiden algorithm is described in the article by\n    Traag et al. (2019) <doi:10.1038/s41598-019-41695-z>.\n    Leidenbase includes code from the packages:\n       igraph version 0.9.8 with license GPL (>= 2),\n       leidenalg version 0.8.10 with license GPL 3.",
    "version": "0.1.36",
    "maintainer": "Brent Ewing <bge@uw.edu>",
    "author": "Brent Ewing [aut, cre],\n  Vincent Traag [ctb],\n  G\u00e1bor Cs\u00e1rdi [ctb],\n  Tam\u00e1s Nepusz [ctb],\n  Szabolcs Horvat [ctb],\n  Fabio Zanini [ctb]",
    "url": "https://github.com/cole-trapnell-lab/leidenbase",
    "bug_reports": "https://github.com/cole-trapnell-lab/leidenbase/issues",
    "repository": "https://cran.r-project.org/package=leidenbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "leidenbase R and C/C++ Wrappers to Run the Leiden find_partition() Function An R to C/C++ interface that runs the Leiden community\n    detection algorithm to find a basic partition (). It runs the\n    equivalent of the 'leidenalg' find_partition() function, which is\n    given in the 'leidenalg' distribution file\n    'leiden/src/functions.py'. This package includes the\n    required source code files from the official 'leidenalg'\n    distribution and functions from the R 'igraph'\n    package.  The 'leidenalg' distribution is available from\n    <https://github.com/vtraag/leidenalg/>\n    and the R 'igraph' package is available from\n    <https://igraph.org/r/>.\n    The Leiden algorithm is described in the article by\n    Traag et al. (2019) <doi:10.1038/s41598-019-41695-z>.\n    Leidenbase includes code from the packages:\n       igraph version 0.9.8 with license GPL (>= 2),\n       leidenalg version 0.8.10 with license GPL 3.  "
  },
  {
    "id": 995,
    "package_name": "pmparser",
    "title": "Create and Maintain a Relational Database of Data from\nPubMed/MEDLINE",
    "description": "Provides a simple interface for extracting various elements from\n  the publicly available PubMed XML files, incorporating PubMed's regular\n  updates, and combining the data with the NIH Open Citation Collection. See\n  Schoenbachler and Hughey (2021) <doi:10.7717/peerj.11071>.",
    "version": "1.0.23",
    "maintainer": "Jake Hughey <jakejhughey@gmail.com>",
    "author": "Jake Hughey [aut, cre],\n  Josh Schoenbachler [aut],\n  Elliot Outland [aut]",
    "url": "https://pmparser.hugheylab.org,\nhttps://github.com/hugheylab/pmparser",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pmparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmparser Create and Maintain a Relational Database of Data from\nPubMed/MEDLINE Provides a simple interface for extracting various elements from\n  the publicly available PubMed XML files, incorporating PubMed's regular\n  updates, and combining the data with the NIH Open Citation Collection. See\n  Schoenbachler and Hughey (2021) <doi:10.7717/peerj.11071>.  "
  },
  {
    "id": 1372,
    "package_name": "traineR",
    "title": "Predictive (Classification and Regression) Models Homologator",
    "description": "Methods to unify the different ways of creating predictive models and their different predictive formats for classification and regression. It includes  methods such as K-Nearest Neighbors Schliep, K. P. (2004) <doi:10.5282/ubm/epub.1769>, Decision Trees Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (2017) <doi:10.1201/9781315139470>,  ADA Boosting Esteban Alfaro, Matias Gamez, Noelia Garc\u00eda (2013) <doi:10.18637/jss.v054.i02>, Extreme Gradient Boosting Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>,  Random Forest Breiman (2001) <doi:10.1023/A:1010933404324>, Neural Networks Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Support Vector Machines Bennett, K. P. & Campbell, C. (2000) <doi:10.1145/380995.380999>, Bayesian Methods Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995) <doi:10.1201/9780429258411>,  Linear Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Quadratic Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>,  Logistic Regression Dobson, A. J., & Barnett, A. G. (2018) <doi:10.1201/9781315182780> and Penalized Logistic Regression Friedman, J. H., Hastie, T., & Tibshirani, R. (2010) <doi:10.18637/jss.v033.i01>.",
    "version": "2.2.9",
    "maintainer": "Oldemar Rodriguez R. <oldemar.rodriguez@ucr.ac.cr>",
    "author": "Oldemar Rodriguez R. [aut, cre],\n  Andres Navarro D. [aut],\n  Ariel Arroyo S. [aut],\n  Diego Jimenez A. [aut],\n  Jennifer Lobo V. [aut]",
    "url": "https://promidat.website/",
    "bug_reports": "https://github.com/PROMiDAT/traineR/issues",
    "repository": "https://cran.r-project.org/package=traineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "traineR Predictive (Classification and Regression) Models Homologator Methods to unify the different ways of creating predictive models and their different predictive formats for classification and regression. It includes  methods such as K-Nearest Neighbors Schliep, K. P. (2004) <doi:10.5282/ubm/epub.1769>, Decision Trees Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (2017) <doi:10.1201/9781315139470>,  ADA Boosting Esteban Alfaro, Matias Gamez, Noelia Garc\u00eda (2013) <doi:10.18637/jss.v054.i02>, Extreme Gradient Boosting Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>,  Random Forest Breiman (2001) <doi:10.1023/A:1010933404324>, Neural Networks Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Support Vector Machines Bennett, K. P. & Campbell, C. (2000) <doi:10.1145/380995.380999>, Bayesian Methods Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995) <doi:10.1201/9780429258411>,  Linear Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Quadratic Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>,  Logistic Regression Dobson, A. J., & Barnett, A. G. (2018) <doi:10.1201/9781315182780> and Penalized Logistic Regression Friedman, J. H., Hastie, T., & Tibshirani, R. (2010) <doi:10.18637/jss.v033.i01>.  "
  },
  {
    "id": 1487,
    "package_name": "ABPS",
    "title": "The Abnormal Blood Profile Score to Detect Blood Doping",
    "description": "An implementation of the Abnormal Blood Profile Score (ABPS,\n    part of the Athlete Biological Passport program of the World Anti-Doping\n    Agency), which combines several blood parameters into a single score in\n    order to detect blood doping (Sottas et al. (2006)\n    <doi:10.2202/1557-4679.1011>). The package also contains functions to\n    calculate other scores used in anti-doping programs, such as the\n    OFF-score (Gore et al. (2003) <http://www.haematologica.org/content/88/3/333>),\n    as well as example data.",
    "version": "0.3",
    "maintainer": "Fr\u00e9d\u00e9ric Sch\u00fctz <schutz@mathgen.ch>",
    "author": "Fr\u00e9d\u00e9ric Sch\u00fctz [aut, cre],\n  Alix Zollinger [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ABPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ABPS The Abnormal Blood Profile Score to Detect Blood Doping An implementation of the Abnormal Blood Profile Score (ABPS,\n    part of the Athlete Biological Passport program of the World Anti-Doping\n    Agency), which combines several blood parameters into a single score in\n    order to detect blood doping (Sottas et al. (2006)\n    <doi:10.2202/1557-4679.1011>). The package also contains functions to\n    calculate other scores used in anti-doping programs, such as the\n    OFF-score (Gore et al. (2003) <http://www.haematologica.org/content/88/3/333>),\n    as well as example data.  "
  },
  {
    "id": 1493,
    "package_name": "ACEP",
    "title": "Analisis Computacional de Eventos de Protesta",
    "description": "La libreria 'ACEP' contiene funciones especificas para\n    desarrollar analisis computacional de eventos de protesta. Asimismo,\n    contiene base de datos con colecciones de notas sobre protestas y\n    diccionarios de palabras conflictivas. Coleccion de diccionarios que\n    reune diccionarios de diferentes origenes.  The 'ACEP' library\n    contains specific functions to perform computational analysis of\n    protest events. It also contains a database with collections of notes\n    on protests and dictionaries of conflicting words. Collection of\n    dictionaries that brings together dictionaries from different sources.",
    "version": "0.0.22",
    "maintainer": "Agustin Nieto <agustin.nieto77@gmail.com>",
    "author": "Agustin Nieto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4467-873X>)",
    "url": "https://github.com/agusnieto77/ACEP,\nhttps://agusnieto77.github.io/ACEP/",
    "bug_reports": "https://github.com/agusnieto77/ACEP/issues",
    "repository": "https://cran.r-project.org/package=ACEP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ACEP Analisis Computacional de Eventos de Protesta La libreria 'ACEP' contiene funciones especificas para\n    desarrollar analisis computacional de eventos de protesta. Asimismo,\n    contiene base de datos con colecciones de notas sobre protestas y\n    diccionarios de palabras conflictivas. Coleccion de diccionarios que\n    reune diccionarios de diferentes origenes.  The 'ACEP' library\n    contains specific functions to perform computational analysis of\n    protest events. It also contains a database with collections of notes\n    on protests and dictionaries of conflicting words. Collection of\n    dictionaries that brings together dictionaries from different sources.  "
  },
  {
    "id": 1502,
    "package_name": "ACWR",
    "title": "Acute Chronic Workload Ratio Calculation",
    "description": "Functions for calculating the acute chronic workload ratio using three \n            different methods: exponentially weighted moving average (EWMA), rolling \n            average coupled (RAC) and rolling averaged uncoupled (RAU). Examples of this \n            methods can be found in Williams et al. (2017) <doi:10.1136/bjsports-2016-096589>\n            for EWMA and Windt & Gabbet (2018) for RAC and RAU <doi: 10.1136/bjsports-2017-098925>.",
    "version": "0.1.0",
    "maintainer": "Jorge R Fernandez-Santos <jorgedelrosario.fernandez@uca.es>",
    "author": "Jorge R Fernandez-Santos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5047-2976>)",
    "url": "https://github.com/JorgeDelro/ACWR",
    "bug_reports": "https://github.com/JorgeDelro/ACWR/issues",
    "repository": "https://cran.r-project.org/package=ACWR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ACWR Acute Chronic Workload Ratio Calculation Functions for calculating the acute chronic workload ratio using three \n            different methods: exponentially weighted moving average (EWMA), rolling \n            average coupled (RAC) and rolling averaged uncoupled (RAU). Examples of this \n            methods can be found in Williams et al. (2017) <doi:10.1136/bjsports-2016-096589>\n            for EWMA and Windt & Gabbet (2018) for RAC and RAU <doi: 10.1136/bjsports-2017-098925>.  "
  },
  {
    "id": 1584,
    "package_name": "ARCensReg",
    "title": "Fitting Univariate Censored Linear Regression Model with\nAutoregressive Errors",
    "description": "It fits a univariate left, right, or interval censored linear regression model\n    with autoregressive errors, considering the normal or the Student-t distribution for the \n    innovations. It provides estimates and standard errors of the parameters, predicts \n    future observations, and supports missing values on the dependent variable.\n    References used for this package:\n    Schumacher, F. L., Lachos, V. H., & Dey, D. K. (2017). Censored regression models with \n    autoregressive errors: A likelihood-based perspective. Canadian Journal of Statistics,\n    45(4), 375-392 <doi:10.1002/cjs.11338>.\n    Schumacher, F. L., Lachos, V. H., Vilca-Labra, F. E., & Castro, L. M. (2018). Influence \n    diagnostics for censored regression models with autoregressive errors. Australian & New \n    Zealand Journal of Statistics, 60(2), 209-229 <doi:10.1111/anzs.12229>.\n    Valeriano, K. A., Schumacher, F. L., Galarza, C. E., & Matos, L. A. (2024). Censored \n    autoregressive regression models with Student\u2010t innovations. Canadian Journal of Statistics, \n    52(3), 804-828 <doi:10.1002/cjs.11804>.",
    "version": "3.0.2",
    "maintainer": "Fernanda L. Schumacher <fernandalschumacher@gmail.com>",
    "author": "Fernanda L. Schumacher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5724-8918>),\n  Katherine A. L. Valeriano [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6388-4753>),\n  Victor H. Lachos [ctb] (ORCID: <https://orcid.org/0000-0002-7239-2459>),\n  Christian G. Morales [ctb] (ORCID:\n    <https://orcid.org/0000-0002-4818-6006>),\n  Larissa A. Matos [ctb] (ORCID: <https://orcid.org/0000-0002-2635-0901>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ARCensReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ARCensReg Fitting Univariate Censored Linear Regression Model with\nAutoregressive Errors It fits a univariate left, right, or interval censored linear regression model\n    with autoregressive errors, considering the normal or the Student-t distribution for the \n    innovations. It provides estimates and standard errors of the parameters, predicts \n    future observations, and supports missing values on the dependent variable.\n    References used for this package:\n    Schumacher, F. L., Lachos, V. H., & Dey, D. K. (2017). Censored regression models with \n    autoregressive errors: A likelihood-based perspective. Canadian Journal of Statistics,\n    45(4), 375-392 <doi:10.1002/cjs.11338>.\n    Schumacher, F. L., Lachos, V. H., Vilca-Labra, F. E., & Castro, L. M. (2018). Influence \n    diagnostics for censored regression models with autoregressive errors. Australian & New \n    Zealand Journal of Statistics, 60(2), 209-229 <doi:10.1111/anzs.12229>.\n    Valeriano, K. A., Schumacher, F. L., Galarza, C. E., & Matos, L. A. (2024). Censored \n    autoregressive regression models with Student\u2010t innovations. Canadian Journal of Statistics, \n    52(3), 804-828 <doi:10.1002/cjs.11804>.  "
  },
  {
    "id": 1619,
    "package_name": "AZIAD",
    "title": "Analyzing Zero-Inflated and Zero-Altered Data",
    "description": "Description: Computes maximum likelihood estimates of general, zero-inflated, and zero-altered models for discrete and continuous distributions. It also performs Kolmogorov-Smirnov (KS) tests and likelihood ratio tests for general, zero-inflated, and zero-altered data. Additionally, it obtains the inverse of the Fisher information matrix and confidence intervals for the parameters of general, zero-inflated, and zero-altered models. The package simulates random deviates from zero-inflated or hurdle models to obtain maximum likelihood estimates. Based on the work of Aldirawi et al. (2022) <doi:10.1007/s42519-021-00230-y> and Dousti Mousavi et al. (2023) <doi:10.1080/00949655.2023.2207020>.",
    "version": "0.0.3",
    "maintainer": "Niloufar Dousti Mousavi <niloufar.dousti@gmail.com>",
    "author": "Niloufar Dousti Mousavi [aut, cre, cph],\n  Hani Aldirawi [aut, cph],\n  Jie Yang [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AZIAD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AZIAD Analyzing Zero-Inflated and Zero-Altered Data Description: Computes maximum likelihood estimates of general, zero-inflated, and zero-altered models for discrete and continuous distributions. It also performs Kolmogorov-Smirnov (KS) tests and likelihood ratio tests for general, zero-inflated, and zero-altered data. Additionally, it obtains the inverse of the Fisher information matrix and confidence intervals for the parameters of general, zero-inflated, and zero-altered models. The package simulates random deviates from zero-inflated or hurdle models to obtain maximum likelihood estimates. Based on the work of Aldirawi et al. (2022) <doi:10.1007/s42519-021-00230-y> and Dousti Mousavi et al. (2023) <doi:10.1080/00949655.2023.2207020>.  "
  },
  {
    "id": 1650,
    "package_name": "AdvancedBasketballStats",
    "title": "Advanced Basketball Statistics",
    "description": "Provides different functionalities and calculations used in the world of basketball to analyze the statistics of the players, the statistics of the teams, the statistics of the quintets and the statistics of the plays. For more details of the calculations included in the package can be found in the book Basketball on Paper written by Dean Oliver.",
    "version": "1.0.1",
    "maintainer": "Francisco Javier Cantero <fco.cantero@edu.uah.es>",
    "author": "Francisco Javier Cantero [aut, cre],\n  Juan Jose Cuadrado [aut],\n  Universidad de Alcala de Henares [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AdvancedBasketballStats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AdvancedBasketballStats Advanced Basketball Statistics Provides different functionalities and calculations used in the world of basketball to analyze the statistics of the players, the statistics of the teams, the statistics of the quintets and the statistics of the plays. For more details of the calculations included in the package can be found in the book Basketball on Paper written by Dean Oliver.  "
  },
  {
    "id": 1690,
    "package_name": "Analitica",
    "title": "Exploratory Data Analysis, Group Comparison Tools, and Other\nProcedures",
    "description": "Provides a comprehensive set of tools for descriptive statistics,\n    graphical data exploration, outlier detection, homoscedasticity testing, and\n    multiple comparison procedures. Includes manual implementations of Levene's test,\n    Bartlett's test, and the Fligner-Killeen test, as well as post hoc comparison\n    methods such as Tukey, Scheff\u00e9, Games-Howell, Brunner-Munzel, and others.\n    This version introduces two new procedures: the Jonckheere-Terpstra trend test\n    and the Jarque-Bera test with Glinskiy's (2024) correction. Designed for use in\n    teaching, applied statistical analysis, and reproducible research. \n    Additionally you can find a post hoc Test Planner, which helps you to make a \n    decision on which procedure is most suitable.",
    "version": "2.2.0",
    "maintainer": "Carlos Jim\u00e9nez-Gallardo <carlos.jimenez@ufrontera.cl>",
    "author": "Carlos Jim\u00e9nez-Gallardo [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Analitica",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Analitica Exploratory Data Analysis, Group Comparison Tools, and Other\nProcedures Provides a comprehensive set of tools for descriptive statistics,\n    graphical data exploration, outlier detection, homoscedasticity testing, and\n    multiple comparison procedures. Includes manual implementations of Levene's test,\n    Bartlett's test, and the Fligner-Killeen test, as well as post hoc comparison\n    methods such as Tukey, Scheff\u00e9, Games-Howell, Brunner-Munzel, and others.\n    This version introduces two new procedures: the Jonckheere-Terpstra trend test\n    and the Jarque-Bera test with Glinskiy's (2024) correction. Designed for use in\n    teaching, applied statistical analysis, and reproducible research. \n    Additionally you can find a post hoc Test Planner, which helps you to make a \n    decision on which procedure is most suitable.  "
  },
  {
    "id": 1700,
    "package_name": "AnnotationBustR",
    "title": "Extract Subsequences from GenBank Annotations",
    "description": "Extraction of subsequences into FASTA files from GenBank annotations where gene names may vary among accessions. Borstein & O'Meara (2018) <doi:10.7717/peerj.5179>.",
    "version": "1.3.0",
    "maintainer": "Samuel R. Borstein <sam@borstein.com>",
    "author": "Samuel R. Borstein <sam@borstein.com>, Brian O'Meara <bomeara@utk.edu>",
    "url": "https://github.com/sborstein/AnnotationBustR,\nhttps://www.ncbi.nlm.nih.gov/nuccore,\nhttps://peerj.com/articles/5179/",
    "bug_reports": "https://github.com/sborstein/AnnotationBustR/issues",
    "repository": "https://cran.r-project.org/package=AnnotationBustR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AnnotationBustR Extract Subsequences from GenBank Annotations Extraction of subsequences into FASTA files from GenBank annotations where gene names may vary among accessions. Borstein & O'Meara (2018) <doi:10.7717/peerj.5179>.  "
  },
  {
    "id": 1725,
    "package_name": "ArgentinAPI",
    "title": "Access Argentinian Data via APIs and Curated Datasets",
    "description": "Provides functions to access data from public RESTful APIs including the 'ArgentinaDatos API', \n    'REST Countries API', and 'World Bank API' related to Argentina's exchange rates, inflation, \n    political figures, holidays, economic indicators, and general country-level statistics. \n    Additionally, the package includes curated datasets related to Argentina, covering topics \n    such as economic indicators, biodiversity, agriculture, human rights, genetic data, and \n    consumer prices. The package supports research and analysis focused on Argentina by \n    integrating open APIs with high-quality datasets from various domains. \n    For more details on the APIs, see: \n        'ArgentinaDatos API' <https://argentinadatos.com/>, \n        'REST Countries API' <https://restcountries.com/>, \n        and 'World Bank API' <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>.",
    "version": "0.2.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/argentinapi,\nhttps://lightbluetitan.github.io/argentinapi/",
    "bug_reports": "https://github.com/lightbluetitan/argentinapi/issues",
    "repository": "https://cran.r-project.org/package=ArgentinAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ArgentinAPI Access Argentinian Data via APIs and Curated Datasets Provides functions to access data from public RESTful APIs including the 'ArgentinaDatos API', \n    'REST Countries API', and 'World Bank API' related to Argentina's exchange rates, inflation, \n    political figures, holidays, economic indicators, and general country-level statistics. \n    Additionally, the package includes curated datasets related to Argentina, covering topics \n    such as economic indicators, biodiversity, agriculture, human rights, genetic data, and \n    consumer prices. The package supports research and analysis focused on Argentina by \n    integrating open APIs with high-quality datasets from various domains. \n    For more details on the APIs, see: \n        'ArgentinaDatos API' <https://argentinadatos.com/>, \n        'REST Countries API' <https://restcountries.com/>, \n        and 'World Bank API' <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>.  "
  },
  {
    "id": 1738,
    "package_name": "Athlytics",
    "title": "Advanced Sports Performance Analysis for 'Strava' Data",
    "description": "Advanced sports performance analysis and modeling for activity data retrieved from 'Strava'. This package focuses on applying established sports science models and statistical methods to gain deeper insights into training load, performance prediction, recovery status, and identifying key performance factors, extending basic data analysis capabilities.",
    "version": "0.1.2",
    "maintainer": "Ang <ang@hezhiang.com>",
    "author": "Ang [aut, cre]",
    "url": "https://hezhiang.com/Athlytics/,\nhttps://github.com/HzaCode/Athlytics",
    "bug_reports": "https://github.com/HzaCode/Athlytics/issues",
    "repository": "https://cran.r-project.org/package=Athlytics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Athlytics Advanced Sports Performance Analysis for 'Strava' Data Advanced sports performance analysis and modeling for activity data retrieved from 'Strava'. This package focuses on applying established sports science models and statistical methods to gain deeper insights into training load, performance prediction, recovery status, and identifying key performance factors, extending basic data analysis capabilities.  "
  },
  {
    "id": 1780,
    "package_name": "BANAM",
    "title": "Bayesian Analysis of the Network Autocorrelation Model",
    "description": "The network autocorrelation model (NAM) can be used for studying the degree of social influence \n    regarding an outcome variable based on one or more known networks. \n    The degree of social influence is quantified via the network autocorrelation parameters. In case of a single\n    network, the Bayesian methods of Dittrich, Leenders, and Mulder\n    (2017) <DOI:10.1016/j.socnet.2016.09.002> and Dittrich, Leenders, and Mulder (2019)\n    <DOI:10.1177/0049124117729712> are implemented using a normal, flat, or independence  \n    Jeffreys prior for the network autocorrelation. In the case of multiple \n    networks, the Bayesian methods of Dittrich, Leenders, and Mulder (2020) \n    <DOI:10.1177/0081175020913899> are implemented using a multivariate normal prior for \n    the network autocorrelation parameters. Flat priors are implemented \n    for estimating the coefficients. For Bayesian testing of equality and order-constrained \n    hypotheses, the default Bayes factor of Gu, Mulder, and Hoijtink, (2018) \n    <DOI:10.1111/bmsp.12110> is used with the posterior mean and posterior covariance \n    matrix of the NAM parameters based on flat priors as input.",
    "version": "0.2.2",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "author": "Joris Mulder [aut, cre],\n  Dino Dittrich [aut, ctb],\n  Roger Leenders [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BANAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BANAM Bayesian Analysis of the Network Autocorrelation Model The network autocorrelation model (NAM) can be used for studying the degree of social influence \n    regarding an outcome variable based on one or more known networks. \n    The degree of social influence is quantified via the network autocorrelation parameters. In case of a single\n    network, the Bayesian methods of Dittrich, Leenders, and Mulder\n    (2017) <DOI:10.1016/j.socnet.2016.09.002> and Dittrich, Leenders, and Mulder (2019)\n    <DOI:10.1177/0049124117729712> are implemented using a normal, flat, or independence  \n    Jeffreys prior for the network autocorrelation. In the case of multiple \n    networks, the Bayesian methods of Dittrich, Leenders, and Mulder (2020) \n    <DOI:10.1177/0081175020913899> are implemented using a multivariate normal prior for \n    the network autocorrelation parameters. Flat priors are implemented \n    for estimating the coefficients. For Bayesian testing of equality and order-constrained \n    hypotheses, the default Bayes factor of Gu, Mulder, and Hoijtink, (2018) \n    <DOI:10.1111/bmsp.12110> is used with the posterior mean and posterior covariance \n    matrix of the NAM parameters based on flat priors as input.  "
  },
  {
    "id": 1783,
    "package_name": "BART",
    "title": "Bayesian Additive Regression Trees",
    "description": "Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes.  For more information see Sparapani, Spanbauer and McCulloch <doi:10.18637/jss.v097.i01>.",
    "version": "2.9.9",
    "maintainer": "Rodney Sparapani <rsparapa@mcw.edu>",
    "author": "Robert McCulloch [aut],\n  Rodney Sparapani [aut, cre],\n  Robert Gramacy [ctb],\n  Matthew Pratola [ctb],\n  Charles Spanbauer [ctb],\n  Martyn Plummer [ctb],\n  Nicky Best [ctb],\n  Kate Cowles [ctb],\n  Karen Vines [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BART",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BART Bayesian Additive Regression Trees Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes.  For more information see Sparapani, Spanbauer and McCulloch <doi:10.18637/jss.v097.i01>.  "
  },
  {
    "id": 1790,
    "package_name": "BAwiR",
    "title": "Analysis of Basketball Data",
    "description": "Collection of tools to work with European basketball data. Functions available are related to friendly \n\tweb scraping, data management and visualization. Data were obtained from <https://www.euroleaguebasketball.net/euroleague/>, \n\t<https://www.euroleaguebasketball.net/eurocup/> and <https://www.acb.com/>, following the instructions \n        of their respectives robots.txt files, when available. Box score data are available for the three leagues. \n\tPlay-by-play and spatial shooting data are also available for the Spanish league. Methods for analysis include a \n\tpopulation pyramid, 2D plots, circular plots of players' percentiles, plots of players' monthly/yearly stats, \n\tteam heatmaps, team shooting plots, team four factors plots, cross-tables with the results of regular season games,\n\tmaps of nationalities, combinations of lineups, possessions-related variables, timeouts,\n\tperformance by periods, personal fouls, offensive rebounds and different types of shooting charts. \n\tPlease see Vinue (2020) <doi:10.1089/big.2018.0124> and Vinue (2024) <doi:10.1089/big.2023.0177>. ",
    "version": "1.4.3",
    "maintainer": "Guillermo Vinue <guillermo.vinue@uv.es>",
    "author": "Guillermo Vinue [aut, cre]",
    "url": "https://www.uv.es/vivigui/basketball_platform.html,\nhttps://www.uv.es/vivigui/, https://www.R-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BAwiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BAwiR Analysis of Basketball Data Collection of tools to work with European basketball data. Functions available are related to friendly \n\tweb scraping, data management and visualization. Data were obtained from <https://www.euroleaguebasketball.net/euroleague/>, \n\t<https://www.euroleaguebasketball.net/eurocup/> and <https://www.acb.com/>, following the instructions \n        of their respectives robots.txt files, when available. Box score data are available for the three leagues. \n\tPlay-by-play and spatial shooting data are also available for the Spanish league. Methods for analysis include a \n\tpopulation pyramid, 2D plots, circular plots of players' percentiles, plots of players' monthly/yearly stats, \n\tteam heatmaps, team shooting plots, team four factors plots, cross-tables with the results of regular season games,\n\tmaps of nationalities, combinations of lineups, possessions-related variables, timeouts,\n\tperformance by periods, personal fouls, offensive rebounds and different types of shooting charts. \n\tPlease see Vinue (2020) <doi:10.1089/big.2018.0124> and Vinue (2024) <doi:10.1089/big.2023.0177>.   "
  },
  {
    "id": 1810,
    "package_name": "BDEsize",
    "title": "Efficient Determination of Sample Size in Balanced Design of\nExperiments",
    "description": "For a balanced design of experiments, this package calculates the sample size required to detect a certain standardized effect size, under a significance level. This package also provides three graphs; detectable standardized effect size vs power, sample size vs detectable standardized effect size, and sample size vs power, which show the mutual relationship between the sample size, power and the detectable standardized effect size. The detailed procedure is described in R. V. Lenth (2006-9) <https://homepage.divms.uiowa.edu/~rlenth/Power/>, Y. B. Lim (1998), M. A. Kastenbaum, D. G. Hoel and K. O. Bowman (1970) <doi:10.2307/2334851>, and Douglas C. Montgomery (2013, ISBN: 0849323312).",
    "version": "1.6",
    "maintainer": "Jong Hee Chung <jochung947@gmail.com>",
    "author": "Jong Hee Chung [aut, cre],\n  Yong Bin Lim [aut],\n  Donghoh Kim [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BDEsize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BDEsize Efficient Determination of Sample Size in Balanced Design of\nExperiments For a balanced design of experiments, this package calculates the sample size required to detect a certain standardized effect size, under a significance level. This package also provides three graphs; detectable standardized effect size vs power, sample size vs detectable standardized effect size, and sample size vs power, which show the mutual relationship between the sample size, power and the detectable standardized effect size. The detailed procedure is described in R. V. Lenth (2006-9) <https://homepage.divms.uiowa.edu/~rlenth/Power/>, Y. B. Lim (1998), M. A. Kastenbaum, D. G. Hoel and K. O. Bowman (1970) <doi:10.2307/2334851>, and Douglas C. Montgomery (2013, ISBN: 0849323312).  "
  },
  {
    "id": 1846,
    "package_name": "BIGDAWG",
    "title": "Case-Control Analysis of Multi-Allelic Loci",
    "description": "Data sets and functions for chi-squared Hardy-Weinberg and case-control association tests of highly polymorphic genetic data [e.g., human leukocyte antigen (HLA) data]. Performs association tests at multiple levels of polymorphism (haplotype, locus and HLA amino-acids) as described in Pappas DJ, Marin W, Hollenbach JA, Mack SJ (2016) <doi:10.1016/j.humimm.2015.12.006>. Combines rare variants to a common class to account for sparse cells in tables as described by Hollenbach JA, Mack SJ, Thomson G, Gourraud PA (2012) <doi:10.1007/978-1-61779-842-9_14>.",
    "version": "3.0.3",
    "maintainer": "Steve Mack <Steven.Mack@ucsf.edu>",
    "author": "Derek Pappas <djpappas75@gmail.com>, Steve Mack <Steven.Mack@ucsf.edu>, Jill Hollenbach <Jill.Hollenbach@ucsf.edu>",
    "url": "http://tools.immunogenomics.org/,\nhttps://github.com/IgDAWG/BIGDAWG",
    "bug_reports": "https://github.com/IgDAWG/BIGDAWG/issues",
    "repository": "https://cran.r-project.org/package=BIGDAWG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BIGDAWG Case-Control Analysis of Multi-Allelic Loci Data sets and functions for chi-squared Hardy-Weinberg and case-control association tests of highly polymorphic genetic data [e.g., human leukocyte antigen (HLA) data]. Performs association tests at multiple levels of polymorphism (haplotype, locus and HLA amino-acids) as described in Pappas DJ, Marin W, Hollenbach JA, Mack SJ (2016) <doi:10.1016/j.humimm.2015.12.006>. Combines rare variants to a common class to account for sparse cells in tables as described by Hollenbach JA, Mack SJ, Thomson G, Gourraud PA (2012) <doi:10.1007/978-1-61779-842-9_14>.  "
  },
  {
    "id": 1875,
    "package_name": "BMRMM",
    "title": "An Implementation of the Bayesian Markov (Renewal) Mixed Models",
    "description": "The Bayesian Markov renewal mixed models take sequentially observed categorical data with continuous duration times, being either state duration or inter-state duration. These models comprehensively analyze the stochastic dynamics of both state transitions and duration times under the influence of multiple exogenous factors and random individual effect. The default setting flexibly models the transition probabilities using Dirichlet mixtures and the duration times using gamma mixtures. It also provides the flexibility of modeling the categorical sequences using Bayesian Markov mixed models alone, either ignoring the duration times altogether or dividing duration time into multiples of an additional category in the sequence by a user-specific unit. The package allows extensive inference of the state transition probabilities and the duration times as well as relevant plots and graphs. It also includes a synthetic data set to demonstrate the desired format of input data set and the utility of various functions. Methods for Bayesian Markov renewal mixed models are as described in: Abhra Sarkar et al., (2018) <doi:10.1080/01621459.2018.1423986> and Yutong Wu et al., (2022) <doi:10.1093/biostatistics/kxac050>.",
    "version": "1.0.1",
    "maintainer": "Yutong Wu <yutong.wu@utexas.edu>",
    "author": "Yutong Wu [aut, cre],\n  Abhra Sarkar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BMRMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BMRMM An Implementation of the Bayesian Markov (Renewal) Mixed Models The Bayesian Markov renewal mixed models take sequentially observed categorical data with continuous duration times, being either state duration or inter-state duration. These models comprehensively analyze the stochastic dynamics of both state transitions and duration times under the influence of multiple exogenous factors and random individual effect. The default setting flexibly models the transition probabilities using Dirichlet mixtures and the duration times using gamma mixtures. It also provides the flexibility of modeling the categorical sequences using Bayesian Markov mixed models alone, either ignoring the duration times altogether or dividing duration time into multiples of an additional category in the sequence by a user-specific unit. The package allows extensive inference of the state transition probabilities and the duration times as well as relevant plots and graphs. It also includes a synthetic data set to demonstrate the desired format of input data set and the utility of various functions. Methods for Bayesian Markov renewal mixed models are as described in: Abhra Sarkar et al., (2018) <doi:10.1080/01621459.2018.1423986> and Yutong Wu et al., (2022) <doi:10.1093/biostatistics/kxac050>.  "
  },
  {
    "id": 1898,
    "package_name": "BRINDA",
    "title": "Computation of BRINDA Adjusted Micronutrient Biomarkers for\nInflammation",
    "description": "Inflammation can affect many micronutrient biomarkers and can thus lead to incorrect diagnosis of individuals and to over- or under-estimate the prevalence of deficiency in a population. Biomarkers Reflecting Inflammation and Nutritional Determinants of Anemia (BRINDA) is a multi-agency and multi-country partnership designed to improve the interpretation of nutrient biomarkers in settings of inflammation and to generate context-specific estimates of risk factors for anemia (Suchdev (2016) <doi:10.3945/an.115.010215>). In the past few years, BRINDA published a series of papers to provide guidance on how to adjust micronutrient biomarkers, retinol binding protein, serum retinol, serum ferritin by Namaste (2020), soluble transferrin receptor (sTfR), serum zinc, serum and Red Blood Cell (RBC) folate, and serum B-12, using inflammation markers, alpha-1-acid glycoprotein (AGP) and/or C-Reactive Protein (CRP) by Namaste (2020) <doi:10.1093/ajcn/nqaa141>, Rohner (2017) <doi:10.3945/ajcn.116.142232>, McDonald (2020) <doi:10.1093/ajcn/nqz304>, and Young (2020) <doi:10.1093/ajcn/nqz303>. The BRINDA inflammation adjustment method mainly focuses on Women of Reproductive Age (WRA) and Preschool-age Children (PSC); however, the general principle of the BRINDA method might apply to other population groups. The BRINDA R package is a user-friendly all-in-one R package that uses a series of functions to implement BRINDA adjustment method, as described above. The BRINDA R package will first carry out rigorous checks and provides users guidance to correct data or input errors (if they occur) prior to inflammation adjustments. After no errors are detected, the package implements the BRINDA inflammation adjustment for up to five micronutrient biomarkers, namely retinol-binding-protein, serum retinol, serum ferritin, sTfR, and serum zinc (when appropriate), using inflammation indicators of AGP and/or CRP for various population groups. Of note, adjustment for serum and RBC folate and serum B-12 is not included in the R package, since evidence shows that no adjustment is needed for these micronutrient biomarkers in either WRA or PSC groups (Young (2020) <doi:10.1093/ajcn/nqz303>).",
    "version": "0.1.5",
    "maintainer": "Hanqi Luo <LUOHANQI@gmail.com>",
    "author": "Hanqi Luo [cre, aut] (ORCID: <https://orcid.org/0000-0001-6253-5818>),\n  O Yaw Addo [aut] (ORCID: <https://orcid.org/0000-0003-1269-759X>),\n  Jiaxi Geng [ctb]",
    "url": "https://github.com/hanqiluo/BRINDA",
    "bug_reports": "https://github.com/hanqiluo/BRINDA/issues",
    "repository": "https://cran.r-project.org/package=BRINDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BRINDA Computation of BRINDA Adjusted Micronutrient Biomarkers for\nInflammation Inflammation can affect many micronutrient biomarkers and can thus lead to incorrect diagnosis of individuals and to over- or under-estimate the prevalence of deficiency in a population. Biomarkers Reflecting Inflammation and Nutritional Determinants of Anemia (BRINDA) is a multi-agency and multi-country partnership designed to improve the interpretation of nutrient biomarkers in settings of inflammation and to generate context-specific estimates of risk factors for anemia (Suchdev (2016) <doi:10.3945/an.115.010215>). In the past few years, BRINDA published a series of papers to provide guidance on how to adjust micronutrient biomarkers, retinol binding protein, serum retinol, serum ferritin by Namaste (2020), soluble transferrin receptor (sTfR), serum zinc, serum and Red Blood Cell (RBC) folate, and serum B-12, using inflammation markers, alpha-1-acid glycoprotein (AGP) and/or C-Reactive Protein (CRP) by Namaste (2020) <doi:10.1093/ajcn/nqaa141>, Rohner (2017) <doi:10.3945/ajcn.116.142232>, McDonald (2020) <doi:10.1093/ajcn/nqz304>, and Young (2020) <doi:10.1093/ajcn/nqz303>. The BRINDA inflammation adjustment method mainly focuses on Women of Reproductive Age (WRA) and Preschool-age Children (PSC); however, the general principle of the BRINDA method might apply to other population groups. The BRINDA R package is a user-friendly all-in-one R package that uses a series of functions to implement BRINDA adjustment method, as described above. The BRINDA R package will first carry out rigorous checks and provides users guidance to correct data or input errors (if they occur) prior to inflammation adjustments. After no errors are detected, the package implements the BRINDA inflammation adjustment for up to five micronutrient biomarkers, namely retinol-binding-protein, serum retinol, serum ferritin, sTfR, and serum zinc (when appropriate), using inflammation indicators of AGP and/or CRP for various population groups. Of note, adjustment for serum and RBC folate and serum B-12 is not included in the R package, since evidence shows that no adjustment is needed for these micronutrient biomarkers in either WRA or PSC groups (Young (2020) <doi:10.1093/ajcn/nqz303>).  "
  },
  {
    "id": 1913,
    "package_name": "BSTZINB",
    "title": "Association Among Disease Counts and Socio-Environmental Factors",
    "description": "Estimation of association between disease or death counts (e.g. COVID-19) and socio-environmental risk factors using a zero-inflated Bayesian spatiotemporal model. Non-spatiotemporal models and/or models without zero-inflation are also included for comparison. Functions to produce corresponding maps are also included. See Chakraborty et al. (2022) <doi:10.1007/s13253-022-00487-1> for more details on the method.",
    "version": "2.0.1",
    "maintainer": "Suman Majumder <smajumd2@gmail.com>",
    "author": "Suman Majumder [cre, aut, cph],\n  Yoon-Bae Jun [aut, cph],\n  Sounak Chakraborty [ctb],\n  Chae-Young Lim [ctb],\n  Tanujit Dey [ctb]",
    "url": "https://github.com/SumanM47/BSTZINB",
    "bug_reports": "https://github.com/SumanM47/BSTZINB/issues",
    "repository": "https://cran.r-project.org/package=BSTZINB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BSTZINB Association Among Disease Counts and Socio-Environmental Factors Estimation of association between disease or death counts (e.g. COVID-19) and socio-environmental risk factors using a zero-inflated Bayesian spatiotemporal model. Non-spatiotemporal models and/or models without zero-inflation are also included for comparison. Functions to produce corresponding maps are also included. See Chakraborty et al. (2022) <doi:10.1007/s13253-022-00487-1> for more details on the method.  "
  },
  {
    "id": 1917,
    "package_name": "BTLLasso",
    "title": "Modelling Heterogeneity in Paired Comparison Data",
    "description": "Performs 'BTLLasso' as described by Schauberger and Tutz (2019) <doi:10.18637/jss.v088.i09> and Schauberger and Tutz (2017) <doi:10.1177/1471082X17693086>. BTLLasso is a method to include different types of variables in paired comparison models and, therefore, to allow for heterogeneity between subjects. Variables can be subject-specific, object-specific and subject-object-specific and can have an influence on the attractiveness/strength of the objects. Suitable L1 penalty terms are used to cluster certain effects and to reduce the complexity of the models.",
    "version": "0.1-14",
    "maintainer": "Gunther Schauberger <gunther.schauberger@tum.de>",
    "author": "Gunther Schauberger [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BTLLasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BTLLasso Modelling Heterogeneity in Paired Comparison Data Performs 'BTLLasso' as described by Schauberger and Tutz (2019) <doi:10.18637/jss.v088.i09> and Schauberger and Tutz (2017) <doi:10.1177/1471082X17693086>. BTLLasso is a method to include different types of variables in paired comparison models and, therefore, to allow for heterogeneity between subjects. Variables can be subject-specific, object-specific and subject-object-specific and can have an influence on the attractiveness/strength of the objects. Suitable L1 penalty terms are used to cluster certain effects and to reduce the complexity of the models.  "
  },
  {
    "id": 1931,
    "package_name": "BaHZING",
    "title": "Bayesian Hierarchical Zero-Inflated Negative Binomial Regression\nwith G-Computation",
    "description": "A Bayesian model for examining the association between\n    environmental mixtures and all Taxa measured in a hierarchical\n    microbiome dataset in a single integrated analysis. Compared with\n    analyzing the associations of environmental mixtures with each Taxa\n    individually, 'BaHZING' controls Type 1 error rates and provides more\n    stable effect estimates when dealing with small sample sizes.",
    "version": "1.0.0",
    "maintainer": "Jesse Goodrich <jagoodri@usc.edu>",
    "author": "Hailey Hampson [aut],\n  Jesse Goodrich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6615-0472>),\n  Hongxu Wang [aut],\n  Tanya Alderete [ctb],\n  Shardul Nazirkar [ctb],\n  David Conti [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BaHZING",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BaHZING Bayesian Hierarchical Zero-Inflated Negative Binomial Regression\nwith G-Computation A Bayesian model for examining the association between\n    environmental mixtures and all Taxa measured in a hierarchical\n    microbiome dataset in a single integrated analysis. Compared with\n    analyzing the associations of environmental mixtures with each Taxa\n    individually, 'BaHZING' controls Type 1 error rates and provides more\n    stable effect estimates when dealing with small sample sizes.  "
  },
  {
    "id": 1945,
    "package_name": "BasketballAnalyzeR",
    "title": "Analysis and Visualization of Basketball Data",
    "description": "Contains data and code to accompany  the book \n             P. Zuccolotto and M. Manisera (2020) Basketball Data Science. Applications with R. CRC Press. ISBN 9781138600799.",
    "version": "0.8.1",
    "maintainer": "Marco Sandri <basketballanalyzer.help@unibs.it>",
    "author": "Marco Sandri [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1422-5695>),\n  Paola Zuccolotto [aut] (ORCID: <https://orcid.org/0000-0003-4399-7018>),\n  Marica Manisera [aut] (ORCID: <https://orcid.org/0000-0002-2982-0243>)",
    "url": "https://github.com/sndmrc/BasketballAnalyzeR/",
    "bug_reports": "https://github.com/sndmrc/BasketballAnalyzeR/issues",
    "repository": "https://cran.r-project.org/package=BasketballAnalyzeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BasketballAnalyzeR Analysis and Visualization of Basketball Data Contains data and code to accompany  the book \n             P. Zuccolotto and M. Manisera (2020) Basketball Data Science. Applications with R. CRC Press. ISBN 9781138600799.  "
  },
  {
    "id": 1975,
    "package_name": "BayesGOF",
    "title": "Bayesian Modeling via Frequentist Goodness-of-Fit",
    "description": "A Bayesian data modeling scheme that performs four interconnected tasks: (i) characterizes the uncertainty of the elicited parametric prior; (ii) provides exploratory diagnostic for checking prior-data conflict; (iii) computes the final statistical prior density estimate; and (iv) executes macro- and micro-inference. Primary reference is Mukhopadhyay, S. and Fletcher, D. 2018 paper \"Generalized Empirical Bayes via Frequentist Goodness of Fit\" (<https://www.nature.com/articles/s41598-018-28130-5 >). ",
    "version": "5.2",
    "maintainer": "Doug Fletcher <tug25070@temple.edu>",
    "author": "Subhadeep Mukhopadhyay, Douglas Fletcher",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesGOF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesGOF Bayesian Modeling via Frequentist Goodness-of-Fit A Bayesian data modeling scheme that performs four interconnected tasks: (i) characterizes the uncertainty of the elicited parametric prior; (ii) provides exploratory diagnostic for checking prior-data conflict; (iii) computes the final statistical prior density estimate; and (iv) executes macro- and micro-inference. Primary reference is Mukhopadhyay, S. and Fletcher, D. 2018 paper \"Generalized Empirical Bayes via Frequentist Goodness of Fit\" (<https://www.nature.com/articles/s41598-018-28130-5 >).   "
  },
  {
    "id": 1991,
    "package_name": "BayesNetBP",
    "title": "Bayesian Network Belief Propagation",
    "description": "Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <https://www.jmlr.org/papers/v6/cowell05a.html>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub. ",
    "version": "1.6.1",
    "maintainer": "Han Yu <hyu9@buffalo.edu>",
    "author": "Han Yu, Rachael Blair, Janhavi Moharil, Andrew Yan",
    "url": "",
    "bug_reports": "https://github.com/hyu-ub/BayesNetBP/issues",
    "repository": "https://cran.r-project.org/package=BayesNetBP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesNetBP Bayesian Network Belief Propagation Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <https://www.jmlr.org/papers/v6/cowell05a.html>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub.   "
  },
  {
    "id": 2045,
    "package_name": "BetaBit",
    "title": "Mini Games from Adventures of Beta and Bit",
    "description": "Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.",
    "version": "2.2",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek [aut, cre],\n  Witold Chodor [trl],\n  Katarzyna Fak [aut],\n  Tomasz Zoltak [aut],\n  Foundation SmarterPoland.pl [cph]",
    "url": "https://github.com/BetaAndBit/Charts",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BetaBit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BetaBit Mini Games from Adventures of Beta and Bit Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.  "
  },
  {
    "id": 2112,
    "package_name": "BlandAltmanLeh",
    "title": "Plots (Slightly Extended) Bland-Altman Plots",
    "description": "Bland-Altman Plots using either base graphics or ggplot2,\n    augmented with confidence intervals, with detailed return values and\n    a sunflowerplot option for data with ties.",
    "version": "0.3.1",
    "maintainer": "Bernhard Lehnert <bernhard.lehnert@uni-greifswald.de>",
    "author": "Bernhard Lehnert",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BlandAltmanLeh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BlandAltmanLeh Plots (Slightly Extended) Bland-Altman Plots Bland-Altman Plots using either base graphics or ggplot2,\n    augmented with confidence intervals, with detailed return values and\n    a sunflowerplot option for data with ties.  "
  },
  {
    "id": 2122,
    "package_name": "BoardGames",
    "title": "Board Games and Tools for Building Board Games",
    "description": "Tools for constructing board/grid based games, as well as readily available game(s) for your entertainment.",
    "version": "1.0.0",
    "maintainer": "Derek Qiu <qiu.derek.d@gmail.com>",
    "author": "Derek Qiu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BoardGames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BoardGames Board Games and Tools for Building Board Games Tools for constructing board/grid based games, as well as readily available game(s) for your entertainment.  "
  },
  {
    "id": 2181,
    "package_name": "CARBayes",
    "title": "Spatial Generalised Linear Mixed Models for Areal Unit Data",
    "description": "Implements a class of univariate and multivariate spatial generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation using a single or multiple Markov chains. The response variable can be binomial, Gaussian, multinomial, Poisson or zero-inflated Poisson (ZIP), and spatial autocorrelation is modelled by a set of random effects that are assigned a conditional autoregressive (CAR) prior distribution. A number of different models are available for univariate spatial data, including models with no random effects as well as random effects modelled by different types of CAR prior, including the BYM model (Besag et al., 1991, <doi:10.1007/BF00116466>) and Leroux model (Leroux et al., 2000, <doi:10.1007/978-1-4612-1284-3_4>). Additionally,  a multivariate CAR (MCAR) model for multivariate spatial data is available, as is a two-level hierarchical model for modelling data relating to individuals within areas. Full details are given in the vignette accompanying this package. The initial creation of this package was supported by the Economic and Social Research Council (ESRC) grant RES-000-22-4256, and on-going development has been supported by the Engineering and Physical Science Research Council (EPSRC) grant EP/J017442/1, ESRC grant ES/K006460/1, Innovate UK / Natural Environment Research Council (NERC) grant NE/N007352/1 and the TB Alliance. ",
    "version": "6.1.1",
    "maintainer": "Duncan Lee <Duncan.Lee@glasgow.ac.uk>",
    "author": "Duncan Lee",
    "url": "https://github.com/duncanplee/CARBayes",
    "bug_reports": "https://github.com/duncanplee/CARBayes/issues",
    "repository": "https://cran.r-project.org/package=CARBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CARBayes Spatial Generalised Linear Mixed Models for Areal Unit Data Implements a class of univariate and multivariate spatial generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation using a single or multiple Markov chains. The response variable can be binomial, Gaussian, multinomial, Poisson or zero-inflated Poisson (ZIP), and spatial autocorrelation is modelled by a set of random effects that are assigned a conditional autoregressive (CAR) prior distribution. A number of different models are available for univariate spatial data, including models with no random effects as well as random effects modelled by different types of CAR prior, including the BYM model (Besag et al., 1991, <doi:10.1007/BF00116466>) and Leroux model (Leroux et al., 2000, <doi:10.1007/978-1-4612-1284-3_4>). Additionally,  a multivariate CAR (MCAR) model for multivariate spatial data is available, as is a two-level hierarchical model for modelling data relating to individuals within areas. Full details are given in the vignette accompanying this package. The initial creation of this package was supported by the Economic and Social Research Council (ESRC) grant RES-000-22-4256, and on-going development has been supported by the Engineering and Physical Science Research Council (EPSRC) grant EP/J017442/1, ESRC grant ES/K006460/1, Innovate UK / Natural Environment Research Council (NERC) grant NE/N007352/1 and the TB Alliance.   "
  },
  {
    "id": 2257,
    "package_name": "CIEE",
    "title": "Estimating and Testing Direct Effects in Directed Acyclic Graphs\nusing Estimating Equations",
    "description": "In many studies across different disciplines, detailed measures of the variables of interest are available. If assumptions can be made regarding the direction of effects between the assessed variables, this has to be considered in the analysis. The functions in this package implement the novel approach CIEE (causal inference using estimating equations; Konigorski et al., 2018, <DOI:10.1002/gepi.22107>) for estimating and testing the direct effect of an exposure variable on a primary outcome, while adjusting for indirect effects of the exposure on the primary outcome through a secondary intermediate outcome and potential factors influencing the secondary outcome. The underlying directed acyclic graph (DAG) of this considered model is described in the vignette. CIEE can be applied to studies in many different fields, and it is implemented here for the analysis of a continuous primary outcome and a time-to-event primary outcome subject to censoring. CIEE uses estimating equations to obtain estimates of the direct effect and robust sandwich standard error estimates. Then, a large-sample Wald-type test statistic is computed for testing the absence of the direct effect. Additionally, standard multiple regression, regression of residuals, and the structural equation modeling approach are implemented for comparison. ",
    "version": "0.1.1",
    "maintainer": "Stefan Konigorski <stefan.konigorski@gmail.com>",
    "author": "Stefan Konigorski [aut, cre],\n  Yildiz E. Yilmaz [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CIEE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CIEE Estimating and Testing Direct Effects in Directed Acyclic Graphs\nusing Estimating Equations In many studies across different disciplines, detailed measures of the variables of interest are available. If assumptions can be made regarding the direction of effects between the assessed variables, this has to be considered in the analysis. The functions in this package implement the novel approach CIEE (causal inference using estimating equations; Konigorski et al., 2018, <DOI:10.1002/gepi.22107>) for estimating and testing the direct effect of an exposure variable on a primary outcome, while adjusting for indirect effects of the exposure on the primary outcome through a secondary intermediate outcome and potential factors influencing the secondary outcome. The underlying directed acyclic graph (DAG) of this considered model is described in the vignette. CIEE can be applied to studies in many different fields, and it is implemented here for the analysis of a continuous primary outcome and a time-to-event primary outcome subject to censoring. CIEE uses estimating equations to obtain estimates of the direct effect and robust sandwich standard error estimates. Then, a large-sample Wald-type test statistic is computed for testing the absence of the direct effect. Additionally, standard multiple regression, regression of residuals, and the structural equation modeling approach are implemented for comparison.   "
  },
  {
    "id": 2270,
    "package_name": "CIplot",
    "title": "Functions to Plot Confidence Interval",
    "description": "Plot confidence interval from the objects of statistical tests such as\n  t.test(), var.test(), cor.test(), prop.test() and fisher.test() ('htest' class),\n  Tukey test [TukeyHSD()], Dunnett test [glht() in 'multcomp' package],\n  logistic regression [glm()], and Tukey or Games-Howell test [posthocTGH() in\n  'userfriendlyscience' package].\n  Users are able to set the styles of lines and points.\n  This package contains the function to calculate odds ratios and their confidence\n  intervals from the result of logistic regression.",
    "version": "1.0",
    "maintainer": "Toshiaki Ara <toshiaki.ara@gmail.com>",
    "author": "Toshiaki Ara",
    "url": "https://github.com/toshi-ara/CIplot",
    "bug_reports": "https://github.com/toshi-ara/CIplot/issues/",
    "repository": "https://cran.r-project.org/package=CIplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CIplot Functions to Plot Confidence Interval Plot confidence interval from the objects of statistical tests such as\n  t.test(), var.test(), cor.test(), prop.test() and fisher.test() ('htest' class),\n  Tukey test [TukeyHSD()], Dunnett test [glht() in 'multcomp' package],\n  logistic regression [glm()], and Tukey or Games-Howell test [posthocTGH() in\n  'userfriendlyscience' package].\n  Users are able to set the styles of lines and points.\n  This package contains the function to calculate odds ratios and their confidence\n  intervals from the result of logistic regression.  "
  },
  {
    "id": 2283,
    "package_name": "CLVTools",
    "title": "Tools for Customer Lifetime Value Estimation",
    "description": "\n    A set of state-of-the-art probabilistic modeling approaches to derive estimates of individual customer lifetime values (CLV).\n    Commonly, probabilistic approaches focus on modelling 3 processes, i.e. individuals' attrition, transaction, and spending process. \n    Latent customer attrition models, which are also known as \"buy-'til-you-die models\", model the attrition as well as the transaction process. \n    They are used to make inferences and predictions about transactional patterns of individual customers such as their future purchase behavior. \n    Moreover, these models have also been used to predict individuals\u2019 long-term engagement in activities such as playing an online game or \n    posting to a social media platform. The spending process is usually modelled by a separate probabilistic model. Combining these results yields in \n    lifetime values estimates for individual customers.\n    This package includes fast and accurate implementations of various probabilistic models for non-contractual settings \n    (e.g., grocery purchases or hotel visits). All implementations support time-invariant covariates, which can be used to control for e.g., \n    socio-demographics. If such an extension has been proposed in literature, we further provide the possibility to control for time-varying \n    covariates to control for e.g., seasonal patterns. \n    Currently, the package includes the following latent attrition models to model individuals' attrition and transaction process: \n    [1] Pareto/NBD model (Pareto/Negative-Binomial-Distribution), \n    [2] the Extended Pareto/NBD model (Pareto/Negative-Binomial-Distribution with time-varying covariates), \n    [3] the BG/NBD model (Beta-Gamma/Negative-Binomial-Distribution) and the \n    [4] GGom/NBD (Gamma-Gompertz/Negative-Binomial-Distribution). \n    Further, we provide an implementation of the Gamma/Gamma model to model the spending process of individuals.",
    "version": "0.12.1",
    "maintainer": "Patrick Bachmann <pbachma@ethz.ch>",
    "author": "Patrick Bachmann [cre, aut],\n  Niels Kuebler [aut],\n  Markus Meierer [aut],\n  Jeffrey Naef [aut],\n  E. Shin Oblander [aut],\n  Patrik Schilter [aut]",
    "url": "https://github.com/bachmannpatrick/CLVTools",
    "bug_reports": "https://github.com/bachmannpatrick/CLVTools/issues",
    "repository": "https://cran.r-project.org/package=CLVTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CLVTools Tools for Customer Lifetime Value Estimation \n    A set of state-of-the-art probabilistic modeling approaches to derive estimates of individual customer lifetime values (CLV).\n    Commonly, probabilistic approaches focus on modelling 3 processes, i.e. individuals' attrition, transaction, and spending process. \n    Latent customer attrition models, which are also known as \"buy-'til-you-die models\", model the attrition as well as the transaction process. \n    They are used to make inferences and predictions about transactional patterns of individual customers such as their future purchase behavior. \n    Moreover, these models have also been used to predict individuals\u2019 long-term engagement in activities such as playing an online game or \n    posting to a social media platform. The spending process is usually modelled by a separate probabilistic model. Combining these results yields in \n    lifetime values estimates for individual customers.\n    This package includes fast and accurate implementations of various probabilistic models for non-contractual settings \n    (e.g., grocery purchases or hotel visits). All implementations support time-invariant covariates, which can be used to control for e.g., \n    socio-demographics. If such an extension has been proposed in literature, we further provide the possibility to control for time-varying \n    covariates to control for e.g., seasonal patterns. \n    Currently, the package includes the following latent attrition models to model individuals' attrition and transaction process: \n    [1] Pareto/NBD model (Pareto/Negative-Binomial-Distribution), \n    [2] the Extended Pareto/NBD model (Pareto/Negative-Binomial-Distribution with time-varying covariates), \n    [3] the BG/NBD model (Beta-Gamma/Negative-Binomial-Distribution) and the \n    [4] GGom/NBD (Gamma-Gompertz/Negative-Binomial-Distribution). \n    Further, we provide an implementation of the Gamma/Gamma model to model the spending process of individuals.  "
  },
  {
    "id": 2316,
    "package_name": "COMPoissonReg",
    "title": "Conway-Maxwell Poisson (COM-Poisson) Regression",
    "description": "Fit Conway-Maxwell Poisson (COM-Poisson or CMP) regression models\n    to count data (Sellers & Shmueli, 2010) <doi:10.1214/09-AOAS306>. The\n    package provides functions for model estimation, dispersion testing, and\n    diagnostics. Zero-inflated CMP regression (Sellers & Raim, 2016)\n    <doi:10.1016/j.csda.2016.01.007> is also supported.",
    "version": "0.8.1",
    "maintainer": "Andrew Raim <andrew.raim@gmail.com>",
    "author": "Kimberly Sellers <kfs7@georgetown.edu>\n\tThomas Lotze <thomas.lotze@thomaslotze.com>\n\tAndrew Raim <andrew.raim@gmail.com>",
    "url": "https://github.com/lotze/COMPoissonReg",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=COMPoissonReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COMPoissonReg Conway-Maxwell Poisson (COM-Poisson) Regression Fit Conway-Maxwell Poisson (COM-Poisson or CMP) regression models\n    to count data (Sellers & Shmueli, 2010) <doi:10.1214/09-AOAS306>. The\n    package provides functions for model estimation, dispersion testing, and\n    diagnostics. Zero-inflated CMP regression (Sellers & Raim, 2016)\n    <doi:10.1016/j.csda.2016.01.007> is also supported.  "
  },
  {
    "id": 2383,
    "package_name": "CaDENCE",
    "title": "Conditional Density Estimation Network Construction and\nEvaluation",
    "description": "Parameters of a user-specified probability distribution are modelled by a multi-layer perceptron artificial neural network. This framework can be used to implement probabilistic nonlinear models including mixture density networks, heteroscedastic regression models, zero-inflated models, etc. following Cannon (2012) <doi:10.1016/j.cageo.2011.08.023>.",
    "version": "1.2.5",
    "maintainer": "Alex J. Cannon <alex.cannon@canada.ca>",
    "author": "Alex J. Cannon",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CaDENCE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CaDENCE Conditional Density Estimation Network Construction and\nEvaluation Parameters of a user-specified probability distribution are modelled by a multi-layer perceptron artificial neural network. This framework can be used to implement probabilistic nonlinear models including mixture density networks, heteroscedastic regression models, zero-inflated models, etc. following Cannon (2012) <doi:10.1016/j.cageo.2011.08.023>.  "
  },
  {
    "id": 2393,
    "package_name": "CamelUp",
    "title": "'CamelUp' Board Game as a Teaching Aid for Introductory\nStatistics",
    "description": "Implements the board game 'CamelUp' for use in introductory statistics classes using a Shiny app. ",
    "version": "2.0.3",
    "maintainer": "Michael Czekanski <middleburystatspackages@gmail.com>",
    "author": "Michael Czekanski [aut, cre],\n  Alex Lyford [aut],\n  Tom Rahr [aut],\n  Tina Chen [aut]",
    "url": "",
    "bug_reports": "https://github.com/mczekanski1/Camel-Up/issues",
    "repository": "https://cran.r-project.org/package=CamelUp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CamelUp 'CamelUp' Board Game as a Teaching Aid for Introductory\nStatistics Implements the board game 'CamelUp' for use in introductory statistics classes using a Shiny app.   "
  },
  {
    "id": 2446,
    "package_name": "ChaosGame",
    "title": "Chaos Game",
    "description": "The main objective of the package is to enter a word of at least two letters based on which an Iterated Function System with Probabilities is constructed, and a two-dimensional fractal containing the chosen word infinitely often is generated via the Chaos Game. Additionally, the package allows to project the two-dimensional fractal on several three-dimensional surfaces and to transform the fractal into another fractal with uniform marginals.",
    "version": "1.5",
    "maintainer": "Lea Maislinger <lea.maislinger@plus.ac.at>",
    "author": "Lea Maislinger [aut, cre],\n  Thimo Kasper [aut],\n  Florian Griessenberger [aut],\n  Manuela Schreyer [aut],\n  Johannes Bartel [aut],\n  Wolfgang Trutschnig [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ChaosGame",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ChaosGame Chaos Game The main objective of the package is to enter a word of at least two letters based on which an Iterated Function System with Probabilities is constructed, and a two-dimensional fractal containing the chosen word infinitely often is generated via the Chaos Game. Additionally, the package allows to project the two-dimensional fractal on several three-dimensional surfaces and to transform the fractal into another fractal with uniform marginals.  "
  },
  {
    "id": 2454,
    "package_name": "ChessGmooG",
    "title": "FIDE Chess Players Ratings for 2015 and 2020",
    "description": "Datasets of the International Chess Federation's player ratings and country information analysed in the book Antony Unwin (2024, ISBN:978-0367674007) \"Getting (more out of) Graphics\".",
    "version": "0.1.0",
    "maintainer": "Antony Unwin <unwin@math.uni-augsburg.de>",
    "author": "Antony Unwin [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ChessGmooG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ChessGmooG FIDE Chess Players Ratings for 2015 and 2020 Datasets of the International Chess Federation's player ratings and country information analysed in the book Antony Unwin (2024, ISBN:978-0367674007) \"Getting (more out of) Graphics\".  "
  },
  {
    "id": 2479,
    "package_name": "ClaimsProblems",
    "title": "Analysis of Conflicting Claims",
    "description": "The analysis of conflicting claims arises when an amount has\n    to be divided among a set of agents with claims that exceed what is\n    available. A rule is a way of selecting a division among the\n    claimants. This package computes the main rules introduced in the\n    literature from ancient times to the present. The inventory of rules\n    covers the proportional and the adjusted proportional rules, the\n    constrained equal awards and the constrained equal losses rules, the\n    constrained egalitarian, the Piniles\u2019 and the minimal overlap rules,\n    the random arrival and the Talmud rules. Besides, the Dominguez and\n    Thomson and the average-of-awards rules are also included.  All of\n    them can be found in the book by W. Thomson (2019), How to divide when\n    there isn't enough. From Aristotle, the Talmud, and Maimonides to the\n    axiomatics of resource allocation', except for the average-of-awards\n    rule, introduced by Mir\u00e1s Calvo et al. (2022),\n    <doi:10.1007/s00355-022-01414-6>.  In addition, graphical diagrams\n    allow the user to represent, among others, the set of awards, the\n    paths of awards, the schedules of awards of a rule, and some indexes.\n    A good understanding of the similarities and differences between the\n    rules is useful for better decision-making. Therefore, this package\n    could be helpful to students, researchers, and managers alike.  For a\n    more detailed explanation of the package, see Mir\u00e1s Calvo et al.\n    (2023), <doi:10.1016/j.dajour.2022.100160>.",
    "version": "1.0.0",
    "maintainer": "Estela S\u00e1nchez Rodr\u00edguez <esanchez@uvigo.es>",
    "author": "Estela S\u00e1nchez Rodr\u00edguez [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-0933-6411>, SiDOR. Departamento de\n    Estat\u00edstica e Investigaci\u00f3n Operativa. Universidade de Vigo.\n    CITMAga. Spain),\n  Iago N\u00fa\u00f1ez Lugilde [aut] (ORCID:\n    <https://orcid.org/0000-0003-3382-0737>, Departamento de\n    Matem\u00e1ticas. MODES. Universidade da Coru\u00f1a. Spain),\n  Miguel \u00c1ngel Mir\u00e1s Calvo [aut] (ORCID:\n    <https://orcid.org/0000-0001-7247-1926>, RGEAF. Departamento de\n    Matem\u00e1ticas. Universidade de Vigo. Spain),\n  Carmen Quinteiro Sandomingo [aut] (ORCID:\n    <https://orcid.org/0000-0002-2711-1945>, Departamento de\n    Matem\u00e1ticas. Universidade de Vigo. Spain),\n  MCIN/AEI/10.13039/501100011033 [fnd] (Project PID2021-124030NB-C33.\n    ERDF A way of making Europe/EU)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ClaimsProblems",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClaimsProblems Analysis of Conflicting Claims The analysis of conflicting claims arises when an amount has\n    to be divided among a set of agents with claims that exceed what is\n    available. A rule is a way of selecting a division among the\n    claimants. This package computes the main rules introduced in the\n    literature from ancient times to the present. The inventory of rules\n    covers the proportional and the adjusted proportional rules, the\n    constrained equal awards and the constrained equal losses rules, the\n    constrained egalitarian, the Piniles\u2019 and the minimal overlap rules,\n    the random arrival and the Talmud rules. Besides, the Dominguez and\n    Thomson and the average-of-awards rules are also included.  All of\n    them can be found in the book by W. Thomson (2019), How to divide when\n    there isn't enough. From Aristotle, the Talmud, and Maimonides to the\n    axiomatics of resource allocation', except for the average-of-awards\n    rule, introduced by Mir\u00e1s Calvo et al. (2022),\n    <doi:10.1007/s00355-022-01414-6>.  In addition, graphical diagrams\n    allow the user to represent, among others, the set of awards, the\n    paths of awards, the schedules of awards of a rule, and some indexes.\n    A good understanding of the similarities and differences between the\n    rules is useful for better decision-making. Therefore, this package\n    could be helpful to students, researchers, and managers alike.  For a\n    more detailed explanation of the package, see Mir\u00e1s Calvo et al.\n    (2023), <doi:10.1016/j.dajour.2022.100160>.  "
  },
  {
    "id": 2481,
    "package_name": "ClassificationEnsembles",
    "title": "Automatically Builds 12 Classification Models",
    "description": "Automatically builds 12 classification models from data. The package returns 26 plots, 5 tables and a summary report.\n    The package automatically builds six individual classification models, including error (RMSE) and predictions. That data is used to create an ensemble, which is then modeled using six methods.\n    The process is repeated as many times as the user requests. The mean of the results are presented in a summary table. \n    The package returns the confusion matrices for all 12 models, tables of the correlation of the numeric data, the results of the variance inflation process, the head of the ensemble and the head of the data frame.",
    "version": "0.7.1",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "https://github.com/InfiniteCuriosity/ClassificationEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/ClassificationEnsembles/issues",
    "repository": "https://cran.r-project.org/package=ClassificationEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClassificationEnsembles Automatically Builds 12 Classification Models Automatically builds 12 classification models from data. The package returns 26 plots, 5 tables and a summary report.\n    The package automatically builds six individual classification models, including error (RMSE) and predictions. That data is used to create an ensemble, which is then modeled using six methods.\n    The process is repeated as many times as the user requests. The mean of the results are presented in a summary table. \n    The package returns the confusion matrices for all 12 models, tables of the correlation of the numeric data, the results of the variance inflation process, the head of the ensemble and the head of the data frame.  "
  },
  {
    "id": 2508,
    "package_name": "ClustAssess",
    "title": "Tools for Assessing Clustering",
    "description": "A set of tools for evaluating clustering robustness using \n    proportion of ambiguously clustered pairs (Senbabaoglu et al. (2014) \n    <doi:10.1038/srep06207>), as well as similarity across methods \n    and method stability using element-centric clustering comparison (Gates et \n    al. (2019) <doi:10.1038/s41598-019-44892-y>). Additionally, this package \n    enables stability-based parameter assessment for graph-based clustering \n    pipelines typical in single-cell data analysis.",
    "version": "1.1.0",
    "maintainer": "Andi Munteanu <am3019@cam.ac.uk>",
    "author": "Andi Munteanu [aut, cre],\n  Arash Shahsavari [aut],\n  Rafael Kollyfas [ctb],\n  Miguel Larraz Lopez de Novales [aut],\n  Liviu Ciortuz [ctb],\n  Irina Mohorianu [aut]",
    "url": "https://github.com/Core-Bioinformatics/ClustAssess,\nhttps://core-bioinformatics.github.io/ClustAssess/",
    "bug_reports": "https://github.com/Core-Bioinformatics/ClustAssess/issues",
    "repository": "https://cran.r-project.org/package=ClustAssess",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClustAssess Tools for Assessing Clustering A set of tools for evaluating clustering robustness using \n    proportion of ambiguously clustered pairs (Senbabaoglu et al. (2014) \n    <doi:10.1038/srep06207>), as well as similarity across methods \n    and method stability using element-centric clustering comparison (Gates et \n    al. (2019) <doi:10.1038/s41598-019-44892-y>). Additionally, this package \n    enables stability-based parameter assessment for graph-based clustering \n    pipelines typical in single-cell data analysis.  "
  },
  {
    "id": 2575,
    "package_name": "CompositeReliability",
    "title": "Determine the Composite Reliability of a Naturalistic,\nUnbalanced Dataset",
    "description": "The reliability of assessment tools is a crucial aspect of monitoring student performance in various educational settings. It ensures that the assessment outcomes accurately reflect a student's true level of performance. However, when assessments are combined, determining composite reliability can be challenging, especially for naturalistic and unbalanced datasets. This package provides an easy-to-use solution for calculating composite reliability for different assessment types. It allows for the inclusion of weight per assessment type and produces extensive G- and D-study results with graphical interpretations. Overall, our approach enhances the reliability of composite assessments, making it suitable for various education contexts.",
    "version": "1.0.3",
    "maintainer": "Joyce Moonen - van Loon <j.moonen@maastrichtuniversity.nl>",
    "author": "Joyce Moonen - van Loon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8883-8822>)",
    "url": "https://github.com/jmoonen/CompositeReliability",
    "bug_reports": "https://github.com/jmoonen/CompositeReliability/issues",
    "repository": "https://cran.r-project.org/package=CompositeReliability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CompositeReliability Determine the Composite Reliability of a Naturalistic,\nUnbalanced Dataset The reliability of assessment tools is a crucial aspect of monitoring student performance in various educational settings. It ensures that the assessment outcomes accurately reflect a student's true level of performance. However, when assessments are combined, determining composite reliability can be challenging, especially for naturalistic and unbalanced datasets. This package provides an easy-to-use solution for calculating composite reliability for different assessment types. It allows for the inclusion of weight per assessment type and produces extensive G- and D-study results with graphical interpretations. Overall, our approach enhances the reliability of composite assessments, making it suitable for various education contexts.  "
  },
  {
    "id": 2594,
    "package_name": "ConfIntVariance",
    "title": "Confidence Interval for the Univariate Population Variance\nwithout Normality Assumption",
    "description": "Surrounds the usual sample variance of a univariate numeric sample with a confidence interval for the population variance. This has been done so far only under the assumption that the underlying distribution is normal. Under the hood, this package implements the unique least-variance unbiased estimator of the variance of the sample variance, in a formula that is equivalent to estimating kurtosis and square of the population variance in an unbiased way and combining them according to the classical formula into an estimator of the variance of the sample variance. Both the sample variance and the estimator of its variance are U-statistics. By the theory of U-statistic, the resulting estimator is unique. See Fuchs, Krautenbacher (2016) <doi:10.1080/15598608.2016.1158675> and the references therein for an overview of unbiased estimation of variances of U-statistics.",
    "version": "1.0.2",
    "maintainer": "Mathias Fuchs<mathias@mathiasfuchs.de>",
    "author": "Mathias Fuchs",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConfIntVariance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConfIntVariance Confidence Interval for the Univariate Population Variance\nwithout Normality Assumption Surrounds the usual sample variance of a univariate numeric sample with a confidence interval for the population variance. This has been done so far only under the assumption that the underlying distribution is normal. Under the hood, this package implements the unique least-variance unbiased estimator of the variance of the sample variance, in a formula that is equivalent to estimating kurtosis and square of the population variance in an unbiased way and combining them according to the classical formula into an estimator of the variance of the sample variance. Both the sample variance and the estimator of its variance are U-statistics. By the theory of U-statistic, the resulting estimator is unique. See Fuchs, Krautenbacher (2016) <doi:10.1080/15598608.2016.1158675> and the references therein for an overview of unbiased estimation of variances of U-statistics.  "
  },
  {
    "id": 2605,
    "package_name": "ConnectednessApproach",
    "title": "Connectedness Approach",
    "description": "The estimation of static and dynamic connectedness measures is created in a modular and user-friendly way. Besides, the time domain connectedness approaches, this package further allows to estimate the frequency connectedness approach, the joint spillover index and the extended joint connectedness approach. In addition, all connectedness frameworks can be based upon orthogonalized and generalized VAR, QVAR, LASSO VAR, Ridge VAR, Elastic Net VAR and TVP-VAR models. Furthermore, the package includes the conditional, decomposed and partial connectedness measures as well as the pairwise connectedness index, influence index and corrected total connectedness index. Finally, a battery of datasets are available allowing to replicate a variety of connectedness papers.",
    "version": "1.0.4",
    "maintainer": "David Gabauer <david.gabauer@hotmail.com>",
    "author": "David Gabauer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConnectednessApproach",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConnectednessApproach Connectedness Approach The estimation of static and dynamic connectedness measures is created in a modular and user-friendly way. Besides, the time domain connectedness approaches, this package further allows to estimate the frequency connectedness approach, the joint spillover index and the extended joint connectedness approach. In addition, all connectedness frameworks can be based upon orthogonalized and generalized VAR, QVAR, LASSO VAR, Ridge VAR, Elastic Net VAR and TVP-VAR models. Furthermore, the package includes the conditional, decomposed and partial connectedness measures as well as the pairwise connectedness index, influence index and corrected total connectedness index. Finally, a battery of datasets are available allowing to replicate a variety of connectedness papers.  "
  },
  {
    "id": 2622,
    "package_name": "CoopGame",
    "title": "Important Concepts of Cooperative Game Theory",
    "description": "The theory of cooperative games with transferable utility offers \n   useful insights into the way parties can share gains from cooperation and \n   secure sustainable agreements, see e.g. one of the books by Chakravarty, \n   Mitra and Sarkar (2015, ISBN:978-1107058798) or by Driessen (1988, \n   ISBN:978-9027727299) for more details. A comprehensive set of tools for \n   cooperative game theory with transferable utility is provided. Users can \n   create special families of cooperative games, like e.g. bankruptcy games, \n   cost sharing games and weighted voting games. There are functions to check \n   various game properties and to compute five different set-valued solution \n   concepts for cooperative games. A large number of point-valued solution \n   concepts is available reflecting the diverse application areas of \n   cooperative game theory. Some of these point-valued solution concepts can \n   be used to analyze weighted voting games and measure the influence of \n   individual voters within a voting body. There are routines for visualizing \n   both set-valued and point-valued solutions in the case of three or four \n   players. ",
    "version": "0.2.2",
    "maintainer": "Jochen Staudacher <jochen.staudacher@hs-kempten.de>",
    "author": "Jochen Staudacher [aut, cre, cph],\n  Johannes Anwander [aut, cph],\n  Alexandra Tiukkel [aut, cph],\n  Michael Maerz [aut, cph],\n  Franz Mueller [aut, cph],\n  Daniel Gebele [aut, cph],\n  Anna Merkle [aut, cph],\n  Fatma Tokay [aut, cph],\n  Kuebra Tokay [aut, cph],\n  Nicole Cyl [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CoopGame",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CoopGame Important Concepts of Cooperative Game Theory The theory of cooperative games with transferable utility offers \n   useful insights into the way parties can share gains from cooperation and \n   secure sustainable agreements, see e.g. one of the books by Chakravarty, \n   Mitra and Sarkar (2015, ISBN:978-1107058798) or by Driessen (1988, \n   ISBN:978-9027727299) for more details. A comprehensive set of tools for \n   cooperative game theory with transferable utility is provided. Users can \n   create special families of cooperative games, like e.g. bankruptcy games, \n   cost sharing games and weighted voting games. There are functions to check \n   various game properties and to compute five different set-valued solution \n   concepts for cooperative games. A large number of point-valued solution \n   concepts is available reflecting the diverse application areas of \n   cooperative game theory. Some of these point-valued solution concepts can \n   be used to analyze weighted voting games and measure the influence of \n   individual voters within a voting body. There are routines for visualizing \n   both set-valued and point-valued solutions in the case of three or four \n   players.   "
  },
  {
    "id": 2648,
    "package_name": "CovSel",
    "title": "Model-Free Covariate Selection",
    "description": "Model-free selection of covariates under unconfoundedness for situations where the parameter of interest is an average causal effect. This package is based on  model-free backward elimination algorithms proposed in de Luna, Waernbaum and Richardson (2011). Marginal co-ordinate hypothesis testing is used in situations where all covariates are continuous while kernel-based smoothing appropriate for mixed data is used otherwise.",
    "version": "1.2.2",
    "maintainer": "Jenny H\u00e4ggstr\u00f6m <jenny.haggstrom@umu.se>",
    "author": "Jenny H\u00e4ggstr\u00f6m [aut, cre],\n  Emma Persson [aut],\n  Sandy Weisberg [aut] (Author of functions originating from the package\n    dr version 3.0.10.)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CovSel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CovSel Model-Free Covariate Selection Model-free selection of covariates under unconfoundedness for situations where the parameter of interest is an average causal effect. This package is based on  model-free backward elimination algorithms proposed in de Luna, Waernbaum and Richardson (2011). Marginal co-ordinate hypothesis testing is used in situations where all covariates are continuous while kernel-based smoothing appropriate for mixed data is used otherwise.  "
  },
  {
    "id": 2663,
    "package_name": "Cronbach",
    "title": "Cronbach's Alpha",
    "description": "Cronbach's alpha and various formulas for confidence intervals. The relevant paper is Tsagris M., Frangos C.C. and Frangos C.C. (2013). \"Confidence intervals for Cronbach's reliability coefficient\". Recent Techniques in Educational Science, 14-16 May, Athens, Greece. ",
    "version": "0.3",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Constantinos Frangos [aut],\n  Christos Frangos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Cronbach",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Cronbach Cronbach's Alpha Cronbach's alpha and various formulas for confidence intervals. The relevant paper is Tsagris M., Frangos C.C. and Frangos C.C. (2013). \"Confidence intervals for Cronbach's reliability coefficient\". Recent Techniques in Educational Science, 14-16 May, Athens, Greece.   "
  },
  {
    "id": 2731,
    "package_name": "DClusterm",
    "title": "Model-Based Detection of Disease Clusters",
    "description": "Model-based methods for the detection of disease clusters\n  using GLMs, GLMMs and zero-inflated models. These methods are described\n  in 'V. G\u00f3mez-Rubio et al.' (2019) <doi:10.18637/jss.v090.i14> and\n  'V. G\u00f3mez-Rubio et al.' (2018) <doi:10.1007/978-3-030-01584-8_1>.",
    "version": "1.0-2",
    "maintainer": "Virgilio Gomez-Rubio <virgilio.gomez@uclm.es>",
    "author": "Virgilio Gomez-Rubio [aut, cre],\n  Paula Esther Moraga Serrano [aut],\n  Barry Rowlingson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DClusterm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DClusterm Model-Based Detection of Disease Clusters Model-based methods for the detection of disease clusters\n  using GLMs, GLMMs and zero-inflated models. These methods are described\n  in 'V. G\u00f3mez-Rubio et al.' (2019) <doi:10.18637/jss.v090.i14> and\n  'V. G\u00f3mez-Rubio et al.' (2018) <doi:10.1007/978-3-030-01584-8_1>.  "
  },
  {
    "id": 2773,
    "package_name": "DHARMa",
    "title": "Residual Diagnostics for Hierarchical (Multi-Level / Mixed)\nRegression Models",
    "description": "The 'DHARMa' package uses a simulation-based approach to create\n    readily interpretable scaled (quantile) residuals for fitted (generalized) linear mixed\n    models. Currently supported are linear and generalized linear (mixed) models from 'lme4'\n    (classes 'lmerMod', 'glmerMod'), 'glmmTMB', 'GLMMadaptive', and 'spaMM'; phylogenetic \n    linear models from 'phylolm' (classes 'phylolm' and 'phyloglm'); generalized additive \n    models ('gam' from 'mgcv'); 'glm' (including 'negbin' from 'MASS', but excluding quasi-distributions) and\n    'lm' model classes. Moreover, externally created simulations, e.g. posterior predictive simulations\n    from Bayesian software such as 'JAGS', 'STAN', or 'BUGS' can be processed as well.\n    The resulting residuals are standardized to values between 0 and 1 and can be interpreted\n    as intuitively as residuals from a linear regression. The package also provides a number of\n    plot and test functions for typical model misspecification problems, such as\n    over/underdispersion, zero-inflation, and residual spatial, phylogenetic and temporal autocorrelation.",
    "version": "0.4.7",
    "maintainer": "Florian Hartig <florian.hartig@biologie.uni-regensburg.de>",
    "author": "Florian Hartig [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6255-9059>),\n  Lukas Lohse [ctb],\n  Melina de Souza leite [ctb]",
    "url": "http://florianhartig.github.io/DHARMa/",
    "bug_reports": "https://github.com/florianhartig/DHARMa/issues",
    "repository": "https://cran.r-project.org/package=DHARMa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DHARMa Residual Diagnostics for Hierarchical (Multi-Level / Mixed)\nRegression Models The 'DHARMa' package uses a simulation-based approach to create\n    readily interpretable scaled (quantile) residuals for fitted (generalized) linear mixed\n    models. Currently supported are linear and generalized linear (mixed) models from 'lme4'\n    (classes 'lmerMod', 'glmerMod'), 'glmmTMB', 'GLMMadaptive', and 'spaMM'; phylogenetic \n    linear models from 'phylolm' (classes 'phylolm' and 'phyloglm'); generalized additive \n    models ('gam' from 'mgcv'); 'glm' (including 'negbin' from 'MASS', but excluding quasi-distributions) and\n    'lm' model classes. Moreover, externally created simulations, e.g. posterior predictive simulations\n    from Bayesian software such as 'JAGS', 'STAN', or 'BUGS' can be processed as well.\n    The resulting residuals are standardized to values between 0 and 1 and can be interpreted\n    as intuitively as residuals from a linear regression. The package also provides a number of\n    plot and test functions for typical model misspecification problems, such as\n    over/underdispersion, zero-inflation, and residual spatial, phylogenetic and temporal autocorrelation.  "
  },
  {
    "id": 2777,
    "package_name": "DICEM",
    "title": "Directness and Intensity of Conflict Expression",
    "description": "A Natural Language Processing Model trained to detect directness and intensity during conflict. See <https://www.mikeyeomans.info>.",
    "version": "0.1.0",
    "maintainer": "Michael Yeomans <mk.yeomans@gmail.com>",
    "author": "Michael Yeomans [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DICEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DICEM Directness and Intensity of Conflict Expression A Natural Language Processing Model trained to detect directness and intensity during conflict. See <https://www.mikeyeomans.info>.  "
  },
  {
    "id": 2791,
    "package_name": "DIMORA",
    "title": "Diffusion Models R Analysis",
    "description": "The implemented methods are: Standard Bass model, Generalized Bass model (with rectangular shock, exponential shock, and mixed shock. You can choose to add from 1 to 3 shocks), Guseo-Guidolin model and Variable Potential Market model, and UCRCD model. The Bass model consists of a simple differential equation that describes the process of how new products get adopted in a population, the Generalized Bass model is a generalization of the Bass model in which there is a \"carrier\" function x(t) that allows to change the speed of time sliding. In some real processes the reachable potential of the resource available in a temporal instant may appear to be not constant over time, because of this we use Variable Potential Market model, in which the Guseo-Guidolin has a particular specification for the market function. The UCRCD model (Unbalanced Competition and Regime Change Diachronic) is a diffusion model used to capture the dynamics of the competitive or collaborative transition.",
    "version": "0.3.6",
    "maintainer": "Savio Andrea <svandr97@gmail.com>",
    "author": "Zanghi Federico, Savio Andrea, Filippo Ziliotto, Bessi Alessandro",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DIMORA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DIMORA Diffusion Models R Analysis The implemented methods are: Standard Bass model, Generalized Bass model (with rectangular shock, exponential shock, and mixed shock. You can choose to add from 1 to 3 shocks), Guseo-Guidolin model and Variable Potential Market model, and UCRCD model. The Bass model consists of a simple differential equation that describes the process of how new products get adopted in a population, the Generalized Bass model is a generalization of the Bass model in which there is a \"carrier\" function x(t) that allows to change the speed of time sliding. In some real processes the reachable potential of the resource available in a temporal instant may appear to be not constant over time, because of this we use Variable Potential Market model, in which the Guseo-Guidolin has a particular specification for the market function. The UCRCD model (Unbalanced Competition and Regime Change Diachronic) is a diffusion model used to capture the dynamics of the competitive or collaborative transition.  "
  },
  {
    "id": 2810,
    "package_name": "DMCfun",
    "title": "Diffusion Model of Conflict (DMC) in Reaction Time Tasks",
    "description": "\n  DMC model simulation detailed in Ulrich, R., Schroeter, H., Leuthold, H., & Birngruber, T. (2015).\n  Automatic and controlled stimulus processing in conflict tasks: Superimposed diffusion processes and delta functions.\n  Cognitive Psychology, 78, 148-174. Ulrich et al. (2015) <doi:10.1016/j.cogpsych.2015.02.005>.\n  Decision processes within choice reaction-time (CRT) tasks are often modelled using evidence accumulation models (EAMs),\n  a variation of which is the Diffusion Decision Model (DDM, for a review, see Ratcliff & McKoon, 2008).\n  Ulrich et al. (2015) introduced a Diffusion Model for Conflict tasks (DMC). The DMC model combines common\n  features from within standard diffusion models with the addition of superimposed controlled and automatic activation.\n  The DMC model is used to explain distributional reaction time (and error rate) patterns in common behavioural\n  conflict-like tasks (e.g., Flanker task, Simon task). This R-package implements the DMC model and provides functionality\n  to fit the model to observed data. Further details are provided in the following paper: \n  Mackenzie, I.G., & Dudschig, C. (2021). DMCfun: An R package for fitting Diffusion Model of Conflict (DMC) to reaction \n  time and error rate data. Methods in Psychology, 100074. <doi:10.1016/j.metip.2021.100074>.",
    "version": "4.0.1",
    "maintainer": "Ian G. Mackenzie <ian.mackenzie@uni-tuebingen.de>",
    "author": "Ian G. Mackenzie [cre, aut],\n  Carolin Dudschig [aut]",
    "url": "https://github.com/igmmgi/DMCfun,\nhttps://CRAN.R-project.org/package=DMCfun,\nhttps://www.sciencedirect.com/science/article/pii/S259026012100031X",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DMCfun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DMCfun Diffusion Model of Conflict (DMC) in Reaction Time Tasks \n  DMC model simulation detailed in Ulrich, R., Schroeter, H., Leuthold, H., & Birngruber, T. (2015).\n  Automatic and controlled stimulus processing in conflict tasks: Superimposed diffusion processes and delta functions.\n  Cognitive Psychology, 78, 148-174. Ulrich et al. (2015) <doi:10.1016/j.cogpsych.2015.02.005>.\n  Decision processes within choice reaction-time (CRT) tasks are often modelled using evidence accumulation models (EAMs),\n  a variation of which is the Diffusion Decision Model (DDM, for a review, see Ratcliff & McKoon, 2008).\n  Ulrich et al. (2015) introduced a Diffusion Model for Conflict tasks (DMC). The DMC model combines common\n  features from within standard diffusion models with the addition of superimposed controlled and automatic activation.\n  The DMC model is used to explain distributional reaction time (and error rate) patterns in common behavioural\n  conflict-like tasks (e.g., Flanker task, Simon task). This R-package implements the DMC model and provides functionality\n  to fit the model to observed data. Further details are provided in the following paper: \n  Mackenzie, I.G., & Dudschig, C. (2021). DMCfun: An R package for fitting Diffusion Model of Conflict (DMC) to reaction \n  time and error rate data. Methods in Psychology, 100074. <doi:10.1016/j.metip.2021.100074>.  "
  },
  {
    "id": 2825,
    "package_name": "DOS",
    "title": "Design of Observational Studies",
    "description": "Contains data sets, examples and software from the book Design of Observational Studies by Paul R. Rosenbaum, New York: Springer, <doi:10.1007/978-1-4419-1213-8>, ISBN 978-1-4419-1212-1.",
    "version": "1.0.0",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DOS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DOS Design of Observational Studies Contains data sets, examples and software from the book Design of Observational Studies by Paul R. Rosenbaum, New York: Springer, <doi:10.1007/978-1-4419-1213-8>, ISBN 978-1-4419-1212-1.  "
  },
  {
    "id": 2826,
    "package_name": "DOS2",
    "title": "Design of Observational Studies, Companion to the Second Edition",
    "description": "Contains data sets, examples and software from the Second Edition of \"Design of Observational Studies\"; see Rosenbaum, P.R. (2010)  <doi:10.1007/978-1-4419-1213-8>.",
    "version": "0.5.2",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DOS2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DOS2 Design of Observational Studies, Companion to the Second Edition Contains data sets, examples and software from the Second Edition of \"Design of Observational Studies\"; see Rosenbaum, P.R. (2010)  <doi:10.1007/978-1-4419-1213-8>.  "
  },
  {
    "id": 2832,
    "package_name": "DPI",
    "title": "The Directed Prediction Index for Causal Direction Inference\nfrom Observational Data",
    "description": "\n    The Directed Prediction Index ('DPI') is\n    a quasi-causal inference (causal discovery) method for observational data\n    designed to quantify the relative endogeneity (relative dependence)\n    of outcome (Y) versus predictor (X) variables in regression models.\n    By comparing the proportion of variance explained (R-squared)\n    between the Y-as-outcome model and the X-as-outcome model\n    while controlling for a sufficient number of possible confounders,\n    it can suggest a plausible (admissible) direction of influence\n    from a less endogenous variable (X) to a more endogenous variable (Y).\n    Methodological details are provided at\n    <https://psychbruce.github.io/DPI/>.\n    This package also includes functions for data simulation and network\n    analysis (correlation, partial correlation, and Bayesian networks).",
    "version": "2025.11",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/DPI/",
    "bug_reports": "https://github.com/psychbruce/DPI/issues",
    "repository": "https://cran.r-project.org/package=DPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DPI The Directed Prediction Index for Causal Direction Inference\nfrom Observational Data \n    The Directed Prediction Index ('DPI') is\n    a quasi-causal inference (causal discovery) method for observational data\n    designed to quantify the relative endogeneity (relative dependence)\n    of outcome (Y) versus predictor (X) variables in regression models.\n    By comparing the proportion of variance explained (R-squared)\n    between the Y-as-outcome model and the X-as-outcome model\n    while controlling for a sufficient number of possible confounders,\n    it can suggest a plausible (admissible) direction of influence\n    from a less endogenous variable (X) to a more endogenous variable (Y).\n    Methodological details are provided at\n    <https://psychbruce.github.io/DPI/>.\n    This package also includes functions for data simulation and network\n    analysis (correlation, partial correlation, and Bayesian networks).  "
  },
  {
    "id": 2851,
    "package_name": "DRviaSPCN",
    "title": "Drug Repurposing in Cancer via a Subpathway Crosstalk Network",
    "description": "A systematic biology tool was developed to repurpose drugs via a subpathway crosstalk network. The operation modes include 1) calculating centrality scores of SPs in the context of gene expression data to reflect the influence of SP crosstalk, 2) evaluating drug-disease reverse association based on disease- and drug-induced SPs weighted by the SP crosstalk, 3) identifying cancer candidate drugs through perturbation analysis. There are also several functions used to visualize the results.",
    "version": "0.1.5",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [aut, cre, cph],\n  Jiashuo Wu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DRviaSPCN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DRviaSPCN Drug Repurposing in Cancer via a Subpathway Crosstalk Network A systematic biology tool was developed to repurpose drugs via a subpathway crosstalk network. The operation modes include 1) calculating centrality scores of SPs in the context of gene expression data to reflect the influence of SP crosstalk, 2) evaluating drug-disease reverse association based on disease- and drug-induced SPs weighted by the SP crosstalk, 3) identifying cancer candidate drugs through perturbation analysis. There are also several functions used to visualize the results.  "
  },
  {
    "id": 2910,
    "package_name": "DataSetsUni",
    "title": "A Collection of Univariate Data Sets",
    "description": "A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.     ",
    "version": "0.1",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  M.H Tahir [ctb],\n  Farrukh Jamal [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DataSetsUni",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataSetsUni A Collection of Univariate Data Sets A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.       "
  },
  {
    "id": 2918,
    "package_name": "DatabionicSwarm",
    "title": "Swarm Intelligence for Self-Organized Clustering",
    "description": "Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: \"Swarm Intelligence for Self-Organized Clustering\" (2020), Artificial Intelligence, <DOI:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: \"Projection Based Clustering through Self-Organization and Swarm Intelligence\" (2018) <DOI:10.1007/978-3-658-20540-9>.",
    "version": "2.0.0",
    "maintainer": "Michael Thrun <m.thrun@gmx.net>",
    "author": "Michael Thrun [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9542-5543>),\n  Quirin Stier [aut, rev] (ORCID:\n    <https://orcid.org/0000-0002-7896-4737>)",
    "url": "https://www.deepbionics.org/",
    "bug_reports": "https://github.com/Mthrun/DatabionicSwarm/issues",
    "repository": "https://cran.r-project.org/package=DatabionicSwarm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatabionicSwarm Swarm Intelligence for Self-Organized Clustering Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: \"Swarm Intelligence for Self-Organized Clustering\" (2020), Artificial Intelligence, <DOI:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: \"Projection Based Clustering through Self-Organization and Swarm Intelligence\" (2018) <DOI:10.1007/978-3-658-20540-9>.  "
  },
  {
    "id": 2965,
    "package_name": "DiPs",
    "title": "Directional Penalties for Optimal Matching in Observational\nStudies",
    "description": "Improves the balance of optimal matching with near-fine balance by giving penalties on the unbalanced covariates with the unbalanced directions. Many directional penalties can also be viewed as Lagrange multipliers, pushing a matched sample in the direction of satisfying a linear constraint that would not be satisfied without penalization.\n    Yu and Rosenbaum (2019) <doi:10.1111/biom.13098>. ",
    "version": "0.6.4",
    "maintainer": "Ruoqi Yu <ruoqiyu125@gmail.com>",
    "author": "Ruoqi Yu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiPs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiPs Directional Penalties for Optimal Matching in Observational\nStudies Improves the balance of optimal matching with near-fine balance by giving penalties on the unbalanced covariates with the unbalanced directions. Many directional penalties can also be viewed as Lagrange multipliers, pushing a matched sample in the direction of satisfying a linear constraint that would not be satisfied without penalization.\n    Yu and Rosenbaum (2019) <doi:10.1111/biom.13098>.   "
  },
  {
    "id": 3064,
    "package_name": "EBMAforecast",
    "title": "Estimate Ensemble Bayesian Model Averaging Forecasts using Gibbs\nSampling or EM-Algorithms",
    "description": "Create forecasts from multiple predictions using ensemble Bayesian model averaging (EBMA). EBMA models can be estimated using an expectation maximization (EM) algorithm or as fully Bayesian models via Gibbs sampling. The methods in this package are Montgomery, Hollenbach, and Ward (2015) <doi:10.1016/j.ijforecast.2014.08.001> and Montgomery, Hollenbach, and Ward (2012) <doi:10.1093/pan/mps002>.",
    "version": "1.0.32",
    "maintainer": "Florian M. Hollenbach <fho.egb@cbs.dk>",
    "author": "Florian M. Hollenbach [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9599-556X>),\n  Jacob M. Montgomery [aut],\n  Michael D. Ward [aut]",
    "url": "https://github.com/fhollenbach/EBMA/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EBMAforecast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EBMAforecast Estimate Ensemble Bayesian Model Averaging Forecasts using Gibbs\nSampling or EM-Algorithms Create forecasts from multiple predictions using ensemble Bayesian model averaging (EBMA). EBMA models can be estimated using an expectation maximization (EM) algorithm or as fully Bayesian models via Gibbs sampling. The methods in this package are Montgomery, Hollenbach, and Ward (2015) <doi:10.1016/j.ijforecast.2014.08.001> and Montgomery, Hollenbach, and Ward (2012) <doi:10.1093/pan/mps002>.  "
  },
  {
    "id": 3169,
    "package_name": "EUfootball",
    "title": "Football Match Data of European Leagues",
    "description": "Contains match results from seven European men's football leagues, namely Premier League (England), Ligue 1 (France), \n\t     Bundesliga (Germany), Serie A (Italy), Primera Division (Spain), Eredivisie (The Netherlands), Super Lig (Turkey).\n\t     Includes Seasons 2010/2011 until 2019/2020 and a set of interesting covariates. Can be used all purposes.",
    "version": "0.0.1",
    "maintainer": "Hendrik van der Wurp <vanderwurp@statistik.tu-dortmund.de>",
    "author": "Hendrik van der Wurp [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1044-417X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EUfootball",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EUfootball Football Match Data of European Leagues Contains match results from seven European men's football leagues, namely Premier League (England), Ligue 1 (France), \n\t     Bundesliga (Germany), Serie A (Italy), Primera Division (Spain), Eredivisie (The Netherlands), Super Lig (Turkey).\n\t     Includes Seasons 2010/2011 until 2019/2020 and a set of interesting covariates. Can be used all purposes.  "
  },
  {
    "id": 3215,
    "package_name": "EloChoice",
    "title": "Preference Rating for Visual Stimuli Based on Elo Ratings",
    "description": "Allows calculating global scores for characteristics of visual stimuli as assessed by human raters. Stimuli are presented as sequence of pairwise comparisons ('contests'), during each of which a rater expresses preference for one stimulus over the other (forced choice). The algorithm for calculating global scores is based on Elo rating, which updates individual scores after each single pairwise contest. Elo rating is widely used to rank chess players according to their performance. Its core feature is that dyadic contests with expected outcomes lead to smaller changes of participants' scores than outcomes that were unexpected. As such, Elo rating is an efficient tool to rate individual stimuli when a large number of such stimuli are paired against each other in the context of experiments where the goal is to rank stimuli according to some characteristic of interest. Clark et al (2018) <doi:10.1371/journal.pone.0190393> provide details.",
    "version": "0.29.4",
    "maintainer": "Christof Neumann <christofneumann1@gmail.com>",
    "author": "Christof Neumann",
    "url": "https://github.com/gobbios/EloChoice",
    "bug_reports": "https://github.com/gobbios/EloChoice/issues",
    "repository": "https://cran.r-project.org/package=EloChoice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EloChoice Preference Rating for Visual Stimuli Based on Elo Ratings Allows calculating global scores for characteristics of visual stimuli as assessed by human raters. Stimuli are presented as sequence of pairwise comparisons ('contests'), during each of which a rater expresses preference for one stimulus over the other (forced choice). The algorithm for calculating global scores is based on Elo rating, which updates individual scores after each single pairwise contest. Elo rating is widely used to rank chess players according to their performance. Its core feature is that dyadic contests with expected outcomes lead to smaller changes of participants' scores than outcomes that were unexpected. As such, Elo rating is an efficient tool to rate individual stimuli when a large number of such stimuli are paired against each other in the context of experiments where the goal is to rank stimuli according to some characteristic of interest. Clark et al (2018) <doi:10.1371/journal.pone.0190393> provide details.  "
  },
  {
    "id": 3254,
    "package_name": "EpistemicGameTheory",
    "title": "Constructing an Epistemic Model for the Games with Two Players",
    "description": "Constructing an epistemic model such that, for every player i and for every choice c(i) which is optimal, there is one type that expresses common belief in rationality.",
    "version": "0.1.2",
    "maintainer": "Bilge Baser <bilge.baser@msgsu.edu.tr>",
    "author": "Bilge Baser",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpistemicGameTheory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpistemicGameTheory Constructing an Epistemic Model for the Games with Two Players Constructing an epistemic model such that, for every player i and for every choice c(i) which is optimal, there is one type that expresses common belief in rationality.  "
  },
  {
    "id": 3282,
    "package_name": "EvolutionaryGames",
    "title": "Important Concepts of Evolutionary Game Theory",
    "description": "Evolutionary game theory applies game theory to evolving populations \n    in biology, see e.g. one of the books by Weibull (1994, ISBN:978-0262731218) \n    or by Sandholm (2010, ISBN:978-0262195874) for more details. A comprehensive \n\tset of tools to illustrate the core concepts of evolutionary game theory, \n\tsuch as evolutionary stability or various evolutionary dynamics, for teaching \n\tand academic research is provided.",
    "version": "0.1.2",
    "maintainer": "Jochen Staudacher <jochen.staudacher@hs-kempten.de>",
    "author": "Daniel Gebele [aut, cph],\n  Jochen Staudacher [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EvolutionaryGames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EvolutionaryGames Important Concepts of Evolutionary Game Theory Evolutionary game theory applies game theory to evolving populations \n    in biology, see e.g. one of the books by Weibull (1994, ISBN:978-0262731218) \n    or by Sandholm (2010, ISBN:978-0262195874) for more details. A comprehensive \n\tset of tools to illustrate the core concepts of evolutionary game theory, \n\tsuch as evolutionary stability or various evolutionary dynamics, for teaching \n\tand academic research is provided.  "
  },
  {
    "id": 3310,
    "package_name": "ExposR",
    "title": "Models Topographic Exposure to Hurricane Winds",
    "description": "The EXPOS model uses a digital elevation model (DEM) to estimate\n    exposed and protected areas for a given hurricane wind direction and\n    inflection angle. The resulting topographic exposure maps can be combined\n    with output from the HURRECON model to estimate hurricane wind damage\n    across a region. For details on the original version of the EXPOS model\n    written in 'Borland Pascal', see: Boose, Foster, and Fluet (1994)\n    <doi:10.2307/2937142>, Boose, Chamberlin, and Foster (2001)\n    <doi:10.1890/0012-9615(2001)071[0027:LARIOH]2.0.CO;2>, and Boose,\n    Serrano, and Foster (2004) <doi:10.1890/02-4057>.",
    "version": "1.2",
    "maintainer": "Emery Boose <boose@fas.harvard.edu>",
    "author": "Emery Boose [aut, cre],\n  President and Fellows of Harvard College [cph]",
    "url": "https://github.com/expos-model/ExposR",
    "bug_reports": "https://github.com/expos-model/ExposR/issues",
    "repository": "https://cran.r-project.org/package=ExposR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExposR Models Topographic Exposure to Hurricane Winds The EXPOS model uses a digital elevation model (DEM) to estimate\n    exposed and protected areas for a given hurricane wind direction and\n    inflection angle. The resulting topographic exposure maps can be combined\n    with output from the HURRECON model to estimate hurricane wind damage\n    across a region. For details on the original version of the EXPOS model\n    written in 'Borland Pascal', see: Boose, Foster, and Fluet (1994)\n    <doi:10.2307/2937142>, Boose, Chamberlin, and Foster (2001)\n    <doi:10.1890/0012-9615(2001)071[0027:LARIOH]2.0.CO;2>, and Boose,\n    Serrano, and Foster (2004) <doi:10.1890/02-4057>.  "
  },
  {
    "id": 3354,
    "package_name": "FDboost",
    "title": "Boosting Functional Regression Models",
    "description": "Regression models for functional data, i.e.,\n    scalar-on-function, function-on-scalar and function-on-function\n    regression models, are fitted by a component-wise gradient boosting\n    algorithm.  For a manual on how to use 'FDboost', see Brockhaus,\n    Ruegamer, Greven (2017) <doi:10.18637/jss.v094.i10>.",
    "version": "1.1-3",
    "maintainer": "David Ruegamer <david.ruegamer@gmail.com>",
    "author": "Sarah Brockhaus [aut] (ORCID: <https://orcid.org/0000-0001-9484-7488>),\n  David Ruegamer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8772-9202>),\n  Almond Stoecker [aut] (ORCID: <https://orcid.org/0000-0001-9160-2397>),\n  Torsten Hothorn [ctb] (ORCID: <https://orcid.org/0000-0001-8301-0471>),\n  with contributions by many others (see inst/CONTRIBUTIONS) [ctb]",
    "url": "https://github.com/boost-R/FDboost",
    "bug_reports": "https://github.com/boost-R/FDboost/issues",
    "repository": "https://cran.r-project.org/package=FDboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FDboost Boosting Functional Regression Models Regression models for functional data, i.e.,\n    scalar-on-function, function-on-scalar and function-on-function\n    regression models, are fitted by a component-wise gradient boosting\n    algorithm.  For a manual on how to use 'FDboost', see Brockhaus,\n    Ruegamer, Greven (2017) <doi:10.18637/jss.v094.i10>.  "
  },
  {
    "id": 3398,
    "package_name": "FPLdata",
    "title": "Read in Fantasy Premier League Data",
    "description": "This data contains a large variety of information on players and their\n  current attributes on Fantasy Premier League\n  <https://fantasy.premierleague.com/>. In particular, it contains a\n  `next_gw_points` (next gameweek points) value for each player\n  given their attributes in the current week. Rows represent player-gameweeks,\n  i.e. for each player there is a row for each gameweek. This\n  makes the data suitable for modelling a player's next gameweek points, given\n  attributes such as form, total points, and cost at the current gameweek.\n  This data can therefore be used to create Fantasy Premier League bots that\n  may use a machine learning algorithm and a linear programming solver\n  (for example) to return the best possible transfers and team to pick for\n  each gameweek, thereby fully automating the decision making process in\n  Fantasy Premier League. This function simply supplies the required data\n  for such a task.",
    "version": "0.1.0",
    "maintainer": "Andrew Little <andrewlittlebristol@gmail.com>",
    "author": "Andrew Little [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FPLdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FPLdata Read in Fantasy Premier League Data This data contains a large variety of information on players and their\n  current attributes on Fantasy Premier League\n  <https://fantasy.premierleague.com/>. In particular, it contains a\n  `next_gw_points` (next gameweek points) value for each player\n  given their attributes in the current week. Rows represent player-gameweeks,\n  i.e. for each player there is a row for each gameweek. This\n  makes the data suitable for modelling a player's next gameweek points, given\n  attributes such as form, total points, and cost at the current gameweek.\n  This data can therefore be used to create Fantasy Premier League bots that\n  may use a machine learning algorithm and a linear programming solver\n  (for example) to return the best possible transfers and team to pick for\n  each gameweek, thereby fully automating the decision making process in\n  Fantasy Premier League. This function simply supplies the required data\n  for such a task.  "
  },
  {
    "id": 3497,
    "package_name": "FlexReg",
    "title": "Regression Models for Bounded Continuous and Discrete Responses",
    "description": "Functions to fit regression models for bounded continuous and discrete responses. In case of bounded continuous  responses (e.g., proportions and rates), available models are the flexible beta (Migliorati, S., Di Brisco, A. M., Ongaro, A. (2018) <doi:10.1214/17-BA1079>), the variance-inflated beta (Di Brisco, A. M., Migliorati, S., Ongaro, A. (2020) <doi:10.1177/1471082X18821213>), the beta (Ferrari, S.L.P., Cribari-Neto, F. (2004) <doi:10.1080/0266476042000214501>), and their augmented versions to handle the presence of zero/one values (Di Brisco, A. M., Migliorati, S. (2020) <doi:10.1002/sim.8406>) are implemented. In case of bounded discrete responses (e.g., bounded counts, such as the number of successes in n trials),  available models are the flexible beta-binomial (Ascari, R., Migliorati, S. (2021) <doi:10.1002/sim.9005>), the beta-binomial, and the binomial are implemented. Inference is dealt with a Bayesian approach based on the Hamiltonian Monte Carlo (HMC) algorithm (Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B. (2014) <doi:10.1201/b16018>). Besides, functions to compute residuals, posterior predictives, goodness of fit measures, convergence diagnostics, and graphical representations are provided.",
    "version": "1.4.1",
    "maintainer": "Roberto Ascari <roberto.ascari@unimib.it>",
    "author": "Roberto Ascari [aut, cre],\n  Agnese M. Di Brisco [aut],\n  Sonia Migliorati [aut],\n  Andrea Ongaro [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FlexReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FlexReg Regression Models for Bounded Continuous and Discrete Responses Functions to fit regression models for bounded continuous and discrete responses. In case of bounded continuous  responses (e.g., proportions and rates), available models are the flexible beta (Migliorati, S., Di Brisco, A. M., Ongaro, A. (2018) <doi:10.1214/17-BA1079>), the variance-inflated beta (Di Brisco, A. M., Migliorati, S., Ongaro, A. (2020) <doi:10.1177/1471082X18821213>), the beta (Ferrari, S.L.P., Cribari-Neto, F. (2004) <doi:10.1080/0266476042000214501>), and their augmented versions to handle the presence of zero/one values (Di Brisco, A. M., Migliorati, S. (2020) <doi:10.1002/sim.8406>) are implemented. In case of bounded discrete responses (e.g., bounded counts, such as the number of successes in n trials),  available models are the flexible beta-binomial (Ascari, R., Migliorati, S. (2021) <doi:10.1002/sim.9005>), the beta-binomial, and the binomial are implemented. Inference is dealt with a Bayesian approach based on the Hamiltonian Monte Carlo (HMC) algorithm (Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B. (2014) <doi:10.1201/b16018>). Besides, functions to compute residuals, posterior predictives, goodness of fit measures, convergence diagnostics, and graphical representations are provided.  "
  },
  {
    "id": 3520,
    "package_name": "ForestFit",
    "title": "Statistical Modelling for Plant Size Distributions",
    "description": "Developed for the following tasks. 1 ) Computing the probability density function,\n             cumulative distribution function, random generation, and estimating the parameters\n \t\t\t of the eleven mixture models. 2 ) Point estimation of the parameters of two - \n\t\t\t parameter Weibull distribution using twelve methods and three - parameter Weibull \n\t\t\t distribution using nine methods. 3 ) The Bayesian inference for the three - \n\t\t\t parameter Weibull distribution. 4 ) Estimating parameters of the three - parameter\n\t\t\t Birnbaum - Saunders, generalized exponential, and Weibull distributions fitted to\n\t\t\t grouped data using three methods including approximated maximum likelihood, \n\t\t\t expectation maximization, and maximum likelihood. 5 ) Estimating the parameters\n\t\t\t of the gamma, log-normal, and Weibull mixture models fitted to the grouped data\n\t\t\t through the EM algorithm, 6 ) Estimating parameters of the nonlinear height curve\n\t\t\t fitted to the height - diameter observation, 7 ) Estimating parameters, computing\n\t\t\t probability density function, cumulative distribution function, and generating\n\t\t\t realizations from gamma shape mixture model introduced by Venturini et al. (2008)\n\t\t\t <doi:10.1214/07-AOAS156> , 8 ) The Bayesian inference, computing probability\n\t\t\t density function, cumulative distribution function, and generating realizations\n\t\t\t from univariate and bivariate Johnson SB distribution, 9 ) Robust multiple linear\n\t\t\t regression analysis when error term follows skewed t distribution, 10 ) Estimating \n\t\t\t parameters of a given distribution fitted to grouped data using method of maximum\n\t\t\t likelihood, and 11 ) Estimating parameters of the Johnson SB distribution through \n\t\t\t the Bayesian, method of moment, conditional maximum likelihood, and two - percentile \n\t\t\t method.",
    "version": "2.4.3",
    "maintainer": "Mahdi Teimouri <teimouri@aut.ac.ir>",
    "author": "Mahdi Teimouri [aut, cre, cph, ctb] (ORCID:\n    <https://orcid.org/0000-0002-5371-9364>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ForestFit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForestFit Statistical Modelling for Plant Size Distributions Developed for the following tasks. 1 ) Computing the probability density function,\n             cumulative distribution function, random generation, and estimating the parameters\n \t\t\t of the eleven mixture models. 2 ) Point estimation of the parameters of two - \n\t\t\t parameter Weibull distribution using twelve methods and three - parameter Weibull \n\t\t\t distribution using nine methods. 3 ) The Bayesian inference for the three - \n\t\t\t parameter Weibull distribution. 4 ) Estimating parameters of the three - parameter\n\t\t\t Birnbaum - Saunders, generalized exponential, and Weibull distributions fitted to\n\t\t\t grouped data using three methods including approximated maximum likelihood, \n\t\t\t expectation maximization, and maximum likelihood. 5 ) Estimating the parameters\n\t\t\t of the gamma, log-normal, and Weibull mixture models fitted to the grouped data\n\t\t\t through the EM algorithm, 6 ) Estimating parameters of the nonlinear height curve\n\t\t\t fitted to the height - diameter observation, 7 ) Estimating parameters, computing\n\t\t\t probability density function, cumulative distribution function, and generating\n\t\t\t realizations from gamma shape mixture model introduced by Venturini et al. (2008)\n\t\t\t <doi:10.1214/07-AOAS156> , 8 ) The Bayesian inference, computing probability\n\t\t\t density function, cumulative distribution function, and generating realizations\n\t\t\t from univariate and bivariate Johnson SB distribution, 9 ) Robust multiple linear\n\t\t\t regression analysis when error term follows skewed t distribution, 10 ) Estimating \n\t\t\t parameters of a given distribution fitted to grouped data using method of maximum\n\t\t\t likelihood, and 11 ) Estimating parameters of the Johnson SB distribution through \n\t\t\t the Bayesian, method of moment, conditional maximum likelihood, and two - percentile \n\t\t\t method.  "
  },
  {
    "id": 3528,
    "package_name": "FourScores",
    "title": "A Game for Human vs. Human or Human vs. AI",
    "description": "A game for two players: Who gets first four in a row (horizontal, vertical or diagonal) wins. As board game published by Milton Bradley, designed by Howard Wexler and Ned Strongin.",
    "version": "1.5.1",
    "maintainer": "Matthias Speidel <matthias.speidel@googlemail.com>",
    "author": "Matthias Speidel [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FourScores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FourScores A Game for Human vs. Human or Human vs. AI A game for two players: Who gets first four in a row (horizontal, vertical or diagonal) wins. As board game published by Milton Bradley, designed by Howard Wexler and Ned Strongin.  "
  },
  {
    "id": 3530,
    "package_name": "FourgameteP",
    "title": "FourGamete Test",
    "description": "The four-gamete test is based on the infinite-sites model which assumes that the probability of the same mutation occurring twice (recurrent or parallel mutations) and the probability of a mutation back to the original state (reverse mutations) are close to zero. Without these types of mutations, the only explanation for observing the four dilocus genotypes (example below) is recombination (Hudson and Kaplan 1985, Genetics 111:147-164). Thus, the presence of all four gametes is also called phylogenetic incompatibility.",
    "version": "0.1.0",
    "maintainer": "Milton T Drott <mtd66@cornell.edu>",
    "author": "Milton T Drott",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FourgameteP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FourgameteP FourGamete Test The four-gamete test is based on the infinite-sites model which assumes that the probability of the same mutation occurring twice (recurrent or parallel mutations) and the probability of a mutation back to the original state (reverse mutations) are close to zero. Without these types of mutations, the only explanation for observing the four dilocus genotypes (example below) is recombination (Hudson and Kaplan 1985, Genetics 111:147-164). Thus, the presence of all four gametes is also called phylogenetic incompatibility.  "
  },
  {
    "id": 3564,
    "package_name": "FuzzySTs",
    "title": "Fuzzy Statistical Tools",
    "description": "The main goal of this package is to present various fuzzy statistical tools. It intends to provide an implementation of the theoretical and empirical approaches presented in the book entitled \"The signed distance measure in fuzzy statistical analysis. Some theoretical, empirical and programming advances\" <doi: 10.1007/978-3-030-76916-1>. For the theoretical approaches, see Berkachy R. and Donze L. (2019) <doi:10.1007/978-3-030-03368-2_1>. For the empirical approaches, see Berkachy R. and Donze L. (2016) <ISBN: 978-989-758-201-1>). Important (non-exhaustive) implementation highlights of this package are as follows: (1) a numerical procedure to estimate the fuzzy difference and the fuzzy square. (2) two numerical methods of fuzzification. (3) a function performing different possibilities of distances, including the signed distance and the generalized signed distance for instance with all its properties. (4) numerical estimations of fuzzy statistical measures such as the variance, the moment, etc. (5) two methods of estimation of the bootstrap distribution of the likelihood ratio in the fuzzy context. (6) an estimation of a fuzzy confidence interval by the likelihood ratio method. (7) testing fuzzy hypotheses and/or fuzzy data by fuzzy confidence intervals in the Kwakernaak - Kruse and Meyer sense. (8) a general method to estimate the fuzzy p-value with fuzzy hypotheses and/or fuzzy data. (9) a method of estimation of global and individual evaluations of linguistic questionnaires. (10) numerical estimations of multi-ways analysis of variance models in the fuzzy context. The unbalance in the considered designs are also foreseen. ",
    "version": "0.4",
    "maintainer": "Redina Berkachy <redina.berkachy@hefr.ch>",
    "author": "Redina Berkachy [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7491-0416>),\n  Laurent Donze [aut] (ORCID: <https://orcid.org/0000-0003-3522-4672>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FuzzySTs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FuzzySTs Fuzzy Statistical Tools The main goal of this package is to present various fuzzy statistical tools. It intends to provide an implementation of the theoretical and empirical approaches presented in the book entitled \"The signed distance measure in fuzzy statistical analysis. Some theoretical, empirical and programming advances\" <doi: 10.1007/978-3-030-76916-1>. For the theoretical approaches, see Berkachy R. and Donze L. (2019) <doi:10.1007/978-3-030-03368-2_1>. For the empirical approaches, see Berkachy R. and Donze L. (2016) <ISBN: 978-989-758-201-1>). Important (non-exhaustive) implementation highlights of this package are as follows: (1) a numerical procedure to estimate the fuzzy difference and the fuzzy square. (2) two numerical methods of fuzzification. (3) a function performing different possibilities of distances, including the signed distance and the generalized signed distance for instance with all its properties. (4) numerical estimations of fuzzy statistical measures such as the variance, the moment, etc. (5) two methods of estimation of the bootstrap distribution of the likelihood ratio in the fuzzy context. (6) an estimation of a fuzzy confidence interval by the likelihood ratio method. (7) testing fuzzy hypotheses and/or fuzzy data by fuzzy confidence intervals in the Kwakernaak - Kruse and Meyer sense. (8) a general method to estimate the fuzzy p-value with fuzzy hypotheses and/or fuzzy data. (9) a method of estimation of global and individual evaluations of linguistic questionnaires. (10) numerical estimations of multi-ways analysis of variance models in the fuzzy context. The unbalance in the considered designs are also foreseen.   "
  },
  {
    "id": 3579,
    "package_name": "GAMens",
    "title": "Applies GAMbag, GAMrsm and GAMens Ensemble Classifiers for\nBinary Classification",
    "description": "Implements the GAMbag, GAMrsm and GAMens ensemble\n    classifiers for binary classification (De Bock et al., 2010) <doi:10.1016/j.csda.2009.12.013>. The ensembles\n    implement Bagging (Breiman, 1996) <doi:10.1023/A:1010933404324>, the Random Subspace Method (Ho, 1998) <doi:10.1109/34.709601>\n    , or both, and use Hastie and Tibshirani's (1990, ISBN:978-0412343902) generalized additive models (GAMs)\n    as base classifiers. Once an ensemble classifier has been trained, it can\n    be used for predictions on new data. A function for cross validation is also\n    included.",
    "version": "1.2.1",
    "maintainer": "Koen W. De Bock <kdebock@audencia.com>",
    "author": "Koen W. De Bock, Kristof Coussement and Dirk Van den Poel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GAMens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GAMens Applies GAMbag, GAMrsm and GAMens Ensemble Classifiers for\nBinary Classification Implements the GAMbag, GAMrsm and GAMens ensemble\n    classifiers for binary classification (De Bock et al., 2010) <doi:10.1016/j.csda.2009.12.013>. The ensembles\n    implement Bagging (Breiman, 1996) <doi:10.1023/A:1010933404324>, the Random Subspace Method (Ho, 1998) <doi:10.1109/34.709601>\n    , or both, and use Hastie and Tibshirani's (1990, ISBN:978-0412343902) generalized additive models (GAMs)\n    as base classifiers. Once an ensemble classifier has been trained, it can\n    be used for predictions on new data. A function for cross validation is also\n    included.  "
  },
  {
    "id": 3689,
    "package_name": "GMSE",
    "title": "Generalised Management Strategy Evaluation Simulator",
    "description": "Integrates game theory and ecological theory to construct \n    social-ecological models that simulate the management of populations and \n    stakeholder actions. These models build off of a previously developed \n    management strategy evaluation (MSE) framework to simulate all aspects of \n    management: population dynamics, manager observation of populations, manager\n    decision making, and stakeholder responses to management decisions. The \n    newly developed generalised management strategy evaluation (GMSE) \n    framework uses genetic algorithms to mimic the decision-making process of \n    managers and stakeholders under conditions of change, uncertainty, and \n    conflict. Simulations can be run using gmse(), gmse_apply(), and\n    gmse_gui() functions.",
    "version": "1.0.0.2",
    "maintainer": "A. Bradley Duthie <brad.duthie@gmail.com>",
    "author": "A. Bradley Duthie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8343-4995>),\n  Adrian Bach [aut],\n  Jeremy Cusack [ctb] (ORCID: <https://orcid.org/0000-0003-3004-1586>),\n  Isabel Jones [ctb] (ORCID: <https://orcid.org/0000-0002-8361-1370>),\n  Jeroen Minderman [aut] (ORCID: <https://orcid.org/0000-0002-8451-5540>),\n  Erlend Nilsen [ctb] (ORCID: <https://orcid.org/0000-0002-5119-8331>),\n  Ochoa Gabriela [aut] (ORCID: <https://orcid.org/0000-0001-7649-5669>),\n  Rocio Pozo [ctb] (ORCID: <https://orcid.org/0000-0002-7546-8076>),\n  Sarobidy Rakotonarivo [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8032-1431>),\n  Bram Van Moorter [ctb] (ORCID: <https://orcid.org/0000-0002-3196-1993>),\n  Nils Bunnefeld [aut, fnd] (ORCID:\n    <https://orcid.org/0000-0002-1349-4463>)",
    "url": "https://confoobio.github.io/gmse/",
    "bug_reports": "https://github.com/confoobio/gmse/issues",
    "repository": "https://cran.r-project.org/package=GMSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GMSE Generalised Management Strategy Evaluation Simulator Integrates game theory and ecological theory to construct \n    social-ecological models that simulate the management of populations and \n    stakeholder actions. These models build off of a previously developed \n    management strategy evaluation (MSE) framework to simulate all aspects of \n    management: population dynamics, manager observation of populations, manager\n    decision making, and stakeholder responses to management decisions. The \n    newly developed generalised management strategy evaluation (GMSE) \n    framework uses genetic algorithms to mimic the decision-making process of \n    managers and stakeholders under conditions of change, uncertainty, and \n    conflict. Simulations can be run using gmse(), gmse_apply(), and\n    gmse_gui() functions.  "
  },
  {
    "id": 3702,
    "package_name": "GPBayes",
    "title": "Tools for Gaussian Process Modeling in Uncertainty\nQuantification",
    "description": "Gaussian processes ('GPs') have been widely used to model spatial data, 'spatio'-temporal data, and computer experiments in diverse areas of statistics including spatial statistics, 'spatio'-temporal statistics, uncertainty quantification, and machine learning. This package creates basic tools for fitting and prediction based on 'GPs' with spatial data, 'spatio'-temporal data, and computer experiments. Key characteristics for this GP tool include: (1) the comprehensive implementation of various covariance functions including the 'Mat\u00e9rn' family and the Confluent 'Hypergeometric' family with isotropic form, tensor form, and automatic relevance determination form, where the isotropic form is widely used in spatial statistics, the tensor form is widely used in design and analysis of computer experiments and uncertainty quantification, and the automatic relevance determination form is widely used in machine learning; (2) implementations via Markov chain Monte Carlo ('MCMC') algorithms and optimization algorithms for GP models with all the implemented covariance functions. The methods for fitting and prediction are mainly implemented in a Bayesian framework; (3) model evaluation via Fisher information and predictive metrics such as predictive scores; (4) built-in functionality for simulating 'GPs' with all the implemented covariance functions; (5) unified implementation to allow easy specification of various 'GPs'. ",
    "version": "0.1.0-6",
    "maintainer": "Pulong Ma <mpulong@gmail.com>",
    "author": "Pulong Ma [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/pulongma/GPBayes/issues",
    "repository": "https://cran.r-project.org/package=GPBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GPBayes Tools for Gaussian Process Modeling in Uncertainty\nQuantification Gaussian processes ('GPs') have been widely used to model spatial data, 'spatio'-temporal data, and computer experiments in diverse areas of statistics including spatial statistics, 'spatio'-temporal statistics, uncertainty quantification, and machine learning. This package creates basic tools for fitting and prediction based on 'GPs' with spatial data, 'spatio'-temporal data, and computer experiments. Key characteristics for this GP tool include: (1) the comprehensive implementation of various covariance functions including the 'Mat\u00e9rn' family and the Confluent 'Hypergeometric' family with isotropic form, tensor form, and automatic relevance determination form, where the isotropic form is widely used in spatial statistics, the tensor form is widely used in design and analysis of computer experiments and uncertainty quantification, and the automatic relevance determination form is widely used in machine learning; (2) implementations via Markov chain Monte Carlo ('MCMC') algorithms and optimization algorithms for GP models with all the implemented covariance functions. The methods for fitting and prediction are mainly implemented in a Bayesian framework; (3) model evaluation via Fisher information and predictive metrics such as predictive scores; (4) built-in functionality for simulating 'GPs' with all the implemented covariance functions; (5) unified implementation to allow easy specification of various 'GPs'.   "
  },
  {
    "id": 3708,
    "package_name": "GPGame",
    "title": "Solving Complex Game Problems using Gaussian Processes",
    "description": "Sequential strategies for finding a game equilibrium are proposed in a black-box setting (expensive pay-off evaluations, no derivatives). The algorithm handles noiseless or noisy evaluations. Two acquisition functions are available. Graphical outputs can be generated automatically. V. Picheny, M. Binois, A. Habbal (2018) <doi:10.1007/s10898-018-0688-0>. M. Binois, V. Picheny, P. Taillandier, A. Habbal (2020) <doi:10.48550/arXiv.1902.06565>.",
    "version": "1.2.1",
    "maintainer": "Mickael Binois <mickael.binois@inria.fr>",
    "author": "Victor Picheny [aut] (ORCID: <https://orcid.org/0000-0002-4948-5542>),\n  Mickael Binois [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7225-1680>)",
    "url": "https://github.com/vpicheny/GPGame",
    "bug_reports": "https://github.com/vpicheny/GPGame/issues",
    "repository": "https://cran.r-project.org/package=GPGame",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GPGame Solving Complex Game Problems using Gaussian Processes Sequential strategies for finding a game equilibrium are proposed in a black-box setting (expensive pay-off evaluations, no derivatives). The algorithm handles noiseless or noisy evaluations. Two acquisition functions are available. Graphical outputs can be generated automatically. V. Picheny, M. Binois, A. Habbal (2018) <doi:10.1007/s10898-018-0688-0>. M. Binois, V. Picheny, P. Taillandier, A. Habbal (2020) <doi:10.48550/arXiv.1902.06565>.  "
  },
  {
    "id": 3726,
    "package_name": "GRAB",
    "title": "Genome-Wide Robust Analysis for Biobank Data (GRAB)",
    "description": "Provides a comprehensive suite of genome-wide association study (GWAS) methods \n    specifically designed for biobank-scale data, including but not limited to, robust approaches \n    for time-to-event traits (Li et al., 2025 <doi:10.1038/s43588-025-00864-z>) and ordinal \n    categorical traits (Bi et al., 2021 <doi:10.1016/j.ajhg.2021.03.019>). The package also offers \n    general frameworks for GWAS of any trait type (Bi et al., 2020 <doi:10.1016/j.ajhg.2020.06.003>), \n    while accounting for sample relatedness (Xu et al., 2025 <doi:10.1038/s41467-025-56669-1>) or \n    population structure (Ma et al., 2025 <doi:10.1186/s13059-025-03827-9>). By accurately \n    approximating score statistic distributions using saddlepoint approximation (SPA), these \n    methods can effectively control type I error rates for rare variants and in the presence of \n    unbalanced phenotype distributions. Additionally, the package includes functions for simulating \n    genotype and phenotype data to support research and method development.",
    "version": "0.2.4",
    "maintainer": "Woody Miao <miaolin@pku.edu.cn>",
    "author": "Wenjian Bi [aut],\n  Wei Zhou [aut],\n  Rounak Dey [aut],\n  Zhangchen Zhao [aut],\n  Seunggeun Lee [aut],\n  Woody Miao [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GRAB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GRAB Genome-Wide Robust Analysis for Biobank Data (GRAB) Provides a comprehensive suite of genome-wide association study (GWAS) methods \n    specifically designed for biobank-scale data, including but not limited to, robust approaches \n    for time-to-event traits (Li et al., 2025 <doi:10.1038/s43588-025-00864-z>) and ordinal \n    categorical traits (Bi et al., 2021 <doi:10.1016/j.ajhg.2021.03.019>). The package also offers \n    general frameworks for GWAS of any trait type (Bi et al., 2020 <doi:10.1016/j.ajhg.2020.06.003>), \n    while accounting for sample relatedness (Xu et al., 2025 <doi:10.1038/s41467-025-56669-1>) or \n    population structure (Ma et al., 2025 <doi:10.1186/s13059-025-03827-9>). By accurately \n    approximating score statistic distributions using saddlepoint approximation (SPA), these \n    methods can effectively control type I error rates for rare variants and in the presence of \n    unbalanced phenotype distributions. Additionally, the package includes functions for simulating \n    genotype and phenotype data to support research and method development.  "
  },
  {
    "id": 3760,
    "package_name": "GTbasedIM",
    "title": "Game Theory-Based Influence Measures",
    "description": "Understanding how features influence a specific response variable becomes crucial in classification problems, with applications ranging from medical diagnosis to customer behavior analysis. \n\t\t\t This packages provides tools to compute such an influence measure grounded on game theory concepts.\n\t\t\t In particular, the influence measures presented in Davila-Pena, Saavedra-Nieves, and Casas-M\u00e9ndez (2024) <doi:10.48550/arXiv.2408.02481> can be obtained. ",
    "version": "1.0.0",
    "maintainer": "Laura Davila-Pena <lauradavila.pena@usc.es>",
    "author": "Laura Davila-Pena [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2175-2546>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GTbasedIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GTbasedIM Game Theory-Based Influence Measures Understanding how features influence a specific response variable becomes crucial in classification problems, with applications ranging from medical diagnosis to customer behavior analysis. \n\t\t\t This packages provides tools to compute such an influence measure grounded on game theory concepts.\n\t\t\t In particular, the influence measures presented in Davila-Pena, Saavedra-Nieves, and Casas-M\u00e9ndez (2024) <doi:10.48550/arXiv.2408.02481> can be obtained.   "
  },
  {
    "id": 3765,
    "package_name": "GUniFrac",
    "title": "Generalized UniFrac Distances, Distance-Based Multivariate\nMethods and Feature-Based Univariate Methods for Microbiome\nData Analysis",
    "description": "A suite of methods for powerful and robust microbiome data analysis including data normalization, data simulation, community-level association testing and differential abundance analysis. It implements generalized UniFrac distances,  Geometric Mean of Pairwise Ratios (GMPR) normalization, semiparametric data simulator, distance-based statistical methods, and feature-based statistical methods. The distance-based statistical methods include three extensions of PERMANOVA: (1) PERMANOVA using the Freedman-Lane permutation scheme, (2) PERMANOVA omnibus test using multiple matrices, and  (3) analytical approach to approximating PERMANOVA p-value. Feature-based statistical methods include linear model-based methods for differential abundance analysis of zero-inflated high-dimensional compositional data. ",
    "version": "1.9",
    "maintainer": "Jun Chen <chen.jun2@mayo.edu>",
    "author": "Jun Chen [aut, cre],\n  Xianyang Zhang [aut],\n  Lu Yang [aut],\n  Lujun Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GUniFrac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GUniFrac Generalized UniFrac Distances, Distance-Based Multivariate\nMethods and Feature-Based Univariate Methods for Microbiome\nData Analysis A suite of methods for powerful and robust microbiome data analysis including data normalization, data simulation, community-level association testing and differential abundance analysis. It implements generalized UniFrac distances,  Geometric Mean of Pairwise Ratios (GMPR) normalization, semiparametric data simulator, distance-based statistical methods, and feature-based statistical methods. The distance-based statistical methods include three extensions of PERMANOVA: (1) PERMANOVA using the Freedman-Lane permutation scheme, (2) PERMANOVA omnibus test using multiple matrices, and  (3) analytical approach to approximating PERMANOVA p-value. Feature-based statistical methods include linear model-based methods for differential abundance analysis of zero-inflated high-dimensional compositional data.   "
  },
  {
    "id": 3786,
    "package_name": "GameTheory",
    "title": "Cooperative Game Theory",
    "description": "Implementation of a common set of punctual solutions  for Cooperative Game Theory.",
    "version": "2.7.1",
    "maintainer": "Sebastian Cano-Berlanga <cano.berlanga@gmail.com>",
    "author": "Sebastian Cano-Berlanga",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GameTheory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GameTheory Cooperative Game Theory Implementation of a common set of punctual solutions  for Cooperative Game Theory.  "
  },
  {
    "id": 3797,
    "package_name": "GenHMM1d",
    "title": "Goodness-of-Fit for Zero-Inflated Univariate Hidden Markov\nModels",
    "description": "Inference, goodness-of-fit tests, and predictions for continuous and discrete univariate  Hidden Markov Models (HMM), including zero-inflated distributions. The goodness-of-fit test is based on a Cramer-von Mises statistic and uses parametric bootstrap to estimate the p-value. The description of the methodology is taken from Nasri et al (2020) <doi:10.1029/2019WR025122>.",
    "version": "0.2.6",
    "maintainer": "Bouchra R. Nasri <bouchra.nasri@umontreal.ca>",
    "author": "Bouchra R. Nasri [aut, cre, cph],\n  Mamadou Yamar Thioub [aut, cph],\n  Bruno N. Remillard [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GenHMM1d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GenHMM1d Goodness-of-Fit for Zero-Inflated Univariate Hidden Markov\nModels Inference, goodness-of-fit tests, and predictions for continuous and discrete univariate  Hidden Markov Models (HMM), including zero-inflated distributions. The goodness-of-fit test is based on a Cramer-von Mises statistic and uses parametric bootstrap to estimate the p-value. The description of the methodology is taken from Nasri et al (2020) <doi:10.1029/2019WR025122>.  "
  },
  {
    "id": 3804,
    "package_name": "GenWin",
    "title": "Spline Based Window Boundaries for Genomic Analyses",
    "description": "Defines window or bin boundaries for the analysis of genomic data.\n    Boundaries are based on the inflection points of a cubic smoothing spline\n    fitted to the raw data. Along with defining boundaries, a technique to\n    evaluate results obtained from unequally-sized windows is provided.\n    Applications are particularly pertinent for, though not limited to, genome\n    scans for selection based on variability between populations (e.g. using\n    Wright's fixations index, Fst, which measures variability in subpopulations\n    relative to the total population).",
    "version": "1.0",
    "maintainer": "Timothy M. Beissinger <timbeissinger@gmail.com>",
    "author": "Timothy M. Beissinger <timbeissinger@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GenWin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GenWin Spline Based Window Boundaries for Genomic Analyses Defines window or bin boundaries for the analysis of genomic data.\n    Boundaries are based on the inflection points of a cubic smoothing spline\n    fitted to the raw data. Along with defining boundaries, a technique to\n    evaluate results obtained from unequally-sized windows is provided.\n    Applications are particularly pertinent for, though not limited to, genome\n    scans for selection based on variability between populations (e.g. using\n    Wright's fixations index, Fst, which measures variability in subpopulations\n    relative to the total population).  "
  },
  {
    "id": 3825,
    "package_name": "GenomicSig",
    "title": "Computation of Genomic Signatures",
    "description": "Genomic signatures represent unique features within a species' DNA, enabling the differentiation of species and offering broad applications across various fields. This package provides essential tools for calculating these specific signatures, streamlining the process for researchers and offering a comprehensive and time-saving solution for genomic analysis.The amino acid contents are identified based on the work published by Sandberg et al. (2003) <doi:10.1016/s0378-1119(03)00581-x> and Xiao et al. (2015) <doi:10.1093/bioinformatics/btv042>. The Average Mutual Information Profiles (AMIP) values are calculated based on the work of Bauer et al. (2008) <doi:10.1186/1471-2105-9-48>. The Chaos Game Representation (CGR) plot visualization was done based on the work of Deschavanne et al. (1999) <doi:10.1093/oxfordjournals.molbev.a026048> and Jeffrey et al. (1990) <doi:10.1093/nar/18.8.2163>. The GC content is calculated based on the work published by Nakabachi et al. (2006) <doi:10.1126/science.1134196> and Barbu et al. (1956) <https://pubmed.ncbi.nlm.nih.gov/13363015>. The Oligonucleotide Frequency Derived Error Gradient (OFDEG) values are computed based on the work published by Saeed et al. (2009) <doi:10.1186/1471-2164-10-S3-S10>. The Relative Synonymous Codon Usage (RSCU) values are calculated based on the work published by Elek (2018) <https://urn.nsk.hr/urn:nbn:hr:217:686131>.",
    "version": "0.1.0",
    "maintainer": "Anu Sharma <anu.sharma@icar.gov.in>",
    "author": "Mailarlinga [aut],\n  Shashi Bhushan Lal [aut],\n  Anu Sharma [aut, cre],\n  Dwijesh Chandra Mishra [aut],\n  Sudhir Srivastava [aut],\n  Sanjeev Kumar [aut],\n  Girish Kumar Jha [aut],\n  Sayanti Guha Majumdar [aut],\n  Megha Garg [aut],\n  Sharanbasappa [ctb],\n  Kabilan S [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GenomicSig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GenomicSig Computation of Genomic Signatures Genomic signatures represent unique features within a species' DNA, enabling the differentiation of species and offering broad applications across various fields. This package provides essential tools for calculating these specific signatures, streamlining the process for researchers and offering a comprehensive and time-saving solution for genomic analysis.The amino acid contents are identified based on the work published by Sandberg et al. (2003) <doi:10.1016/s0378-1119(03)00581-x> and Xiao et al. (2015) <doi:10.1093/bioinformatics/btv042>. The Average Mutual Information Profiles (AMIP) values are calculated based on the work of Bauer et al. (2008) <doi:10.1186/1471-2105-9-48>. The Chaos Game Representation (CGR) plot visualization was done based on the work of Deschavanne et al. (1999) <doi:10.1093/oxfordjournals.molbev.a026048> and Jeffrey et al. (1990) <doi:10.1093/nar/18.8.2163>. The GC content is calculated based on the work published by Nakabachi et al. (2006) <doi:10.1126/science.1134196> and Barbu et al. (1956) <https://pubmed.ncbi.nlm.nih.gov/13363015>. The Oligonucleotide Frequency Derived Error Gradient (OFDEG) values are computed based on the work published by Saeed et al. (2009) <doi:10.1186/1471-2164-10-S3-S10>. The Relative Synonymous Codon Usage (RSCU) values are calculated based on the work published by Elek (2018) <https://urn.nsk.hr/urn:nbn:hr:217:686131>.  "
  },
  {
    "id": 4006,
    "package_name": "Hapi",
    "title": "Inference of Chromosome-Length Haplotypes Using Genomic Data of\nSingle Gamete Cells",
    "description": "Inference of chromosome-length haplotypes using a few haploid \n\tgametes of an individual. The gamete genotype data may be generated from various platforms \n\tincluding genotyping arrays and sequencing even with low-coverage. Hapi simply takes \n\tgenotype data of known hetSNPs in single gamete cells as input and report the high-resolution \n\thaplotypes as well as confidence of each phased hetSNPs. The package also includes a module \n\tallowing downstream analyses and visualization of identified crossovers in the gametes. ",
    "version": "0.0.3",
    "maintainer": "Ruidong Li <rli012@ucr.edu>",
    "author": "Ruidong Li,\n\tHan Qu,\n\tJinfeng Chen,\n\tShibo Wang,\n\tLe Zhang,\n\tJulong Wei,\n\tSergio Pietro Ferrante,\n\tMikeal L. Roose,\n\tZhenyu Jia",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Hapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Hapi Inference of Chromosome-Length Haplotypes Using Genomic Data of\nSingle Gamete Cells Inference of chromosome-length haplotypes using a few haploid \n\tgametes of an individual. The gamete genotype data may be generated from various platforms \n\tincluding genotyping arrays and sequencing even with low-coverage. Hapi simply takes \n\tgenotype data of known hetSNPs in single gamete cells as input and report the high-resolution \n\thaplotypes as well as confidence of each phased hetSNPs. The package also includes a module \n\tallowing downstream analyses and visualization of identified crossovers in the gametes.   "
  },
  {
    "id": 4017,
    "package_name": "HeckmanEM",
    "title": "Fit Normal, Student-t or Contaminated Normal Heckman Selection\nModels",
    "description": "It performs maximum likelihood estimation for the Heckman selection model (Normal, Student-t or Contaminated normal) using an EM-algorithm <doi:10.1016/j.jmva.2021.104737>. It also performs influence diagnostic through global and local influence for four possible perturbation schema. ",
    "version": "0.2-2",
    "maintainer": "Marcos Prates <marcosop@est.ufmg.br>",
    "author": "Marcos Prates [aut, cre, trl] (ORCID:\n    <https://orcid.org/0000-0001-8077-4898>),\n  Victor Lachos [aut] (ORCID: <https://orcid.org/0000-0002-7239-2459>),\n  Dipak Dey [aut],\n  Marcos Oliveira [aut, ctb],\n  Christian Galarza [ctb],\n  Katherine Loor [ctb],\n  Alejandro Ordonez [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HeckmanEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HeckmanEM Fit Normal, Student-t or Contaminated Normal Heckman Selection\nModels It performs maximum likelihood estimation for the Heckman selection model (Normal, Student-t or Contaminated normal) using an EM-algorithm <doi:10.1016/j.jmva.2021.104737>. It also performs influence diagnostic through global and local influence for four possible perturbation schema.   "
  },
  {
    "id": 4055,
    "package_name": "HyRiM",
    "title": "Multicriteria Risk Management using Zero-Sum Games with\nVector-Valued Payoffs that are Probability Distributions",
    "description": "Construction and analysis of multivalued zero-sum matrix games over the abstract space of probability distributions, which describe the losses in each scenario of defense vs. attack action. The distributions can be compiled directly from expert opinions or other empirical data (insofar available). The package implements the methods put forth in the EU project HyRiM (Hybrid Risk Management for Utility Networks), FP7 EU Project Number 608090. The method has been published in Rass, S., K\u00f6nig, S., Schauer, S., 2016. Decisions with Uncertain Consequences-A Total Ordering on Loss-Distributions. PLoS ONE 11, e0168583. <doi:10.1371/journal.pone.0168583>, and applied for advanced persistent thread modeling in Rass, S., K\u00f6nig, S., Schauer, S., 2017. Defending Against Advanced Persistent Threats Using Game-Theory. PLoS ONE 12, e0168675. <doi:10.1371/journal.pone.0168675>. A volume covering the wider range of aspects of risk management, partially based on the theory implemented in the package is the book edited by S. Rass and S. Schauer, 2018. Game Theory for Security and Risk Management: From Theory to Practice. Springer, <doi:10.1007/978-3-319-75268-6>, ISBN 978-3-319-75267-9.",
    "version": "2.0.2",
    "maintainer": "\"Stefan Rass, on behalf of the Austrian Institute of Technology\" <stefan.rass@jku.at>",
    "author": "Stefan Rass, Sandra Koenig, Ali Alshawish",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HyRiM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HyRiM Multicriteria Risk Management using Zero-Sum Games with\nVector-Valued Payoffs that are Probability Distributions Construction and analysis of multivalued zero-sum matrix games over the abstract space of probability distributions, which describe the losses in each scenario of defense vs. attack action. The distributions can be compiled directly from expert opinions or other empirical data (insofar available). The package implements the methods put forth in the EU project HyRiM (Hybrid Risk Management for Utility Networks), FP7 EU Project Number 608090. The method has been published in Rass, S., K\u00f6nig, S., Schauer, S., 2016. Decisions with Uncertain Consequences-A Total Ordering on Loss-Distributions. PLoS ONE 11, e0168583. <doi:10.1371/journal.pone.0168583>, and applied for advanced persistent thread modeling in Rass, S., K\u00f6nig, S., Schauer, S., 2017. Defending Against Advanced Persistent Threats Using Game-Theory. PLoS ONE 12, e0168675. <doi:10.1371/journal.pone.0168675>. A volume covering the wider range of aspects of risk management, partially based on the theory implemented in the package is the book edited by S. Rass and S. Schauer, 2018. Game Theory for Security and Risk Management: From Theory to Practice. Springer, <doi:10.1007/978-3-319-75268-6>, ISBN 978-3-319-75267-9.  "
  },
  {
    "id": 4069,
    "package_name": "IATanalytics",
    "title": "Compute Effect Sizes and Reliability for Implicit Association\nTest (IAT) Data",
    "description": "Quickly score raw data outputted from an Implicit Association Test (IAT; Greenwald, McGhee, & Schwartz, 1998) <doi:10.1037/0022-3514.74.6.1464>. IAT scores are calculated as specified by Greenwald, Nosek, and Banaji (2003) <doi:10.1037/0022-3514.85.2.197>.    The output of this function is a data frame that consists of four rows containing the following information: (1) the overall IAT effect size for the participant's dataset, (2) the effect size calculated for odd trials only, (3) the effect size calculated for even trials only, and (4) the proportion of trials with reaction times under 300ms (which is important for exclusion purposes). Items (2) and (3) allow for a measure of the internal consistency of the IAT. Specifically, you can use the subsetted IAT effect sizes for odd and even trials to calculate Cronbach's alpha across participants in the sample.    The input function consists of three arguments. First, indicate the name of the dataset to be analyzed. This is the only required input. Second, indicate the number of trials in your entire IAT (the default is set to 220, which is typical for most IATs). Last, indicate whether congruent trials (e.g., flowers and pleasant) or incongruent trials (e.g., guns and pleasant) were presented first for this participant (the default is set to congruent).    Data files should consist of six columns organized in order as follows: Block (0-6), trial (0-19 for training blocks, 0-39 for test blocks), category (dependent on your IAT), the type of item within that category (dependent on your IAT), a dummy variable indicating whether the participant was correct or incorrect on that trial (0=correct, 1=incorrect), and the participant\u2019s reaction time (in milliseconds).    A sample dataset (titled 'sampledata') is included in this package to practice with.",
    "version": "0.2.0",
    "maintainer": "Daniel Storage <danielstorage@icloud.com>",
    "author": "Daniel Storage [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IATanalytics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IATanalytics Compute Effect Sizes and Reliability for Implicit Association\nTest (IAT) Data Quickly score raw data outputted from an Implicit Association Test (IAT; Greenwald, McGhee, & Schwartz, 1998) <doi:10.1037/0022-3514.74.6.1464>. IAT scores are calculated as specified by Greenwald, Nosek, and Banaji (2003) <doi:10.1037/0022-3514.85.2.197>.    The output of this function is a data frame that consists of four rows containing the following information: (1) the overall IAT effect size for the participant's dataset, (2) the effect size calculated for odd trials only, (3) the effect size calculated for even trials only, and (4) the proportion of trials with reaction times under 300ms (which is important for exclusion purposes). Items (2) and (3) allow for a measure of the internal consistency of the IAT. Specifically, you can use the subsetted IAT effect sizes for odd and even trials to calculate Cronbach's alpha across participants in the sample.    The input function consists of three arguments. First, indicate the name of the dataset to be analyzed. This is the only required input. Second, indicate the number of trials in your entire IAT (the default is set to 220, which is typical for most IATs). Last, indicate whether congruent trials (e.g., flowers and pleasant) or incongruent trials (e.g., guns and pleasant) were presented first for this participant (the default is set to congruent).    Data files should consist of six columns organized in order as follows: Block (0-6), trial (0-19 for training blocks, 0-39 for test blocks), category (dependent on your IAT), the type of item within that category (dependent on your IAT), a dummy variable indicating whether the participant was correct or incorrect on that trial (0=correct, 1=incorrect), and the participant\u2019s reaction time (in milliseconds).    A sample dataset (titled 'sampledata') is included in this package to practice with.  "
  },
  {
    "id": 4133,
    "package_name": "IFTPredictor",
    "title": "Predictions Using Item-Focused Tree Models",
    "description": "This function predicts item response probabilities and item \n  responses using the item-focused tree model. The item-focused tree model\n  combines logistic regression with recursive partitioning to detect \n  Differential Item Functioning in dichotomous items. The model applies \n  partitioning rules to the data, splitting it into homogeneous subgroups, and \n  uses logistic regression within each subgroup to explain the data. \n  Differential Item Functioning detection is achieved by examining potential \n  group differences in item response patterns. This method is useful for \n  understanding how different predictors, such as demographic or psychological \n  factors, influence item responses across subgroups.",
    "version": "0.1.0",
    "maintainer": "Muditha L. Bodawatte Gedara <muditha.lakmali.1993@gmail.com>",
    "author": "Muditha L. Bodawatte Gedara [aut, cre],\n  Barret A. Monchka [aut],\n  Lisa M. Lix [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IFTPredictor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IFTPredictor Predictions Using Item-Focused Tree Models This function predicts item response probabilities and item \n  responses using the item-focused tree model. The item-focused tree model\n  combines logistic regression with recursive partitioning to detect \n  Differential Item Functioning in dichotomous items. The model applies \n  partitioning rules to the data, splitting it into homogeneous subgroups, and \n  uses logistic regression within each subgroup to explain the data. \n  Differential Item Functioning detection is achieved by examining potential \n  group differences in item response patterns. This method is useful for \n  understanding how different predictors, such as demographic or psychological \n  factors, influence item responses across subgroups.  "
  },
  {
    "id": 4134,
    "package_name": "IGC.CSM",
    "title": "Simulate Impact of Different Urban Policies Through a General\nEquilibrium Model",
    "description": "Develops a General Equilibrium (GE) Model, which estimates key variables such as wages, the number of residents and workers, the prices of the floor space, and its distribution between commercial and residential use, as in Ahlfeldt et al., (2015) <doi:10.3982/ECTA10876>. By doing so, the model allows understanding the economic influence of different urban policies.",
    "version": "0.2.0",
    "maintainer": "Roman Zarate <rd.zarate40@gmail.com>",
    "author": "David Zarruk [aut],\n  Roman Zarate [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IGC.CSM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IGC.CSM Simulate Impact of Different Urban Policies Through a General\nEquilibrium Model Develops a General Equilibrium (GE) Model, which estimates key variables such as wages, the number of residents and workers, the prices of the floor space, and its distribution between commercial and residential use, as in Ahlfeldt et al., (2015) <doi:10.3982/ECTA10876>. By doing so, the model allows understanding the economic influence of different urban policies.  "
  },
  {
    "id": 4182,
    "package_name": "IRTBEMM",
    "title": "Family of Bayesian EMM Algorithm for Item Response Models",
    "description": "Applying the family of the Bayesian Expectation-Maximization-Maximization (BEMM) algorithm to estimate: (1) Three parameter logistic (3PL) model proposed by Birnbaum (1968, ISBN:9780201043105); (2) four parameter logistic (4PL) model proposed by Barton & Lord (1981) <doi:10.1002/j.2333-8504.1981.tb01255.x>; (3) one parameter logistic guessing (1PLG) and (4) one parameter logistic ability-based guessing (1PLAG) models proposed by San Mart\u00edn et al (2006) <doi:10.1177/0146621605282773>. The BEMM family includes (1) the BEMM algorithm for 3PL model proposed by Guo & Zheng (2019) <doi:10.3389/fpsyg.2019.01175>; (2) the BEMM algorithm for 1PLG model and (3) the BEMM algorithm for 1PLAG model proposed by Guo, Wu, Zheng, & Chen (2021) <doi:10.1177/0146621621990761>; (4) the BEMM algorithm for 4PL model proposed by Zheng, Guo, & Kern (2021) <doi:10.1177/21582440211052556>; and (5) their maximum likelihood estimation versions proposed by Zheng, Meng, Guo, & Liu (2018) <doi:10.3389/fpsyg.2017.02302>. Thus, both Bayesian modal estimates and maximum likelihood estimates are available.",
    "version": "1.0.8",
    "maintainer": "Shaoyang Guo <syguo1992@outlook.com>",
    "author": "Shaoyang Guo [aut, cre, cph],\n  Chanjin Zheng [aut],\n  Justin L Kern [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IRTBEMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IRTBEMM Family of Bayesian EMM Algorithm for Item Response Models Applying the family of the Bayesian Expectation-Maximization-Maximization (BEMM) algorithm to estimate: (1) Three parameter logistic (3PL) model proposed by Birnbaum (1968, ISBN:9780201043105); (2) four parameter logistic (4PL) model proposed by Barton & Lord (1981) <doi:10.1002/j.2333-8504.1981.tb01255.x>; (3) one parameter logistic guessing (1PLG) and (4) one parameter logistic ability-based guessing (1PLAG) models proposed by San Mart\u00edn et al (2006) <doi:10.1177/0146621605282773>. The BEMM family includes (1) the BEMM algorithm for 3PL model proposed by Guo & Zheng (2019) <doi:10.3389/fpsyg.2019.01175>; (2) the BEMM algorithm for 1PLG model and (3) the BEMM algorithm for 1PLAG model proposed by Guo, Wu, Zheng, & Chen (2021) <doi:10.1177/0146621621990761>; (4) the BEMM algorithm for 4PL model proposed by Zheng, Guo, & Kern (2021) <doi:10.1177/21582440211052556>; and (5) their maximum likelihood estimation versions proposed by Zheng, Meng, Guo, & Liu (2018) <doi:10.3389/fpsyg.2017.02302>. Thus, both Bayesian modal estimates and maximum likelihood estimates are available.  "
  },
  {
    "id": 4190,
    "package_name": "ISAR",
    "title": "Introduction to Sports Analytics using R (ISAR) Data",
    "description": "We provide data sets used in the textbook \"Introduction to Sports Analytics using R\" by Elmore and Urbaczweski (2025). ",
    "version": "1.0.1",
    "maintainer": "Ryan Elmore <Ryan.Elmore@du.edu>",
    "author": "Ryan Elmore [cre, aut]",
    "url": "https://github.com/rtelmore/ISAR",
    "bug_reports": "https://github.com/rtelmore/ISAR/issues",
    "repository": "https://cran.r-project.org/package=ISAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ISAR Introduction to Sports Analytics using R (ISAR) Data We provide data sets used in the textbook \"Introduction to Sports Analytics using R\" by Elmore and Urbaczweski (2025).   "
  },
  {
    "id": 4207,
    "package_name": "ITNr",
    "title": "Analysis of the International Trade Network",
    "description": "Functions to clean and process international trade data into an international trade network (ITN) are provided. It then provides a set a functions to undertake analysis and plots of the ITN (extract the backbone, centrality, blockmodels, clustering). Examining the key players in the ITN and regional trade patterns. ",
    "version": "0.7.0",
    "maintainer": "Matthew Smith <matt_smith.90@hotmail.co.uk>",
    "author": "Matthew Smith",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ITNr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ITNr Analysis of the International Trade Network Functions to clean and process international trade data into an international trade network (ITN) are provided. It then provides a set a functions to undertake analysis and plots of the ITN (extract the backbone, centrality, blockmodels, clustering). Examining the key players in the ITN and regional trade patterns.   "
  },
  {
    "id": 4240,
    "package_name": "IndiAPIs",
    "title": "Access Indian Data via Public APIs and Curated Datasets",
    "description": "Provides functions to access data from public RESTful APIs including \n    'World Bank API', and 'REST Countries API', retrieving real-time or historical \n    data related to India, such as economic indicators, and international \n    demographic and geopolitical indicators. Additionally, the package includes one of the largest \n    curated collections of open datasets focused on India, covering topics such as population, economy, weather, politics, health, biodiversity, sports, agriculture, cybercrime, infrastructure, and more. The package supports reproducible research and teaching by integrating \n    reliable international APIs and structured datasets from public, academic, and government sources. \n    For more information on the APIs, see: \n    'World Bank API' <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    'REST Countries API' <https://restcountries.com/>.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/indiapis,\nhttps://lightbluetitan.github.io/indiapis/",
    "bug_reports": "https://github.com/lightbluetitan/indiapis/issues",
    "repository": "https://cran.r-project.org/package=IndiAPIs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IndiAPIs Access Indian Data via Public APIs and Curated Datasets Provides functions to access data from public RESTful APIs including \n    'World Bank API', and 'REST Countries API', retrieving real-time or historical \n    data related to India, such as economic indicators, and international \n    demographic and geopolitical indicators. Additionally, the package includes one of the largest \n    curated collections of open datasets focused on India, covering topics such as population, economy, weather, politics, health, biodiversity, sports, agriculture, cybercrime, infrastructure, and more. The package supports reproducible research and teaching by integrating \n    reliable international APIs and structured datasets from public, academic, and government sources. \n    For more information on the APIs, see: \n    'World Bank API' <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    'REST Countries API' <https://restcountries.com/>.  "
  },
  {
    "id": 4245,
    "package_name": "Inflation",
    "title": "Core Inflation",
    "description": "Provides access to core inflation functions. Four different core inflation \n functions are provided. The well known trimmed means, exclusion and double weighing methods, \n alongside the new Triple Filter method introduced in Ferreira et al. (2016) <https://goo.gl/UYLhcj>.",
    "version": "0.1.0",
    "maintainer": "Pedro Costa Ferreira <pedro.guilherme@fgv.br>",
    "author": "Pedro Costa Ferreira [aut, cre],\n  Daiane Marcolino [aut],\n  Talitha Speranza [aut],\n  Fernando Teixeira [aut]",
    "url": "https://github.com/fernote7/Inflation",
    "bug_reports": "https://github.com/fernote7/Inflation/issues",
    "repository": "https://cran.r-project.org/package=Inflation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Inflation Core Inflation Provides access to core inflation functions. Four different core inflation \n functions are provided. The well known trimmed means, exclusion and double weighing methods, \n alongside the new Triple Filter method introduced in Ferreira et al. (2016) <https://goo.gl/UYLhcj>.  "
  },
  {
    "id": 4269,
    "package_name": "IntervalQuestionStat",
    "title": "Tools to Deal with Interval-Valued Responses in Questionnaires",
    "description": "A user-friendly toolbox for doing the statistical analysis of\n  interval-valued responses in questionnaires measuring intrinsically\n  imprecise human attributes or features (attitudes, perceptions, opinions,\n  feelings, etc.). In particular, this package provides S4 classes, methods,\n  and functions in order to compute basic arithmetic and statistical operations\n  with interval-valued data; prepare customized plots; associate each\n  interval-valued response to its equivalent Likert-type and visual analogue\n  scales answers through the minimum theta-distance and the mid-point criteria;\n  analyze the reliability of respondents' answers from the internal consistency\n  point of view by means of Cronbach's alpha coefficient; and simulate\n  interval-valued responses in this type of questionnaires. The package also\n  incorporates some real-life data that can be used to illustrate its working\n  with several non-trivial reproducible examples. The methodology used in this\n  package is based in many theoretical and applied publications from\n  SMIRE+CoDiRE (Statistical Methods with Imprecise Random Elements and\n  Comparison of Distributions of Random Elements) Research Group\n  (<https://bellman.ciencias.uniovi.es/smire+codire/>)\n  from the University of Oviedo (Spain).",
    "version": "0.2.0",
    "maintainer": "Jos\u00e9 Garc\u00eda-Garc\u00eda <garciagarjose@uniovi.es>",
    "author": "Jos\u00e9 Garc\u00eda-Garc\u00eda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0991-1866>),\n  Mar\u00eda Asunci\u00f3n Lubiano [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9847-5164>)",
    "url": "https://github.com/garciagarjose/IntervalQuestionStat/",
    "bug_reports": "https://github.com/garciagarjose/IntervalQuestionStat/issues/",
    "repository": "https://cran.r-project.org/package=IntervalQuestionStat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IntervalQuestionStat Tools to Deal with Interval-Valued Responses in Questionnaires A user-friendly toolbox for doing the statistical analysis of\n  interval-valued responses in questionnaires measuring intrinsically\n  imprecise human attributes or features (attitudes, perceptions, opinions,\n  feelings, etc.). In particular, this package provides S4 classes, methods,\n  and functions in order to compute basic arithmetic and statistical operations\n  with interval-valued data; prepare customized plots; associate each\n  interval-valued response to its equivalent Likert-type and visual analogue\n  scales answers through the minimum theta-distance and the mid-point criteria;\n  analyze the reliability of respondents' answers from the internal consistency\n  point of view by means of Cronbach's alpha coefficient; and simulate\n  interval-valued responses in this type of questionnaires. The package also\n  incorporates some real-life data that can be used to illustrate its working\n  with several non-trivial reproducible examples. The methodology used in this\n  package is based in many theoretical and applied publications from\n  SMIRE+CoDiRE (Statistical Methods with Imprecise Random Elements and\n  Comparison of Distributions of Random Elements) Research Group\n  (<https://bellman.ciencias.uniovi.es/smire+codire/>)\n  from the University of Oviedo (Spain).  "
  },
  {
    "id": 4309,
    "package_name": "JNplots",
    "title": "Visualize Outputs from the 'Johnson-Neyman' Technique",
    "description": "Aids in the calculation and visualization of regions of non-significance using the 'Johnson-Neyman' technique and its extensions as described by Bauer and Curran (2005) <doi:10.1207/s15327906mbr4003_5> to assess the influence of categorical and continuous moderators. Allows correcting for phylogenetic relatedness.",
    "version": "0.1.2",
    "maintainer": "Ken Toyama <ken.toyama7@gmail.com>",
    "author": "Ken Toyama [cre, aut] (ORCID: <https://orcid.org/0000-0002-8331-4894>)",
    "url": "https://github.com/kenstoyama/JNplots",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JNplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JNplots Visualize Outputs from the 'Johnson-Neyman' Technique Aids in the calculation and visualization of regions of non-significance using the 'Johnson-Neyman' technique and its extensions as described by Bauer and Curran (2005) <doi:10.1207/s15327906mbr4003_5> to assess the influence of categorical and continuous moderators. Allows correcting for phylogenetic relatedness.  "
  },
  {
    "id": 4333,
    "package_name": "JoSAE",
    "title": "Unit-Level and Area-Level Small Area Estimation",
    "description": "Implementation of some unit and area level EBLUP estimators as well as the estimators of their MSE also under heteroscedasticity. The package further documents the publications Breidenbach and Astrup (2012) <DOI:10.1007/s10342-012-0596-7>, Breidenbach et al. (2016) <DOI:10.1016/j.rse.2015.07.026> and Breidenbach et al. (2018 in press). The vignette further explains the use of the implemented functions.",
    "version": "0.3.0",
    "maintainer": "Johannes Breidenbach <job@nibio.no>",
    "author": "Johannes Breidenbach",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JoSAE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JoSAE Unit-Level and Area-Level Small Area Estimation Implementation of some unit and area level EBLUP estimators as well as the estimators of their MSE also under heteroscedasticity. The package further documents the publications Breidenbach and Astrup (2012) <DOI:10.1007/s10342-012-0596-7>, Breidenbach et al. (2016) <DOI:10.1016/j.rse.2015.07.026> and Breidenbach et al. (2018 in press). The vignette further explains the use of the implemented functions.  "
  },
  {
    "id": 4495,
    "package_name": "LPmerge",
    "title": "Merging Linkage Maps by Linear Programming",
    "description": "Creates a consensus genetic map by merging linkage maps from different populations.  The software uses linear programming (LP) to efficiently minimize the mean absolute error between the consensus map and the linkage maps.  This minimization is performed subject to linear inequality constraints that ensure the ordering of the markers in the linkage maps is preserved.  When marker order is inconsistent between linkage maps, a minimum set of ordinal constraints is deleted to resolve the conflicts.",
    "version": "1.7",
    "maintainer": "Jeffrey Endelman <endelman@wisc.edu>",
    "author": "Jeffrey Endelman",
    "url": "http://potatobreeding.cals.wisc.edu/software",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LPmerge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LPmerge Merging Linkage Maps by Linear Programming Creates a consensus genetic map by merging linkage maps from different populations.  The software uses linear programming (LP) to efficiently minimize the mean absolute error between the consensus map and the linkage maps.  This minimization is performed subject to linear inequality constraints that ensure the ordering of the markers in the linkage maps is preserved.  When marker order is inconsistent between linkage maps, a minimum set of ordinal constraints is deleted to resolve the conflicts.  "
  },
  {
    "id": 4541,
    "package_name": "Lahman",
    "title": "Sean 'Lahman' Baseball Database",
    "description": "Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2024, as recorded in the 2025\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated.",
    "version": "13.0-0",
    "maintainer": "Chris Dalzell <cdalzell@gmail.com>",
    "author": "Michael Friendly [aut],\n  Chris Dalzell [cre, aut],\n  Martin Monkman [aut],\n  Dennis Murphy [aut],\n  Vanessa Foot [ctb],\n  Justeena Zaki-Azat [ctb],\n  Daniel J Eck [ctb],\n  Sean Lahman [cph]",
    "url": "https://cdalzell.github.io/Lahman/,\nhttps://CRAN.R-project.org/package=Lahman",
    "bug_reports": "https://github.com/cdalzell/Lahman/issues",
    "repository": "https://cran.r-project.org/package=Lahman",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Lahman Sean 'Lahman' Baseball Database Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2024, as recorded in the 2025\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated.  "
  },
  {
    "id": 4553,
    "package_name": "Latamverse",
    "title": "Latin American Data via 'RESTful' APIs and Curated Datasets",
    "description": "Brings together a comprehensive collection \n    of R packages providing access to API functions and curated datasets from Argentina, Brazil, \n    Chile, Colombia, and Peru. Includes real-time and historical data through public \n    'RESTful' APIs ('Nager.Date', World Bank API, REST Countries API, and country-specific APIs) and \n    extensive curated collections of open datasets covering economics, demographics, public health, \n    environmental data, political indicators, social metrics, and cultural information. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to Latin American data sources, facilitating \n    reproducible research, comparative analysis, and teaching applications focused \n    on these five major Latin American countries.\n    Included packages:\n    - 'ArgentinAPI': API functions and curated datasets for Argentina covering exchange rates, inflation, political figures, national holidays and more.\n    - 'BrazilDataAPI': API functions and curated datasets for Brazil covering postal codes, banks, economic indicators, holidays, company registrations and more.\n    - 'ChileDataAPI': API functions and curated datasets for Chile covering financial indicators ('UF', UTM, Dollar, Euro, Yen, Copper, Bitcoin, 'IPSA' index), holidays and more.\n    - 'ColombiAPI': API functions and curated datasets for Colombia covering geographic locations, cultural attractions, economic indicators, demographic data, national holidays and more.\n    - 'PeruAPIs': API functions and curated datasets for Peru covering economic indicators, demographics, national holidays, administrative divisions, electoral data, biodiversity and more.\n    For more information on the APIs, see: \n    'Nager.Date' <https://date.nager.at/Api>, \n    World Bank API <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    REST Countries API <https://restcountries.com/>,\n    'ArgentinaDatos' API <https://argentinadatos.com/>,\n    'BrasilAPI' <https://brasilapi.com.br/>,\n    'FINDIC' <https://findic.cl/>,\n    and API-Colombia <https://api-colombia.com/>.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/latamverse,\nhttps://lightbluetitan.github.io/latamverse/",
    "bug_reports": "https://github.com/lightbluetitan/latamverse/issues",
    "repository": "https://cran.r-project.org/package=Latamverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Latamverse Latin American Data via 'RESTful' APIs and Curated Datasets Brings together a comprehensive collection \n    of R packages providing access to API functions and curated datasets from Argentina, Brazil, \n    Chile, Colombia, and Peru. Includes real-time and historical data through public \n    'RESTful' APIs ('Nager.Date', World Bank API, REST Countries API, and country-specific APIs) and \n    extensive curated collections of open datasets covering economics, demographics, public health, \n    environmental data, political indicators, social metrics, and cultural information. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to Latin American data sources, facilitating \n    reproducible research, comparative analysis, and teaching applications focused \n    on these five major Latin American countries.\n    Included packages:\n    - 'ArgentinAPI': API functions and curated datasets for Argentina covering exchange rates, inflation, political figures, national holidays and more.\n    - 'BrazilDataAPI': API functions and curated datasets for Brazil covering postal codes, banks, economic indicators, holidays, company registrations and more.\n    - 'ChileDataAPI': API functions and curated datasets for Chile covering financial indicators ('UF', UTM, Dollar, Euro, Yen, Copper, Bitcoin, 'IPSA' index), holidays and more.\n    - 'ColombiAPI': API functions and curated datasets for Colombia covering geographic locations, cultural attractions, economic indicators, demographic data, national holidays and more.\n    - 'PeruAPIs': API functions and curated datasets for Peru covering economic indicators, demographics, national holidays, administrative divisions, electoral data, biodiversity and more.\n    For more information on the APIs, see: \n    'Nager.Date' <https://date.nager.at/Api>, \n    World Bank API <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    REST Countries API <https://restcountries.com/>,\n    'ArgentinaDatos' API <https://argentinadatos.com/>,\n    'BrasilAPI' <https://brasilapi.com.br/>,\n    'FINDIC' <https://findic.cl/>,\n    and API-Colombia <https://api-colombia.com/>.  "
  },
  {
    "id": 4575,
    "package_name": "LiblineaR",
    "title": "Linear Predictive Models Based on the LIBLINEAR C/C++ Library",
    "description": "A wrapper around the LIBLINEAR C/C++ library for machine\n        learning (available at\n        <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>). LIBLINEAR is\n        a simple library for solving large-scale regularized linear\n        classification and regression. It currently supports\n        L2-regularized classification (such as logistic regression,\n        L2-loss linear SVM and L1-loss linear SVM) as well as\n        L1-regularized classification (such as L2-loss linear SVM and\n        logistic regression) and L2-regularized support vector\n        regression (with L1- or L2-loss). The main features of\n        LiblineaR include multi-class classification (one-vs-the rest,\n        and Crammer & Singer method), cross validation for model\n        selection, probability estimates (logistic regression only) or\n        weights for unbalanced data. The estimation of the models is\n        particularly fast as compared to other libraries.",
    "version": "2.10-24",
    "maintainer": "Thibault Helleputte <thibault.helleputte@dnalytics.com>",
    "author": "Thibault Helleputte [cre, aut, cph],\n  J\u00e9r\u00f4me Paul [aut],\n  Pierre Gramme [aut]",
    "url": "<https://dnalytics.com/software/liblinear/>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LiblineaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LiblineaR Linear Predictive Models Based on the LIBLINEAR C/C++ Library A wrapper around the LIBLINEAR C/C++ library for machine\n        learning (available at\n        <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>). LIBLINEAR is\n        a simple library for solving large-scale regularized linear\n        classification and regression. It currently supports\n        L2-regularized classification (such as logistic regression,\n        L2-loss linear SVM and L1-loss linear SVM) as well as\n        L1-regularized classification (such as L2-loss linear SVM and\n        logistic regression) and L2-regularized support vector\n        regression (with L1- or L2-loss). The main features of\n        LiblineaR include multi-class classification (one-vs-the rest,\n        and Crammer & Singer method), cross validation for model\n        selection, probability estimates (logistic regression only) or\n        weights for unbalanced data. The estimation of the models is\n        particularly fast as compared to other libraries.  "
  },
  {
    "id": 4585,
    "package_name": "LikertEZ",
    "title": "Easy Analysis and Visualization of Likert Scale Data",
    "description": "Provides functions for summarizing, visualizing, and analyzing \n    Likert-scale survey data. Includes support for computing descriptive statistics, \n    Relative Importance Index (RII), reliability analysis (Cronbach's Alpha), \n    and response distribution plots.",
    "version": "0.1.0",
    "maintainer": "Mohammad Mollazehi <mmolazehi@lu.edu.qa>",
    "author": "Mohammad Mollazehi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LikertEZ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LikertEZ Easy Analysis and Visualization of Likert Scale Data Provides functions for summarizing, visualizing, and analyzing \n    Likert-scale survey data. Includes support for computing descriptive statistics, \n    Relative Importance Index (RII), reliability analysis (Cronbach's Alpha), \n    and response distribution plots.  "
  },
  {
    "id": 4586,
    "package_name": "LikertMakeR",
    "title": "Synthesise and Correlate Likert Scale and Rating-Scale Data\nBased on Summary Statistics",
    "description": "Generate and correlate synthetic Likert and rating-scale data\n    with predefined means, standard deviations, Cronbach's Alpha, Factor\n    Loading table, coefficients, and other summary statistics.  \n    Worked examples and documentation are available in the package \n    articles, accessible via the package website,\n    <https://winzarh.github.io/LikertMakeR/>. ",
    "version": "1.3.0",
    "maintainer": "Hume Winzar <winzar@gmail.com>",
    "author": "Hume Winzar [cre, aut] (ORCID: <https://orcid.org/0000-0001-7475-2641>)",
    "url": "https://github.com/WinzarH/LikertMakeR/,\nhttps://winzarh.github.io/LikertMakeR/",
    "bug_reports": "https://github.com/WinzarH/LikertMakeR/issues",
    "repository": "https://cran.r-project.org/package=LikertMakeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LikertMakeR Synthesise and Correlate Likert Scale and Rating-Scale Data\nBased on Summary Statistics Generate and correlate synthetic Likert and rating-scale data\n    with predefined means, standard deviations, Cronbach's Alpha, Factor\n    Loading table, coefficients, and other summary statistics.  \n    Worked examples and documentation are available in the package \n    articles, accessible via the package website,\n    <https://winzarh.github.io/LikertMakeR/>.   "
  },
  {
    "id": 4617,
    "package_name": "LogisticEnsembles",
    "title": "Automatically Runs 24 Logistic Models (Individual and Ensembles)",
    "description": "Automatically returns 24 logistic models including 13 individual models and 11 ensembles of models of logistic data. The package also returns 25 plots, 5 tables, and a summary report. The package automatically\n    builds all 24 models, reports all results, and provides graphics to show how the models performed. This can be used for a wide range of data, such as sports or medical data. The package includes medical data (the Pima Indians data set), and\n    information about the performance of Lebron James. The package can be used to analyze many other examples, such as stock market data. The package automatically returns many values for each model, such as\n    True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, Positive Predictive Value, Negative Predictive Value, F1 Score, Area Under the Curve. The package also returns 36 Receiver\n    Operating Characteristic (ROC) curves for each of the 24 models.",
    "version": "0.8.2",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "https://github.com/InfiniteCuriosity/LogisticEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/LogisticEnsembles/issues",
    "repository": "https://cran.r-project.org/package=LogisticEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LogisticEnsembles Automatically Runs 24 Logistic Models (Individual and Ensembles) Automatically returns 24 logistic models including 13 individual models and 11 ensembles of models of logistic data. The package also returns 25 plots, 5 tables, and a summary report. The package automatically\n    builds all 24 models, reports all results, and provides graphics to show how the models performed. This can be used for a wide range of data, such as sports or medical data. The package includes medical data (the Pima Indians data set), and\n    information about the performance of Lebron James. The package can be used to analyze many other examples, such as stock market data. The package automatically returns many values for each model, such as\n    True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, Positive Predictive Value, Negative Predictive Value, F1 Score, Area Under the Curve. The package also returns 36 Receiver\n    Operating Characteristic (ROC) curves for each of the 24 models.  "
  },
  {
    "id": 4636,
    "package_name": "MACER",
    "title": "Molecular Acquisition, Cleaning, and Evaluation in R 'MACER'",
    "description": "To assist biological researchers in assembling taxonomically and marker focused molecular sequence data sets. 'MACER' accepts a list of genera as a user input and uses NCBI-GenBank and BOLD as resources to download and assemble molecular sequence datasets. These datasets are then assembled by marker, aligned, trimmed, and cleaned. The use of this package allows the publication of specific parameters to ensure reproducibility. The 'MACER' package has four core functions and an example run through using all of these functions can be found in the associated repository <https://github.com/rgyoung6/MACER_example>.",
    "version": "0.2.1",
    "maintainer": "Robert G Young <rgyoung6@gmail.com>",
    "author": "Robert G Young [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6731-2506>),\n  Rekkab Gill [aut],\n  Daniel Gillis [aut],\n  Robert H Hanner [aut, cph]",
    "url": "<https://github.com/rgyoung6/MACER>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MACER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MACER Molecular Acquisition, Cleaning, and Evaluation in R 'MACER' To assist biological researchers in assembling taxonomically and marker focused molecular sequence data sets. 'MACER' accepts a list of genera as a user input and uses NCBI-GenBank and BOLD as resources to download and assemble molecular sequence datasets. These datasets are then assembled by marker, aligned, trimmed, and cleaned. The use of this package allows the publication of specific parameters to ensure reproducibility. The 'MACER' package has four core functions and an example run through using all of these functions can be found in the associated repository <https://github.com/rgyoung6/MACER_example>.  "
  },
  {
    "id": 4662,
    "package_name": "MASSExtra",
    "title": "Some 'MASS' Enhancements",
    "description": "Some enhancements, extensions and additions\n    to the facilities of the recommended 'MASS' package \n    that are useful mainly for teaching purposes, with\n    more convenient default settings and user interfaces.\n    Key functions from 'MASS' are imported and re-exported\n    to avoid masking conflicts.  In addition we provide\n    some additional functions mainly used to illustrate\n    coding paradigms and techniques, such as Gramm-Schmidt\n    orthogonalisation and generalised eigenvalue problems.  ",
    "version": "1.2.2",
    "maintainer": "Bill Venables <bill.venables@gmail.com>",
    "author": "Bill Venables",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MASSExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MASSExtra Some 'MASS' Enhancements Some enhancements, extensions and additions\n    to the facilities of the recommended 'MASS' package \n    that are useful mainly for teaching purposes, with\n    more convenient default settings and user interfaces.\n    Key functions from 'MASS' are imported and re-exported\n    to avoid masking conflicts.  In addition we provide\n    some additional functions mainly used to illustrate\n    coding paradigms and techniques, such as Gramm-Schmidt\n    orthogonalisation and generalised eigenvalue problems.    "
  },
  {
    "id": 4667,
    "package_name": "MAZE",
    "title": "Mediation Analysis for Zero-Inflated Mediators",
    "description": "A novel mediation analysis approach to address zero-inflated mediators containing true zeros and false zeros. See Jiang et al (2023) \"A Novel Causal Mediation Analysis Approach for Zero-Inflated Mediators\" <arXiv:2301.10064> for more details.",
    "version": "0.0.2",
    "maintainer": "Meilin Jiang <meilin.jiang@ufl.edu>",
    "author": "Meilin Jiang [aut, cre],\n  Zhigang Li [aut]",
    "url": "https://github.com/meilinjiang/MAZE",
    "bug_reports": "https://github.com/meilinjiang/MAZE/issues",
    "repository": "https://cran.r-project.org/package=MAZE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MAZE Mediation Analysis for Zero-Inflated Mediators A novel mediation analysis approach to address zero-inflated mediators containing true zeros and false zeros. See Jiang et al (2023) \"A Novel Causal Mediation Analysis Approach for Zero-Inflated Mediators\" <arXiv:2301.10064> for more details.  "
  },
  {
    "id": 4690,
    "package_name": "MCL",
    "title": "Markov Cluster Algorithm",
    "description": "Contains the Markov cluster algorithm (MCL) for identifying clusters in networks and graphs. The algorithm simulates random walks on a (n x n) matrix as the adjacency matrix of a graph. It alternates an expansion step and an inflation step until an equilibrium state is reached.",
    "version": "1.0",
    "maintainer": "Ronja Foraita <foraita@bips.uni-bremen.de>",
    "author": "Martin L. J\u00e4ger",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MCL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MCL Markov Cluster Algorithm Contains the Markov cluster algorithm (MCL) for identifying clusters in networks and graphs. The algorithm simulates random walks on a (n x n) matrix as the adjacency matrix of a graph. It alternates an expansion step and an inflation step until an equilibrium state is reached.  "
  },
  {
    "id": 4755,
    "package_name": "MHCtools",
    "title": "Analysis of MHC Data in Non-Model Species",
    "description": "Fifteen tools for bioinformatics processing and analysis of major \n    histocompatibility complex (MHC) data. The functions are tailored for amplicon data \n    sets that have been filtered using the dada2 method (for more information on \n    dada2, visit <https://benjjneb.github.io/dada2/> ), but even other types of data \n    sets can be analyzed.\n    The ReplMatch() function matches replicates in data sets in order to evaluate \n    genotyping success.\n    The GetReplTable() and GetReplStats() functions perform such an evaluation.\n    The CreateFas() function creates a fasta file with all the sequences in the data \n    set.\n    The CreateSamplesFas() function creates individual fasta files for each sample in \n    the data set.\n    The DistCalc() function calculates Grantham, Sandberg, or p-distances from pairwise \n    comparisons of all sequences in a data set, and mean distances of all pairwise \n    comparisons within each sample in a data set. The function additionally outputs five \n    tables with physico-chemical z-descriptor values (based on Sandberg et al. 1998) for \n    each amino acid position in all sequences in the data set. These tables may be useful \n    for further downstream analyses, such as estimation of MHC supertypes.\n    The BootKmeans() function is a wrapper for the kmeans() function of the 'stats'\n    package, which allows for bootstrapping. Bootstrapping k-estimates may be\n    desirable in data sets, where e.g. BIC- vs. k-values do not produce clear\n    inflection points (\"elbows\"). BootKmeans() performs multiple runs of kmeans() and \n    estimates optimal k-values based on a user-defined threshold of BIC reduction. The \n    method is an automated and bootstrapped version of visually inspecting elbow plots \n    of BIC- vs. k-values.\n    The ClusterMatch() function is a tool for evaluating whether different k-means() \n    clustering models identify similar clusters, and summarize bootstrap model stats as \n    means for different estimated values of k. It is designed to take files produced by \n    the BootKmeans() function as input, but other data can be analyzed if the \n    descriptions of the required data formats are observed carefully.\n    The PapaDiv() function compares parent pairs in the data set and calculate their \n    joint MHC diversity, taking into account sequence variants that occur in both \n    parents.\n    The HpltFind() function infers putative haplotypes from families in the data \n    set. \n    The GetHpltTable() and GetHpltStats() functions evaluate the accuracy of \n    the haplotype inference.\n    The CreateHpltOccTable() function creates a binary (logical) haplotype-sequence \n    occurrence matrix from the output of HpltFind(), for easy overview of which \n    sequences are present in which haplotypes. \n    The HpltMatch() function compares haplotypes to help identify overlapping and \n    potentially identical types.\n    The NestTablesXL() function translates the output from HpltFind() to an Excel \n    workbook, that provides a convenient overview for evaluation and curating of the \n    inferred putative haplotypes.",
    "version": "1.5.5",
    "maintainer": "Jacob Roved <jacob.roved@biol.lu.se>",
    "author": "Jacob Roved [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MHCtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHCtools Analysis of MHC Data in Non-Model Species Fifteen tools for bioinformatics processing and analysis of major \n    histocompatibility complex (MHC) data. The functions are tailored for amplicon data \n    sets that have been filtered using the dada2 method (for more information on \n    dada2, visit <https://benjjneb.github.io/dada2/> ), but even other types of data \n    sets can be analyzed.\n    The ReplMatch() function matches replicates in data sets in order to evaluate \n    genotyping success.\n    The GetReplTable() and GetReplStats() functions perform such an evaluation.\n    The CreateFas() function creates a fasta file with all the sequences in the data \n    set.\n    The CreateSamplesFas() function creates individual fasta files for each sample in \n    the data set.\n    The DistCalc() function calculates Grantham, Sandberg, or p-distances from pairwise \n    comparisons of all sequences in a data set, and mean distances of all pairwise \n    comparisons within each sample in a data set. The function additionally outputs five \n    tables with physico-chemical z-descriptor values (based on Sandberg et al. 1998) for \n    each amino acid position in all sequences in the data set. These tables may be useful \n    for further downstream analyses, such as estimation of MHC supertypes.\n    The BootKmeans() function is a wrapper for the kmeans() function of the 'stats'\n    package, which allows for bootstrapping. Bootstrapping k-estimates may be\n    desirable in data sets, where e.g. BIC- vs. k-values do not produce clear\n    inflection points (\"elbows\"). BootKmeans() performs multiple runs of kmeans() and \n    estimates optimal k-values based on a user-defined threshold of BIC reduction. The \n    method is an automated and bootstrapped version of visually inspecting elbow plots \n    of BIC- vs. k-values.\n    The ClusterMatch() function is a tool for evaluating whether different k-means() \n    clustering models identify similar clusters, and summarize bootstrap model stats as \n    means for different estimated values of k. It is designed to take files produced by \n    the BootKmeans() function as input, but other data can be analyzed if the \n    descriptions of the required data formats are observed carefully.\n    The PapaDiv() function compares parent pairs in the data set and calculate their \n    joint MHC diversity, taking into account sequence variants that occur in both \n    parents.\n    The HpltFind() function infers putative haplotypes from families in the data \n    set. \n    The GetHpltTable() and GetHpltStats() functions evaluate the accuracy of \n    the haplotype inference.\n    The CreateHpltOccTable() function creates a binary (logical) haplotype-sequence \n    occurrence matrix from the output of HpltFind(), for easy overview of which \n    sequences are present in which haplotypes. \n    The HpltMatch() function compares haplotypes to help identify overlapping and \n    potentially identical types.\n    The NestTablesXL() function translates the output from HpltFind() to an Excel \n    workbook, that provides a convenient overview for evaluation and curating of the \n    inferred putative haplotypes.  "
  },
  {
    "id": 4829,
    "package_name": "MNB",
    "title": "Diagnostic Tools for a Multivariate Negative Binomial Regression\nModel",
    "description": "Diagnostic tools as residual analysis, global, \n    local and total-local influence for the multivariate model \n    from the random intercept Poisson generalized log gamma model \n    are available in this package. Including also, the estimation \n    process by maximum likelihood method, for details see \n    Fabio, L. C; Villegas, C. L.; Carrasco, J.M.F and de Castro, M. (2023) \n    <doi:10.1080/03610926.2021.1939380> and F\u00e1bio, L. C.; Villegas, C.; \n    Mamun, A. S. M. A. and Carrasco, J. M. F. (2025) <doi:10.28951/bjb.v43i1.728>.",
    "version": "1.2.0",
    "maintainer": "Jalmar Carrasco <carrascojalmar@gmail.com>",
    "author": "Jalmar Carrasco [aut, cre],\n  Cristian Lobos [aut],\n  Lizandra Fabio [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MNB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MNB Diagnostic Tools for a Multivariate Negative Binomial Regression\nModel Diagnostic tools as residual analysis, global, \n    local and total-local influence for the multivariate model \n    from the random intercept Poisson generalized log gamma model \n    are available in this package. Including also, the estimation \n    process by maximum likelihood method, for details see \n    Fabio, L. C; Villegas, C. L.; Carrasco, J.M.F and de Castro, M. (2023) \n    <doi:10.1080/03610926.2021.1939380> and F\u00e1bio, L. C.; Villegas, C.; \n    Mamun, A. S. M. A. and Carrasco, J. M. F. (2025) <doi:10.28951/bjb.v43i1.728>.  "
  },
  {
    "id": 4878,
    "package_name": "MSBStatsData",
    "title": "Data Sets for Courses at the M\u00fcnster School of Business",
    "description": "Provides sample data sets that are used in statistics and data science courses at the M\u00fcnster School of Business. The datasets refer to different business topics but also other domains, e.g. sports, traffic, etc.",
    "version": "0.0.2",
    "maintainer": "Michael B\u00fccker <michael.buecker@fh-muenster.de>",
    "author": "Michael B\u00fccker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0045-8460>),\n  Niels Schl\u00fcsener [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MSBStatsData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSBStatsData Data Sets for Courses at the M\u00fcnster School of Business Provides sample data sets that are used in statistics and data science courses at the M\u00fcnster School of Business. The datasets refer to different business topics but also other domains, e.g. sports, traffic, etc.  "
  },
  {
    "id": 4915,
    "package_name": "MTest",
    "title": "A Procedure for Multicollinearity Testing using Bootstrap",
    "description": "Functions for detecting multicollinearity. This test gives statistical support to two of the most famous methods for detecting multicollinearity in applied work: Klein\u2019s rule and Variance Inflation Factor (VIF). See the URL for the papers associated with this package, as for instance, Morales-O\u00f1ate and Morales-O\u00f1ate (2015) <doi:10.33333/rp.vol51n2.05>.",
    "version": "1.0.4",
    "maintainer": "V\u00edctor Morales-O\u00f1ate <vmorales.ppb@gmail.com>",
    "author": "V\u00edctor Morales-O\u00f1ate [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1922-6571>),\n  Bol\u00edvar Morales-O\u00f1ate [aut] (ORCID:\n    <https://orcid.org/0000-0003-4980-8759>)",
    "url": "https://github.com/vmoprojs/MTest",
    "bug_reports": "https://github.com/vmoprojs/MTest/issues",
    "repository": "https://cran.r-project.org/package=MTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MTest A Procedure for Multicollinearity Testing using Bootstrap Functions for detecting multicollinearity. This test gives statistical support to two of the most famous methods for detecting multicollinearity in applied work: Klein\u2019s rule and Variance Inflation Factor (VIF). See the URL for the papers associated with this package, as for instance, Morales-O\u00f1ate and Morales-O\u00f1ate (2015) <doi:10.33333/rp.vol51n2.05>.  "
  },
  {
    "id": 4961,
    "package_name": "MarZIC",
    "title": "Marginal Mediation Effects with Zero-Inflated Compositional\nMediator",
    "description": "A way to estimate and test marginal mediation effects for \n             zero-inflated compositional mediators. Estimates of Natural Indirect Effect (NIE),\n             Natural Direct Effect (NDE) of each taxon, as well as their standard errors and \n             confident intervals, were provided as outputs. Zeros will not be imputed during \n             analysis. See Wu et al. (2022) <doi:10.3390/genes13061049>. ",
    "version": "1.0.1",
    "maintainer": "Zhigang Li <lzg2151@gmail.com>",
    "author": "Quran Wu [aut],\n  Zhigang Li [aut, cre]",
    "url": "https://www.mdpi.com/2073-4425/13/6/1049",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MarZIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MarZIC Marginal Mediation Effects with Zero-Inflated Compositional\nMediator A way to estimate and test marginal mediation effects for \n             zero-inflated compositional mediators. Estimates of Natural Indirect Effect (NIE),\n             Natural Direct Effect (NDE) of each taxon, as well as their standard errors and \n             confident intervals, were provided as outputs. Zeros will not be imputed during \n             analysis. See Wu et al. (2022) <doi:10.3390/genes13061049>.   "
  },
  {
    "id": 5001,
    "package_name": "MedZIsc",
    "title": "Statistical Framework for Co-Mediators of Zero-Inflated\nSingle-Cell Data",
    "description": "A causal mediation framework for single-cell data that incorporates two key features ('MedZIsc', pronounced Magics): (1) zero-inflation using beta regression and (2) overdispersed expression counts using negative binomial regression. This approach also includes a screening step based on penalized and marginal models to handle high-dimensionality. Full methodological details are available in our recent preprint by Ahn S and Li Z (2025) <doi:10.48550/arXiv.2505.22986>.",
    "version": "0.0.4",
    "maintainer": "Seungjun Ahn <seungjun.ahn@mountsinai.org>",
    "author": "Seungjun Ahn [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4816-8924>),\n  Zhigang Li [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MedZIsc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MedZIsc Statistical Framework for Co-Mediators of Zero-Inflated\nSingle-Cell Data A causal mediation framework for single-cell data that incorporates two key features ('MedZIsc', pronounced Magics): (1) zero-inflation using beta regression and (2) overdispersed expression counts using negative binomial regression. This approach also includes a screening step based on penalized and marginal models to handle high-dimensionality. Full methodological details are available in our recent preprint by Ahn S and Li Z (2025) <doi:10.48550/arXiv.2505.22986>.  "
  },
  {
    "id": 5018,
    "package_name": "MetaIntegration",
    "title": "Ensemble Meta-Prediction Framework",
    "description": "An ensemble meta-prediction framework to integrate multiple regression \n    models into a current study. Gu, T., Taylor, J.M.G. and Mukherjee, B. (2020) \n    <arXiv:2010.09971>.\n    A meta-analysis framework along with two weighted estimators as the ensemble \n    of empirical Bayes estimators, which combines the estimates from the different \n    external models. The proposed framework is flexible and robust in the ways \n    that (i) it is capable of incorporating external models that use a slightly \n    different set of covariates; (ii) it is able to identify the most relevant \n    external information and diminish the influence of information that is less \n    compatible with the internal data; and (iii) it nicely balances the bias-variance \n    trade-off while preserving the most efficiency gain. The proposed estimators \n    are more efficient than the naive analysis of the internal data and other \n    naive combinations of external estimators.",
    "version": "0.1.2",
    "maintainer": "Michael Kleinsasser <mkleinsa@umich.edu>",
    "author": "Tian Gu [aut],\n  Bhramar Mukherjee [aut],\n  Michael Kleinsasser [cre]",
    "url": "https://github.com/umich-biostatistics/MetaIntegration",
    "bug_reports": "https://github.com/umich-biostatistics/MetaIntegration/issues",
    "repository": "https://cran.r-project.org/package=MetaIntegration",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MetaIntegration Ensemble Meta-Prediction Framework An ensemble meta-prediction framework to integrate multiple regression \n    models into a current study. Gu, T., Taylor, J.M.G. and Mukherjee, B. (2020) \n    <arXiv:2010.09971>.\n    A meta-analysis framework along with two weighted estimators as the ensemble \n    of empirical Bayes estimators, which combines the estimates from the different \n    external models. The proposed framework is flexible and robust in the ways \n    that (i) it is capable of incorporating external models that use a slightly \n    different set of covariates; (ii) it is able to identify the most relevant \n    external information and diminish the influence of information that is less \n    compatible with the internal data; and (iii) it nicely balances the bias-variance \n    trade-off while preserving the most efficiency gain. The proposed estimators \n    are more efficient than the naive analysis of the internal data and other \n    naive combinations of external estimators.  "
  },
  {
    "id": 5055,
    "package_name": "MicrobiomeStat",
    "title": "Statistical Methods for Microbiome Compositional Data",
    "description": "A suite of methods for powerful and robust microbiome data analysis addressing zero-inflation, phylogenetic structure and compositional effects (Zhou et al. (2022)<doi:10.1186/s13059-022-02655-5>).  The methods can be applied to the analysis of other (high-dimensional) compositional data arising from sequencing experiments.",
    "version": "1.2",
    "maintainer": "Jun Chen <chen.jun2@mayo.edu>",
    "author": "Xianyang Zhang [aut],\n  Jun Chen [aut, cre],\n  Huijuan Zhou [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MicrobiomeStat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MicrobiomeStat Statistical Methods for Microbiome Compositional Data A suite of methods for powerful and robust microbiome data analysis addressing zero-inflation, phylogenetic structure and compositional effects (Zhou et al. (2022)<doi:10.1186/s13059-022-02655-5>).  The methods can be applied to the analysis of other (high-dimensional) compositional data arising from sequencing experiments.  "
  },
  {
    "id": 5097,
    "package_name": "MoLE",
    "title": "Modeling Language Evolution",
    "description": "Model for simulating language evolution in terms of cultural evolution (Smith & Kirby (2008) <DOI:10.1098/rstb.2008.0145>; Deacon 1997). The focus is on the emergence of argument-marking systems (Dowty (1991) <DOI:10.1353/lan.1991.0021>, Van Valin 1999, Dryer 2002, Lestrade 2015a), i.e. noun marking (Aristar (1997) <DOI:10.1075/sl.21.2.04ari>, Lestrade (2010) <DOI:10.7282/T3ZG6R4S>), person indexing (Ariel 1999, Dahl (2000) <DOI:10.1075/fol.7.1.03dah>, Bhat 2004), and word order (Dryer 2013), but extensions are foreseen. Agents start out with a protolanguage (a language without grammar; Bickerton (1981) <DOI:10.17169/langsci.b91.109>, Jackendoff 2002, Arbib (2015) <DOI:10.1002/9781118346136.ch27>) and interact through language games (Steels 1997). Over time, grammatical constructions emerge that may or may not become obligatory (for which the tolerance principle is assumed; Yang 2016). Throughout the simulation, uniformitarianism of principles is assumed (Hopper (1987) <DOI:10.3765/bls.v13i0.1834>, Givon (1995) <DOI:10.1075/z.74>, Croft (2000), Saffran (2001) <DOI:10.1111/1467-8721.01243>, Heine & Kuteva 2007), in which maximal psychological validity is aimed at (Grice (1975) <DOI:10.1057/9780230005853_5>, Levelt 1989, Gaerdenfors 2000) and language representation is usage based (Tomasello 2003, Bybee 2010). In Lestrade (2015b) <DOI:10.15496/publikation-8640>, Lestrade (2015c) <DOI:10.1075/avt.32.08les>, and Lestrade (2016) <DOI:10.17617/2.2248195>), which reported on the results of preliminary versions, this package was announced as WDWTW (for who does what to whom), but for reasons of pronunciation and generalization the title was changed.",
    "version": "1.0.1",
    "maintainer": "Sander Lestrade <samlestrade@protonmail.com>",
    "author": "Sander Lestrade",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MoLE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MoLE Modeling Language Evolution Model for simulating language evolution in terms of cultural evolution (Smith & Kirby (2008) <DOI:10.1098/rstb.2008.0145>; Deacon 1997). The focus is on the emergence of argument-marking systems (Dowty (1991) <DOI:10.1353/lan.1991.0021>, Van Valin 1999, Dryer 2002, Lestrade 2015a), i.e. noun marking (Aristar (1997) <DOI:10.1075/sl.21.2.04ari>, Lestrade (2010) <DOI:10.7282/T3ZG6R4S>), person indexing (Ariel 1999, Dahl (2000) <DOI:10.1075/fol.7.1.03dah>, Bhat 2004), and word order (Dryer 2013), but extensions are foreseen. Agents start out with a protolanguage (a language without grammar; Bickerton (1981) <DOI:10.17169/langsci.b91.109>, Jackendoff 2002, Arbib (2015) <DOI:10.1002/9781118346136.ch27>) and interact through language games (Steels 1997). Over time, grammatical constructions emerge that may or may not become obligatory (for which the tolerance principle is assumed; Yang 2016). Throughout the simulation, uniformitarianism of principles is assumed (Hopper (1987) <DOI:10.3765/bls.v13i0.1834>, Givon (1995) <DOI:10.1075/z.74>, Croft (2000), Saffran (2001) <DOI:10.1111/1467-8721.01243>, Heine & Kuteva 2007), in which maximal psychological validity is aimed at (Grice (1975) <DOI:10.1057/9780230005853_5>, Levelt 1989, Gaerdenfors 2000) and language representation is usage based (Tomasello 2003, Bybee 2010). In Lestrade (2015b) <DOI:10.15496/publikation-8640>, Lestrade (2015c) <DOI:10.1075/avt.32.08les>, and Lestrade (2016) <DOI:10.17617/2.2248195>), which reported on the results of preliminary versions, this package was announced as WDWTW (for who does what to whom), but for reasons of pronunciation and generalization the title was changed.  "
  },
  {
    "id": 5145,
    "package_name": "MultiATSM",
    "title": "Multicountry Term Structure of Interest Rates Models",
    "description": "Package for estimating, analyzing, and forecasting multi-country macro-finance affine term structure models (ATSMs). All setups build on the single-country unspanned macroeconomic risk framework from Joslin, Priebsch, and Singleton (2014, JF) <doi:10.1111/jofi.12131>. Multicountry extensions by Jotikasthira, Le, and Lundblad (2015, JFE) <doi:10.1016/j.jfineco.2014.09.004>, Candelon and Moura (2023, EM) <doi:10.1016/j.econmod.2023.106453>, and Candelon and Moura (2024, JFEC) <doi:10.1093/jjfinec/nbae008> are also available. The package also provides tools for bias correction as in Bauer Rudebusch and Wu (2012, JBES) <doi:10.1080/07350015.2012.693855>, bootstrap analysis, and several graphical/numerical outputs. ",
    "version": "1.5.1",
    "maintainer": "Rubens Moura <rubens.gtmoura@gmail.com>",
    "author": "Rubens Moura [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8105-4729>)",
    "url": "https://github.com/rubensmoura87/MultiATSM,\nhttps://rubensmoura87.github.io/MultiATSM/",
    "bug_reports": "https://github.com/rubensmoura87/MultiATSM/issues",
    "repository": "https://cran.r-project.org/package=MultiATSM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiATSM Multicountry Term Structure of Interest Rates Models Package for estimating, analyzing, and forecasting multi-country macro-finance affine term structure models (ATSMs). All setups build on the single-country unspanned macroeconomic risk framework from Joslin, Priebsch, and Singleton (2014, JF) <doi:10.1111/jofi.12131>. Multicountry extensions by Jotikasthira, Le, and Lundblad (2015, JFE) <doi:10.1016/j.jfineco.2014.09.004>, Candelon and Moura (2023, EM) <doi:10.1016/j.econmod.2023.106453>, and Candelon and Moura (2024, JFEC) <doi:10.1093/jjfinec/nbae008> are also available. The package also provides tools for bias correction as in Bauer Rudebusch and Wu (2012, JBES) <doi:10.1080/07350015.2012.693855>, bootstrap analysis, and several graphical/numerical outputs.   "
  },
  {
    "id": 5153,
    "package_name": "MultiGroupSequential",
    "title": "Group-Sequential Procedures with Multiple Hypotheses",
    "description": "It is often challenging to strongly control the family-wise type-1 error rate in the group-sequential trials with multiple endpoints (hypotheses). The inflation of type-1 error rate comes from two sources (S1) repeated testing individual hypothesis and (S2) simultaneous testing multiple hypotheses. The 'MultiGroupSequential' package is intended to help researchers to tackle this challenge. The procedures provided include the sequential procedures described in Luo and Quan (2023) <doi:10.1080/19466315.2023.2191989> and the graphical procedure proposed by Maurer and Bretz (2013) <doi:10.1080/19466315.2013.807748>. Luo and Quan (2013) describes three procedures, and the functions to implement these procedures are (1) seqgspgx() implements a sequential graphical procedure based on the group-sequential p-values; (2) seqgsphh() implements a sequential Hochberg/Hommel procedure based on the group-sequential p-values; and (3) seqqvalhh() implements a sequential Hochberg/Hommel procedure based on the q-values. In addition, seqmbgx() implements the sequential graphical procedure described in Maurer and Bretz (2013).",
    "version": "1.1.0",
    "maintainer": "Xiaodong Luo <Xiaodong.Luo@sanofi.com>",
    "author": "Xiaodong Luo [aut, cre],\n  Hui Quan [ctb],\n  Sanofi [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiGroupSequential",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiGroupSequential Group-Sequential Procedures with Multiple Hypotheses It is often challenging to strongly control the family-wise type-1 error rate in the group-sequential trials with multiple endpoints (hypotheses). The inflation of type-1 error rate comes from two sources (S1) repeated testing individual hypothesis and (S2) simultaneous testing multiple hypotheses. The 'MultiGroupSequential' package is intended to help researchers to tackle this challenge. The procedures provided include the sequential procedures described in Luo and Quan (2023) <doi:10.1080/19466315.2023.2191989> and the graphical procedure proposed by Maurer and Bretz (2013) <doi:10.1080/19466315.2013.807748>. Luo and Quan (2013) describes three procedures, and the functions to implement these procedures are (1) seqgspgx() implements a sequential graphical procedure based on the group-sequential p-values; (2) seqgsphh() implements a sequential Hochberg/Hommel procedure based on the group-sequential p-values; and (3) seqqvalhh() implements a sequential Hochberg/Hommel procedure based on the q-values. In addition, seqmbgx() implements the sequential graphical procedure described in Maurer and Bretz (2013).  "
  },
  {
    "id": 5158,
    "package_name": "MultiNMix",
    "title": "Multi-Species N-Mixture (MNM) Models with 'nimble'",
    "description": "Simulating data and fitting multi-species N-mixture models using 'nimble'. Includes features for handling zero-inflation and temporal correlation, Bayesian inference, model diagnostics, parameter estimation, and predictive checks. Designed for ecological studies with zero-altered or time-series data. Mimnagh, N., Parnell, A., Prado, E., & Moral, R. A. (2022) <doi:10.1007/s10651-022-00542-7>. Royle, J. A. (2004) <doi:10.1111/j.0006-341X.2004.00142.x>.",
    "version": "0.1.0",
    "maintainer": "Niamh Mimnagh <niamhmimnagh@gmail.com>",
    "author": "Niamh Mimnagh [aut, cre],\n  Rafael de Andrade Moral [aut]",
    "url": "https://github.com/niamhmimnagh/MultiNMix",
    "bug_reports": "https://github.com/niamhmimnagh/MultiNMix/issues",
    "repository": "https://cran.r-project.org/package=MultiNMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiNMix Multi-Species N-Mixture (MNM) Models with 'nimble' Simulating data and fitting multi-species N-mixture models using 'nimble'. Includes features for handling zero-inflation and temporal correlation, Bayesian inference, model diagnostics, parameter estimation, and predictive checks. Designed for ecological studies with zero-altered or time-series data. Mimnagh, N., Parnell, A., Prado, E., & Moral, R. A. (2022) <doi:10.1007/s10651-022-00542-7>. Royle, J. A. (2004) <doi:10.1111/j.0006-341X.2004.00142.x>.  "
  },
  {
    "id": 5195,
    "package_name": "NBAloveR",
    "title": "Help Basketball Data Analysis",
    "description": "Provides interface to the online basketball data resources such as\n      Basketball reference API <https://www.basketball-reference.com/> and helps\n      R users analyze basketball data.",
    "version": "0.1.3.3",
    "maintainer": "Koki Ando <koki.25.ando@gmail.com>",
    "author": "Koki Ando [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NBAloveR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NBAloveR Help Basketball Data Analysis Provides interface to the online basketball data resources such as\n      Basketball reference API <https://www.basketball-reference.com/> and helps\n      R users analyze basketball data.  "
  },
  {
    "id": 5217,
    "package_name": "NFLSimulatoR",
    "title": "Simulating Plays and Drives in the NFL",
    "description": "The intent here is to enable the simulation of plays/drives and\n    evaluate game-play strategies in the National Football League (NFL).\n    Built-in strategies include going for it on fourth down and varying the \n    proportion of passing/rushing plays during a drive. The user should be\n    familiar with nflscrapR data before trying to write his/her own \n    strategies. This work is inspired by a blog post by Mike Lopez, \n    currently the  Director of Data and Analytics at the NFL, Lopez (2019) <https://statsbylopez.netlify.app/post/resampling-nfl-drives/>.",
    "version": "0.4.0",
    "maintainer": "Ryan Elmore <Ryan.Elmore@du.edu>",
    "author": "Ryan Elmore [cre, aut] (ORCID: <https://orcid.org/0000-0002-0092-4532>),\n  Ben Williams [aut] (ORCID: <https://orcid.org/0000-0001-8474-5066>),\n  Will Palmquist [aut] (ORCID: <https://orcid.org/0000-0002-6100-0923>)",
    "url": "https://github.com/rtelmore/NFLSimulatoR/",
    "bug_reports": "https://github.com/rtelmore/NFLSimulatoR/issues/",
    "repository": "https://cran.r-project.org/package=NFLSimulatoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NFLSimulatoR Simulating Plays and Drives in the NFL The intent here is to enable the simulation of plays/drives and\n    evaluate game-play strategies in the National Football League (NFL).\n    Built-in strategies include going for it on fourth down and varying the \n    proportion of passing/rushing plays during a drive. The user should be\n    familiar with nflscrapR data before trying to write his/her own \n    strategies. This work is inspired by a blog post by Mike Lopez, \n    currently the  Director of Data and Analytics at the NFL, Lopez (2019) <https://statsbylopez.netlify.app/post/resampling-nfl-drives/>.  "
  },
  {
    "id": 5223,
    "package_name": "NHLData",
    "title": "Scores for Every Season Since the Founding of the NHL in 1917",
    "description": "Each dataset contains scores for every game during a specific season of the NHL.",
    "version": "1.0.0",
    "maintainer": "D. Lukke Sweet <dls229@zips.uakron.edu>",
    "author": "D. Lukke Sweet",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NHLData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NHLData Scores for Every Season Since the Founding of the NHL in 1917 Each dataset contains scores for every game during a specific season of the NHL.  "
  },
  {
    "id": 5241,
    "package_name": "NMAoutlier",
    "title": "Detecting Outliers in Network Meta-Analysis",
    "description": "A set of functions providing several outlier (i.e., studies with extreme findings) and influential detection measures and methodologies in network meta-analysis :\n               - simple outlier and influential detection measures\n               - outlier and influential detection measures by considering study deletion (shift the mean)\n               - plots for outlier and influential detection measures\n\t       - Q-Q plot for network meta-analysis\n               - Forward Search algorithm in network meta-analysis. \n               - forward plots to monitor statistics in each step of the forward search algorithm\n               - forward plots for summary estimates and their confidence intervals in each step of forward search algorithm.   ",
    "version": "0.2.1",
    "maintainer": "Maria Petropoulou <m.petropoulou.a@gmail.com>",
    "author": "Maria Petropoulou [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7147-3644>),\n  Guido Schwarzer [aut] (ORCID: <https://orcid.org/0000-0001-6214-9087>),\n  Agapios Panos [aut],\n  Dimitris Mavridis [aut] (ORCID:\n    <https://orcid.org/0000-0003-1041-4592>)",
    "url": "https://github.com/petropouloumaria/NMAoutlier",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NMAoutlier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NMAoutlier Detecting Outliers in Network Meta-Analysis A set of functions providing several outlier (i.e., studies with extreme findings) and influential detection measures and methodologies in network meta-analysis :\n               - simple outlier and influential detection measures\n               - outlier and influential detection measures by considering study deletion (shift the mean)\n               - plots for outlier and influential detection measures\n\t       - Q-Q plot for network meta-analysis\n               - Forward Search algorithm in network meta-analysis. \n               - forward plots to monitor statistics in each step of the forward search algorithm\n               - forward plots for summary estimates and their confidence intervals in each step of forward search algorithm.     "
  },
  {
    "id": 5283,
    "package_name": "NU.Learning",
    "title": "Nonparametric and Unsupervised Learning from Cross-Sectional\nObservational Data",
    "description": "Especially when cross-sectional data are observational, effects of treatment selection\n  bias and confounding are best revealed by using Nonparametric and Unsupervised methods to\n  \"Design\" the analysis of the given data ...rather than the collection of \"designed data\".\n  Specifically, the \"effect-size distribution\" that best quantifies a potentially causal\n  relationship between a numeric y-Outcome variable and either a binary t-Treatment or\n  continuous e-Exposure variable needs to consist of BLOCKS of relatively well-matched\n  experimental units (e.g. patients) that have the most similar X-confounder characteristics.\n  Since our NU Learning approach will form BLOCKS by \"clustering\" experimental units in\n  confounder X-space, the implicit statistical model for learning is One-Way ANOVA. Within\n  Block measures of effect-size are then either [a] LOCAL Treatment Differences (LTDs) between\n  Within-Cluster y-Outcome Means (\"new\" minus \"control\") when treatment choice is\n  Binary or else [b] LOCAL Rank Correlations (LRCs) when the e-Exposure variable is numeric\n  with (hopefully many) more than two levels. An Instrumental Variable (IV) method is also\n  provided so that Local Average y-Outcomes (LAOs) within BLOCKS may also contribute\n  information for effect-size inferences when X-Covariates are assumed to influence Treatment\n  choice or Exposure level but otherwise have no direct effects on y-Outcomes. Finally, a\n  \"Most-Like-Me\" function provides histograms of effect-size distributions to aid\n  Doctor-Patient (or Researcher-Society) communications about Heterogeneous Outcomes.\n  Obenchain and Young (2013) <doi:10.1080/15598608.2013.772821>; Obenchain, Young and Krstic\n  (2019) <doi:10.1016/j.yrtph.2019.104418>.",
    "version": "1.5",
    "maintainer": "Bob Obenchain <wizbob@att.net>",
    "author": "Bob Obenchain [aut, cre],\n  Stan Young [ctb]",
    "url": "https://www.r-project.org, http://localcontrolstatistics.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NU.Learning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NU.Learning Nonparametric and Unsupervised Learning from Cross-Sectional\nObservational Data Especially when cross-sectional data are observational, effects of treatment selection\n  bias and confounding are best revealed by using Nonparametric and Unsupervised methods to\n  \"Design\" the analysis of the given data ...rather than the collection of \"designed data\".\n  Specifically, the \"effect-size distribution\" that best quantifies a potentially causal\n  relationship between a numeric y-Outcome variable and either a binary t-Treatment or\n  continuous e-Exposure variable needs to consist of BLOCKS of relatively well-matched\n  experimental units (e.g. patients) that have the most similar X-confounder characteristics.\n  Since our NU Learning approach will form BLOCKS by \"clustering\" experimental units in\n  confounder X-space, the implicit statistical model for learning is One-Way ANOVA. Within\n  Block measures of effect-size are then either [a] LOCAL Treatment Differences (LTDs) between\n  Within-Cluster y-Outcome Means (\"new\" minus \"control\") when treatment choice is\n  Binary or else [b] LOCAL Rank Correlations (LRCs) when the e-Exposure variable is numeric\n  with (hopefully many) more than two levels. An Instrumental Variable (IV) method is also\n  provided so that Local Average y-Outcomes (LAOs) within BLOCKS may also contribute\n  information for effect-size inferences when X-Covariates are assumed to influence Treatment\n  choice or Exposure level but otherwise have no direct effects on y-Outcomes. Finally, a\n  \"Most-Like-Me\" function provides histograms of effect-size distributions to aid\n  Doctor-Patient (or Researcher-Society) communications about Heterogeneous Outcomes.\n  Obenchain and Young (2013) <doi:10.1080/15598608.2013.772821>; Obenchain, Young and Krstic\n  (2019) <doi:10.1016/j.yrtph.2019.104418>.  "
  },
  {
    "id": 5311,
    "package_name": "NetMix",
    "title": "Dynamic Mixed-Membership Network Regression Model",
    "description": "Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,\n             incorporating node-level predictors of mixed-membership vectors, as well as \n             dyad-level predictors. For networks observed over time, the model defines a hidden\n             Markov process that allows the effects of node-level predictors to evolve in discrete,\n             historical periods. In addition, the package offers a variety of utilities for \n             exploring results of estimation, including tools for conducting posterior \n             predictive checks of goodness-of-fit and several plotting functions. The package \n             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic\n             Blockmodel Regression for Social Networks: Application to International Conflicts',\n             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.",
    "version": "0.2.0.3",
    "maintainer": "Santiago Olivella <olivella@unc.edu>",
    "author": "Santiago Olivella [aut, cre],\n  Adeline Lo [aut],\n  Tyler Pratt [aut],\n  Kosuke Imai [ctb]",
    "url": "",
    "bug_reports": "https://github.com/solivella/NetMix/issues",
    "repository": "https://cran.r-project.org/package=NetMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetMix Dynamic Mixed-Membership Network Regression Model Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,\n             incorporating node-level predictors of mixed-membership vectors, as well as \n             dyad-level predictors. For networks observed over time, the model defines a hidden\n             Markov process that allows the effects of node-level predictors to evolve in discrete,\n             historical periods. In addition, the package offers a variety of utilities for \n             exploring results of estimation, including tools for conducting posterior \n             predictive checks of goodness-of-fit and several plotting functions. The package \n             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic\n             Blockmodel Regression for Social Networks: Application to International Conflicts',\n             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.  "
  },
  {
    "id": 5317,
    "package_name": "NetVAR",
    "title": "Network Structures in VAR Models",
    "description": "Vector AutoRegressive (VAR) type models with tailored regularisation structures are provided to uncover network type structures in the data, such as influential time series (influencers). Currently the package implements the LISAR model from Zhang and Trimborn (2023) <doi:10.2139/ssrn.4619531>. The package automatically derives the required regularisation sequences and refines it during the estimation to provide the optimal model. The package allows for model optimisation under various loss functions such as Mean Squared Forecasting Error (MSFE), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). It provides a dedicated class, allowing for summary prints of the optimal model and a plotting function to conveniently analyse the optimal model via heatmaps.",
    "version": "0.1-2",
    "maintainer": "Simon Trimborn <trimborn.econometrics@gmail.com>",
    "author": "Simon Trimborn [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NetVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetVAR Network Structures in VAR Models Vector AutoRegressive (VAR) type models with tailored regularisation structures are provided to uncover network type structures in the data, such as influential time series (influencers). Currently the package implements the LISAR model from Zhang and Trimborn (2023) <doi:10.2139/ssrn.4619531>. The package automatically derives the required regularisation sequences and refines it during the estimation to provide the optimal model. The package allows for model optimisation under various loss functions such as Mean Squared Forecasting Error (MSFE), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). It provides a dedicated class, allowing for summary prints of the optimal model and a plotting function to conveniently analyse the optimal model via heatmaps.  "
  },
  {
    "id": 5331,
    "package_name": "NeuroDataSets",
    "title": "A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets",
    "description": "Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.",
    "version": "0.3.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/neurodatasets,\nhttps://lightbluetitan.github.io/neurodatasets/",
    "bug_reports": "https://github.com/lightbluetitan/neurodatasets/issues",
    "repository": "https://cran.r-project.org/package=NeuroDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NeuroDataSets A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.  "
  },
  {
    "id": 5348,
    "package_name": "NonParRolCor",
    "title": "a Non-Parametric Statistical Significance Test for Rolling\nWindow Correlation",
    "description": "Estimates and plots (as a single plot and as a heat map) the rolling window correlation coefficients between two time series and computes their statistical significance, which is carried out through a non-parametric computing-intensive method. This method addresses the effects due to the multiple testing (inflation of the Type I error) when the statistical significance is estimated for the rolling window correlation coefficients. The method is based on Monte Carlo simulations by permuting one of the variables (e.g., the dependent) under analysis and keeping fixed the other variable (e.g., the independent). We improve the computational efficiency of this method to reduce the computation time through parallel computing. The 'NonParRolCor' package also provides examples with synthetic and real-life environmental time series to exemplify its use. Methods derived from R. Telford (2013) <https://quantpalaeo.wordpress.com/2013/01/04/> and J.M. Polanco-Martinez and J.L. Lopez-Martinez (2021) <doi:10.1016/j.ecoinf.2021.101379>.",
    "version": "0.8.0",
    "maintainer": "Josue M. Polanco-Martinez <josue.m.polanco@gmail.com>",
    "author": "Josue M. Polanco-Martinez [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-7164-0185>),\n  Jose L. Lopez-Martinez [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2489-7559>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NonParRolCor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NonParRolCor a Non-Parametric Statistical Significance Test for Rolling\nWindow Correlation Estimates and plots (as a single plot and as a heat map) the rolling window correlation coefficients between two time series and computes their statistical significance, which is carried out through a non-parametric computing-intensive method. This method addresses the effects due to the multiple testing (inflation of the Type I error) when the statistical significance is estimated for the rolling window correlation coefficients. The method is based on Monte Carlo simulations by permuting one of the variables (e.g., the dependent) under analysis and keeping fixed the other variable (e.g., the independent). We improve the computational efficiency of this method to reduce the computation time through parallel computing. The 'NonParRolCor' package also provides examples with synthetic and real-life environmental time series to exemplify its use. Methods derived from R. Telford (2013) <https://quantpalaeo.wordpress.com/2013/01/04/> and J.M. Polanco-Martinez and J.L. Lopez-Martinez (2021) <doi:10.1016/j.ecoinf.2021.101379>.  "
  },
  {
    "id": 5358,
    "package_name": "NoviceDeveloperResources",
    "title": "Resources to Assist Novice Developers",
    "description": "Assist novice developers when preparing a single package or a set of integrated packages to submit to CRAN. Automate the following individual or batch processing: check local source packages;  build local .tar.gz source files; install packages from local .tar.gz files; detect conflicts between function names in the environment.",
    "version": "1.2.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NoviceDeveloperResources",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NoviceDeveloperResources Resources to Assist Novice Developers Assist novice developers when preparing a single package or a set of integrated packages to submit to CRAN. Automate the following individual or batch processing: check local source packages;  build local .tar.gz source files; install packages from local .tar.gz files; detect conflicts between function names in the environment.  "
  },
  {
    "id": 5359,
    "package_name": "NoviceDeveloperResources2",
    "title": "Further Resources to Assist Novice Developers",
    "description": "Assist novice developers when preparing a single package or a set of integrated packages to submit to CRAN. Provide additional resources to facilitate the automation of the following individual or batch processing: check local source packages;  build local .tar.gz source files; install packages from local .tar.gz files; detect conflicts between function names in the environment. The additional resources include determining the identity and ordering of the packages to process when updating an imported package.",
    "version": "1.1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NoviceDeveloperResources2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NoviceDeveloperResources2 Further Resources to Assist Novice Developers Assist novice developers when preparing a single package or a set of integrated packages to submit to CRAN. Provide additional resources to facilitate the automation of the following individual or batch processing: check local source packages;  build local .tar.gz source files; install packages from local .tar.gz files; detect conflicts between function names in the environment. The additional resources include determining the identity and ordering of the packages to process when updating an imported package.  "
  },
  {
    "id": 5360,
    "package_name": "NumericEnsembles",
    "title": "Automatically Runs 18 Individual and 14 Ensembles of Models",
    "description": "Automatically runs 18 individual models and 14 ensembles on numeric data, for a total of 32 models. The package automatically returns complete results on all 32 models,\n    30 charts and six tables. The user simply provides the tidy data, and answers a few questions (for example, how many times would you like to resample the data).\n    From there the package randomly splits the data into train, test and validation sets, fits each of models on the training data, makes predictions on the test and validation sets,\n    measures root mean squared error (RMSE), removes features above a user-set level of Variance Inflation Factor, and has several optional features including scaling\n    all numeric data, four different ways to handle strings in the data. Perhaps the most significant feature is the package's ability to make predictions\n    using the 32 pre trained models on totally new (untrained) data if the user selects that feature. This feature alone represents a very effective solution\n    to the issue of reproducibility of models in data science. The package can also randomly resample the data as many times as the user sets, thus giving more\n    accurate results than a single run. The graphs provide many results that are not typically found. For example, the package automatically calculates the Kolmogorov-Smirnov\n    test for each of the 32 models and plots a bar chart of the results, a bias bar chart of each of the 32 models, as well as several plots for exploratory data\n    analysis (automatic histograms of the numeric data, automatic histograms of the numeric data). The package also automatically creates a summary report\n    that can be both sorted and searched for each of the 32 models, including RMSE, bias, train RMSE, test RMSE, validation RMSE, overfitting and duration.\n    The best results on the holdout data typically beat the best results in data science competitions and published results for the same data set.",
    "version": "0.10.3",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "http://www.NumericEnsembles.com,\nhttps://github.com/InfiniteCuriosity/NumericEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/NumericEnsembles/issues",
    "repository": "https://cran.r-project.org/package=NumericEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NumericEnsembles Automatically Runs 18 Individual and 14 Ensembles of Models Automatically runs 18 individual models and 14 ensembles on numeric data, for a total of 32 models. The package automatically returns complete results on all 32 models,\n    30 charts and six tables. The user simply provides the tidy data, and answers a few questions (for example, how many times would you like to resample the data).\n    From there the package randomly splits the data into train, test and validation sets, fits each of models on the training data, makes predictions on the test and validation sets,\n    measures root mean squared error (RMSE), removes features above a user-set level of Variance Inflation Factor, and has several optional features including scaling\n    all numeric data, four different ways to handle strings in the data. Perhaps the most significant feature is the package's ability to make predictions\n    using the 32 pre trained models on totally new (untrained) data if the user selects that feature. This feature alone represents a very effective solution\n    to the issue of reproducibility of models in data science. The package can also randomly resample the data as many times as the user sets, thus giving more\n    accurate results than a single run. The graphs provide many results that are not typically found. For example, the package automatically calculates the Kolmogorov-Smirnov\n    test for each of the 32 models and plots a bar chart of the results, a bias bar chart of each of the 32 models, as well as several plots for exploratory data\n    analysis (automatic histograms of the numeric data, automatic histograms of the numeric data). The package also automatically creates a summary report\n    that can be both sorted and searched for each of the 32 models, including RMSE, bias, train RMSE, test RMSE, validation RMSE, overfitting and duration.\n    The best results on the holdout data typically beat the best results in data science competitions and published results for the same data set.  "
  },
  {
    "id": 5431,
    "package_name": "OlympicRshiny",
    "title": "'Shiny' Application for Olympic Data",
    "description": "'Shiny' Application to visualize Olympic Data. From 1896 to\n    2016. Even Winter Olympics events are included. Data is from Kaggle at\n    <https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results>.",
    "version": "1.0.2",
    "maintainer": "Amalan Mahendran <amalan0595@gmail.com>",
    "author": "Amalan Mahendran [cre, aut]",
    "url": "https://github.com/Amalan-ConStat/OlympicRshiny,https://amalan-con-stat.shinyapps.io/OlympicRshiny/,https://amalan-con-stat.shinyapps.io/olympic/",
    "bug_reports": "https://github.com/Amalan-ConStat/OlympicRshiny/issues",
    "repository": "https://cran.r-project.org/package=OlympicRshiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OlympicRshiny 'Shiny' Application for Olympic Data 'Shiny' Application to visualize Olympic Data. From 1896 to\n    2016. Even Winter Olympics events are included. Data is from Kaggle at\n    <https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results>.  "
  },
  {
    "id": 5450,
    "package_name": "Oncofilterfast",
    "title": "Aids in the Analysis of Genes Influencing Cancer Survival",
    "description": "Aids in the analysis of genes influencing cancer survival by including a principal function, calculator(), which calculates the P-value for each provided gene under the optimal cutoff in cancer survival studies. Grounded in methodologies from significant works, this package references Therneau's 'survival' package (Therneau, 2024; <https://CRAN.R-project.org/package=survival>) and the survival analysis extensions by Therneau and Grambsch (2000, ISBN 0-387-98784-3). It also integrates the 'survminer' package by Kassambara et al. (2021; <https://CRAN.R-project.org/package=survminer>), enhancing survival curve visualizations with 'ggplot2'.",
    "version": "1.0.0",
    "maintainer": "Pheonix Chen <shuaiyuchen4@gmail.com>",
    "author": "Pheonix Chen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0747-0681>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Oncofilterfast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Oncofilterfast Aids in the Analysis of Genes Influencing Cancer Survival Aids in the analysis of genes influencing cancer survival by including a principal function, calculator(), which calculates the P-value for each provided gene under the optimal cutoff in cancer survival studies. Grounded in methodologies from significant works, this package references Therneau's 'survival' package (Therneau, 2024; <https://CRAN.R-project.org/package=survival>) and the survival analysis extensions by Therneau and Grambsch (2000, ISBN 0-387-98784-3). It also integrates the 'survminer' package by Kassambara et al. (2021; <https://CRAN.R-project.org/package=survminer>), enhancing survival curve visualizations with 'ggplot2'.  "
  },
  {
    "id": 5461,
    "package_name": "OpEnHiMR",
    "title": "Optimization Based Ensemble Model for Prediction of Histone\nModifications in Rice",
    "description": "The comprehensive knowledge of epigenetic modifications in plants, encompassing histone modifications in regulating gene expression, is not completely ingrained. It is noteworthy that histone deacetylation and histone H3 lysine 27 trimethylation (H3K27me3) play a role in repressing transcription in eukaryotes. In contrast, histone acetylation (H3K9ac) and H3K4me3 have been inevitably linked to the stimulation of gene expression, which significantly influences plant development and plays a role in plant responses to biotic and abiotic stresses. To our knowledge this the first multiclass classifier for predicting histone modification in plants. <doi:10.1186/s12864-019-5489-4>.",
    "version": "0.1.1",
    "maintainer": "Dipro Sinha <diprosinha@gmail.com>",
    "author": "Dipro Sinha [aut, cre],\n  Sneha Murmu [aut],\n  Girish Kumar Jha [aut],\n  Md Yeasin [aut],\n  Saikath Das [aut],\n  Sougata Bhattacharjee [aut],\n  Dwijesh Chandra Mishra [aut],\n  Neeraj Budhlakoti [aut],\n  Sudhir Srivastava [aut],\n  Sunil Archak [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OpEnHiMR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpEnHiMR Optimization Based Ensemble Model for Prediction of Histone\nModifications in Rice The comprehensive knowledge of epigenetic modifications in plants, encompassing histone modifications in regulating gene expression, is not completely ingrained. It is noteworthy that histone deacetylation and histone H3 lysine 27 trimethylation (H3K27me3) play a role in repressing transcription in eukaryotes. In contrast, histone acetylation (H3K9ac) and H3K4me3 have been inevitably linked to the stimulation of gene expression, which significantly influences plant development and plays a role in plant responses to biotic and abiotic stresses. To our knowledge this the first multiclass classifier for predicting histone modification in plants. <doi:10.1186/s12864-019-5489-4>.  "
  },
  {
    "id": 5627,
    "package_name": "PLreg",
    "title": "Power Logit Regression for Modeling Bounded Data",
    "description": "Power logit regression models for bounded\n  continuous data, in which the density generator may be normal, Student-t, \n  power exponential, slash, hyperbolic, sinh-normal, or type II logistic. \n  Diagnostic tools associated with the fitted model, such as the residuals, \n  local influence measures, leverage measures, and goodness-of-fit statistics,\n  are implemented. The estimation process follows the maximum likelihood approach\n  and, currently, the package supports two types of estimators: the usual maximum \n  likelihood estimator and the penalized maximum likelihood estimator. More details\n  about power logit regression models are described in \n  Queiroz and Ferrari (2022) <arXiv:2202.01697>.",
    "version": "0.4.1",
    "maintainer": "Felipe Queiroz <ffelipeq@outlook.com>",
    "author": "Felipe Queiroz [aut, cre],\n  Silvia Ferrari [aut]",
    "url": "https://github.com/ffqueiroz/PLreg",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PLreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PLreg Power Logit Regression for Modeling Bounded Data Power logit regression models for bounded\n  continuous data, in which the density generator may be normal, Student-t, \n  power exponential, slash, hyperbolic, sinh-normal, or type II logistic. \n  Diagnostic tools associated with the fitted model, such as the residuals, \n  local influence measures, leverage measures, and goodness-of-fit statistics,\n  are implemented. The estimation process follows the maximum likelihood approach\n  and, currently, the package supports two types of estimators: the usual maximum \n  likelihood estimator and the penalized maximum likelihood estimator. More details\n  about power logit regression models are described in \n  Queiroz and Ferrari (2022) <arXiv:2202.01697>.  "
  },
  {
    "id": 5631,
    "package_name": "PMCMRplus",
    "title": "Calculate Pairwise Multiple Comparisons of Mean Rank Sums\nExtended",
    "description": "For one-way layout experiments the one-way ANOVA can\n\t     be performed as an omnibus test. All-pairs multiple comparisons \n\t     tests (Tukey-Kramer test, Scheffe test, LSD-test) \n\t     and many-to-one tests (Dunnett test) for normally distributed \n\t     residuals and equal within variance are available. Furthermore,\n\t     all-pairs tests (Games-Howell test, Tamhane's T2 test, \n\t     Dunnett T3 test, Ury-Wiggins-Hochberg test) and many-to-one\n\t     (Tamhane-Dunnett Test) for normally distributed residuals \n\t     and heterogeneous variances are provided. Van der Waerden's normal\n\t     scores test for omnibus, all-pairs and many-to-one tests is\n\t     provided for non-normally distributed residuals and homogeneous\n\t     variances. The Kruskal-Wallis, BWS and Anderson-Darling\n\t     omnibus test and all-pairs tests\n\t     (Nemenyi test, Dunn test, Conover test, Dwass-Steele-Critchlow-\n\t     Fligner test) as well as many-to-one (Nemenyi test, Dunn test,\n\t     U-test) are given for the analysis of variance by ranks. \n             Non-parametric trend tests (Jonckheere test, Cuzick test,\n\t     Johnson-Mehrotra test, Spearman test) are included. \n\t     In addition, a Friedman-test for one-way ANOVA with repeated \n\t     measures on ranks (CRBD) and Skillings-Mack test for unbalanced \n\t     CRBD is provided with consequent all-pairs tests (Nemenyi test, \n\t     Siegel test, Miller test, Conover test, Exact test)\n\t     and many-to-one tests (Nemenyi test, Demsar test, Exact test). \n\t     A trend can be tested with Pages's test. Durbin's test \n\t     for a two-way balanced incomplete block design (BIBD) is given \n\t     in this package as well as Gore's test for CRBD with multiple\n\t     observations per cell is given.  Outlier tests, Mandel's k- and\n\t     h statistic as well as functions for Type I error and Power \n\t     analysis as well as generic summary, print and plot methods \n             are provided.",
    "version": "1.9.12",
    "maintainer": "Thorsten Pohlert <thorsten.pohlert@gmx.de>",
    "author": "Thorsten Pohlert [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3855-3025>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PMCMRplus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PMCMRplus Calculate Pairwise Multiple Comparisons of Mean Rank Sums\nExtended For one-way layout experiments the one-way ANOVA can\n\t     be performed as an omnibus test. All-pairs multiple comparisons \n\t     tests (Tukey-Kramer test, Scheffe test, LSD-test) \n\t     and many-to-one tests (Dunnett test) for normally distributed \n\t     residuals and equal within variance are available. Furthermore,\n\t     all-pairs tests (Games-Howell test, Tamhane's T2 test, \n\t     Dunnett T3 test, Ury-Wiggins-Hochberg test) and many-to-one\n\t     (Tamhane-Dunnett Test) for normally distributed residuals \n\t     and heterogeneous variances are provided. Van der Waerden's normal\n\t     scores test for omnibus, all-pairs and many-to-one tests is\n\t     provided for non-normally distributed residuals and homogeneous\n\t     variances. The Kruskal-Wallis, BWS and Anderson-Darling\n\t     omnibus test and all-pairs tests\n\t     (Nemenyi test, Dunn test, Conover test, Dwass-Steele-Critchlow-\n\t     Fligner test) as well as many-to-one (Nemenyi test, Dunn test,\n\t     U-test) are given for the analysis of variance by ranks. \n             Non-parametric trend tests (Jonckheere test, Cuzick test,\n\t     Johnson-Mehrotra test, Spearman test) are included. \n\t     In addition, a Friedman-test for one-way ANOVA with repeated \n\t     measures on ranks (CRBD) and Skillings-Mack test for unbalanced \n\t     CRBD is provided with consequent all-pairs tests (Nemenyi test, \n\t     Siegel test, Miller test, Conover test, Exact test)\n\t     and many-to-one tests (Nemenyi test, Demsar test, Exact test). \n\t     A trend can be tested with Pages's test. Durbin's test \n\t     for a two-way balanced incomplete block design (BIBD) is given \n\t     in this package as well as Gore's test for CRBD with multiple\n\t     observations per cell is given.  Outlier tests, Mandel's k- and\n\t     h statistic as well as functions for Type I error and Power \n\t     analysis as well as generic summary, print and plot methods \n             are provided.  "
  },
  {
    "id": 5642,
    "package_name": "PODES",
    "title": "Village Potential Statistics of Indonesia",
    "description": "Village potential statistics (PODES) collects various information on village potential and challenges faced by villages in Indonesia. Information related to village potential includes economy, security, health, employment, communication and information, sports, entertainment, development, community empowerment, education, socio-culture, transportation in the village. Information related to challenges includes natural disasters, public health, environmental pollution, social problems and security disturbances that occur in the village.",
    "version": "0.1.0",
    "maintainer": "Fadhlul Mubarak <mubarakfadhlul@gmail.com>",
    "author": "Fadhlul Mubarak [aut, cre],\n  Nurniswah [aut],\n  Vinny Yuliani Sundara [aut],\n  Belya Efrina [aut],\n  Chrisella Anastasya Hutagalung [aut],\n  Desta Tiurmaida Hutagaol [aut],\n  Habibi Yansyah [aut],\n  Laura Zararah Arjha [aut],\n  Nur Fitri Patricia [aut],\n  Fakhrur Razi [aut],\n  Riri Oktari Ulma [aut],\n  Ahmad Syukron Prasaja [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PODES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PODES Village Potential Statistics of Indonesia Village potential statistics (PODES) collects various information on village potential and challenges faced by villages in Indonesia. Information related to village potential includes economy, security, health, employment, communication and information, sports, entertainment, development, community empowerment, education, socio-culture, transportation in the village. Information related to challenges includes natural disasters, public health, environmental pollution, social problems and security disturbances that occur in the village.  "
  },
  {
    "id": 5653,
    "package_name": "POV",
    "title": "Partition of Variation Variance Component Analysis Method",
    "description": "An implementation of the Partition Of variation (POV) method as\n    developed by Dr. Thomas A Little <https://thomasalittleconsulting.com> in\n    1993 for the analysis of semiconductor data for hard drive manufacturing.\n    POV is based on sequential sum of squares and is an exact method that\n    explains all observed variation. It quantitates both the between and within\n    factor variation effects and can quantitate the influence of both continuous\n    and categorical factors.",
    "version": "0.1.4",
    "maintainer": "Paul Deen <paulext@gmail.com>",
    "author": "Paul Deen [aut, cre]",
    "url": "https://github.com/PaulAntonDeen/POV-R-Package,\nhttps://thomasalittleconsulting.com",
    "bug_reports": "https://github.com/PaulAntonDeen/POV-R-Package/issues",
    "repository": "https://cran.r-project.org/package=POV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "POV Partition of Variation Variance Component Analysis Method An implementation of the Partition Of variation (POV) method as\n    developed by Dr. Thomas A Little <https://thomasalittleconsulting.com> in\n    1993 for the analysis of semiconductor data for hard drive manufacturing.\n    POV is based on sequential sum of squares and is an exact method that\n    explains all observed variation. It quantitates both the between and within\n    factor variation effects and can quantitate the influence of both continuous\n    and categorical factors.  "
  },
  {
    "id": 5692,
    "package_name": "PSAboot",
    "title": "Bootstrapping for Propensity Score Analysis",
    "description": "It is often advantageous to test a hypothesis more than once\n    in the context of propensity score analysis (Rosenbaum, 2012)\n    <doi:10.1093/biomet/ass032>. The functions in this package facilitate\n    bootstrapping for propensity score analysis (PSA). By default,\n    bootstrapping using two classification tree methods (using 'rpart' and\n    'ctree' functions), two matching methods (using 'Matching' and\n    'MatchIt' packages), and stratification with logistic regression.  A\n    framework is described for users to implement additional propensity\n    score methods.  Visualizations are emphasized for diagnosing balance;\n    exploring the correlation relationships between bootstrap samples and\n    methods; and to summarize results.",
    "version": "1.3.9",
    "maintainer": "Jason Bryer <jason@bryer.org>",
    "author": "Jason Bryer [aut, cre] (ORCID: <https://orcid.org/0000-0002-2454-0402>)",
    "url": "https://github.com/jbryer/PSAboot",
    "bug_reports": "https://github.com/jbryer/PSAboot/issues",
    "repository": "https://cran.r-project.org/package=PSAboot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PSAboot Bootstrapping for Propensity Score Analysis It is often advantageous to test a hypothesis more than once\n    in the context of propensity score analysis (Rosenbaum, 2012)\n    <doi:10.1093/biomet/ass032>. The functions in this package facilitate\n    bootstrapping for propensity score analysis (PSA). By default,\n    bootstrapping using two classification tree methods (using 'rpart' and\n    'ctree' functions), two matching methods (using 'Matching' and\n    'MatchIt' packages), and stratification with logistic regression.  A\n    framework is described for users to implement additional propensity\n    score methods.  Visualizations are emphasized for diagnosing balance;\n    exploring the correlation relationships between bootstrap samples and\n    methods; and to summarize results.  "
  },
  {
    "id": 5705,
    "package_name": "PSS.Health",
    "title": "Power and Sample Size for Health Researchers via Shiny",
    "description": "Power and Sample Size for Health Researchers is a Shiny application that brings together a \n    series of functions  related to sample size and power calculations for common analysis in the healthcare \n    field. There are functionalities to calculate the power, sample size to estimate or test hypotheses for means and \n    proportions (including test for correlated groups, equivalence, non-inferiority and superiority), association, correlations coefficients, \n    regression coefficients (linear, logistic, gamma, and Cox), linear mixed model, \n    Cronbach's alpha, interobserver agreement, intraclass correlation coefficients,\n    limit of agreement on Bland-Altman plots,\n    area under the curve, sensitivity and specificity incorporating the prevalence of disease. \n    You can also use the online version at <https://hcpa-unidade-bioestatistica.shinyapps.io/PSS_Health/>.",
    "version": "1.1.5",
    "maintainer": "Rog\u00e9rio Boff Borges <roborges@hcpa.edu.br>",
    "author": "Rog\u00e9rio Boff Borges [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2548-1889>),\n  Guilherme Serpa Azambuja [aut] (ORCID:\n    <https://orcid.org/0000-0001-6246-4538>),\n  Aline Castello Branco Mancuso [aut] (ORCID:\n    <https://orcid.org/0000-0001-6033-8335>),\n  Vanessa Bielefeldt Leotti [aut] (ORCID:\n    <https://orcid.org/0000-0003-3860-9367>),\n  V\u00e2nia Naomi Hirakata [aut] (ORCID:\n    <https://orcid.org/0000-0003-4645-2080>),\n  Suzi Alves Camey [aut] (ORCID: <https://orcid.org/0000-0002-5564-081X>),\n  Stela Maris de Jezus Castro [aut] (ORCID:\n    <https://orcid.org/0000-0001-5862-6709>),\n  Gustavo Thomas [aut] (ORCID: <https://orcid.org/0000-0002-4327-8307>),\n  Hospital de Cl\u00ednicas de Porto Alegre [fnd]",
    "url": "https://hcpa-unidade-bioestatistica.shinyapps.io/PSS_Health/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PSS.Health",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PSS.Health Power and Sample Size for Health Researchers via Shiny Power and Sample Size for Health Researchers is a Shiny application that brings together a \n    series of functions  related to sample size and power calculations for common analysis in the healthcare \n    field. There are functionalities to calculate the power, sample size to estimate or test hypotheses for means and \n    proportions (including test for correlated groups, equivalence, non-inferiority and superiority), association, correlations coefficients, \n    regression coefficients (linear, logistic, gamma, and Cox), linear mixed model, \n    Cronbach's alpha, interobserver agreement, intraclass correlation coefficients,\n    limit of agreement on Bland-Altman plots,\n    area under the curve, sensitivity and specificity incorporating the prevalence of disease. \n    You can also use the online version at <https://hcpa-unidade-bioestatistica.shinyapps.io/PSS_Health/>.  "
  },
  {
    "id": 5709,
    "package_name": "PScr",
    "title": "Estimation for the Power Series Cure Rate Model",
    "description": "Provide estimation for particular cases of the power series cure rate model \n             <doi:10.1080/03610918.2011.639971>. For the distribution of the concurrent causes the \n             alternative models are the Poisson, logarithmic, negative binomial and Bernoulli (which \n             are includes in the original work), the polylogarithm model \n             <doi:10.1080/00949655.2018.1451850> and the Flory-Schulz <doi:10.3390/math10244643>. \n             The estimation procedure is based on the EM algorithm discussed in \n             <doi:10.1080/03610918.2016.1202276>.\n             For the distribution of the time-to-event the alternative models are slash half-normal, \n             Weibull, gamma and Birnbaum-Saunders distributions.",
    "version": "1.1",
    "maintainer": "Diego Gallardo <diego.gallardo.mateluna@gmail.com>",
    "author": "Diego Gallardo [aut, cre],\n  Reza Azimi [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PScr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PScr Estimation for the Power Series Cure Rate Model Provide estimation for particular cases of the power series cure rate model \n             <doi:10.1080/03610918.2011.639971>. For the distribution of the concurrent causes the \n             alternative models are the Poisson, logarithmic, negative binomial and Bernoulli (which \n             are includes in the original work), the polylogarithm model \n             <doi:10.1080/00949655.2018.1451850> and the Flory-Schulz <doi:10.3390/math10244643>. \n             The estimation procedure is based on the EM algorithm discussed in \n             <doi:10.1080/03610918.2016.1202276>.\n             For the distribution of the time-to-event the alternative models are slash half-normal, \n             Weibull, gamma and Birnbaum-Saunders distributions.  "
  },
  {
    "id": 5739,
    "package_name": "PakNAcc",
    "title": "'shiny' App for National Accounts",
    "description": "Provides a comprehensive suite of tools for analyzing Pakistan's Quarterly National Accounts data. Users can gain detailed insights into Pakistan's economic performance, visualize quarterly trends, and detect patterns and anomalies in key economic indicators. Compare sector contributions\u2014including agriculture, industry, and services\u2014to understand their influence on economic growth or decline. Customize analyses by filtering and manipulating data to focus on specific areas of interest. Ideal for policymakers, researchers, and analysts aiming to make informed, data-driven decisions based on timely and detailed economic insights.",
    "version": "0.3.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5923-1714>),\n  Zahid Asghar [ctb]",
    "url": "https://myaseen208.com/PakNAcc/\nhttps://myaseen208.shinyapps.io/PakNAcc/\nhttps://CRAN.R-project.org/package=PakNAcc",
    "bug_reports": "https://github.com/myaseen208/PakNAcc/issues",
    "repository": "https://cran.r-project.org/package=PakNAcc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakNAcc 'shiny' App for National Accounts Provides a comprehensive suite of tools for analyzing Pakistan's Quarterly National Accounts data. Users can gain detailed insights into Pakistan's economic performance, visualize quarterly trends, and detect patterns and anomalies in key economic indicators. Compare sector contributions\u2014including agriculture, industry, and services\u2014to understand their influence on economic growth or decline. Customize analyses by filtering and manipulating data to focus on specific areas of interest. Ideal for policymakers, researchers, and analysts aiming to make informed, data-driven decisions based on timely and detailed economic insights.  "
  },
  {
    "id": 5767,
    "package_name": "Paris2024Colours",
    "title": "Color Palettes Inspired by Paris 2024 Olympic and Paralympic\nGames",
    "description": "Palettes inspired by Paris 2024 Olympic and Paralympic Games for \n    data visualizations. Length of color palettes is configurable.",
    "version": "0.2.0",
    "maintainer": "Maxime Kuntz <maxime.kuntz75@gmail.com>",
    "author": "Maxime Kuntz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2068-904X>)",
    "url": "https://github.com/maximekuntz/Paris2024Colours,\nhttps://maximekuntz.github.io/Paris2024Colours/",
    "bug_reports": "https://github.com/maximekuntz/Paris2024Colours/issues",
    "repository": "https://cran.r-project.org/package=Paris2024Colours",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Paris2024Colours Color Palettes Inspired by Paris 2024 Olympic and Paralympic\nGames Palettes inspired by Paris 2024 Olympic and Paralympic Games for \n    data visualizations. Length of color palettes is configurable.  "
  },
  {
    "id": 5768,
    "package_name": "PartCensReg",
    "title": "Estimation and Diagnostics for Partially Linear Censored\nRegression Models Based on Heavy-Tailed Distributions",
    "description": "It estimates the parameters of a partially linear regression censored model via maximum penalized likelihood through of ECME algorithm. The model belong to the semiparametric class, that including a parametric and nonparametric component. The error term considered belongs to the scale-mixture of normal (SMN) distribution, that includes well-known heavy tails distributions as the Student-t distribution, among others. To examine the performance of the fitted model, case-deletion and local influence techniques are provided to show its robust aspect against outlying and influential observations. This work is based in Ferreira, C. S., & Paula, G. A. (2017) <doi:10.1080/02664763.2016.1267124> but considering the SMN family.",
    "version": "1.39",
    "maintainer": "Marcela Nunez Lemus <marcela.nunez.lemus@gmail.com>",
    "author": "Marcela Nunez Lemus, Christian E. Galarza, Larissa Avila Matos, Victor H Lachos",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PartCensReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PartCensReg Estimation and Diagnostics for Partially Linear Censored\nRegression Models Based on Heavy-Tailed Distributions It estimates the parameters of a partially linear regression censored model via maximum penalized likelihood through of ECME algorithm. The model belong to the semiparametric class, that including a parametric and nonparametric component. The error term considered belongs to the scale-mixture of normal (SMN) distribution, that includes well-known heavy tails distributions as the Student-t distribution, among others. To examine the performance of the fitted model, case-deletion and local influence techniques are provided to show its robust aspect against outlying and influential observations. This work is based in Ferreira, C. S., & Paula, G. A. (2017) <doi:10.1080/02664763.2016.1267124> but considering the SMN family.  "
  },
  {
    "id": 5772,
    "package_name": "PathwaySpace",
    "title": "Spatial Projection of Network Signals along Geodesic Paths",
    "description": "For a given graph containing vertices, edges, and a signal associated with the vertices, the 'PathwaySpace' package performs a convolution operation, which involves a weighted combination of neighboring vertices and their associated signals. The package then uses a decay function to project these signals, creating geodesic paths on a 2D-image space. 'PathwaySpace' could have various applications, such as visualizing network data in a graphical format that highlights the relationships and signal strengths between vertices. It can be particularly useful for understanding the influence of signals through complex networks. By combining graph theory, signal processing, and visualization, the 'PathwaySpace' package provides a novel way of representing graph data.",
    "version": "1.1.0",
    "maintainer": "Mauro Castro <mauro.a.castro@gmail.com>",
    "author": "Victor Apolonio [ctb],\n  Vinicius Chagas [ctb],\n  Sysbiolab and Collaborators [fnd],\n  Mauro Castro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4942-8131>)",
    "url": "https://sysbiolab.github.io/PathwaySpace/,\nhttps://github.com/sysbiolab/PathwaySpace",
    "bug_reports": "https://github.com/sysbiolab/PathwaySpace/issues",
    "repository": "https://cran.r-project.org/package=PathwaySpace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PathwaySpace Spatial Projection of Network Signals along Geodesic Paths For a given graph containing vertices, edges, and a signal associated with the vertices, the 'PathwaySpace' package performs a convolution operation, which involves a weighted combination of neighboring vertices and their associated signals. The package then uses a decay function to project these signals, creating geodesic paths on a 2D-image space. 'PathwaySpace' could have various applications, such as visualizing network data in a graphical format that highlights the relationships and signal strengths between vertices. It can be particularly useful for understanding the influence of signals through complex networks. By combining graph theory, signal processing, and visualization, the 'PathwaySpace' package provides a novel way of representing graph data.  "
  },
  {
    "id": 5846,
    "package_name": "PlayerChart",
    "title": "Generate Pizza Chart: Player Stats 0-100",
    "description": "Create an interactive pizza chart visualizing a specific player's statistics across various attributes in a sports dataset. The chart is constructed based on input parameters: 'data', a dataframe containing player data for any sports; 'player_stats_col', a vector specifying the names of the columns from the dataframe that will be used to create slices in the pizza chart, with statistics ranging between 0 and 100; 'name_col', specifying the name of the column in the dataframe that contains the player names; and 'player_name', representing the specific player whose statistics will be visualized in the chart, serving as the chart title.",
    "version": "1.0.0",
    "maintainer": "Amal Panwar <panwar.amal1995@gmail.com>",
    "author": "Amal Panwar [aut, cre] (ORCID: <https://orcid.org/0009-0003-9046-9207>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PlayerChart",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PlayerChart Generate Pizza Chart: Player Stats 0-100 Create an interactive pizza chart visualizing a specific player's statistics across various attributes in a sports dataset. The chart is constructed based on input parameters: 'data', a dataframe containing player data for any sports; 'player_stats_col', a vector specifying the names of the columns from the dataframe that will be used to create slices in the pizza chart, with statistics ranging between 0 and 100; 'name_col', specifying the name of the column in the dataframe that contains the player names; and 'player_name', representing the specific player whose statistics will be visualized in the chart, serving as the chart title.  "
  },
  {
    "id": 5847,
    "package_name": "PlayerRatings",
    "title": "Dynamic Updating Methods for Player Ratings Estimation",
    "description": "Implements schemes for estimating player or \n  team skill based on dynamic updating. Implemented methods include \n  Elo, Glicko, Glicko-2 and Stephenson. Contains pdf documentation \n  of a reproducible analysis using approximately two million chess \n  matches. Also contains an Elo based method for multi-player games\n  where the result is a placing or a score. This includes zero-sum\n  games such as poker and mahjong.",
    "version": "1.1-0",
    "maintainer": "Alec Stephenson <alec_stephenson@hotmail.com>",
    "author": "Alec Stephenson and Jeff Sonas.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PlayerRatings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PlayerRatings Dynamic Updating Methods for Player Ratings Estimation Implements schemes for estimating player or \n  team skill based on dynamic updating. Implemented methods include \n  Elo, Glicko, Glicko-2 and Stephenson. Contains pdf documentation \n  of a reproducible analysis using approximately two million chess \n  matches. Also contains an Elo based method for multi-player games\n  where the result is a placing or a score. This includes zero-sum\n  games such as poker and mahjong.  "
  },
  {
    "id": 5855,
    "package_name": "PoA",
    "title": "Finds the Price of Anarchy for Routing Games",
    "description": "Computes the optimal flow, Nash flow and the Price of Anarchy for any routing game defined within the game theoretical framework. The input is a routing game in the form of it\u2019s cost and flow functions. Then transforms this into an optimisation problem, allowing both Nash and Optimal flows to be solved by nonlinear optimisation. See <https://en.wikipedia.org/wiki/Congestion_game> and Knight and Harper (2013) <doi:10.1016/j.ejor.2013.04.003> for more information.",
    "version": "1.2.1",
    "maintainer": "Hector Haffenden <haffendenh@cardiff.ac.uk>",
    "author": "Hector Haffenden",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PoA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PoA Finds the Price of Anarchy for Routing Games Computes the optimal flow, Nash flow and the Price of Anarchy for any routing game defined within the game theoretical framework. The input is a routing game in the form of it\u2019s cost and flow functions. Then transforms this into an optimisation problem, allowing both Nash and Optimal flows to be solved by nonlinear optimisation. See <https://en.wikipedia.org/wiki/Congestion_game> and Knight and Harper (2013) <doi:10.1016/j.ejor.2013.04.003> for more information.  "
  },
  {
    "id": 5931,
    "package_name": "ProSportsDraftData",
    "title": "Professional Sports Draft Data",
    "description": "We provide comprehensive draft data for major professional sports leagues, including the National Football League (NFL), National Basketball Association (NBA), and National Hockey League (NHL). It offers access to both historical and current draft data, allowing for detailed analysis and research on player biases and player performance. The package is useful for sports fans and researchers interested in identifying biases and trends within scouting reports. Created by web scraping data from leading websites that cover professional sports player scouting reports, the package allows users to filter and summarize data for analytical purposes. For further details on the methods used, please refer to Wickham (2022) \"rvest: Easily Harvest (Scrape) Web Pages\" <https://CRAN.R-project.org/package=rvest> and Harrison (2023) \"RSelenium: R Bindings for Selenium WebDriver\" <https://CRAN.R-project.org/package=RSelenium>.",
    "version": "1.0.3",
    "maintainer": "Benjamin Ginsburg <benjamin.ginsburg@du.edu>",
    "author": "Benjamin Ginsburg [aut, cre]",
    "url": "https://github.com/Ginsburg1/ProSportsDraftData",
    "bug_reports": "https://github.com/Ginsburg1/ProSportsDraftData/issues",
    "repository": "https://cran.r-project.org/package=ProSportsDraftData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProSportsDraftData Professional Sports Draft Data We provide comprehensive draft data for major professional sports leagues, including the National Football League (NFL), National Basketball Association (NBA), and National Hockey League (NHL). It offers access to both historical and current draft data, allowing for detailed analysis and research on player biases and player performance. The package is useful for sports fans and researchers interested in identifying biases and trends within scouting reports. Created by web scraping data from leading websites that cover professional sports player scouting reports, the package allows users to filter and summarize data for analytical purposes. For further details on the methods used, please refer to Wickham (2022) \"rvest: Easily Harvest (Scrape) Web Pages\" <https://CRAN.R-project.org/package=rvest> and Harrison (2023) \"RSelenium: R Bindings for Selenium WebDriver\" <https://CRAN.R-project.org/package=RSelenium>.  "
  },
  {
    "id": 5968,
    "package_name": "PulmoDataSets",
    "title": "A Curated Collection of Pulmonary and Respiratory Disease\nDatasets",
    "description": "Provides a comprehensive and curated collection of datasets related to the lungs, respiratory system, and associated diseases. \n    This package includes epidemiological, clinical, experimental, and simulated datasets on conditions such as lung cancer, asthma, \n    Chronic Obstructive Pulmonary Disease (COPD), tuberculosis, whooping cough, pneumonia, influenza, and other respiratory illnesses. \n    It is designed to support data exploration, statistical modeling, teaching, and research in pulmonary medicine, public health, \n    environmental epidemiology, and respiratory disease surveillance.",
    "version": "0.2.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/pulmodatasets,\nhttps://lightbluetitan.github.io/pulmodatasets/",
    "bug_reports": "https://github.com/lightbluetitan/pulmodatasets/issues",
    "repository": "https://cran.r-project.org/package=PulmoDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PulmoDataSets A Curated Collection of Pulmonary and Respiratory Disease\nDatasets Provides a comprehensive and curated collection of datasets related to the lungs, respiratory system, and associated diseases. \n    This package includes epidemiological, clinical, experimental, and simulated datasets on conditions such as lung cancer, asthma, \n    Chronic Obstructive Pulmonary Disease (COPD), tuberculosis, whooping cough, pneumonia, influenza, and other respiratory illnesses. \n    It is designed to support data exploration, statistical modeling, teaching, and research in pulmonary medicine, public health, \n    environmental epidemiology, and respiratory disease surveillance.  "
  },
  {
    "id": 5996,
    "package_name": "QGameTheory",
    "title": "Quantum Game Theory Simulator",
    "description": "General purpose toolbox for simulating quantum versions of game theoretic models (Flitney and Abbott 2002) <arXiv:quant-ph/0208069>. Quantum (Nielsen and Chuang 2010, ISBN:978-1-107-00217-3) versions of models that have been handled are: Penny Flip Game (David A. Meyer 1998) <arXiv:quant-ph/9804010>, Prisoner's Dilemma (J. Orlin Grabbe 2005) <arXiv:quant-ph/0506219>, Two Person Duel (Flitney and Abbott 2004) <arXiv:quant-ph/0305058>, Battle of the Sexes (Nawaz and Toor 2004) <arXiv:quant-ph/0110096>, Hawk and Dove Game (Nawaz and Toor 2010) <arXiv:quant-ph/0108075>, Newcomb's Paradox (Piotrowski and Sladkowski 2002) <arXiv:quant-ph/0202074> and Monty Hall Problem (Flitney and Abbott 2002) <arXiv:quant-ph/0109035>.",
    "version": "0.1.2",
    "maintainer": "Indranil Ghosh <indranilg49@gmail.com>",
    "author": "Indranil Ghosh",
    "url": "https://github.com/indrag49/QGameTheory",
    "bug_reports": "https://github.com/indrag49/QGameTheory/issues",
    "repository": "https://cran.r-project.org/package=QGameTheory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QGameTheory Quantum Game Theory Simulator General purpose toolbox for simulating quantum versions of game theoretic models (Flitney and Abbott 2002) <arXiv:quant-ph/0208069>. Quantum (Nielsen and Chuang 2010, ISBN:978-1-107-00217-3) versions of models that have been handled are: Penny Flip Game (David A. Meyer 1998) <arXiv:quant-ph/9804010>, Prisoner's Dilemma (J. Orlin Grabbe 2005) <arXiv:quant-ph/0506219>, Two Person Duel (Flitney and Abbott 2004) <arXiv:quant-ph/0305058>, Battle of the Sexes (Nawaz and Toor 2004) <arXiv:quant-ph/0110096>, Hawk and Dove Game (Nawaz and Toor 2010) <arXiv:quant-ph/0108075>, Newcomb's Paradox (Piotrowski and Sladkowski 2002) <arXiv:quant-ph/0202074> and Monty Hall Problem (Flitney and Abbott 2002) <arXiv:quant-ph/0109035>.  "
  },
  {
    "id": 6053,
    "package_name": "R.methodsS3",
    "title": "S3 Methods Simplified",
    "description": "Methods that simplify the setup of S3 generic functions and S3 methods.  Major effort has been made in making definition of methods as simple as possible with a minimum of maintenance for package developers.  For example, generic functions are created automatically, if missing, and naming conflict are automatically solved, if possible.  The method setMethodS3() is a good start for those who in the future may want to migrate to S4.  This is a cross-platform package implemented in pure R that generates standard S3 methods.",
    "version": "1.8.2",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph]",
    "url": "https://github.com/HenrikBengtsson/R.methodsS3",
    "bug_reports": "https://github.com/HenrikBengtsson/R.methodsS3/issues",
    "repository": "https://cran.r-project.org/package=R.methodsS3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R.methodsS3 S3 Methods Simplified Methods that simplify the setup of S3 generic functions and S3 methods.  Major effort has been made in making definition of methods as simple as possible with a minimum of maintenance for package developers.  For example, generic functions are created automatically, if missing, and naming conflict are automatically solved, if possible.  The method setMethodS3() is a good start for those who in the future may want to migrate to S4.  This is a cross-platform package implemented in pure R that generates standard S3 methods.  "
  },
  {
    "id": 6094,
    "package_name": "RANSAC",
    "title": "Robust Model Fitting Using the RANSAC Algorithm",
    "description": "Provides tools for robust regression model fitting using the RANSAC (Random Sample Consensus) algorithm. RANSAC is an iterative method to estimate parameters of a model from a dataset that contains outliers. This package allows fitting both linear lm and nonlinear nls models using RANSAC, helping users obtain more reliable models in the presence of noisy or corrupted data. The methods are particularly useful in contexts where traditional least squares regression fails due to the influence of outliers. Implementations include support for performance metrics such as RMSE, MAE, and R\u00b2 based on the inlier subset. For further details, see Fischler and Bolles (1981) <doi:10.1145/358669.358692>.",
    "version": "0.1.0",
    "maintainer": "Jadson Abreu <jadson.ap@gmail.com>",
    "author": "Jadson Abreu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RANSAC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RANSAC Robust Model Fitting Using the RANSAC Algorithm Provides tools for robust regression model fitting using the RANSAC (Random Sample Consensus) algorithm. RANSAC is an iterative method to estimate parameters of a model from a dataset that contains outliers. This package allows fitting both linear lm and nonlinear nls models using RANSAC, helping users obtain more reliable models in the presence of noisy or corrupted data. The methods are particularly useful in contexts where traditional least squares regression fails due to the influence of outliers. Implementations include support for performance metrics such as RMSE, MAE, and R\u00b2 based on the inlier subset. For further details, see Fischler and Bolles (1981) <doi:10.1145/358669.358692>.  "
  },
  {
    "id": 6161,
    "package_name": "RDota2",
    "title": "An R Steam API Client for Valve's Dota2",
    "description": "An R API Client for Valve's Dota2. RDota2 can be easily used \n    to connect to the Steam API and retrieve data for Valve's popular video \n    game Dota2. You can find out more about Dota2 at \n    <http://store.steampowered.com/app/570/>.",
    "version": "0.1.6",
    "maintainer": "Theo Boutaris <teoboot2007@hotmail.com>",
    "author": "Theo Boutaris [aut, cre, cph]",
    "url": "https://github.com/LyzandeR/RDota2",
    "bug_reports": "https://github.com/LyzandeR/RDota2/issues",
    "repository": "https://cran.r-project.org/package=RDota2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RDota2 An R Steam API Client for Valve's Dota2 An R API Client for Valve's Dota2. RDota2 can be easily used \n    to connect to the Steam API and retrieve data for Valve's popular video \n    game Dota2. You can find out more about Dota2 at \n    <http://store.steampowered.com/app/570/>.  "
  },
  {
    "id": 6170,
    "package_name": "REDI",
    "title": "Robust Exponential Decreasing Index",
    "description": "Implementation of the Robust Exponential Decreasing Index (REDI),\n    proposed in the article by Issa Moussa, Arthur Leroy et al. (2019)\n    <https://bmjopensem.bmj.com/content/bmjosem/5/1/e000573.full.pdf>.\n    The REDI represents a measure of cumulated workload, robust to missing data,\n    providing control of the decreasing influence of workload over time. \n    Various functions are provided to format data, compute REDI, and \n    visualise results in a simple and convenient way. ",
    "version": "1.0.0",
    "maintainer": "Alexia Grenouillat <alexia.grenouillat00@gmail.com>",
    "author": "Alexia Grenouillat [aut, cre],\n  Arthur Leroy [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=REDI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDI Robust Exponential Decreasing Index Implementation of the Robust Exponential Decreasing Index (REDI),\n    proposed in the article by Issa Moussa, Arthur Leroy et al. (2019)\n    <https://bmjopensem.bmj.com/content/bmjosem/5/1/e000573.full.pdf>.\n    The REDI represents a measure of cumulated workload, robust to missing data,\n    providing control of the decreasing influence of workload over time. \n    Various functions are provided to format data, compute REDI, and \n    visualise results in a simple and convenient way.   "
  },
  {
    "id": 6203,
    "package_name": "RGAN",
    "title": "Generative Adversarial Nets (GAN) in R",
    "description": "An easy way to get started with Generative Adversarial Nets (GAN) in R. The GAN algorithm was initially \n    described by Goodfellow et al. 2014 <https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf>. A GAN can be used to learn the joint distribution of complex data by \n    comparison. A GAN consists of two neural networks a Generator and a Discriminator, where the two\n    neural networks play an adversarial minimax game.\n    Built-in GAN models make the training of GANs in R possible in one line and make it easy to \n    experiment with different design choices (e.g. different network architectures, value functions, optimizers).\n    The built-in GAN models work with tabular data (e.g. to produce synthetic data) and image data. \n    Methods to post-process the output of GAN models to enhance the quality of samples are available.",
    "version": "0.1.1",
    "maintainer": "Marcel Neunhoeffer <marcel.neunhoeffer@gmail.com>",
    "author": "Marcel Neunhoeffer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9137-5785>)",
    "url": "https://github.com/mneunhoe/RGAN",
    "bug_reports": "https://github.com/mneunhoe/RGAN/issues",
    "repository": "https://cran.r-project.org/package=RGAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RGAN Generative Adversarial Nets (GAN) in R An easy way to get started with Generative Adversarial Nets (GAN) in R. The GAN algorithm was initially \n    described by Goodfellow et al. 2014 <https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf>. A GAN can be used to learn the joint distribution of complex data by \n    comparison. A GAN consists of two neural networks a Generator and a Discriminator, where the two\n    neural networks play an adversarial minimax game.\n    Built-in GAN models make the training of GANs in R possible in one line and make it easy to \n    experiment with different design choices (e.g. different network architectures, value functions, optimizers).\n    The built-in GAN models work with tabular data (e.g. to produce synthetic data) and image data. \n    Methods to post-process the output of GAN models to enhance the quality of samples are available.  "
  },
  {
    "id": 6236,
    "package_name": "RIIM",
    "title": "Randomization-Based Inference Under Inexact Matching",
    "description": "Randomization-based inference for average treatment effects in potentially inexactly matched observational studies. It implements the inverse post-matching probability weighting framework proposed by the authors. The post-matching probability calculation follows the approach of Pimentel and Huang (2024) <doi:10.1093/jrsssb/qkae033>. The optimal full matching method is based on Hansen (2004) <doi:10.1198/106186006X137047>. The variance estimator extends the method proposed in Fogarty (2018) <doi:10.1111/rssb.12290> from the perfect randomization settings to the potentially inexact matching case. Comparisons are made with conventional methods, as described in Rosenbaum (2002) <doi:10.1007/978-1-4757-3692-2>, Fogarty (2018) <doi:10.1111/rssb.12290>, and Kang et al. (2016) <doi:10.1214/15-aoas894>. ",
    "version": "2.0.0",
    "maintainer": "Jianan Zhu <jz4698@nyu.edu>",
    "author": "Jianan Zhu [aut, cre],\n  Jeffrey Zhang [aut],\n  Zijian Guo [aut],\n  Siyu Heng [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RIIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RIIM Randomization-Based Inference Under Inexact Matching Randomization-based inference for average treatment effects in potentially inexactly matched observational studies. It implements the inverse post-matching probability weighting framework proposed by the authors. The post-matching probability calculation follows the approach of Pimentel and Huang (2024) <doi:10.1093/jrsssb/qkae033>. The optimal full matching method is based on Hansen (2004) <doi:10.1198/106186006X137047>. The variance estimator extends the method proposed in Fogarty (2018) <doi:10.1111/rssb.12290> from the perfect randomization settings to the potentially inexact matching case. Comparisons are made with conventional methods, as described in Rosenbaum (2002) <doi:10.1007/978-1-4757-3692-2>, Fogarty (2018) <doi:10.1111/rssb.12290>, and Kang et al. (2016) <doi:10.1214/15-aoas894>.   "
  },
  {
    "id": 6258,
    "package_name": "RKUM",
    "title": "Robust Kernel Unsupervised Methods",
    "description": "Robust  kernel center matrix, robust  kernel cross-covariance operator for kernel unsupervised methods, kernel canonical correlation analysis, \n influence function of identifying significant outliers or atypical objects from multimodal datasets. Alam, M. A,  Fukumizu, K., Wang  Y.-P. (2018) <doi:10.1016/j.neucom.2018.04.008>.\n   Alam, M. A,  Calhoun, C. D.,  Wang  Y.-P. (2018) <doi:10.1016/j.csda.2018.03.013>.",
    "version": "0.1.1.1",
    "maintainer": "Md Ashad Alam <malam@tulane.edu>",
    "author": "Md Ashad Alam",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RKUM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RKUM Robust Kernel Unsupervised Methods Robust  kernel center matrix, robust  kernel cross-covariance operator for kernel unsupervised methods, kernel canonical correlation analysis, \n influence function of identifying significant outliers or atypical objects from multimodal datasets. Alam, M. A,  Fukumizu, K., Wang  Y.-P. (2018) <doi:10.1016/j.neucom.2018.04.008>.\n   Alam, M. A,  Calhoun, C. D.,  Wang  Y.-P. (2018) <doi:10.1016/j.csda.2018.03.013>.  "
  },
  {
    "id": 6260,
    "package_name": "RKelly",
    "title": "Translate Odds and Probabilities",
    "description": "Calculates the Kelly criterion (Kelly, J.L. (1956) <doi:10.1002/j.1538-7305.1956.tb03809.x>) for bets given quoted prices, model predictions and commissions.\n    Additionally it contains helper functions to calculate the probabilities for wins and draws in multi-leg games.",
    "version": "1.0",
    "maintainer": "Arvid Kingl <akingl2016@gmail.com>",
    "author": "Arvid Kingl [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RKelly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RKelly Translate Odds and Probabilities Calculates the Kelly criterion (Kelly, J.L. (1956) <doi:10.1002/j.1538-7305.1956.tb03809.x>) for bets given quoted prices, model predictions and commissions.\n    Additionally it contains helper functions to calculate the probabilities for wins and draws in multi-leg games.  "
  },
  {
    "id": 6335,
    "package_name": "ROKET",
    "title": "Optimal Transport-Based Kernel Regression",
    "description": "Perform optimal transport on somatic point mutations and kernel regression hypothesis testing by integrating pathway level similarities at the gene level (Little et al. (2023) <doi:10.1111/biom.13769>). The software implements balanced and unbalanced optimal transport and omnibus tests with 'C++' across a set of tumor samples and allows for multi-threading to decrease computational runtime.",
    "version": "1.0.0",
    "maintainer": "Paul Little <pllittle321@gmail.com>",
    "author": "Paul Little [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ROKET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ROKET Optimal Transport-Based Kernel Regression Perform optimal transport on somatic point mutations and kernel regression hypothesis testing by integrating pathway level similarities at the gene level (Little et al. (2023) <doi:10.1111/biom.13769>). The software implements balanced and unbalanced optimal transport and omnibus tests with 'C++' across a set of tumor samples and allows for multi-threading to decrease computational runtime.  "
  },
  {
    "id": 6395,
    "package_name": "RSDK",
    "title": "Sudoku with R",
    "description": "This is a sudoku game package with a shiny application for playing .",
    "version": "1.0.1",
    "maintainer": "EL KHMISSI Mohamed <mohamed.el-khmissi01@etu.umontpellier.fr>",
    "author": "EL KHMISSI Mohamed",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSDK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSDK Sudoku with R This is a sudoku game package with a shiny application for playing .  "
  },
  {
    "id": 6450,
    "package_name": "RWmisc",
    "title": "Miscellaneous Spatial Functions",
    "description": "Contains convenience functions for working with spatial data across\n    multiple UTM zones, raster-vector operations common in the analysis of \n    conflict data, and converting degrees, minutes, and seconds latitude and\n    longitude coordinates to decimal degrees.",
    "version": "0.1.2",
    "maintainer": "Rob Williams <jayrobwilliams@gmail.com>",
    "author": "Rob Williams [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9259-3883>)",
    "url": "https://github.com/jayrobwilliams/RWmisc",
    "bug_reports": "https://github.com/jayrobwilliams/RWmisc/issues",
    "repository": "https://cran.r-project.org/package=RWmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RWmisc Miscellaneous Spatial Functions Contains convenience functions for working with spatial data across\n    multiple UTM zones, raster-vector operations common in the analysis of \n    conflict data, and converting degrees, minutes, and seconds latitude and\n    longitude coordinates to decimal degrees.  "
  },
  {
    "id": 6460,
    "package_name": "Racmacs",
    "title": "Antigenic Cartography Macros",
    "description": "A toolkit for making antigenic maps from immunological assay data,\n    in order to quantify and visualize antigenic differences between different\n    pathogen strains as described in\n    Smith et al. (2004) <doi:10.1126/science.1097211> and used in the World\n    Health Organization influenza vaccine strain selection process. Additional\n    functions allow for the diagnostic evaluation of antigenic maps and an\n    interactive viewer is provided to explore antigenic relationships amongst\n    several strains and incorporate the visualization of associated genetic\n    information.",
    "version": "1.2.9",
    "maintainer": "Sam Wilks <sw463@cam.ac.uk>",
    "author": "Sam Wilks [aut, cre]",
    "url": "https://acorg.github.io/Racmacs/,\nhttps://github.com/acorg/Racmacs/",
    "bug_reports": "https://github.com/acorg/Racmacs/issues",
    "repository": "https://cran.r-project.org/package=Racmacs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Racmacs Antigenic Cartography Macros A toolkit for making antigenic maps from immunological assay data,\n    in order to quantify and visualize antigenic differences between different\n    pathogen strains as described in\n    Smith et al. (2004) <doi:10.1126/science.1097211> and used in the World\n    Health Organization influenza vaccine strain selection process. Additional\n    functions allow for the diagnostic evaluation of antigenic maps and an\n    interactive viewer is provided to explore antigenic relationships amongst\n    several strains and incorporate the visualization of associated genetic\n    information.  "
  },
  {
    "id": 6488,
    "package_name": "Raquifer",
    "title": "Estimate the Water Influx into Hydrocarbon Reservoirs",
    "description": "Generate a table of cumulative water influx into hydrocarbon reservoirs over time using un-steady and pseudo-steady state models. Van Everdingen, A. F. and Hurst, W. (1949) <doi:10.2118/949305-G>. Fetkovich, M. J. (1971) <doi:10.2118/2603-PA>. Yildiz, T. and Khosravi, A. (2007) <doi:10.2118/103283-PA>. ",
    "version": "0.1.0",
    "maintainer": "Farshad Tabasinejad <farshad.tabasinejad@susaenergy.com>",
    "author": "Farshad Tabasinejad [aut, cre]",
    "url": "https://susaenergy.github.io/Raquifer_ws/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Raquifer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Raquifer Estimate the Water Influx into Hydrocarbon Reservoirs Generate a table of cumulative water influx into hydrocarbon reservoirs over time using un-steady and pseudo-steady state models. Van Everdingen, A. F. and Hurst, W. (1949) <doi:10.2118/949305-G>. Fetkovich, M. J. (1971) <doi:10.2118/2603-PA>. Yildiz, T. and Khosravi, A. (2007) <doi:10.2118/103283-PA>.   "
  },
  {
    "id": 6705,
    "package_name": "Rfractran",
    "title": "A 'FRACTRAN' Interpreter and Some Helper Functions",
    "description": "'FRACTRAN' is an obscure yet tantalizing programming language invented by John Conway of 'Game of Life' fame. The code consists of a sequence of fractions. The rules are simple. First, select an integer to initialize the process. Second, multiply the integer by the first fraction. If an integer results, start again with the new integer. If not, try the next fraction. Finally, if no such multiplication yields an integer, terminate the program. For more information, see <https://en.wikipedia.org/wiki/FRACTRAN> . ",
    "version": "1.0.1",
    "maintainer": "Carl Witthoft <cellocgw@gmail.com>",
    "author": "Carl Witthoft [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rfractran",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rfractran A 'FRACTRAN' Interpreter and Some Helper Functions 'FRACTRAN' is an obscure yet tantalizing programming language invented by John Conway of 'Game of Life' fame. The code consists of a sequence of fractions. The rules are simple. First, select an integer to initialize the process. Second, multiply the integer by the first fraction. If an integer results, start again with the new integer. If not, try the next fraction. Finally, if no such multiplication yields an integer, terminate the program. For more information, see <https://en.wikipedia.org/wiki/FRACTRAN> .   "
  },
  {
    "id": 6762,
    "package_name": "RobKF",
    "title": "Innovative and/or Additive Outlier Robust Kalman Filtering",
    "description": "Implements a series of robust Kalman filtering approaches. It implements the additive outlier robust filters of Ruckdeschel et al. (2014) <arXiv:1204.3358> and Agamennoni et al. (2018) <doi:10.1109/ICRA.2011.5979605>, the innovative outlier robust filter of Ruckdeschel et al. (2014) <arXiv:1204.3358>, as well as the innovative and additive outlier robust filter of Fisch et al. (2020) <arXiv:2007.03238>.",
    "version": "1.0.2",
    "maintainer": "Daniel Grose <dan.grose@lancaster.ac.uk>",
    "author": "Alex TM Fisch [aut],\n  Daniel Grose [aut, cre],\n  Idris A Eckley [aut, ths],\n  Paul Fearnhead [aut, ths],\n  Lawrence Bardwell [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobKF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobKF Innovative and/or Additive Outlier Robust Kalman Filtering Implements a series of robust Kalman filtering approaches. It implements the additive outlier robust filters of Ruckdeschel et al. (2014) <arXiv:1204.3358> and Agamennoni et al. (2018) <doi:10.1109/ICRA.2011.5979605>, the innovative outlier robust filter of Ruckdeschel et al. (2014) <arXiv:1204.3358>, as well as the innovative and additive outlier robust filter of Fisch et al. (2020) <arXiv:2007.03238>.  "
  },
  {
    "id": 6782,
    "package_name": "RobustPrediction",
    "title": "Robust Tuning and Training for Cross-Source Prediction",
    "description": "Provides robust parameter tuning and model training for predictive models applied across data sources where the data distribution varies slightly from source to source. This package implements three primary tuning methods: cross-validation-based internal tuning, external tuning, and the 'RobustTuneC' method. External tuning includes a conservative option where parameters are tuned internally on the training data and validating on an external dataset, providing a slightly pessimistic estimate. It supports Lasso, Ridge, Random Forest, Boosting, and Support Vector Machine classifiers. Currently, only binary classification is supported. The response variable must be the first column of the dataset and a factor with exactly two levels. The tuning methods are based on the paper by Nicole Ellenbach, Anne-Laure Boulesteix, Bernd Bischl, Kristian Unger, and Roman Hornung (2021) \"Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning\" <doi:10.1007/s00357-020-09368-z>.",
    "version": "0.1.7",
    "maintainer": "Yuting He <yutingh19@gmail.com>",
    "author": "Yuting He [aut, cre],\n  Nicole Ellenbach [ctb],\n  Roman Hornung [ctb]",
    "url": "https://github.com/Yuting-He/RobustPrediction",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobustPrediction",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobustPrediction Robust Tuning and Training for Cross-Source Prediction Provides robust parameter tuning and model training for predictive models applied across data sources where the data distribution varies slightly from source to source. This package implements three primary tuning methods: cross-validation-based internal tuning, external tuning, and the 'RobustTuneC' method. External tuning includes a conservative option where parameters are tuned internally on the training data and validating on an external dataset, providing a slightly pessimistic estimate. It supports Lasso, Ridge, Random Forest, Boosting, and Support Vector Machine classifiers. Currently, only binary classification is supported. The response variable must be the first column of the dataset and a factor with exactly two levels. The tuning methods are based on the paper by Nicole Ellenbach, Anne-Laure Boulesteix, Bernd Bischl, Kristian Unger, and Roman Hornung (2021) \"Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning\" <doi:10.1007/s00357-020-09368-z>.  "
  },
  {
    "id": 6792,
    "package_name": "RootsExtremaInflections",
    "title": "Finds Roots, Extrema and Inflection Points of a Curve",
    "description": "Implementation of Taylor Regression Estimator (TRE), \n   Tulip Extreme Finding Estimator (TEFE), Bell Extreme Finding Estimator (BEFE),\n   Integration Extreme Finding Estimator (IEFE) and \n   Integration Root Finding Estimator (IRFE) for roots, extrema and inflections of a curve .     \n   Christopoulos, DT (2019) <doi:10.13140/RG.2.2.17158.32324> .\n   Christopoulos, DT (2016) <doi:10.2139/ssrn.3043076> .\n   Christopoulos, DT (2016) <https://demovtu.veltech.edu.in/wp-content/uploads/2016/04/Paper-04-2016.pdf> .\n   Christopoulos, DT (2014) <doi:10.48550/arXiv.1206.5478> .",
    "version": "1.2.5",
    "maintainer": "Demetris T. Christopoulos <dchristop@econ.uoa.gr>",
    "author": "Demetris T. Christopoulos [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RootsExtremaInflections",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RootsExtremaInflections Finds Roots, Extrema and Inflection Points of a Curve Implementation of Taylor Regression Estimator (TRE), \n   Tulip Extreme Finding Estimator (TEFE), Bell Extreme Finding Estimator (BEFE),\n   Integration Extreme Finding Estimator (IEFE) and \n   Integration Root Finding Estimator (IRFE) for roots, extrema and inflections of a curve .     \n   Christopoulos, DT (2019) <doi:10.13140/RG.2.2.17158.32324> .\n   Christopoulos, DT (2016) <doi:10.2139/ssrn.3043076> .\n   Christopoulos, DT (2016) <https://demovtu.veltech.edu.in/wp-content/uploads/2016/04/Paper-04-2016.pdf> .\n   Christopoulos, DT (2014) <doi:10.48550/arXiv.1206.5478> .  "
  },
  {
    "id": 6809,
    "package_name": "Rquake",
    "title": "Seismic Hypocenter Determination",
    "description": "Non-linear inversion for hypocenter estimation and analysis of seismic data collected continuously, or in trigger mode. The functions organize other functions from 'RSEIS' and 'GEOmap' to help researchers pick, locate, and store hypocenters for detailed seismic investigation. Error ellipsoids and station influence are estimated via jackknife analysis. References include  Iversen, E. S., and J. M. Lees (1996)<doi:10.1785/BSSA0860061853>.",
    "version": "2.5-1",
    "maintainer": "Jonathan M. Lees <jonathan.lees@unc.edu>",
    "author": "Jonathan M. Lees [aut, cre],\n  Baptiste Auguie [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rquake",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rquake Seismic Hypocenter Determination Non-linear inversion for hypocenter estimation and analysis of seismic data collected continuously, or in trigger mode. The functions organize other functions from 'RSEIS' and 'GEOmap' to help researchers pick, locate, and store hypocenters for detailed seismic investigation. Error ellipsoids and station influence are estimated via jackknife analysis. References include  Iversen, E. S., and J. M. Lees (1996)<doi:10.1785/BSSA0860061853>.  "
  },
  {
    "id": 6821,
    "package_name": "Rsgf",
    "title": "SGF (Smart Game File) File Format Import",
    "description": "Import SGF (Smart Game File) into R.",
    "version": "1.0.0",
    "maintainer": "Brad Cable <brad@bcable.net>",
    "author": "Brad Cable",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rsgf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rsgf SGF (Smart Game File) File Format Import Import SGF (Smart Game File) into R.  "
  },
  {
    "id": 6897,
    "package_name": "SC2API",
    "title": "Blizzard SC2 API Wrapper",
    "description": "A wrapper for Blizzard's Starcraft II (a 2010 real-time strategy game) Application Programming Interface (API). All documented API calls are implemented in an easy-to-use and consistent manner.",
    "version": "1.0.0",
    "maintainer": "Samuel Morrissette <samuel.morrissette01@gmail.com>",
    "author": "Samuel Morrissette [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SC2API",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SC2API Blizzard SC2 API Wrapper A wrapper for Blizzard's Starcraft II (a 2010 real-time strategy game) Application Programming Interface (API). All documented API calls are implemented in an easy-to-use and consistent manner.  "
  },
  {
    "id": 6999,
    "package_name": "SIRmcmc",
    "title": "Compartmental Susceptible-Infectious-Recovered (SIR) Model of\nCommunity and Household Infection",
    "description": "We build an Susceptible-Infectious-Recovered (SIR) model where the rate of infection is the sum of the household rate and the community rate. We estimate the posterior distribution of the parameters using the Metropolis algorithm. Further details may be found in: F Scott Dahlgren, Ivo M Foppa, Melissa S Stockwell, Celibell Y Vargas, Philip LaRussa, Carrie Reed (2021) \"Household transmission of influenza A and B within a prospective cohort during the 2013-2014 and 2014-2015 seasons\" <doi:10.1002/sim.9181>.",
    "version": "1.1.1",
    "maintainer": "F Scott Dahlgren <fdahlgr@gmail.com>",
    "author": "F Scott Dahlgren and Ivo M Foppa",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SIRmcmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIRmcmc Compartmental Susceptible-Infectious-Recovered (SIR) Model of\nCommunity and Household Infection We build an Susceptible-Infectious-Recovered (SIR) model where the rate of infection is the sum of the household rate and the community rate. We estimate the posterior distribution of the parameters using the Metropolis algorithm. Further details may be found in: F Scott Dahlgren, Ivo M Foppa, Melissa S Stockwell, Celibell Y Vargas, Philip LaRussa, Carrie Reed (2021) \"Household transmission of influenza A and B within a prospective cohort during the 2013-2014 and 2014-2015 seasons\" <doi:10.1002/sim.9181>.  "
  },
  {
    "id": 7094,
    "package_name": "SPORTSCausal",
    "title": "Spillover Time Series Causal Inference",
    "description": "A time series causal inference model for Randomized Controlled Trial (RCT) under spillover effect. 'SPORTSCausal' (Spillover Time Series Causal Inference) separates treatment effect and spillover effect from given responses of experiment group and control group by predicting the response without treatment. It reports both effects by fitting the Bayesian Structural Time Series (BSTS) model based on 'CausalImpact', as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>. ",
    "version": "1.0",
    "maintainer": "Feiyu Yue <yuefyopals@gmail.com>",
    "author": "Zihao Zheng and Feiyu Yue",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPORTSCausal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPORTSCausal Spillover Time Series Causal Inference A time series causal inference model for Randomized Controlled Trial (RCT) under spillover effect. 'SPORTSCausal' (Spillover Time Series Causal Inference) separates treatment effect and spillover effect from given responses of experiment group and control group by predicting the response without treatment. It reports both effects by fitting the Bayesian Structural Time Series (BSTS) model based on 'CausalImpact', as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>.   "
  },
  {
    "id": 7192,
    "package_name": "ScottKnott",
    "title": "The ScottKnott Clustering Algorithm",
    "description": "Perform the balanced (Scott and Knott, 1974) and unbalanced <doi:10.1590/1984-70332017v17n1a1> Scott & Knott algorithm.",
    "version": "1.3-3",
    "maintainer": "Ivan Bezerra Allaman <ivanalaman@gmail.com>",
    "author": "Jose Claudio Faria [aut],\n  Enio G. Jelihovschi [aut],\n  Ivan Bezerra Allaman [aut, cre]",
    "url": "https://github.com/ivanalaman/ScottKnott,\nhttps://lec.pro.br/software/pac-r/scottknott",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ScottKnott",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ScottKnott The ScottKnott Clustering Algorithm Perform the balanced (Scott and Knott, 1974) and unbalanced <doi:10.1590/1984-70332017v17n1a1> Scott & Knott algorithm.  "
  },
  {
    "id": 7210,
    "package_name": "SeleMix",
    "title": "Selective Editing via Mixture Models",
    "description": "Detection of outliers and influential errors using a latent variable model. ",
    "version": "1.0.3",
    "maintainer": "Teresa Buglielli <teresa.buglielli@gmail.com>",
    "author": "Ugo Guarnera [aut],\n  Teresa Buglielli [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SeleMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SeleMix Selective Editing via Mixture Models Detection of outliers and influential errors using a latent variable model.   "
  },
  {
    "id": 7214,
    "package_name": "SelectionBias",
    "title": "Calculates Bounds for the Selection Bias for Binary Treatment\nand Outcome Variables",
    "description": "Computes bounds and sensitivity parameters as part of sensitivity \n    analysis for selection bias. Different bounds are provided: the SV (Smith \n    and VanderWeele), sharp bounds,  AF (assumption-free) bound, GAF (generalized \n    AF), and CAF (counterfactual AF) bounds. The calculation of the sensitivity \n    parameters for the SV, sharp, and GAF bounds assume an additional dependence \n    structure in form of a generalized M-structure. The bounds can be\n    calculated for any structure as long as the necessary assumptions hold. See \n    Smith and VanderWeele (2019) <doi:10.1097/EDE.0000000000001032>,\n    Zetterstrom, Sj\u00f6lander, and Waernabum (2025) <doi:10.1177/09622802251374168>,\n    Zetterstrom and Waernbaum (2022) <doi:10.1515/em-2022-0108>, and\n    Zetterstrom (2024) <doi:10.1515/em-2023-0033>.",
    "version": "2.1.0",
    "maintainer": "Stina Zetterstrom <stina.zetterstrom@gmail.com>",
    "author": "Stina Zetterstrom [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0730-9425>),\n  Ingeborg Waernbaum [aut] (ORCID:\n    <https://orcid.org/0000-0002-4457-5311>)",
    "url": "https://github.com/StinaZet/SelectionBias",
    "bug_reports": "https://github.com/StinaZet/SelectionBias/issues",
    "repository": "https://cran.r-project.org/package=SelectionBias",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SelectionBias Calculates Bounds for the Selection Bias for Binary Treatment\nand Outcome Variables Computes bounds and sensitivity parameters as part of sensitivity \n    analysis for selection bias. Different bounds are provided: the SV (Smith \n    and VanderWeele), sharp bounds,  AF (assumption-free) bound, GAF (generalized \n    AF), and CAF (counterfactual AF) bounds. The calculation of the sensitivity \n    parameters for the SV, sharp, and GAF bounds assume an additional dependence \n    structure in form of a generalized M-structure. The bounds can be\n    calculated for any structure as long as the necessary assumptions hold. See \n    Smith and VanderWeele (2019) <doi:10.1097/EDE.0000000000001032>,\n    Zetterstrom, Sj\u00f6lander, and Waernabum (2025) <doi:10.1177/09622802251374168>,\n    Zetterstrom and Waernbaum (2022) <doi:10.1515/em-2022-0108>, and\n    Zetterstrom (2024) <doi:10.1515/em-2023-0033>.  "
  },
  {
    "id": 7228,
    "package_name": "SensIAT",
    "title": "Sensitivity Analysis for Irregular Assessment Times",
    "description": "Sensitivity analysis for trials with irregular and informative \n    assessment times, based on a new influence function-based, augmented \n    inverse intensity-weighted estimator.",
    "version": "0.3.0",
    "maintainer": "Andrew Redd <andrew.redd@hsc.utah.edu>",
    "author": "Andrew Redd [aut, cre] (ORCID: <https://orcid.org/0000-0002-6149-2438>),\n  Yujing Gao [aut],\n  Shu Yang [aut],\n  Bonnie Smith [aut],\n  Ravi Varadhan [aut],\n  Agatha Mallett [ctb, ctr],\n  Daniel Scharfstein [pdr, aut] (ORCID:\n    <https://orcid.org/0000-0001-7482-9653>),\n  University of Utah [cph]",
    "url": "https://github.com/UofUEpiBio/SensIAT,\nhttps://uofuepibio.github.io/SensIAT/",
    "bug_reports": "https://github.com/UofUEpiBio/SensIAT/issues",
    "repository": "https://cran.r-project.org/package=SensIAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SensIAT Sensitivity Analysis for Irregular Assessment Times Sensitivity analysis for trials with irregular and informative \n    assessment times, based on a new influence function-based, augmented \n    inverse intensity-weighted estimator.  "
  },
  {
    "id": 7230,
    "package_name": "SensitivityCaseControl",
    "title": "Sensitivity Analysis for Case-Control Studies",
    "description": "Sensitivity analysis for case-control studies in which some cases may meet a more narrow definition of being a case compared to other cases which only meet a broad definition.  The sensitivity analyses are described in Small, Cheng, Halloran and Rosenbaum (2013, \"Case Definition and Sensitivity Analysis\", Journal of the American Statistical Association, 1457-1468).  The functions sens.analysis.mh and sens.analysis.aberrant.rank provide sensitivity analyses based on the Mantel-Haenszel test statistic and aberrant rank test statistic as described in Rosenbaum (1991, \"Sensitivity Analysis for Matched Case Control Studies\", Biometrics); see also Section 1 of Small et al.  The function adaptive.case.test provides adaptive inferences as described in Section 5 of Small et al.  The function adaptive.noether.brown provides a sensitivity analysis for a matched cohort study based on an adaptive test.  The other functions in the package are internal functions.  ",
    "version": "2.2",
    "maintainer": "Dylan Small <dsmall@wharton.upenn.edu>",
    "author": "Dylan Small",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SensitivityCaseControl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SensitivityCaseControl Sensitivity Analysis for Case-Control Studies Sensitivity analysis for case-control studies in which some cases may meet a more narrow definition of being a case compared to other cases which only meet a broad definition.  The sensitivity analyses are described in Small, Cheng, Halloran and Rosenbaum (2013, \"Case Definition and Sensitivity Analysis\", Journal of the American Statistical Association, 1457-1468).  The functions sens.analysis.mh and sens.analysis.aberrant.rank provide sensitivity analyses based on the Mantel-Haenszel test statistic and aberrant rank test statistic as described in Rosenbaum (1991, \"Sensitivity Analysis for Matched Case Control Studies\", Biometrics); see also Section 1 of Small et al.  The function adaptive.case.test provides adaptive inferences as described in Section 5 of Small et al.  The function adaptive.noether.brown provides a sensitivity analysis for a matched cohort study based on an adaptive test.  The other functions in the package are internal functions.    "
  },
  {
    "id": 7247,
    "package_name": "ShapeChange",
    "title": "Change-Point Estimation using Shape-Restricted Splines",
    "description": "In a scatterplot where the response variable is Gaussian, Poisson or binomial, we consider the case in which the mean function is smooth with a change-point, which is a mode, an inflection point or a jump point. The main routine estimates the mean curve and the change-point as well using shape-restricted B-splines. An optional subroutine delivering a bootstrap confidence interval for the change-point is incorporated in the main routine. ",
    "version": "1.5",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "author": "Xiyue Liao and Mary C Meyer ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ShapeChange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShapeChange Change-Point Estimation using Shape-Restricted Splines In a scatterplot where the response variable is Gaussian, Poisson or binomial, we consider the case in which the mean function is smooth with a change-point, which is a mode, an inflection point or a jump point. The main routine estimates the mean curve and the change-point as well using shape-restricted B-splines. An optional subroutine delivering a bootstrap confidence interval for the change-point is incorporated in the main routine.   "
  },
  {
    "id": 7249,
    "package_name": "ShapeSelectForest",
    "title": "Shape Selection for Landsat Time Series of Forest Dynamics",
    "description": "Landsat satellites collect important data about global forest conditions. Documentation about Landsat's role in forest disturbance estimation is available at the site <https://landsat.gsfc.nasa.gov/>. By constrained quadratic B-splines, this package delivers an optimal shape-restricted trajectory to a time series of Landsat imagery for the purpose of modeling annual forest disturbance dynamics to behave in an ecologically sensible manner assuming one of seven possible \"shapes\", namely, flat, decreasing, one-jump (decreasing, jump up, decreasing), inverted vee (increasing then decreasing), vee (decreasing then increasing), linear increasing, and double-jump (decreasing, jump up, decreasing, jump up, decreasing). The main routine selects the best shape according to the minimum Bayes information criterion (BIC) or the cone information criterion (CIC), which is defined as the log of the estimated predictive squared error. The package also provides parameters summarizing the temporal pattern including year(s) of inflection, magnitude of change, pre- and post-inflection rates of growth or recovery. In addition, it contains routines for converting a flat map of disturbance agents to time-series disturbance maps and a graphical routine displaying the fitted trajectory of Landsat imagery. ",
    "version": "1.7",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "author": "Xiyue Liao [aut, cre] (ORCID: <https://orcid.org/0000-0002-4508-9219>),\n  Mary Meyer [aut],\n  Elizabeth Freeman [aut],\n  Gretchen Moisen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ShapeSelectForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShapeSelectForest Shape Selection for Landsat Time Series of Forest Dynamics Landsat satellites collect important data about global forest conditions. Documentation about Landsat's role in forest disturbance estimation is available at the site <https://landsat.gsfc.nasa.gov/>. By constrained quadratic B-splines, this package delivers an optimal shape-restricted trajectory to a time series of Landsat imagery for the purpose of modeling annual forest disturbance dynamics to behave in an ecologically sensible manner assuming one of seven possible \"shapes\", namely, flat, decreasing, one-jump (decreasing, jump up, decreasing), inverted vee (increasing then decreasing), vee (decreasing then increasing), linear increasing, and double-jump (decreasing, jump up, decreasing, jump up, decreasing). The main routine selects the best shape according to the minimum Bayes information criterion (BIC) or the cone information criterion (CIC), which is defined as the log of the estimated predictive squared error. The package also provides parameters summarizing the temporal pattern including year(s) of inflection, magnitude of change, pre- and post-inflection rates of growth or recovery. In addition, it contains routines for converting a flat map of disturbance agents to time-series disturbance maps and a graphical routine displaying the fitted trajectory of Landsat imagery.   "
  },
  {
    "id": 7268,
    "package_name": "SiFINeT",
    "title": "Single Cell Feature Identification with Network Topology",
    "description": "Cluster-independent method based on topology structure of gene co-expression network for identifying feature gene sets, extracting cellular subpopulations, and elucidating intrinsic relationships among these subpopulations. Without prior cell clustering, SifiNet circumvents potential inaccuracies in clustering that may influence subsequent analyses. This method is introduced in Qi Gao, Zhicheng Ji, Liuyang Wang, Kouros Owzar, Qi-Jing Li, Cliburn Chan, Jichun Xie \"SifiNet: a robust and accurate method to identify feature gene sets and annotate cells\" (2024) <doi:10.1093/nar/gkae307>.",
    "version": "1.13",
    "maintainer": "Qi Gao <gqi@med.umich.edu>",
    "author": "Qi Gao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SiFINeT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SiFINeT Single Cell Feature Identification with Network Topology Cluster-independent method based on topology structure of gene co-expression network for identifying feature gene sets, extracting cellular subpopulations, and elucidating intrinsic relationships among these subpopulations. Without prior cell clustering, SifiNet circumvents potential inaccuracies in clustering that may influence subsequent analyses. This method is introduced in Qi Gao, Zhicheng Ji, Liuyang Wang, Kouros Owzar, Qi-Jing Li, Cliburn Chan, Jichun Xie \"SifiNet: a robust and accurate method to identify feature gene sets and annotate cells\" (2024) <doi:10.1093/nar/gkae307>.  "
  },
  {
    "id": 7284,
    "package_name": "SimComp",
    "title": "Simultaneous Comparisons for Multiple Endpoints",
    "description": "Simultaneous tests and confidence intervals are provided for one-way experimental designs with one or many normally distributed, primary response variables (endpoints). Differences (Hasler and Hothorn, 2011 <doi:10.2202/1557-4679.1258>) or ratios (Hasler and Hothorn, 2012 <doi:10.1080/19466315.2011.633868>) of means can be considered. Various contrasts can be chosen, unbalanced sample sizes are allowed as well as heterogeneous variances (Hasler and Hothorn, 2008 <doi:10.1002/bimj.200710466>) or covariance matrices (Hasler, 2014 <doi:10.1515/ijb-2012-0015>).",
    "version": "3.6",
    "maintainer": "Mario Hasler <hasler@email.uni-kiel.de>",
    "author": "Mario Hasler [aut, cre],\n  Christof Kluss [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimComp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimComp Simultaneous Comparisons for Multiple Endpoints Simultaneous tests and confidence intervals are provided for one-way experimental designs with one or many normally distributed, primary response variables (endpoints). Differences (Hasler and Hothorn, 2011 <doi:10.2202/1557-4679.1258>) or ratios (Hasler and Hothorn, 2012 <doi:10.1080/19466315.2011.633868>) of means can be considered. Various contrasts can be chosen, unbalanced sample sizes are allowed as well as heterogeneous variances (Hasler and Hothorn, 2008 <doi:10.1002/bimj.200710466>) or covariance matrices (Hasler, 2014 <doi:10.1515/ijb-2012-0015>).  "
  },
  {
    "id": 7287,
    "package_name": "SimCorrMix",
    "title": "Simulation of Correlated Data with Multiple Variable Types\nIncluding Continuous and Count Mixture Distributions",
    "description": "Generate continuous (normal, non-normal, or mixture distributions), binary, ordinal, \n    and count (regular or zero-inflated, Poisson or Negative Binomial) variables with a specified \n    correlation matrix, or one continuous variable with a mixture distribution.  This package can \n    be used to simulate data sets that mimic real-world clinical or genetic data sets (i.e., \n    plasmodes, as in Vaughan et al., 2009 <DOI:10.1016/j.csda.2008.02.032>).  The methods \n    extend those found in the 'SimMultiCorrData' R package.  Standard normal variables with an \n    imposed intermediate correlation matrix are transformed to generate the desired distributions.  \n    Continuous variables are simulated using either Fleishman (1978)'s third order \n    <DOI:10.1007/BF02293811> or Headrick (2002)'s fifth order \n    <DOI:10.1016/S0167-9473(02)00072-5> polynomial transformation method (the power method \n    transformation, PMT).  Non-mixture distributions require the user to specify mean, variance, \n    skewness, standardized kurtosis, and standardized fifth and sixth cumulants.  Mixture \n    distributions require these inputs for the component distributions plus the mixing \n    probabilities.  Simulation occurs at the component level for continuous mixture \n    distributions.  The target correlation matrix is specified in terms of correlations with \n    components of continuous mixture variables.  These components are transformed into the \n    desired mixture variables using random multinomial variables based on the mixing \n    probabilities.  However, the package provides functions to approximate expected correlations \n    with continuous mixture variables given target correlations with the components. Binary and \n    ordinal variables are simulated using a modification of ordsample() in package 'GenOrd'.  \n    Count variables are simulated using the inverse CDF method.  There are two simulation \n    pathways which calculate intermediate correlations involving count variables differently.  \n    Correlation Method 1 adapts Yahav and Shmueli's 2012 method <DOI:10.1002/asmb.901> and \n    performs best with large count variable means and positive correlations or small means and \n    negative correlations.  Correlation Method 2 adapts Barbiero and Ferrari's 2015 \n    modification of the 'GenOrd' package <DOI:10.1002/asmb.2072> and performs best under the \n    opposite scenarios.  The optional error loop may be used to improve the accuracy of the \n    final correlation matrix.  The package also contains functions to calculate the \n    standardized cumulants of continuous mixture distributions, check parameter inputs, \n    calculate feasible correlation boundaries, and summarize and plot simulated variables.",
    "version": "0.1.1",
    "maintainer": "Allison Cynthia Fialkowski <allijazz@uab.edu>",
    "author": "Allison Cynthia Fialkowski",
    "url": "https://github.com/AFialkowski/SimCorrMix",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimCorrMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimCorrMix Simulation of Correlated Data with Multiple Variable Types\nIncluding Continuous and Count Mixture Distributions Generate continuous (normal, non-normal, or mixture distributions), binary, ordinal, \n    and count (regular or zero-inflated, Poisson or Negative Binomial) variables with a specified \n    correlation matrix, or one continuous variable with a mixture distribution.  This package can \n    be used to simulate data sets that mimic real-world clinical or genetic data sets (i.e., \n    plasmodes, as in Vaughan et al., 2009 <DOI:10.1016/j.csda.2008.02.032>).  The methods \n    extend those found in the 'SimMultiCorrData' R package.  Standard normal variables with an \n    imposed intermediate correlation matrix are transformed to generate the desired distributions.  \n    Continuous variables are simulated using either Fleishman (1978)'s third order \n    <DOI:10.1007/BF02293811> or Headrick (2002)'s fifth order \n    <DOI:10.1016/S0167-9473(02)00072-5> polynomial transformation method (the power method \n    transformation, PMT).  Non-mixture distributions require the user to specify mean, variance, \n    skewness, standardized kurtosis, and standardized fifth and sixth cumulants.  Mixture \n    distributions require these inputs for the component distributions plus the mixing \n    probabilities.  Simulation occurs at the component level for continuous mixture \n    distributions.  The target correlation matrix is specified in terms of correlations with \n    components of continuous mixture variables.  These components are transformed into the \n    desired mixture variables using random multinomial variables based on the mixing \n    probabilities.  However, the package provides functions to approximate expected correlations \n    with continuous mixture variables given target correlations with the components. Binary and \n    ordinal variables are simulated using a modification of ordsample() in package 'GenOrd'.  \n    Count variables are simulated using the inverse CDF method.  There are two simulation \n    pathways which calculate intermediate correlations involving count variables differently.  \n    Correlation Method 1 adapts Yahav and Shmueli's 2012 method <DOI:10.1002/asmb.901> and \n    performs best with large count variable means and positive correlations or small means and \n    negative correlations.  Correlation Method 2 adapts Barbiero and Ferrari's 2015 \n    modification of the 'GenOrd' package <DOI:10.1002/asmb.2072> and performs best under the \n    opposite scenarios.  The optional error loop may be used to improve the accuracy of the \n    final correlation matrix.  The package also contains functions to calculate the \n    standardized cumulants of continuous mixture distributions, check parameter inputs, \n    calculate feasible correlation boundaries, and summarize and plot simulated variables.  "
  },
  {
    "id": 7289,
    "package_name": "SimEUCartelLaw",
    "title": "Simulation of Legal Exemption System for European Cartel Law",
    "description": "Monte Carlo simulations of a game-theoretic model for the \n\tlegal exemption system of the European cartel law are implemented\n\tin order to estimate the (mean) deterrent effect of this system.\n\tThe input and output parameters of the simulated cartel \n\topportunities can be visualized by three-dimensional projections.\n\tA description of the model is given in Moritz et al. (2018)\n\t<doi:10.1515/bejeap-2017-0235>.",
    "version": "1.0.4",
    "maintainer": "Martin Becker <martin.becker@mx.uni-saarland.de>",
    "author": "Martin Becker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2336-9751>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimEUCartelLaw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimEUCartelLaw Simulation of Legal Exemption System for European Cartel Law Monte Carlo simulations of a game-theoretic model for the \n\tlegal exemption system of the European cartel law are implemented\n\tin order to estimate the (mean) deterrent effect of this system.\n\tThe input and output parameters of the simulated cartel \n\topportunities can be visualized by three-dimensional projections.\n\tA description of the model is given in Moritz et al. (2018)\n\t<doi:10.1515/bejeap-2017-0235>.  "
  },
  {
    "id": 7333,
    "package_name": "Snake",
    "title": "Game of Snake",
    "description": "Implements snake in R as a programming example, see <https://en.wikipedia.org/wiki/Snake_(video_game_genre)>.",
    "version": "1.0",
    "maintainer": "Carsten Croonenbroeck <carsten.croonenbroeck@uni-rostock.de>",
    "author": "Carsten Croonenbroeck [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Snake",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Snake Game of Snake Implements snake in R as a programming example, see <https://en.wikipedia.org/wiki/Snake_(video_game_genre)>.  "
  },
  {
    "id": 7334,
    "package_name": "SnakesAndLaddersAnalysis",
    "title": "Play and Analyse the Game of Snakes and Ladders",
    "description": "Plays the game of Snakes and Ladders and has tools for analyses. The tools included allow you to find the average moves to win, frequency of each square, importance of the snakes and the ladders, the most common square and the plotting of the game played.",
    "version": "2.1.0",
    "maintainer": "Hector Haffenden <haffendenh@cardiff.ac.uk>",
    "author": "Hector haffenden <haffendenh@cardiff.ac.uk>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SnakesAndLaddersAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SnakesAndLaddersAnalysis Play and Analyse the Game of Snakes and Ladders Plays the game of Snakes and Ladders and has tools for analyses. The tools included allow you to find the average moves to win, frequency of each square, importance of the snakes and the ladders, the most common square and the plotting of the game played.  "
  },
  {
    "id": 7405,
    "package_name": "SpatialRoMLE",
    "title": "Robust Maximum Likelihood Estimation for Spatial Error Model",
    "description": "Provides robust estimation for spatial error model to presence of outliers in the residuals. The classical estimation methods can be influenced by the presence of outliers in the data. We proposed a robust estimation approach based on the robustified likelihood equations for spatial error model (Vural Yildirim & Yeliz Mert Kantar (2020): Robust estimation approach for spatial error model, Journal of Statistical Computation and Simulation, <doi:10.1080/00949655.2020.1740223>).",
    "version": "0.1.1.1",
    "maintainer": "Vural Yildirim <vurall_yildirim@hotmail.com>",
    "author": "Vural Yildirim [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6517-7849>),\n  Yeliz Mert Kantar [aut, ths] (ORCID:\n    <https://orcid.org/0000-0001-7101-8943>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatialRoMLE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatialRoMLE Robust Maximum Likelihood Estimation for Spatial Error Model Provides robust estimation for spatial error model to presence of outliers in the residuals. The classical estimation methods can be influenced by the presence of outliers in the data. We proposed a robust estimation approach based on the robustified likelihood equations for spatial error model (Vural Yildirim & Yeliz Mert Kantar (2020): Robust estimation approach for spatial error model, Journal of Statistical Computation and Simulation, <doi:10.1080/00949655.2020.1740223>).  "
  },
  {
    "id": 7436,
    "package_name": "Sshaped",
    "title": "Nonparametric, Tuning-Free Estimation of S-Shaped Functions",
    "description": "Estimation of an S-shaped function and its corresponding inflection point via a least squares approach. A sequential mixed primal-dual based algorithm is implemented for the fast computation. Details can be found in Feng et al. (2022) <doi:10.1111/rssb.12481>.",
    "version": "1.2",
    "maintainer": "Yining Chen <y.chen101@lse.ac.uk>",
    "author": "Oliver Y. Feng [aut],\n  Yining Chen [aut, cre],\n  Qiyang Han [aut],\n  Raymond J. Carroll [aut],\n  Richard J. Samworth [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Sshaped",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Sshaped Nonparametric, Tuning-Free Estimation of S-Shaped Functions Estimation of an S-shaped function and its corresponding inflection point via a least squares approach. A sequential mixed primal-dual based algorithm is implemented for the fast computation. Details can be found in Feng et al. (2022) <doi:10.1111/rssb.12481>.  "
  },
  {
    "id": 7447,
    "package_name": "StakeholderAnalysis",
    "title": "Measuring Stakeholder Influence",
    "description": "Proposes an original instrument for measuring stakeholder influence on the development of an infrastructure project that is carried through by a municipality, drawing on stakeholder classifications (Mitchell, Agle, & Wood, 1997) and input-output modelling (Hester & Adams, 2013). Mitchell R., Agle B.R., & Wood D.J. <doi:10.2307/259247> Hester, P.T., & Adams, K.M. (2013) <doi:10.1016/j.procs.2013.09.282>.",
    "version": "1.2",
    "maintainer": "Lech Kujawski <lech.kujawski@ug.edu.pl>",
    "author": "Anna Zamojska [aut],\n  Piotr Zientara [aut],\n  Sebastian Susmarski [aut],\n  Lech Kujawski [aut, cre]",
    "url": "https://www.r-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StakeholderAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StakeholderAnalysis Measuring Stakeholder Influence Proposes an original instrument for measuring stakeholder influence on the development of an infrastructure project that is carried through by a municipality, drawing on stakeholder classifications (Mitchell, Agle, & Wood, 1997) and input-output modelling (Hester & Adams, 2013). Mitchell R., Agle B.R., & Wood D.J. <doi:10.2307/259247> Hester, P.T., & Adams, K.M. (2013) <doi:10.1016/j.procs.2013.09.282>.  "
  },
  {
    "id": 7478,
    "package_name": "StratPal",
    "title": "Stratigraphic Paleobiology Modeling Pipelines",
    "description": "The fossil record is a joint expression of ecological, taphonomic, \n    evolutionary, and stratigraphic processes (Holland and Patzkowsky, 2012, ISBN:978-0226649382).\n    This package allowing to simulate biological processes in the time domain\n    (e.g., trait evolution, fossil abundance, phylogenetic trees), and examine how their expression\n    in the rock record (stratigraphic domain) is influenced based on \n    age-depth models, ecological niche models, and taphonomic effects.\n    Functions simulating common processes used in modeling trait evolution, biostratigraphy or \n    event type data such as first/last occurrences are provided and can be used \n    standalone or as part of a pipeline. The package comes with example \n    data sets and tutorials in several vignettes, which can be used as a \n    template to set up one's own simulation.",
    "version": "0.7.1",
    "maintainer": "Niklas Hohmann <N.H.Hohmann@uu.nl>",
    "author": "Niklas Hohmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1559-1838>)",
    "url": "https://mindthegap-erc.github.io/StratPal/ ,\nhttps://github.com/MindTheGap-ERC/StratPal",
    "bug_reports": "https://github.com/MindTheGap-ERC/StratPal/issues",
    "repository": "https://cran.r-project.org/package=StratPal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StratPal Stratigraphic Paleobiology Modeling Pipelines The fossil record is a joint expression of ecological, taphonomic, \n    evolutionary, and stratigraphic processes (Holland and Patzkowsky, 2012, ISBN:978-0226649382).\n    This package allowing to simulate biological processes in the time domain\n    (e.g., trait evolution, fossil abundance, phylogenetic trees), and examine how their expression\n    in the rock record (stratigraphic domain) is influenced based on \n    age-depth models, ecological niche models, and taphonomic effects.\n    Functions simulating common processes used in modeling trait evolution, biostratigraphy or \n    event type data such as first/last occurrences are provided and can be used \n    standalone or as part of a pipeline. The package comes with example \n    data sets and tutorials in several vignettes, which can be used as a \n    template to set up one's own simulation.  "
  },
  {
    "id": 7496,
    "package_name": "SuRF.vs",
    "title": "Subsampling Ranking Forward Selection (SuRF)",
    "description": "Performs variable selection based on subsampling, ranking forward selection. Details of the method are published in Lihui Liu, Hong Gu, Johan Van Limbergen, Toby Kenney (2020) SuRF: A new method for sparse variable selection, with application in microbiome data analysis  Statistics in Medicine 40 897-919 <doi:10.1002/sim.8809>. Xo is the matrix of predictor variables. y is the response variable. Currently only binary responses using logistic regression are supported. X is a matrix of additional predictors which should be scaled to have sum 1 prior to analysis. fold is the number of folds for cross-validation. Alpha is the parameter for the elastic net method used in the subsampling procedure: the default value of 1 corresponds to LASSO. prop is the proportion of variables to remove in the each subsample. weights indicates whether observations should be weighted by class size. When the class sizes are unbalanced, weighting observations can improve results. B is the number of subsamples to use for ranking the variables. C is the number of permutations to use for estimating the critical value of the null distribution. If the 'doParallel' package is installed, the function can be run in parallel by setting ncores to the number of threads to use. If the default value of 1 is used, or if the 'doParallel' package is not installed, the function does not run in parallel. display.progress indicates whether the function should display messages indicating its progress. family is a family variable for the glm() fitting. Note that the 'glmnet' package does not permit the use of nonstandard link functions, so will always use the default link function. However, the glm() fitting will use the specified link. The default is binomial with logistic regression, because this is a common use case. pval is the p-value for inclusion of a variable in the model. Under the null case, the number of false positives will be geometrically distributed with this as probability of success, so if this parameter is set to p, the expected number of false positives should be p/(1-p).",
    "version": "1.1.0.1",
    "maintainer": "Toby Kenney <tkenney@mathstat.dal.ca>",
    "author": "Lihui Liu [aut],\n  Toby Kenney [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SuRF.vs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SuRF.vs Subsampling Ranking Forward Selection (SuRF) Performs variable selection based on subsampling, ranking forward selection. Details of the method are published in Lihui Liu, Hong Gu, Johan Van Limbergen, Toby Kenney (2020) SuRF: A new method for sparse variable selection, with application in microbiome data analysis  Statistics in Medicine 40 897-919 <doi:10.1002/sim.8809>. Xo is the matrix of predictor variables. y is the response variable. Currently only binary responses using logistic regression are supported. X is a matrix of additional predictors which should be scaled to have sum 1 prior to analysis. fold is the number of folds for cross-validation. Alpha is the parameter for the elastic net method used in the subsampling procedure: the default value of 1 corresponds to LASSO. prop is the proportion of variables to remove in the each subsample. weights indicates whether observations should be weighted by class size. When the class sizes are unbalanced, weighting observations can improve results. B is the number of subsamples to use for ranking the variables. C is the number of permutations to use for estimating the critical value of the null distribution. If the 'doParallel' package is installed, the function can be run in parallel by setting ncores to the number of threads to use. If the default value of 1 is used, or if the 'doParallel' package is not installed, the function does not run in parallel. display.progress indicates whether the function should display messages indicating its progress. family is a family variable for the glm() fitting. Note that the 'glmnet' package does not permit the use of nonstandard link functions, so will always use the default link function. However, the glm() fitting will use the specified link. The default is binomial with logistic regression, because this is a common use case. pval is the p-value for inclusion of a variable in the model. Under the null case, the number of false positives will be geometrically distributed with this as probability of success, so if this parameter is set to p, the expected number of false positives should be p/(1-p).  "
  },
  {
    "id": 7507,
    "package_name": "Sunclarco",
    "title": "Survival Analysis using Copulas",
    "description": "Survival analysis for unbalanced clusters using Archimedean copulas (Prenen et al. (2016) <DOI:10.1111/rssb.12174>).",
    "version": "1.0.0",
    "maintainer": "Roel Braekers <roel.braekers@uhasselt.be>",
    "author": "Leen Prenen, Roel Braekers, Luc Duchateau and Ewoud De Troyer",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Sunclarco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Sunclarco Survival Analysis using Copulas Survival analysis for unbalanced clusters using Archimedean copulas (Prenen et al. (2016) <DOI:10.1111/rssb.12174>).  "
  },
  {
    "id": 7694,
    "package_name": "TUGLab",
    "title": "A Laboratory for TU Games",
    "description": "Cooperative game theory models decision-making situations in\n    which a group of agents, called players, may achieve certain benefits\n    by cooperating to reach an optimal outcome. It has great potential in\n    different fields, since it offers a scenario to analyze and solve\n    problems in which cooperation is essential to achieve a common goal.\n    The 'TUGLab' (Transferable Utility Games Laboratory) R package\n    contains a set of scripts that could serve as a helpful complement to\n    the books and other materials used in courses on cooperative game\n    theory, and also as a practical tool for researchers working in this\n    field. The 'TUGLab' project was born in 2006 trying to highlight the\n    geometrical aspects of the theory of cooperative games for 3 and 4\n    players. 'TUGlabWeb' is an online platform on which the basic\n    functions of 'TUGLab' are implemented, and it is being used all over\n    the world as a resource in degree, master's and doctoral programs.\n    This package is an extension of the first versions and enables users\n    to work with games in general (computational restrictions aside). The\n    user can check properties of games, compute well-known games and\n    calculate several set-valued and single-valued solutions such as the\n    core, the Shapley value, the nucleolus or the core-center. The package\n    also illustrates how the Shapley value flexibly adapts to various\n    cooperative game settings, including weighted players and coalitions,\n    a priori unions, and restricted communication structures. In keeping\n    with the original philosophy of the first versions, special emphasis\n    is placed on the graphical representation of the solution concepts for\n    3 and 4 players.",
    "version": "0.0.1",
    "maintainer": "\u00c1lvaro de Prado Saborido <alvarodepradosaborido@gmail.com>",
    "author": "\u00c1lvaro de Prado Saborido [aut, cre] (Departamento de Estat\u00edstica e\n    Investigaci\u00f3n Operativa. Universidade de Vigo. Spain),\n  Alejandro Bern\u00e1rdez Ferrad\u00e1s [ctb] (ORCID:\n    <https://orcid.org/0009-0006-0960-3555>, SiDOR. Departamento de\n    Estat\u00edstica e Investigaci\u00f3n Operativa. Universidade de Vigo.\n    CITMAga. Spain),\n  Miguel \u00c1ngel Mir\u00e1s Calvo [aut] (ORCID:\n    <https://orcid.org/0000-0001-7247-1926>, RGEAF. Departamento de\n    Matem\u00e1ticas. Universidade de Vigo. Spain),\n  Iago N\u00fa\u00f1ez Lugilde [aut] (ORCID:\n    <https://orcid.org/0000-0003-3382-0737>, Departamento de\n    Matem\u00e1ticas. MODES. Universidade da Coru\u00f1a. Spain),\n  Carmen Quinteiro Sandomingo [aut] (ORCID:\n    <https://orcid.org/0000-0002-2711-1945>, Departamento de\n    Matem\u00e1ticas. Universidade de Vigo. Spain),\n  Estela S\u00e1nchez Rodr\u00edguez [aut] (ORCID:\n    <https://orcid.org/0000-0002-0933-6411>, SiDOR. Departamento de\n    Estat\u00edstica e Investigaci\u00f3n Operativa. Universidade de Vigo.\n    CITMAga. Spain),\n  MCIN/AEI/10.13039/501100011033 [fnd] (Project PID2021-124030NB-C33.\n    ERDF A way of making Europe/EU)",
    "url": "http://tuglabweb.uvigo.es/TUGlabWEB2/index.php,\nhttps://mmiras.webs.uvigo.es/TUGlab/",
    "bug_reports": "https://github.com/esanchez-coder/TUGLab/issues",
    "repository": "https://cran.r-project.org/package=TUGLab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TUGLab A Laboratory for TU Games Cooperative game theory models decision-making situations in\n    which a group of agents, called players, may achieve certain benefits\n    by cooperating to reach an optimal outcome. It has great potential in\n    different fields, since it offers a scenario to analyze and solve\n    problems in which cooperation is essential to achieve a common goal.\n    The 'TUGLab' (Transferable Utility Games Laboratory) R package\n    contains a set of scripts that could serve as a helpful complement to\n    the books and other materials used in courses on cooperative game\n    theory, and also as a practical tool for researchers working in this\n    field. The 'TUGLab' project was born in 2006 trying to highlight the\n    geometrical aspects of the theory of cooperative games for 3 and 4\n    players. 'TUGlabWeb' is an online platform on which the basic\n    functions of 'TUGLab' are implemented, and it is being used all over\n    the world as a resource in degree, master's and doctoral programs.\n    This package is an extension of the first versions and enables users\n    to work with games in general (computational restrictions aside). The\n    user can check properties of games, compute well-known games and\n    calculate several set-valued and single-valued solutions such as the\n    core, the Shapley value, the nucleolus or the core-center. The package\n    also illustrates how the Shapley value flexibly adapts to various\n    cooperative game settings, including weighted players and coalitions,\n    a priori unions, and restricted communication structures. In keeping\n    with the original philosophy of the first versions, special emphasis\n    is placed on the graphical representation of the solution concepts for\n    3 and 4 players.  "
  },
  {
    "id": 7709,
    "package_name": "TapeR",
    "title": "Flexible Tree Taper Curves Based on Semiparametric Mixed Models",
    "description": "Implementation of functions for fitting taper curves (a semiparametric \n  linear mixed effects taper model) to diameter measurements along stems. Further \n  functions are provided to estimate the uncertainty around the predicted curves, \n  to calculate timber volume (also by sections) and marginal (e.g., upper) diameters. \n  For cases where tree heights are not measured, methods for estimating\n  additional variance in volume predictions resulting from uncertainties in\n  tree height models (tariffs) are provided.  The example data include the taper \n  curve parameters for Norway spruce used in the 3rd German NFI fitted to 380 trees \n  and a subset of section-wise diameter measurements of these trees. The functions \n  implemented here are detailed in Kublin, E., Breidenbach, J., Kaendler, G. (2013)\n  <doi:10.1007/s10342-013-0715-0>.",
    "version": "0.5.3",
    "maintainer": "Christian Vonderach <christian.vonderach@forst.bwl.de>",
    "author": "Edgar Kublin [aut],\n  Johannes Breidenbach [ctb],\n  Christian Vonderach [ctb, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TapeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TapeR Flexible Tree Taper Curves Based on Semiparametric Mixed Models Implementation of functions for fitting taper curves (a semiparametric \n  linear mixed effects taper model) to diameter measurements along stems. Further \n  functions are provided to estimate the uncertainty around the predicted curves, \n  to calculate timber volume (also by sections) and marginal (e.g., upper) diameters. \n  For cases where tree heights are not measured, methods for estimating\n  additional variance in volume predictions resulting from uncertainties in\n  tree height models (tariffs) are provided.  The example data include the taper \n  curve parameters for Norway spruce used in the 3rd German NFI fitted to 380 trees \n  and a subset of section-wise diameter measurements of these trees. The functions \n  implemented here are detailed in Kublin, E., Breidenbach, J., Kaendler, G. (2013)\n  <doi:10.1007/s10342-013-0715-0>.  "
  },
  {
    "id": 7713,
    "package_name": "TaxaNorm",
    "title": "Feature-Wise Normalization for Microbiome Sequencing Data",
    "description": "A novel feature-wise normalization method based on a zero-inflated negative binomial model. This method assumes that the effects of sequencing depth vary for each taxon on their mean and also incorporates a rational link of zero probability and taxon dispersion as a function of sequencing depth. Ziyue Wang, Dillon Lloyd, Shanshan Zhao, Alison Motsinger-Reif (2023) <doi:10.1101/2023.10.31.563648>.",
    "version": "2.4",
    "maintainer": "Dillon Lloyd <dtlloyd@ncsu.edu>",
    "author": "Ziyue Wang [aut],\n  Dillon Lloyd [aut, cre, cph],\n  Shanshan Zhao [aut, ctb],\n  Alison Motsinger-Reif [aut, ctb]",
    "url": "https://github.com/wangziyue57/TaxaNorm",
    "bug_reports": "https://github.com/wangziyue57/TaxaNorm/issues",
    "repository": "https://cran.r-project.org/package=TaxaNorm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TaxaNorm Feature-Wise Normalization for Microbiome Sequencing Data A novel feature-wise normalization method based on a zero-inflated negative binomial model. This method assumes that the effects of sequencing depth vary for each taxon on their mean and also incorporates a rational link of zero probability and taxon dispersion as a function of sequencing depth. Ziyue Wang, Dillon Lloyd, Shanshan Zhao, Alison Motsinger-Reif (2023) <doi:10.1101/2023.10.31.563648>.  "
  },
  {
    "id": 7736,
    "package_name": "TestDesign",
    "title": "Optimal Test Design Approach to Fixed and Adaptive Test\nConstruction",
    "description": "Uses the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and\n    van der Linden (2018) <doi:10.1201/9781315117430> to construct fixed, adaptive, and parallel tests.\n    Supports the following mixed-integer programming (MIP) solver packages: 'Rsymphony',\n    'highs', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package is not available from CRAN; see <https://www.gurobi.com/downloads/>.",
    "version": "1.7.0",
    "maintainer": "Seung W. Choi <schoi@austin.utexas.edu>",
    "author": "Seung W. Choi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4777-5420>),\n  Sangdon Lim [aut] (ORCID: <https://orcid.org/0000-0002-2988-014X>)",
    "url": "https://choi-phd.github.io/TestDesign/ (documentation)",
    "bug_reports": "https://github.com/choi-phd/TestDesign/issues/",
    "repository": "https://cran.r-project.org/package=TestDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestDesign Optimal Test Design Approach to Fixed and Adaptive Test\nConstruction Uses the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and\n    van der Linden (2018) <doi:10.1201/9781315117430> to construct fixed, adaptive, and parallel tests.\n    Supports the following mixed-integer programming (MIP) solver packages: 'Rsymphony',\n    'highs', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package is not available from CRAN; see <https://www.gurobi.com/downloads/>.  "
  },
  {
    "id": 7836,
    "package_name": "TrueSkillThroughTime",
    "title": "Skill Estimation Based on a Single Bayesian Network",
    "description": "\n    Most estimators implemented by the video game industry cannot obtain reliable initial estimates nor guarantee comparability between distant estimates. TrueSkill Through Time solves all these problems by modeling the entire history of activities using a single Bayesian network allowing the information to propagate correctly throughout the system. This algorithm requires only a few iterations to converge, allowing millions of observations to be analyzed using any low-end computer.\n    Landfried G, Mocskos E (2025). \"TrueSkill Through Time: Reliable Initial Skill Estimates and Historical Comparability with Julia, Python, and R.\" <doi:10.18637/jss.v112.i06>.\n    The core ideas implemented in this project were developed by Dangauthier P, Herbrich R, Minka T, Graepel T (2007). \"Trueskill through time: Revisiting the history of chess.\".",
    "version": "1.0.0",
    "maintainer": "Gustavo Landfried <gustavolandfried@gmail.com>",
    "author": "Gustavo Landfried [aut, cre]",
    "url": "https://github.com/glandfried/TrueSkillThroughTime.R",
    "bug_reports": "https://github.com/glandfried/TrueSkillThroughTime.R/issues",
    "repository": "https://cran.r-project.org/package=TrueSkillThroughTime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TrueSkillThroughTime Skill Estimation Based on a Single Bayesian Network \n    Most estimators implemented by the video game industry cannot obtain reliable initial estimates nor guarantee comparability between distant estimates. TrueSkill Through Time solves all these problems by modeling the entire history of activities using a single Bayesian network allowing the information to propagate correctly throughout the system. This algorithm requires only a few iterations to converge, allowing millions of observations to be analyzed using any low-end computer.\n    Landfried G, Mocskos E (2025). \"TrueSkill Through Time: Reliable Initial Skill Estimates and Historical Comparability with Julia, Python, and R.\" <doi:10.18637/jss.v112.i06>.\n    The core ideas implemented in this project were developed by Dangauthier P, Herbrich R, Minka T, Graepel T (2007). \"Trueskill through time: Revisiting the history of chess.\".  "
  },
  {
    "id": 7866,
    "package_name": "UHM",
    "title": "Unified Zero-Inflated Hurdle Regression Models",
    "description": "Run a Gibbs sampler for hurdle models to analyze data showing an excess of zeros, which is common in zero-inflated count and semi-continuous models. The package includes the hurdle model under Gaussian, Gamma, inverse Gaussian, Weibull, Exponential, Beta, Poisson, negative binomial, logarithmic, Bell, generalized Poisson, and binomial distributional assumptions. The models described in Ganjali et al. (2024).",
    "version": "0.3.0",
    "maintainer": "Taban Baghfalaki <t.baghfalaki@gmail.com>",
    "author": "Taban Baghfalaki [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-2100-4532>),\n  Mojtaba Ganjali [aut] (ORCID: <https://orcid.org/0000-0002-8574-1750>),\n  Narayanaswamy Balakrishnan [aut]",
    "url": "https://github.com/tbaghfalaki/UHM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UHM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UHM Unified Zero-Inflated Hurdle Regression Models Run a Gibbs sampler for hurdle models to analyze data showing an excess of zeros, which is common in zero-inflated count and semi-continuous models. The package includes the hurdle model under Gaussian, Gamma, inverse Gaussian, Weibull, Exponential, Beta, Poisson, negative binomial, logarithmic, Bell, generalized Poisson, and binomial distributional assumptions. The models described in Ganjali et al. (2024).  "
  },
  {
    "id": 7925,
    "package_name": "VCA",
    "title": "Variance Component Analysis",
    "description": "\n ANOVA and REML estimation of linear mixed models is implemented, once following\n Searle et al. (1991, ANOVA for unbalanced data), once making use of the 'lme4' package.\n The primary objective of this package is to perform a variance component analysis (VCA)\n according to CLSI EP05-A3 guideline \"Evaluation of Precision of Quantitative Measurement\n Procedures\" (2014). There are plotting methods for visualization of an experimental design,\n plotting random effects and residuals. For ANOVA type estimation two methods for computing\n ANOVA mean squares are implemented (SWEEP and quadratic forms). The covariance matrix of \n variance components can be derived, which is used in estimating confidence intervals. Linear\n hypotheses of fixed effects and LS means can be computed. LS means can be computed at specific\n values of covariables and with custom weighting schemes for factor variables. See ?VCA for a\n more comprehensive description of the features. ",
    "version": "1.5.2",
    "maintainer": "Andre Schuetzenmeister <andre.schuetzenmeister@roche.com>",
    "author": "Andre Schuetzenmeister [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2964-5502>),\n  Florian Dufey [aut] (ORCID: <https://orcid.org/0000-0001-6467-8556>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VCA Variance Component Analysis \n ANOVA and REML estimation of linear mixed models is implemented, once following\n Searle et al. (1991, ANOVA for unbalanced data), once making use of the 'lme4' package.\n The primary objective of this package is to perform a variance component analysis (VCA)\n according to CLSI EP05-A3 guideline \"Evaluation of Precision of Quantitative Measurement\n Procedures\" (2014). There are plotting methods for visualization of an experimental design,\n plotting random effects and residuals. For ANOVA type estimation two methods for computing\n ANOVA mean squares are implemented (SWEEP and quadratic forms). The covariance matrix of \n variance components can be derived, which is used in estimating confidence intervals. Linear\n hypotheses of fixed effects and LS means can be computed. LS means can be computed at specific\n values of covariables and with custom weighting schemes for factor variables. See ?VCA for a\n more comprehensive description of the features.   "
  },
  {
    "id": 7938,
    "package_name": "VGAMextra",
    "title": "Additions and Extensions of the 'VGAM' Package",
    "description": "Extending the functionalities of the 'VGAM' package with\n         additional functions and datasets. At present, 'VGAMextra'\n         comprises new family functions (ffs) to estimate several time\n         series models by maximum likelihood using Fisher scoring, \n         unlike popular packages in CRAN relying on optim(), including\n         ARMA-GARCH-like models, the Order-(p, d, q) ARIMAX model (non-\n         seasonal), the Order-(p) VAR model, error correction models\n         for cointegrated time series, and ARMA-structures with Student-t \n         errors. For independent data, new ffs to estimate the inverse-\n         Weibull, the inverse-gamma, the generalized beta of the second\n         kind and the general multivariate normal distributions are\n         available. In addition, 'VGAMextra' incorporates new VGLM-links\n         for the mean-function, and the quantile-function (as an alternative\n         to ordinary quantile modelling) of several 1-parameter distributions,\n         that are compatible with the class of VGLM/VGAM family functions.\n         Currently, only fixed-effects models are implemented. All functions\n         are subject to change; see the NEWS for further details on the\n         latest changes.",
    "version": "0.0-9",
    "maintainer": "Victor Miranda <victor.miranda@aut.ac.nz>",
    "author": "Victor Miranda [aut, cre, cph],\n  Thomas Yee [ctb, ths, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VGAMextra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VGAMextra Additions and Extensions of the 'VGAM' Package Extending the functionalities of the 'VGAM' package with\n         additional functions and datasets. At present, 'VGAMextra'\n         comprises new family functions (ffs) to estimate several time\n         series models by maximum likelihood using Fisher scoring, \n         unlike popular packages in CRAN relying on optim(), including\n         ARMA-GARCH-like models, the Order-(p, d, q) ARIMAX model (non-\n         seasonal), the Order-(p) VAR model, error correction models\n         for cointegrated time series, and ARMA-structures with Student-t \n         errors. For independent data, new ffs to estimate the inverse-\n         Weibull, the inverse-gamma, the generalized beta of the second\n         kind and the general multivariate normal distributions are\n         available. In addition, 'VGAMextra' incorporates new VGLM-links\n         for the mean-function, and the quantile-function (as an alternative\n         to ordinary quantile modelling) of several 1-parameter distributions,\n         that are compatible with the class of VGLM/VGAM family functions.\n         Currently, only fixed-effects models are implemented. All functions\n         are subject to change; see the NEWS for further details on the\n         latest changes.  "
  },
  {
    "id": 7955,
    "package_name": "VSOLassoBag",
    "title": "Variable Selection Oriented LASSO Bagging Algorithm",
    "description": "A wrapped LASSO approach by integrating an ensemble learning strategy to help select efficient, stable, and high confidential variables from omics-based data. Using a bagging strategy in combination of a parametric method or inflection point search method for cut-off threshold determination. This package can integrate and vote variables generated from multiple LASSO models to determine the optimal candidates. Luo H, Zhao Q, et al (2020) <doi:10.1126/scitranslmed.aax7533> for more details.",
    "version": "1.0",
    "maintainer": "Chaoye Wang <wangcy1@sysucc.org.cn>",
    "author": "Jiaqi Liang [aut],\n  Chaoye Wang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VSOLassoBag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VSOLassoBag Variable Selection Oriented LASSO Bagging Algorithm A wrapped LASSO approach by integrating an ensemble learning strategy to help select efficient, stable, and high confidential variables from omics-based data. Using a bagging strategy in combination of a parametric method or inflection point search method for cut-off threshold determination. This package can integrate and vote variables generated from multiple LASSO models to determine the optimal candidates. Luo H, Zhao Q, et al (2020) <doi:10.1126/scitranslmed.aax7533> for more details.  "
  },
  {
    "id": 8004,
    "package_name": "WAnova",
    "title": "Welch's Anova from Summary Statistics",
    "description": "Provides the functions to perform a Welch's one-way Anova with fixed effects\n  based on summary statistics (sample size, means, standard deviation) and the Games-Howell post hoc test\n  for multiple comparisons and provides the effect size estimator adjusted omega squared.\n  In addition sample size estimation can be computed based on Levy's method, and a Monte Carlo\n  simulation is included to bootstrap residual normality and homoscedasticity\n  Welch, B. L. (1951) <doi:10.1093/biomet/38.3-4.330>\n  Kirk, R. E. (1996) <doi:10.1177/0013164496056005002>\n  Carroll, R. M., & Nordholm, L. A. (1975) <doi:10.1177/001316447503500304>\n  Albers, C., & Lakens, D. (2018) <doi:10.1016/j.jesp.2017.09.004>\n  Games, P. A., & Howell, J. F. (1976) <doi:10.2307/1164979>\n  Levy, K. J. (1978a) <doi:10.1080/00949657808810246>\n  Show-Li, J., & Gwowen, S. (2014) <doi:10.1111/bmsp.12006>.",
    "version": "0.4.0",
    "maintainer": "Niklas Burgard <info@burgard.online>",
    "author": "Niklas Burgard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WAnova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WAnova Welch's Anova from Summary Statistics Provides the functions to perform a Welch's one-way Anova with fixed effects\n  based on summary statistics (sample size, means, standard deviation) and the Games-Howell post hoc test\n  for multiple comparisons and provides the effect size estimator adjusted omega squared.\n  In addition sample size estimation can be computed based on Levy's method, and a Monte Carlo\n  simulation is included to bootstrap residual normality and homoscedasticity\n  Welch, B. L. (1951) <doi:10.1093/biomet/38.3-4.330>\n  Kirk, R. E. (1996) <doi:10.1177/0013164496056005002>\n  Carroll, R. M., & Nordholm, L. A. (1975) <doi:10.1177/001316447503500304>\n  Albers, C., & Lakens, D. (2018) <doi:10.1016/j.jesp.2017.09.004>\n  Games, P. A., & Howell, J. F. (1976) <doi:10.2307/1164979>\n  Levy, K. J. (1978a) <doi:10.1080/00949657808810246>\n  Show-Li, J., & Gwowen, S. (2014) <doi:10.1111/bmsp.12006>.  "
  },
  {
    "id": 8125,
    "package_name": "ZIBR",
    "title": "A Zero-Inflated Beta Random Effect Model",
    "description": "A two-part zero-inflated Beta regression model with random \n    effects (ZIBR) for testing the association between microbial abundance \n    and clinical covariates for longitudinal microbiome data. Eric Z. Chen \n    and Hongzhe Li (2016) <doi:10.1093/bioinformatics/btw308>.",
    "version": "1.0.2",
    "maintainer": "Charlie Bushman <ctbushman@gmail.com>",
    "author": "Eric Zhang Chen [aut, cph],\n  Charlie Bushman [cre]",
    "url": "https://github.com/PennChopMicrobiomeProgram/ZIBR",
    "bug_reports": "https://github.com/PennChopMicrobiomeProgram/ZIBR/issues",
    "repository": "https://cran.r-project.org/package=ZIBR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIBR A Zero-Inflated Beta Random Effect Model A two-part zero-inflated Beta regression model with random \n    effects (ZIBR) for testing the association between microbial abundance \n    and clinical covariates for longitudinal microbiome data. Eric Z. Chen \n    and Hongzhe Li (2016) <doi:10.1093/bioinformatics/btw308>.  "
  },
  {
    "id": 8126,
    "package_name": "ZIDW",
    "title": "Zero-Inflated Discrete Weibull Models",
    "description": "Parameter estimation for zero-inflated discrete Weibull (ZIDW) regression models, the univariate setting, distribution functions, functions to generate randomized quantile residuals a pseudo R2, and plotting of rootograms. For more details, see Kalktawi (2017)  <https://bura.brunel.ac.uk/handle/2438/14476>, Taconeli and Rodrigues de Lara (2022) <doi:10.1080/00949655.2021.2005597>, and Yeh and Young (2025) <doi:10.1080/03610918.2025.2464076>.",
    "version": "0.1.0",
    "maintainer": "Derek S. Young <derek.young@uky.edu>",
    "author": "Peng Yeh [aut],\n  Derek S. Young [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3048-3803>)",
    "url": "https://github.com/dsy109/ZIDW",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ZIDW",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIDW Zero-Inflated Discrete Weibull Models Parameter estimation for zero-inflated discrete Weibull (ZIDW) regression models, the univariate setting, distribution functions, functions to generate randomized quantile residuals a pseudo R2, and plotting of rootograms. For more details, see Kalktawi (2017)  <https://bura.brunel.ac.uk/handle/2438/14476>, Taconeli and Rodrigues de Lara (2022) <doi:10.1080/00949655.2021.2005597>, and Yeh and Young (2025) <doi:10.1080/03610918.2025.2464076>.  "
  },
  {
    "id": 8127,
    "package_name": "ZIHINAR1",
    "title": "Zero-Inflated and Hurdle INAR(1) Models",
    "description": "Provides tools for estimating Zero-Inflated INAR(1) \n    (ZI-INAR(1)) and Hurdle INAR(1) (H-INAR(1)) models using 'Stan'. \n    It allows users to simulate time series data for these models, \n    estimate parameters, and evaluate model fit using various criteria. \n    Functions include model estimation, simulation, and likelihood-based metrics.",
    "version": "0.1.0",
    "maintainer": "Fusheng Yang <fusheng.yang@uconn.edu>",
    "author": "Fusheng Yang [aut, cre],\n  Victor Hugo Lachos Davila [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ZIHINAR1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIHINAR1 Zero-Inflated and Hurdle INAR(1) Models Provides tools for estimating Zero-Inflated INAR(1) \n    (ZI-INAR(1)) and Hurdle INAR(1) (H-INAR(1)) models using 'Stan'. \n    It allows users to simulate time series data for these models, \n    estimate parameters, and evaluate model fit using various criteria. \n    Functions include model estimation, simulation, and likelihood-based metrics.  "
  },
  {
    "id": 8128,
    "package_name": "ZIM",
    "title": "Zero-Inflated Models (ZIM) for Count Time Series with Excess\nZeros",
    "description": "Analyze count time series with excess zeros. \n    Two types of statistical models are supported: Markov regression by Yang et al.\n    (2013) <doi:10.1016/j.stamet.2013.02.001> and state-space models by Yang et al. \n    (2015) <doi:10.1177/1471082X14535530>. They are also known as observation-driven and \n    parameter-driven models respectively in the time series literature. The functions used for \n    Markov regression or observation-driven models can also be used to fit ordinary regression models \n    with independent data under the zero-inflated Poisson (ZIP) or zero-inflated negative binomial (ZINB) \n    assumption. Besides, the package contains some miscellaneous functions to compute density, distribution, \n    quantile, and generate random numbers from ZIP and ZINB distributions.",
    "version": "1.1.0",
    "maintainer": "Ming Yang <mingyang@biostatstudio.com>",
    "author": "Ming Yang [aut, cre],\n  Gideon Zamba [aut],\n  Joseph Cavanaugh [aut]",
    "url": "https://github.com/biostatstudio/ZIM",
    "bug_reports": "https://github.com/biostatstudio/ZIM/issues",
    "repository": "https://cran.r-project.org/package=ZIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIM Zero-Inflated Models (ZIM) for Count Time Series with Excess\nZeros Analyze count time series with excess zeros. \n    Two types of statistical models are supported: Markov regression by Yang et al.\n    (2013) <doi:10.1016/j.stamet.2013.02.001> and state-space models by Yang et al. \n    (2015) <doi:10.1177/1471082X14535530>. They are also known as observation-driven and \n    parameter-driven models respectively in the time series literature. The functions used for \n    Markov regression or observation-driven models can also be used to fit ordinary regression models \n    with independent data under the zero-inflated Poisson (ZIP) or zero-inflated negative binomial (ZINB) \n    assumption. Besides, the package contains some miscellaneous functions to compute density, distribution, \n    quantile, and generate random numbers from ZIP and ZINB distributions.  "
  },
  {
    "id": 8129,
    "package_name": "ZIM4rv",
    "title": "Gene\u2010based Association Tests of Zero\u2010inflated Count Phenotype\nfor Rare Variants",
    "description": "Gene\u2010based association tests to model count data with excessive zeros and rare variants using zero-inflated Poisson/zero-inflated negative Binomial regression framework. This method was originally described by Fan, Sun, and Li in Genetic Epidemiology 46(1):73-86 <doi:10.1002/gepi.22438>.",
    "version": "0.1.1",
    "maintainer": "Xiaomin Liu <e0717571@u.nus.edu>",
    "author": "Xiaomin Liu [aut, cre, cph]",
    "url": "https://github.com/fanx0037/ZIM4rv",
    "bug_reports": "https://github.com/fanx0037/ZIM4rv/issues",
    "repository": "https://cran.r-project.org/package=ZIM4rv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIM4rv Gene\u2010based Association Tests of Zero\u2010inflated Count Phenotype\nfor Rare Variants Gene\u2010based association tests to model count data with excessive zeros and rare variants using zero-inflated Poisson/zero-inflated negative Binomial regression framework. This method was originally described by Fan, Sun, and Li in Genetic Epidemiology 46(1):73-86 <doi:10.1002/gepi.22438>.  "
  },
  {
    "id": 8130,
    "package_name": "ZINAR1",
    "title": "Simulates ZINAR(1) Model and Estimates Its Parameters Under\nFrequentist Approach",
    "description": "Generates Realizations of First-Order Integer Valued Autoregressive Processes with Zero-Inflated Innovations (ZINAR(1)) and Estimates its Parameters as described in Garay et al. (2021) <doi:10.1007/978-3-030-82110-4_2>.",
    "version": "0.1.0",
    "maintainer": "Jo\u00e3o Vitor Ribeiro <joao.vitorribeiro@ufpe.br>",
    "author": "Aldo M. Garay [aut],\n  Jo\u00e3o Vitor Ribeiro [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ZINAR1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZINAR1 Simulates ZINAR(1) Model and Estimates Its Parameters Under\nFrequentist Approach Generates Realizations of First-Order Integer Valued Autoregressive Processes with Zero-Inflated Innovations (ZINAR(1)) and Estimates its Parameters as described in Garay et al. (2021) <doi:10.1007/978-3-030-82110-4_2>.  "
  },
  {
    "id": 8131,
    "package_name": "ZINARp",
    "title": "Simulate INAR/ZINAR(p) Models and Estimate Its Parameters",
    "description": "Simulation, exploratory data analysis and Bayesian analysis of the p-order Integer-valued Autoregressive (INAR(p)) and Zero-inflated p-order Integer-valued Autoregressive (ZINAR(p)) processes, as described in Garay et al. (2020) <doi:10.1080/00949655.2020.1754819>. ",
    "version": "0.1.0",
    "maintainer": "Tharso Augustus Rossiter Ara\u00fajo Monteiro <tharso.augustus@ufpe.br>",
    "author": "Aldo William Medina Garay [aut],\n  Francyelle de Lima Medina [aut],\n  Tharso Augustus Rossiter Ara\u00fajo Monteiro [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ZINARp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZINARp Simulate INAR/ZINAR(p) Models and Estimate Its Parameters Simulation, exploratory data analysis and Bayesian analysis of the p-order Integer-valued Autoregressive (INAR(p)) and Zero-inflated p-order Integer-valued Autoregressive (ZINAR(p)) processes, as described in Garay et al. (2020) <doi:10.1080/00949655.2020.1754819>.   "
  },
  {
    "id": 8132,
    "package_name": "ZIPBayes",
    "title": "Bayesian Methods in the Analysis of Zero-Inflated Poisson Model",
    "description": "Implementation of zero-inflated Poisson models under Bayesian framework using data augmentation as discussed in Chapter 5 of Zhang (2020) <https://hdl.handle.net/10012/16378>. This package is constructed in accommodating four different scenarios: the general scenario, the scenario with measurement error in responses, the external validation scenario, and the internal validation scenario.",
    "version": "1.0.2",
    "maintainer": "Qihuang Zhang <qihuang.zhang@uwaterloo.ca>",
    "author": "Qihuang Zhang [aut, cre, cph],\n  Grace Y. Yi [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ZIPBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIPBayes Bayesian Methods in the Analysis of Zero-Inflated Poisson Model Implementation of zero-inflated Poisson models under Bayesian framework using data augmentation as discussed in Chapter 5 of Zhang (2020) <https://hdl.handle.net/10012/16378>. This package is constructed in accommodating four different scenarios: the general scenario, the scenario with measurement error in responses, the external validation scenario, and the internal validation scenario.  "
  },
  {
    "id": 8133,
    "package_name": "ZIPFA",
    "title": "Zero Inflated Poisson Factor Analysis",
    "description": "Estimation methods for zero-inflated Poisson factor analysis (ZIPFA) on sparse data. \n    It provides estimates of coefficients in a new type of zero-inflated regression. \n    It provides a cross-validation method to determine the potential rank of the data in the ZIPFA \n    and conducts zero-inflated Poisson factor analysis based on the determined rank.",
    "version": "0.8.1",
    "maintainer": "Tianchen Xu <tx2155@columbia.edu>",
    "author": "Tianchen Xu [aut, cre] (<https://orcid.org/0000-0002-0102-7630>),\n  Ryan T. Demmer [aut],\n  Gen Li [aut]",
    "url": "https://zjph602xtc.github.io/ZIPFA/,\nhttps://arxiv.org/abs/1910.11985",
    "bug_reports": "https://github.com/zjph602xtc/ZIPFA/issues",
    "repository": "https://cran.r-project.org/package=ZIPFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIPFA Zero Inflated Poisson Factor Analysis Estimation methods for zero-inflated Poisson factor analysis (ZIPFA) on sparse data. \n    It provides estimates of coefficients in a new type of zero-inflated regression. \n    It provides a cross-validation method to determine the potential rank of the data in the ZIPFA \n    and conducts zero-inflated Poisson factor analysis based on the determined rank.  "
  },
  {
    "id": 8134,
    "package_name": "ZIPG",
    "title": "Zero-Inflated Poisson-Gamma Regression",
    "description": "We provide a flexible Zero-inflated Poisson-Gamma Model (ZIPG) by connecting both the mean abundance and the variability to different covariates, and build valid statistical inference procedures for both parameter estimation and hypothesis testing. These functions can be used to analyze microbiome count data with zero-inflation and overdispersion. The model is discussed in Jiang et al (2023) <doi:10.1080/01621459.2022.2151447>.",
    "version": "1.1",
    "maintainer": "Roulan Jiang <roulan2000@gmail.com>",
    "author": "Roulan Jiang [aut, cre],\n  Tianying Wang [aut]",
    "url": "https://github.com/roulan2000/ZIPG",
    "bug_reports": "https://github.com/roulan2000/ZIPG/issues",
    "repository": "https://cran.r-project.org/package=ZIPG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIPG Zero-Inflated Poisson-Gamma Regression We provide a flexible Zero-inflated Poisson-Gamma Model (ZIPG) by connecting both the mean abundance and the variability to different covariates, and build valid statistical inference procedures for both parameter estimation and hypothesis testing. These functions can be used to analyze microbiome count data with zero-inflation and overdispersion. The model is discussed in Jiang et al (2023) <doi:10.1080/01621459.2022.2151447>.  "
  },
  {
    "id": 8135,
    "package_name": "ZIprop",
    "title": "Permutations Tests and Performance Indicator for Zero-Inflated\nProportions Response",
    "description": "Permutations tests to identify factor correlated to zero-inflated proportions response. Provide a performance indicator based on Spearman correlation to quantify the part of correlation explained by the selected set of factors. See details for the method at the following preprint e.g.: <https://hal.archives-ouvertes.fr/hal-02936779v3>. ",
    "version": "0.1.1",
    "maintainer": "Melina Ribaud <melina.ribaud@gmail.com>",
    "author": "Melina Ribaud",
    "url": "https://gitlab.paca.inrae.fr/meribaud/ziprop",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ZIprop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIprop Permutations Tests and Performance Indicator for Zero-Inflated\nProportions Response Permutations tests to identify factor correlated to zero-inflated proportions response. Provide a performance indicator based on Spearman correlation to quantify the part of correlation explained by the selected set of factors. See details for the method at the following preprint e.g.: <https://hal.archives-ouvertes.fr/hal-02936779v3>.   "
  },
  {
    "id": 8156,
    "package_name": "aamatch",
    "title": "Artless Automatic Multivariate Matching for Observational\nStudies",
    "description": "Implements a simple version of multivariate matching using a propensity score, near-exact matching, near-fine balance, and robust Mahalanobis distance matching (Rosenbaum 2020 <doi:10.1146/annurev-statistics-031219-041058>).  You specify the variables, and the program does everything else.",
    "version": "0.3.7",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aamatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aamatch Artless Automatic Multivariate Matching for Observational\nStudies Implements a simple version of multivariate matching using a propensity score, near-exact matching, near-fine balance, and robust Mahalanobis distance matching (Rosenbaum 2020 <doi:10.1146/annurev-statistics-031219-041058>).  You specify the variables, and the program does everything else.  "
  },
  {
    "id": 8192,
    "package_name": "accelmissing",
    "title": "Missing Value Imputation for Accelerometer Data",
    "description": "We present a statistical method for imputing missing values in accelerometer data. The methodology includes both parametric and semi-parametric multiple imputation under the zero-inflated Poisson lognormal model. It also offers several functions to preprocess accelerometer data before imputation. These include detecting wear and non-wear time, selecting valid days and subjects, and generating plots.",
    "version": "2.2",
    "maintainer": "Jung Ae Lee <jungaeleeb@gmail.com>",
    "author": "Jung Ae Lee [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=accelmissing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "accelmissing Missing Value Imputation for Accelerometer Data We present a statistical method for imputing missing values in accelerometer data. The methodology includes both parametric and semi-parametric multiple imputation under the zero-inflated Poisson lognormal model. It also offers several functions to preprocess accelerometer data before imputation. These include detecting wear and non-wear time, selecting valid days and subjects, and generating plots.  "
  },
  {
    "id": 8209,
    "package_name": "acled.api",
    "title": "Automated Retrieval of ACLED Conflict Event Data",
    "description": "Access and manage the application programming interface (API) of the Armed Conflict Location & Event Data Project (ACLED) at <https://acleddata.com/>. The package makes it easy to retrieve a user-defined sample (or all of the available data) of ACLED, enabling a seamless integration of regular data updates into the research work flow. It requires a minimal number of dependencies. See the package's README file for a note on replicability when drawing on ACLED data. When using this package, you acknowledge that you have read ACLED's terms and conditions of use, and that you agree with their attribution requirements.",
    "version": "1.1.8",
    "maintainer": "Christoph Dworschak <dworschak@posteo.de>",
    "author": "Christoph Dworschak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0196-9545>),\n  Rob Williams [ctb] (ORCID: <https://orcid.org/0000-0001-9259-3883>)",
    "url": "<https://gitlab.com/chris-dworschak/acled.api>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=acled.api",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acled.api Automated Retrieval of ACLED Conflict Event Data Access and manage the application programming interface (API) of the Armed Conflict Location & Event Data Project (ACLED) at <https://acleddata.com/>. The package makes it easy to retrieve a user-defined sample (or all of the available data) of ACLED, enabling a seamless integration of regular data updates into the research work flow. It requires a minimal number of dependencies. See the package's README file for a note on replicability when drawing on ACLED data. When using this package, you acknowledge that you have read ACLED's terms and conditions of use, and that you agree with their attribution requirements.  "
  },
  {
    "id": 8210,
    "package_name": "acledR",
    "title": "Manipulate ACLED Data",
    "description": "Tools working with data from ACLED (Armed Conflict Location and Event Data). Functions include simplified access to ACLED's API (<https://apidocs.acleddata.com/>), methods for keeping local versions of ACLED data up-to-date, and functions for common ACLED data transformations.",
    "version": "1.0.1",
    "maintainer": "Trey Billing <t.billing@acleddata.com>",
    "author": "Armed Conflict Location and Event Data ACLED [cph],\n  Trey Billing [aut, cre],\n  Lucas Fagliano [aut],\n  Katayoun Kishi [ctb]",
    "url": "https://dtacled.github.io/acledR/",
    "bug_reports": "https://github.com/dtacled/acledR/issues",
    "repository": "https://cran.r-project.org/package=acledR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acledR Manipulate ACLED Data Tools working with data from ACLED (Armed Conflict Location and Event Data). Functions include simplified access to ACLED's API (<https://apidocs.acleddata.com/>), methods for keeping local versions of ACLED data up-to-date, and functions for common ACLED data transformations.  "
  },
  {
    "id": 8327,
    "package_name": "adsoRptionMCMC",
    "title": "Bayesian Estimation of Adsorption Isotherms via MCMC",
    "description": "\n  Provides tools for Bayesian parameter estimation of adsorption isotherm models using Markov Chain Monte Carlo (MCMC) methods. \n  This package enables users to fit non-linear and linear adsorption isotherm models\u2014Freundlich, Langmuir, and Temkin\u2014within a \n  probabilistic framework, capturing uncertainty and parameter correlations. It provides posterior summaries, 95% credible intervals, \n  convergence diagnostics (Gelman-Rubin), and visualizations through trace and density plots. With this R package, researchers can \n  rigorously analyze adsorption behavior in environmental and chemical systems using robust Bayesian inference. For more details, \n  see Gilks et al. (1995) <doi:10.1201/b14835>, and Gamerman & Lopes (2006) <doi:10.1201/9781482296426>.",
    "version": "0.1.0",
    "maintainer": "Paul Angelo C. Manlapaz <pacmanlapaz@gmail.com>",
    "author": "Paul Angelo C. Manlapaz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1203-2064>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=adsoRptionMCMC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adsoRptionMCMC Bayesian Estimation of Adsorption Isotherms via MCMC \n  Provides tools for Bayesian parameter estimation of adsorption isotherm models using Markov Chain Monte Carlo (MCMC) methods. \n  This package enables users to fit non-linear and linear adsorption isotherm models\u2014Freundlich, Langmuir, and Temkin\u2014within a \n  probabilistic framework, capturing uncertainty and parameter correlations. It provides posterior summaries, 95% credible intervals, \n  convergence diagnostics (Gelman-Rubin), and visualizations through trace and density plots. With this R package, researchers can \n  rigorously analyze adsorption behavior in environmental and chemical systems using robust Bayesian inference. For more details, \n  see Gilks et al. (1995) <doi:10.1201/b14835>, and Gamerman & Lopes (2006) <doi:10.1201/9781482296426>.  "
  },
  {
    "id": 8390,
    "package_name": "aifeducation",
    "title": "Artificial Intelligence for Education",
    "description": "In social and educational settings, the use of Artificial\n    Intelligence (AI) is a challenging task. Relevant data is often only\n    available in handwritten forms, or the use of data is restricted by\n    privacy policies. This often leads to small data sets. Furthermore, in\n    the educational and social sciences, data is often unbalanced in terms\n    of frequencies. To support educators as well as educational and social\n    researchers in using the potentials of AI for their work, this package\n    provides a unified interface for neural nets in 'PyTorch' to deal with\n    natural language problems. In addition, the package ships with a shiny\n    app, providing a graphical user interface.  This allows the usage of\n    AI for people without skills in writing python/R scripts.  The tools\n    integrate existing mathematical and statistical methods for dealing\n    with small data sets via pseudo-labeling (e.g. Cascante-Bonilla et al.\n    (2020) <doi:10.48550/arXiv.2001.06001>) and imbalanced data via the\n    creation of synthetic cases (e.g.  Islam et al. (2012)\n    <doi:10.1016/j.asoc.2021.108288>).  Performance evaluation of AI is\n    connected to measures from content analysis which educational and\n    social researchers are generally more familiar with (e.g. Berding &\n    Pargmann (2022) <doi:10.30819/5581>, Gwet (2014)\n    <ISBN:978-0-9708062-8-4>, Krippendorff (2019)\n    <doi:10.4135/9781071878781>). Estimation of energy consumption and CO2\n    emissions during model training is done with the 'python' library\n    'codecarbon'.  Finally, all objects created with this package allow to\n    share trained AI models with other people.",
    "version": "1.1.3",
    "maintainer": "Berding Florian <florian.berding@uni-hamburg.de>",
    "author": "Berding Florian [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3593-1695>),\n  Tykhonova Yuliia [aut] (ORCID: <https://orcid.org/0009-0006-9015-1006>),\n  Pargmann Julia [ctb] (ORCID: <https://orcid.org/0000-0003-3616-0172>),\n  Leube Anna [ctb] (ORCID: <https://orcid.org/0009-0001-6949-1608>),\n  Riebenbauer Elisabeth [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8535-3694>),\n  Rebmann Karin [ctb],\n  Slopinski Andreas [ctb]",
    "url": "https://fberding.github.io/aifeducation/",
    "bug_reports": "https://github.com/FBerding/aifeducation/issues",
    "repository": "https://cran.r-project.org/package=aifeducation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aifeducation Artificial Intelligence for Education In social and educational settings, the use of Artificial\n    Intelligence (AI) is a challenging task. Relevant data is often only\n    available in handwritten forms, or the use of data is restricted by\n    privacy policies. This often leads to small data sets. Furthermore, in\n    the educational and social sciences, data is often unbalanced in terms\n    of frequencies. To support educators as well as educational and social\n    researchers in using the potentials of AI for their work, this package\n    provides a unified interface for neural nets in 'PyTorch' to deal with\n    natural language problems. In addition, the package ships with a shiny\n    app, providing a graphical user interface.  This allows the usage of\n    AI for people without skills in writing python/R scripts.  The tools\n    integrate existing mathematical and statistical methods for dealing\n    with small data sets via pseudo-labeling (e.g. Cascante-Bonilla et al.\n    (2020) <doi:10.48550/arXiv.2001.06001>) and imbalanced data via the\n    creation of synthetic cases (e.g.  Islam et al. (2012)\n    <doi:10.1016/j.asoc.2021.108288>).  Performance evaluation of AI is\n    connected to measures from content analysis which educational and\n    social researchers are generally more familiar with (e.g. Berding &\n    Pargmann (2022) <doi:10.30819/5581>, Gwet (2014)\n    <ISBN:978-0-9708062-8-4>, Krippendorff (2019)\n    <doi:10.4135/9781071878781>). Estimation of energy consumption and CO2\n    emissions during model training is done with the 'python' library\n    'codecarbon'.  Finally, all objects created with this package allow to\n    share trained AI models with other people.  "
  },
  {
    "id": 8417,
    "package_name": "alcoholSurv",
    "title": "Light Daily Alcohol and Longevity",
    "description": "Contains data from an observational study concerning possible effects of light daily alcohol consumption on survival and on HDL cholesterol.  It also replicates various simple analyses in Rosenbaum (2025a) <doi:10.1080/09332480.2025.2473291>.  Finally, it includes new R code in wgtRankCef() that implements and replicates a new method for constructing evidence factors in observational block designs. ",
    "version": "0.7.0",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=alcoholSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "alcoholSurv Light Daily Alcohol and Longevity Contains data from an observational study concerning possible effects of light daily alcohol consumption on survival and on HDL cholesterol.  It also replicates various simple analyses in Rosenbaum (2025a) <doi:10.1080/09332480.2025.2473291>.  Finally, it includes new R code in wgtRankCef() that implements and replicates a new method for constructing evidence factors in observational block designs.   "
  },
  {
    "id": 8477,
    "package_name": "amen",
    "title": "Additive and Multiplicative Effects Models for Networks and\nRelational Data",
    "description": "Analysis of dyadic network and relational data using additive and\n    multiplicative effects (AME) models. The basic model includes\n    regression terms, the covariance structure of the social relations model\n    (Warner, Kenny and Stoto (1979) <DOI:10.1037/0022-3514.37.10.1742>, \n    Wong (1982) <DOI:10.2307/2287296>), and multiplicative factor\n    models (Hoff(2009) <DOI:10.1007/s10588-008-9040-4>). \n    Several different link functions accommodate different\n    relational data structures, including binary/network data, normal\n    relational data, zero-inflated positive outcomes using a tobit model, ordinal relational data and data from\n    fixed-rank nomination schemes. Several of these link functions are\n    discussed in Hoff, Fosdick, Volfovsky and Stovel (2013) \n    <DOI:10.1017/nws.2013.17>. Development of this\n    software was supported in part by NIH grant R01HD067509.",
    "version": "1.4.5",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "author": "Peter Hoff [aut, cre],\n  Bailey Fosdick [aut],\n  Alex Volfovsky [aut],\n  Yanjun He [ctb]",
    "url": "https://github.com/pdhoff/amen",
    "bug_reports": "https://github.com/pdhoff/amen/issues",
    "repository": "https://cran.r-project.org/package=amen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amen Additive and Multiplicative Effects Models for Networks and\nRelational Data Analysis of dyadic network and relational data using additive and\n    multiplicative effects (AME) models. The basic model includes\n    regression terms, the covariance structure of the social relations model\n    (Warner, Kenny and Stoto (1979) <DOI:10.1037/0022-3514.37.10.1742>, \n    Wong (1982) <DOI:10.2307/2287296>), and multiplicative factor\n    models (Hoff(2009) <DOI:10.1007/s10588-008-9040-4>). \n    Several different link functions accommodate different\n    relational data structures, including binary/network data, normal\n    relational data, zero-inflated positive outcomes using a tobit model, ordinal relational data and data from\n    fixed-rank nomination schemes. Several of these link functions are\n    discussed in Hoff, Fosdick, Volfovsky and Stovel (2013) \n    <DOI:10.1017/nws.2013.17>. Development of this\n    software was supported in part by NIH grant R01HD067509.  "
  },
  {
    "id": 8482,
    "package_name": "amp",
    "title": "Statistical Test for the Multivariate Point Null Hypotheses",
    "description": "A testing framework for testing the multivariate point null hypothesis. \n    A testing framework described in Elder et al. (2022) <arXiv:2203.01897> to test the multivariate point null hypothesis.  After the user selects a parameter of interest and defines the assumed data generating mechanism, this information should be encoded in functions for the parameter estimator and its corresponding influence curve. Some parameter and data generating mechanism combinations have codings in this package, and are explained in detail in the article.",
    "version": "1.0.0",
    "maintainer": "Adam Elder <shmelder@gmail.com>",
    "author": "Adam Elder [aut, cre] (ORCID: <https://orcid.org/0000-0003-1665-2639>),\n  Marco Carone [ths] (ORCID: <https://orcid.org/0000-0003-2106-0953>),\n  Alex Luedtke [ths] (ORCID: <https://orcid.org/0000-0002-9936-3236>)",
    "url": "",
    "bug_reports": "https://github.com/adam-s-elder/amp/issues",
    "repository": "https://cran.r-project.org/package=amp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amp Statistical Test for the Multivariate Point Null Hypotheses A testing framework for testing the multivariate point null hypothesis. \n    A testing framework described in Elder et al. (2022) <arXiv:2203.01897> to test the multivariate point null hypothesis.  After the user selects a parameter of interest and defines the assumed data generating mechanism, this information should be encoded in functions for the parameter estimator and its corresponding influence curve. Some parameter and data generating mechanism combinations have codings in this package, and are explained in detail in the article.  "
  },
  {
    "id": 8496,
    "package_name": "andorR",
    "title": "Optimisation of the Analysis of AND-OR Decision Trees",
    "description": "A decision support tool to strategically prioritise evidence gathering \n  in complex, hierarchical AND-OR decision trees. It is designed for situations \n  with incomplete or uncertain information where the goal is to reach a confident \n  conclusion as efficiently as possible (responding to the minimum number of \n  questions, and only spending resources on generating improved evidence when \n  it is of significant value to the final decision). The framework excels in \n  complex analyses with multiple potential successful pathways to a conclusion \n  ('OR' nodes). Key features include a dynamic influence index to guide users to \n  the most impactful question, a system for propagating answers and \n  semi-quantitative confidence scores (0-5) up the tree, and post-conclusion \n  guidance to identify the best actions to increase the final confidence. \n  These components are brought together in an interactive command-line workflow \n  that guides the analysis from start to finish.",
    "version": "0.3.1",
    "maintainer": "Angus R Cameron <angus.cameron@epimundi.com>",
    "author": "Angus R Cameron [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8801-0366>),\n  EpiMundi [cph, fnd]",
    "url": "https://epimundi.github.io/andorR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=andorR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "andorR Optimisation of the Analysis of AND-OR Decision Trees A decision support tool to strategically prioritise evidence gathering \n  in complex, hierarchical AND-OR decision trees. It is designed for situations \n  with incomplete or uncertain information where the goal is to reach a confident \n  conclusion as efficiently as possible (responding to the minimum number of \n  questions, and only spending resources on generating improved evidence when \n  it is of significant value to the final decision). The framework excels in \n  complex analyses with multiple potential successful pathways to a conclusion \n  ('OR' nodes). Key features include a dynamic influence index to guide users to \n  the most impactful question, a system for propagating answers and \n  semi-quantitative confidence scores (0-5) up the tree, and post-conclusion \n  guidance to identify the best actions to increase the final confidence. \n  These components are brought together in an interactive command-line workflow \n  that guides the analysis from start to finish.  "
  },
  {
    "id": 8573,
    "package_name": "approxmatch",
    "title": "Approximately Optimal Fine Balance Matching with Multiple Groups",
    "description": "Tools for constructing a matched design with multiple comparison groups.\n Further specifications of refined covariate balance restriction and exact match on \n covariate can be imposed. Matches are approximately optimal in  the sense that the \n cost of the solution is at most twice the optimal cost, Crama and Spieksma (1992) \n <doi:10.1016/0377-2217(92)90078-N>, Karmakar, Small and Rosenbaum (2019)\n <doi:10.1080/10618600.2019.1584900>.",
    "version": "2.0",
    "maintainer": "Bikram Karmakar <bkarmakar@ufl.edu>",
    "author": "Bikram Karmakar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=approxmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "approxmatch Approximately Optimal Fine Balance Matching with Multiple Groups Tools for constructing a matched design with multiple comparison groups.\n Further specifications of refined covariate balance restriction and exact match on \n covariate can be imposed. Matches are approximately optimal in  the sense that the \n cost of the solution is at most twice the optimal cost, Crama and Spieksma (1992) \n <doi:10.1016/0377-2217(92)90078-N>, Karmakar, Small and Rosenbaum (2019)\n <doi:10.1080/10618600.2019.1584900>.  "
  },
  {
    "id": 8621,
    "package_name": "argo",
    "title": "Accurate Estimation of Influenza Epidemics using Google Search\nData",
    "description": "Augmented Regression with General Online data (ARGO) for accurate estimation of influenza epidemics in United States on national level, regional level and state level. It replicates the method introduced in paper Yang, S., Santillana, M. and Kou, S.C. (2015) <doi:10.1073/pnas.1515373112>; Ning, S., Yang, S. and Kou, S.C. (2019) <doi:10.1038/s41598-019-41559-6>; Yang, S., Ning, S. and Kou, S.C. (2021) <doi:10.1038/s41598-021-83084-5>.",
    "version": "3.0.3",
    "maintainer": "Shihao Yang <shihao.yang@isye.gatech.edu>",
    "author": "Shaoyang Ning [aut],\n  Shihao Yang [aut, cre],\n  S. C. Kou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=argo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "argo Accurate Estimation of Influenza Epidemics using Google Search\nData Augmented Regression with General Online data (ARGO) for accurate estimation of influenza epidemics in United States on national level, regional level and state level. It replicates the method introduced in paper Yang, S., Santillana, M. and Kou, S.C. (2015) <doi:10.1073/pnas.1515373112>; Ning, S., Yang, S. and Kou, S.C. (2019) <doi:10.1038/s41598-019-41559-6>; Yang, S., Ning, S. and Kou, S.C. (2021) <doi:10.1038/s41598-021-83084-5>.  "
  },
  {
    "id": 8665,
    "package_name": "asciiruler",
    "title": "Render an ASCII Ruler",
    "description": "An ASCII ruler is for measuring text and is especially useful for sequence analysis. Included in this package are methods to create ASCII rulers and associated GenBank sequence blocks, multi-column text displays that make it easy for viewers to locate nucleotides by position.",
    "version": "0.2",
    "maintainer": "Jeremy Leipzig <leipzig@gmail.com>",
    "author": "Jeremy Leipzig <leipzig@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=asciiruler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asciiruler Render an ASCII Ruler An ASCII ruler is for measuring text and is especially useful for sequence analysis. Included in this package are methods to create ASCII rulers and associated GenBank sequence blocks, multi-column text displays that make it easy for viewers to locate nucleotides by position.  "
  },
  {
    "id": 8694,
    "package_name": "aster",
    "title": "Aster Models",
    "description": "Aster models (Geyer, Wagenius, and Shaw, 2007,\n    <doi:10.1093/biomet/asm030>; Shaw, Geyer, Wagenius, Hangelbroek, and\n    Etterson, 2008, <doi:10.1086/588063>; Geyer, Ridley, Latta, Etterson,\n    and Shaw, 2013, <doi:10.1214/13-AOAS653>) are exponential family\n    regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, life table analysis,\n    zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    There are also random effects versions of these models.",
    "version": "1.3-7",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "author": "Charles J. Geyer [aut, cre]",
    "url": "https://www.stat.umn.edu/geyer/aster/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aster Aster Models Aster models (Geyer, Wagenius, and Shaw, 2007,\n    <doi:10.1093/biomet/asm030>; Shaw, Geyer, Wagenius, Hangelbroek, and\n    Etterson, 2008, <doi:10.1086/588063>; Geyer, Ridley, Latta, Etterson,\n    and Shaw, 2013, <doi:10.1214/13-AOAS653>) are exponential family\n    regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, life table analysis,\n    zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    There are also random effects versions of these models.  "
  },
  {
    "id": 8695,
    "package_name": "aster2",
    "title": "Aster Models",
    "description": "Aster models are exponential family regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    Unlike the aster package, this package does dependence groups (nodes of\n    the graph need not be conditionally independent given their predecessor\n    node), including multinomial and two-parameter normal as families.  Thus\n    this package also generalizes mark-capture-recapture analysis.",
    "version": "0.3-2",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "author": "Charles J. Geyer [aut, cre]",
    "url": "https://www.stat.umn.edu/geyer/aster/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aster2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aster2 Aster Models Aster models are exponential family regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    Unlike the aster package, this package does dependence groups (nodes of\n    the graph need not be conditionally independent given their predecessor\n    node), including multinomial and two-parameter normal as families.  Thus\n    this package also generalizes mark-capture-recapture analysis.  "
  },
  {
    "id": 8707,
    "package_name": "atRisk",
    "title": "At-Risk",
    "description": "The at-Risk (aR) approach is based on a two-step parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the aR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al. (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.",
    "version": "0.2.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Quentin Lajaunie [aut, cre],\n  Guillaume Flament [aut, ctb],\n  Christophe Hurlin [aut],\n  Souzan Kazemi [rev]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=atRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "atRisk At-Risk The at-Risk (aR) approach is based on a two-step parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the aR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al. (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.  "
  },
  {
    "id": 8799,
    "package_name": "axisandallies",
    "title": "Axis and Allies Spring",
    "description": "Simulates battles in the board game Axis and Allies Spring 1942, and calculates your probability of winning a battle. This speeds the game up significantly.",
    "version": "0.1.1",
    "maintainer": "TJ Weaver <weaverthomasjohn@gmail.com>",
    "author": "TJ Weaver [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=axisandallies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "axisandallies Axis and Allies Spring Simulates battles in the board game Axis and Allies Spring 1942, and calculates your probability of winning a battle. This speeds the game up significantly.  "
  },
  {
    "id": 8862,
    "package_name": "barrel",
    "title": "Covariance-Based Ellipses and Annotation Tools for Ordination\nPlots",
    "description": "Provides tools to visualize ordination results in 'R' by adding covariance-based ellipses, centroids, vectors, and confidence regions to plots created with 'ggplot2'. \n    The package extends the 'vegan' framework and supports Principal Component Analysis (PCA), Redundancy Analysis (RDA), and Non-metric Multidimensional Scaling (NMDS). \n    Ellipses can represent either group dispersion (standard deviation, SD) or centroid precision (standard error, SE), following Wang et al. (2015) <doi:10.1371/journal.pone.0118537>. \n    Robust estimators of covariance are implemented, including the Minimum Covariance Determinant (MCD) method of Hubert et al. (2018) <doi:10.1002/wics.1421>. \n    This approach reduces the influence of outliers. \n    barrel is particularly useful for multivariate ecological datasets, promoting reproducible, publication-quality ordination graphics with minimal effort.",
    "version": "0.1.0",
    "maintainer": "Diego Barranco-Elena <diego.barranco@udl.cat>",
    "author": "Diego Barranco-Elena [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=barrel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "barrel Covariance-Based Ellipses and Annotation Tools for Ordination\nPlots Provides tools to visualize ordination results in 'R' by adding covariance-based ellipses, centroids, vectors, and confidence regions to plots created with 'ggplot2'. \n    The package extends the 'vegan' framework and supports Principal Component Analysis (PCA), Redundancy Analysis (RDA), and Non-metric Multidimensional Scaling (NMDS). \n    Ellipses can represent either group dispersion (standard deviation, SD) or centroid precision (standard error, SE), following Wang et al. (2015) <doi:10.1371/journal.pone.0118537>. \n    Robust estimators of covariance are implemented, including the Minimum Covariance Determinant (MCD) method of Hubert et al. (2018) <doi:10.1002/wics.1421>. \n    This approach reduces the influence of outliers. \n    barrel is particularly useful for multivariate ecological datasets, promoting reproducible, publication-quality ordination graphics with minimal effort.  "
  },
  {
    "id": 8867,
    "package_name": "bartMan",
    "title": "Create Visualisations for BART Models",
    "description": "Investigating and visualising Bayesian Additive Regression Tree (BART) (Chipman, H. A., George, E. I., & McCulloch, R. E. 2010) <doi:10.1214/09-AOAS285> model fits.  We construct conventional plots to analyze a model\u2019s performance and stability  as well as create new tree-based plots to analyze variable importance, interaction, and tree structure.  We employ Value Suppressing Uncertainty Palettes (VSUP) to construct heatmaps that display variable importance  and interactions jointly using colour scale to represent posterior uncertainty.  Our visualisations are designed to work with the most popular BART R packages available, namely 'BART' Rodney Sparapani and Charles Spanbauer and Robert McCulloch 2021 <doi:10.18637/jss.v097.i01>,  'dbarts'  (Vincent Dorie 2023) <https://CRAN.R-project.org/package=dbarts>,  and 'bartMachine' (Adam Kapelner and Justin Bleich 2016) <doi:10.18637/jss.v070.i04>.",
    "version": "0.2.1",
    "maintainer": "Alan Inglis <alan.inglis@mu.ie>",
    "author": "Alan Inglis [aut, cre],\n  Andrew Parnell [aut],\n  Catherine Hurley [aut],\n  Claus Wilke [ctb] (Developer of VSUP script)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bartMan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bartMan Create Visualisations for BART Models Investigating and visualising Bayesian Additive Regression Tree (BART) (Chipman, H. A., George, E. I., & McCulloch, R. E. 2010) <doi:10.1214/09-AOAS285> model fits.  We construct conventional plots to analyze a model\u2019s performance and stability  as well as create new tree-based plots to analyze variable importance, interaction, and tree structure.  We employ Value Suppressing Uncertainty Palettes (VSUP) to construct heatmaps that display variable importance  and interactions jointly using colour scale to represent posterior uncertainty.  Our visualisations are designed to work with the most popular BART R packages available, namely 'BART' Rodney Sparapani and Charles Spanbauer and Robert McCulloch 2021 <doi:10.18637/jss.v097.i01>,  'dbarts'  (Vincent Dorie 2023) <https://CRAN.R-project.org/package=dbarts>,  and 'bartMachine' (Adam Kapelner and Justin Bleich 2016) <doi:10.18637/jss.v070.i04>.  "
  },
  {
    "id": 8875,
    "package_name": "baseballr",
    "title": "Acquiring and Analyzing Baseball Data",
    "description": "Provides numerous utilities for acquiring and analyzing\n    baseball data from online sources such as 'Baseball Reference' <https://www.baseball-reference.com/>,\n    'FanGraphs' <https://www.fangraphs.com/>, and the 'MLB Stats' API <https://www.mlb.com/>.",
    "version": "1.6.0",
    "maintainer": "Saiem Gilani <saiem.gilani@gmail.com>",
    "author": "Bill Petti [aut],\n  Saiem Gilani [aut, cre],\n  Ben Baumer [ctb],\n  Ben Dilday [ctb],\n  Robert Frey [ctb],\n  Camden Kay [ctb]",
    "url": "https://billpetti.github.io/baseballr/,\nhttps://github.com/BillPetti/baseballr",
    "bug_reports": "https://github.com/BillPetti/baseballr/issues",
    "repository": "https://cran.r-project.org/package=baseballr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "baseballr Acquiring and Analyzing Baseball Data Provides numerous utilities for acquiring and analyzing\n    baseball data from online sources such as 'Baseball Reference' <https://www.baseball-reference.com/>,\n    'FanGraphs' <https://www.fangraphs.com/>, and the 'MLB Stats' API <https://www.mlb.com/>.  "
  },
  {
    "id": 8900,
    "package_name": "batchtma",
    "title": "Batch Effect Adjustments",
    "description": "Different adjustment methods for batch effects in biomarker data,\n  such as from tissue microarrays. Some methods attempt to retain differences \n  between batches that may be due to between-batch differences in \"biological\"\n  factors that influence biomarker values.",
    "version": "0.1.6",
    "maintainer": "Konrad Stopsack <stopsack@post.harvard.edu>",
    "author": "Konrad Stopsack [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0722-1311>),\n  Travis Gerke [aut] (ORCID: <https://orcid.org/0000-0002-9500-8907>)",
    "url": "https://stopsack.github.io/batchtma/,\nhttps://github.com/stopsack/batchtma",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=batchtma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "batchtma Batch Effect Adjustments Different adjustment methods for batch effects in biomarker data,\n  such as from tissue microarrays. Some methods attempt to retain differences \n  between batches that may be due to between-batch differences in \"biological\"\n  factors that influence biomarker values.  "
  },
  {
    "id": 8921,
    "package_name": "bayesPO",
    "title": "Bayesian Inference for Presence-Only Data",
    "description": "Presence-Only data is best modelled with a Point Process Model.\n    The work of Moreira and Gamerman (2022) <doi:10.1214/21-AOAS1569> provides\n    a way to use exact Bayesian inference to model this type of data, which is\n    implemented in this package.",
    "version": "0.5.0",
    "maintainer": "Guido Alberti Moreira <guidoalber@gmail.com>",
    "author": "Guido Alberti Moreira [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-7557-0874>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bayesPO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayesPO Bayesian Inference for Presence-Only Data Presence-Only data is best modelled with a Point Process Model.\n    The work of Moreira and Gamerman (2022) <doi:10.1214/21-AOAS1569> provides\n    a way to use exact Bayesian inference to model this type of data, which is\n    implemented in this package.  "
  },
  {
    "id": 8931,
    "package_name": "bayesZIB",
    "title": "Bayesian Zero-Inflated Bernoulli Regression Model",
    "description": "Fits a Bayesian zero-inflated Bernoulli regression model handling (potentially) different covariates for the zero-inflated\n    and non zero-inflated parts. See Mori\u00f1a D, Puig P, Navarro A. (2021) <doi:10.1186/s12874-021-01427-2>.",
    "version": "0.0.5",
    "maintainer": "David Mori\u00f1a Soler <dmorina@ub.edu>",
    "author": "David Mori\u00f1a Soler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5949-7443>),\n  Pedro Puig [aut],\n  Albert Navarro [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bayesZIB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayesZIB Bayesian Zero-Inflated Bernoulli Regression Model Fits a Bayesian zero-inflated Bernoulli regression model handling (potentially) different covariates for the zero-inflated\n    and non zero-inflated parts. See Mori\u00f1a D, Puig P, Navarro A. (2021) <doi:10.1186/s12874-021-01427-2>.  "
  },
  {
    "id": 8937,
    "package_name": "bayescount",
    "title": "Power Calculations and Bayesian Analysis of Count Distributions\nand FECRT Data using MCMC",
    "description": "A set of functions to allow analysis of count data (such\n        as faecal egg count data) using Bayesian MCMC methods.  Returns\n        information on the possible values for mean count, coefficient\n        of variation and zero inflation (true prevalence) present in\n        the data.  A complete faecal egg count reduction test (FECRT)\n        model is implemented, which returns inference on the true\n        efficacy of the drug from the pre- and post-treatment data\n        provided, using non-parametric bootstrapping as well as using\n        Bayesian MCMC.  Functions to perform power analyses for faecal\n        egg counts (including FECRT) are also provided.",
    "version": "0.9.99-9",
    "maintainer": "Matthew Denwood <md@sund.ku.dk>",
    "author": "Matthew Denwood [aut, cre]",
    "url": "https://bayescount.sourceforge.net",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bayescount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayescount Power Calculations and Bayesian Analysis of Count Distributions\nand FECRT Data using MCMC A set of functions to allow analysis of count data (such\n        as faecal egg count data) using Bayesian MCMC methods.  Returns\n        information on the possible values for mean count, coefficient\n        of variation and zero inflation (true prevalence) present in\n        the data.  A complete faecal egg count reduction test (FECRT)\n        model is implemented, which returns inference on the true\n        efficacy of the drug from the pre- and post-treatment data\n        provided, using non-parametric bootstrapping as well as using\n        Bayesian MCMC.  Functions to perform power analyses for faecal\n        egg counts (including FECRT) are also provided.  "
  },
  {
    "id": 9017,
    "package_name": "beast",
    "title": "Bayesian Estimation of Change-Points in the Slope of\nMultivariate Time-Series",
    "description": "Assume that a temporal process is composed of contiguous segments with differing slopes and replicated noise-corrupted time series measurements are observed. The unknown mean of the data generating process is modelled as a piecewise linear function of time with an unknown number of change-points. The package infers the joint posterior distribution of the number and position of change-points as well as the unknown mean parameters per time-series by MCMC sampling. A-priori, the proposed model uses an overfitting number of mean parameters but, conditionally on a set of change-points, only a subset of them influences the likelihood. An exponentially decreasing prior distribution on the number of change-points gives rise to a posterior distribution concentrating on sparse representations of the underlying sequence, but also available is the Poisson distribution. See Papastamoulis et al (2017) <arXiv:1709.06111> for a detailed presentation of the method.",
    "version": "1.1",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "author": "Panagiotis Papastamoulis",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=beast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "beast Bayesian Estimation of Change-Points in the Slope of\nMultivariate Time-Series Assume that a temporal process is composed of contiguous segments with differing slopes and replicated noise-corrupted time series measurements are observed. The unknown mean of the data generating process is modelled as a piecewise linear function of time with an unknown number of change-points. The package infers the joint posterior distribution of the number and position of change-points as well as the unknown mean parameters per time-series by MCMC sampling. A-priori, the proposed model uses an overfitting number of mean parameters but, conditionally on a set of change-points, only a subset of them influences the likelihood. An exponentially decreasing prior distribution on the number of change-points gives rise to a posterior distribution concentrating on sparse representations of the underlying sequence, but also available is the Poisson distribution. See Papastamoulis et al (2017) <arXiv:1709.06111> for a detailed presentation of the method.  "
  },
  {
    "id": 9031,
    "package_name": "bellreg",
    "title": "Count Regression Models Based on the Bell Distribution",
    "description": "Bell regression models for count data with overdispersion. The implemented models account for ordinary and zero-inflated regression models under both frequentist and Bayesian approaches. Theoretical details regarding the models implemented in the package can be found in Castellares et al. (2018) <doi:10.1016/j.apm.2017.12.014> and Lemonte et al. (2020) <doi:10.1080/02664763.2019.1636940>.",
    "version": "0.0.2.2",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "author": "Fabio Demarqui [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9236-1986>),\n  Marcos Prates [ctb] (ORCID: <https://orcid.org/0000-0001-8077-4898>),\n  Fredy Caceres [ctb],\n  Andrew Johnson [ctb]",
    "url": "https://github.com/fndemarqui/bellreg,\nhttps://fndemarqui.github.io/bellreg/",
    "bug_reports": "https://github.com/fndemarqui/bellreg/issues",
    "repository": "https://cran.r-project.org/package=bellreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bellreg Count Regression Models Based on the Bell Distribution Bell regression models for count data with overdispersion. The implemented models account for ordinary and zero-inflated regression models under both frequentist and Bayesian approaches. Theoretical details regarding the models implemented in the package can be found in Castellares et al. (2018) <doi:10.1016/j.apm.2017.12.014> and Lemonte et al. (2020) <doi:10.1080/02664763.2019.1636940>.  "
  },
  {
    "id": 9094,
    "package_name": "bibs",
    "title": "Bayesian Inference for the Birnbaum-Saunders Distribution",
    "description": "Developed for the following tasks. 1- Simulating and computing the maximum likelihood \n              estimator for the Birnbaum-Saunders (BS) distribution, 2- Computing the Bayesian estimator for\n              the parameters of the BS distribution based on reference prior proposed by Xu and Tang (2010)\n              <doi:10.1016/j.csda.2009.08.004> and conjugate prior. 3- Computing the Bayesian estimator for\n              the BS distribution based on conjugate prior. 4- Computing the Bayesian estimator for the BS\n              distribution based on Jeffrey prior given by Achcar (1993) <doi:10.1016/0167-9473(93)90170-X>\n              5- Computing the Bayesian estimator for the BS distribution under progressive type-II censoring\n              scheme.",
    "version": "1.1.1",
    "maintainer": "Mahdi Teimouri <teimouri@aut.ac.ir>",
    "author": "Mahdi Teimouri",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bibs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bibs Bayesian Inference for the Birnbaum-Saunders Distribution Developed for the following tasks. 1- Simulating and computing the maximum likelihood \n              estimator for the Birnbaum-Saunders (BS) distribution, 2- Computing the Bayesian estimator for\n              the parameters of the BS distribution based on reference prior proposed by Xu and Tang (2010)\n              <doi:10.1016/j.csda.2009.08.004> and conjugate prior. 3- Computing the Bayesian estimator for\n              the BS distribution based on conjugate prior. 4- Computing the Bayesian estimator for the BS\n              distribution based on Jeffrey prior given by Achcar (1993) <doi:10.1016/0167-9473(93)90170-X>\n              5- Computing the Bayesian estimator for the BS distribution under progressive type-II censoring\n              scheme.  "
  },
  {
    "id": 9105,
    "package_name": "bigPCAcpp",
    "title": "Principal Component Analysis for 'bigmemory' Matrices",
    "description": "High performance principal component analysis routines\n       that operate directly on 'bigmemory::big.matrix' objects. The\n       package avoids materialising large matrices in memory by\n       streaming data through 'BLAS' and 'LAPACK' kernels and provides\n       helpers to derive scores, loadings, correlations, and\n       contribution diagnostics, including utilities that stream\n       results into 'bigmemory'-backed matrices for file-based\n       workflows. Additional interfaces expose 'scalable' singular value\n       decomposition, robust PCA, and robust SVD algorithms so that\n       users can explore large matrices while tempering the influence\n       of outliers. 'Scalable' principal component analysis is also implemented,\n       Elgamal, Yabandeh, Aboulnaga, Mustafa, and Hefeeda (2015) \n       <doi:10.1145/2723372.2751520>.",
    "version": "0.9.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Frederic Bertrand [aut, cre]",
    "url": "https://fbertran.github.io/bigPCAcpp/,\nhttps://github.com/fbertran/bigPCAcpp/",
    "bug_reports": "https://github.com/fbertran/bigPCAcpp/issues/",
    "repository": "https://cran.r-project.org/package=bigPCAcpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigPCAcpp Principal Component Analysis for 'bigmemory' Matrices High performance principal component analysis routines\n       that operate directly on 'bigmemory::big.matrix' objects. The\n       package avoids materialising large matrices in memory by\n       streaming data through 'BLAS' and 'LAPACK' kernels and provides\n       helpers to derive scores, loadings, correlations, and\n       contribution diagnostics, including utilities that stream\n       results into 'bigmemory'-backed matrices for file-based\n       workflows. Additional interfaces expose 'scalable' singular value\n       decomposition, robust PCA, and robust SVD algorithms so that\n       users can explore large matrices while tempering the influence\n       of outliers. 'Scalable' principal component analysis is also implemented,\n       Elgamal, Yabandeh, Aboulnaga, Mustafa, and Hefeeda (2015) \n       <doi:10.1145/2723372.2751520>.  "
  },
  {
    "id": 9120,
    "package_name": "bigmatch",
    "title": "Making Optimal Matching Size-Scalable Using Optimal Calipers",
    "description": "Implements optimal matching with near-fine balance in large observational studies with the use of optimal calipers to get a sparse network. The caliper is optimal in the sense that it is as small as possible such that a matching exists. The main functions in the 'bigmatch' package are optcal() to find the optimal caliper, optconstant() to find the optimal number of nearest neighbors, and nfmatch() to find a near-fine balance match with a caliper and a restriction on the number of nearest neighbors. \n    Yu, R., Silber, J. H., and Rosenbaum, P. R. (2020). <DOI:10.1214/19-sts699>. ",
    "version": "0.6.4",
    "maintainer": "Ruoqi Yu <ruoqiyu125@gmail.com>",
    "author": "Ruoqi Yu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bigmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigmatch Making Optimal Matching Size-Scalable Using Optimal Calipers Implements optimal matching with near-fine balance in large observational studies with the use of optimal calipers to get a sparse network. The caliper is optimal in the sense that it is as small as possible such that a matching exists. The main functions in the 'bigmatch' package are optcal() to find the optimal caliper, optconstant() to find the optimal number of nearest neighbors, and nfmatch() to find a near-fine balance match with a caliper and a restriction on the number of nearest neighbors. \n    Yu, R., Silber, J. H., and Rosenbaum, P. R. (2020). <DOI:10.1214/19-sts699>.   "
  },
  {
    "id": 9121,
    "package_name": "bigmds",
    "title": "Multidimensional Scaling for Big Data",
    "description": "MDS is a statistic tool for reduction of dimensionality, using as input a distance\n    matrix of dimensions n \u00d7 n. When n is large, classical algorithms suffer from\n    computational problems and MDS configuration can not be obtained.\n    With this package, we address these problems by means of six algorithms, being two of them \n    original proposals:\n        - Landmark MDS proposed by De Silva V. and JB. Tenenbaum (2004).\n        - Interpolation MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Reduced MDS proposed by Paradis E (2018).\n        - Pivot MDS proposed by Brandes U. and C. Pich (2007)\n        - Divide-and-conquer MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Fast MDS, proposed by Yang, T., J. Liu, L. McMillan and W. Wang (2006).",
    "version": "3.0.0",
    "maintainer": "Cristian Pach\u00f3n Garc\u00eda <cc.pachon@gmail.com>",
    "author": "Cristian Pach\u00f3n Garc\u00eda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9518-4874>),\n  Pedro Delicado [aut] (ORCID: <https://orcid.org/0000-0003-3933-4852>)",
    "url": "https://github.com/pachoning/bigmds",
    "bug_reports": "https://github.com/pachoning/bigmds/issues",
    "repository": "https://cran.r-project.org/package=bigmds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigmds Multidimensional Scaling for Big Data MDS is a statistic tool for reduction of dimensionality, using as input a distance\n    matrix of dimensions n \u00d7 n. When n is large, classical algorithms suffer from\n    computational problems and MDS configuration can not be obtained.\n    With this package, we address these problems by means of six algorithms, being two of them \n    original proposals:\n        - Landmark MDS proposed by De Silva V. and JB. Tenenbaum (2004).\n        - Interpolation MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Reduced MDS proposed by Paradis E (2018).\n        - Pivot MDS proposed by Brandes U. and C. Pich (2007)\n        - Divide-and-conquer MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Fast MDS, proposed by Yang, T., J. Liu, L. McMillan and W. Wang (2006).  "
  },
  {
    "id": 9141,
    "package_name": "bild",
    "title": "A Package for BInary Longitudinal Data",
    "description": "Performs logistic regression for binary longitudinal\n  data, allowing for serial dependence among observations from a given\n  individual and a random intercept term. Estimation is via maximization\n  of the exact likelihood of a suitably defined model. Missing values and \n  unbalanced data are allowed, with some restrictions. \n  M. Helena Goncalves et al.(2007) <DOI: 10.18637/jss.v046.i09>.",
    "version": "1.2-1",
    "maintainer": "M. Helena Goncalves <mhgoncal@ualg.pt>",
    "author": "M. Helena Goncalves, M. Salome Cabral and Adelchi Azzalini, \n  apart from a set of Fortran-77 subroutines written by R. Piessens\n  and E. de Doncker, belonging to the suite \"Quadpack\".",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bild",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bild A Package for BInary Longitudinal Data Performs logistic regression for binary longitudinal\n  data, allowing for serial dependence among observations from a given\n  individual and a random intercept term. Estimation is via maximization\n  of the exact likelihood of a suitably defined model. Missing values and \n  unbalanced data are allowed, with some restrictions. \n  M. Helena Goncalves et al.(2007) <DOI: 10.18637/jss.v046.i09>.  "
  },
  {
    "id": 9232,
    "package_name": "bivpois",
    "title": "Bivariate Poisson Distribution",
    "description": "Maximum likelihood estimation, random values generation, density computation and other functions for the bivariate Poisson distribution. References include: Kawamura K. (1984). \"Direct calculation of maximum likelihood estimator for the bivariate Poisson distribution\". Kodai Mathematical Journal, 7(2): 211--221. <doi:10.2996/kmj/1138036908>. Kocherlakota S. and Kocherlakota K. (1992). \"Bivariate discrete distributions\". CRC Press. <doi:10.1201/9781315138480>. Karlis D. and Ntzoufras I. (2003). \"Analysis of sports data by using bivariate Poisson models\". Journal of the Royal Statistical Society: Series D (The Statistician), 52(3): 381--393. <doi:10.1111/1467-9884.00366>.",
    "version": "1.1",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bivpois",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bivpois Bivariate Poisson Distribution Maximum likelihood estimation, random values generation, density computation and other functions for the bivariate Poisson distribution. References include: Kawamura K. (1984). \"Direct calculation of maximum likelihood estimator for the bivariate Poisson distribution\". Kodai Mathematical Journal, 7(2): 211--221. <doi:10.2996/kmj/1138036908>. Kocherlakota S. and Kocherlakota K. (1992). \"Bivariate discrete distributions\". CRC Press. <doi:10.1201/9781315138480>. Karlis D. and Ntzoufras I. (2003). \"Analysis of sports data by using bivariate Poisson models\". Journal of the Royal Statistical Society: Series D (The Statistician), 52(3): 381--393. <doi:10.1111/1467-9884.00366>.  "
  },
  {
    "id": 9237,
    "package_name": "bizicount",
    "title": "Bivariate Zero-Inflated Count Models Using Copulas",
    "description": "Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Ne\u0161lehov\u00e1 (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.",
    "version": "1.3.4",
    "maintainer": "John Niehaus <jniehaus2257@gmail.com>",
    "author": "John Niehaus [aut, cre]",
    "url": "https://github.com/jmniehaus/bizicount",
    "bug_reports": "https://github.com/jmniehaus/bizicount/issues",
    "repository": "https://cran.r-project.org/package=bizicount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bizicount Bivariate Zero-Inflated Count Models Using Copulas Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Ne\u0161lehov\u00e1 (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.  "
  },
  {
    "id": 9321,
    "package_name": "bonn",
    "title": "Access INKAR Database",
    "description": "Retrieve and import data from the INKAR database (Indikatoren und Karten zur Raum- und Stadtentwicklung Datenbank, <https://www.inkar.de>) of the Federal Office for Building and Regional Planning (BBSR) in Bonn using their JSON API. ",
    "version": "1.0.3",
    "maintainer": "Moritz Marbach <m.marbach@ucl.ac.uk>",
    "author": "Moritz Marbach [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7101-2821>)",
    "url": "https://github.com/sumtxt/bonn/",
    "bug_reports": "https://github.com/sumtxt/bonn/issues",
    "repository": "https://cran.r-project.org/package=bonn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bonn Access INKAR Database Retrieve and import data from the INKAR database (Indikatoren und Karten zur Raum- und Stadtentwicklung Datenbank, <https://www.inkar.de>) of the Federal Office for Building and Regional Planning (BBSR) in Bonn using their JSON API.   "
  },
  {
    "id": 9351,
    "package_name": "bootwar",
    "title": "Nonparametric Bootstrap Test with Pooled Resampling Card Game",
    "description": "The card game War is simple in its rules but can be lengthy. In\n    another domain, the nonparametric bootstrap test with pooled resampling\n    (nbpr) methods, as outlined in Dwivedi, Mallawaarachchi, and Alvarado (2017) <doi:10.1002/sim.7263>,\n    is optimal for comparing paired or unpaired means in non-normal data,\n    especially for small sample size studies. However, many researchers are\n    unfamiliar with these methods. The 'bootwar' package bridges this gap by\n    enabling users to grasp the concepts of nbpr via Boot War, a variation of the\n    card game War designed for small samples. The package provides functions like\n    score_keeper() and play_round() to streamline gameplay and scoring. Once a\n    predetermined number of rounds concludes, users can employ the analyze_game()\n    function to derive game results. This function leverages the 'npboottprm'\n    package's nonparboot() to report nbpr results and, for comparative analysis,\n    also reports results from the 'stats' package's t.test() function. Additionally,\n    'bootwar' features an interactive 'shiny' web application, bootwar(). This\n    offers a user-centric interface to experience Boot War, enhancing understanding\n    of nbpr methods across various distributions, sample sizes, number of bootstrap\n    resamples, and confidence intervals.",
    "version": "0.2.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/bootwar",
    "bug_reports": "https://github.com/mightymetrika/bootwar/issues",
    "repository": "https://cran.r-project.org/package=bootwar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootwar Nonparametric Bootstrap Test with Pooled Resampling Card Game The card game War is simple in its rules but can be lengthy. In\n    another domain, the nonparametric bootstrap test with pooled resampling\n    (nbpr) methods, as outlined in Dwivedi, Mallawaarachchi, and Alvarado (2017) <doi:10.1002/sim.7263>,\n    is optimal for comparing paired or unpaired means in non-normal data,\n    especially for small sample size studies. However, many researchers are\n    unfamiliar with these methods. The 'bootwar' package bridges this gap by\n    enabling users to grasp the concepts of nbpr via Boot War, a variation of the\n    card game War designed for small samples. The package provides functions like\n    score_keeper() and play_round() to streamline gameplay and scoring. Once a\n    predetermined number of rounds concludes, users can employ the analyze_game()\n    function to derive game results. This function leverages the 'npboottprm'\n    package's nonparboot() to report nbpr results and, for comparative analysis,\n    also reports results from the 'stats' package's t.test() function. Additionally,\n    'bootwar' features an interactive 'shiny' web application, bootwar(). This\n    offers a user-centric interface to experience Boot War, enhancing understanding\n    of nbpr methods across various distributions, sample sizes, number of bootstrap\n    resamples, and confidence intervals.  "
  },
  {
    "id": 9360,
    "package_name": "boutliers",
    "title": "Outlier Detection and Influence Diagnostics for Meta-Analysis",
    "description": "Computational tools for outlier detection and influence diagnostics in meta-analysis (Noma et al. (2025) <doi:10.1101/2025.09.18.25336125>). Bootstrap distributions of influence statistics are computed, and explicit thresholds for identifying outliers are provided. These methods can also be applied to the analysis of influential centers or regions in multicenter or multiregional clinical trials (Aoki, Noma and Gosho (2021) <doi:10.1080/24709360.2021.1921944>, Nakamura and Noma (2021) <doi:10.5691/jjb.41.117>).",
    "version": "2.1-3",
    "maintainer": "Hisashi Noma <noma@ism.ac.jp>",
    "author": "Hisashi Noma [aut, cre],\n  Kazushi Maruo [aut],\n  Masahiko Gosho [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=boutliers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "boutliers Outlier Detection and Influence Diagnostics for Meta-Analysis Computational tools for outlier detection and influence diagnostics in meta-analysis (Noma et al. (2025) <doi:10.1101/2025.09.18.25336125>). Bootstrap distributions of influence statistics are computed, and explicit thresholds for identifying outliers are provided. These methods can also be applied to the analysis of influential centers or regions in multicenter or multiregional clinical trials (Aoki, Noma and Gosho (2021) <doi:10.1080/24709360.2021.1921944>, Nakamura and Noma (2021) <doi:10.5691/jjb.41.117>).  "
  },
  {
    "id": 9449,
    "package_name": "bsgof",
    "title": "Birnbaum-Saunders Goodness-of-Fit Test",
    "description": "Performs goodness of fit test for the Birnbaum-Saunders distribution and provides the maximum likelihood estimate and the method-of-moments estimate. For more details, see Park and Wang (2013) <arXiv:2308.10150>. This work was supported by the National Research Foundation of Korea (NRF) grants funded by the Korea government (MSIT) (No. 2022R1A2C1091319, RS-2023-00242528).",
    "version": "0.23.8",
    "maintainer": "Chanseok Park <statpnu@gmail.com>",
    "author": "Chanseok Park [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2208-3498>),\n  Min Wang [ctb] (ORCID: <https://orcid.org/0000-0002-9233-7844>)",
    "url": "https://AppliedStat.GitHub.io/R/",
    "bug_reports": "https://GitHub.com/AppliedStat/R/issues",
    "repository": "https://cran.r-project.org/package=bsgof",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsgof Birnbaum-Saunders Goodness-of-Fit Test Performs goodness of fit test for the Birnbaum-Saunders distribution and provides the maximum likelihood estimate and the method-of-moments estimate. For more details, see Park and Wang (2013) <arXiv:2308.10150>. This work was supported by the National Research Foundation of Korea (NRF) grants funded by the Korea government (MSIT) (No. 2022R1A2C1091319, RS-2023-00242528).  "
  },
  {
    "id": 9486,
    "package_name": "bullwhipgame",
    "title": "Bullwhip Effect Demo in Shiny",
    "description": "The bullwhipgame is an educational game that has as purpose the illustration and exploration of the bullwhip effect,i.e, the increase in demand variability along the supply chain. Marchena Marlene (2010) <arXiv:1009.3977>.  ",
    "version": "0.1.0",
    "maintainer": "Marlene Marchena <marchenamarlene@gmail.com>",
    "author": "Marlene Marchena",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bullwhipgame",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bullwhipgame Bullwhip Effect Demo in Shiny The bullwhipgame is an educational game that has as purpose the illustration and exploration of the bullwhip effect,i.e, the increase in demand variability along the supply chain. Marchena Marlene (2010) <arXiv:1009.3977>.    "
  },
  {
    "id": 9492,
    "package_name": "bundesligR",
    "title": "All Final Tables of the Bundesliga",
    "description": "All final tables of Germany's highest football (soccer!) league, the Bundesliga. Contains data from 1964 to 2016.",
    "version": "0.1.0",
    "maintainer": "Philipp Ottolinger <philipp@ottolinger.de>",
    "author": "Philipp Ottolinger",
    "url": "https://github.com/ottlngr/bundesligR",
    "bug_reports": "https://github.com/ottlngr/bundesligR/issues",
    "repository": "https://cran.r-project.org/package=bundesligR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bundesligR All Final Tables of the Bundesliga All final tables of Germany's highest football (soccer!) league, the Bundesliga. Contains data from 1964 to 2016.  "
  },
  {
    "id": 9515,
    "package_name": "bzinb",
    "title": "Bivariate Zero-Inflated Negative Binomial Model Estimator",
    "description": "Provides a maximum likelihood estimation of Bivariate Zero-Inflated Negative Binomial (BZINB) model or the nested model parameters. Also estimates the underlying correlation of the a pair of count data. See Cho, H., Liu, C., Preisser, J., and Wu, D. (In preparation) for details.",
    "version": "1.0.8",
    "maintainer": "Hunyong Cho <hunyong.cho@gmail.com>",
    "author": "Hunyong Cho, Chuwen Liu, Jinyoung Park, Di Wu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bzinb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bzinb Bivariate Zero-Inflated Negative Binomial Model Estimator Provides a maximum likelihood estimation of Bivariate Zero-Inflated Negative Binomial (BZINB) model or the nested model parameters. Also estimates the underlying correlation of the a pair of count data. See Cho, H., Liu, C., Preisser, J., and Wu, D. (In preparation) for details.  "
  },
  {
    "id": 9643,
    "package_name": "catSurv",
    "title": "Computerized Adaptive Testing for Survey Research",
    "description": "Provides methods of computerized adaptive testing for survey researchers.  See Montgomery and Rossiter (2020) <doi:10.1093/jssam/smz027>. Includes functionality for data fit with the classic item response methods including the latent trait model, the Birnbaum three parameter model, the graded response, and the generalized partial credit model.  Additionally, includes several ability parameter estimation and item selection routines.  During item selection, all calculations are done in compiled C++ code.",
    "version": "1.6.0",
    "maintainer": "Erin Rossiter <erossite@nd.edu>",
    "author": "Jacob Montgomery [aut],\n  Erin Rossiter [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/erossiter/catSurv/issues",
    "repository": "https://cran.r-project.org/package=catSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catSurv Computerized Adaptive Testing for Survey Research Provides methods of computerized adaptive testing for survey researchers.  See Montgomery and Rossiter (2020) <doi:10.1093/jssam/smz027>. Includes functionality for data fit with the classic item response methods including the latent trait model, the Birnbaum three parameter model, the graded response, and the generalized partial credit model.  Additionally, includes several ability parameter estimation and item selection routines.  During item selection, all calculations are done in compiled C++ code.  "
  },
  {
    "id": 9652,
    "package_name": "catfun",
    "title": "Categorical Data Analysis",
    "description": "Includes wrapper functions around existing functions for the analysis of categorical data and introduces functions for calculating risk differences and matched odds ratios. R currently supports a wide variety of tools for the analysis of categorical data. However, many functions are spread across a variety of packages with differing syntax and poor compatibility with each another. prop_test() combines the functions binom.test(), prop.test() and BinomCI() into one output. prop_power() allows for power and sample size calculations for both balanced and unbalanced designs. riskdiff() is used for calculating risk differences and matched_or() is used for calculating matched odds ratios. For further information on methods used that are not documented in other packages see Nathan Mantel and William Haenszel (1959) <doi:10.1093/jnci/22.4.719> and Alan Agresti (2002) <ISBN:0-471-36093-7>. ",
    "version": "0.1.4",
    "maintainer": "Nick Williams <ntwilliams.personal@gmail.com>",
    "author": "Nick Williams",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catfun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catfun Categorical Data Analysis Includes wrapper functions around existing functions for the analysis of categorical data and introduces functions for calculating risk differences and matched odds ratios. R currently supports a wide variety of tools for the analysis of categorical data. However, many functions are spread across a variety of packages with differing syntax and poor compatibility with each another. prop_test() combines the functions binom.test(), prop.test() and BinomCI() into one output. prop_power() allows for power and sample size calculations for both balanced and unbalanced designs. riskdiff() is used for calculating risk differences and matched_or() is used for calculating matched odds ratios. For further information on methods used that are not documented in other packages see Nathan Mantel and William Haenszel (1959) <doi:10.1093/jnci/22.4.719> and Alan Agresti (2002) <ISBN:0-471-36093-7>.   "
  },
  {
    "id": 9731,
    "package_name": "cdid",
    "title": "The Chained Difference-in-Differences",
    "description": "Extends the 'did' package to improve efficiency and handling of unbalanced panel data. Bellego, Benatia, and Dortet-Bernadet (2024), \"The Chained Difference-in-Differences\", Journal of Econometrics, <doi:10.1016/j.jeconom.2024.105783>.",
    "version": "0.1.1",
    "maintainer": "David Benatia <david.benatia@hec.ca>",
    "author": "David Benatia [cre, aut],\n  Christophe Bell\u00e9go [aut],\n  Joel Cuerrier [aut],\n  Vincent Dortet-Bernadet [aut]",
    "url": "https://github.com/joelcuerrier/cdid",
    "bug_reports": "https://github.com/joelcuerrier/cdid/issues",
    "repository": "https://cran.r-project.org/package=cdid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdid The Chained Difference-in-Differences Extends the 'did' package to improve efficiency and handling of unbalanced panel data. Bellego, Benatia, and Dortet-Bernadet (2024), \"The Chained Difference-in-Differences\", Journal of Econometrics, <doi:10.1016/j.jeconom.2024.105783>.  "
  },
  {
    "id": 9740,
    "package_name": "ceblR",
    "title": "Extract Data from the Canadian Elite Basketball League",
    "description": "Gather boxscore and play-by-play data from the Canadian Elite Basketball League (CEBL) <https://www.cebl.ca> to create a repository of basic and advanced statistics for teams and players.",
    "version": "1.0.0",
    "maintainer": "David Awosoga <odo.awosoga@gmail.com>",
    "author": "David Awosoga [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2741-5770>),\n  Matthew Chow [aut]",
    "url": "https://github.com/awosoga/ceblR, https://awosoga.github.io/ceblR/",
    "bug_reports": "https://github.com/awosoga/ceblR/issues",
    "repository": "https://cran.r-project.org/package=ceblR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ceblR Extract Data from the Canadian Elite Basketball League Gather boxscore and play-by-play data from the Canadian Elite Basketball League (CEBL) <https://www.cebl.ca> to create a repository of basic and advanced statistics for teams and players.  "
  },
  {
    "id": 9779,
    "package_name": "cfbfastR",
    "title": "Access College Football Play by Play Data",
    "description": "A utility to quickly obtain clean and tidy college football\n    data. Serves as a wrapper around the\n    <https://collegefootballdata.com/> API and provides functions to\n    access live play by play and box score data from ESPN\n    <https://www.espn.com> when available. It provides users the\n    capability to access a plethora of endpoints, and supplement that data\n    with additional information (Expected Points Added/Win Probability\n    added).",
    "version": "2.0.0",
    "maintainer": "Saiem Gilani <saiem.gilani@gmail.com>",
    "author": "Saiem Gilani [cre, aut],\n  Akshay Easwaran [aut],\n  Jared Lee [aut],\n  Eric Hess [aut],\n  Michael Egle [ctb],\n  Nate Manzo [ctb],\n  Jason DeLoach [ctb],\n  Tej Seth [ctb],\n  Conor McQuiston [ctb],\n  Tan Ho [ctb],\n  Keegan Abdoo [ctb],\n  Matt Spencer [ctb],\n  Sebastian Carl [ctb],\n  John Edwards [ctb],\n  Brad Hill [ctb]",
    "url": "https://cfbfastR.sportsdataverse.org/,\nhttps://github.com/sportsdataverse/cfbfastR",
    "bug_reports": "https://github.com/sportsdataverse/cfbfastR/issues",
    "repository": "https://cran.r-project.org/package=cfbfastR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cfbfastR Access College Football Play by Play Data A utility to quickly obtain clean and tidy college football\n    data. Serves as a wrapper around the\n    <https://collegefootballdata.com/> API and provides functions to\n    access live play by play and box score data from ESPN\n    <https://www.espn.com> when available. It provides users the\n    capability to access a plethora of endpoints, and supplement that data\n    with additional information (Expected Points Added/Win Probability\n    added).  "
  },
  {
    "id": 9804,
    "package_name": "changepoint.influence",
    "title": "Package to Calculate the Influence of the Data on a Changepoint\nSegmentation",
    "description": "Allows users to input their data, segmentation and function used for the segmentation (and additional arguments) and the package calculates the influence of the data on the changepoint locations, see Wilms et al. (2022) <doi:10.1080/10618600.2021.2000873>.  Currently this can only be used with the changepoint package functions to identify changes, but we plan to extend this.  There are options for different types of graphics to assess the influence.",
    "version": "1.0.2",
    "maintainer": "Rebecca Killick <r.killick@lancs.ac.uk>",
    "author": "Rebecca Killick [aut, cre],\n  Ines Wilms [aut]",
    "url": "https://github.com/rkillick/changepoint.influence/",
    "bug_reports": "https://github.com/rkillick/changepoint.influence/issues",
    "repository": "https://cran.r-project.org/package=changepoint.influence",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "changepoint.influence Package to Calculate the Influence of the Data on a Changepoint\nSegmentation Allows users to input their data, segmentation and function used for the segmentation (and additional arguments) and the package calculates the influence of the data on the changepoint locations, see Wilms et al. (2022) <doi:10.1080/10618600.2021.2000873>.  Currently this can only be used with the changepoint package functions to identify changes, but we plan to extend this.  There are options for different types of graphics to assess the influence.  "
  },
  {
    "id": 9847,
    "package_name": "chess",
    "title": "Read, Write, Create and Explore Chess Games",
    "description": "This is an opinionated wrapper around the\n    python-chess package. It allows users to read and write PGN files as\n    well as create and explore game trees such as the ones seen in chess\n    books.",
    "version": "1.0.1",
    "maintainer": "C. Lente <clente@curso-r.com>",
    "author": "C. Lente [aut, cre]",
    "url": "https://github.com/curso-r/chess",
    "bug_reports": "https://github.com/curso-r/chess/issues",
    "repository": "https://cran.r-project.org/package=chess",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chess Read, Write, Create and Explore Chess Games This is an opinionated wrapper around the\n    python-chess package. It allows users to read and write PGN files as\n    well as create and explore game trees such as the ones seen in chess\n    books.  "
  },
  {
    "id": 9848,
    "package_name": "chess2plyrs",
    "title": "Chess Game Creation and Tools",
    "description": "A chess program which allows the user to create a game, add moves,\n             check for legal moves and game result, plot the board, take back, read and write FEN (Forsyth\u2013Edwards Notation).\n             A basic chess engine based on minimax is implemented.",
    "version": "0.3.0",
    "maintainer": "Luigi Annicchiarico <luigi.annic@gmail.com>",
    "author": "Luigi Annicchiarico [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chess2plyrs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chess2plyrs Chess Game Creation and Tools A chess program which allows the user to create a game, add moves,\n             check for legal moves and game result, plot the board, take back, read and write FEN (Forsyth\u2013Edwards Notation).\n             A basic chess engine based on minimax is implemented.  "
  },
  {
    "id": 10119,
    "package_name": "cocron",
    "title": "Statistical Comparisons of Two or more Alpha Coefficients",
    "description": "Statistical tests for the comparison between two or more alpha\n    coefficients based on either dependent or independent groups of individuals.\n    A web interface is available at http://comparingcronbachalphas.org. A plugin\n    for the R GUI and IDE RKWard is included. Please install RKWard from https://\n    rkward.kde.org to use this feature. The respective R package 'rkward' cannot be\n    installed directly from a repository, as it is a part of RKWard.",
    "version": "1.0-1",
    "maintainer": "Birk Diedenhofen <mail@birkdiedenhofen.de>",
    "author": "Birk Diedenhofen [aut, cre]",
    "url": "http://comparingcronbachalphas.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cocron",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cocron Statistical Comparisons of Two or more Alpha Coefficients Statistical tests for the comparison between two or more alpha\n    coefficients based on either dependent or independent groups of individuals.\n    A web interface is available at http://comparingcronbachalphas.org. A plugin\n    for the R GUI and IDE RKWard is included. Please install RKWard from https://\n    rkward.kde.org to use this feature. The respective R package 'rkward' cannot be\n    installed directly from a repository, as it is a part of RKWard.  "
  },
  {
    "id": 10128,
    "package_name": "codebreaker",
    "title": "Retro Logic Game",
    "description": "Logic game in the style of the early 1980s home computers \n    that can be played in the R console. This game is inspired by \n    Mastermind, a game that became popular in the 1970s.\n    Can you break the code?",
    "version": "1.0.1",
    "maintainer": "Roland Krasser <roland.krasser@gmail.com>",
    "author": "Roland Krasser [aut, cre]",
    "url": "https://github.com/rolkra/codebreaker",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=codebreaker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "codebreaker Retro Logic Game Logic game in the style of the early 1980s home computers \n    that can be played in the R console. This game is inspired by \n    Mastermind, a game that became popular in the 1970s.\n    Can you break the code?  "
  },
  {
    "id": 10143,
    "package_name": "coefficientalpha",
    "title": "Robust Coefficient Alpha and Omega with Missing and Non-Normal\nData",
    "description": "Cronbach's alpha and McDonald's omega are widely used reliability or internal consistency measures in social, behavioral and education sciences. Alpha is reported in nearly every study that involves measuring a construct through multiple test items. The package 'coefficientalpha' calculates coefficient alpha and coefficient omega with missing data and non-normal data. Robust standard errors and confidence intervals are also provided. A test is also available to test the tau-equivalent and homogeneous assumptions. Since Version 0.5, the bootstrap confidence intervals were added.",
    "version": "0.7.2",
    "maintainer": "Zhiyong Zhang <johnnyzhz@gmail.com>",
    "author": "Zhiyong Zhang and Ke-Hai Yuan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=coefficientalpha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "coefficientalpha Robust Coefficient Alpha and Omega with Missing and Non-Normal\nData Cronbach's alpha and McDonald's omega are widely used reliability or internal consistency measures in social, behavioral and education sciences. Alpha is reported in nearly every study that involves measuring a construct through multiple test items. The package 'coefficientalpha' calculates coefficient alpha and coefficient omega with missing data and non-normal data. Robust standard errors and confidence intervals are also provided. A test is also available to test the tau-equivalent and homogeneous assumptions. Since Version 0.5, the bootstrap confidence intervals were added.  "
  },
  {
    "id": 10164,
    "package_name": "cold",
    "title": "Count Longitudinal Data",
    "description": "Performs regression analysis for longitudinal count data,  \n   allowing for serial dependence among observations from a given \n   individual and two dimensional random effects on the linear predictor. \n   Estimation is via maximization of the exact likelihood of a suitably \n   defined model. Missing values and unbalanced data are allowed. \n   Details can be found in the accompanying scientific papers: \n   Goncalves & Cabral (2021, Journal of Statistical Software, \n   <doi:10.18637/jss.v099.i03>) and Goncalves et al. \n   (2007, Computational Statistics & Data Analysis, \n   <doi:10.1016/j.csda.2007.03.002>).",
    "version": "2.0-3",
    "maintainer": "M. Helena Goncalves <mhgoncal@ualg.pt>",
    "author": "M. Helena Goncalves and M. Salome Cabral,\n  apart from a set of Fortran-77 subroutines written by R. Piessens\n  and E. de Doncker, belonging to the suite \"Quadpack\".",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cold",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cold Count Longitudinal Data Performs regression analysis for longitudinal count data,  \n   allowing for serial dependence among observations from a given \n   individual and two dimensional random effects on the linear predictor. \n   Estimation is via maximization of the exact likelihood of a suitably \n   defined model. Missing values and unbalanced data are allowed. \n   Details can be found in the accompanying scientific papers: \n   Goncalves & Cabral (2021, Journal of Statistical Software, \n   <doi:10.18637/jss.v099.i03>) and Goncalves et al. \n   (2007, Computational Statistics & Data Analysis, \n   <doi:10.1016/j.csda.2007.03.002>).  "
  },
  {
    "id": 10174,
    "package_name": "collin",
    "title": "Visualization the Effects of Collinearity in Distributed Lag\nModels and Other Linear Models",
    "description": "Tool to assessing whether the results of a study could be influenced by\n    collinearity. Simulations under a given hypothesized truth regarding effects of an\n    exposure on the outcome are used and the resulting curves of lagged effects are\n    visualized. A user's manual is provided, which includes detailed examples (e.g. a\n    cohort study looking for windows of vulnerability to air pollution, a time series\n    study examining the linear association of air pollution with hospital admissions,\n    and a time series study examining the non-linear association between temperature and\n    mortality). The methods are described in Basagana and Barrera-Gomez (2021) <doi:10.1093/ije/dyab179>.",
    "version": "0.0.4",
    "maintainer": "Jose Barrera-Gomez <jose.barrera@isglobal.org>",
    "author": "Jose Barrera-Gomez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2688-6036>),\n  Xavier Basagana [aut] (ORCID: <https://orcid.org/0000-0002-8457-1489>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=collin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "collin Visualization the Effects of Collinearity in Distributed Lag\nModels and Other Linear Models Tool to assessing whether the results of a study could be influenced by\n    collinearity. Simulations under a given hypothesized truth regarding effects of an\n    exposure on the outcome are used and the resulting curves of lagged effects are\n    visualized. A user's manual is provided, which includes detailed examples (e.g. a\n    cohort study looking for windows of vulnerability to air pollution, a time series\n    study examining the linear association of air pollution with hospital admissions,\n    and a time series study examining the non-linear association between temperature and\n    mortality). The methods are described in Basagana and Barrera-Gomez (2021) <doi:10.1093/ije/dyab179>.  "
  },
  {
    "id": 10200,
    "package_name": "colorr",
    "title": "Color Palettes for EPL, MLB, NBA, NHL, and NFL Teams",
    "description": "Color palettes for EPL, MLB, NBA, NHL, and NFL teams.",
    "version": "1.0.0",
    "maintainer": "Charles Crabtree <ccrabtr@umich.edu>",
    "author": "Charles Crabtree [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=colorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colorr Color Palettes for EPL, MLB, NBA, NHL, and NFL Teams Color palettes for EPL, MLB, NBA, NHL, and NFL teams.  "
  },
  {
    "id": 10249,
    "package_name": "comperes",
    "title": "Manage Competition Results",
    "description": "Tools for storing and managing competition results.\n    Competition is understood as a set of games in which players gain some\n    abstract scores.  There are two ways for storing results: in long (one\n    row per game-player) and wide (one row per game with fixed amount of\n    players) formats. This package provides functions for creation and\n    conversion between them. Also there are functions for computing their\n    summary and Head-to-Head values for players. They leverage grammar of\n    data manipulation from 'dplyr'.",
    "version": "0.2.7",
    "maintainer": "Evgeni Chasnovski <evgeni.chasnovski@gmail.com>",
    "author": "Evgeni Chasnovski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1617-4019>)",
    "url": "https://github.com/echasnovski/comperes",
    "bug_reports": "https://github.com/echasnovski/comperes/issues",
    "repository": "https://cran.r-project.org/package=comperes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "comperes Manage Competition Results Tools for storing and managing competition results.\n    Competition is understood as a set of games in which players gain some\n    abstract scores.  There are two ways for storing results: in long (one\n    row per game-player) and wide (one row per game with fixed amount of\n    players) formats. This package provides functions for creation and\n    conversion between them. Also there are functions for computing their\n    summary and Head-to-Head values for players. They leverage grammar of\n    data manipulation from 'dplyr'.  "
  },
  {
    "id": 10252,
    "package_name": "compindexR",
    "title": "Calculates Composite Index",
    "description": "It uses the first-order sensitivity index to measure whether the weights assigned by the creator of the composite indicator match the actual importance of the variables. Moreover, the variance inflation factor is used to reduce the set of correlated variables. In the case of a discrepancy between the importance and the assigned weight, the script determines weights that allow adjustment of the weights to the intended impact of variables. If the optimised weights are unable to reflect the desired importance, the highly correlated variables are reduced, taking into account variance inflation factor. The final outcome of the script is the calculated value of the composite indicator based on optimal weights and a reduced set of variables, and the linear ordering of the analysed objects.",
    "version": "0.1.3",
    "maintainer": "Olgun Aydin <olgun.aydin@pg.edu.pl>",
    "author": "Olgun Aydin [cre] (ORCID: <https://orcid.org/0000-0002-7090-0931>),\n  Marta Kuc-Czarnecka [aut] (ORCID:\n    <https://orcid.org/0000-0003-2970-9980>),\n  Michal Bernard Pietrzak [aut] (ORCID:\n    <https://orcid.org/0000-0002-9263-4478>)",
    "url": "https://github.com/olgnaydn/compindexR",
    "bug_reports": "https://github.com/olgnaydn/compindexR/issues",
    "repository": "https://cran.r-project.org/package=compindexR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "compindexR Calculates Composite Index It uses the first-order sensitivity index to measure whether the weights assigned by the creator of the composite indicator match the actual importance of the variables. Moreover, the variance inflation factor is used to reduce the set of correlated variables. In the case of a discrepancy between the importance and the assigned weight, the script determines weights that allow adjustment of the weights to the intended impact of variables. If the optimised weights are unable to reflect the desired importance, the highly correlated variables are reduced, taking into account variance inflation factor. The final outcome of the script is the calculated value of the composite indicator based on optimal weights and a reduced set of variables, and the linear ordering of the analysed objects.  "
  },
  {
    "id": 10259,
    "package_name": "compositeReliabilityInNestedDesigns",
    "title": "Optimizing the Composite Reliability in Multivariate Nested\nDesigns",
    "description": "The reliability of assessment tools is a crucial aspect of monitoring student performance in various educational settings. It ensures that the assessment outcomes accurately reflect a student's true level of performance. However, when assessments are combined, determining composite reliability can be challenging, especially for naturalistic and unbalanced datasets in nested design as is often the case for Workplace-Based Assessments. This package is designed to estimate composite reliability in nested designs using multivariate generalizability theory and enhance the analysis of assessment data. The package allows for the inclusion of weight per assessment type and produces extensive G- and D-study results with graphical interpretations, and options to find the set of weights that maximizes the composite reliability or minimizes the standard error of measurement (SEM).",
    "version": "1.0.4",
    "maintainer": "Joyce Moonen - van Loon <j.moonen@maastrichtuniversity.nl>",
    "author": "Joyce Moonen - van Loon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8883-8822>)",
    "url": "https://github.com/jmoonen/compositeReliabilityInNestedDesigns",
    "bug_reports": "https://github.com/jmoonen/compositeReliabilityInNestedDesigns/issues",
    "repository": "https://cran.r-project.org/package=compositeReliabilityInNestedDesigns",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "compositeReliabilityInNestedDesigns Optimizing the Composite Reliability in Multivariate Nested\nDesigns The reliability of assessment tools is a crucial aspect of monitoring student performance in various educational settings. It ensures that the assessment outcomes accurately reflect a student's true level of performance. However, when assessments are combined, determining composite reliability can be challenging, especially for naturalistic and unbalanced datasets in nested design as is often the case for Workplace-Based Assessments. This package is designed to estimate composite reliability in nested designs using multivariate generalizability theory and enhance the analysis of assessment data. The package allows for the inclusion of weight per assessment type and produces extensive G- and D-study results with graphical interpretations, and options to find the set of weights that maximizes the composite reliability or minimizes the standard error of measurement (SEM).  "
  },
  {
    "id": 10268,
    "package_name": "conMItion",
    "title": "Conditional Mutual Information Estimation for Multi-Omics Data",
    "description": "The biases introduced in association measures, particularly mutual information, \n    are influenced by factors such as tumor purity, mutation burden, and hypermethylation. \n    This package provides the estimation of conditional mutual information (CMI) and its \n    statistical significance with a focus on its application to multi-omics data. Utilizing \n    B-spline functions (inspired by Daub et al. (2004) <doi:10.1186/1471-2105-5-118>), the package offers tools to estimate the association between heterogeneous multi-\n    omics data, while removing the effects of confounding factors. This helps to unravel complex\n    biological interactions. In addition, it includes methods to evaluate the statistical significance \n    of these associations, providing a robust framework for multi-omics data integration and \n    analysis. This package is ideal for researchers in computational biology, bioinformatics, \n    and systems biology seeking a comprehensive tool for understanding interdependencies in \n    omics data.",
    "version": "0.2.1",
    "maintainer": "Gaojianyong Wang <gjywang@gmail.com>",
    "author": "Gaojianyong Wang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=conMItion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conMItion Conditional Mutual Information Estimation for Multi-Omics Data The biases introduced in association measures, particularly mutual information, \n    are influenced by factors such as tumor purity, mutation burden, and hypermethylation. \n    This package provides the estimation of conditional mutual information (CMI) and its \n    statistical significance with a focus on its application to multi-omics data. Utilizing \n    B-spline functions (inspired by Daub et al. (2004) <doi:10.1186/1471-2105-5-118>), the package offers tools to estimate the association between heterogeneous multi-\n    omics data, while removing the effects of confounding factors. This helps to unravel complex\n    biological interactions. In addition, it includes methods to evaluate the statistical significance \n    of these associations, providing a robust framework for multi-omics data integration and \n    analysis. This package is ideal for researchers in computational biology, bioinformatics, \n    and systems biology seeking a comprehensive tool for understanding interdependencies in \n    omics data.  "
  },
  {
    "id": 10286,
    "package_name": "conditionalProbNspades",
    "title": "Conditional Probabilities of Distributions Across Hearts Hands",
    "description": "Provides some tabulated data to be be referred to in a discussion in a vignette accompanying my upcoming R package 'playWholeHandDriverPassParams'. In addition to that specific purpose, these may also provide data and illustrate some computational approaches that are relevant to card games like hearts or bridge.This package refers to authentic data from Gregory Stoll <https://gregstoll.com/~gregstoll/bridge/math.html>, and details of performing the probability calculations from Jeremy L. Martin <https://jlmartin.ku.edu/~jlmartin/bridge/basics.pdf>.",
    "version": "1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=conditionalProbNspades",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conditionalProbNspades Conditional Probabilities of Distributions Across Hearts Hands Provides some tabulated data to be be referred to in a discussion in a vignette accompanying my upcoming R package 'playWholeHandDriverPassParams'. In addition to that specific purpose, these may also provide data and illustrate some computational approaches that are relevant to card games like hearts or bridge.This package refers to authentic data from Gregory Stoll <https://gregstoll.com/~gregstoll/bridge/math.html>, and details of performing the probability calculations from Jeremy L. Martin <https://jlmartin.ku.edu/~jlmartin/bridge/basics.pdf>.  "
  },
  {
    "id": 10447,
    "package_name": "countDM",
    "title": "Estimation of Count Data Models",
    "description": "The maximum likelihood estimation (MLE) of the count data models along with standard error of the estimates and Akaike information model section criterion are provided. The functions allow to compute the MLE for the following distributions such as the Bell distribution, the Borel distribution, the Poisson distribution, zero inflated Bell distribution, zero inflated Bell Touchard distribution, zero inflated Poisson distribution, zero one inflated Bell distribution and zero one inflated Poisson distribution. Moreover, the probability mass function (PMF), distribution function (CDF), quantile function (QF) and random numbers generation of the Bell Touchard and zero inflated Bell Touchard distribution are also provided. ",
    "version": "0.1.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  M.H. Tahir [aut],\n  Saima Shakoor [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=countDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countDM Estimation of Count Data Models The maximum likelihood estimation (MLE) of the count data models along with standard error of the estimates and Akaike information model section criterion are provided. The functions allow to compute the MLE for the following distributions such as the Bell distribution, the Borel distribution, the Poisson distribution, zero inflated Bell distribution, zero inflated Bell Touchard distribution, zero inflated Poisson distribution, zero one inflated Bell distribution and zero one inflated Poisson distribution. Moreover, the probability mass function (PMF), distribution function (CDF), quantile function (QF) and random numbers generation of the Bell Touchard and zero inflated Bell Touchard distribution are also provided.   "
  },
  {
    "id": 10449,
    "package_name": "countSTAR",
    "title": "Flexible Modeling of Count Data",
    "description": "For Bayesian and classical inference and prediction with count-valued data,\n    Simultaneous Transformation and Rounding (STAR) Models provide a flexible, interpretable,\n    and easy-to-use approach. STAR models the observed count data using a rounded \n    continuous data model and incorporates a transformation for greater flexibility.\n    Implicitly, STAR formalizes the commonly-applied yet incoherent procedure of \n    (i) transforming count-valued data and subsequently \n    (ii) modeling the transformed data using Gaussian models. \n    STAR is well-defined for count-valued data, which is reflected in predictive accuracy, \n    and is designed to account for zero-inflation, bounded or censored data, and over- or underdispersion. \n    Importantly, STAR is easy to combine with existing MCMC or point estimation\n    methods for continuous data, which allows seamless adaptation of continuous data\n    models (such as linear regressions, additive models, BART, random forests,\n    and gradient boosting machines) for count-valued data. The package also includes several\n    methods for modeling count time series data, namely via warped Dynamic Linear Models. \n    For more details and background on these methodologies, see the works of \n    Kowal and Canale (2020) <doi:10.1214/20-EJS1707>, \n    Kowal and Wu (2022) <doi:10.1111/biom.13617>, \n    King and Kowal (2022) <arXiv:2110.14790>, and \n    Kowal and Wu (2023) <arXiv:2110.12316>.",
    "version": "1.0.2",
    "maintainer": "Brian King <brianking387@gmail.com>",
    "author": "Brian King [aut, cre],\n  Dan Kowal [aut]",
    "url": "https://bking124.github.io/countSTAR/\nhttps://github.com/bking124/countSTAR",
    "bug_reports": "https://github.com/bking124/countSTAR/issues",
    "repository": "https://cran.r-project.org/package=countSTAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countSTAR Flexible Modeling of Count Data For Bayesian and classical inference and prediction with count-valued data,\n    Simultaneous Transformation and Rounding (STAR) Models provide a flexible, interpretable,\n    and easy-to-use approach. STAR models the observed count data using a rounded \n    continuous data model and incorporates a transformation for greater flexibility.\n    Implicitly, STAR formalizes the commonly-applied yet incoherent procedure of \n    (i) transforming count-valued data and subsequently \n    (ii) modeling the transformed data using Gaussian models. \n    STAR is well-defined for count-valued data, which is reflected in predictive accuracy, \n    and is designed to account for zero-inflation, bounded or censored data, and over- or underdispersion. \n    Importantly, STAR is easy to combine with existing MCMC or point estimation\n    methods for continuous data, which allows seamless adaptation of continuous data\n    models (such as linear regressions, additive models, BART, random forests,\n    and gradient boosting machines) for count-valued data. The package also includes several\n    methods for modeling count time series data, namely via warped Dynamic Linear Models. \n    For more details and background on these methodologies, see the works of \n    Kowal and Canale (2020) <doi:10.1214/20-EJS1707>, \n    Kowal and Wu (2022) <doi:10.1111/biom.13617>, \n    King and Kowal (2022) <arXiv:2110.14790>, and \n    Kowal and Wu (2023) <arXiv:2110.12316>.  "
  },
  {
    "id": 10460,
    "package_name": "countts",
    "title": "Thomson Sampling for Zero-Inflated Count Outcomes",
    "description": "A specialized tool is designed for assessing contextual bandit algorithms, particularly those aimed at handling overdispersed and zero-inflated count data. It offers a simulated testing environment that includes various models like Poisson, Overdispersed Poisson, Zero-inflated Poisson, and Zero-inflated Overdispersed Poisson. The package is capable of executing five specific algorithms: Linear Thompson sampling with log transformation on the outcome, Thompson sampling Poisson, Thompson sampling Negative Binomial, Thompson sampling Zero-inflated Poisson, and Thompson sampling Zero-inflated Negative Binomial. Additionally, it can generate regret plots to evaluate the performance of contextual bandit algorithms. This package is based on the algorithms by Liu et al. (2023) <arXiv:2311.14359>.",
    "version": "0.1.0",
    "maintainer": "Tanujit Chakraborty <tanujitisi@gmail.com>",
    "author": "Xueqing Liu [aut],\n  Nina Deliu [aut],\n  Tanujit Chakraborty [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3479-2187>),\n  Lauren Bell [aut],\n  Bibhas Chakraborty [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=countts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countts Thomson Sampling for Zero-Inflated Count Outcomes A specialized tool is designed for assessing contextual bandit algorithms, particularly those aimed at handling overdispersed and zero-inflated count data. It offers a simulated testing environment that includes various models like Poisson, Overdispersed Poisson, Zero-inflated Poisson, and Zero-inflated Overdispersed Poisson. The package is capable of executing five specific algorithms: Linear Thompson sampling with log transformation on the outcome, Thompson sampling Poisson, Thompson sampling Negative Binomial, Thompson sampling Zero-inflated Poisson, and Thompson sampling Zero-inflated Negative Binomial. Additionally, it can generate regret plots to evaluate the performance of contextual bandit algorithms. This package is based on the algorithms by Liu et al. (2023) <arXiv:2311.14359>.  "
  },
  {
    "id": 10487,
    "package_name": "covidmx",
    "title": "Descarga y analiza datos de COVID-19 en M\u00e9xico",
    "description": "Herramientas para el an\u00e1lisis de datos de COVID-19 en M\u00e9xico. Descarga y analiza \n  los datos para COVID-19 de la Direccion General de Epidemiolog\u00eda de M\u00e9xico (DGE) \n  <https://www.gob.mx/salud/documentos/datos-abiertos-152127>,\n  la Red de Infecciones Respiratorias Agudas Graves (Red IRAG)\n  <https://www.gits.igg.unam.mx/red-irag-dashboard/reviewHome> y la Iniciativa Global \n  para compartir todos los datos de influenza (GISAID)\n  <https://gisaid.org/>. \n  English: Downloads and analyzes data  of COVID-19 from the  Mexican General \n  Directorate of Epidemiology (DGE), the Network of \n  Severe Acute Respiratory  Infections (IRAG network),and the Global \n  Initiative on Sharing All Influenza Data GISAID.",
    "version": "0.7.7",
    "maintainer": "Rodrigo Zepeda-Tello <rzepeda17@gmail.com>",
    "author": "Rodrigo Zepeda-Tello [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4471-5270>),\n  Mauricio Hernandez-Avila [aut],\n  Alberto Almui\u00f1a [ctb] (Author of included zzz fragment),\n  Jonah Gabry [ctb] (Author of included cmdstanr fragment),\n  Rok \u010ce\u0161novar [ctb] (Author of included cmdstanr fragment),\n  Ben Bales [ctb] (Author of included cmdstanr fragment),\n  Mitzi Morris [ctb] (Author of included cmdstanr fragment),\n  Mikhail Popov [ctb] (Author of included cmdstanr fragment),\n  Mike Lawrence [ctb] (Author of included cmdstanr fragment),\n  William Michael Landau [ctb] (Author of included cmdstanr fragment),\n  Jacob Socolar [ctb] (Author of included cmdstanr fragment),\n  Andrew Johnson [ctb] (Author of included cmdstanr fragment),\n  Instituto Mexicano del Seguro Social [cph, fnd]",
    "url": "https://github.com/RodrigoZepeda/covidmx,\nhttps://rodrigozepeda.github.io/covidmx/",
    "bug_reports": "https://github.com/RodrigoZepeda/covidmx/issues",
    "repository": "https://cran.r-project.org/package=covidmx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covidmx Descarga y analiza datos de COVID-19 en M\u00e9xico Herramientas para el an\u00e1lisis de datos de COVID-19 en M\u00e9xico. Descarga y analiza \n  los datos para COVID-19 de la Direccion General de Epidemiolog\u00eda de M\u00e9xico (DGE) \n  <https://www.gob.mx/salud/documentos/datos-abiertos-152127>,\n  la Red de Infecciones Respiratorias Agudas Graves (Red IRAG)\n  <https://www.gits.igg.unam.mx/red-irag-dashboard/reviewHome> y la Iniciativa Global \n  para compartir todos los datos de influenza (GISAID)\n  <https://gisaid.org/>. \n  English: Downloads and analyzes data  of COVID-19 from the  Mexican General \n  Directorate of Epidemiology (DGE), the Network of \n  Severe Acute Respiratory  Infections (IRAG network),and the Global \n  Initiative on Sharing All Influenza Data GISAID.  "
  },
  {
    "id": 10557,
    "package_name": "creepyalien",
    "title": "Puzzle Game for the R Console",
    "description": "Puzzle game that can be played in the R console.\n  Help the alien to find the ship.",
    "version": "1.0.0",
    "maintainer": "Roland Krasser <roland.krasser@gmail.com>",
    "author": "Roland Krasser [aut, cre],\n  Peter Prevos [aut]",
    "url": "https://github.com/rolkra/creepyalien",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=creepyalien",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "creepyalien Puzzle Game for the R Console Puzzle game that can be played in the R console.\n  Help the alien to find the ship.  "
  },
  {
    "id": 10563,
    "package_name": "crimCV",
    "title": "Group-Based Modelling of Longitudinal Data",
    "description": "A finite mixture of Zero-Inflated Poisson (ZIP) models for analyzing criminal trajectories.",
    "version": "1.0.0",
    "maintainer": "Jason D. Nielsen <jdn@math.carleton.ca>",
    "author": "Jason D. Nielsen <drjdnielsen@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crimCV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crimCV Group-Based Modelling of Longitudinal Data A finite mixture of Zero-Inflated Poisson (ZIP) models for analyzing criminal trajectories.  "
  },
  {
    "id": 10593,
    "package_name": "crossmatch",
    "title": "The Cross-Match Test",
    "description": "Performs the cross-match test that is an exact, \n\t\tdistribution free test of equality of 2 high dimensional multivariate distributions.   \n\t\tThe input is a distance matrix and the labels of\n        the two groups to be compared, the output is the number of\n        cross-matches and a p-value. See Rosenbaum (2005) <doi:10.1111/j.1467-9868.2005.00513.x>.",
    "version": "1.4-0",
    "maintainer": "Marieke Stolte <stolte@statistik.tu-dortmund.de>",
    "author": "Ruth Heller [aut, cph],\n  Dylan Small [aut, cph],\n  Paul Rosenbaum [aut, cph],\n  Marieke Stolte [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crossmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crossmatch The Cross-Match Test Performs the cross-match test that is an exact, \n\t\tdistribution free test of equality of 2 high dimensional multivariate distributions.   \n\t\tThe input is a distance matrix and the labels of\n        the two groups to be compared, the output is the number of\n        cross-matches and a p-value. See Rosenbaum (2005) <doi:10.1111/j.1467-9868.2005.00513.x>.  "
  },
  {
    "id": 10617,
    "package_name": "crwbmetareg",
    "title": "Cluster Robust Wild Bootstrap Meta Regression",
    "description": "In meta regression sometimes the studies have multiple effects that are correlated. For this reason cluster robust standard errors must be computed. However, since the clusters are unbalanced the wild bootstrap is suggested. See Oczkowski E. and Doucouliagos H. (2015). \"Wine prices and quality ratings: a meta-regression analysis\". American Journal of Agricultural Economics, 97(1): 103--121. <doi:10.1093/ajae/aau057> and Cameron A. C., Gelbach J. B. and Miller D. L. (2008). \"Bootstrap-based improvements for inference with clustered errors\". The Review of Economics and Statistics, 90(3): 414--427. <doi:10.1162/rest.90.3.414>.",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crwbmetareg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crwbmetareg Cluster Robust Wild Bootstrap Meta Regression In meta regression sometimes the studies have multiple effects that are correlated. For this reason cluster robust standard errors must be computed. However, since the clusters are unbalanced the wild bootstrap is suggested. See Oczkowski E. and Doucouliagos H. (2015). \"Wine prices and quality ratings: a meta-regression analysis\". American Journal of Agricultural Economics, 97(1): 103--121. <doi:10.1093/ajae/aau057> and Cameron A. C., Gelbach J. B. and Miller D. L. (2008). \"Bootstrap-based improvements for inference with clustered errors\". The Review of Economics and Statistics, 90(3): 414--427. <doi:10.1162/rest.90.3.414>.  "
  },
  {
    "id": 10702,
    "package_name": "cursr",
    "title": "Cursor and Terminal Manipulation",
    "description": "A toolbox for developing applications, games, simulations, or\n    agent-based models in the R terminal. Included functions allow users to move the \n    cursor around the terminal screen, change text colors and attributes, clear the\n    screen, hide and show the cursor, map key presses to functions, draw shapes and curves,\n    among others. Most functionalities require users to be in a terminal (not the R GUI).",
    "version": "0.1.0",
    "maintainer": "Chris Mann <cmann3@unl.edu>",
    "author": "Chris Mann",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cursr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cursr Cursor and Terminal Manipulation A toolbox for developing applications, games, simulations, or\n    agent-based models in the R terminal. Included functions allow users to move the \n    cursor around the terminal screen, change text colors and attributes, clear the\n    screen, hide and show the cursor, map key presses to functions, draw shapes and curves,\n    among others. Most functionalities require users to be in a terminal (not the R GUI).  "
  },
  {
    "id": 10719,
    "package_name": "cvAUC",
    "title": "Cross-Validated Area Under the ROC Curve Confidence Intervals",
    "description": "Tools for working with and evaluating cross-validated area under the ROC curve (AUC) estimators.  The primary functions of the package are ci.cvAUC and ci.pooled.cvAUC, which report cross-validated AUC and compute confidence intervals for cross-validated AUC estimates based on influence curves for i.i.d. and pooled repeated measures data, respectively.  One benefit to using influence curve based confidence intervals is that they require much less computation time than bootstrapping methods.  The utility functions, AUC and cvAUC, are simple wrappers for functions from the ROCR package.",
    "version": "1.1.4",
    "maintainer": "Erin LeDell <oss@ledell.org>",
    "author": "Erin LeDell, Maya Petersen, Mark van der Laan",
    "url": "https://github.com/ledell/cvAUC",
    "bug_reports": "https://github.com/ledell/cvAUC/issues",
    "repository": "https://cran.r-project.org/package=cvAUC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvAUC Cross-Validated Area Under the ROC Curve Confidence Intervals Tools for working with and evaluating cross-validated area under the ROC curve (AUC) estimators.  The primary functions of the package are ci.cvAUC and ci.pooled.cvAUC, which report cross-validated AUC and compute confidence intervals for cross-validated AUC estimates based on influence curves for i.i.d. and pooled repeated measures data, respectively.  One benefit to using influence curve based confidence intervals is that they require much less computation time than bootstrapping methods.  The utility functions, AUC and cvAUC, are simple wrappers for functions from the ROCR package.  "
  },
  {
    "id": 10746,
    "package_name": "cytofan",
    "title": "Plot Fan Plots for Cytometry Data using 'ggplot2'",
    "description": "An implementation of Fan plots for cytometry data in 'ggplot2'. \n    For reference see Britton, E.; Fisher, P. & J. Whitley (1998) The Inflation Report Projections: Understanding the Fan Chart \n    <https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart>).",
    "version": "0.1.1",
    "maintainer": "Yann Abraham <yann.abraham@gmail.com>",
    "author": "Yann Abraham [aut, cre]",
    "url": "https://github.com/yannabraham/cytofan",
    "bug_reports": "https://github.com/yannabraham/cytofan/issues",
    "repository": "https://cran.r-project.org/package=cytofan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cytofan Plot Fan Plots for Cytometry Data using 'ggplot2' An implementation of Fan plots for cytometry data in 'ggplot2'. \n    For reference see Britton, E.; Fisher, P. & J. Whitley (1998) The Inflation Report Projections: Understanding the Fan Chart \n    <https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart>).  "
  },
  {
    "id": 10887,
    "package_name": "dblcens",
    "title": "Compute the NPMLE of Distribution Function from Doubly Censored\nData, Plus the Empirical Likelihood Ratio for F(T)",
    "description": "Doubly censored data, as described in Chang and Yang (1987) <doi: 10.1214/aos/1176350608>), are commonly seen in many fields. We use EM algorithm to compute the non-parametric MLE (NPMLE) of the cummulative probability function/survival function and the two censoring distributions. One can also specify a constraint F(T)=C, it will return the constrained NPMLE and the -2 log empirical likelihood ratio for this constraint. This can be used to test the hypothesis about the constraint and, by inverting the test, find confidence intervals for probability or quantile via empirical likelihood ratio theorem. Influence functions of hat F may also be calculated, but currently, the it may be slow.",
    "version": "1.1.9",
    "maintainer": "Yifan Yang <yfyang.86@hotmail.com>",
    "author": "Mai Zhou [aut],\n  Li Lee [aut],\n  Kun Chen [aut],\n  Yifan Yang [aut, cre, cph]",
    "url": "https://github.com/yfyang86/dblcens/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dblcens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dblcens Compute the NPMLE of Distribution Function from Doubly Censored\nData, Plus the Empirical Likelihood Ratio for F(T) Doubly censored data, as described in Chang and Yang (1987) <doi: 10.1214/aos/1176350608>), are commonly seen in many fields. We use EM algorithm to compute the non-parametric MLE (NPMLE) of the cummulative probability function/survival function and the two censoring distributions. One can also specify a constraint F(T)=C, it will return the constrained NPMLE and the -2 log empirical likelihood ratio for this constraint. This can be used to test the hypothesis about the constraint and, by inverting the test, find confidence intervals for probability or quantile via empirical likelihood ratio theorem. Influence functions of hat F may also be calculated, but currently, the it may be slow.  "
  },
  {
    "id": 10905,
    "package_name": "dclust",
    "title": "Divisive Hierarchical Clustering",
    "description": "Contains a single function dclust() for divisive hierarchical clustering based on \n    recursive k-means partitioning (k = 2). Useful for clustering large datasets\n    where computation of a n x n distance matrix is not feasible (e.g. n > 10,000 records).\n    For further information see Steinbach, Karypis and Kumar (2000) <http://glaros.dtc.umn.edu/gkhome/fetch/papers/docclusterKDDTMW00.pdf>.",
    "version": "0.1.0",
    "maintainer": "Shaun Wilkinson <shaunpwilkinson@gmail.com>",
    "author": "Shaun Wilkinson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7332-7931>),\n  Paolo Giordani [aut]",
    "url": "http://github.com/shaunpwilkinson/dclust",
    "bug_reports": "http://github.com/shaunpwilkinson/dclust/issues",
    "repository": "https://cran.r-project.org/package=dclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dclust Divisive Hierarchical Clustering Contains a single function dclust() for divisive hierarchical clustering based on \n    recursive k-means partitioning (k = 2). Useful for clustering large datasets\n    where computation of a n x n distance matrix is not feasible (e.g. n > 10,000 records).\n    For further information see Steinbach, Karypis and Kumar (2000) <http://glaros.dtc.umn.edu/gkhome/fetch/papers/docclusterKDDTMW00.pdf>.  "
  },
  {
    "id": 10921,
    "package_name": "ddecompose",
    "title": "Detailed Distributional Decomposition",
    "description": "Implements the Oaxaca-Blinder decomposition method and generalizations of it that decompose differences in distributional statistics beyond the mean.\n    The function ob_decompose() decomposes differences in the mean outcome between two groups into one part explained by different covariates (composition effect) and into another part due to differences in the way covariates are linked to the outcome variable (structure effect). The function further divides the two effects into the contribution of each covariate and allows for weighted doubly robust decompositions. For distributional statistics beyond the mean, the function performs the recentered influence function (RIF) decomposition proposed by Firpo, Fortin, and Lemieux (2018).\n    The function dfl_decompose() divides differences in distributional statistics into an composition effect and a structure effect using inverse probability weighting as introduced by DiNardo, Fortin, and Lemieux (1996). The function also allows to sequentially decompose the composition effect into the contribution of single covariates.\n    References: \n    Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. (2018) <doi:10.3390/econometrics6020028>. \"Decomposing Wage Distributions Using Recentered Influence Function Regressions.\"\n    Fortin, Nicole M., Thomas Lemieux, and Sergio Firpo. (2011) <doi:10.3386/w16045>. \"Decomposition Methods in Economics.\"\n    DiNardo, John, Nicole M. Fortin, and Thomas Lemieux. (1996) <doi:10.2307/2171954>. \"Labor Market Institutions and the Distribution of Wages, 1973-1992: A Semiparametric Approach.\"\n    Oaxaca, Ronald. (1973) <doi:10.2307/2525981>. \"Male-Female Wage Differentials in Urban Labor Markets.\"\n    Blinder, Alan S. (1973) <doi:10.2307/144855>. \"Wage Discrimination: Reduced Form and Structural Estimates.\"",
    "version": "1.0.0",
    "maintainer": "Samuel Meier <samuel.meier+ddecompose@immerda.ch>",
    "author": "David Gallusser [aut],\n  Samuel Meier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ddecompose",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ddecompose Detailed Distributional Decomposition Implements the Oaxaca-Blinder decomposition method and generalizations of it that decompose differences in distributional statistics beyond the mean.\n    The function ob_decompose() decomposes differences in the mean outcome between two groups into one part explained by different covariates (composition effect) and into another part due to differences in the way covariates are linked to the outcome variable (structure effect). The function further divides the two effects into the contribution of each covariate and allows for weighted doubly robust decompositions. For distributional statistics beyond the mean, the function performs the recentered influence function (RIF) decomposition proposed by Firpo, Fortin, and Lemieux (2018).\n    The function dfl_decompose() divides differences in distributional statistics into an composition effect and a structure effect using inverse probability weighting as introduced by DiNardo, Fortin, and Lemieux (1996). The function also allows to sequentially decompose the composition effect into the contribution of single covariates.\n    References: \n    Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. (2018) <doi:10.3390/econometrics6020028>. \"Decomposing Wage Distributions Using Recentered Influence Function Regressions.\"\n    Fortin, Nicole M., Thomas Lemieux, and Sergio Firpo. (2011) <doi:10.3386/w16045>. \"Decomposition Methods in Economics.\"\n    DiNardo, John, Nicole M. Fortin, and Thomas Lemieux. (1996) <doi:10.2307/2171954>. \"Labor Market Institutions and the Distribution of Wages, 1973-1992: A Semiparametric Approach.\"\n    Oaxaca, Ronald. (1973) <doi:10.2307/2525981>. \"Male-Female Wage Differentials in Urban Labor Markets.\"\n    Blinder, Alan S. (1973) <doi:10.2307/144855>. \"Wage Discrimination: Reduced Form and Structural Estimates.\"  "
  },
  {
    "id": 10938,
    "package_name": "debest",
    "title": "Duration Estimation for Biomarker Enrichment Studies and Trials",
    "description": "A general framework using mixture Weibull distributions to accurately predict biomarker-guided trial duration accounting for heterogeneous population. Extensive simulations are performed to evaluate the impact of heterogeneous population and the dynamics of biomarker characteristics and disease on the study duration. Several influential parameters including median survival time, enrollment rate, biomarker prevalence and effect size are identified. Efficiency gains of biomarker-guided trials can be quantitatively compared to the traditional all-comers design. For reference, see Zhang et al. (2024) <arXiv:2401.00540>.",
    "version": "0.1.0",
    "maintainer": "Hong Zhang <hzhang@wpi.edu>",
    "author": "Hong Zhang [aut, cre] (ORCID: <https://orcid.org/0000-0002-8869-8671>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=debest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "debest Duration Estimation for Biomarker Enrichment Studies and Trials A general framework using mixture Weibull distributions to accurately predict biomarker-guided trial duration accounting for heterogeneous population. Extensive simulations are performed to evaluate the impact of heterogeneous population and the dynamics of biomarker characteristics and disease on the study duration. Several influential parameters including median survival time, enrollment rate, biomarker prevalence and effect size are identified. Efficiency gains of biomarker-guided trials can be quantitatively compared to the traditional all-comers design. For reference, see Zhang et al. (2024) <arXiv:2401.00540>.  "
  },
  {
    "id": 10970,
    "package_name": "deepregression",
    "title": "Fitting Deep Distributional Regression",
    "description": "\n    Allows for the specification of semi-structured deep distributional regression models which are fitted in a neural network as \n    proposed by Ruegamer et al. (2023) <doi:10.18637/jss.v105.i02>.\n    Predictors can be modeled using structured (penalized) linear effects, structured non-linear effects or using an unstructured deep network model.",
    "version": "2.3.2",
    "maintainer": "David Ruegamer <david.ruegamer@gmail.com>",
    "author": "David Ruegamer [aut, cre],\n  Christopher Marquardt [ctb],\n  Laetitia Frost [ctb],\n  Florian Pfisterer [ctb],\n  Philipp Baumann [ctb],\n  Chris Kolb [ctb],\n  Lucas Kook [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=deepregression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deepregression Fitting Deep Distributional Regression \n    Allows for the specification of semi-structured deep distributional regression models which are fitted in a neural network as \n    proposed by Ruegamer et al. (2023) <doi:10.18637/jss.v105.i02>.\n    Predictors can be modeled using structured (penalized) linear effects, structured non-linear effects or using an unstructured deep network model.  "
  },
  {
    "id": 10974,
    "package_name": "deeptrafo",
    "title": "Fitting Deep Conditional Transformation Models",
    "description": "Allows for the specification of deep conditional transformation \n    models (DCTMs) and ordinal neural network transformation models, as \n    described in Baumann et al (2021) <doi:10.1007/978-3-030-86523-8_1> and \n    Kook et al (2022) <doi:10.1016/j.patcog.2021.108263>. Extensions such as\n    autoregressive DCTMs (Ruegamer et al, 2023, <doi:10.1007/s11222-023-10212-8>)\n    and transformation ensembles (Kook et al, 2022, <doi:10.48550/arXiv.2205.12729>)\n    are implemented. The software package is described in Kook et al (2024,\n    <doi:10.18637/jss.v111.i10>).",
    "version": "1.0-0",
    "maintainer": "Lucas Kook <lucasheinrich.kook@gmail.com>",
    "author": "Lucas Kook [aut, cre],\n  Philipp Baumann [aut],\n  David Ruegamer [aut]",
    "url": "https://github.com/neural-structured-additive-learning/deeptrafo",
    "bug_reports": "https://github.com/neural-structured-additive-learning/deeptrafo/issues",
    "repository": "https://cran.r-project.org/package=deeptrafo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deeptrafo Fitting Deep Conditional Transformation Models Allows for the specification of deep conditional transformation \n    models (DCTMs) and ordinal neural network transformation models, as \n    described in Baumann et al (2021) <doi:10.1007/978-3-030-86523-8_1> and \n    Kook et al (2022) <doi:10.1016/j.patcog.2021.108263>. Extensions such as\n    autoregressive DCTMs (Ruegamer et al, 2023, <doi:10.1007/s11222-023-10212-8>)\n    and transformation ensembles (Kook et al, 2022, <doi:10.48550/arXiv.2205.12729>)\n    are implemented. The software package is described in Kook et al (2024,\n    <doi:10.18637/jss.v111.i10>).  "
  },
  {
    "id": 11165,
    "package_name": "dineq",
    "title": "Decomposition of (Income) Inequality",
    "description": "Decomposition of (income) inequality by population sub groups. \n    For a decomposition on a single variable the mean log deviation can be used\n      (see Mookherjee Shorrocks (1982) <DOI:10.2307/2232673>).\n    For a decomposition on multiple variables a regression based technique can be \n      used (see Fields (2003) <DOI:10.1016/s0147-9121(03)22001-x>).\n    Recentered influence function regression for marginal effects of the (income \n      or wealth) distribution  (see Firpo et al. (2009) <DOI:10.3982/ECTA6822>).\n    Some extensions to inequality functions to handle weights and/or missings.",
    "version": "0.1.0",
    "maintainer": "Ren\u00e9 Schulenberg <reneschulenberg@gmail.com>",
    "author": "Ren\u00e9 Schulenberg ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dineq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dineq Decomposition of (Income) Inequality Decomposition of (income) inequality by population sub groups. \n    For a decomposition on a single variable the mean log deviation can be used\n      (see Mookherjee Shorrocks (1982) <DOI:10.2307/2232673>).\n    For a decomposition on multiple variables a regression based technique can be \n      used (see Fields (2003) <DOI:10.1016/s0147-9121(03)22001-x>).\n    Recentered influence function regression for marginal effects of the (income \n      or wealth) distribution  (see Firpo et al. (2009) <DOI:10.3982/ECTA6822>).\n    Some extensions to inequality functions to handle weights and/or missings.  "
  },
  {
    "id": 11183,
    "package_name": "discFA",
    "title": "Discrete Factor Analysis",
    "description": "Discrete factor analysis for dependent Poisson and negative binomial models with truncation, zero inflation, and zero inflated truncation.",
    "version": "1.0.1",
    "maintainer": "Reza A. Belaghi <rezaarabi11@gmail.com>",
    "author": "Yasin Asar [aut] (ORCID: <https://orcid.org/0000-0003-1109-8456>),\n  Reza A. Belaghi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6989-9267>),\n  Rolf Larsson [aut] (ORCID: <https://orcid.org/0000-0003-0336-2974>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=discFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "discFA Discrete Factor Analysis Discrete factor analysis for dependent Poisson and negative binomial models with truncation, zero inflation, and zero inflated truncation.  "
  },
  {
    "id": 11250,
    "package_name": "diversityForest",
    "title": "Innovative Complex Split Procedures in Random Forests Through\nCandidate Split Sampling",
    "description": "Implementation of three methods based on the diversity forest (DF) algorithm \n  (Hornung, 2022, <doi:10.1007/s42979-021-00920-1>), a split-finding approach that \n  enables complex split procedures in random forests.\n  The package includes:\n    1. Interaction forests (IFs) (Hornung & Boulesteix, 2022, <doi:10.1016/j.csda.2022.107460>): \n    Model quantitative and qualitative interaction effects using bivariable splitting. \n    Come with the Effect Importance Measure (EIM), which can be used to identify variable \n    pairs that have well-interpretable quantitative and qualitative interaction effects \n    with high predictive relevance.\n\t2. Two random forest-based variable importance measures (VIMs) for multi-class outcomes: \n\tthe class-focused VIM, which ranks covariates by their ability to distinguish individual \n\toutcome classes from the others, and the discriminatory VIM, which measures overall \n\tcovariate influence irrespective of class-specific relevance.\n    3. The basic form of diversity forests that uses conventional univariable, binary \n    splitting (Hornung, 2022).\n  Except for the multi-class VIMs, all methods support categorical, metric, and survival \n  outcomes. The package includes visualization tools for interpreting the identified \n  covariate effects.\n  Built as a fork of the 'ranger' R package (main author: Marvin N. Wright), which \n  implements random forests using an efficient C++ implementation.",
    "version": "0.6.0",
    "maintainer": "Roman Hornung <hornung@ibe.med.uni-muenchen.de>",
    "author": "Roman Hornung [aut, cre],\n  Marvin N. Wright [ctb, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=diversityForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diversityForest Innovative Complex Split Procedures in Random Forests Through\nCandidate Split Sampling Implementation of three methods based on the diversity forest (DF) algorithm \n  (Hornung, 2022, <doi:10.1007/s42979-021-00920-1>), a split-finding approach that \n  enables complex split procedures in random forests.\n  The package includes:\n    1. Interaction forests (IFs) (Hornung & Boulesteix, 2022, <doi:10.1016/j.csda.2022.107460>): \n    Model quantitative and qualitative interaction effects using bivariable splitting. \n    Come with the Effect Importance Measure (EIM), which can be used to identify variable \n    pairs that have well-interpretable quantitative and qualitative interaction effects \n    with high predictive relevance.\n\t2. Two random forest-based variable importance measures (VIMs) for multi-class outcomes: \n\tthe class-focused VIM, which ranks covariates by their ability to distinguish individual \n\toutcome classes from the others, and the discriminatory VIM, which measures overall \n\tcovariate influence irrespective of class-specific relevance.\n    3. The basic form of diversity forests that uses conventional univariable, binary \n    splitting (Hornung, 2022).\n  Except for the multi-class VIMs, all methods support categorical, metric, and survival \n  outcomes. The package includes visualization tools for interpreting the identified \n  covariate effects.\n  Built as a fork of the 'ranger' R package (main author: Marvin N. Wright), which \n  implements random forests using an efficient C++ implementation.  "
  },
  {
    "id": 11258,
    "package_name": "diyar",
    "title": "Record Linkage and Epidemiological Case Definitions in 'R'",
    "description": "An R package for iterative and batched record linkage, \n    and applying epidemiological case definitions.\n    'diyar' can be used for deterministic and probabilistic record linkage, \n    or multistage record linkage combining both approaches.\n    It features the implementation of nested match criteria, and mechanisms to \n    address missing data and conflicting matches during stepwise record linkage.\n    Case definitions are implemented by assigning records to groups based on \n    match criteria such as person or place, and overlapping time or duration of \n    events e.g. sample collection dates or periods of hospital stays.\n    Matching records are assigned a unique group ID. Index and duplicate records \n    are removed or further analyses as required.  ",
    "version": "0.5.1",
    "maintainer": "Olisaeloka Nsonwu <olisa.nsonwu@gmail.com>",
    "author": "Olisaeloka Nsonwu",
    "url": "https://olisansonwu.github.io/diyar/index.html",
    "bug_reports": "https://github.com/OlisaNsonwu/diyar/issues",
    "repository": "https://cran.r-project.org/package=diyar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diyar Record Linkage and Epidemiological Case Definitions in 'R' An R package for iterative and batched record linkage, \n    and applying epidemiological case definitions.\n    'diyar' can be used for deterministic and probabilistic record linkage, \n    or multistage record linkage combining both approaches.\n    It features the implementation of nested match criteria, and mechanisms to \n    address missing data and conflicting matches during stepwise record linkage.\n    Case definitions are implemented by assigning records to groups based on \n    match criteria such as person or place, and overlapping time or duration of \n    events e.g. sample collection dates or periods of hospital stays.\n    Matching records are assigned a unique group ID. Index and duplicate records \n    are removed or further analyses as required.    "
  },
  {
    "id": 11282,
    "package_name": "dndR",
    "title": "Dungeons & Dragons Functions for Players and Dungeon Masters",
    "description": "The goal of 'dndR' is to provide a suite of Dungeons & Dragons related functions.\n    This package is meant to be useful both to players and Dungeon Masters (DMs).\n    Some functions apply to many tabletop role-playing games (e.g., dice rolling), but others are focused on Fifth Edition (a.k.a. \"5e\") and where possible both the 2014 and 2024 versions are supported.",
    "version": "3.1.0",
    "maintainer": "Nicholas Lyon <njlyon@alumni.iastate.edu>",
    "author": "Nicholas Lyon [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3905-1078>),\n  Tim Schatto-Eckrodt [aut] (https://kudusch.de),\n  Humberto Nappo [aut] (ORCID: <https://orcid.org/0000-0001-7810-1635>),\n  Billy Mitchell [aut] (https://wj-mitchell.github.io/)",
    "url": "https://njlyon0.github.io/dndR/",
    "bug_reports": "https://github.com/njlyon0/dndR/issues",
    "repository": "https://cran.r-project.org/package=dndR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dndR Dungeons & Dragons Functions for Players and Dungeon Masters The goal of 'dndR' is to provide a suite of Dungeons & Dragons related functions.\n    This package is meant to be useful both to players and Dungeon Masters (DMs).\n    Some functions apply to many tabletop role-playing games (e.g., dice rolling), but others are focused on Fifth Edition (a.k.a. \"5e\") and where possible both the 2014 and 2024 versions are supported.  "
  },
  {
    "id": 11401,
    "package_name": "drugDemand",
    "title": "Drug Demand Forecasting",
    "description": "Performs drug demand forecasting by modeling drug dispensing data while taking into account predicted enrollment and treatment discontinuation dates. The gap time between randomization and the first drug dispensing visit is modeled using interval-censored exponential, Weibull, log-logistic, or log-normal distributions (Anderson-Bergman (2017) <doi:10.18637/jss.v081.i12>). The number of skipped visits is modeled using Poisson, zero-inflated Poisson, or negative binomial distributions (Zeileis, Kleiber & Jackman (2008) <doi:10.18637/jss.v027.i08>). The gap time between two consecutive drug dispensing visits given the number of skipped visits is modeled using linear regression based on least squares or least absolute deviations (Birkes & Dodge (1993, ISBN:0-471-56881-3)). The number of dispensed doses is modeled using linear or linear mixed-effects models (McCulloch & Searle (2001, ISBN:0-471-19364-X)).",
    "version": "0.1.3",
    "maintainer": "Kaifeng Lu <kaifenglu@gmail.com>",
    "author": "Kaifeng Lu [aut, cre] (ORCID: <https://orcid.org/0000-0002-6160-7119>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=drugDemand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drugDemand Drug Demand Forecasting Performs drug demand forecasting by modeling drug dispensing data while taking into account predicted enrollment and treatment discontinuation dates. The gap time between randomization and the first drug dispensing visit is modeled using interval-censored exponential, Weibull, log-logistic, or log-normal distributions (Anderson-Bergman (2017) <doi:10.18637/jss.v081.i12>). The number of skipped visits is modeled using Poisson, zero-inflated Poisson, or negative binomial distributions (Zeileis, Kleiber & Jackman (2008) <doi:10.18637/jss.v027.i08>). The gap time between two consecutive drug dispensing visits given the number of skipped visits is modeled using linear regression based on least squares or least absolute deviations (Birkes & Dodge (1993, ISBN:0-471-56881-3)). The number of dispensed doses is modeled using linear or linear mixed-effects models (McCulloch & Searle (2001, ISBN:0-471-19364-X)).  "
  },
  {
    "id": 11429,
    "package_name": "dstat",
    "title": "Conditional Sensitivity Analysis for Matched Observational\nStudies",
    "description": "A d-statistic tests the null hypothesis of no treatment effect in a matched, nonrandomized study of the effects caused by treatments.  A d-statistic focuses on subsets of matched pairs that demonstrate insensitivity to unmeasured bias in such an observational study, correcting for double-use of the data by conditional inference. This conditional inference can, in favorable circumstances, substantially increase the power of a sensitivity analysis (Rosenbaum (2010) <doi:10.1007/978-1-4419-1213-8_14>).  There are two examples, one concerning unemployment from Lalive et al. (2006) <doi:10.1111/j.1467-937X.2006.00406.x>, the other concerning smoking and periodontal disease from Rosenbaum (2017) <doi:10.1214/17-STS621>.  ",
    "version": "1.0.4",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dstat Conditional Sensitivity Analysis for Matched Observational\nStudies A d-statistic tests the null hypothesis of no treatment effect in a matched, nonrandomized study of the effects caused by treatments.  A d-statistic focuses on subsets of matched pairs that demonstrate insensitivity to unmeasured bias in such an observational study, correcting for double-use of the data by conditional inference. This conditional inference can, in favorable circumstances, substantially increase the power of a sensitivity analysis (Rosenbaum (2010) <doi:10.1007/978-1-4419-1213-8_14>).  There are two examples, one concerning unemployment from Lalive et al. (2006) <doi:10.1111/j.1467-937X.2006.00406.x>, the other concerning smoking and periodontal disease from Rosenbaum (2017) <doi:10.1214/17-STS621>.    "
  },
  {
    "id": 11472,
    "package_name": "dySEM",
    "title": "Dyadic Structural Equation Modeling",
    "description": "Scripting of structural equation models via 'lavaan' for\n    Dyadic Data Analysis, and helper functions for supplemental\n    calculations, tabling, and model visualization.  Current models\n    supported include Dyadic Confirmatory Factor Analysis, the Actor\u2013Partner \n    Interdependence Model (observed and latent), the Common Fate Model\n    (observed and latent), Mutual Influence Model (latent), and the Bifactor\n    Dyadic Model (latent).",
    "version": "1.1.1",
    "maintainer": "John Sakaluk <jksakaluk@gmail.com>",
    "author": "John Sakaluk [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2515-9822>),\n  Omar Camanto [aut],\n  Robyn Kilshaw [ctb],\n  Alexandra Fisher [ctb]",
    "url": "https://github.com/jsakaluk/dySEM,\nhttps://jsakaluk.github.io/dySEM/",
    "bug_reports": "https://github.com/jsakaluk/dySEM/issues",
    "repository": "https://cran.r-project.org/package=dySEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dySEM Dyadic Structural Equation Modeling Scripting of structural equation models via 'lavaan' for\n    Dyadic Data Analysis, and helper functions for supplemental\n    calculations, tabling, and model visualization.  Current models\n    supported include Dyadic Confirmatory Factor Analysis, the Actor\u2013Partner \n    Interdependence Model (observed and latent), the Common Fate Model\n    (observed and latent), Mutual Influence Model (latent), and the Bifactor\n    Dyadic Model (latent).  "
  },
  {
    "id": 11526,
    "package_name": "eRTG3D",
    "title": "Empirically Informed Random Trajectory Generation in 3-D",
    "description": "Creates realistic random trajectories in a 3-D space between two given fix points, so-called conditional empirical random walks (CERWs). The trajectory generation is based on empirical distribution functions extracted from observed trajectories (training data) and thus reflects the geometrical movement characteristics of the mover. A digital elevation model (DEM), representing the Earth's surface, and a background layer of probabilities (e.g. food sources, uplift potential, waterbodies, etc.) can be used to influence the trajectories.\n    Unterfinger M (2018). \"3-D Trajectory Simulation in Movement Ecology: Conditional Empirical Random Walk\". Master's thesis, University of Zurich. <https://www.geo.uzh.ch/dam/jcr:6194e41e-055c-4635-9807-53c5a54a3be7/MasterThesis_Unterfinger_2018.pdf>.\n    Technitis G, Weibel R, Kranstauber B, Safi K (2016). \"An algorithm for empirically informed random trajectory generation between two endpoints\". GIScience 2016: Ninth International Conference on Geographic Information Science, 9, online. <doi:10.5167/uzh-130652>.",
    "version": "0.7.0",
    "maintainer": "Merlin Unterfinger <info@munterfinger.ch>",
    "author": "Merlin Unterfinger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2020-2366>),\n  Kamran Safi [ctb, ths],\n  George Technitis [ctb, ths],\n  Robert Weibel [ths]",
    "url": "https://munterfi.github.io/eRTG3D/,\nhttps://github.com/munterfi/eRTG3D/",
    "bug_reports": "https://github.com/munterfi/eRTG3D/issues/",
    "repository": "https://cran.r-project.org/package=eRTG3D",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eRTG3D Empirically Informed Random Trajectory Generation in 3-D Creates realistic random trajectories in a 3-D space between two given fix points, so-called conditional empirical random walks (CERWs). The trajectory generation is based on empirical distribution functions extracted from observed trajectories (training data) and thus reflects the geometrical movement characteristics of the mover. A digital elevation model (DEM), representing the Earth's surface, and a background layer of probabilities (e.g. food sources, uplift potential, waterbodies, etc.) can be used to influence the trajectories.\n    Unterfinger M (2018). \"3-D Trajectory Simulation in Movement Ecology: Conditional Empirical Random Walk\". Master's thesis, University of Zurich. <https://www.geo.uzh.ch/dam/jcr:6194e41e-055c-4635-9807-53c5a54a3be7/MasterThesis_Unterfinger_2018.pdf>.\n    Technitis G, Weibel R, Kranstauber B, Safi K (2016). \"An algorithm for empirically informed random trajectory generation between two endpoints\". GIScience 2016: Ninth International Conference on Geographic Information Science, 9, online. <doi:10.5167/uzh-130652>.  "
  },
  {
    "id": 11554,
    "package_name": "easyanova",
    "title": "Analysis of Variance and Other Important Complementary Analyses",
    "description": "Perform analysis of variance and other important complementary\n  analyses.  The functions are easy to use.  Performs analysis in various\n  designs, with balanced and unbalanced data.",
    "version": "11.0",
    "maintainer": "Emmanuel Arnhold <emmanuelarnhold@yahoo.com.br>",
    "author": "Emmanuel Arnhold [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=easyanova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easyanova Analysis of Variance and Other Important Complementary Analyses Perform analysis of variance and other important complementary\n  analyses.  The functions are easy to use.  Performs analysis in various\n  designs, with balanced and unbalanced data.  "
  },
  {
    "id": 11736,
    "package_name": "electoral",
    "title": "Allocating Seats Methods and Party System Scores",
    "description": "Highest averages & largest remainders allocating seats methods and\n    several party system scores.\n    Implemented highest averages allocating seats methods are D'Hondt, Webster,\n    Danish, Imperiali, Hill-Huntington, Dean, Modified Sainte-Lague,\n    equal proportions and Adams.\n    Implemented largest remainders allocating seats methods are Hare, Droop,\n    Hangenbach-Bischoff, Imperial, modified Imperial and quotas & remainders.\n    The main advantage of this package is that ties are always reported\n    and not incorrectly allocated.\n    Party system scores provided are competitiveness, concentration,\n    effective number of parties, party nationalization score,\n    party system nationalization score and volatility.\n    References:\n    Gallagher (1991) <doi:10.1016/0261-3794(91)90004-C>.\n    Norris (2004, ISBN:0-521-82977-1).\n    Laakso & Taagepera (1979) <https://escholarship.org/uc/item/703827nv>.\n    Jones & Mainwaring (2003) <https://kellogg.nd.edu/sites/default/files/old_files/documents/304_0.pdf>.\n    Pedersen (1979) <https://janda.org/c24/Readings/Pedersen/Pedersen.htm>.\n    Golosov (2010) <doi:10.1177/1354068809339538>.\n    Golosov (2014) <doi:10.1177/1354068814549342>.",
    "version": "0.1.4",
    "maintainer": "Jorge Albuja <albuja@yahoo.com>",
    "author": "Jorge Albuja [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=electoral",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "electoral Allocating Seats Methods and Party System Scores Highest averages & largest remainders allocating seats methods and\n    several party system scores.\n    Implemented highest averages allocating seats methods are D'Hondt, Webster,\n    Danish, Imperiali, Hill-Huntington, Dean, Modified Sainte-Lague,\n    equal proportions and Adams.\n    Implemented largest remainders allocating seats methods are Hare, Droop,\n    Hangenbach-Bischoff, Imperial, modified Imperial and quotas & remainders.\n    The main advantage of this package is that ties are always reported\n    and not incorrectly allocated.\n    Party system scores provided are competitiveness, concentration,\n    effective number of parties, party nationalization score,\n    party system nationalization score and volatility.\n    References:\n    Gallagher (1991) <doi:10.1016/0261-3794(91)90004-C>.\n    Norris (2004, ISBN:0-521-82977-1).\n    Laakso & Taagepera (1979) <https://escholarship.org/uc/item/703827nv>.\n    Jones & Mainwaring (2003) <https://kellogg.nd.edu/sites/default/files/old_files/documents/304_0.pdf>.\n    Pedersen (1979) <https://janda.org/c24/Readings/Pedersen/Pedersen.htm>.\n    Golosov (2010) <doi:10.1177/1354068809339538>.\n    Golosov (2014) <doi:10.1177/1354068814549342>.  "
  },
  {
    "id": 11751,
    "package_name": "elo",
    "title": "Ranking Teams by Elo Rating and Comparable Methods",
    "description": "A flexible framework for calculating Elo ratings and resulting\n    rankings of any two-team-per-matchup system (chess, sports leagues, 'Go',\n    etc.). This implementation is capable of evaluating a variety of matchups,\n    Elo rating updates, and win probabilities, all based on the basic Elo\n    rating system. It also includes methods to benchmark performance,\n    including logistic regression and Markov chain models.",
    "version": "3.0.2",
    "maintainer": "Ethan Heinzen <heinzen.ethan@mayo.edu>",
    "author": "Ethan Heinzen [aut, cre]",
    "url": "https://github.com/eheinzen/elo,\nhttps://cran.r-project.org/package=elo,\nhttps://eheinzen.github.io/elo/",
    "bug_reports": "https://github.com/eheinzen/elo/issues",
    "repository": "https://cran.r-project.org/package=elo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elo Ranking Teams by Elo Rating and Comparable Methods A flexible framework for calculating Elo ratings and resulting\n    rankings of any two-team-per-matchup system (chess, sports leagues, 'Go',\n    etc.). This implementation is capable of evaluating a variety of matchups,\n    Elo rating updates, and win probabilities, all based on the basic Elo\n    rating system. It also includes methods to benchmark performance,\n    including logistic regression and Markov chain models.  "
  },
  {
    "id": 11842,
    "package_name": "epidatr",
    "title": "Client for Delphi's 'Epidata' API",
    "description": "The Delphi 'Epidata' API provides real-time access to\n    epidemiological surveillance data for influenza, 'COVID-19', and other\n    diseases for the USA at various geographical resolutions, both from\n    official government sources such as the Center for Disease Control\n    (CDC) and Google Trends and private partners such as Facebook and\n    Change 'Healthcare'. It is built and maintained by the Carnegie Mellon\n    University Delphi research group. To cite this API: David C. Farrow,\n    Logan C. Brooks, Aaron 'Rumack', Ryan J. 'Tibshirani', 'Roni'\n    'Rosenfeld' (2015). Delphi 'Epidata' API.\n    <https://github.com/cmu-delphi/delphi-epidata>.",
    "version": "1.2.2",
    "maintainer": "David Weber <davidweb@andrew.cmu.edu>",
    "author": "Logan Brooks [aut],\n  Dmitry Shemetov [aut],\n  Samuel Gratzl [aut],\n  David Weber [ctb, cre],\n  Nat DeFries [ctb],\n  Alex Reinhart [ctb],\n  Daniel J. McDonald [ctb],\n  Kean Ming Tan [ctb],\n  Will Townes [ctb],\n  George Haff [ctb],\n  Kathryn Mazaitis [ctb]",
    "url": "https://cmu-delphi.github.io/epidatr/,\nhttps://cmu-delphi.github.io/delphi-epidata/,\nhttps://github.com/cmu-delphi/epidatr",
    "bug_reports": "https://github.com/cmu-delphi/epidatr/issues",
    "repository": "https://cran.r-project.org/package=epidatr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epidatr Client for Delphi's 'Epidata' API The Delphi 'Epidata' API provides real-time access to\n    epidemiological surveillance data for influenza, 'COVID-19', and other\n    diseases for the USA at various geographical resolutions, both from\n    official government sources such as the Center for Disease Control\n    (CDC) and Google Trends and private partners such as Facebook and\n    Change 'Healthcare'. It is built and maintained by the Carnegie Mellon\n    University Delphi research group. To cite this API: David C. Farrow,\n    Logan C. Brooks, Aaron 'Rumack', Ryan J. 'Tibshirani', 'Roni'\n    'Rosenfeld' (2015). Delphi 'Epidata' API.\n    <https://github.com/cmu-delphi/delphi-epidata>.  "
  },
  {
    "id": 11919,
    "package_name": "esback",
    "title": "Expected Shortfall Backtesting",
    "description": "Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.",
    "version": "0.3.1",
    "maintainer": "Sebastian Bayer <sebastian.bayer@uni-konstanz.de>",
    "author": "Sebastian Bayer [aut, cre],\n  Timo Dimitriadis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=esback",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "esback Expected Shortfall Backtesting Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.  "
  },
  {
    "id": 11968,
    "package_name": "euroleaguer",
    "title": "'Euroleague basketball API'",
    "description": "Unofficial API wrapper for 'Euroleague' and 'Eurocup' basketball API (<https://www.euroleaguebasketball.net/en/euroleague/>), it allows to retrieve real-time and historical standard and advanced statistics about competitions, teams, players and games.",
    "version": "0.2.0",
    "maintainer": "Flavio Leccese <flavioleccese92@gmail.com>",
    "author": "Flavio Leccese [aut, cre]",
    "url": "https://github.com/FlavioLeccese92/euroleaguer/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=euroleaguer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "euroleaguer 'Euroleague basketball API' Unofficial API wrapper for 'Euroleague' and 'Eurocup' basketball API (<https://www.euroleaguebasketball.net/en/euroleague/>), it allows to retrieve real-time and historical standard and advanced statistics about competitions, teams, players and games.  "
  },
  {
    "id": 11981,
    "package_name": "evenBreak",
    "title": "A Posteriori Probs of Suits Breaking Evenly Across Four Hands",
    "description": "We quantitatively evaluated the assertion that says if one suit is found to be evenly distributed among the 4 players, the rest of the suits are more likely to be evenly distributed. Our mathematical analyses show that, if one suit is found to be evenly distributed, then a second suit has a slightly elevated probability (ranging between 10% to 15%) of being evenly distributed. If two suits are found to be evenly distributed, then a third suit has a substantially elevated probability (ranging between 30% to 50%) of being evenly distributed.This package refers to methods and authentic data from Ely Culbertson <https://www.bridgebum.com/law_of_symmetry.php>, Gregory Stoll <https://gregstoll.com/~gregstoll/bridge/math.html>, and details of performing the probability calculations from Jeremy L. Martin <https://jlmartin.ku.edu/~jlmartin/bridge/basics.pdf>, Emile Borel and Andre Cheron (1954) \"The Mathematical Theory of Bridge\",Antonio Vivaldi and Gianni Barracho (2001, ISBN:0 7134 8663 5)  \"Probabilities and Alternatives in Bridge\", Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>.",
    "version": "1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=evenBreak",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evenBreak A Posteriori Probs of Suits Breaking Evenly Across Four Hands We quantitatively evaluated the assertion that says if one suit is found to be evenly distributed among the 4 players, the rest of the suits are more likely to be evenly distributed. Our mathematical analyses show that, if one suit is found to be evenly distributed, then a second suit has a slightly elevated probability (ranging between 10% to 15%) of being evenly distributed. If two suits are found to be evenly distributed, then a third suit has a substantially elevated probability (ranging between 30% to 50%) of being evenly distributed.This package refers to methods and authentic data from Ely Culbertson <https://www.bridgebum.com/law_of_symmetry.php>, Gregory Stoll <https://gregstoll.com/~gregstoll/bridge/math.html>, and details of performing the probability calculations from Jeremy L. Martin <https://jlmartin.ku.edu/~jlmartin/bridge/basics.pdf>, Emile Borel and Andre Cheron (1954) \"The Mathematical Theory of Bridge\",Antonio Vivaldi and Gianni Barracho (2001, ISBN:0 7134 8663 5)  \"Probabilities and Alternatives in Bridge\", Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>.  "
  },
  {
    "id": 11996,
    "package_name": "evident",
    "title": "Evidence Factors in Observational Studies",
    "description": "Contains a collection of examples of evidence factors in observational studies from the book Replication and Evidence Factors in Observational Studies by Paul R. Rosenbaum (2021) <doi:10.1201/9781003039648>.",
    "version": "1.0.4",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=evident",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evident Evidence Factors in Observational Studies Contains a collection of examples of evidence factors in observational studies from the book Replication and Evidence Factors in Observational Studies by Paul R. Rosenbaum (2021) <doi:10.1201/9781003039648>.  "
  },
  {
    "id": 11998,
    "package_name": "evinf",
    "title": "Inference with Extreme Value Inflated Count Data",
    "description": "Allows users to model and draw inferences from extreme value inflated count data, and to evaluate these models and compare to non extreme-value inflated counterparts. The package is built to be compatible with standard presentation tools such as 'broom', 'tidy', and 'modelsummary'.",
    "version": "0.8.10",
    "maintainer": "David Randahl <david.randahl@pcr.uu.se>",
    "author": "David Randahl [cre, aut],\n  Johan Vegelius [aut]",
    "url": "https://github.com/Doktorandahl/evinf",
    "bug_reports": "https://github.com/Doktorandahl/evinf/issues",
    "repository": "https://cran.r-project.org/package=evinf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evinf Inference with Extreme Value Inflated Count Data Allows users to model and draw inferences from extreme value inflated count data, and to evaluate these models and compare to non extreme-value inflated counterparts. The package is built to be compatible with standard presentation tools such as 'broom', 'tidy', and 'modelsummary'.  "
  },
  {
    "id": 12003,
    "package_name": "evobiR",
    "title": "Comparative and Population Genetic Analyses",
    "description": "Comparative analysis of continuous traits influencing discrete states, and utility tools to facilitate comparative analyses.  Implementations of ABBA/BABA type statistics to test for introgression in genomic data. Wright-Fisher, phylogenetic tree, and statistical distribution Shiny interactive simulations for use in teaching.",
    "version": "1.1",
    "maintainer": "Heath Blackmon <coleoguy@gmail.com>",
    "author": "Heath Blackmon and Richard H. Adams",
    "url": "http://www.uta.edu/karyodb/evobiR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=evobiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evobiR Comparative and Population Genetic Analyses Comparative analysis of continuous traits influencing discrete states, and utility tools to facilitate comparative analyses.  Implementations of ABBA/BABA type statistics to test for introgression in genomic data. Wright-Fisher, phylogenetic tree, and statistical distribution Shiny interactive simulations for use in teaching.  "
  },
  {
    "id": 12077,
    "package_name": "extraDistr",
    "title": "Additional Univariate and Multivariate Distributions",
    "description": "Density, distribution function, quantile function\n    and random generation for a number of univariate\n    and multivariate distributions. This package implements the\n    following distributions: Bernoulli, beta-binomial, beta-negative\n    binomial, beta prime, Bhattacharjee, Birnbaum-Saunders,\n    bivariate normal, bivariate Poisson, categorical, Dirichlet,\n    Dirichlet-multinomial, discrete gamma, discrete Laplace,\n    discrete normal, discrete uniform, discrete Weibull, Frechet,\n    gamma-Poisson, generalized extreme value, Gompertz,\n    generalized Pareto, Gumbel, half-Cauchy, half-normal, half-t,\n    Huber density, inverse chi-squared, inverse-gamma, Kumaraswamy,\n    Laplace, location-scale t, logarithmic, Lomax, multivariate\n    hypergeometric, multinomial, negative hypergeometric,\n    non-standard beta, normal mixture, Poisson mixture, Pareto,\n    power, reparametrized beta, Rayleigh, shifted Gompertz, Skellam,\n    slash, triangular, truncated binomial, truncated normal,\n    truncated Poisson, Tukey lambda, Wald, zero-inflated binomial,\n    zero-inflated negative binomial, zero-inflated Poisson.",
    "version": "1.10.0",
    "maintainer": "Tymoteusz Wolodzko <twolodzko+extraDistr@gmail.com>",
    "author": "Tymoteusz Wolodzko",
    "url": "https://github.com/twolodzko/extraDistr",
    "bug_reports": "https://github.com/twolodzko/extraDistr/issues",
    "repository": "https://cran.r-project.org/package=extraDistr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extraDistr Additional Univariate and Multivariate Distributions Density, distribution function, quantile function\n    and random generation for a number of univariate\n    and multivariate distributions. This package implements the\n    following distributions: Bernoulli, beta-binomial, beta-negative\n    binomial, beta prime, Bhattacharjee, Birnbaum-Saunders,\n    bivariate normal, bivariate Poisson, categorical, Dirichlet,\n    Dirichlet-multinomial, discrete gamma, discrete Laplace,\n    discrete normal, discrete uniform, discrete Weibull, Frechet,\n    gamma-Poisson, generalized extreme value, Gompertz,\n    generalized Pareto, Gumbel, half-Cauchy, half-normal, half-t,\n    Huber density, inverse chi-squared, inverse-gamma, Kumaraswamy,\n    Laplace, location-scale t, logarithmic, Lomax, multivariate\n    hypergeometric, multinomial, negative hypergeometric,\n    non-standard beta, normal mixture, Poisson mixture, Pareto,\n    power, reparametrized beta, Rayleigh, shifted Gompertz, Skellam,\n    slash, triangular, truncated binomial, truncated normal,\n    truncated Poisson, Tukey lambda, Wald, zero-inflated binomial,\n    zero-inflated negative binomial, zero-inflated Poisson.  "
  },
  {
    "id": 12081,
    "package_name": "extrafrail",
    "title": "Estimation and Additional Tools for Alternative Shared Frailty\nModels",
    "description": "Provide estimation and data generation tools for new multivariate frailty models.\n             This version includes the gamma, inverse Gaussian, weighted Lindley, Birnbaum-Saunders,\n\t     truncated normal, mixture of inverse Gaussian, mixture of Birnbaum-Saunders, \n             generalized exponential and Jorgensen-Seshadri-Whitmore as the distribution for\n             frailty terms. For the basal model, it is considered a parametric approach based \n             on the exponential, Weibull and the piecewise exponential distributions as well as a \n             semiparametric approach. For details, see Gallardo et al. (2024) <doi:10.1007/s11222-024-10458-w>,\n             Gallardo et al. (2025) <doi:10.1002/bimj.70044>, Kiprotich et al. (2025) <doi:10.1177/09622802251338984>\n             and Gallardo et al. (2025) <doi:10.1038/s41598-025-15903-y>.",
    "version": "1.14",
    "maintainer": "Diego Gallardo <dgallardo@ubiobio.cl>",
    "author": "Diego Gallardo [aut, cre],\n  Marcelo Bourguignon [aut],\n  John Santib<c3><a1><c3><b1>ez [ctb],\n  Gilbert Kiprotich [ctb],\n  Pedro Ramos [ctb],\n  Thomas Augustin [ctb],\n  Zohreh Mohammadi [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=extrafrail",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extrafrail Estimation and Additional Tools for Alternative Shared Frailty\nModels Provide estimation and data generation tools for new multivariate frailty models.\n             This version includes the gamma, inverse Gaussian, weighted Lindley, Birnbaum-Saunders,\n\t     truncated normal, mixture of inverse Gaussian, mixture of Birnbaum-Saunders, \n             generalized exponential and Jorgensen-Seshadri-Whitmore as the distribution for\n             frailty terms. For the basal model, it is considered a parametric approach based \n             on the exponential, Weibull and the piecewise exponential distributions as well as a \n             semiparametric approach. For details, see Gallardo et al. (2024) <doi:10.1007/s11222-024-10458-w>,\n             Gallardo et al. (2025) <doi:10.1002/bimj.70044>, Kiprotich et al. (2025) <doi:10.1177/09622802251338984>\n             and Gallardo et al. (2025) <doi:10.1038/s41598-025-15903-y>.  "
  },
  {
    "id": 12089,
    "package_name": "extrememix",
    "title": "Bayesian Estimation of Extreme Value Mixture Models",
    "description": "Fits extreme value mixture models, which are models for tails not requiring selection of a threshold, for continuous data. It includes functions for model comparison, estimation of quantity of interest in extreme value analysis and plotting. Reference: CN Behrens, HF Lopes, D Gamerman (2004) <doi:10.1191/1471082X04st075oa>. FF do Nascimento, D. Gamerman, HF Lopes <doi:10.1007/s11222-011-9270-z>.",
    "version": "0.0.1",
    "maintainer": "Manuele Leonelli <manuele.leonelli@ie.edu>",
    "author": "Manuele Leonelli [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2562-5192>)",
    "url": "https://github.com/manueleleonelli/extrememix",
    "bug_reports": "https://github.com/manueleleonelli/extrememix/issues",
    "repository": "https://cran.r-project.org/package=extrememix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extrememix Bayesian Estimation of Extreme Value Mixture Models Fits extreme value mixture models, which are models for tails not requiring selection of a threshold, for continuous data. It includes functions for model comparison, estimation of quantity of interest in extreme value analysis and plotting. Reference: CN Behrens, HF Lopes, D Gamerman (2004) <doi:10.1191/1471082X04st075oa>. FF do Nascimento, D. Gamerman, HF Lopes <doi:10.1007/s11222-011-9270-z>.  "
  },
  {
    "id": 12152,
    "package_name": "facmodTS",
    "title": "Time Series Factor Models for Asset Returns",
    "description": "Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.",
    "version": "1.0",
    "maintainer": "Doug Martin <martinrd3d@gmail.com>",
    "author": "Doug Martin [cre, aut],\n  Eric Zivot [aut],\n  Sangeetha Srinivasan [aut],\n  Avinash Acharya [ctb],\n  Yi-An Chen [ctb],\n  Kirk Li [ctb],\n  Lingjie Yi [ctb],\n  Justin Shea [ctb],\n  Mido Shammaa [ctb],\n  Jon Spinney [ctb]",
    "url": "https://github.com/robustport/facmodTS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=facmodTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "facmodTS Time Series Factor Models for Asset Returns Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.  "
  },
  {
    "id": 12193,
    "package_name": "faoutlier",
    "title": "Influential Case Detection Methods for Factor Analysis and\nStructural Equation Models",
    "description": "Tools for detecting and summarize influential cases that\n    can affect exploratory and confirmatory factor analysis models as well as\n    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>; \n    Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <doi:10.3389/fpsyg.2012.00055>).",
    "version": "0.7.7",
    "maintainer": "Phil Chalmers <rphilip.chalmers@gmail.com>",
    "author": "Phil Chalmers [aut, cre]",
    "url": "https://github.com/philchalmers/faoutlier",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=faoutlier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "faoutlier Influential Case Detection Methods for Factor Analysis and\nStructural Equation Models Tools for detecting and summarize influential cases that\n    can affect exploratory and confirmatory factor analysis models as well as\n    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>; \n    Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <doi:10.3389/fpsyg.2012.00055>).  "
  },
  {
    "id": 12225,
    "package_name": "fastVoteR",
    "title": "Efficient Voting Methods for Committee Selection",
    "description": "A fast 'Rcpp'-based implementation of polynomially-computable\n    voting theory methods for committee ranking and scoring. The package\n    includes methods such as Approval Voting (AV), Satisfaction Approval\n    Voting (SAV), sequential Proportional Approval Voting (PAV), and\n    sequential Phragmen's Rule. Weighted variants of these methods are\n    also provided, allowing for differential voter influence.",
    "version": "0.0.1",
    "maintainer": "John Zobolas <bblodfon@gmail.com>",
    "author": "John Zobolas [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-3609-8674>),\n  Anne-Marie George [ctb] (ORCID:\n    <https://orcid.org/0000-0001-9232-8211>)",
    "url": "https://bblodfon.github.io/fastVoteR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fastVoteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastVoteR Efficient Voting Methods for Committee Selection A fast 'Rcpp'-based implementation of polynomially-computable\n    voting theory methods for committee ranking and scoring. The package\n    includes methods such as Approval Voting (AV), Satisfaction Approval\n    Voting (SAV), sequential Proportional Approval Voting (PAV), and\n    sequential Phragmen's Rule. Weighted variants of these methods are\n    also provided, allowing for differential voter influence.  "
  },
  {
    "id": 12263,
    "package_name": "fastrmodels",
    "title": "Models for the 'nflfastR' Package",
    "description": "A data package that hosts all models for the 'nflfastR'\n    package.",
    "version": "2.0.0",
    "maintainer": "Sebastian Carl <mrcaseb@gmail.com>",
    "author": "Sebastian Carl [cre, aut],\n  Ben Baldwin [aut]",
    "url": "https://github.com/nflverse/fastrmodels",
    "bug_reports": "https://github.com/nflverse/fastrmodels/issues",
    "repository": "https://cran.r-project.org/package=fastrmodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastrmodels Models for the 'nflfastR' Package A data package that hosts all models for the 'nflfastR'\n    package.  "
  },
  {
    "id": 12264,
    "package_name": "fastshap",
    "title": "Fast Approximate Shapley Values",
    "description": "Computes fast (relative to other implementations) approximate \n    Shapley values for any supervised learning model. Shapley values help to \n    explain the predictions from any black box model using ideas from game \n    theory; see Strumbel and Kononenko (2014) <doi:10.1007/s10115-013-0679-x> \n    for details.",
    "version": "0.1.1",
    "maintainer": "Brandon Greenwell <greenwell.brandon@gmail.com>",
    "author": "Brandon Greenwell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8120-0084>)",
    "url": "https://github.com/bgreenwell/fastshap,\nhttps://bgreenwell.github.io/fastshap/",
    "bug_reports": "https://github.com/bgreenwell/fastshap/issues",
    "repository": "https://cran.r-project.org/package=fastshap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastshap Fast Approximate Shapley Values Computes fast (relative to other implementations) approximate \n    Shapley values for any supervised learning model. Shapley values help to \n    explain the predictions from any black box model using ideas from game \n    theory; see Strumbel and Kononenko (2014) <doi:10.1007/s10115-013-0679-x> \n    for details.  "
  },
  {
    "id": 12280,
    "package_name": "fbglm",
    "title": "Fractional Binomial Regression Model",
    "description": "Fit a fractional binomial regression model and\n extended zero-inflated negative binomial regression \n model to count data with excess zeros using maximum\n likelihood estimation.\n Compare zero-inflated regression models via Vuong \n closeness test. ",
    "version": "1.5.0",
    "maintainer": "Jeonghwa Lee <leejb@uncw.edu>",
    "author": "Jeonghwa Lee [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2023-144X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fbglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fbglm Fractional Binomial Regression Model Fit a fractional binomial regression model and\n extended zero-inflated negative binomial regression \n model to count data with excess zeros using maximum\n likelihood estimation.\n Compare zero-inflated regression models via Vuong \n closeness test.   "
  },
  {
    "id": 12308,
    "package_name": "fdaconcur",
    "title": "Concurrent Regression and History Index Models for Functional\nData",
    "description": "Provides an implementation of concurrent or varying coefficient regression methods for \n    functional data. The implementations are done for both dense and sparsely observed functional\n    data. Pointwise confidence bands can be constructed for each case. Further, the influence of\n    past predictor values are modeled by a smooth history index function, \n    while the effects on the response are described by smooth varying coefficient functions, \n    which are very useful in analyzing real data such as COVID data.\n    References: Yao, F., M\u00fcller, H.G., Wang, J.L. (2005) <doi:10.1214/009053605000000660>.\n                Sent\u00fcrk, D., M\u00fcller, H.G. (2010) <doi:10.1198/jasa.2010.tm09228>.",
    "version": "0.1.3",
    "maintainer": "Su I Iao <siao@ucdavis.edu>",
    "author": "Su I Iao [aut, cre],\n  Satarupa Bhattacharjee [aut],\n  Yaqing Chen [aut],\n  Changbo Zhu [aut],\n  Han Chen [aut],\n  Yidong Zhou [aut],\n  \u00c1lvaro Gajardo [aut],\n  Poorbita Kundu [aut],\n  Hang Zhou [aut],\n  Hans-Georg M\u00fcller [cph, ths, aut]",
    "url": "https://github.com/functionaldata/tFDAconcur",
    "bug_reports": "https://github.com/functionaldata/tFDAconcur/issues",
    "repository": "https://cran.r-project.org/package=fdaconcur",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdaconcur Concurrent Regression and History Index Models for Functional\nData Provides an implementation of concurrent or varying coefficient regression methods for \n    functional data. The implementations are done for both dense and sparsely observed functional\n    data. Pointwise confidence bands can be constructed for each case. Further, the influence of\n    past predictor values are modeled by a smooth history index function, \n    while the effects on the response are described by smooth varying coefficient functions, \n    which are very useful in analyzing real data such as COVID data.\n    References: Yao, F., M\u00fcller, H.G., Wang, J.L. (2005) <doi:10.1214/009053605000000660>.\n                Sent\u00fcrk, D., M\u00fcller, H.G. (2010) <doi:10.1198/jasa.2010.tm09228>.  "
  },
  {
    "id": 12342,
    "package_name": "fee",
    "title": "Estimate the First-Exposure Effect (FEE) using Count Data Models",
    "description": "Estimates the first-exposure effect (FEE) using a\n    one-inflated positive Poisson model, or a one-inflated zero-truncated\n    negative binomial model. In addition, estimates the marginal FEE, and\n    standard errors for the FEE and marginal FEE.",
    "version": "1.0.0",
    "maintainer": "Ryan T. Godwin <ryan.godwin@umanitoba.ca>",
    "author": "Ryan T. Godwin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fee Estimate the First-Exposure Effect (FEE) using Count Data Models Estimates the first-exposure effect (FEE) using a\n    one-inflated positive Poisson model, or a one-inflated zero-truncated\n    negative binomial model. In addition, estimates the marginal FEE, and\n    standard errors for the FEE and marginal FEE.  "
  },
  {
    "id": 12358,
    "package_name": "ffmanova",
    "title": "Fifty-Fifty MANOVA",
    "description": "General linear modeling with multiple responses (MANCOVA). An overall p-value for each model term is calculated by the 50-50 MANOVA method by Langsrud (2002) <doi:10.1111/1467-9884.00320>, which handles collinear responses. Rotation testing, described by Langsrud (2005) <doi:10.1007/s11222-005-4789-5>, is used to compute adjusted single response p-values according to familywise error rates and false discovery rates (FDR). The approach to FDR is described in the appendix of Moen et al. (2005) <doi:10.1128/AEM.71.4.2086-2094.2005>. Unbalanced designs are handled by Type II sums of squares as argued in Langsrud (2003) <doi:10.1023/A:1023260610025>. Furthermore, the Type II philosophy is extended to continuous design variables as described in Langsrud et al. (2007) <doi:10.1080/02664760701594246>. This means that the method is invariant to scale changes and that common pitfalls are avoided.",
    "version": "1.1.2",
    "maintainer": "\u00d8yvind Langsrud <oyl@ssb.no>",
    "author": "\u00d8yvind Langsrud [aut, cre],\n  Bj\u00f8rn-Helge Mevik [aut]",
    "url": "https://github.com/olangsrud/ffmanova",
    "bug_reports": "https://github.com/olangsrud/ffmanova/issues",
    "repository": "https://cran.r-project.org/package=ffmanova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffmanova Fifty-Fifty MANOVA General linear modeling with multiple responses (MANCOVA). An overall p-value for each model term is calculated by the 50-50 MANOVA method by Langsrud (2002) <doi:10.1111/1467-9884.00320>, which handles collinear responses. Rotation testing, described by Langsrud (2005) <doi:10.1007/s11222-005-4789-5>, is used to compute adjusted single response p-values according to familywise error rates and false discovery rates (FDR). The approach to FDR is described in the appendix of Moen et al. (2005) <doi:10.1128/AEM.71.4.2086-2094.2005>. Unbalanced designs are handled by Type II sums of squares as argued in Langsrud (2003) <doi:10.1023/A:1023260610025>. Furthermore, the Type II philosophy is extended to continuous design variables as described in Langsrud et al. (2007) <doi:10.1080/02664760701594246>. This means that the method is invariant to scale changes and that common pitfalls are avoided.  "
  },
  {
    "id": 12360,
    "package_name": "ffscrapr",
    "title": "API Client for Fantasy Football League Platforms",
    "description": "Helps access various Fantasy Football APIs by handling\n    authentication and rate-limiting, forming appropriate calls, and\n    returning tidy dataframes which can be easily connected to other data\n    sources.",
    "version": "1.4.8",
    "maintainer": "Tan Ho <tan@tanho.ca>",
    "author": "Tan Ho [aut, cre],\n  Tony ElHabr [ctb],\n  Joe Sydlowski [ctb]",
    "url": "https://ffscrapr.ffverse.com, https://github.com/ffverse/ffscrapr,\nhttps://api.myfantasyleague.com/2020/api_info,\nhttps://docs.sleeper.com,\nhttps://www.fleaflicker.com/api-docs/index.html,\nhttps://www.espn.com/fantasy/,\nhttps://www.nflfastr.com/reference/load_player_stats.html",
    "bug_reports": "https://github.com/ffverse/ffscrapr/issues",
    "repository": "https://cran.r-project.org/package=ffscrapr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffscrapr API Client for Fantasy Football League Platforms Helps access various Fantasy Football APIs by handling\n    authentication and rate-limiting, forming appropriate calls, and\n    returning tidy dataframes which can be easily connected to other data\n    sources.  "
  },
  {
    "id": 12460,
    "package_name": "fitzRoy",
    "title": "Easily Scrape and Process AFL Data",
    "description": "An easy package for scraping and processing Australia Rules Football (AFL)\n    data. 'fitzRoy' provides a range of functions for accessing publicly available data \n    from 'AFL Tables' <https://afltables.com/afl/afl_index.html>, 'Footy Wire' <https://www.footywire.com> and\n    'The Squiggle' <https://squiggle.com.au>. Further functions allow for easy processing, \n    cleaning and transformation of this data into formats that can be used for analysis. ",
    "version": "1.6.0",
    "maintainer": "James Day <jamesthomasday@gmail.com>",
    "author": "James Day [cre, aut],\n  Robert Nguyen [aut],\n  Matthew Erbs [ctb],\n  Oscar Lane [aut],\n  Jason Zivkovic [ctb],\n  Jacob Holden [ctb]",
    "url": "https://jimmyday12.github.io/fitzRoy/,\nhttps://github.com/jimmyday12/fitzRoy,\nhttps://github.com/jimmyday12/fitzroy",
    "bug_reports": "https://github.com/jimmyday12/fitzRoy/issues",
    "repository": "https://cran.r-project.org/package=fitzRoy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fitzRoy Easily Scrape and Process AFL Data An easy package for scraping and processing Australia Rules Football (AFL)\n    data. 'fitzRoy' provides a range of functions for accessing publicly available data \n    from 'AFL Tables' <https://afltables.com/afl/afl_index.html>, 'Footy Wire' <https://www.footywire.com> and\n    'The Squiggle' <https://squiggle.com.au>. Further functions allow for easy processing, \n    cleaning and transformation of this data into formats that can be used for analysis.   "
  },
  {
    "id": 12484,
    "package_name": "flassomsm",
    "title": "Penalized Estimation for Multi-State Models with Lasso and Fused\nPenalties",
    "description": "Provides a suite of methods for detecting influential \n    subjects in longitudinal datasets, particularly when observations \n    occur at irregular time points. The methods identify individuals \n    whose response trajectories deviate significantly from the population \n    pattern, enabling detection of anomalies or subjects exerting undue \n    influence on model outcomes.",
    "version": "0.1.0",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "author": "Atanu Bhattacharjee [aut, cre, ctb],\n  Gajendra Kumar Vishwakarma [aut, ctb],\n  Abhipsa Tripathy [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=flassomsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flassomsm Penalized Estimation for Multi-State Models with Lasso and Fused\nPenalties Provides a suite of methods for detecting influential \n    subjects in longitudinal datasets, particularly when observations \n    occur at irregular time points. The methods identify individuals \n    whose response trajectories deviate significantly from the population \n    pattern, enabling detection of anomalies or subjects exerting undue \n    influence on model outcomes.  "
  },
  {
    "id": 12577,
    "package_name": "footBayes",
    "title": "Fitting Bayesian and MLE Football Models",
    "description": "This is the first package allowing for the estimation,\n             visualization and prediction of the most well-known \n             football models: double Poisson, bivariate Poisson,\n             Skellam, student_t, diagonal-inflated bivariate Poisson, and\n             zero-inflated Skellam. It supports both maximum likelihood estimation (MLE, for \n             'static' models only) and Bayesian inference.\n             For Bayesian methods, it incorporates several techniques:\n             MCMC sampling with Hamiltonian Monte Carlo, variational inference using\n             either the Pathfinder algorithm or Automatic Differentiation Variational\n             Inference (ADVI), and the Laplace approximation.\n             The package compiles all the 'CmdStan' models once during installation\n             using the 'instantiate' package.\n             The model construction relies on the most well-known football references, such as \n             Dixon and Coles (1997) <doi:10.1111/1467-9876.00065>,\n             Karlis and Ntzoufras (2003) <doi:10.1111/1467-9884.00366> and\n             Egidi, Pauli and Torelli (2018) <doi:10.1177/1471082X18798414>.",
    "version": "2.0.0",
    "maintainer": "Leonardo Egidi <legidi@units.it>",
    "author": "Leonardo Egidi [aut, cre],\n  Roberto Macr\u00ec Demartino [aut],\n  Vasilis Palaskas. [aut]",
    "url": "https://github.com/leoegidi/footbayes",
    "bug_reports": "https://github.com/leoegidi/footbayes/issues",
    "repository": "https://cran.r-project.org/package=footBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "footBayes Fitting Bayesian and MLE Football Models This is the first package allowing for the estimation,\n             visualization and prediction of the most well-known \n             football models: double Poisson, bivariate Poisson,\n             Skellam, student_t, diagonal-inflated bivariate Poisson, and\n             zero-inflated Skellam. It supports both maximum likelihood estimation (MLE, for \n             'static' models only) and Bayesian inference.\n             For Bayesian methods, it incorporates several techniques:\n             MCMC sampling with Hamiltonian Monte Carlo, variational inference using\n             either the Pathfinder algorithm or Automatic Differentiation Variational\n             Inference (ADVI), and the Laplace approximation.\n             The package compiles all the 'CmdStan' models once during installation\n             using the 'instantiate' package.\n             The model construction relies on the most well-known football references, such as \n             Dixon and Coles (1997) <doi:10.1111/1467-9876.00065>,\n             Karlis and Ntzoufras (2003) <doi:10.1111/1467-9884.00366> and\n             Egidi, Pauli and Torelli (2018) <doi:10.1177/1471082X18798414>.  "
  },
  {
    "id": 12578,
    "package_name": "footballpenaltiesBL",
    "title": "Penalties in the German Men's Football Bundesliga",
    "description": "Basic analysis of all penalties taken in the German men's Bundesliga\n  between the start of its inaugural season and May 2017. The main functions are\n  suitable printing and plotting functions. Flexible selection of a player is\n  supported via grep. Missed penalties can easily be included or excluded, depending\n  on the user's wishes.",
    "version": "1.0.0",
    "maintainer": "Leo N. Geppert <geppert@statistik.uni-dortmund.de>",
    "author": "Leo N. Geppert [aut, cre], Peter Gn\u00e4ndinger [ctb], Katja Ickstadt [ctb],\n  Bj\u00f6rn Bornkamp [ctb], Arno Fritsch [ctb], Oliver Ku\u00df [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=footballpenaltiesBL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "footballpenaltiesBL Penalties in the German Men's Football Bundesliga Basic analysis of all penalties taken in the German men's Bundesliga\n  between the start of its inaugural season and May 2017. The main functions are\n  suitable printing and plotting functions. Flexible selection of a player is\n  supported via grep. Missed penalties can easily be included or excluded, depending\n  on the user's wishes.  "
  },
  {
    "id": 12639,
    "package_name": "fourinarow",
    "title": "Play \"Four in a Row\"",
    "description": "Play or simulate games of \"Four in a Row\" in the R console. This \n    package is designed for educational purposes, encouraging users to write\n    their own functions to play the game automatically. It contains a \n    collection of built-in functions that play the game at various skill levels,\n    for users to test their own functions against.",
    "version": "0.1.1",
    "maintainer": "Kelly Street <street.kelly@gmail.com>",
    "author": "Kelly Street [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6379-5013>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fourinarow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fourinarow Play \"Four in a Row\" Play or simulate games of \"Four in a Row\" in the R console. This \n    package is designed for educational purposes, encouraging users to write\n    their own functions to play the game automatically. It contains a \n    collection of built-in functions that play the game at various skill levels,\n    for users to test their own functions against.  "
  },
  {
    "id": 12718,
    "package_name": "fruclimadapt",
    "title": "Evaluation Tools for Assessing Climate Adaptation of Fruit Tree\nSpecies",
    "description": "Climate is a critical component limiting growing range of plant species, which\n    also determines cultivar adaptation to a region. The evaluation of climate influence on\n    fruit production is critical for decision-making in the design stage of orchards and \n    vineyards and in the evaluation of the potential consequences of future climate. Bio-\n    climatic indices and plant phenology are commonly used to describe the suitability of \n    climate for growing quality fruit and to provide temporal and spatial information about \n    regarding ongoing and future changes. 'fruclimadapt' streamlines the assessment of \n    climate adaptation and the identification of potential risks for grapevines and fruit \n    trees. Procedures in the package allow to i) downscale daily meteorological variables\n    to hourly values (Forster et al (2016) <doi:10.5194/gmd-9-2315-2016>),\n    ii) estimate chilling and forcing heat accumulation (Miranda et al (2019)\n    <https://ec.europa.eu/eip/agriculture/sites/default/files/fg30_mp5_phenology_critical_temperatures.pdf>),\n    iii) estimate plant phenology (Schwartz (2012) <doi:10.1007/978-94-007-6925-0>), iv) \n    calculate bioclimatic indices to evaluate fruit tree and grapevine adaptation (e.g. Badr \n    et al (2017) <doi:10.3354/cr01532>), v) estimate the incidence of weather-related disorders \n    in fruits (e.g. Snyder and de Melo-Abreu (2005, ISBN:92-5-105328-6) and vi)\n    estimate plant water requirements (Allen et al (1998, ISBN:92-5-104219-5)). ",
    "version": "0.4.5",
    "maintainer": "Carlos Miranda <carlos.miranda@unavarra.es>",
    "author": "Carlos Miranda",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fruclimadapt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fruclimadapt Evaluation Tools for Assessing Climate Adaptation of Fruit Tree\nSpecies Climate is a critical component limiting growing range of plant species, which\n    also determines cultivar adaptation to a region. The evaluation of climate influence on\n    fruit production is critical for decision-making in the design stage of orchards and \n    vineyards and in the evaluation of the potential consequences of future climate. Bio-\n    climatic indices and plant phenology are commonly used to describe the suitability of \n    climate for growing quality fruit and to provide temporal and spatial information about \n    regarding ongoing and future changes. 'fruclimadapt' streamlines the assessment of \n    climate adaptation and the identification of potential risks for grapevines and fruit \n    trees. Procedures in the package allow to i) downscale daily meteorological variables\n    to hourly values (Forster et al (2016) <doi:10.5194/gmd-9-2315-2016>),\n    ii) estimate chilling and forcing heat accumulation (Miranda et al (2019)\n    <https://ec.europa.eu/eip/agriculture/sites/default/files/fg30_mp5_phenology_critical_temperatures.pdf>),\n    iii) estimate plant phenology (Schwartz (2012) <doi:10.1007/978-94-007-6925-0>), iv) \n    calculate bioclimatic indices to evaluate fruit tree and grapevine adaptation (e.g. Badr \n    et al (2017) <doi:10.3354/cr01532>), v) estimate the incidence of weather-related disorders \n    in fruits (e.g. Snyder and de Melo-Abreu (2005, ISBN:92-5-105328-6) and vi)\n    estimate plant water requirements (Allen et al (1998, ISBN:92-5-104219-5)).   "
  },
  {
    "id": 12744,
    "package_name": "fugue",
    "title": "Sensitivity Analysis Optimized for Matched Sets of Varied Sizes",
    "description": "As in music, a fugue statistic repeats a theme in small variations.  Here, the psi-function that defines an m-statistic is slightly altered to maintain the same design sensitivity in matched sets of different sizes.  The main functions in the package are sen() and senCI().  For sensitivity analyses for m-statistics, see Rosenbaum (2007) Biometrics 63 456-464 <doi:10.1111/j.1541-0420.2006.00717.x>.",
    "version": "0.1.7",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Xinran Li and Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fugue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fugue Sensitivity Analysis Optimized for Matched Sets of Varied Sizes As in music, a fugue statistic repeats a theme in small variations.  Here, the psi-function that defines an m-statistic is slightly altered to maintain the same design sensitivity in matched sets of different sizes.  The main functions in the package are sen() and senCI().  For sensitivity analyses for m-statistics, see Rosenbaum (2007) Biometrics 63 456-464 <doi:10.1111/j.1541-0420.2006.00717.x>.  "
  },
  {
    "id": 12749,
    "package_name": "fun",
    "title": "Use R for Fun",
    "description": "This is a collection of R games and other funny stuff, such as the\n    classic Mine sweeper and sliding puzzles.",
    "version": "0.3",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\n  Yixuan Qiu [aut],\n  Taiyun Wei [aut]",
    "url": "https://github.com/yihui/fun",
    "bug_reports": "https://github.com/yihui/fun/issues",
    "repository": "https://cran.r-project.org/package=fun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fun Use R for Fun This is a collection of R games and other funny stuff, such as the\n    classic Mine sweeper and sliding puzzles.  "
  },
  {
    "id": 12784,
    "package_name": "fusen",
    "title": "Build a Package from Rmarkdown Files",
    "description": "Use Rmarkdown First method to build your package. Start your\n    package with documentation, functions, examples and tests in the same\n    unique file. Everything can be set from the Rmarkdown template file\n    provided in your project, then inflated as a package. Inflating the\n    template copies the relevant chunks and sections in the appropriate\n    files required for package development.",
    "version": "0.7.2",
    "maintainer": "Vincent Guyader <vincent@thinkr.fr>",
    "author": "Sebastien Rochette [aut] (ORCID:\n    <https://orcid.org/0000-0002-1565-9313>, creator),\n  Vincent Guyader [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0671-9270>),\n  Yohann Mansiaux [aut],\n  ThinkR [cph]",
    "url": "https://thinkr-open.github.io/fusen/,\nhttps://github.com/Thinkr-open/fusen",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fusen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fusen Build a Package from Rmarkdown Files Use Rmarkdown First method to build your package. Start your\n    package with documentation, functions, examples and tests in the same\n    unique file. Everything can be set from the Rmarkdown template file\n    provided in your project, then inflated as a package. Inflating the\n    template copies the relevant chunks and sections in the appropriate\n    files required for package development.  "
  },
  {
    "id": 12863,
    "package_name": "gameR",
    "title": "Color Palettes Inspired by Video Games",
    "description": "Palettes based on video games.",
    "version": "0.0.7",
    "maintainer": "Nathan Constantine-Cooke <nathan.constantine-cooke@ed.ac.uk>",
    "author": "Nathan Constantine-Cooke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4437-8713>),\n  Hugh Warden [ctb] (ORCID: <https://orcid.org/0000-0002-4308-7316>),\n  Sergej Ruff [ctb] (ORCID: <https://orcid.org/0009-0000-8264-6347>)",
    "url": "https://www.constantine-cooke.com/gameR/,\nhttps://github.com/nathansam/gameR/",
    "bug_reports": "https://github.com/nathansam/gameR/issues",
    "repository": "https://cran.r-project.org/package=gameR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gameR Color Palettes Inspired by Video Games Palettes based on video games.  "
  },
  {
    "id": 12864,
    "package_name": "gamesGA",
    "title": "Genetic Algorithm for Sequential Symmetric Games",
    "description": "Finds adaptive strategies for sequential symmetric \n    games using a genetic algorithm. Currently, any symmetric two by two matrix\n    is allowed, and strategies can remember the history of an opponent's play\n    from the previous three rounds of moves in iterated interactions between\n    players. The genetic algorithm returns a list of adaptive strategies given\n    payoffs, and the mean fitness of strategies in each generation.",
    "version": "1.1.3.7",
    "maintainer": "A. Bradley Duthie <brad.duthie@gmail.com>",
    "author": "A. Bradley Duthie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8343-4995>)",
    "url": "https://bradduthie.github.io/gamesGA/",
    "bug_reports": "https://github.com/bradduthie/gamesGA/issues",
    "repository": "https://cran.r-project.org/package=gamesGA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gamesGA Genetic Algorithm for Sequential Symmetric Games Finds adaptive strategies for sequential symmetric \n    games using a genetic algorithm. Currently, any symmetric two by two matrix\n    is allowed, and strategies can remember the history of an opponent's play\n    from the previous three rounds of moves in iterated interactions between\n    players. The genetic algorithm returns a list of adaptive strategies given\n    payoffs, and the mean fitness of strategies in each generation.  "
  },
  {
    "id": 12873,
    "package_name": "gamlss.inf",
    "title": "Fitting Mixed (Inflated and Adjusted) Distributions",
    "description": "This is an add-on package to 'gamlss'. The purpose of this package is to allow users to fit GAMLSS (Generalised Additive Models for Location Scale and Shape) models when the response variable is defined either in the intervals [0,1), (0,1] and [0,1] (inflated at zero and/or one distributions), or in the positive real line including zero (zero-adjusted distributions). The mass points at zero and/or one are treated as extra parameters with the possibility to include a linear predictor for both. The package also allows transformed or truncated distributions from the GAMLSS family to be used for the continuous part of the distribution. Standard methods and GAMLSS diagnostics can be used with the resulting fitted object. ",
    "version": "1.0-2",
    "maintainer": "Marco Enea  <marco.enea@unipa.it>",
    "author": "Marco Enea [aut, cre, cph],\n  Mikis Stasinopoulos [aut],\n  Bob Rigby [aut],\n  Abu Hossain [aut]",
    "url": "https://www.gamlss.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gamlss.inf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gamlss.inf Fitting Mixed (Inflated and Adjusted) Distributions This is an add-on package to 'gamlss'. The purpose of this package is to allow users to fit GAMLSS (Generalised Additive Models for Location Scale and Shape) models when the response variable is defined either in the intervals [0,1), (0,1] and [0,1] (inflated at zero and/or one distributions), or in the positive real line including zero (zero-adjusted distributions). The mass points at zero and/or one are treated as extra parameters with the possibility to include a linear predictor for both. The package also allows transformed or truncated distributions from the GAMLSS family to be used for the continuous part of the distribution. Standard methods and GAMLSS diagnostics can be used with the resulting fitted object.   "
  },
  {
    "id": 12898,
    "package_name": "garma",
    "title": "Fitting and Forecasting Gegenbauer ARMA Time Series Models",
    "description": "Methods for estimating univariate long memory-seasonal/cyclical\n             Gegenbauer time series processes. See for example (2022) <doi:10.1007/s00362-022-01290-3>.\n             Refer to the vignette for details of fitting these processes.",
    "version": "0.9.24",
    "maintainer": "Richard Hunt <maint@huntemail.id.au>",
    "author": "Richard Hunt [aut, cre]",
    "url": "https://github.com/rlph50/garma",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=garma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "garma Fitting and Forecasting Gegenbauer ARMA Time Series Models Methods for estimating univariate long memory-seasonal/cyclical\n             Gegenbauer time series processes. See for example (2022) <doi:10.1007/s00362-022-01290-3>.\n             Refer to the vignette for details of fitting these processes.  "
  },
  {
    "id": 12924,
    "package_name": "gbmt",
    "title": "Group-Based Multivariate Trajectory Modeling",
    "description": "Estimation and analysis of group-based multivariate trajectory models (Nagin, 2018 <doi:10.1177/0962280216673085>; Magrini, 2022 <doi:10.1007/s10182-022-00437-9>). The package implements an Expectation-Maximization (EM) algorithm allowing unbalanced panel and missing values, and provides several functionalities for prediction and graphical representation.",
    "version": "0.1.4",
    "maintainer": "Alessandro Magrini <alessandro.magrini@unifi.it>",
    "author": "Alessandro Magrini [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gbmt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gbmt Group-Based Multivariate Trajectory Modeling Estimation and analysis of group-based multivariate trajectory models (Nagin, 2018 <doi:10.1177/0962280216673085>; Magrini, 2022 <doi:10.1007/s10182-022-00437-9>). The package implements an Expectation-Maximization (EM) algorithm allowing unbalanced panel and missing values, and provides several functionalities for prediction and graphical representation.  "
  },
  {
    "id": 12961,
    "package_name": "geboes.score",
    "title": "Evaluate the Geboes Score for Histology in Ulcerative Colitis",
    "description": "Evaluate and validate the Geboes score for histological assessment\n  of inflammation in ulcerative colitis.  The original Geboes score from Geboes,\n  et al. (2000) <doi:10.1136/gut.47.3.404>, binary version from Li, et al.\n  (2019) <doi:10.1093/ecco-jcc/jjz022>, and continuous version from Magro, et\n  al. (2020) <doi:10.1093/ecco-jcc/jjz123> are all described and implemented.",
    "version": "1.0.0",
    "maintainer": "Bill Denney <wdenney@humanpredictions.com>",
    "author": "Bill Denney [aut, cre] (ORCID: <https://orcid.org/0000-0002-5759-428X>)",
    "url": "https://billdenney.github.io/geboes.score/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geboes.score",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geboes.score Evaluate the Geboes Score for Histology in Ulcerative Colitis Evaluate and validate the Geboes score for histological assessment\n  of inflammation in ulcerative colitis.  The original Geboes score from Geboes,\n  et al. (2000) <doi:10.1136/gut.47.3.404>, binary version from Li, et al.\n  (2019) <doi:10.1093/ecco-jcc/jjz022>, and continuous version from Magro, et\n  al. (2020) <doi:10.1093/ecco-jcc/jjz123> are all described and implemented.  "
  },
  {
    "id": 13010,
    "package_name": "generalRSS",
    "title": "Statistical Tools for Balanced and Unbalanced Ranked Set\nSampling",
    "description": "Ranked Set Sampling (RSS) is a stratified sampling method known for its efficiency compared to Simple Random Sampling (SRS). When sample allocation is equal across strata, it is referred to as balanced RSS (BRSS) whereas unequal allocation is called unbalanced RSS (URSS), which is particularly effective for asymmetric or skewed distributions. This package offers practical statistical tools and sampling methods for both BRSS and URSS, emphasizing flexible sampling designs and inference for population means, medians, proportions, and Area Under the Curve (AUC). It incorporates parametric and nonparametric tests, including empirical likelihood ratio (LR) methods. The package provides ranked set sampling methods from a given population, including sampling with imperfect ranking using auxiliary variables. Furthermore, it provides tools for efficient sample allocation in URSS, ensuring greater efficiency than SRS and BRSS. For more details, refer e.g. to Chen et al. (2003) <doi:10.1007/978-0-387-21664-5>, Ahn et al. (2022) <doi:10.1007/978-3-031-14525-4_3>, and Ahn et al. (2024) <doi:10.1111/insr.12589>.",
    "version": "0.1.3",
    "maintainer": "Soohyun Ahn <shahn@ajou.ac.kr>",
    "author": "Soohyun Ahn [aut, cre],\n  Chul Moon [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=generalRSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "generalRSS Statistical Tools for Balanced and Unbalanced Ranked Set\nSampling Ranked Set Sampling (RSS) is a stratified sampling method known for its efficiency compared to Simple Random Sampling (SRS). When sample allocation is equal across strata, it is referred to as balanced RSS (BRSS) whereas unequal allocation is called unbalanced RSS (URSS), which is particularly effective for asymmetric or skewed distributions. This package offers practical statistical tools and sampling methods for both BRSS and URSS, emphasizing flexible sampling designs and inference for population means, medians, proportions, and Area Under the Curve (AUC). It incorporates parametric and nonparametric tests, including empirical likelihood ratio (LR) methods. The package provides ranked set sampling methods from a given population, including sampling with imperfect ranking using auxiliary variables. Furthermore, it provides tools for efficient sample allocation in URSS, ensuring greater efficiency than SRS and BRSS. For more details, refer e.g. to Chen et al. (2003) <doi:10.1007/978-0-387-21664-5>, Ahn et al. (2022) <doi:10.1007/978-3-031-14525-4_3>, and Ahn et al. (2024) <doi:10.1111/insr.12589>.  "
  },
  {
    "id": 13019,
    "package_name": "geneviewer",
    "title": "Gene Cluster Visualizations",
    "description": "Provides tools for plotting gene clusters and transcripts by \n  importing data from GenBank, FASTA, and GFF files. It performs BLASTP and \n  MUMmer alignments [Altschul et al. (1990) <doi:10.1016/S0022-2836(05)80360-2>;\n  Delcher et al. (1999) <doi:10.1093/nar/27.11.2369>] and displays results on \n  gene arrow maps. Extensive customization options are available, including \n  legends, labels, annotations, scales, colors, tooltips, and more.",
    "version": "0.1.11",
    "maintainer": "Niels van der Velden <n.s.j.vandervelden@gmail.com>",
    "author": "Niels van der Velden [aut, cre]",
    "url": "https://github.com/nvelden/geneviewer",
    "bug_reports": "https://github.com/nvelden/geneviewer/issues",
    "repository": "https://cran.r-project.org/package=geneviewer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geneviewer Gene Cluster Visualizations Provides tools for plotting gene clusters and transcripts by \n  importing data from GenBank, FASTA, and GFF files. It performs BLASTP and \n  MUMmer alignments [Altschul et al. (1990) <doi:10.1016/S0022-2836(05)80360-2>;\n  Delcher et al. (1999) <doi:10.1093/nar/27.11.2369>] and displays results on \n  gene arrow maps. Extensive customization options are available, including \n  legends, labels, annotations, scales, colors, tooltips, and more.  "
  },
  {
    "id": 13039,
    "package_name": "genstab",
    "title": "Resampling Based Yield Stability Analyses",
    "description": "Several yield stability analyses are mentioned in this package: variation and regression based yield stability analyses. Resampling techniques are integrated with these stability analyses. The function stab.mean() provides the genotypic means and ranks including their corresponding confidence intervals. The function stab.var() provides the genotypic variances over environments including their corresponding confidence intervals. The function stab.fw() is an extended method from the Finlay-Wilkinson method (1963). This method can include several other factors that might impact yield stability. Resampling technique is integrated into this method. A few missing data points or unbalanced data are allowed too. The function stab.fw.check() is an extended method from the Finlay-Wilkinson method (1963). The yield stability is evaluated via common check line(s). Resampling technique is integrated.",
    "version": "1.0.0",
    "maintainer": "Jixiang Wu <jixiang.wu@sdstate.edu>",
    "author": "Jixiang Wu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=genstab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genstab Resampling Based Yield Stability Analyses Several yield stability analyses are mentioned in this package: variation and regression based yield stability analyses. Resampling techniques are integrated with these stability analyses. The function stab.mean() provides the genotypic means and ranks including their corresponding confidence intervals. The function stab.var() provides the genotypic variances over environments including their corresponding confidence intervals. The function stab.fw() is an extended method from the Finlay-Wilkinson method (1963). This method can include several other factors that might impact yield stability. Resampling technique is integrated into this method. A few missing data points or unbalanced data are allowed too. The function stab.fw.check() is an extended method from the Finlay-Wilkinson method (1963). The yield stability is evaluated via common check line(s). Resampling technique is integrated.  "
  },
  {
    "id": 13089,
    "package_name": "geopsych",
    "title": "Methods of Applied Psychology and Psychometrics in Geographical\nAnalysis",
    "description": "Integrating applied psychological and psychometric methods into geographical analysis. With the emergence of geo-referenced questionnaires, spatially explicit psychological and psychometric methods can offer a geographically contextualised approach that reflects latent traits and processes at a more local scale, leading to more tailored research and decision-making processes. The implemented methods include Geographically Weighted Cronbach's alpha and its bandwidth selection. See Zhang & Li (2025) <doi:10.1111/gean.70021>.",
    "version": "0.1.0",
    "maintainer": "Sui Zhang <sui.zhang@manchester.ac.uk>",
    "author": "Sui Zhang [aut, cre] (ORCID: <https://orcid.org/0000-0002-4143-2331>)",
    "url": "https://github.com/ZhangSui921/geopsych",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geopsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geopsych Methods of Applied Psychology and Psychometrics in Geographical\nAnalysis Integrating applied psychological and psychometric methods into geographical analysis. With the emergence of geo-referenced questionnaires, spatially explicit psychological and psychometric methods can offer a geographically contextualised approach that reflects latent traits and processes at a more local scale, leading to more tailored research and decision-making processes. The implemented methods include Geographically Weighted Cronbach's alpha and its bandwidth selection. See Zhang & Li (2025) <doi:10.1111/gean.70021>.  "
  },
  {
    "id": 13130,
    "package_name": "getspres",
    "title": "SPRE Statistics for Exploring Heterogeneity in Meta-Analysis",
    "description": "An implementation of SPRE (standardised predicted random-effects)\n    statistics in R to explore heterogeneity in genetic association meta-\n    analyses, as described by Magosi et al. (2019) \n    <doi:10.1093/bioinformatics/btz590>. SPRE statistics are precision \n    weighted residuals that indicate the direction and extent with which \n    individual study-effects in a meta-analysis deviate from the average \n    genetic effect. Overly influential positive outliers have the potential \n    to inflate average genetic effects in a meta-analysis whilst negative \n    outliers might lower or change the direction of effect. See the 'getspres' \n    website for documentation and examples \n    <https://magosil86.github.io/getspres/>.",
    "version": "0.2.0",
    "maintainer": "Lerato E Magosi <magosil86@gmail.com>",
    "author": "Lerato E Magosi [aut],\n  Jemma C Hopewell [aut],\n  Martin Farrall [aut],\n  Lerato E Magosi [cre]",
    "url": "https://magosil86.github.io/getspres/",
    "bug_reports": "https://github.com/magosil86/getspres/issues",
    "repository": "https://cran.r-project.org/package=getspres",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "getspres SPRE Statistics for Exploring Heterogeneity in Meta-Analysis An implementation of SPRE (standardised predicted random-effects)\n    statistics in R to explore heterogeneity in genetic association meta-\n    analyses, as described by Magosi et al. (2019) \n    <doi:10.1093/bioinformatics/btz590>. SPRE statistics are precision \n    weighted residuals that indicate the direction and extent with which \n    individual study-effects in a meta-analysis deviate from the average \n    genetic effect. Overly influential positive outliers have the potential \n    to inflate average genetic effects in a meta-analysis whilst negative \n    outliers might lower or change the direction of effect. See the 'getspres' \n    website for documentation and examples \n    <https://magosil86.github.io/getspres/>.  "
  },
  {
    "id": 13164,
    "package_name": "ggalluvial",
    "title": "Alluvial Plots in 'ggplot2'",
    "description": "Alluvial plots use variable-width ribbons and stacked bar plots to\n    represent multi-dimensional or repeated-measures data with categorical or\n    ordinal variables; see Riehmann, Hanfler, and Froehlich (2005)\n    <doi:10.1109/INFVIS.2005.1532152> and Rosvall and Bergstrom (2010)\n    <doi:10.1371/journal.pone.0008694>.\n    Alluvial plots are statistical graphics in the sense of Wilkinson (2006)\n    <doi:10.1007/0-387-28695-0>; they share elements with Sankey diagrams and\n    parallel sets plots but are uniquely determined from the data and a small\n    set of parameters. This package extends Wickham's (2010)\n    <doi:10.1198/jcgs.2009.07098> layered grammar of graphics to generate\n    alluvial plots from tidy data.",
    "version": "0.12.5",
    "maintainer": "Jason Cory Brunson <cornelioid@gmail.com>",
    "author": "Jason Cory Brunson [aut, cre],\n  Quentin D. Read [aut]",
    "url": "http://corybrunson.github.io/ggalluvial/",
    "bug_reports": "https://github.com/corybrunson/ggalluvial/issues",
    "repository": "https://cran.r-project.org/package=ggalluvial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggalluvial Alluvial Plots in 'ggplot2' Alluvial plots use variable-width ribbons and stacked bar plots to\n    represent multi-dimensional or repeated-measures data with categorical or\n    ordinal variables; see Riehmann, Hanfler, and Froehlich (2005)\n    <doi:10.1109/INFVIS.2005.1532152> and Rosvall and Bergstrom (2010)\n    <doi:10.1371/journal.pone.0008694>.\n    Alluvial plots are statistical graphics in the sense of Wilkinson (2006)\n    <doi:10.1007/0-387-28695-0>; they share elements with Sankey diagrams and\n    parallel sets plots but are uniquely determined from the data and a small\n    set of parameters. This package extends Wickham's (2010)\n    <doi:10.1198/jcgs.2009.07098> layered grammar of graphics to generate\n    alluvial plots from tidy data.  "
  },
  {
    "id": 13199,
    "package_name": "ggdmcModel",
    "title": "Model Builders for 'ggdmc' Package",
    "description": "A suite of tools for specifying and examining experimental designs related to choice response time models (e.g., the Diffusion Decision Model). This package allows users to define how experimental factors influence one or more model parameters using R-style formula syntax, while also checking the logical consistency of these associations. Additionally, it integrates with the 'ggdmc' package, which employs Differential Evolution Markov Chain Monte Carlo (DE-MCMC) sampling to optimise model parameters. For further details on the model-building approach, see Heathcote, Lin, Reynolds, Strickland, Gretton, and Matzke (2019) <doi:10.3758/s13428-018-1067-y>.",
    "version": "0.2.9.0",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "author": "Yi-Shin Lin [aut, cre]",
    "url": "https://github.com/yxlin/ggdmcModel",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggdmcModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggdmcModel Model Builders for 'ggdmc' Package A suite of tools for specifying and examining experimental designs related to choice response time models (e.g., the Diffusion Decision Model). This package allows users to define how experimental factors influence one or more model parameters using R-style formula syntax, while also checking the logical consistency of these associations. Additionally, it integrates with the 'ggdmc' package, which employs Differential Evolution Markov Chain Monte Carlo (DE-MCMC) sampling to optimise model parameters. For further details on the model-building approach, see Heathcote, Lin, Reynolds, Strickland, Gretton, and Matzke (2019) <doi:10.3758/s13428-018-1067-y>.  "
  },
  {
    "id": 13211,
    "package_name": "ggfootball",
    "title": "Plotting Football Matches Expected Goals (xG) Stats with\n'Understat' Data",
    "description": "Scrapes football match shots data from 'Understat' <https://understat.com/> and visualizes it using interactive plots:\n    - A detailed shot map displaying the location, type, and xG value of shots taken by both teams.\n    - An xG timeline chart showing the cumulative xG for each team over time, annotated with the details of scored goals.",
    "version": "0.2.1",
    "maintainer": "Aymen Nasri <aymennasrii@proton.me>",
    "author": "Aymen Nasri [aut, cre, cph]",
    "url": "http://aymennasri.me/ggfootball/",
    "bug_reports": "https://github.com/aymennasri/ggfootball/issues",
    "repository": "https://cran.r-project.org/package=ggfootball",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggfootball Plotting Football Matches Expected Goals (xG) Stats with\n'Understat' Data Scrapes football match shots data from 'Understat' <https://understat.com/> and visualizes it using interactive plots:\n    - A detailed shot map displaying the location, type, and xG value of shots taken by both teams.\n    - An xG timeline chart showing the cumulative xG for each team over time, annotated with the details of scored goals.  "
  },
  {
    "id": 13320,
    "package_name": "ggsoccer",
    "title": "Plot Soccer Event Data",
    "description": "The 'ggplot2' package provides a powerful set of tools\n  for visualising and investigating data. The 'ggsoccer' package provides a\n  set of functions for elegantly displaying and exploring soccer event data\n  with 'ggplot2'. Providing extensible layers and themes, it is designed to\n  work smoothly with a variety of popular sports data providers.",
    "version": "0.2.0",
    "maintainer": "Ben Torvaney <torvaney@protonmail.com>",
    "author": "Ben Torvaney [aut, cre]",
    "url": "https://torvaney.github.io/ggsoccer/,\nhttps://github.com/Torvaney/ggsoccer",
    "bug_reports": "https://github.com/Torvaney/ggsoccer/issues",
    "repository": "https://cran.r-project.org/package=ggsoccer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggsoccer Plot Soccer Event Data The 'ggplot2' package provides a powerful set of tools\n  for visualising and investigating data. The 'ggsoccer' package provides a\n  set of functions for elegantly displaying and exploring soccer event data\n  with 'ggplot2'. Providing extensible layers and themes, it is designed to\n  work smoothly with a variety of popular sports data providers.  "
  },
  {
    "id": 13432,
    "package_name": "glmmTMB",
    "title": "Generalized Linear Mixed Models using Template Model Builder",
    "description": "Fit linear and generalized linear mixed models with various\n    extensions, including zero-inflation. The models are fitted using maximum\n    likelihood estimation via 'TMB' (Template Model Builder). Random effects are\n    assumed to be Gaussian on the scale of the linear predictor and are integrated\n    out using the Laplace approximation. Gradients are calculated using automatic\n    differentiation.",
    "version": "1.1.13",
    "maintainer": "Mollie Brooks <mollieebrooks@gmail.com>",
    "author": "Mollie Brooks [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6963-8326>),\n  Ben Bolker [aut] (ORCID: <https://orcid.org/0000-0002-2127-0443>),\n  Kasper Kristensen [aut],\n  Martin Maechler [aut] (ORCID: <https://orcid.org/0000-0002-8685-9910>),\n  Arni Magnusson [aut] (ORCID: <https://orcid.org/0000-0003-2769-6741>),\n  Maeve McGillycuddy [ctb],\n  Hans Skaug [aut] (ORCID: <https://orcid.org/0000-0003-4235-2592>),\n  Anders Nielsen [aut] (ORCID: <https://orcid.org/0000-0001-9683-9262>),\n  Casper Berg [aut] (ORCID: <https://orcid.org/0000-0002-3812-5269>),\n  Koen van Bentham [aut],\n  Nafis Sadat [ctb] (ORCID: <https://orcid.org/0000-0001-5715-616X>),\n  Daniel L\u00fcdecke [ctb] (ORCID: <https://orcid.org/0000-0002-8895-3206>),\n  Russ Lenth [ctb],\n  Joseph O'Brien [ctb] (ORCID: <https://orcid.org/0000-0001-9851-5077>),\n  Charles J. Geyer [ctb],\n  Mikael Jagan [ctb] (ORCID: <https://orcid.org/0000-0002-3542-2938>),\n  Brenton Wiernik [ctb] (ORCID: <https://orcid.org/0000-0001-9560-6336>),\n  Daniel B. Stouffer [ctb] (ORCID:\n    <https://orcid.org/0000-0001-9436-9674>),\n  Michael Agronah [ctb] (ORCID: <https://orcid.org/0009-0007-2655-4094>),\n  Hatice T\u00fcl K\u00fcbra Akdur [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2144-0518>),\n  Daniel Saban\u00e9s Bov\u00e9 [ctb] (ORCID:\n    <https://orcid.org/0000-0002-0176-9239>),\n  Nikolas Krieger [ctb] (ORCID: <https://orcid.org/0000-0002-4581-3545>)",
    "url": "https://github.com/glmmTMB/glmmTMB",
    "bug_reports": "https://github.com/glmmTMB/glmmTMB/issues",
    "repository": "https://cran.r-project.org/package=glmmTMB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glmmTMB Generalized Linear Mixed Models using Template Model Builder Fit linear and generalized linear mixed models with various\n    extensions, including zero-inflation. The models are fitted using maximum\n    likelihood estimation via 'TMB' (Template Model Builder). Random effects are\n    assumed to be Gaussian on the scale of the linear predictor and are integrated\n    out using the Laplace approximation. Gradients are calculated using automatic\n    differentiation.  "
  },
  {
    "id": 13446,
    "package_name": "glmtoolbox",
    "title": "Set of Tools to Data Analysis using Generalized Linear Models",
    "description": "Set of tools for the statistical analysis of data using: (1) normal linear models; (2) generalized linear models; (3) negative binomial regression models as alternative to the Poisson regression models under the presence of overdispersion; (4) beta-binomial and random-clumped binomial regression models as alternative to the binomial regression models under the presence of overdispersion; (5) Zero-inflated and zero-altered regression models to deal with zero-excess in count data; (6) generalized nonlinear models; (7) generalized estimating equations for cluster correlated data.",
    "version": "0.1.12",
    "maintainer": "Luis Hernando Vanegas <lhvanegasp@unal.edu.co>",
    "author": "Luis Hernando Vanegas [aut, cre],\n  Luz Marina Rond\u00f3n [aut],\n  Gilberto A. Paula [aut]",
    "url": "https://mlgs.netlify.app/",
    "bug_reports": "https://github.com/lhvanegasp/glmtoolbox/issues",
    "repository": "https://cran.r-project.org/package=glmtoolbox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glmtoolbox Set of Tools to Data Analysis using Generalized Linear Models Set of tools for the statistical analysis of data using: (1) normal linear models; (2) generalized linear models; (3) negative binomial regression models as alternative to the Poisson regression models under the presence of overdispersion; (4) beta-binomial and random-clumped binomial regression models as alternative to the binomial regression models under the presence of overdispersion; (5) Zero-inflated and zero-altered regression models to deal with zero-excess in count data; (6) generalized nonlinear models; (7) generalized estimating equations for cluster correlated data.  "
  },
  {
    "id": 13504,
    "package_name": "gnrprod",
    "title": "Estimates Gross Output Functions",
    "description": "Estimation of gross output production functions and productivity in the presence of numerous fixed (nonflexible) and a single flexible input using the nonparametric identification strategy specified in Gandhi, Navarro, and Rivers (2020) <doi:10.1086/707736>. Monte Carlo evidence from the paper demonstrates high performance in estimating production function elasticities.",
    "version": "1.1.2",
    "maintainer": "David J. Jin <jindavid@sas.upenn.edu>",
    "author": "David J. Jin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gnrprod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gnrprod Estimates Gross Output Functions Estimation of gross output production functions and productivity in the presence of numerous fixed (nonflexible) and a single flexible input using the nonparametric identification strategy specified in Gandhi, Navarro, and Rivers (2020) <doi:10.1086/707736>. Monte Carlo evidence from the paper demonstrates high performance in estimating production function elasticities.  "
  },
  {
    "id": 13527,
    "package_name": "gomms",
    "title": "GLM-Based Ordination Method",
    "description": "A zero-inflated quasi-Poisson factor model to display similarity between samples visually in a low (2 or 3) dimensional space.",
    "version": "1.0",
    "maintainer": "Michael B. Sohn <msohn@mail.med.upenn.edu>",
    "author": "Michael B. Sohn",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gomms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gomms GLM-Based Ordination Method A zero-inflated quasi-Poisson factor model to display similarity between samples visually in a low (2 or 3) dimensional space.  "
  },
  {
    "id": 13555,
    "package_name": "goweragreement",
    "title": "Bayesian Gower Agreement for Categorical Data",
    "description": "Provides tools for applying the Bayesian Gower agreement methodology (presented in the package vignette) to nominal or ordinal data. The framework can accommodate any number of units, any number of coders, and missingness; and can handle both one-way and two-way random study designs. Influential units and/or coders can be identified easily using leave-one-out statistics.",
    "version": "1.0-1",
    "maintainer": "John Hughes <drjphughesjr@gmail.com>",
    "author": "John Hughes [aut, cre]",
    "url": "http://www.johnhughes.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=goweragreement",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "goweragreement Bayesian Gower Agreement for Categorical Data Provides tools for applying the Bayesian Gower agreement methodology (presented in the package vignette) to nominal or ordinal data. The framework can accommodate any number of units, any number of coders, and missingness; and can handle both one-way and two-way random study designs. Influential units and/or coders can be identified easily using leave-one-out statistics.  "
  },
  {
    "id": 13592,
    "package_name": "granova",
    "title": "Graphical Analysis of Variance",
    "description": "This small collection of functions provides what we call elemental graphics for display of analysis of variance\n    results, David C. Hoaglin, Frederick Mosteller and John W. Tukey (1991, ISBN:978-0-471-52735-0), Paul R. Rosenbaum (1989) \n    <doi:10.2307/2684513>, Robert M. Pruzek and James E. Helmreich <https://jse.amstat.org/v17n1/helmreich.html>. \n    The term elemental derives from the fact that each function is aimed at construction of\n    graphical displays that afford direct visualizations of data with respect to the fundamental \n    questions that drive the particular analysis of variance methods. These functions can be \n    particularly helpful for students and non-statistician analysts. But these methods should be\n    quite generally helpful for work-a-day applications of all kinds, as they can help to identify\n    outliers, clusters or patterns, as well as highlight the role of non-linear transformations of data.",
    "version": "2.3",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Frederic Bertrand [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0837-8281>),\n  Robert M. Pruzek [aut],\n  James E. Helmreich [aut]",
    "url": "https://fbertran.github.io/granova/,\nhttps://github.com/fbertran/granova/",
    "bug_reports": "https://github.com/fbertran/granova/issues/",
    "repository": "https://cran.r-project.org/package=granova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "granova Graphical Analysis of Variance This small collection of functions provides what we call elemental graphics for display of analysis of variance\n    results, David C. Hoaglin, Frederick Mosteller and John W. Tukey (1991, ISBN:978-0-471-52735-0), Paul R. Rosenbaum (1989) \n    <doi:10.2307/2684513>, Robert M. Pruzek and James E. Helmreich <https://jse.amstat.org/v17n1/helmreich.html>. \n    The term elemental derives from the fact that each function is aimed at construction of\n    graphical displays that afford direct visualizations of data with respect to the fundamental \n    questions that drive the particular analysis of variance methods. These functions can be \n    particularly helpful for students and non-statistician analysts. But these methods should be\n    quite generally helpful for work-a-day applications of all kinds, as they can help to identify\n    outliers, clusters or patterns, as well as highlight the role of non-linear transformations of data.  "
  },
  {
    "id": 13598,
    "package_name": "graph4lg",
    "title": "Build Graphs for Landscape Genetics Analysis",
    "description": "Build graphs for landscape genetics analysis. This set of \n\tfunctions can be used to import and convert spatial and genetic data \n\tinitially in different formats, import landscape graphs created with \n\t'GRAPHAB' software (Foltete et al., 2012) <doi:10.1016/j.envsoft.2012.07.002>, \n\tmake diagnosis plots of isolation by distance relationships in order to \n\tchoose how to build genetic graphs, create graphs with a large range of \n\tpruning methods, weight their links with several genetic distances, plot \n\tand analyse graphs,\tcompare them with other graphs. It uses functions from \n\tother packages such as 'adegenet' \n\t(Jombart, 2008) <doi:10.1093/bioinformatics/btn129> and 'igraph' (Csardi\n\tet Nepusz, 2006) <https://igraph.org/>. It also implements methods \n\tcommonly used in landscape genetics to create graphs, described by Dyer et \n\tNason (2004) <doi:10.1111/j.1365-294X.2004.02177.x> and Greenbaum et \n\tFefferman (2017) <doi:10.1111/mec.14059>, and to analyse distance data \n\t(van Strien et al., 2015) <doi:10.1038/hdy.2014.62>.",
    "version": "1.8.0",
    "maintainer": "Paul Savary <psavary@protonmail.com>",
    "author": "Paul Savary [aut, cre] (ORCID: <https://orcid.org/0000-0002-2104-9941>),\n  Gilles Vuidel [ctb] (ORCID: <https://orcid.org/0000-0001-6330-6136>),\n  Tyler Rudolph [ctb],\n  Alexandrine Daniel [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=graph4lg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "graph4lg Build Graphs for Landscape Genetics Analysis Build graphs for landscape genetics analysis. This set of \n\tfunctions can be used to import and convert spatial and genetic data \n\tinitially in different formats, import landscape graphs created with \n\t'GRAPHAB' software (Foltete et al., 2012) <doi:10.1016/j.envsoft.2012.07.002>, \n\tmake diagnosis plots of isolation by distance relationships in order to \n\tchoose how to build genetic graphs, create graphs with a large range of \n\tpruning methods, weight their links with several genetic distances, plot \n\tand analyse graphs,\tcompare them with other graphs. It uses functions from \n\tother packages such as 'adegenet' \n\t(Jombart, 2008) <doi:10.1093/bioinformatics/btn129> and 'igraph' (Csardi\n\tet Nepusz, 2006) <https://igraph.org/>. It also implements methods \n\tcommonly used in landscape genetics to create graphs, described by Dyer et \n\tNason (2004) <doi:10.1111/j.1365-294X.2004.02177.x> and Greenbaum et \n\tFefferman (2017) <doi:10.1111/mec.14059>, and to analyse distance data \n\t(van Strien et al., 2015) <doi:10.1038/hdy.2014.62>.  "
  },
  {
    "id": 13619,
    "package_name": "grattanInflators",
    "title": "Inflators for Australian Policy Analysis",
    "description": "Using Australian Bureau of Statistics indices, provides functions\n    that convert historical, nominal statistics to real, contemporary values \n    without worrying about date input quality, performance, or the ABS catalogue.",
    "version": "0.5.4",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=grattanInflators",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grattanInflators Inflators for Australian Policy Analysis Using Australian Bureau of Statistics indices, provides functions\n    that convert historical, nominal statistics to real, contemporary values \n    without worrying about date input quality, performance, or the ABS catalogue.  "
  },
  {
    "id": 13675,
    "package_name": "growR",
    "title": "Implementation of the Vegetation Model ModVege",
    "description": "Run grass growth simulations using a grass growth model based \n    on ModVege (Jouven, M., P. Carr\u00e8re, and R. Baumont \"Model Predicting \n    Dynamics of Biomass, Structure and Digestibility of Herbage in Managed \n    Permanent Pastures. 1. Model Description.\" (2006) \n    <doi:10.1111/j.1365-2494.2006.00515.x>). The implementation in\n    this package contains a few additions to the above cited version of ModVege,\n    such as simulations of management decisions, and influences of snow cover.\n    As such, the model is fit to simulate grass growth in mountainous \n    regions, such as the Swiss Alps. The package also contains routines for \n    calibrating the model and helpful tools for analysing model outputs and \n    performance.",
    "version": "1.3.0",
    "maintainer": "Kevin Kramer <kevin.pasqual.kramer@protonmail.ch>",
    "author": "Kevin Kramer [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5523-6924>)",
    "url": "https://github.com/kuadrat/growR, https://kuadrat.github.io/growR/",
    "bug_reports": "https://github.com/kuadrat/growR/issues",
    "repository": "https://cran.r-project.org/package=growR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "growR Implementation of the Vegetation Model ModVege Run grass growth simulations using a grass growth model based \n    on ModVege (Jouven, M., P. Carr\u00e8re, and R. Baumont \"Model Predicting \n    Dynamics of Biomass, Structure and Digestibility of Herbage in Managed \n    Permanent Pastures. 1. Model Description.\" (2006) \n    <doi:10.1111/j.1365-2494.2006.00515.x>). The implementation in\n    this package contains a few additions to the above cited version of ModVege,\n    such as simulations of management decisions, and influences of snow cover.\n    As such, the model is fit to simulate grass growth in mountainous \n    regions, such as the Swiss Alps. The package also contains routines for \n    calibrating the model and helpful tools for analysing model outputs and \n    performance.  "
  },
  {
    "id": 13680,
    "package_name": "growthcurver",
    "title": "Simple Metrics to Summarize Growth Curves",
    "description": "Fits the logistic equation to\n    microbial growth curve data (e.g., repeated absorbance measurements\n    taken from a plate reader over time). From this fit, a variety of\n    metrics are provided, including the maximum growth rate,\n    the doubling time, the carrying capacity, the area under the logistic\n    curve, and the time to the inflection point. Method described in \n    Sprouffske and Wagner (2016) <doi:10.1186/s12859-016-1016-7>.",
    "version": "0.3.1",
    "maintainer": "Kathleen Sprouffske <sprouffske@gmail.com>",
    "author": "Kathleen Sprouffske [aut, cre]",
    "url": "https://github.com/sprouffske/growthcurver",
    "bug_reports": "https://github.com/sprouffske/growthcurver/issues",
    "repository": "https://cran.r-project.org/package=growthcurver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "growthcurver Simple Metrics to Summarize Growth Curves Fits the logistic equation to\n    microbial growth curve data (e.g., repeated absorbance measurements\n    taken from a plate reader over time). From this fit, a variety of\n    metrics are provided, including the maximum growth rate,\n    the doubling time, the carrying capacity, the area under the logistic\n    curve, and the time to the inflection point. Method described in \n    Sprouffske and Wagner (2016) <doi:10.1186/s12859-016-1016-7>.  "
  },
  {
    "id": 13708,
    "package_name": "gsisdecoder",
    "title": "High Efficient Functions to Decode NFL Player IDs",
    "description": "A set of high efficient functions to decode identifiers of National \n  Football League players.",
    "version": "0.0.1",
    "maintainer": "Sebastian Carl <mrcaseb@gmail.com>",
    "author": "Sebastian Carl [aut, cre]",
    "url": "https://github.com/mrcaseb/gsisdecoder",
    "bug_reports": "https://github.com/mrcaseb/gsisdecoder/issues",
    "repository": "https://cran.r-project.org/package=gsisdecoder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gsisdecoder High Efficient Functions to Decode NFL Player IDs A set of high efficient functions to decode identifiers of National \n  Football League players.  "
  },
  {
    "id": 13761,
    "package_name": "gwbr",
    "title": "Local and Global Beta Regression",
    "description": "Fit a regression model for when the response variable is presented as a ratio or proportion. This adjustment can occur globally, with the same estimate for the entire study space, or locally, where a beta regression model is fitted for each region, considering only influential locations for that area. Da Silva, A. R. and Lima, A. O. (2017) <doi:10.1016/j.spasta.2017.07.011>.",
    "version": "1.0.5",
    "maintainer": "Roberto Marques <robertomarques_23@yahoo.com.br>",
    "author": "Roberto Marques [aut, cre],\n  Alan da Silva [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gwbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gwbr Local and Global Beta Regression Fit a regression model for when the response variable is presented as a ratio or proportion. This adjustment can occur globally, with the same estimate for the entire study space, or locally, where a beta regression model is fitted for each region, considering only influential locations for that area. Da Silva, A. R. and Lima, A. O. (2017) <doi:10.1016/j.spasta.2017.07.011>.  "
  },
  {
    "id": 13766,
    "package_name": "gwzinbr",
    "title": "Geographically Weighted Zero Inflated Negative Binomial\nRegression",
    "description": "Fits a geographically weighted regression model\n\tusing zero inflated probability distributions. Has the zero \n\tinflated negative binomial distribution (zinb) as default, \n\tbut also accepts the zero inflated Poisson (zip), negative\n\tbinomial (negbin) and Poisson distributions. Can also fit \n\tthe global versions of each regression model.\n\tDa Silva, A. R. & De Sousa, M. D. R. (2023). \"Geographically weighted zero-inflated negative binomial regression: A general case for count data\", Spatial Statistics <doi:10.1016/j.spasta.2023.100790>.\n\tBrunsdon, C., Fotheringham, A. S., & Charlton, M. E. (1996). \"Geographically weighted regression: a method for exploring spatial nonstationarity\", Geographical Analysis, <doi:10.1111/j.1538-4632.1996.tb00936.x>.\n\tYau, K. K. W., Wang, K., & Lee, A. H. (2003). \"Zero-inflated negative binomial mixed regression modeling of over-dispersed count data with extra zeros\", Biometrical Journal, <doi:10.1002/bimj.200390024>.",
    "version": "0.1.0",
    "maintainer": "J\u00e9ssica Vasconcelos <jehh.vasconcelosabreu@gmail.com>",
    "author": "J\u00e9ssica Vasconcelos [aut, cre],\n  Juliana Rosa [aut],\n  Alan da Silva [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gwzinbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gwzinbr Geographically Weighted Zero Inflated Negative Binomial\nRegression Fits a geographically weighted regression model\n\tusing zero inflated probability distributions. Has the zero \n\tinflated negative binomial distribution (zinb) as default, \n\tbut also accepts the zero inflated Poisson (zip), negative\n\tbinomial (negbin) and Poisson distributions. Can also fit \n\tthe global versions of each regression model.\n\tDa Silva, A. R. & De Sousa, M. D. R. (2023). \"Geographically weighted zero-inflated negative binomial regression: A general case for count data\", Spatial Statistics <doi:10.1016/j.spasta.2023.100790>.\n\tBrunsdon, C., Fotheringham, A. S., & Charlton, M. E. (1996). \"Geographically weighted regression: a method for exploring spatial nonstationarity\", Geographical Analysis, <doi:10.1111/j.1538-4632.1996.tb00936.x>.\n\tYau, K. K. W., Wang, K., & Lee, A. H. (2003). \"Zero-inflated negative binomial mixed regression modeling of over-dispersed count data with extra zeros\", Biometrical Journal, <doi:10.1002/bimj.200390024>.  "
  },
  {
    "id": 13794,
    "package_name": "ham",
    "title": "Healthcare Analysis Methods",
    "description": "Conducts analyses for healthcare program evaluations or intervention \n    studies. Calculates regression analyses for standard ordinary least squares \n    (OLS or linear) or logistic models. Performs regression models used for \n    causal modeling such as differences-in-differences (DID) and interrupted \n    time series (ITS) models. Provides limited interpretations of model \n    results and a ranking of variable importance in models. Performs \n    propensity score models, top-coding of model outcome variables, and \n    can return new data with the newly formed variables. Also performs Cronbach's \n    alpha for various scale items (e.g., survey questions). See Github URL for \n    examples in the README file. For more details on the statistical methods, see \n    Allen & Yen (1979, ISBN:0-8185-0283-5), \n    Angrist & Pischke (2009, ISBN:9780691120355), \n    Harrell (2016, ISBN:978-3-319-19424-0), \n    Kline (1999, ISBN:9780415211581),  \n    Linden (2015) <doi:10.1177/1536867X1501500208>,\n    Merlo (2006) <doi:10.1136/jech.2004.029454>\n    Muthen & Satorra (1995) <doi:10.2307/271070>, and\n    Rabe-Hesketh & Skrondal (2008, ISBN:978-1-59718-040-5).",
    "version": "1.1.0",
    "maintainer": "Stephen Zuniga <rms.shiny@gmail.com>",
    "author": "Stephen Zuniga [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1458-3924>)",
    "url": "https://github.com/szuniga07/ham",
    "bug_reports": "https://github.com/szuniga07/ham/issues",
    "repository": "https://cran.r-project.org/package=ham",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ham Healthcare Analysis Methods Conducts analyses for healthcare program evaluations or intervention \n    studies. Calculates regression analyses for standard ordinary least squares \n    (OLS or linear) or logistic models. Performs regression models used for \n    causal modeling such as differences-in-differences (DID) and interrupted \n    time series (ITS) models. Provides limited interpretations of model \n    results and a ranking of variable importance in models. Performs \n    propensity score models, top-coding of model outcome variables, and \n    can return new data with the newly formed variables. Also performs Cronbach's \n    alpha for various scale items (e.g., survey questions). See Github URL for \n    examples in the README file. For more details on the statistical methods, see \n    Allen & Yen (1979, ISBN:0-8185-0283-5), \n    Angrist & Pischke (2009, ISBN:9780691120355), \n    Harrell (2016, ISBN:978-3-319-19424-0), \n    Kline (1999, ISBN:9780415211581),  \n    Linden (2015) <doi:10.1177/1536867X1501500208>,\n    Merlo (2006) <doi:10.1136/jech.2004.029454>\n    Muthen & Satorra (1995) <doi:10.2307/271070>, and\n    Rabe-Hesketh & Skrondal (2008, ISBN:978-1-59718-040-5).  "
  },
  {
    "id": 13809,
    "package_name": "happytime",
    "title": "Two Games to Relieve the Boredom",
    "description": "There are two interesting games in this package, one is 2048 games(for windows), using up and down to control the direction until there is a 2048 figure. And the other is 'what to eat today',preparing for people who choose difficulties, including most of the delicious Cantonese cuisine.",
    "version": "0.1.0",
    "maintainer": "Xiuwen Wang <wangxw36@mail2.sysu.edu.cn>",
    "author": "Xiuwen Wang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=happytime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "happytime Two Games to Relieve the Boredom There are two interesting games in this package, one is 2048 games(for windows), using up and down to control the direction until there is a 2048 figure. And the other is 'what to eat today',preparing for people who choose difficulties, including most of the delicious Cantonese cuisine.  "
  },
  {
    "id": 13873,
    "package_name": "healthcare.antitrust",
    "title": "Healthcare Antitrust Analysis",
    "description": "Antitrust analysis of\n    healthcare markets. Contains functions to implement the \n    semiparametric estimation technique described in Raval, Rosenbaum,\n    and Tenn (2017) \"A Semiparametric Discrete Choice Model: An Application\n    to Hospital Mergers\" <doi:10.1111/ecin.12454>.",
    "version": "0.1.4",
    "maintainer": "Matthew T Panhans <mpanhans@gmail.com>",
    "author": "Matthew T Panhans [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7874-6740>)",
    "url": "https://github.com/mpanhans/healthcare.antitrust",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=healthcare.antitrust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "healthcare.antitrust Healthcare Antitrust Analysis Antitrust analysis of\n    healthcare markets. Contains functions to implement the \n    semiparametric estimation technique described in Raval, Rosenbaum,\n    and Tenn (2017) \"A Semiparametric Discrete Choice Model: An Application\n    to Hospital Mergers\" <doi:10.1111/ecin.12454>.  "
  },
  {
    "id": 13954,
    "package_name": "hicp",
    "title": "Harmonised Index of Consumer Prices",
    "description": "The Harmonised Index of Consumer Prices (HICP) is the key economic figure to measure inflation in the euro area.\n              The methodology underlying the HICP is documented in the HICP Methodological Manual (<https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/w/ks-gq-24-003>).\n              Based on the manual, this package provides functions to access and work with HICP data from Eurostat's public database (<https://ec.europa.eu/eurostat/data/database>).",
    "version": "1.0.0",
    "maintainer": "Sebastian Weinand <sebastian.weinand@ec.europa.eu>",
    "author": "Sebastian Weinand [aut, cre]",
    "url": "https://github.com/eurostat/hicp",
    "bug_reports": "https://github.com/eurostat/hicp/issues",
    "repository": "https://cran.r-project.org/package=hicp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hicp Harmonised Index of Consumer Prices The Harmonised Index of Consumer Prices (HICP) is the key economic figure to measure inflation in the euro area.\n              The methodology underlying the HICP is documented in the HICP Methodological Manual (<https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/w/ks-gq-24-003>).\n              Based on the manual, this package provides functions to access and work with HICP data from Eurostat's public database (<https://ec.europa.eu/eurostat/data/database>).  "
  },
  {
    "id": 14035,
    "package_name": "hoopR",
    "title": "Access Men's Basketball Play by Play Data",
    "description": "A utility to quickly obtain clean and tidy men's\n    basketball play by play data. Provides functions to access\n    live play by play and box score data from ESPN<https://www.espn.com> with shot locations\n    when available. It is also a full NBA Stats API<https://www.nba.com/stats/> wrapper.\n    It is also a scraping and aggregating interface for Ken Pomeroy's \n    men's college basketball statistics website<https://kenpom.com>. It provides users with an\n    active subscription the capability to scrape the website tables and\n    analyze the data for themselves.",
    "version": "2.1.0",
    "maintainer": "Saiem Gilani <saiem.gilani@gmail.com>",
    "author": "Saiem Gilani [aut, cre],\n  Jason Lee [ctb],\n  Billy Fryer [ctb],\n  Ross Drucker [ctb],\n  Vladislav Shufinskiy [ctb]",
    "url": "https://github.com/sportsdataverse/hoopR,\nhttp://hoopr.sportsdataverse.org/",
    "bug_reports": "https://github.com/sportsdataverse/hoopR/issues",
    "repository": "https://cran.r-project.org/package=hoopR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hoopR Access Men's Basketball Play by Play Data A utility to quickly obtain clean and tidy men's\n    basketball play by play data. Provides functions to access\n    live play by play and box score data from ESPN<https://www.espn.com> with shot locations\n    when available. It is also a full NBA Stats API<https://www.nba.com/stats/> wrapper.\n    It is also a scraping and aggregating interface for Ken Pomeroy's \n    men's college basketball statistics website<https://kenpom.com>. It provides users with an\n    active subscription the capability to scrape the website tables and\n    analyze the data for themselves.  "
  },
  {
    "id": 14051,
    "package_name": "howler",
    "title": "'Shiny' Extension of 'howler.js'",
    "description": "Audio interactivity within 'shiny' applications using 'howler.js'. Enables the\n    status of the audio player to be sent from the UI to the server, and events such as\n    playing and pausing the audio can be triggered from the server.",
    "version": "0.3.0",
    "maintainer": "Ashley Baldry <arbaldry91@gmail.com>",
    "author": "Ashley Baldry [aut, cre],\n  James Simpson [aut] (Creator of howler.js)",
    "url": "https://github.com/ashbaldry/howler,\nhttps://github.com/goldfire/howler.js",
    "bug_reports": "https://github.com/ashbaldry/howler/issues",
    "repository": "https://cran.r-project.org/package=howler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "howler 'Shiny' Extension of 'howler.js' Audio interactivity within 'shiny' applications using 'howler.js'. Enables the\n    status of the audio player to be sent from the UI to the server, and events such as\n    playing and pausing the audio can be triggered from the server.  "
  },
  {
    "id": 14064,
    "package_name": "hrtlFMC",
    "title": "Half Replicate of Two Level Factorial Run Order with Minimum\nLevel Changes",
    "description": "It is used to construct run sequences with minimum changes for half replicate of two level factorial run order. Experimenter can save time and resources by minimizing the number of changes in levels of individual factor and therefore the total number of changes. It consists of the function minimal_hrtlf(). This technique can be employed to any half replicate of two level factorial run order where the number of factors are greater than two. In Design of Experiments (DOE) theory, two level of a factor can be represented as integers e.g. - 1 for low and 1 for high. User is expected to enter total number of factors to be considered in the experiment. minimal_hrtlf() provides the required run sequences for the input number of factors.  The output also gives the number of changes of each factor along with total number of changes in the run sequence. Due to restricted randomization the minimally changed run sequences of half replicate of two level factorial run order will be affected by trend effect. The output also provides the Trend Factor value of the run order. Trend factor value will lies between 0 to 1. Higher the values, lesser the influence of trend effects on the run order.",
    "version": "0.1.0",
    "maintainer": "Bijoy Chanda <bijoychanda08@gmail.com>",
    "author": "Arpan Bhowmik [aut, ctb, cph],\n  Eldho Varghese [aut, ctb, cph],\n  Seema Jaggi [aut, ctb, cph],\n  Bijoy Chanda [aut, cre, cph],\n  Anindita Datta [aut, ctb, cph],\n  Tanuj Misra [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hrtlFMC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hrtlFMC Half Replicate of Two Level Factorial Run Order with Minimum\nLevel Changes It is used to construct run sequences with minimum changes for half replicate of two level factorial run order. Experimenter can save time and resources by minimizing the number of changes in levels of individual factor and therefore the total number of changes. It consists of the function minimal_hrtlf(). This technique can be employed to any half replicate of two level factorial run order where the number of factors are greater than two. In Design of Experiments (DOE) theory, two level of a factor can be represented as integers e.g. - 1 for low and 1 for high. User is expected to enter total number of factors to be considered in the experiment. minimal_hrtlf() provides the required run sequences for the input number of factors.  The output also gives the number of changes of each factor along with total number of changes in the run sequence. Due to restricted randomization the minimally changed run sequences of half replicate of two level factorial run order will be affected by trend effect. The output also provides the Trend Factor value of the run order. Trend factor value will lies between 0 to 1. Higher the values, lesser the influence of trend effects on the run order.  "
  },
  {
    "id": 14106,
    "package_name": "hurdlr",
    "title": "Zero-Inflated and Hurdle Modelling Using Bayesian Inference",
    "description": "When considering count data, it is often the case that many more zero counts than would be expected of some given distribution are observed. It is well established that data such as this can be reliably modelled using zero-inflated or hurdle distributions, both of which may be applied using the functions in this package. Bayesian analysis methods are used to best model problematic count data that cannot be fit to any typical distribution. The package functions are flexible and versatile, and can be applied to varying count distributions, parameter estimation with or without explanatory variable information, and are able to allow for multiple hurdles as it is also not uncommon that count data have an abundance of large-number observations which would be considered outliers of the typical distribution. In lieu of throwing out data or misspecifying the typical distribution, these extreme observations can be applied to a second, extreme distribution. With the given functions of this package, such a two-hurdle model may be easily specified in order to best manage data that is both zero-inflated and over-dispersed.",
    "version": "0.1",
    "maintainer": "Earvin Balderama <ebalderama@luc.edu>",
    "author": "Earvin Balderama [aut, cre],\n  Taylor Trippe [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hurdlr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hurdlr Zero-Inflated and Hurdle Modelling Using Bayesian Inference When considering count data, it is often the case that many more zero counts than would be expected of some given distribution are observed. It is well established that data such as this can be reliably modelled using zero-inflated or hurdle distributions, both of which may be applied using the functions in this package. Bayesian analysis methods are used to best model problematic count data that cannot be fit to any typical distribution. The package functions are flexible and versatile, and can be applied to varying count distributions, parameter estimation with or without explanatory variable information, and are able to allow for multiple hurdles as it is also not uncommon that count data have an abundance of large-number observations which would be considered outliers of the typical distribution. In lieu of throwing out data or misspecifying the typical distribution, these extreme observations can be applied to a second, extreme distribution. With the given functions of this package, such a two-hurdle model may be easily specified in order to best manage data that is both zero-inflated and over-dispersed.  "
  },
  {
    "id": 14120,
    "package_name": "hybridModels",
    "title": "An R Package for the Stochastic Simulation of Disease Spreading\nin Dynamic Networks",
    "description": "Simulates stochastic hybrid models for transmission of infectious\n    diseases in dynamic networks. It is a metapopulation model in which each\n    node in the network is a sub-population and disease spreads within nodes\n    and among them, combining two approaches: stochastic simulation algorithm\n    (<doi:10.1146/annurev.physchem.58.032806.104637>) and individual-based\n    approach, respectively. Equations that models spread within nodes are\n    customizable and there are two link types among nodes: migration and\n    influence (commuting). More information in Fernando S. Marques,\n    Jose H. H. Grisi-Filho, Marcos Amaku et al. (2020) <doi:10.18637/jss.v094.i06>.",
    "version": "0.3.8",
    "maintainer": "Fernando S. Marques <fernandosix@gmail.com>",
    "author": "Fernando S. Marques [aut, cre],\n  Jose H. H. Grisi-Filho [aut],\n  Marcos Amaku [aut]",
    "url": "https://github.com/fernandosm/hybridModels",
    "bug_reports": "https://github.com/fernandosm/hybridModels/issues",
    "repository": "https://cran.r-project.org/package=hybridModels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hybridModels An R Package for the Stochastic Simulation of Disease Spreading\nin Dynamic Networks Simulates stochastic hybrid models for transmission of infectious\n    diseases in dynamic networks. It is a metapopulation model in which each\n    node in the network is a sub-population and disease spreads within nodes\n    and among them, combining two approaches: stochastic simulation algorithm\n    (<doi:10.1146/annurev.physchem.58.032806.104637>) and individual-based\n    approach, respectively. Equations that models spread within nodes are\n    customizable and there are two link types among nodes: migration and\n    influence (commuting). More information in Fernando S. Marques,\n    Jose H. H. Grisi-Filho, Marcos Amaku et al. (2020) <doi:10.18637/jss.v094.i06>.  "
  },
  {
    "id": 14194,
    "package_name": "iPRISM",
    "title": "Intelligent Predicting Response to Cancer Immunotherapy Through\nSystematic Modeling",
    "description": "\n    Immunotherapy has revolutionized cancer treatment, but predicting patient\n    response remains challenging. Here, we presented Intelligent Predicting\n    Response to cancer Immunotherapy through Systematic Modeling (iPRISM), a\n    novel network-based model that integrates multiple data types to predict\n    immunotherapy outcomes. It incorporates gene expression, biological\n    functional network, tumor microenvironment characteristics, immune-related\n    pathways, and clinical data to provide a comprehensive view of factors\n    influencing immunotherapy efficacy. By identifying key genetic and\n    immunological factors, it provides an insight for more personalized\n    treatment strategies and combination therapies to overcome resistance\n    mechanisms.",
    "version": "0.1.1",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [aut, cre, ctb],\n  Yinchun Su [aut],\n  Siyuan Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iPRISM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iPRISM Intelligent Predicting Response to Cancer Immunotherapy Through\nSystematic Modeling \n    Immunotherapy has revolutionized cancer treatment, but predicting patient\n    response remains challenging. Here, we presented Intelligent Predicting\n    Response to cancer Immunotherapy through Systematic Modeling (iPRISM), a\n    novel network-based model that integrates multiple data types to predict\n    immunotherapy outcomes. It incorporates gene expression, biological\n    functional network, tumor microenvironment characteristics, immune-related\n    pathways, and clinical data to provide a comprehensive view of factors\n    influencing immunotherapy efficacy. By identifying key genetic and\n    immunological factors, it provides an insight for more personalized\n    treatment strategies and combination therapies to overcome resistance\n    mechanisms.  "
  },
  {
    "id": 14205,
    "package_name": "iTOS",
    "title": "Methods and Examples from Introduction to the Theory of\nObservational Studies",
    "description": "Supplements for a book, \"iTOS\" = \"Introduction to the Theory of Observational Studies.\"  Data sets are 'aHDL' from Rosenbaum (2023a) <doi:10.1111/biom.13558> and 'bingeM' from Rosenbaum (2023b) <doi:10.1111/biom.13921>.  The function makematch() uses two-criteria matching from Zhang et al. (2023) <doi:10.1080/01621459.2021.1981337> to create the matched data 'bingeM' from 'binge'.  The makematch() function also implements optimal matching (Rosenbaum (1989) <doi:10.2307/2290079>) and matching with fine or near-fine balance (Rosenbaum et al. (2007) <doi:10.1198/016214506000001059> and Yang et al (2012) <doi:10.1111/j.1541-0420.2011.01691.x>).  The book makes use of two other R packages, 'weightedRank' and 'tightenBlock'.",
    "version": "1.0.3",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iTOS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iTOS Methods and Examples from Introduction to the Theory of\nObservational Studies Supplements for a book, \"iTOS\" = \"Introduction to the Theory of Observational Studies.\"  Data sets are 'aHDL' from Rosenbaum (2023a) <doi:10.1111/biom.13558> and 'bingeM' from Rosenbaum (2023b) <doi:10.1111/biom.13921>.  The function makematch() uses two-criteria matching from Zhang et al. (2023) <doi:10.1080/01621459.2021.1981337> to create the matched data 'bingeM' from 'binge'.  The makematch() function also implements optimal matching (Rosenbaum (1989) <doi:10.2307/2290079>) and matching with fine or near-fine balance (Rosenbaum et al. (2007) <doi:10.1198/016214506000001059> and Yang et al (2012) <doi:10.1111/j.1541-0420.2011.01691.x>).  The book makes use of two other R packages, 'weightedRank' and 'tightenBlock'.  "
  },
  {
    "id": 14208,
    "package_name": "iZID",
    "title": "Identify Zero-Inflated Distributions",
    "description": "Computes bootstrapped Monte Carlo estimate of p value of Kolmogorov-Smirnov (KS) test and \n    likelihood ratio test for zero-inflated count data, based on the work of Aldirawi et al. (2019) \n    <doi:10.1109/BHI.2019.8834661>. With the package, user can also find tools to simulate \n    random deviates from zero inflated or hurdle models and obtain  maximum likelihood estimate\n    of unknown parameters in these models.",
    "version": "0.0.1",
    "maintainer": "Lei Wang <slimewanglei@163.com>",
    "author": "Lei Wang [aut, cre, cph],\n  Hani Aldirawi [aut, cph],\n  Jie Yang [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iZID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iZID Identify Zero-Inflated Distributions Computes bootstrapped Monte Carlo estimate of p value of Kolmogorov-Smirnov (KS) test and \n    likelihood ratio test for zero-inflated count data, based on the work of Aldirawi et al. (2019) \n    <doi:10.1109/BHI.2019.8834661>. With the package, user can also find tools to simulate \n    random deviates from zero inflated or hurdle models and obtain  maximum likelihood estimate\n    of unknown parameters in these models.  "
  },
  {
    "id": 14221,
    "package_name": "ibelief",
    "title": "Belief Function Implementation",
    "description": "Some basic functions to implement belief functions including:\n    transformation between belief functions using the method introduced by\n    Philippe Smets <arXiv:1304.1122>, evidence combination, evidence\n    discounting, decision-making, and constructing masses. Currently, thirteen\n    combination rules and six decision rules are supported. It can also be\n    used to generate different types of random masses when working on belief\n    combination and conflict management.",
    "version": "1.3.1",
    "maintainer": "Kuang Zhou <kzhoumath@163.com>",
    "author": "Kuang Zhou <kzhoumath@163.com>; Arnaud Martin\n    <arnaud.martin@univ-rennes1.fr>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ibelief",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ibelief Belief Function Implementation Some basic functions to implement belief functions including:\n    transformation between belief functions using the method introduced by\n    Philippe Smets <arXiv:1304.1122>, evidence combination, evidence\n    discounting, decision-making, and constructing masses. Currently, thirteen\n    combination rules and six decision rules are supported. It can also be\n    used to generate different types of random masses when working on belief\n    combination and conflict management.  "
  },
  {
    "id": 14235,
    "package_name": "iccCounts",
    "title": "Intraclass Correlation Coefficient for Count Data",
    "description": "Estimates the intraclass correlation coefficient (ICC) for count data to assess repeatability (intra-methods concordance) and concordance (between-method concordance). In the concordance setting, the ICC is equivalent to the concordance correlation coefficient estimated by variance components. The ICC is estimated using the estimates from generalized linear mixed models. The within-subjects distributions considered are: Poisson; Negative Binomial with additive and proportional extradispersion; Zero-Inflated Poisson; and Zero-Inflated Negative Binomial with additive and proportional extradispersion. The statistical methodology used to estimate the ICC with count data can be found in Carrasco (2010) <doi:10.1111/j.1541-0420.2009.01335.x>.",
    "version": "1.1.2",
    "maintainer": "Josep L. Carrasco <jlcarrasco@ub.edu>",
    "author": "Josep L. Carrasco <jlcarrasco@ub.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iccCounts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iccCounts Intraclass Correlation Coefficient for Count Data Estimates the intraclass correlation coefficient (ICC) for count data to assess repeatability (intra-methods concordance) and concordance (between-method concordance). In the concordance setting, the ICC is equivalent to the concordance correlation coefficient estimated by variance components. The ICC is estimated using the estimates from generalized linear mixed models. The within-subjects distributions considered are: Poisson; Negative Binomial with additive and proportional extradispersion; Zero-Inflated Poisson; and Zero-Inflated Negative Binomial with additive and proportional extradispersion. The statistical methodology used to estimate the ICC with count data can be found in Carrasco (2010) <doi:10.1111/j.1541-0420.2009.01335.x>.  "
  },
  {
    "id": 14295,
    "package_name": "ife",
    "title": "Autodiff for Influence Function Based Estimates",
    "description": "Implements an S7 class for estimates based on influence functions, \n  with forward mode automatic differentiation defined for standard arithmetic \n  operations.",
    "version": "0.2.1",
    "maintainer": "Nicholas Williams <ntwilliams.personal@gmail.com>",
    "author": "Nicholas Williams [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1378-4831>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ife",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ife Autodiff for Influence Function Based Estimates Implements an S7 class for estimates based on influence functions, \n  with forward mode automatic differentiation defined for standard arithmetic \n  operations.  "
  },
  {
    "id": 14345,
    "package_name": "imdbapi",
    "title": "Get Movie, Television Data from the 'imdb' Database",
    "description": "Provides API access to the <http://imdbapi.net> which maintains metadata\n             about movies, games and television shows through a public API.",
    "version": "0.1.0",
    "maintainer": "Yuan Li <851277048@qq.com>",
    "author": "Daxuan Deng  Yuan Li",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=imdbapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imdbapi Get Movie, Television Data from the 'imdb' Database Provides API access to the <http://imdbapi.net> which maintains metadata\n             about movies, games and television shows through a public API.  "
  },
  {
    "id": 14358,
    "package_name": "impactflu",
    "title": "Quantification of Population-Level Impact of Vaccination",
    "description": "\n  Implements the compartment model from Tokars (2018) \n  <doi:10.1016/j.vaccine.2018.10.026>. This enables quantification of \n  population-wide impact of vaccination against vaccine-preventable \n  diseases such as influenza.",
    "version": "0.1.0",
    "maintainer": "Arseniy Khvorov <khvorov45@gmail.com>",
    "author": "Arseniy Khvorov [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=impactflu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "impactflu Quantification of Population-Level Impact of Vaccination \n  Implements the compartment model from Tokars (2018) \n  <doi:10.1016/j.vaccine.2018.10.026>. This enables quantification of \n  population-wide impact of vaccination against vaccine-preventable \n  diseases such as influenza.  "
  },
  {
    "id": 14365,
    "package_name": "importar",
    "title": "Enables Importing/Loading of Packages or Functions While\nCreating an Alias for Them",
    "description": "Enables 'Python'-like importing/loading of packages or functions\n    with aliasing to prevent namespace conflicts.",
    "version": "0.1.1",
    "maintainer": "Andrea Cantieni <andrea.cantieni@phsz.ch>",
    "author": "Andrea Cantieni",
    "url": "https://github.com/andreaphsz/importar",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=importar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "importar Enables Importing/Loading of Packages or Functions While\nCreating an Alias for Them Enables 'Python'-like importing/loading of packages or functions\n    with aliasing to prevent namespace conflicts.  "
  },
  {
    "id": 14369,
    "package_name": "imprinting",
    "title": "Calculate Birth Year-Specific Probabilities of Immune Imprinting\nto Influenza",
    "description": "Reconstruct birth-year specific probabilities of immune imprinting to influenza A, using the methods of Gostic et al. (2016) <doi:10.1126/science.aag1322>. Plot, save, or export the calculated probabilities for use in your own research. By default, the package calculates subtype-specific imprinting probabilities, but with user-provided frequency data, it is possible to calculate probabilities for arbitrary kinds of primary exposure to influenza A, including primary vaccination and exposure to specific clades, strains, etc.",
    "version": "0.1.1",
    "maintainer": "Alex Byrnes <abyrnes@uchicago.edu>",
    "author": "Katelyn Gostic [aut],\n  Alex Byrnes [ctb, cre]",
    "url": "https://cobeylab.github.io/imprinting/",
    "bug_reports": "https://github.com/cobeylab/imprinting/issues",
    "repository": "https://cran.r-project.org/package=imprinting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imprinting Calculate Birth Year-Specific Probabilities of Immune Imprinting\nto Influenza Reconstruct birth-year specific probabilities of immune imprinting to influenza A, using the methods of Gostic et al. (2016) <doi:10.1126/science.aag1322>. Plot, save, or export the calculated probabilities for use in your own research. By default, the package calculates subtype-specific imprinting probabilities, but with user-provided frequency data, it is possible to calculate probabilities for arbitrary kinds of primary exposure to influenza A, including primary vaccination and exposure to specific clades, strains, etc.  "
  },
  {
    "id": 14402,
    "package_name": "india",
    "title": "Influence Diagnostics in Statistical Models",
    "description": "Set of routines for influence diagnostics by using case-deletion in ordinary least \n    squares, nonlinear regression [Ross (1987). <doi:10.2307/3315198>], ridge estimation [Walker and Birch (1988). <doi:10.1080/00401706.1988.10488370>] \n    and least absolute deviations (LAD) regression [Sun and Wei (2004). <doi:10.1016/j.spl.2003.08.018>].",
    "version": "0.1-1",
    "maintainer": "Felipe Osorio <faosorios.stat@gmail.com>",
    "author": "Felipe Osorio [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4675-5201>)",
    "url": "https://github.com/faosorios/india",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=india",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "india Influence Diagnostics in Statistical Models Set of routines for influence diagnostics by using case-deletion in ordinary least \n    squares, nonlinear regression [Ross (1987). <doi:10.2307/3315198>], ridge estimation [Walker and Birch (1988). <doi:10.1080/00401706.1988.10488370>] \n    and least absolute deviations (LAD) regression [Sun and Wei (2004). <doi:10.1016/j.spl.2003.08.018>].  "
  },
  {
    "id": 14415,
    "package_name": "infectiousR",
    "title": "Access Infectious and Epidemiological Data via 'disease.sh API'",
    "description": "Provides functions to access real-time infectious disease data from the 'disease.sh API',\n    including COVID-19 global, US states, continent, and country statistics, vaccination coverage,\n    influenza-like illness data from the Centers for Disease Control and Prevention (CDC), and more. \n    Also includes curated datasets on a variety of infectious diseases such as influenza, measles, dengue, \n    Ebola, tuberculosis, meningitis, AIDS, and others. The package supports epidemiological research \n    and data analysis by combining API access with high-quality historical and survey datasets on infectious diseases. \n    For more details on the 'disease.sh API', see <https://disease.sh/>.",
    "version": "0.1.1",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre]",
    "url": "https://github.com/lightbluetitan/infectiousr,\nhttps://lightbluetitan.github.io/infectiousr/",
    "bug_reports": "https://github.com/lightbluetitan/infectiousr/issues",
    "repository": "https://cran.r-project.org/package=infectiousR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "infectiousR Access Infectious and Epidemiological Data via 'disease.sh API' Provides functions to access real-time infectious disease data from the 'disease.sh API',\n    including COVID-19 global, US states, continent, and country statistics, vaccination coverage,\n    influenza-like illness data from the Centers for Disease Control and Prevention (CDC), and more. \n    Also includes curated datasets on a variety of infectious diseases such as influenza, measles, dengue, \n    Ebola, tuberculosis, meningitis, AIDS, and others. The package supports epidemiological research \n    and data analysis by combining API access with high-quality historical and survey datasets on infectious diseases. \n    For more details on the 'disease.sh API', see <https://disease.sh/>.  "
  },
  {
    "id": 14423,
    "package_name": "inflection",
    "title": "Finds the Inflection Point of a Curve",
    "description": "Implementation of methods Extremum Surface Estimator (ESE) and \n   Extremum Distance Estimator (EDE) to identify the inflection point of a curve .\n   Christopoulos, DT (2014) <doi:10.48550/arXiv.1206.5478> .\n   Christopoulos, DT (2016) <https://demovtu.veltech.edu.in/wp-content/uploads/2016/04/Paper-04-2016.pdf> .\n   Christopoulos, DT (2016) <doi:10.2139/ssrn.3043076> .",
    "version": "1.3.7",
    "maintainer": "Demetris T. Christopoulos <dchristop@econ.uoa.gr>",
    "author": "Demetris T. Christopoulos [aut, cre]",
    "url": "https://CRAN.R-project.org/package=inflection",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=inflection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inflection Finds the Inflection Point of a Curve Implementation of methods Extremum Surface Estimator (ESE) and \n   Extremum Distance Estimator (EDE) to identify the inflection point of a curve .\n   Christopoulos, DT (2014) <doi:10.48550/arXiv.1206.5478> .\n   Christopoulos, DT (2016) <https://demovtu.veltech.edu.in/wp-content/uploads/2016/04/Paper-04-2016.pdf> .\n   Christopoulos, DT (2016) <doi:10.2139/ssrn.3043076> .  "
  },
  {
    "id": 14424,
    "package_name": "influence.ME",
    "title": "Tools for Detecting Influential Data in Mixed Effects Models",
    "description": "Provides a collection of tools for\n        detecting influential cases in generalized mixed effects\n        models. It analyses models that were estimated using 'lme4'. The\n        basic rationale behind identifying influential data is that\n        when single units are omitted from the data, models\n        based on these data should not produce substantially different\n        estimates. To standardize the assessment of how influential a\n        (single group of) observation(s) is, several measures of\n        influence are common practice, such as Cook's Distance. \n        In addition, we provide a measure of percentage change of the fixed point \n        estimates and a simple procedure to detect changing levels of significance.",
    "version": "0.9-9",
    "maintainer": "Rense Nieuwenhuis <rense.nieuwenhuis@sofi.su.se>",
    "author": "Rense Nieuwenhuis, Ben Pelzer, Manfred te Grotenhuis",
    "url": "http://www.rensenieuwenhuis.nl/r-project/influenceme/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=influence.ME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influence.ME Tools for Detecting Influential Data in Mixed Effects Models Provides a collection of tools for\n        detecting influential cases in generalized mixed effects\n        models. It analyses models that were estimated using 'lme4'. The\n        basic rationale behind identifying influential data is that\n        when single units are omitted from the data, models\n        based on these data should not produce substantially different\n        estimates. To standardize the assessment of how influential a\n        (single group of) observation(s) is, several measures of\n        influence are common practice, such as Cook's Distance. \n        In addition, we provide a measure of percentage change of the fixed point \n        estimates and a simple procedure to detect changing levels of significance.  "
  },
  {
    "id": 14425,
    "package_name": "influence.SEM",
    "title": "Case Influence in Structural Equation Models",
    "description": "A set of tools for evaluating several measures of case influence for structural equation models. ",
    "version": "2.4",
    "maintainer": "Massimiliano Pastore <massimiliano.pastore@unipd.it>",
    "author": "Massimiliano Pastore [aut, cre],\n  Gianmarco Altoe' [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=influence.SEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influence.SEM Case Influence in Structural Equation Models A set of tools for evaluating several measures of case influence for structural equation models.   "
  },
  {
    "id": 14426,
    "package_name": "influenceAUC",
    "title": "Identify Influential Observations in Binary Classification",
    "description": "Ke, B. S., Chiang, A. J., & Chang, Y. C. I. (2018) <doi:10.1080/10543406.2017.1377728> provide two theoretical methods (influence function and local influence) based on the area under the receiver operating characteristic curve (AUC) to quantify the numerical impact of each observation to the overall AUC. Alternative graphical tools, cumulative lift charts, are proposed to reveal the existences and approximate locations of those influential observations through data visualization.",
    "version": "0.1.2",
    "maintainer": "Bo-Shiang Ke <naivete0907@gmail.com>",
    "author": "Bo-Shiang Ke [cre, aut, cph],\n  Yuan-chin Ivan Chang [aut],\n  Wen-Ting Wang [aut]",
    "url": "",
    "bug_reports": "https://github.com/BoShiangKe/InfluenceAUC/issues",
    "repository": "https://cran.r-project.org/package=influenceAUC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influenceAUC Identify Influential Observations in Binary Classification Ke, B. S., Chiang, A. J., & Chang, Y. C. I. (2018) <doi:10.1080/10543406.2017.1377728> provide two theoretical methods (influence function and local influence) based on the area under the receiver operating characteristic curve (AUC) to quantify the numerical impact of each observation to the overall AUC. Alternative graphical tools, cumulative lift charts, are proposed to reveal the existences and approximate locations of those influential observations through data visualization.  "
  },
  {
    "id": 14427,
    "package_name": "influenceR",
    "title": "Software Tools to Quantify Structural Importance of Nodes in a\nNetwork",
    "description": "Provides functionality to compute various node centrality measures on networks.\n    Included are functions to compute betweenness centrality (by utilizing Madduri and Bader's\n    SNAP library), implementations of constraint and effective network size by Burt (2000) <doi:10.1016/S0191-3085(00)22009-1>; algorithm \n    to identify key players by Borgatti (2006) <doi:10.1007/s10588-006-7084-x>; and the \n    bridging algorithm by Valente and Fujimoto (2010) <doi:10.1016/j.socnet.2010.03.003>. \n    On Unix systems, the betweenness, Key Players, and\n    bridging implementations are parallelized with OpenMP, which may run\n    faster on systems which have OpenMP configured.",
    "version": "0.1.5",
    "maintainer": "Aditya Khanna <khanna7.work@gmail.com>",
    "author": "Simon Jacobs [aut], Aditya Khanna [aut, cre], Kamesh Madduri [ctb], David Bader [ctb]",
    "url": "https://github.com/khanna-lab/influenceR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=influenceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influenceR Software Tools to Quantify Structural Importance of Nodes in a\nNetwork Provides functionality to compute various node centrality measures on networks.\n    Included are functions to compute betweenness centrality (by utilizing Madduri and Bader's\n    SNAP library), implementations of constraint and effective network size by Burt (2000) <doi:10.1016/S0191-3085(00)22009-1>; algorithm \n    to identify key players by Borgatti (2006) <doi:10.1007/s10588-006-7084-x>; and the \n    bridging algorithm by Valente and Fujimoto (2010) <doi:10.1016/j.socnet.2010.03.003>. \n    On Unix systems, the betweenness, Key Players, and\n    bridging implementations are parallelized with OpenMP, which may run\n    faster on systems which have OpenMP configured.  "
  },
  {
    "id": 14428,
    "package_name": "influential",
    "title": "Identification and Classification of the Most Influential Nodes",
    "description": "Contains functions for the classification and ranking of top candidate features, reconstruction of networks from\n    adjacency matrices and data frames, analysis of the topology of the network \n    and calculation of centrality measures, and identification of the most\n    influential nodes. Also, a function is provided for running SIRIR model, which \n    is the combination of leave-one-out cross validation technique and the conventional SIR model, on a network to unsupervisedly rank the true influence of vertices. Additionally, some functions have been provided for the assessment \n    of dependence and correlation of two network centrality measures as well as \n    the conditional probability of deviation from their corresponding means in opposite direction.\n    Fred Viole and David Nawrocki (2013, ISBN:1490523995).\n    Csardi G, Nepusz T (2006). \"The igraph software package for complex network research.\" InterJournal, Complex Systems, 1695.\n    Adopted algorithms and sources are referenced in function document.",
    "version": "2.2.9",
    "maintainer": "Adrian Salavaty <abbas.salavaty@gmail.com>",
    "author": "Abbas (Adrian) Salavaty [aut, cre], Mirana Ramialison [ths], Peter D. Currie [ths]",
    "url": "https://github.com/asalavaty/influential,\nhttps://asalavaty.github.io/influential/",
    "bug_reports": "https://github.com/asalavaty/influential/issues",
    "repository": "https://cran.r-project.org/package=influential",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influential Identification and Classification of the Most Influential Nodes Contains functions for the classification and ranking of top candidate features, reconstruction of networks from\n    adjacency matrices and data frames, analysis of the topology of the network \n    and calculation of centrality measures, and identification of the most\n    influential nodes. Also, a function is provided for running SIRIR model, which \n    is the combination of leave-one-out cross validation technique and the conventional SIR model, on a network to unsupervisedly rank the true influence of vertices. Additionally, some functions have been provided for the assessment \n    of dependence and correlation of two network centrality measures as well as \n    the conditional probability of deviation from their corresponding means in opposite direction.\n    Fred Viole and David Nawrocki (2013, ISBN:1490523995).\n    Csardi G, Nepusz T (2006). \"The igraph software package for complex network research.\" InterJournal, Complex Systems, 1695.\n    Adopted algorithms and sources are referenced in function document.  "
  },
  {
    "id": 14429,
    "package_name": "influxdbr",
    "title": "R Interface to InfluxDB",
    "description": "An R interface to the InfluxDB time series database <https://www.influxdata.com>. This package allows you to fetch and write time series data from/to an InfluxDB server. Additionally, handy wrappers for the Influx Query Language (IQL) to manage and explore a remote database are provided. ",
    "version": "0.14.2",
    "maintainer": "Dominik Leutnant <leutnant@fh-muenster.de>",
    "author": "Dominik Leutnant [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3293-2315>)",
    "url": "https://github.com/dleutnant/influxdbr",
    "bug_reports": "http://github.com/dleutnant/influxdbr/issues",
    "repository": "https://cran.r-project.org/package=influxdbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influxdbr R Interface to InfluxDB An R interface to the InfluxDB time series database <https://www.influxdata.com>. This package allows you to fetch and write time series data from/to an InfluxDB server. Additionally, handy wrappers for the Influx Query Language (IQL) to manage and explore a remote database are provided.   "
  },
  {
    "id": 14433,
    "package_name": "informedSen",
    "title": "Sensitivity Analysis Informed by a Test for Bias",
    "description": "After testing for biased treatment assignment in an observational study using an unaffected outcome, the sensitivity analysis is constrained to be compatible with that test.  The package uses the optimization software gurobi obtainable from <https://www.gurobi.com/>, together with its associated R package, also called gurobi; see: <https://www.gurobi.com/documentation/7.0/refman/installing_the_r_package.html>.  The method is a substantial computational and practical enhancement of a concept introduced in Rosenbaum (1992) Detecting bias with confidence in observational studies Biometrika, 79(2), 367-374  <doi:10.1093/biomet/79.2.367>.",
    "version": "1.0.7",
    "maintainer": "Paul R Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=informedSen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "informedSen Sensitivity Analysis Informed by a Test for Bias After testing for biased treatment assignment in an observational study using an unaffected outcome, the sensitivity analysis is constrained to be compatible with that test.  The package uses the optimization software gurobi obtainable from <https://www.gurobi.com/>, together with its associated R package, also called gurobi; see: <https://www.gurobi.com/documentation/7.0/refman/installing_the_r_package.html>.  The method is a substantial computational and practical enhancement of a concept introduced in Rosenbaum (1992) Detecting bias with confidence in observational studies Biometrika, 79(2), 367-374  <doi:10.1093/biomet/79.2.367>.  "
  },
  {
    "id": 14440,
    "package_name": "injurytools",
    "title": "A Toolkit for Sports Injury and Illness Data Analysis",
    "description": "Sports Injury Data analysis aims to identify and describe the\n    magnitude of the injury problem, and to gain more insights (e.g.\n    determine potential risk factors) by statistical modelling approaches.\n    The 'injurytools' package provides standardized routines and utilities\n    that simplify such analyses. It offers functions for data preparation,\n    informative visualizations and descriptive and model-based analyses.",
    "version": "2.0.0",
    "maintainer": "Lore Zumeta Olaskoaga <lorezumeta@gmail.com>",
    "author": "Lore Zumeta Olaskoaga [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6141-1469>),\n  Dae-Jin Lee [ctb] (ORCID: <https://orcid.org/0000-0002-8995-8535>)",
    "url": "https://github.com/lzumeta/injurytools,\nhttps://lzumeta.github.io/injurytools/",
    "bug_reports": "https://github.com/lzumeta/injurytools/issues",
    "repository": "https://cran.r-project.org/package=injurytools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "injurytools A Toolkit for Sports Injury and Illness Data Analysis Sports Injury Data analysis aims to identify and describe the\n    magnitude of the injury problem, and to gain more insights (e.g.\n    determine potential risk factors) by statistical modelling approaches.\n    The 'injurytools' package provides standardized routines and utilities\n    that simplify such analyses. It offers functions for data preparation,\n    informative visualizations and descriptive and model-based analyses.  "
  },
  {
    "id": 14477,
    "package_name": "intendo",
    "title": "A Group of Fun Datasets of Various Sizes and Differing Levels of\nQuality",
    "description": "Four datasets are provided here from the 'Intendo' game\n    'Super Jetroid'. It is data from the 2015 year of operation and it comprises\n    a revenue table ('all_revenue'), a daily users table ('users_daily'), a user\n    summary table ('user_summary'), and a table with data on all user sessions\n    ('all_sessions'). These core datasets come in different sizes, and, each of\n    them has a variant that was intentionally made faulty (totally riddled with\n    errors and inconsistencies). This suite of tables is useful for testing with\n    packages that focus on data validation and data documentation.",
    "version": "0.1.1",
    "maintainer": "Richard Iannone <riannone@me.com>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3925-190X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intendo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intendo A Group of Fun Datasets of Various Sizes and Differing Levels of\nQuality Four datasets are provided here from the 'Intendo' game\n    'Super Jetroid'. It is data from the 2015 year of operation and it comprises\n    a revenue table ('all_revenue'), a daily users table ('users_daily'), a user\n    summary table ('user_summary'), and a table with data on all user sessions\n    ('all_sessions'). These core datasets come in different sizes, and, each of\n    them has a variant that was intentionally made faulty (totally riddled with\n    errors and inconsistencies). This suite of tables is useful for testing with\n    packages that focus on data validation and data documentation.  "
  },
  {
    "id": 14519,
    "package_name": "invGauss",
    "title": "Threshold Regression that Fits the (Randomized Drift) Inverse\nGaussian Distribution to Survival Data",
    "description": "Fits the (randomized drift) inverse Gaussian distribution to survival data. The model is described in Aalen OO, Borgan O, Gjessing HK. Survival and Event History Analysis. A Process Point of View. Springer, 2008. It is based on describing time to event as the barrier hitting time of a Wiener process, where drift towards the barrier has been randomized with a Gaussian distribution. The model allows covariates to influence starting values of the Wiener process and/or average drift towards a barrier, with a user-defined choice of link functions. ",
    "version": "1.2",
    "maintainer": "Hakon K. Gjessing <hakon.gjessing@uib.no>",
    "author": "Hakon K. Gjessing",
    "url": "http://www.uib.no/smis/gjessing/projects/invgauss/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=invGauss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "invGauss Threshold Regression that Fits the (Randomized Drift) Inverse\nGaussian Distribution to Survival Data Fits the (randomized drift) inverse Gaussian distribution to survival data. The model is described in Aalen OO, Borgan O, Gjessing HK. Survival and Event History Analysis. A Process Point of View. Springer, 2008. It is based on describing time to event as the barrier hitting time of a Wiener process, where drift towards the barrier has been randomized with a Gaussian distribution. The model allows covariates to influence starting values of the Wiener process and/or average drift towards a barrier, with a user-defined choice of link functions.   "
  },
  {
    "id": 14532,
    "package_name": "ioanalysis",
    "title": "Input Output Analysis",
    "description": "Calculates fundamental IO matrices (Leontief, Wassily W. (1951) <doi:10.1038/scientificamerican1051-15>); within period analysis via various rankings and coefficients (Sonis and Hewings (2006) <doi:10.1080/09535319200000013>, Blair and Miller (2009) <ISBN:978-0-521-73902-3>, Antras et al (2012) <doi:10.3386/w17819>, Hummels, Ishii, and Yi (2001) <doi:10.1016/S0022-1996(00)00093-3>); across period analysis with impact analysis (Dietzenbacher, van der Linden, and Steenge (2006) <doi:10.1080/09535319300000017>, Sonis, Hewings, and Guo (2006) <doi:10.1080/09535319600000002>); and a variety of table operators.",
    "version": "0.3.4",
    "maintainer": "John Wade <jjpwade2@illinois.edu>",
    "author": "John Wade [aut, cre],\n  Ignacio Sarmiento-Barbieri [aut]",
    "url": "http://www.real.illinois.edu",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ioanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ioanalysis Input Output Analysis Calculates fundamental IO matrices (Leontief, Wassily W. (1951) <doi:10.1038/scientificamerican1051-15>); within period analysis via various rankings and coefficients (Sonis and Hewings (2006) <doi:10.1080/09535319200000013>, Blair and Miller (2009) <ISBN:978-0-521-73902-3>, Antras et al (2012) <doi:10.3386/w17819>, Hummels, Ishii, and Yi (2001) <doi:10.1016/S0022-1996(00)00093-3>); across period analysis with impact analysis (Dietzenbacher, van der Linden, and Steenge (2006) <doi:10.1080/09535319300000017>, Sonis, Hewings, and Guo (2006) <doi:10.1080/09535319600000002>); and a variety of table operators.  "
  },
  {
    "id": 14587,
    "package_name": "irrNA",
    "title": "Coefficients of Interrater Reliability \u2013 Generalized for\nRandomly Incomplete Datasets",
    "description": "Provides coefficients of interrater reliability that are generalized to cope with randomly incomplete (i.e. unbalanced) datasets without any imputation of missing values or any (row-wise or column-wise) omissions of actually available data. Applied to complete (balanced) datasets, these generalizations yield the same results as the common procedures, namely the Intraclass Correlation according to McGraw & Wong (1996) \\doi{10.1037/1082-989X.1.1.30} and the Coefficient of Concordance according to Kendall & Babington Smith (1939) \\doi{10.1214/aoms/1177732186}.",
    "version": "0.2.3",
    "maintainer": "Markus Brueckl <markus.brueckl@tu-berlin.de>",
    "author": "Markus Brueckl [aut, cre], Florian Heuer [aut, trl]",
    "url": "https://CRAN.R-project.org/package=irrNA",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=irrNA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "irrNA Coefficients of Interrater Reliability \u2013 Generalized for\nRandomly Incomplete Datasets Provides coefficients of interrater reliability that are generalized to cope with randomly incomplete (i.e. unbalanced) datasets without any imputation of missing values or any (row-wise or column-wise) omissions of actually available data. Applied to complete (balanced) datasets, these generalizations yield the same results as the common procedures, namely the Intraclass Correlation according to McGraw & Wong (1996) \\doi{10.1037/1082-989X.1.1.30} and the Coefficient of Concordance according to Kendall & Babington Smith (1939) \\doi{10.1214/aoms/1177732186}.  "
  },
  {
    "id": 14650,
    "package_name": "itscalledsoccer",
    "title": "American Soccer Analysis API Client",
    "description": "Provides a wrapper around the same API <https://app.americansocceranalysis.com/api/v1/__docs__/>\n    that powers the American Soccer Analysis app.",
    "version": "0.3.2",
    "maintainer": "Tyler Richardett <tyler.richardett@gmail.com>",
    "author": "Tyler Richardett [aut, cre],\n  Akshay Easwaran [aut],\n  American Soccer Analysis [cph]",
    "url": "https://github.com/American-Soccer-Analysis/itscalledsoccer-r,\nhttps://american-soccer-analysis.github.io/itscalledsoccer-r/",
    "bug_reports": "https://github.com/American-Soccer-Analysis/itscalledsoccer-r/issues",
    "repository": "https://cran.r-project.org/package=itscalledsoccer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "itscalledsoccer American Soccer Analysis API Client Provides a wrapper around the same API <https://app.americansocceranalysis.com/api/v1/__docs__/>\n    that powers the American Soccer Analysis app.  "
  },
  {
    "id": 14802,
    "package_name": "kangar00",
    "title": "Kernel Approaches for Nonlinear Genetic Association Regression",
    "description": "Methods to extract information on pathways, genes and various single-nucleotid polymorphisms (SNPs) from online databases. It provides functions for data preparation and evaluation of genetic influence on a binary outcome using the logistic kernel machine test (LKMT). Three different kernel functions are offered to analyze genotype information in this variance component test: A linear kernel, a size-adjusted kernel and a network-based kernel).",
    "version": "1.4.2",
    "maintainer": "Juliane Manitz <r@manitz.org>",
    "author": "Juliane Manitz [aut, cre],\n  Benjamin Hofner [aut],\n  Stefanie Friedrichs [aut],\n  Patricia Burger [aut],\n  Ngoc Thuy Ha [aut],\n  Saskia Freytag [ctb],\n  Heike Bickeboeller [ctb]",
    "url": "https://kangar00.manitz.org/",
    "bug_reports": "https://github.com/jmanitz/kangar00/issues",
    "repository": "https://cran.r-project.org/package=kangar00",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kangar00 Kernel Approaches for Nonlinear Genetic Association Regression Methods to extract information on pathways, genes and various single-nucleotid polymorphisms (SNPs) from online databases. It provides functions for data preparation and evaluation of genetic influence on a binary outcome using the logistic kernel machine test (LKMT). Three different kernel functions are offered to analyze genotype information in this variance component test: A linear kernel, a size-adjusted kernel and a network-based kernel).  "
  },
  {
    "id": 14806,
    "package_name": "kaos",
    "title": "Encoding of Sequences Based on Frequency Matrix Chaos Game\nRepresentation",
    "description": "Sequences encoding by using the chaos game representation.\n    L\u00f6chel et al. (2019) <doi:10.1093/bioinformatics/btz493>.",
    "version": "0.1.2",
    "maintainer": "Hannah Franziska L\u00f6chel <loechelh@mathematik.uni-marburg.de>",
    "author": "Dominik Eger and Hannah Franziska L\u00f6chel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kaos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kaos Encoding of Sequences Based on Frequency Matrix Chaos Game\nRepresentation Sequences encoding by using the chaos game representation.\n    L\u00f6chel et al. (2019) <doi:10.1093/bioinformatics/btz493>.  "
  },
  {
    "id": 14810,
    "package_name": "kappalab",
    "title": "Non-Additive Measure and Integral Manipulation Functions",
    "description": "S4 tool box for capacity (or non-additive measure, fuzzy measure) and integral manipulation in a finite setting. It contains routines for handling various types of set functions such as games or capacities. It can be used to compute several non-additive integrals: the Choquet integral, the Sugeno integral, and the symmetric and asymmetric Choquet integrals. An analysis of capacities in terms of decision behavior can be performed through the computation of various indices such as the Shapley value, the interaction index, the orness degree, etc. The well-known M\u00f6bius transform, as well as other equivalent representations of set functions can also be computed. Kappalab further contains seven capacity identification routines: three least squares based approaches, a method based on linear programming, a maximum entropy like method based on variance minimization, a minimum distance approach and an unsupervised approach based on parametric entropies. The functions contained in Kappalab can for instance be used in the framework of multicriteria decision making or cooperative game theory.",
    "version": "0.4-12",
    "maintainer": "Ivan Kojadinovic <ivan.kojadinovic@univ-pau.fr>",
    "author": "Michel Grabisch, Ivan Kojadinovic, Patrick Meyer.  ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kappalab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kappalab Non-Additive Measure and Integral Manipulation Functions S4 tool box for capacity (or non-additive measure, fuzzy measure) and integral manipulation in a finite setting. It contains routines for handling various types of set functions such as games or capacities. It can be used to compute several non-additive integrals: the Choquet integral, the Sugeno integral, and the symmetric and asymmetric Choquet integrals. An analysis of capacities in terms of decision behavior can be performed through the computation of various indices such as the Shapley value, the interaction index, the orness degree, etc. The well-known M\u00f6bius transform, as well as other equivalent representations of set functions can also be computed. Kappalab further contains seven capacity identification routines: three least squares based approaches, a method based on linear programming, a maximum entropy like method based on variance minimization, a minimum distance approach and an unsupervised approach based on parametric entropies. The functions contained in Kappalab can for instance be used in the framework of multicriteria decision making or cooperative game theory.  "
  },
  {
    "id": 14860,
    "package_name": "keyplayer",
    "title": "Locating Key Players in Social Networks",
    "description": "Computes group centrality scores and identifies the most central group of players in a network.",
    "version": "1.0.4",
    "maintainer": "Weihua An <weihua.an@emory.edu>",
    "author": "Weihua An; Yu-Hsin Liu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=keyplayer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "keyplayer Locating Key Players in Social Networks Computes group centrality scores and identifies the most central group of players in a network.  "
  },
  {
    "id": 14874,
    "package_name": "kgschart",
    "title": "KGS Rank Graph Parser",
    "description": "Restore underlining numeric data from rating history graph of \n    KGS (an online platform of the game of go, <http://www.gokgs.com/>). \n    A shiny application is also provided.",
    "version": "1.3.5",
    "maintainer": "Kota Mori <kmori05@gmail.com>",
    "author": "Kota Mori [aut, cre]",
    "url": "https://github.com/kota7/kgschart",
    "bug_reports": "https://github.com/kota7/kgschart/issues",
    "repository": "https://cran.r-project.org/package=kgschart",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kgschart KGS Rank Graph Parser Restore underlining numeric data from rating history graph of \n    KGS (an online platform of the game of go, <http://www.gokgs.com/>). \n    A shiny application is also provided.  "
  },
  {
    "id": 14899,
    "package_name": "klausuR",
    "title": "Multiple Choice Test Evaluation",
    "description": "A set of functions designed to quickly generate results of a multiple choice\n          test. Generates detailed global results, lists for anonymous feedback and\n          personalised result feedback (in LaTeX and/or PDF format), as well as item\n          statistics like Cronbach's alpha or disciminatory power. 'klausuR' also\n          includes a plugin for the R GUI and IDE RKWard, providing graphical dialogs for\n          its basic features. The respective R package 'rkward' cannot be installed\n          directly from a repository, as it is a part of RKWard. To make full use of this\n          feature, please install RKWard from <https://rkward.kde.org> (plugins are\n          detected automatically). Due to some restrictions on CRAN, the full package\n          sources are only available from the project homepage.",
    "version": "0.12-14",
    "maintainer": "m.eik michalke <meik.michalke@hhu.de>",
    "author": "m.eik michalke [aut, cre]",
    "url": "https://reaktanz.de/?c=hacking&s=klausuR",
    "bug_reports": "https://github.com/unDocUMeantIt/klausuR/issues",
    "repository": "https://cran.r-project.org/package=klausuR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "klausuR Multiple Choice Test Evaluation A set of functions designed to quickly generate results of a multiple choice\n          test. Generates detailed global results, lists for anonymous feedback and\n          personalised result feedback (in LaTeX and/or PDF format), as well as item\n          statistics like Cronbach's alpha or disciminatory power. 'klausuR' also\n          includes a plugin for the R GUI and IDE RKWard, providing graphical dialogs for\n          its basic features. The respective R package 'rkward' cannot be installed\n          directly from a repository, as it is a part of RKWard. To make full use of this\n          feature, please install RKWard from <https://rkward.kde.org> (plugins are\n          detected automatically). Due to some restrictions on CRAN, the full package\n          sources are only available from the project homepage.  "
  },
  {
    "id": 14909,
    "package_name": "kmeRtone",
    "title": "Multi-Purpose and Flexible k-Meric Enrichment Analysis Software",
    "description": "A multi-purpose and flexible k-meric enrichment analysis software. \n    'kmeRtone' measures the enrichment of k-mers by comparing the population of \n    k-mers in the case loci with a carefully devised internal negative control \n    group, consisting of k-mers from regions close to, yet sufficiently distant \n    from, the case loci to mitigate any potential sequencing bias. This method \n    effectively captures both the local sequencing variations and broader \n    sequence influences, while also correcting for potential biases, thereby \n    ensuring more accurate analysis. The core functionality of 'kmeRtone' is \n    the SCORE() function, which calculates the susceptibility scores for k-mers in \n    case and control regions. Case regions are defined by the genomic coordinates \n    provided in a file by the user and the control regions can be constructed \n    relative to the case regions or provided directly. The k-meric susceptibility \n    scores are calculated by using a one-proportion z-statistic. 'kmeRtone' is \n    highly flexible by allowing users to also specify their target k-mer patterns\n    and quantify the corresponding k-mer enrichment scores in the context of \n    these patterns, allowing for a more comprehensive approach to understanding \n    the functional implications of specific DNA sequences on a genomic scale\n    (e.g., CT motifs upon UV radiation damage).\n    Adib A. Abdullah, Patrick Pflughaupt, Claudia Feng, Aleksandr B. Sahakyan (2024) Bioinformatics (submitted).",
    "version": "1.0",
    "maintainer": "Aleksandr Sahakyan <sahakyanlab@cantab.net>",
    "author": "Adib Abdullah [aut],\n  Patrick Pflughaupt [aut],\n  Aleksandr Sahakyan [aut, cre]",
    "url": "https://github.com/SahakyanLab/kmeRtone",
    "bug_reports": "https://github.com/SahakyanLab/kmeRtone/issues",
    "repository": "https://cran.r-project.org/package=kmeRtone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kmeRtone Multi-Purpose and Flexible k-Meric Enrichment Analysis Software A multi-purpose and flexible k-meric enrichment analysis software. \n    'kmeRtone' measures the enrichment of k-mers by comparing the population of \n    k-mers in the case loci with a carefully devised internal negative control \n    group, consisting of k-mers from regions close to, yet sufficiently distant \n    from, the case loci to mitigate any potential sequencing bias. This method \n    effectively captures both the local sequencing variations and broader \n    sequence influences, while also correcting for potential biases, thereby \n    ensuring more accurate analysis. The core functionality of 'kmeRtone' is \n    the SCORE() function, which calculates the susceptibility scores for k-mers in \n    case and control regions. Case regions are defined by the genomic coordinates \n    provided in a file by the user and the control regions can be constructed \n    relative to the case regions or provided directly. The k-meric susceptibility \n    scores are calculated by using a one-proportion z-statistic. 'kmeRtone' is \n    highly flexible by allowing users to also specify their target k-mer patterns\n    and quantify the corresponding k-mer enrichment scores in the context of \n    these patterns, allowing for a more comprehensive approach to understanding \n    the functional implications of specific DNA sequences on a genomic scale\n    (e.g., CT motifs upon UV radiation damage).\n    Adib A. Abdullah, Patrick Pflughaupt, Claudia Feng, Aleksandr B. Sahakyan (2024) Bioinformatics (submitted).  "
  },
  {
    "id": 14964,
    "package_name": "ktweedie",
    "title": "'Tweedie' Compound Poisson Model in the Reproducing Kernel\nHilbert Space",
    "description": "Kernel-based 'Tweedie' compound Poisson gamma model using high-dimensional predictors for the analyses of zero-inflated response variables. The package features built-in estimation, prediction and cross-validation tools and supports choice of different kernel functions. For more details, please see Yi Lian, Archer Yi Yang, Boxiang Wang, Peng Shi & Robert William Platt (2023) <doi:10.1080/00401706.2022.2156615>.",
    "version": "1.0.3",
    "maintainer": "Yi Lian <yi.lian@mail.mcgill.ca>",
    "author": "Yi Lian [aut, cre],\n  Archer Yi Yang [aut, cph],\n  Boxiang Wang [aut],\n  Peng Shi [aut],\n  Robert W. Platt [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ktweedie",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ktweedie 'Tweedie' Compound Poisson Model in the Reproducing Kernel\nHilbert Space Kernel-based 'Tweedie' compound Poisson gamma model using high-dimensional predictors for the analyses of zero-inflated response variables. The package features built-in estimation, prediction and cross-validation tools and supports choice of different kernel functions. For more details, please see Yi Lian, Archer Yi Yang, Boxiang Wang, Peng Shi & Robert William Platt (2023) <doi:10.1080/00401706.2022.2156615>.  "
  },
  {
    "id": 15026,
    "package_name": "languageserversetup",
    "title": "Automated Setup and Auto Run for R Language Server",
    "description": "Allows to install the R 'languageserver' with all dependencies\n    into a separate library and use that independent installation\n    automatically when R is instantiated as a language server process.\n    Useful for making language server seamless to use without\n    running into package version conflicts.",
    "version": "0.1.2",
    "maintainer": "Jozef Hajnala <jozef.hajnala@gmail.com>",
    "author": "Jozef Hajnala [aut, cre]",
    "url": "https://github.com/jozefhajnala/languageserversetup",
    "bug_reports": "https://github.com/jozefhajnala/languageserversetup/issues",
    "repository": "https://cran.r-project.org/package=languageserversetup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "languageserversetup Automated Setup and Auto Run for R Language Server Allows to install the R 'languageserver' with all dependencies\n    into a separate library and use that independent installation\n    automatically when R is instantiated as a language server process.\n    Useful for making language server seamless to use without\n    running into package version conflicts.  "
  },
  {
    "id": 15034,
    "package_name": "latdiag",
    "title": "Draws Diagrams Useful for Checking Latent Scales",
    "description": "A graph\n  proposed by Rosenbaum is useful\n  for checking some properties of various\n  sorts of latent scale, this program generates commands\n  to obtain the graph using 'dot' from 'graphviz'.",
    "version": "0.3",
    "maintainer": "Michael Dewey <lists@dewey.myzen.co.uk>",
    "author": "Michael Dewey [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7522-3677>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=latdiag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "latdiag Draws Diagrams Useful for Checking Latent Scales A graph\n  proposed by Rosenbaum is useful\n  for checking some properties of various\n  sorts of latent scale, this program generates commands\n  to obtain the graph using 'dot' from 'graphviz'.  "
  },
  {
    "id": 15038,
    "package_name": "latentcor",
    "title": "Fast Computation of Latent Correlations for Mixed Data",
    "description": "The first stand-alone R package for computation of latent correlation that takes into account all variable types (continuous/binary/ordinal/zero-inflated),\n             comes with an optimized memory footprint, and is computationally efficient, essentially making latent correlation estimation almost as fast as rank-based correlation estimation.\n             The estimation is based on latent copula Gaussian models.\n             For continuous/binary types, see Fan, J., Liu, H., Ning, Y., and Zou, H. (2017).\n             For ternary type, see Quan X., Booth J.G. and Wells M.T. (2018) <doi:10.48550/arXiv.1809.06255>.\n             For truncated type or zero-inflated type, see Yoon G., Carroll R.J. and Gaynanova I. (2020) <doi:10.1093/biomet/asaa007>.\n             For approximation method of computation, see Yoon G., M\u00fcller C.L. and Gaynanova I. (2021) <doi:10.1080/10618600.2021.1882468>. The latter method uses multi-linear interpolation originally implemented in the R package <https://cran.r-project.org/package=chebpol>.",
    "version": "2.0.2",
    "maintainer": "Irina Gaynanova <irinagn@umich.edu>",
    "author": "Mingze Huang [aut] (ORCID: <https://orcid.org/0000-0003-3919-1564>),\n  Grace Yoon [aut] (ORCID: <https://orcid.org/0000-0003-3263-1352>),\n  Christian M&uuml;ller [aut] (ORCID:\n    <https://orcid.org/0000-0002-3821-7083>),\n  Irina Gaynanova [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4116-0268>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=latentcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "latentcor Fast Computation of Latent Correlations for Mixed Data The first stand-alone R package for computation of latent correlation that takes into account all variable types (continuous/binary/ordinal/zero-inflated),\n             comes with an optimized memory footprint, and is computationally efficient, essentially making latent correlation estimation almost as fast as rank-based correlation estimation.\n             The estimation is based on latent copula Gaussian models.\n             For continuous/binary types, see Fan, J., Liu, H., Ning, Y., and Zou, H. (2017).\n             For ternary type, see Quan X., Booth J.G. and Wells M.T. (2018) <doi:10.48550/arXiv.1809.06255>.\n             For truncated type or zero-inflated type, see Yoon G., Carroll R.J. and Gaynanova I. (2020) <doi:10.1093/biomet/asaa007>.\n             For approximation method of computation, see Yoon G., M\u00fcller C.L. and Gaynanova I. (2021) <doi:10.1080/10618600.2021.1882468>. The latter method uses multi-linear interpolation originally implemented in the R package <https://cran.r-project.org/package=chebpol>.  "
  },
  {
    "id": 15050,
    "package_name": "lava",
    "title": "Latent Variable Models",
    "description": "A general implementation of Structural Equation Models\n\twith latent variables (MLE, 2SLS, and composite likelihood\n\testimators) with both continuous, censored, and ordinal\n\toutcomes (Holst and Budtz-Joergensen (2013) <doi:10.1007/s00180-012-0344-y>).\n\tMixture latent variable models and non-linear latent variable models\n\t(Holst and Budtz-Joergensen (2020) <doi:10.1093/biostatistics/kxy082>).\n\tThe package also provides methods for graph exploration (d-separation,\n\tback-door criterion), simulation of general non-linear latent variable\n\tmodels, and estimation of influence functions for a broad range of\n\tstatistical models.",
    "version": "1.8.2",
    "maintainer": "Klaus K. Holst <klaus@holst.it>",
    "author": "Klaus K. Holst [aut, cre],\n  Brice Ozenne [ctb],\n  Thomas Gerds [ctb]",
    "url": "https://kkholst.github.io/lava/",
    "bug_reports": "https://github.com/kkholst/lava/issues",
    "repository": "https://cran.r-project.org/package=lava",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lava Latent Variable Models A general implementation of Structural Equation Models\n\twith latent variables (MLE, 2SLS, and composite likelihood\n\testimators) with both continuous, censored, and ordinal\n\toutcomes (Holst and Budtz-Joergensen (2013) <doi:10.1007/s00180-012-0344-y>).\n\tMixture latent variable models and non-linear latent variable models\n\t(Holst and Budtz-Joergensen (2020) <doi:10.1093/biostatistics/kxy082>).\n\tThe package also provides methods for graph exploration (d-separation,\n\tback-door criterion), simulation of general non-linear latent variable\n\tmodels, and estimation of influence functions for a broad range of\n\tstatistical models.  "
  },
  {
    "id": 15112,
    "package_name": "leakyIV",
    "title": "Leaky Instrumental Variables",
    "description": "Instrumental variables (IVs) are a popular and powerful tool for \n  estimating causal effects in the presence of unobserved confounding. However, \n  classical methods rely on strong assumptions such as the exclusion criterion,\n  which states that instrumental effects must be entirely mediated by \n  treatments. In the so-called \"leaky\" IV setting, candidate instruments are \n  allowed to have some direct influence on outcomes, rendering the average \n  treatment effect (ATE) unidentifiable. But with limits on the amount of \n  information leakage, we may still recover sharp bounds on the ATE, providing \n  partial identification. This package implements methods for ATE bounding in \n  the leaky IV setting with linear structural equations. For details, see Watson\n  et al. (2024) <doi:10.48550/arXiv.2404.04446>.",
    "version": "0.0.1",
    "maintainer": "David S. Watson <david.s.watson11@gmail.com>",
    "author": "David S. Watson [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9632-2159>)",
    "url": "https://github.com/dswatson/leakyIV",
    "bug_reports": "https://github.com/dswatson/leakyIV/issues",
    "repository": "https://cran.r-project.org/package=leakyIV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "leakyIV Leaky Instrumental Variables Instrumental variables (IVs) are a popular and powerful tool for \n  estimating causal effects in the presence of unobserved confounding. However, \n  classical methods rely on strong assumptions such as the exclusion criterion,\n  which states that instrumental effects must be entirely mediated by \n  treatments. In the so-called \"leaky\" IV setting, candidate instruments are \n  allowed to have some direct influence on outcomes, rendering the average \n  treatment effect (ATE) unidentifiable. But with limits on the amount of \n  information leakage, we may still recover sharp bounds on the ATE, providing \n  partial identification. This package implements methods for ATE bounding in \n  the leaky IV setting with linear structural equations. For details, see Watson\n  et al. (2024) <doi:10.48550/arXiv.2404.04446>.  "
  },
  {
    "id": 15214,
    "package_name": "linbin",
    "title": "Binning and Plotting of Linearly Referenced Data",
    "description": "Short for 'linear binning', the linbin package provides functions\n    for manipulating, binning, and plotting linearly referenced data. Although\n    developed for data collected on river networks, it can be used with any interval\n    or point data referenced to a 1-dimensional coordinate system. Flexible bin\n    generation and batch processing makes it easy to compute and visualize variables\n    at multiple scales, useful for identifying patterns within and between variables\n    and investigating the influence of scale of observation on data interpretation.",
    "version": "0.1.3",
    "maintainer": "Ethan Z. Welty <ethan.welty+linbin@gmail.com>",
    "author": "Ethan Z. Welty [aut, cre],\n  Christian E. Torgersen [ctb] (author support and guidance),\n  Samuel J. Brenkman [ctb] (elwha and quinault datasets),\n  Jeffrey J. Duda [ctb] (elwha dataset),\n  Jonathan B. Armstrong [ctb] (fishmotion dataset)",
    "url": "https://github.com/ezwelty/linbin",
    "bug_reports": "https://github.com/ezwelty/linbin/issues",
    "repository": "https://cran.r-project.org/package=linbin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "linbin Binning and Plotting of Linearly Referenced Data Short for 'linear binning', the linbin package provides functions\n    for manipulating, binning, and plotting linearly referenced data. Although\n    developed for data collected on river networks, it can be used with any interval\n    or point data referenced to a 1-dimensional coordinate system. Flexible bin\n    generation and batch processing makes it easy to compute and visualize variables\n    at multiple scales, useful for identifying patterns within and between variables\n    and investigating the influence of scale of observation on data interpretation.  "
  },
  {
    "id": 15302,
    "package_name": "lmtestrob",
    "title": "Outlier Robust Specification Testing",
    "description": "Robust test(s) for model diagnostics in regression. The current version contains a robust test for functional specification (linearity). The test is based on the robust bounded-influence test by Heritier and Ronchetti (1994) <doi:10.1080/01621459.1994.10476822>.",
    "version": "0.1",
    "maintainer": "Mikhail Zhelonkin <Mikhail.Zhelonkin@gmail.com>",
    "author": "Mikhail Zhelonkin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6912-4074>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lmtestrob",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmtestrob Outlier Robust Specification Testing Robust test(s) for model diagnostics in regression. The current version contains a robust test for functional specification (linearity). The test is based on the robust bounded-influence test by Heritier and Ronchetti (1994) <doi:10.1080/01621459.1994.10476822>.  "
  },
  {
    "id": 15304,
    "package_name": "lmviz",
    "title": "A Package to Visualize Linear Models Features and Play with Them",
    "description": "Contains a suite of shiny applications\n    meant to explore linear model inference feature through simulation\n    and games.",
    "version": "0.2.0",
    "maintainer": "Francesco Pauli <francesco.pauli@deams.units.it>",
    "author": "Francesco Pauli (see file LICENSEMEDIA for credits on sounds and images)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lmviz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmviz A Package to Visualize Linear Models Features and Play with Them Contains a suite of shiny applications\n    meant to explore linear model inference feature through simulation\n    and games.  "
  },
  {
    "id": 15305,
    "package_name": "lmw",
    "title": "Linear Model Weights",
    "description": "Computes the implied weights of linear regression models for estimating\n     average causal effects and provides diagnostics based on these weights. These\n     diagnostics rely on the analyses in Chattopadhyay and Zubizarreta (2023)\n     <doi:10.1093/biomet/asac058> where\n     several regression estimators are represented as weighting estimators, in connection\n     to inverse probability weighting. 'lmw' provides tools to diagnose\n     representativeness, balance, extrapolation, and influence for these models,\n     clarifying the target population of inference. Tools are also available to\n     simplify estimating treatment effects for specific target populations of interest.",
    "version": "0.0.2",
    "maintainer": "Noah Greifer <ngreifer@iq.harvard.edu>",
    "author": "Ambarish Chattopadhyay [aut] (ORCID:\n    <https://orcid.org/0000-0002-1502-0974>),\n  Noah Greifer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3067-7154>),\n  Jose Zubizarreta [aut] (ORCID: <https://orcid.org/0000-0002-0322-147X>)",
    "url": "https://github.com/ngreifer/lmw",
    "bug_reports": "https://github.com/ngreifer/lmw/issues",
    "repository": "https://cran.r-project.org/package=lmw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmw Linear Model Weights Computes the implied weights of linear regression models for estimating\n     average causal effects and provides diagnostics based on these weights. These\n     diagnostics rely on the analyses in Chattopadhyay and Zubizarreta (2023)\n     <doi:10.1093/biomet/asac058> where\n     several regression estimators are represented as weighting estimators, in connection\n     to inverse probability weighting. 'lmw' provides tools to diagnose\n     representativeness, balance, extrapolation, and influence for these models,\n     clarifying the target population of inference. Tools are also available to\n     simplify estimating treatment effects for specific target populations of interest.  "
  },
  {
    "id": 15306,
    "package_name": "lncDIFF",
    "title": "Long Non-Coding RNA Differential Expression Analysis",
    "description": "We developed an approach to detect differential expression features in long non-coding RNA low counts, using generalized linear model with zero-inflated exponential quasi likelihood ratio test. Methods implemented in this package are described in Li (2019) <doi:10.1186/s12864-019-5926-4>. ",
    "version": "1.0.0",
    "maintainer": "Qian Li <qian.li10000@gmail.com>",
    "author": "Qian Li [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lncDIFF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lncDIFF Long Non-Coding RNA Differential Expression Analysis We developed an approach to detect differential expression features in long non-coding RNA low counts, using generalized linear model with zero-inflated exponential quasi likelihood ratio test. Methods implemented in this package are described in Li (2019) <doi:10.1186/s12864-019-5926-4>.   "
  },
  {
    "id": 15325,
    "package_name": "locationgamer",
    "title": "Identification of Location Game Equilibria in Networks",
    "description": "Identification of equilibrium locations in location games (Hotelling (1929) \n    <doi:10.2307/2224214>). In these games, two competing actors place\n    customer-serving units in two locations simultaneously. Customers make the \n    decision to visit the location that is closest to them. The functions in \n    this package include Prim algorithm (Prim (1957) \n    <doi:10.1002/j.1538-7305.1957.tb01515.x>) to find the minimum spanning tree \n    connecting all network vertices, an implementation of Dijkstra algorithm \n    (Dijkstra (1959) <doi:10.1007/BF01386390>) to find the shortest distance and \n    path between any two vertices, a self-developed algorithm using elimination \n    of purely dominated strategies to find the equilibrium, and several plotting \n    functions.",
    "version": "0.1.0",
    "maintainer": "Maximilian Zellner <zellnermaximilian@gmail.com>",
    "author": "Maximilian Zellner",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=locationgamer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "locationgamer Identification of Location Game Equilibria in Networks Identification of equilibrium locations in location games (Hotelling (1929) \n    <doi:10.2307/2224214>). In these games, two competing actors place\n    customer-serving units in two locations simultaneously. Customers make the \n    decision to visit the location that is closest to them. The functions in \n    this package include Prim algorithm (Prim (1957) \n    <doi:10.1002/j.1538-7305.1957.tb01515.x>) to find the minimum spanning tree \n    connecting all network vertices, an implementation of Dijkstra algorithm \n    (Dijkstra (1959) <doi:10.1007/BF01386390>) to find the shortest distance and \n    path between any two vertices, a self-developed algorithm using elimination \n    of purely dominated strategies to find the equilibrium, and several plotting \n    functions.  "
  },
  {
    "id": 15344,
    "package_name": "logcondiscr",
    "title": "Estimate a Log-Concave Probability Mass Function from Discrete\ni.i.d. Observations",
    "description": "Given independent and identically distributed observations X(1), ..., X(n), allows to compute the maximum likelihood estimator (MLE) of probability mass function (pmf) under the assumption that it is log-concave, see Weyermann (2007) and Balabdaoui, Jankowski, Rufibach, and Pavlides (2012). The main functions of the package are 'logConDiscrMLE' that allows computation of the log-concave MLE, 'logConDiscrCI' that computes pointwise confidence bands for the MLE, and 'kInflatedLogConDiscr' that computes a mixture of a log-concave PMF and a point mass at k.",
    "version": "1.0.6",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "author": "Kaspar Rufibach <kaspar.rufibach@gmail.com> and Fadoua Balabdaoui <fadoua@ceremade.dauphine.fr> and Hanna Jankowski <hkj@mathstat.yorku.ca> and Kathrin Weyermann <kathrin.weyermann@bkw-fmb.ch>",
    "url": "http://www.kasparrufibach.ch ,\nhttp://www.ceremade.dauphine.fr/~fadoua ,\nhttp://www.math.yorku.ca/~hkj",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=logcondiscr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "logcondiscr Estimate a Log-Concave Probability Mass Function from Discrete\ni.i.d. Observations Given independent and identically distributed observations X(1), ..., X(n), allows to compute the maximum likelihood estimator (MLE) of probability mass function (pmf) under the assumption that it is log-concave, see Weyermann (2007) and Balabdaoui, Jankowski, Rufibach, and Pavlides (2012). The main functions of the package are 'logConDiscrMLE' that allows computation of the log-concave MLE, 'logConDiscrCI' that computes pointwise confidence bands for the MLE, and 'kInflatedLogConDiscr' that computes a mixture of a log-concave PMF and a point mass at k.  "
  },
  {
    "id": 15449,
    "package_name": "ltm",
    "title": "Latent Trait Models under IRT",
    "description": "Analysis of multivariate dichotomous and polytomous data using latent trait models under the Item Response Theory approach. It includes the Rasch, the Two-Parameter Logistic, the Birnbaum's Three-Parameter, the Graded Response, and the Generalized Partial Credit Models.",
    "version": "1.2-0",
    "maintainer": "Dimitris Rizopoulos <d.rizopoulos@erasmusmc.nl>",
    "author": "Dimitris Rizopoulos <d.rizopoulos@erasmusmc.nl>",
    "url": "https://github.com/drizopoulos/ltm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ltm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ltm Latent Trait Models under IRT Analysis of multivariate dichotomous and polytomous data using latent trait models under the Item Response Theory approach. It includes the Rasch, the Two-Parameter Logistic, the Birnbaum's Three-Parameter, the Graded Response, and the Generalized Partial Credit Models.  "
  },
  {
    "id": 15481,
    "package_name": "mBvs",
    "title": "Bayesian Variable Selection Methods for Multivariate Data",
    "description": "Bayesian variable selection methods for data with multivariate responses and multiple covariates. The package contains implementations of multivariate Bayesian variable selection methods for continuous data (Lee et al., Biometrics, 2017 <doi:10.1111/biom.12557>) and zero-inflated count data (Lee et al., Biostatistics, 2020 <doi:10.1093/biostatistics/kxy067>).",
    "version": "1.92",
    "maintainer": "Kyu Ha Lee <klee15239@gmail.com>",
    "author": "Kyu Ha Lee, Mahlet G. Tadesse, Brent A. Coull, Jacqueline R. Starr",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mBvs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mBvs Bayesian Variable Selection Methods for Multivariate Data Bayesian variable selection methods for data with multivariate responses and multiple covariates. The package contains implementations of multivariate Bayesian variable selection methods for continuous data (Lee et al., Biometrics, 2017 <doi:10.1111/biom.12557>) and zero-inflated count data (Lee et al., Biostatistics, 2020 <doi:10.1093/biostatistics/kxy067>).  "
  },
  {
    "id": 15513,
    "package_name": "maczic",
    "title": "Mediation Analysis for Count and Zero-Inflated Count Data",
    "description": "Performs causal mediation analysis for count and zero-inflated\n    count data without or with a post-treatment confounder; calculates power\n    to detect prespecified causal mediation effects, direct effects, and \n    total effects; performs sensitivity analysis when there is a treatment-\n    induced mediator-outcome confounder as described by Cheng, J., Cheng, N.F., \n    Guo, Z., Gregorich, S., Ismail, A.I., Gansky, S.A. (2018) \n    <doi:10.1177/0962280216686131>. Implements Instrumental Variable (IV) \n    method to estimate the controlled (natural) direct and mediation effects, \n    and compute the bootstrap Confidence Intervals as described by Guo, Z., \n    Small, D.S., Gansky, S.A., Cheng, J. (2018) <doi:10.1111/rssc.12233>. This \n    software was made possible by Grant R03DE028410 from the National \n    Institute of Dental and Craniofacial Research, a component of the National \n    Institutes of Health.",
    "version": "1.1.0",
    "maintainer": "Nancy Cheng <Nancy.Cheng@ucsf.edu>",
    "author": "Nancy Cheng [aut, cre],\n  Zijian Guo [aut],\n  Jing Cheng [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=maczic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maczic Mediation Analysis for Count and Zero-Inflated Count Data Performs causal mediation analysis for count and zero-inflated\n    count data without or with a post-treatment confounder; calculates power\n    to detect prespecified causal mediation effects, direct effects, and \n    total effects; performs sensitivity analysis when there is a treatment-\n    induced mediator-outcome confounder as described by Cheng, J., Cheng, N.F., \n    Guo, Z., Gregorich, S., Ismail, A.I., Gansky, S.A. (2018) \n    <doi:10.1177/0962280216686131>. Implements Instrumental Variable (IV) \n    method to estimate the controlled (natural) direct and mediation effects, \n    and compute the bootstrap Confidence Intervals as described by Guo, Z., \n    Small, D.S., Gansky, S.A., Cheng, J. (2018) <doi:10.1111/rssc.12233>. This \n    software was made possible by Grant R03DE028410 from the National \n    Institute of Dental and Craniofacial Research, a component of the National \n    Institutes of Health.  "
  },
  {
    "id": 15673,
    "package_name": "matrisk",
    "title": "Macroeconomic-at-Risk",
    "description": "The Macroeconomics-at-Risk (MaR) approach is based on a two-step semi-parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the MaR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.",
    "version": "0.1.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Quentin Lajaunie [aut, cre],\n  Guillaume Flament [aut],\n  Christophe Hurlin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrisk Macroeconomic-at-Risk The Macroeconomics-at-Risk (MaR) approach is based on a two-step semi-parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the MaR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.  "
  },
  {
    "id": 15682,
    "package_name": "matrixcut",
    "title": "Determines Clustering Threshold Based on Similarity Values",
    "description": "The user must supply a matrix filled with similarity values. The software will search for significant differences between similarity values at different hierarchical levels. The algorithm will return a Loess-smoothed plot of the similarity values along with the inflection point, if there are any. There is the option to search for an inflection point within a specified range. The package also has a function that will return the matrix components at a specified cutoff. References: Mullner. <ArXiv:1109.2378>; Cserhati, Carter. (2020, Journal of Creation 34(3):41-50), <https://dl0.creation.com/articles/p137/c13759/j34-3_64-73.pdf>.",
    "version": "0.0.1",
    "maintainer": "Matthew Cserhati <csmatyi@protonmail.com>",
    "author": "Matthew Cserhati [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3673-9152>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matrixcut",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrixcut Determines Clustering Threshold Based on Similarity Values The user must supply a matrix filled with similarity values. The software will search for significant differences between similarity values at different hierarchical levels. The algorithm will return a Loess-smoothed plot of the similarity values along with the inflection point, if there are any. There is the option to search for an inflection point within a specified range. The package also has a function that will return the matrix components at a specified cutoff. References: Mullner. <ArXiv:1109.2378>; Cserhati, Carter. (2020, Journal of Creation 34(3):41-50), <https://dl0.creation.com/articles/p137/c13759/j34-3_64-73.pdf>.  "
  },
  {
    "id": 15686,
    "package_name": "matrixsampling",
    "title": "Simulations of Matrix Variate Distributions",
    "description": "Provides samplers for various matrix variate distributions: Wishart, inverse-Wishart, normal, t, inverted-t, Beta type I, Beta type II, Gamma, confluent hypergeometric. Allows to simulate the noncentral Wishart distribution without the integer restriction on the degrees of freedom.",
    "version": "2.0.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent",
    "url": "https://github.com/stla/matrixsampling",
    "bug_reports": "https://github.com/stla/matrixsampling/issues",
    "repository": "https://cran.r-project.org/package=matrixsampling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrixsampling Simulations of Matrix Variate Distributions Provides samplers for various matrix variate distributions: Wishart, inverse-Wishart, normal, t, inverted-t, Beta type I, Beta type II, Gamma, confluent hypergeometric. Allows to simulate the noncentral Wishart distribution without the integer restriction on the degrees of freedom.  "
  },
  {
    "id": 15691,
    "package_name": "matuR",
    "title": "Athlete Maturation and Biobanding",
    "description": "Identifying maturation stages across young athletes is paramount for talent identification. Furthermore, the concept of biobanding, or grouping of athletes based on their biological development, instead of their chronological age, has been widely researched. The goal of this package is to help professionals working in the field of strength & conditioning and talent ID obtain common maturation metrics and as well as to quickly visualize this information via several plotting options. For the methods behind the computed maturation metrics implemented in this package refer to Khamis, H. J., & Roche, A. F. (1994) <https://pubmed.ncbi.nlm.nih.gov/7936860/>, Mirwald, R.L et al., (2002) <https://pubmed.ncbi.nlm.nih.gov/11932580/> and Cumming, Sean P. et al., (2017) <doi:10.1519/SSC.0000000000000281>. ",
    "version": "0.0.1.0",
    "maintainer": "Jose Fernandez <jose.fernandezdv@gmail.com>",
    "author": "Jose Fernandez [aut, cre]",
    "url": "https://github.com/josedv82/matuR",
    "bug_reports": "https://github.com/josedv82/matuR/issues",
    "repository": "https://cran.r-project.org/package=matuR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matuR Athlete Maturation and Biobanding Identifying maturation stages across young athletes is paramount for talent identification. Furthermore, the concept of biobanding, or grouping of athletes based on their biological development, instead of their chronological age, has been widely researched. The goal of this package is to help professionals working in the field of strength & conditioning and talent ID obtain common maturation metrics and as well as to quickly visualize this information via several plotting options. For the methods behind the computed maturation metrics implemented in this package refer to Khamis, H. J., & Roche, A. F. (1994) <https://pubmed.ncbi.nlm.nih.gov/7936860/>, Mirwald, R.L et al., (2002) <https://pubmed.ncbi.nlm.nih.gov/11932580/> and Cumming, Sean P. et al., (2017) <doi:10.1519/SSC.0000000000000281>.   "
  },
  {
    "id": 15708,
    "package_name": "mazeinda",
    "title": "Monotonic Association on Zero-Inflated Data",
    "description": "Methods for calculating and testing the significance of\n  pairwise monotonic association from and based on the work of\n  Pimentel (2009) <doi:10.4135/9781412985291.n2>. Computation of association of vectors from one\n  or multiple sets can be performed in parallel thanks to the\n  packages 'foreach' and 'doMC'.",
    "version": "0.0.2",
    "maintainer": "Alice Albasi <albasialice@gmail.com>",
    "author": "Alice Albasi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mazeinda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mazeinda Monotonic Association on Zero-Inflated Data Methods for calculating and testing the significance of\n  pairwise monotonic association from and based on the work of\n  Pimentel (2009) <doi:10.4135/9781412985291.n2>. Computation of association of vectors from one\n  or multiple sets can be performed in parallel thanks to the\n  packages 'foreach' and 'doMC'.  "
  },
  {
    "id": 15763,
    "package_name": "mcount",
    "title": "Marginalized Count Regression Models",
    "description": "Implementation of marginalized models for zero-inflated count data. This package provides a tool to implement an estimation algorithm for the marginalized count models, which\n  directly makes inference on the effect of each covariate on the marginal mean of the outcome. \n  The method involves the marginalized zero-inflated Poisson model described \n  in Long et al. (2014) <doi:10.1002/sim.6293>.",
    "version": "1.0.0",
    "maintainer": "Zhengyang Zhou <zhengyang.zhou@unthsc.edu>",
    "author": "Zhengyang Zhou [aut, cre]\n        Dateng Li [aut]\n        David Huh [aut]\n        Eun-Young Mun [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcount Marginalized Count Regression Models Implementation of marginalized models for zero-inflated count data. This package provides a tool to implement an estimation algorithm for the marginalized count models, which\n  directly makes inference on the effect of each covariate on the marginal mean of the outcome. \n  The method involves the marginalized zero-inflated Poisson model described \n  in Long et al. (2014) <doi:10.1002/sim.6293>.  "
  },
  {
    "id": 15822,
    "package_name": "mediationsens",
    "title": "Simulation-Based Sensitivity Analysis for Causal Mediation\nStudies",
    "description": "Simulation-based sensitivity analysis for causal mediation studies. It numerically and graphically evaluates the sensitivity of causal mediation analysis results \n to the presence of unmeasured pretreatment confounding. The proposed method has primary advantages over existing methods. \n First, using an unmeasured pretreatment confounder conditional associations with the treatment, mediator, and outcome as \n sensitivity parameters, the method enables users to intuitively assess sensitivity in reference to prior knowledge about the \n strength of a potential unmeasured pretreatment confounder. Second, the method accurately reflects the influence of unmeasured \n pretreatment confounding on the efficiency of estimation of the causal effects. Third, the method can be implemented in \n different causal mediation analysis approaches, including regression-based, simulation-based, and propensity score-based \n methods. It is applicable to both randomized experiments and observational studies.",
    "version": "0.0.3",
    "maintainer": "Xu Qin <xuqin@pitt.edu>",
    "author": "Xu Qin and Fan Yang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mediationsens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mediationsens Simulation-Based Sensitivity Analysis for Causal Mediation\nStudies Simulation-based sensitivity analysis for causal mediation studies. It numerically and graphically evaluates the sensitivity of causal mediation analysis results \n to the presence of unmeasured pretreatment confounding. The proposed method has primary advantages over existing methods. \n First, using an unmeasured pretreatment confounder conditional associations with the treatment, mediator, and outcome as \n sensitivity parameters, the method enables users to intuitively assess sensitivity in reference to prior knowledge about the \n strength of a potential unmeasured pretreatment confounder. Second, the method accurately reflects the influence of unmeasured \n pretreatment confounding on the efficiency of estimation of the causal effects. Third, the method can be implemented in \n different causal mediation analysis approaches, including regression-based, simulation-based, and propensity score-based \n methods. It is applicable to both randomized experiments and observational studies.  "
  },
  {
    "id": 15886,
    "package_name": "metabolic",
    "title": "Datasets and Functions for Reproducing Meta-Analyses",
    "description": "Dataset and functions from the meta-analysis published in Medicine & Science in Sports & Exercise. \n    It contains all the data and functions to reproduce the analysis.\n    \"Effectiveness of HIIE versus MICT in Improving Cardiometabolic Risk Factors in Health and Disease: A Meta-analysis\".\n    Felipe Mattioni Maturana, Peter Martus, Stephan Zipfel, Andreas M Nie\u00df (2020) <doi:10.1249/MSS.0000000000002506>.",
    "version": "0.1.2",
    "maintainer": "Felipe Mattioni Maturana <felipe.mattioni@med.uni-tuebingen.de>",
    "author": "Felipe Mattioni Maturana [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4221-6104>)",
    "url": "https://github.com/fmmattioni/metabolic",
    "bug_reports": "https://github.com/fmmattioni/metabolic/issues",
    "repository": "https://cran.r-project.org/package=metabolic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metabolic Datasets and Functions for Reproducing Meta-Analyses Dataset and functions from the meta-analysis published in Medicine & Science in Sports & Exercise. \n    It contains all the data and functions to reproduce the analysis.\n    \"Effectiveness of HIIE versus MICT in Improving Cardiometabolic Risk Factors in Health and Disease: A Meta-analysis\".\n    Felipe Mattioni Maturana, Peter Martus, Stephan Zipfel, Andreas M Nie\u00df (2020) <doi:10.1249/MSS.0000000000002506>.  "
  },
  {
    "id": 15905,
    "package_name": "metaheuristicOpt",
    "title": "Metaheuristic for Optimization",
    "description": "An implementation of metaheuristic algorithms for continuous optimization. Currently, the package contains the implementations of 21 algorithms, as follows: particle swarm optimization (Kennedy and Eberhart, 1995), ant lion optimizer (Mirjalili, 2015 <doi:10.1016/j.advengsoft.2015.01.010>), grey wolf optimizer (Mirjalili et al., 2014 <doi:10.1016/j.advengsoft.2013.12.007>), dragonfly algorithm (Mirjalili, 2015 <doi:10.1007/s00521-015-1920-1>), firefly algorithm (Yang, 2009 <doi:10.1007/978-3-642-04944-6_14>), genetic algorithm (Holland, 1992, ISBN:978-0262581110), grasshopper optimisation algorithm (Saremi et al., 2017 <doi:10.1016/j.advengsoft.2017.01.004>), harmony search algorithm (Mahdavi et al., 2007 <doi:10.1016/j.amc.2006.11.033>), moth flame optimizer (Mirjalili, 2015 <doi:10.1016/j.knosys.2015.07.006>, sine cosine algorithm (Mirjalili, 2016 <doi:10.1016/j.knosys.2015.12.022>),  whale optimization algorithm (Mirjalili and Lewis, 2016 <doi:10.1016/j.advengsoft.2016.01.008>), clonal selection algorithm (Castro, 2002 <doi:10.1109/TEVC.2002.1011539>), differential evolution (Das & Suganthan, 2011), shuffled frog leaping (Eusuff, Landsey & Pasha, 2006), cat swarm optimization (Chu et al., 2006), artificial bee colony algorithm (Karaboga & Akay, 2009), krill-herd algorithm (Gandomi & Alavi, 2012), cuckoo search (Yang & Deb, 2009), bat algorithm (Yang, 2012), gravitational based search (Rashedi et al., 2009) and black hole optimization (Hatamlou, 2013).",
    "version": "2.0.0",
    "maintainer": "Lala Septem Riza <lala.s.riza@upi.edu>",
    "author": "Lala Septem Riza [aut, cre],\n  Iip [aut],\n  Eddy Prasetyo Nugroho [aut],\n  Muhammad Bima Adi Prabowo [aut],\n  Enjun Junaeti [aut],\n  Ade Gafar Abdullah [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metaheuristicOpt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaheuristicOpt Metaheuristic for Optimization An implementation of metaheuristic algorithms for continuous optimization. Currently, the package contains the implementations of 21 algorithms, as follows: particle swarm optimization (Kennedy and Eberhart, 1995), ant lion optimizer (Mirjalili, 2015 <doi:10.1016/j.advengsoft.2015.01.010>), grey wolf optimizer (Mirjalili et al., 2014 <doi:10.1016/j.advengsoft.2013.12.007>), dragonfly algorithm (Mirjalili, 2015 <doi:10.1007/s00521-015-1920-1>), firefly algorithm (Yang, 2009 <doi:10.1007/978-3-642-04944-6_14>), genetic algorithm (Holland, 1992, ISBN:978-0262581110), grasshopper optimisation algorithm (Saremi et al., 2017 <doi:10.1016/j.advengsoft.2017.01.004>), harmony search algorithm (Mahdavi et al., 2007 <doi:10.1016/j.amc.2006.11.033>), moth flame optimizer (Mirjalili, 2015 <doi:10.1016/j.knosys.2015.07.006>, sine cosine algorithm (Mirjalili, 2016 <doi:10.1016/j.knosys.2015.12.022>),  whale optimization algorithm (Mirjalili and Lewis, 2016 <doi:10.1016/j.advengsoft.2016.01.008>), clonal selection algorithm (Castro, 2002 <doi:10.1109/TEVC.2002.1011539>), differential evolution (Das & Suganthan, 2011), shuffled frog leaping (Eusuff, Landsey & Pasha, 2006), cat swarm optimization (Chu et al., 2006), artificial bee colony algorithm (Karaboga & Akay, 2009), krill-herd algorithm (Gandomi & Alavi, 2012), cuckoo search (Yang & Deb, 2009), bat algorithm (Yang, 2012), gravitational based search (Rashedi et al., 2009) and black hole optimization (Hatamlou, 2013).  "
  },
  {
    "id": 15913,
    "package_name": "metamicrobiomeR",
    "title": "Microbiome Data Analysis & Meta-Analysis with GAMLSS-BEZI &\nRandom Effects",
    "description": "Generalized Additive Model for Location, Scale and Shape (GAMLSS) \n    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data \n    (with various options for data transformation/normalization to address compositional effects) and \n    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies \n    are implemented. \n    Random Forest model to predict microbiome age based on relative abundances of  \n    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), \n    comparison of multiple diversity indexes using linear/linear mixed effect models \n    and some data display/visualization are also implemented.\n    The reference paper is published by \n    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> . ",
    "version": "1.2",
    "maintainer": "Nhan Ho <nhanhocumc@gmail.com>",
    "author": "Nhan Ho [aut, cre]",
    "url": "https://github.com/nhanhocu/metamicrobiomeR",
    "bug_reports": "https://github.com/nhanhocu/metamicrobiomeR/issues",
    "repository": "https://cran.r-project.org/package=metamicrobiomeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metamicrobiomeR Microbiome Data Analysis & Meta-Analysis with GAMLSS-BEZI &\nRandom Effects Generalized Additive Model for Location, Scale and Shape (GAMLSS) \n    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data \n    (with various options for data transformation/normalization to address compositional effects) and \n    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies \n    are implemented. \n    Random Forest model to predict microbiome age based on relative abundances of  \n    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), \n    comparison of multiple diversity indexes using linear/linear mixed effect models \n    and some data display/visualization are also implemented.\n    The reference paper is published by \n    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> .   "
  },
  {
    "id": 15952,
    "package_name": "mets",
    "title": "Analysis of Multivariate Event Times",
    "description": "Implementation of various statistical models for multivariate\n    event history data <doi:10.1007/s10985-013-9244-x>. Including multivariate\n    cumulative incidence models <doi:10.1002/sim.6016>, and  bivariate random\n    effects probit models (Liability models) <doi:10.1016/j.csda.2015.01.014>.\n    Modern methods for survival analysis, including regression modelling (Cox, Fine-Gray, \n    Ghosh-Lin, Binomial regression) with fast computation of influence functions. ",
    "version": "1.3.8",
    "maintainer": "Klaus K. Holst <klaus@holst.it>",
    "author": "Klaus K. Holst [aut, cre],\n  Thomas Scheike [aut]",
    "url": "https://kkholst.github.io/mets/",
    "bug_reports": "https://github.com/kkholst/mets/issues",
    "repository": "https://cran.r-project.org/package=mets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mets Analysis of Multivariate Event Times Implementation of various statistical models for multivariate\n    event history data <doi:10.1007/s10985-013-9244-x>. Including multivariate\n    cumulative incidence models <doi:10.1002/sim.6016>, and  bivariate random\n    effects probit models (Liability models) <doi:10.1016/j.csda.2015.01.014>.\n    Modern methods for survival analysis, including regression modelling (Cox, Fine-Gray, \n    Ghosh-Lin, Binomial regression) with fast computation of influence functions.   "
  },
  {
    "id": 15990,
    "package_name": "miLineage",
    "title": "Association Tests for Microbial Lineages on a Taxonomic Tree",
    "description": "A variety of association tests for microbiome data analysis including Quasi-Conditional Association Tests (QCAT) described in Tang Z.-Z. et al.(2017) <doi:10.1093/bioinformatics/btw804> and Zero-Inflated Generalized Dirichlet Multinomial (ZIGDM) tests described in Tang Z.-Z. & Chen G. (2017, submitted).",
    "version": "2.1",
    "maintainer": "Zheng-Zheng Tang <tang@biostat.wisc.edu>",
    "author": "Zheng-Zheng Tang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=miLineage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "miLineage Association Tests for Microbial Lineages on a Taxonomic Tree A variety of association tests for microbiome data analysis including Quasi-Conditional Association Tests (QCAT) described in Tang Z.-Z. et al.(2017) <doi:10.1093/bioinformatics/btw804> and Zero-Inflated Generalized Dirichlet Multinomial (ZIGDM) tests described in Tang Z.-Z. & Chen G. (2017, submitted).  "
  },
  {
    "id": 16063,
    "package_name": "min2HalfFFD",
    "title": "Minimally Changed Two-Level Half-Fractional Factorial Designs",
    "description": "In many agricultural, engineering, industrial, post-harvest and processing experiments, the number of factor level changes and hence the total number of changes is of serious concern as such experiments may consists of hard-to-change factors where it is physically very difficult to change levels of some factors or sometime such experiments may require normalization time to obtain adequate operating condition. For this reason, run orders that offer the minimum  number of factor level changes and at the same time minimize the possible influence of systematic trend effects on the experimentation have been sought. Factorial designs with minimum changes in factors level may be preferred for such situations as these minimally changed run orders will minimize the cost of the experiments. This technique can be employed to any half replicate of two level factorial run order where the number of factors are greater than two. For method details see, Bhowmik, A., Varghese, E., Jaggi, S. and Varghese, C. (2017) <doi:10.1080/03610926.2016.1152490>. This package generates all possible minimally changed two-level half-fractional factorial designs for different experimental setups along with various statistical criteria to measure the performance of these designs through a user-friendly interface. It consist of the function minimal.2halfFFD() which launches the application interface.",
    "version": "0.1.0",
    "maintainer": "Bijoy Chanda <bijoychanda08@gmail.com>",
    "author": "Bijoy Chanda [aut, cre, ctb],\n  Arpan Bhowmik [aut, ctb],\n  Seema Jaggi [aut],\n  Eldho Varghese [aut, ctb],\n  Cini Varghese [aut],\n  Anindita Datta [aut],\n  Dibyendu Deb [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=min2HalfFFD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "min2HalfFFD Minimally Changed Two-Level Half-Fractional Factorial Designs In many agricultural, engineering, industrial, post-harvest and processing experiments, the number of factor level changes and hence the total number of changes is of serious concern as such experiments may consists of hard-to-change factors where it is physically very difficult to change levels of some factors or sometime such experiments may require normalization time to obtain adequate operating condition. For this reason, run orders that offer the minimum  number of factor level changes and at the same time minimize the possible influence of systematic trend effects on the experimentation have been sought. Factorial designs with minimum changes in factors level may be preferred for such situations as these minimally changed run orders will minimize the cost of the experiments. This technique can be employed to any half replicate of two level factorial run order where the number of factors are greater than two. For method details see, Bhowmik, A., Varghese, E., Jaggi, S. and Varghese, C. (2017) <doi:10.1080/03610926.2016.1152490>. This package generates all possible minimally changed two-level half-fractional factorial designs for different experimental setups along with various statistical criteria to measure the performance of these designs through a user-friendly interface. It consist of the function minimal.2halfFFD() which launches the application interface.  "
  },
  {
    "id": 16064,
    "package_name": "minFactorial",
    "title": "All Possible Minimally Changed Factorial Run Orders",
    "description": "In many agricultural, engineering, industrial, post-harvest and processing experiments, the number of factor level changes and hence the total number of changes is of serious concern as such experiments may consists of hard-to-change factors where it is physically very difficult to change levels of some factors or sometime such experiments may require normalization time to obtain adequate operating condition. For this reason, run orders that offer the minimum  number of factor level changes and at the same time minimize the possible influence of systematic trend effects on the experimentation have been sought. Factorial designs with minimum changes in factors level may be preferred for such situations as these minimally changed run orders will minimize the cost of the experiments. For method details see, Bhowmik, A.,Varghese, E., Jaggi, S. and Varghese, C. (2017)<doi:10.1080/03610926.2016.1152490>.This package used to construct all possible minimally changed factorial run orders for different experimental set ups along with different statistical criteria to measure the performance of these designs. It consist of the function minFactDesign().",
    "version": "0.1.0",
    "maintainer": "Bijoy Chanda <bijoychanda08@gmail.com>",
    "author": "Arpan Bhowmik [aut, ctb],\n  Bijoy Chanda [aut, cre, ctb],\n  Seema Jaggi [aut],\n  Eldho Varghese [aut, ctb],\n  Cini Varghese [aut],\n  Anindita Datta [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minFactorial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minFactorial All Possible Minimally Changed Factorial Run Orders In many agricultural, engineering, industrial, post-harvest and processing experiments, the number of factor level changes and hence the total number of changes is of serious concern as such experiments may consists of hard-to-change factors where it is physically very difficult to change levels of some factors or sometime such experiments may require normalization time to obtain adequate operating condition. For this reason, run orders that offer the minimum  number of factor level changes and at the same time minimize the possible influence of systematic trend effects on the experimentation have been sought. Factorial designs with minimum changes in factors level may be preferred for such situations as these minimally changed run orders will minimize the cost of the experiments. For method details see, Bhowmik, A.,Varghese, E., Jaggi, S. and Varghese, C. (2017)<doi:10.1080/03610926.2016.1152490>.This package used to construct all possible minimally changed factorial run orders for different experimental set ups along with different statistical criteria to measure the performance of these designs. It consist of the function minFactDesign().  "
  },
  {
    "id": 16067,
    "package_name": "minb",
    "title": "Multiple-Inflated Negative Binomial Model",
    "description": "Count data is prevalent and informative, with widespread\n    application in many fields such as social psychology, personality, and\n    public health. Classical statistical methods for the analysis of count\n    outcomes are commonly variants of the log-linear model, including\n    Poisson regression and Negative Binomial regression. However, a\n    typical problem with count data modeling is inflation, in the sense\n    that the counts are evidently accumulated on some integers. Such an\n    inflation problem could distort the distribution of the observed\n    counts, further bias estimation and increase error, making the classic\n    methods infeasible. Traditional inflated value selection methods based\n    on histogram inspection are easy to neglect true points and\n    computationally expensive in addition. Therefore, we propose a\n    multiple-inflated negative binomial model to handle count data\n    modeling with multiple inflated values, achieving data-driven inflated\n    value selection. The proposed approach provides simultaneous\n    identification of important regression predictors on the target count\n    response as well. More details about the proposed method are described in \n    Li, Y., Wu, M., Wu, M., & Ma, S. (2023) <arXiv:2309.15585>.",
    "version": "0.1.0",
    "maintainer": "Mingcong Wu <wumingcong@ruc.edu.cn>",
    "author": "Yang Li [aut],\n  Mingcong Wu [aut, cre],\n  Mengyun Wu [aut],\n  Shuangge Ma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minb Multiple-Inflated Negative Binomial Model Count data is prevalent and informative, with widespread\n    application in many fields such as social psychology, personality, and\n    public health. Classical statistical methods for the analysis of count\n    outcomes are commonly variants of the log-linear model, including\n    Poisson regression and Negative Binomial regression. However, a\n    typical problem with count data modeling is inflation, in the sense\n    that the counts are evidently accumulated on some integers. Such an\n    inflation problem could distort the distribution of the observed\n    counts, further bias estimation and increase error, making the classic\n    methods infeasible. Traditional inflated value selection methods based\n    on histogram inspection are easy to neglect true points and\n    computationally expensive in addition. Therefore, we propose a\n    multiple-inflated negative binomial model to handle count data\n    modeling with multiple inflated values, achieving data-driven inflated\n    value selection. The proposed approach provides simultaneous\n    identification of important regression predictors on the target count\n    response as well. More details about the proposed method are described in \n    Li, Y., Wu, M., Wu, M., & Ma, S. (2023) <arXiv:2309.15585>.  "
  },
  {
    "id": 16070,
    "package_name": "mineSweepR",
    "title": "Mine Sweeper Game",
    "description": "This is the very popular mine sweeper game! The game requires you to find out tiles that contain mines through clues from unmasking neighboring tiles. Each tile that does not contain a mine shows the number of mines in its adjacent tiles. If you unmask all tiles that do not contain mines, you win the game; if you unmask any tile that contains a mine, you lose the game. For further game instructions, please run `help(run_game)` and check details. This game runs in X11-compatible devices with `grDevices::x11()`.",
    "version": "0.1.1",
    "maintainer": "Xiurui Zhu <zxr6@163.com>",
    "author": "Xiurui Zhu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mineSweepR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mineSweepR Mine Sweeper Game This is the very popular mine sweeper game! The game requires you to find out tiles that contain mines through clues from unmasking neighboring tiles. Each tile that does not contain a mine shows the number of mines in its adjacent tiles. If you unmask all tiles that do not contain mines, you win the game; if you unmask any tile that contains a mine, you lose the game. For further game instructions, please run `help(run_game)` and check details. This game runs in X11-compatible devices with `grDevices::x11()`.  "
  },
  {
    "id": 16073,
    "package_name": "minesweeper",
    "title": "Play Minesweeper",
    "description": "Play and record games of minesweeper using a graphics device that supports event handling.\n    Replay recorded games and save GIF animations of them. Based on classic minesweeper as detailed by\n    Crow P. (1997) <https://minesweepergame.com/math/a-mathematical-introduction-to-the-game-of-minesweeper-1997.pdf>.",
    "version": "1.0.1",
    "maintainer": "Harry Thompson <harry@mayesfield.uk>",
    "author": "Harry Thompson [aut, cre, cph]",
    "url": "https://github.com/hrryt/minesweeper",
    "bug_reports": "https://github.com/hrryt/minesweeper/issues",
    "repository": "https://cran.r-project.org/package=minesweeper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minesweeper Play Minesweeper Play and record games of minesweeper using a graphics device that supports event handling.\n    Replay recorded games and save GIF animations of them. Based on classic minesweeper as detailed by\n    Crow P. (1997) <https://minesweepergame.com/math/a-mathematical-introduction-to-the-game-of-minesweeper-1997.pdf>.  "
  },
  {
    "id": 16091,
    "package_name": "minorparties",
    "title": "Quantitatively Analyze Minor Political Parties",
    "description": "Tools for calculating I-Scores, a simple way to measure how successful minor political parties are at influencing the major parties in their environment. I-Scores are designed to be a more comprehensive measurement of minor party success than vote share and legislative seats won, the current standard measurements, which do not reflect the strategies that most minor parties employ. The procedure leverages the Manifesto Project's NLP model to identify the issue areas that sentences discuss, see Burst et al. (2024) <doi:10.25522/manifesto.manifestoberta.56topics.context.2024.1.1>, and the Wordfish algorithm to estimate the relative positions that platforms take on those issue areas, see Slapin and Proksch (2008) <doi:10.1111/j.1540-5907.2008.00338.x>.",
    "version": "1.0.0",
    "maintainer": "Theodore Gercken <tgercken@hamilton.edu>",
    "author": "Theodore Gercken [aut, cre, cph]",
    "url": "https://gerckentheodore.github.io/minorparties/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minorparties",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minorparties Quantitatively Analyze Minor Political Parties Tools for calculating I-Scores, a simple way to measure how successful minor political parties are at influencing the major parties in their environment. I-Scores are designed to be a more comprehensive measurement of minor party success than vote share and legislative seats won, the current standard measurements, which do not reflect the strategies that most minor parties employ. The procedure leverages the Manifesto Project's NLP model to identify the issue areas that sentences discuss, see Burst et al. (2024) <doi:10.25522/manifesto.manifestoberta.56topics.context.2024.1.1>, and the Wordfish algorithm to estimate the relative positions that platforms take on those issue areas, see Slapin and Proksch (2008) <doi:10.1111/j.1540-5907.2008.00338.x>.  "
  },
  {
    "id": 16102,
    "package_name": "mirt",
    "title": "Multidimensional Item Response Theory",
    "description": "Analysis of discrete response data using\n    unidimensional and multidimensional item analysis models under the Item\n    Response Theory paradigm (Chalmers (2012) <doi:10.18637/jss.v048.i06>). \n    Exploratory and confirmatory item factor analysis models\n\tare estimated with quadrature (EM) or stochastic (MHRM) methods. Confirmatory\n    bi-factor and two-tier models are available for modeling item testlets using\n\tdimension reduction EM algorithms, while multiple group analyses and \n\tmixed effects designs are included for detecting differential item, bundle, \n\tand test functioning, and for modeling item and person covariates. \n\tFinally, latent class models such as the DINA, DINO, multidimensional latent class, \n\tmixture IRT models, and zero-inflated response models are supported, as well \n\tas a wide family of probabilistic unfolding models.",
    "version": "1.45.1",
    "maintainer": "Phil Chalmers <rphilip.chalmers@gmail.com>",
    "author": "Phil Chalmers [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5332-2810>),\n  Joshua Pritikin [ctb],\n  Alexander Robitzsch [ctb],\n  Mateusz Zoltak [ctb],\n  KwonHyun Kim [ctb],\n  Carl F. Falk [ctb],\n  Adam Meade [ctb],\n  Lennart Schneider [ctb],\n  David King [ctb],\n  Chen-Wei Liu [ctb],\n  Ogreden Oguzhan [ctb]",
    "url": "https://philchalmers.github.io/mirt/,\nhttps://github.com/philchalmers/mirt/wiki,\nhttps://groups.google.com/forum/#!forum/mirt-package",
    "bug_reports": "https://github.com/philchalmers/mirt/issues?state=open",
    "repository": "https://cran.r-project.org/package=mirt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mirt Multidimensional Item Response Theory Analysis of discrete response data using\n    unidimensional and multidimensional item analysis models under the Item\n    Response Theory paradigm (Chalmers (2012) <doi:10.18637/jss.v048.i06>). \n    Exploratory and confirmatory item factor analysis models\n\tare estimated with quadrature (EM) or stochastic (MHRM) methods. Confirmatory\n    bi-factor and two-tier models are available for modeling item testlets using\n\tdimension reduction EM algorithms, while multiple group analyses and \n\tmixed effects designs are included for detecting differential item, bundle, \n\tand test functioning, and for modeling item and person covariates. \n\tFinally, latent class models such as the DINA, DINO, multidimensional latent class, \n\tmixture IRT models, and zero-inflated response models are supported, as well \n\tas a wide family of probabilistic unfolding models.  "
  },
  {
    "id": 16128,
    "package_name": "missalpha",
    "title": "Find Range of Cronbach Alpha with a Dataset Including Missing\nData",
    "description": "Provides functions to calculate the minimum and maximum possible \n    values of Cronbach's alpha when item-level missing data are present. \n    Cronbach's alpha (Cronbach, 1951 <doi:10.1007/BF02310555>) is one of the most widely used \n    measures of internal consistency in the social, behavioral, and medical sciences \n    (Bland & Altman, 1997 <doi:10.1136/bmj.314.7080.572>; Tavakol & Dennick, 2011 \n    <doi:10.5116/ijme.4dfb.8dfd>). However, conventional implementations assume \n    complete data, and listwise deletion is often applied when missingness occurs, \n    which can lead to biased or overly optimistic reliability estimates (Enders, 2003 \n    <doi:10.1037/1082-989X.8.3.322>). This package implements computational strategies \n    including enumeration, Monte Carlo sampling, and optimization algorithms \n    (e.g., Genetic Algorithm, Differential Evolution, Sequential Least Squares \n    Programming) to obtain sharp lower and upper bounds of Cronbach's alpha under \n    arbitrary missing data patterns. The approach is motivated by Manski's partial \n    identification framework and pessimistic bounding ideas from optimization literature.",
    "version": "0.2.0",
    "maintainer": "Biying Zhou <biying.zhou@psu.edu>",
    "author": "Feng Ji [aut],\n  Biying Zhou [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=missalpha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "missalpha Find Range of Cronbach Alpha with a Dataset Including Missing\nData Provides functions to calculate the minimum and maximum possible \n    values of Cronbach's alpha when item-level missing data are present. \n    Cronbach's alpha (Cronbach, 1951 <doi:10.1007/BF02310555>) is one of the most widely used \n    measures of internal consistency in the social, behavioral, and medical sciences \n    (Bland & Altman, 1997 <doi:10.1136/bmj.314.7080.572>; Tavakol & Dennick, 2011 \n    <doi:10.5116/ijme.4dfb.8dfd>). However, conventional implementations assume \n    complete data, and listwise deletion is often applied when missingness occurs, \n    which can lead to biased or overly optimistic reliability estimates (Enders, 2003 \n    <doi:10.1037/1082-989X.8.3.322>). This package implements computational strategies \n    including enumeration, Monte Carlo sampling, and optimization algorithms \n    (e.g., Genetic Algorithm, Differential Evolution, Sequential Least Squares \n    Programming) to obtain sharp lower and upper bounds of Cronbach's alpha under \n    arbitrary missing data patterns. The approach is motivated by Manski's partial \n    identification framework and pessimistic bounding ideas from optimization literature.  "
  },
  {
    "id": 16170,
    "package_name": "mixpoissonreg",
    "title": "Mixed Poisson Regression for Overdispersed Count Data",
    "description": "Fits mixed Poisson regression models (Poisson-Inverse Gaussian or Negative-Binomial) on data sets with response variables being count data. The models can have varying precision parameter, where a linear regression structure (through a link function) is assumed to hold on the precision parameter. The Expectation-Maximization algorithm for both these models (Poisson Inverse Gaussian and Negative Binomial) is an important contribution of this package. Another important feature of this package is the set of functions to perform global and local influence analysis. See Barreto-Souza and Simas (2016) <doi:10.1007/s11222-015-9601-6> for further details.  ",
    "version": "1.0.0",
    "maintainer": "Alexandre B. Simas <alexandre.impa@gmail.com>",
    "author": "Alexandre B. Simas [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2562-2829>),\n  Wagner Barreto-Souza [aut] (ORCID:\n    <https://orcid.org/0000-0003-0831-7881>)",
    "url": "https://github.com/vpnsctl/mixpoissonreg/,\nhttps://vpnsctl.github.io/mixpoissonreg/",
    "bug_reports": "https://github.com/vpnsctl/mixpoissonreg/issues",
    "repository": "https://cran.r-project.org/package=mixpoissonreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixpoissonreg Mixed Poisson Regression for Overdispersed Count Data Fits mixed Poisson regression models (Poisson-Inverse Gaussian or Negative-Binomial) on data sets with response variables being count data. The models can have varying precision parameter, where a linear regression structure (through a link function) is assumed to hold on the precision parameter. The Expectation-Maximization algorithm for both these models (Poisson Inverse Gaussian and Negative Binomial) is an important contribution of this package. Another important feature of this package is the set of functions to perform global and local influence analysis. See Barreto-Souza and Simas (2016) <doi:10.1007/s11222-015-9601-6> for further details.    "
  },
  {
    "id": 16188,
    "package_name": "mlbplotR",
    "title": "Create 'ggplot2' and 'gt' Visuals with Major League Baseball\nLogos",
    "description": "Tools to help visualize Major League Baseball analysis in 'ggplot2' \n  and 'gt'. You provide team/player information and 'mlbplotR' will transform \n  that information into team colors, logos, or player headshots for graphics.",
    "version": "1.2.0",
    "maintainer": "Camden Kay <camden.kay23@gmail.com>",
    "author": "Sebastian Carl [aut],\n  Camden Kay [aut, cre, cph]",
    "url": "https://github.com/camdenk/mlbplotR,\nhttps://camdenk.github.io/mlbplotR/",
    "bug_reports": "https://github.com/camdenk/mlbplotR/issues",
    "repository": "https://cran.r-project.org/package=mlbplotR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlbplotR Create 'ggplot2' and 'gt' Visuals with Major League Baseball\nLogos Tools to help visualize Major League Baseball analysis in 'ggplot2' \n  and 'gt'. You provide team/player information and 'mlbplotR' will transform \n  that information into team colors, logos, or player headshots for graphics.  "
  },
  {
    "id": 16189,
    "package_name": "mlbstats",
    "title": "Major League Baseball Player Statistics Calculator",
    "description": "Computational functions for player metrics in major league baseball including batting, pitching, fielding, base-running, and overall player statistics. This package is actively maintained with new metrics being added as they are developed.",
    "version": "0.1.0",
    "maintainer": "Philip D. Waggoner <philip.waggoner@gmail.com>",
    "author": "Philip D. Waggoner <philip.waggoner@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlbstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlbstats Major League Baseball Player Statistics Calculator Computational functions for player metrics in major league baseball including batting, pitching, fielding, base-running, and overall player statistics. This package is actively maintained with new metrics being added as they are developed.  "
  },
  {
    "id": 16199,
    "package_name": "mlf",
    "title": "Machine Learning Foundations",
    "description": "Offers a gentle introduction to machine learning concepts for practitioners with a statistical pedigree: decomposition of model error (bias-variance trade-off), nonlinear correlations, information theory and functional permutation/bootstrap simulations. Sz\u00e9kely GJ, Rizzo ML, Bakirov NK. (2007). <doi:10.1214/009053607000000505>. Reshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ, Lander ES, Mitzenmacher M, Sabeti PC. (2011). <doi:10.1126/science.1205438>.",
    "version": "1.2.1",
    "maintainer": "Kyle Peterson <petersonkdon@gmail.com>",
    "author": "Kyle Peterson [aut, cre]",
    "url": "http://mlf-project.us/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlf Machine Learning Foundations Offers a gentle introduction to machine learning concepts for practitioners with a statistical pedigree: decomposition of model error (bias-variance trade-off), nonlinear correlations, information theory and functional permutation/bootstrap simulations. Sz\u00e9kely GJ, Rizzo ML, Bakirov NK. (2007). <doi:10.1214/009053607000000505>. Reshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ, Lander ES, Mitzenmacher M, Sabeti PC. (2011). <doi:10.1126/science.1205438>.  "
  },
  {
    "id": 16268,
    "package_name": "mmcards",
    "title": "Playing Cards Utility Functions",
    "description": "Early insights in probability theory were largely influenced by\n    questions about gambling and games of chance, as noted by Blitzstein and\n    Hwang (2019, ISBN:978-1138369917). In modern times, playing cards continue\n    to serve as an effective teaching tool for probability, statistics, and even\n    'R' programming, as demonstrated by Grolemund (2014, ISBN:978-1449359010).\n    The 'mmcards' package offers a collection of utility functions designed to\n    aid in the creation, manipulation, and utilization of playing card decks in\n    multiple formats. These include a standard 52-card deck, as well as\n    alternative decks such as decks defined by custom anonymous functions and\n    custom interleaved decks. Optimized for the development of educational\n    'shiny' applications, the package is particularly useful for teaching\n    statistics and probability through card-based games. Functions include\n    shuffle_deck(), which creates either a shuffled standard deck or a shuffled\n    custom alternative deck; deal_card(), which takes a deck and returns a list\n    object containing both the dealt card and the updated deck; and i_deck(),\n    which adds image paths to card objects, further enriching the package's\n    utility in the development of interactive 'shiny' application card games.",
    "version": "0.1.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmcards",
    "bug_reports": "https://github.com/mightymetrika/mmcards/issues",
    "repository": "https://cran.r-project.org/package=mmcards",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmcards Playing Cards Utility Functions Early insights in probability theory were largely influenced by\n    questions about gambling and games of chance, as noted by Blitzstein and\n    Hwang (2019, ISBN:978-1138369917). In modern times, playing cards continue\n    to serve as an effective teaching tool for probability, statistics, and even\n    'R' programming, as demonstrated by Grolemund (2014, ISBN:978-1449359010).\n    The 'mmcards' package offers a collection of utility functions designed to\n    aid in the creation, manipulation, and utilization of playing card decks in\n    multiple formats. These include a standard 52-card deck, as well as\n    alternative decks such as decks defined by custom anonymous functions and\n    custom interleaved decks. Optimized for the development of educational\n    'shiny' applications, the package is particularly useful for teaching\n    statistics and probability through card-based games. Functions include\n    shuffle_deck(), which creates either a shuffled standard deck or a shuffled\n    custom alternative deck; deal_card(), which takes a deck and returns a list\n    object containing both the dealt card and the updated deck; and i_deck(),\n    which adds image paths to card objects, further enriching the package's\n    utility in the development of interactive 'shiny' application card games.  "
  },
  {
    "id": 16276,
    "package_name": "mmiCATs",
    "title": "Cluster Adjusted t Statistic Applications",
    "description": "Simulation results detailed in Esarey and Menger (2019) <doi:10.1017/psrm.2017.42>\n    demonstrate that cluster adjusted t statistics (CATs) are an effective method\n    for correcting standard errors in scenarios with a small number of clusters.\n    The 'mmiCATs' package offers a suite of tools for working with CATs. The\n    mmiCATs() function initiates a 'shiny' web application, facilitating\n    the analysis of data utilizing CATs, as implemented in the cluster.im.glm()\n    function from the 'clusterSEs' package. Additionally, the pwr_func_lmer()\n    function is designed to simplify the process of conducting simulations to\n    compare mixed effects models with CATs models. For educational purposes, the\n    CloseCATs() function launches a 'shiny' application card game, aimed at enhancing\n    users' understanding of the conditions under which CATs should be preferred\n    over random intercept models.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmiCATs",
    "bug_reports": "https://github.com/mightymetrika/mmiCATs/issues",
    "repository": "https://cran.r-project.org/package=mmiCATs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmiCATs Cluster Adjusted t Statistic Applications Simulation results detailed in Esarey and Menger (2019) <doi:10.1017/psrm.2017.42>\n    demonstrate that cluster adjusted t statistics (CATs) are an effective method\n    for correcting standard errors in scenarios with a small number of clusters.\n    The 'mmiCATs' package offers a suite of tools for working with CATs. The\n    mmiCATs() function initiates a 'shiny' web application, facilitating\n    the analysis of data utilizing CATs, as implemented in the cluster.im.glm()\n    function from the 'clusterSEs' package. Additionally, the pwr_func_lmer()\n    function is designed to simplify the process of conducting simulations to\n    compare mixed effects models with CATs models. For educational purposes, the\n    CloseCATs() function launches a 'shiny' application card game, aimed at enhancing\n    users' understanding of the conditions under which CATs should be preferred\n    over random intercept models.  "
  },
  {
    "id": 16277,
    "package_name": "mmibain",
    "title": "Bayesian Informative Hypotheses Evaluation Web Applications",
    "description": "Researchers often have expectations about the relations between means\n    of different groups or standardized regression coefficients; using informative\n    hypothesis testing to incorporate these expectations into the analysis through\n    order constraints increases statistical power\n    Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>. Another valuable\n    tool, the Bayes factor, can evaluate evidence for multiple hypotheses without\n    concerns about multiple testing, and can be used in Bayesian updating\n    Hoijtink, Mulder, van Lissa & Gu (2019) <doi:10.1037/met0000201>. The 'bain'\n    R package enables informative hypothesis testing using the Bayes factor. The\n    'mmibain' package provides 'shiny' web applications based on 'bain'. The\n    RepliCrisis() function launches a 'shiny' card game to simulate the evaluation\n    of replication studies while the mmibain() function launches a 'shiny'\n    application to fit Bayesian informative hypotheses evaluation models from\n    'bain'.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmibain",
    "bug_reports": "https://github.com/mightymetrika/mmibain/issues",
    "repository": "https://cran.r-project.org/package=mmibain",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmibain Bayesian Informative Hypotheses Evaluation Web Applications Researchers often have expectations about the relations between means\n    of different groups or standardized regression coefficients; using informative\n    hypothesis testing to incorporate these expectations into the analysis through\n    order constraints increases statistical power\n    Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>. Another valuable\n    tool, the Bayes factor, can evaluate evidence for multiple hypotheses without\n    concerns about multiple testing, and can be used in Bayesian updating\n    Hoijtink, Mulder, van Lissa & Gu (2019) <doi:10.1037/met0000201>. The 'bain'\n    R package enables informative hypothesis testing using the Bayes factor. The\n    'mmibain' package provides 'shiny' web applications based on 'bain'. The\n    RepliCrisis() function launches a 'shiny' card game to simulate the evaluation\n    of replication studies while the mmibain() function launches a 'shiny'\n    application to fit Bayesian informative hypotheses evaluation models from\n    'bain'.  "
  },
  {
    "id": 16279,
    "package_name": "mmirestriktor",
    "title": "Informative Hypothesis Testing Web Applications",
    "description": "Offering enhanced statistical power compared to traditional\n    hypothesis testing methods, informative hypothesis testing allows researchers\n    to explicitly model their expectations regarding the relationships among\n    parameters. An important software tool for this framework is 'restriktor'.\n    The 'mmirestriktor' package provides 'shiny' web applications to implement\n    some of the basic functionality of 'restriktor'. The mmirestriktor() function\n    launches a 'shiny' application for fitting and analyzing models with\n    constraints. The FbarCards() function launches a card game application which\n    can help build intuition about informative hypothesis testing. The\n    iht_interpreter() helps interpret informative hypothesis testing results based\n    on guidelines in Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>.",
    "version": "0.3.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmirestriktor",
    "bug_reports": "https://github.com/mightymetrika/mmirestriktor/issues",
    "repository": "https://cran.r-project.org/package=mmirestriktor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmirestriktor Informative Hypothesis Testing Web Applications Offering enhanced statistical power compared to traditional\n    hypothesis testing methods, informative hypothesis testing allows researchers\n    to explicitly model their expectations regarding the relationships among\n    parameters. An important software tool for this framework is 'restriktor'.\n    The 'mmirestriktor' package provides 'shiny' web applications to implement\n    some of the basic functionality of 'restriktor'. The mmirestriktor() function\n    launches a 'shiny' application for fitting and analyzing models with\n    constraints. The FbarCards() function launches a card game application which\n    can help build intuition about informative hypothesis testing. The\n    iht_interpreter() helps interpret informative hypothesis testing results based\n    on guidelines in Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>.  "
  },
  {
    "id": 16287,
    "package_name": "mmsample",
    "title": "Multivariate Matched Sampling",
    "description": "Subset a control group to match an intervention group on a set of features using multivariate matching and propensity score calipers. Based on methods in Rosenbaum and Rubin (1985).",
    "version": "0.1",
    "maintainer": "Eoin O'Connell <eoin.m.oconnell@gmail.com>",
    "author": "Eoin O'Connell [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mmsample",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmsample Multivariate Matched Sampling Subset a control group to match an intervention group on a set of features using multivariate matching and propensity score calipers. Based on methods in Rosenbaum and Rubin (1985).  "
  },
  {
    "id": 16297,
    "package_name": "mnt",
    "title": "Affine Invariant Tests of Multivariate Normality",
    "description": "Various affine invariant multivariate normality tests are provided. It is designed to accompany the survey article Ebner, B. and Henze, N. (2020) <arXiv:2004.07332> titled \"Tests for multivariate normality -- a critical review with emphasis on weighted L^2-statistics\". We implement new and time honoured L^2-type tests of multivariate normality, such as the Baringhaus-Henze-Epps-Pulley (BHEP) test, the Henze-Zirkler test, the test of Henze-Jim\u00e9nes-Gamero, the test of Henze-Jim\u00e9nes-Gamero-Meintanis, the test of Henze-Visage, the D\u00f6rr-Ebner-Henze test based on harmonic oscillator and the D\u00f6rr-Ebner-Henze test based on a double estimation in a PDE. Secondly, we include the measures of multivariate skewness and kurtosis by Mardia, Koziol, Malkovich and Afifi and M\u00f3ri, Rohatgi and Sz\u00e9kely, as well as the associated tests. Thirdly, we include the tests of multivariate normality by Cox and Small, the 'energy' test of Sz\u00e9kely and Rizzo, the tests based on spherical harmonics by Manzotti and Quiroz and the test of Pudelko. All the functions and tests need the data to be a n x d matrix where n is the samplesize (number of rows) and d is the dimension (number of columns).",
    "version": "1.3",
    "maintainer": "Bruno Ebner <bruno.ebner@kit.edu>",
    "author": "Lucas Butsch [aut],\n  Bruno Ebner [aut, cre],\n  Jaco Visagie [ctb],\n  Johann Siemens [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mnt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mnt Affine Invariant Tests of Multivariate Normality Various affine invariant multivariate normality tests are provided. It is designed to accompany the survey article Ebner, B. and Henze, N. (2020) <arXiv:2004.07332> titled \"Tests for multivariate normality -- a critical review with emphasis on weighted L^2-statistics\". We implement new and time honoured L^2-type tests of multivariate normality, such as the Baringhaus-Henze-Epps-Pulley (BHEP) test, the Henze-Zirkler test, the test of Henze-Jim\u00e9nes-Gamero, the test of Henze-Jim\u00e9nes-Gamero-Meintanis, the test of Henze-Visage, the D\u00f6rr-Ebner-Henze test based on harmonic oscillator and the D\u00f6rr-Ebner-Henze test based on a double estimation in a PDE. Secondly, we include the measures of multivariate skewness and kurtosis by Mardia, Koziol, Malkovich and Afifi and M\u00f3ri, Rohatgi and Sz\u00e9kely, as well as the associated tests. Thirdly, we include the tests of multivariate normality by Cox and Small, the 'energy' test of Sz\u00e9kely and Rizzo, the tests based on spherical harmonics by Manzotti and Quiroz and the test of Pudelko. All the functions and tests need the data to be a n x d matrix where n is the samplesize (number of rows) and d is the dimension (number of columns).  "
  },
  {
    "id": 16337,
    "package_name": "modifiedmk",
    "title": "Modified Versions of Mann Kendall and Spearman's Rho Trend Tests",
    "description": "Power of non-parametric Mann-Kendall test and Spearman\u2019s Rho test is highly influenced by serially correlated data. To address this issue, trend tests may be applied on the modified versions of the time series data by  Block Bootstrapping (BBS), Prewhitening (PW) , Trend Free Prewhitening (TFPW), Bias Corrected Prewhitening and Variance Correction Approach by calculating effective sample size.\n    Mann, H. B. (1945).<doi:10.1017/CBO9781107415324.004>.\n    Kendall, M. (1975). Multivariate analysis. Charles Griffin&Company Ltd,. \n    sen, P. K. (1968).<doi:10.2307/2285891>.\n    \u00d6n\u00f6z, B., & Bayazit, M. (2012) <doi:10.1002/hyp.8438>.\n    Hamed, K. H. (2009).<doi:10.1016/j.jhydrol.2009.01.040>.\n    Yue, S., & Wang, C. Y. (2002) <doi:10.1029/2001WR000861>.\n    Yue, S., Pilon, P., Phinney, B., & Cavadias, G. (2002) <doi:10.1002/hyp.1095>.\n    Hamed, K. H., & Ramachandra Rao, A. (1998) <doi:10.1016/S0022-1694(97)00125-X>.\n    Yue, S., & Wang, C. Y. (2004) <doi:10.1023/B:WARM.0000043140.61082.60>.",
    "version": "1.6",
    "maintainer": "Sandeep Kumar Patakamuri <sandeep.patakamuri@gmail.com>",
    "author": "Sandeep Kumar Patakamuri [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8965-8287>),\n  Nicole O'Brien [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=modifiedmk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modifiedmk Modified Versions of Mann Kendall and Spearman's Rho Trend Tests Power of non-parametric Mann-Kendall test and Spearman\u2019s Rho test is highly influenced by serially correlated data. To address this issue, trend tests may be applied on the modified versions of the time series data by  Block Bootstrapping (BBS), Prewhitening (PW) , Trend Free Prewhitening (TFPW), Bias Corrected Prewhitening and Variance Correction Approach by calculating effective sample size.\n    Mann, H. B. (1945).<doi:10.1017/CBO9781107415324.004>.\n    Kendall, M. (1975). Multivariate analysis. Charles Griffin&Company Ltd,. \n    sen, P. K. (1968).<doi:10.2307/2285891>.\n    \u00d6n\u00f6z, B., & Bayazit, M. (2012) <doi:10.1002/hyp.8438>.\n    Hamed, K. H. (2009).<doi:10.1016/j.jhydrol.2009.01.040>.\n    Yue, S., & Wang, C. Y. (2002) <doi:10.1029/2001WR000861>.\n    Yue, S., Pilon, P., Phinney, B., & Cavadias, G. (2002) <doi:10.1002/hyp.1095>.\n    Hamed, K. H., & Ramachandra Rao, A. (1998) <doi:10.1016/S0022-1694(97)00125-X>.\n    Yue, S., & Wang, C. Y. (2004) <doi:10.1023/B:WARM.0000043140.61082.60>.  "
  },
  {
    "id": 16424,
    "package_name": "movegroup",
    "title": "Visualizing and Quantifying Space Use Data for Groups of Animals",
    "description": "Offers an easy and automated way to scale up individual-level space use analysis to \n    that of groups. Contains a function from the 'move' package to calculate a dynamic Brownian \n    bridge movement model from movement data for individual animals, as well as functions to \n    visualize and quantify space use for individuals aggregated in groups. Originally written with \n    passive acoustic telemetry in mind, this package also provides functionality to account for\n    unbalanced acoustic receiver array designs, and satellite tag data.",
    "version": "2024.03.05",
    "maintainer": "Simon Dedman <simondedman@gmail.com>",
    "author": "Simon Dedman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9108-972X>),\n  Maurits van Zinnicq Bergmann [aut] (ORCID:\n    <https://orcid.org/0000-0002-8414-5025>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=movegroup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "movegroup Visualizing and Quantifying Space Use Data for Groups of Animals Offers an easy and automated way to scale up individual-level space use analysis to \n    that of groups. Contains a function from the 'move' package to calculate a dynamic Brownian \n    bridge movement model from movement data for individual animals, as well as functions to \n    visualize and quantify space use for individuals aggregated in groups. Originally written with \n    passive acoustic telemetry in mind, this package also provides functionality to account for\n    unbalanced acoustic receiver array designs, and satellite tag data.  "
  },
  {
    "id": 16430,
    "package_name": "mpath",
    "title": "Regularized Linear Models",
    "description": "Algorithms compute robust estimators for loss functions in the concave convex (CC) family by the iteratively reweighted convex optimization (IRCO), an extension of the iteratively reweighted least squares (IRLS). The IRCO reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include robust (penalized) generalized linear models and robust support vector machines. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,\n      Wang et al. (2015) <doi:10.1002/bimj.201400143>,\n      Wang et al. (2016) <doi:10.1177/0962280214530608>,\n      Wang (2021) <doi:10.1007/s11749-021-00770-2>,\n      Wang (2024) <doi:10.1111/anzs.12409>.",
    "version": "0.4-2.26",
    "maintainer": "Zhu Wang <zwang145@uthsc.edu>",
    "author": "Zhu Wang, with contributions from Achim Zeileis, Simon Jackman, Brian Ripley, and Patrick Breheny",
    "url": "https://github.com/zhuwang46/mpath",
    "bug_reports": "https://github.com/zhuwang46/mpath",
    "repository": "https://cran.r-project.org/package=mpath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mpath Regularized Linear Models Algorithms compute robust estimators for loss functions in the concave convex (CC) family by the iteratively reweighted convex optimization (IRCO), an extension of the iteratively reweighted least squares (IRLS). The IRCO reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include robust (penalized) generalized linear models and robust support vector machines. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,\n      Wang et al. (2015) <doi:10.1002/bimj.201400143>,\n      Wang et al. (2016) <doi:10.1177/0962280214530608>,\n      Wang (2021) <doi:10.1007/s11749-021-00770-2>,\n      Wang (2024) <doi:10.1111/anzs.12409>.  "
  },
  {
    "id": 16441,
    "package_name": "mppR",
    "title": "Multi-Parent Population QTL Analysis",
    "description": "Analysis of experimental multi-parent populations to detect regions\n             of the genome (called quantitative trait loci, QTLs) influencing\n             phenotypic traits measured in unique and multiple environments.\n             The population must be composed of crosses between a set of at\n             least three parents (e.g. factorial design, 'diallel', or nested\n             association mapping). The functions cover data processing,\n             QTL detection, and results visualization. The implemented methodology\n             is described in Garin, Wimmer, Mezmouk, Malosetti and van Eeuwijk (2017)\n             <doi:10.1007/s00122-017-2923-3>, in Garin, Malosetti\n             and van Eeuwijk (2020) <doi: 10.1007/s00122-020-03621-0>,\n             and in Garin, Diallo, Tekete, Thera, ..., and Rami (2024) <doi: 10.1093/genetics/iyae003>.",
    "version": "1.5.0",
    "maintainer": "Vincent Garin <vincent.garin6@gmail.com>",
    "author": "Vincent Garin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5571-1841>),\n  Valentin Wimmer [aut],\n  Dietrich Borchardt [ctb, dtc],\n  Fred van Eeuwijk [ctb, ths],\n  Marcos Malosetti [ctb, ths]",
    "url": "https://github.com/vincentgarin/mppR",
    "bug_reports": "https://github.com/vincentgarin/mppR/issues",
    "repository": "https://cran.r-project.org/package=mppR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mppR Multi-Parent Population QTL Analysis Analysis of experimental multi-parent populations to detect regions\n             of the genome (called quantitative trait loci, QTLs) influencing\n             phenotypic traits measured in unique and multiple environments.\n             The population must be composed of crosses between a set of at\n             least three parents (e.g. factorial design, 'diallel', or nested\n             association mapping). The functions cover data processing,\n             QTL detection, and results visualization. The implemented methodology\n             is described in Garin, Wimmer, Mezmouk, Malosetti and van Eeuwijk (2017)\n             <doi:10.1007/s00122-017-2923-3>, in Garin, Malosetti\n             and van Eeuwijk (2020) <doi: 10.1007/s00122-020-03621-0>,\n             and in Garin, Diallo, Tekete, Thera, ..., and Rami (2024) <doi: 10.1093/genetics/iyae003>.  "
  },
  {
    "id": 16473,
    "package_name": "msamp",
    "title": "Estimate Sample Size to Detect Bacterial Contamination in a\nProduct Lot",
    "description": "Estimates the sample size needed to detect microbial contamination \n in a lot with a user-specified detection probability and user-specified analytical sensitivity.\n Various patterns of microbial contamination are accounted for: homogeneous (Poisson), \n heterogeneous (Poisson-Gamma) or localized(Zero-inflated Poisson).\n  Ida Jongenburger et al. (2010) <doi:10.1016/j.foodcont.2012.02.004> \n  \"Impact of microbial distributions on food safety\".\n  Leroy Simon (1963) <doi:10.1017/S0515036100001975>\n  \"Casualty Actuarial Society - The Negative Binomial and Poisson Distributions Compared\". ",
    "version": "1.0.0",
    "maintainer": "Martine Ferguson <martine.ferguson@fda.hhs.gov>",
    "author": "Martine Ferguson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4479-3674>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=msamp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "msamp Estimate Sample Size to Detect Bacterial Contamination in a\nProduct Lot Estimates the sample size needed to detect microbial contamination \n in a lot with a user-specified detection probability and user-specified analytical sensitivity.\n Various patterns of microbial contamination are accounted for: homogeneous (Poisson), \n heterogeneous (Poisson-Gamma) or localized(Zero-inflated Poisson).\n  Ida Jongenburger et al. (2010) <doi:10.1016/j.foodcont.2012.02.004> \n  \"Impact of microbial distributions on food safety\".\n  Leroy Simon (1963) <doi:10.1017/S0515036100001975>\n  \"Casualty Actuarial Society - The Negative Binomial and Poisson Distributions Compared\".   "
  },
  {
    "id": 16531,
    "package_name": "multbxxc",
    "title": "Auxiliary Routines for Influx Software",
    "description": "Contains auxiliary routines for influx software. This packages is not intended to be used directly. Influx was published here: Sokol et al. (2012) <doi:10.1093/bioinformatics/btr716>.",
    "version": "1.0.3",
    "maintainer": "Serguei Sokol <sokol@insa-toulouse.fr>",
    "author": "Serguei Sokol",
    "url": "https://github.com/sgsokol/influx/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multbxxc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multbxxc Auxiliary Routines for Influx Software Contains auxiliary routines for influx software. This packages is not intended to be used directly. Influx was published here: Sokol et al. (2012) <doi:10.1093/bioinformatics/btr716>.  "
  },
  {
    "id": 16621,
    "package_name": "mumarinex",
    "title": "Computation of the Multivariate Marine Recovery Index",
    "description": "Computation of the multivariate marine recovery index, including functions for data visualization and ecological diagnostics of marine ecosystems. The computational details are described in the original publication. Reference: Chauvel, N., Grall, J., Thi\u00e9baut, E., Houbin, C., Pezy, J.P. (in press). \"A general-purpose Multivariate Marine Recovery Index for quantifying the influence of human activities on benthic habitat ecological status\". Ecological Indicators.",
    "version": "1.0",
    "maintainer": "Nathan Chauvel <nathan.chauvel@outlook.fr>",
    "author": "Nathan Chauvel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8464-9944>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mumarinex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mumarinex Computation of the Multivariate Marine Recovery Index Computation of the multivariate marine recovery index, including functions for data visualization and ecological diagnostics of marine ecosystems. The computational details are described in the original publication. Reference: Chauvel, N., Grall, J., Thi\u00e9baut, E., Houbin, C., Pezy, J.P. (in press). \"A general-purpose Multivariate Marine Recovery Index for quantifying the influence of human activities on benthic habitat ecological status\". Ecological Indicators.  "
  },
  {
    "id": 16641,
    "package_name": "mvMISE",
    "title": "A General Framework of Multivariate Mixed-Effects Selection\nModels",
    "description": "Offers a general framework of multivariate mixed-effects\n        models for the joint analysis of multiple correlated outcomes with clustered \n        data structures and potential missingness proposed by Wang et al. (2018) <doi:10.1093/biostatistics/kxy022>. The missingness of outcome values may \n        depend on the values themselves (missing not at random and non-ignorable), \n        or may depend on only the covariates (missing at random and ignorable), or both.\n        This package provides functions for two models: 1) mvMISE_b() \n        allows correlated outcome-specific random intercepts with a factor-analytic \n        structure, and 2) mvMISE_e() allows the correlated outcome-specific \n        error terms with a graphical lasso penalty on the error precision matrix. Both functions \n        are motivated by the multivariate data analysis on data with clustered structures \n        from labelling-based quantitative proteomic studies. These models and functions \n        can also be applied to univariate and multivariate analyses of clustered data \n        with balanced or unbalanced design and no missingness.",
    "version": "1.0",
    "maintainer": "Jiebiao Wang <randel.wang@gmail.com>",
    "author": "Jiebiao Wang and Lin S. Chen",
    "url": "https://github.com/randel/mvMISE",
    "bug_reports": "https://github.com/randel/mvMISE/issues",
    "repository": "https://cran.r-project.org/package=mvMISE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvMISE A General Framework of Multivariate Mixed-Effects Selection\nModels Offers a general framework of multivariate mixed-effects\n        models for the joint analysis of multiple correlated outcomes with clustered \n        data structures and potential missingness proposed by Wang et al. (2018) <doi:10.1093/biostatistics/kxy022>. The missingness of outcome values may \n        depend on the values themselves (missing not at random and non-ignorable), \n        or may depend on only the covariates (missing at random and ignorable), or both.\n        This package provides functions for two models: 1) mvMISE_b() \n        allows correlated outcome-specific random intercepts with a factor-analytic \n        structure, and 2) mvMISE_e() allows the correlated outcome-specific \n        error terms with a graphical lasso penalty on the error precision matrix. Both functions \n        are motivated by the multivariate data analysis on data with clustered structures \n        from labelling-based quantitative proteomic studies. These models and functions \n        can also be applied to univariate and multivariate analyses of clustered data \n        with balanced or unbalanced design and no missingness.  "
  },
  {
    "id": 16659,
    "package_name": "mvglmmRank",
    "title": "Multivariate Generalized Linear Mixed Models for Ranking Sports\nTeams",
    "description": "Maximum likelihood estimates are obtained via an EM algorithm with either a first-order or a fully exponential Laplace approximation as documented by Broatch and Karl (2018) <doi:10.48550/arXiv.1710.05284>,\n    Karl, Yang, and Lohr (2014) <doi:10.1016/j.csda.2013.11.019>, and by \n\tKarl (2012) <doi:10.1515/1559-0410.1471>. Karl and Zimmerman <doi:10.1016/j.jspi.2020.06.004> use this package to illustrate how the home field effect estimator from a mixed model can be biased under nonrandom scheduling. ",
    "version": "1.2-4",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "author": "Andrew T. Karl [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-5933-8706>),\n  Jennifer Broatch [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvglmmRank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvglmmRank Multivariate Generalized Linear Mixed Models for Ranking Sports\nTeams Maximum likelihood estimates are obtained via an EM algorithm with either a first-order or a fully exponential Laplace approximation as documented by Broatch and Karl (2018) <doi:10.48550/arXiv.1710.05284>,\n    Karl, Yang, and Lohr (2014) <doi:10.1016/j.csda.2013.11.019>, and by \n\tKarl (2012) <doi:10.1515/1559-0410.1471>. Karl and Zimmerman <doi:10.1016/j.jspi.2020.06.004> use this package to illustrate how the home field effect estimator from a mixed model can be biased under nonrandom scheduling.   "
  },
  {
    "id": 16662,
    "package_name": "mvinfluence",
    "title": "Influence Measures and Diagnostic Plots for Multivariate Linear\nModels",
    "description": "Computes regression deletion diagnostics for multivariate linear models and provides some associated\n\tdiagnostic plots.  The diagnostic measures include hat-values (leverages), generalized Cook's distance, and\n\tgeneralized squared 'studentized' residuals.  Several types of plots to detect influential observations are\n\tprovided.",
    "version": "0.9.2",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>)",
    "url": "https://github.com/friendly/mvinfluence,\nhttps://friendly.github.io/mvinfluence/",
    "bug_reports": "https://github.com/friendly/mvinfluence/issues",
    "repository": "https://cran.r-project.org/package=mvinfluence",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvinfluence Influence Measures and Diagnostic Plots for Multivariate Linear\nModels Computes regression deletion diagnostics for multivariate linear models and provides some associated\n\tdiagnostic plots.  The diagnostic measures include hat-values (leverages), generalized Cook's distance, and\n\tgeneralized squared 'studentized' residuals.  Several types of plots to detect influential observations are\n\tprovided.  "
  },
  {
    "id": 16704,
    "package_name": "mzipmed",
    "title": "Mediation using MZIP Model",
    "description": "We implement functions allowing for mediation analysis to be performed in cases where the mediator is a count variable with excess zeroes. First a function is provided allowing users to perform analysis for zero-inflated count variables using the marginalized zero-inflated Poisson (MZIP) model (Long et al. 2014 <DOI:10.1002/sim.6293>). Using the counterfactual approach to mediation and MZIP we can obtain natural direct and indirect effects for the overall population. Using delta method processes variance estimation can be performed instantaneously. Alternatively, bootstrap standard errors can be used. We also provide functions for cases with exposure-mediator interactions with four-way decomposition of total effect.",
    "version": "1.4.0",
    "maintainer": "Andrew Sims <ams329@uab.edu>",
    "author": "Andrew Sims [aut, cre] (ORCID: <https://orcid.org/0000-0001-8525-4381>),\n  Dustin Long [aut],\n  Hemant Tiwari [aut],\n  Leann Long [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mzipmed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mzipmed Mediation using MZIP Model We implement functions allowing for mediation analysis to be performed in cases where the mediator is a count variable with excess zeroes. First a function is provided allowing users to perform analysis for zero-inflated count variables using the marginalized zero-inflated Poisson (MZIP) model (Long et al. 2014 <DOI:10.1002/sim.6293>). Using the counterfactual approach to mediation and MZIP we can obtain natural direct and indirect effects for the overall population. Using delta method processes variance estimation can be performed instantaneously. Alternatively, bootstrap standard errors can be used. We also provide functions for cases with exposure-mediator interactions with four-way decomposition of total effect.  "
  },
  {
    "id": 16752,
    "package_name": "nbapalettes",
    "title": "An NBA Jersey Palette Generator",
    "description": "Palettes generated from NBA jersey colorways.",
    "version": "0.1.0",
    "maintainer": "Murray Josh <joshua.murray@utoronto.ca>",
    "author": "Murray Josh [aut, cre]",
    "url": "https://github.com/murrayjw/nbapalettes",
    "bug_reports": "https://github.com/murrayjw/nbapalettes/issues",
    "repository": "https://cran.r-project.org/package=nbapalettes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nbapalettes An NBA Jersey Palette Generator Palettes generated from NBA jersey colorways.  "
  },
  {
    "id": 16757,
    "package_name": "nblR",
    "title": "Data Extraction of Australian NBL Basketball Statistics",
    "description": "Allow users to obtain basketball statistics for \n    the Australian basketball league 'NBL'<https://nbl.com.au/>. \n    Stats include play-by-play, shooting locations, results and \n    box scores for teams and players.",
    "version": "0.0.4",
    "maintainer": "Jason Zivkovic <jaseziv83@gmail.com>",
    "author": "Jason Zivkovic [aut, cre, cph]",
    "url": "https://github.com/JaseZiv/nblR",
    "bug_reports": "https://github.com/JaseZiv/nblR/issues",
    "repository": "https://cran.r-project.org/package=nblR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nblR Data Extraction of Australian NBL Basketball Statistics Allow users to obtain basketball statistics for \n    the Australian basketball league 'NBL'<https://nbl.com.au/>. \n    Stats include play-by-play, shooting locations, results and \n    box scores for teams and players.  "
  },
  {
    "id": 16762,
    "package_name": "ncaavolleyballr",
    "title": "Extract Data from NCAA Women's and Men's Volleyball Website",
    "description": "Extracts team records/schedules and player statistics for the \n    2020-2025 National Collegiate Athletic Association (NCAA) women's and men's\n    divisions I, II, and III volleyball teams from <https://stats.ncaa.org>. \n    Functions can aggregate statistics for teams, conferences, divisions, or \n    custom groups of teams.",
    "version": "0.5.0",
    "maintainer": "Jeffrey R. Stevens <jeffrey.r.stevens@protonmail.com>",
    "author": "Jeffrey R. Stevens [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2375-1360>)",
    "url": "https://github.com/JeffreyRStevens/ncaavolleyballr,\nhttps://jeffreyrstevens.github.io/ncaavolleyballr/",
    "bug_reports": "https://github.com/JeffreyRStevens/ncaavolleyballr/issues",
    "repository": "https://cran.r-project.org/package=ncaavolleyballr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ncaavolleyballr Extract Data from NCAA Women's and Men's Volleyball Website Extracts team records/schedules and player statistics for the \n    2020-2025 National Collegiate Athletic Association (NCAA) women's and men's\n    divisions I, II, and III volleyball teams from <https://stats.ncaa.org>. \n    Functions can aggregate statistics for teams, conferences, divisions, or \n    custom groups of teams.  "
  },
  {
    "id": 16883,
    "package_name": "nfl4th",
    "title": "Functions to Calculate Optimal Fourth Down Decisions in the\nNational Football League",
    "description": "A set of functions to estimate outcomes of fourth down\n    plays in the National Football League and obtain fourth down plays\n    from <https://www.nfl.com/> and <https://www.espn.com/>.",
    "version": "1.0.4",
    "maintainer": "Ben Baldwin <bbaldwin206@gmail.com>",
    "author": "Ben Baldwin [aut, cre, cph],\n  Sebastian Carl [ctb]",
    "url": "https://www.nfl4th.com/, https://github.com/nflverse/nfl4th/,\nhttps://github.com/nflverse/nfl4th",
    "bug_reports": "https://github.com/nflverse/nfl4th/issues",
    "repository": "https://cran.r-project.org/package=nfl4th",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nfl4th Functions to Calculate Optimal Fourth Down Decisions in the\nNational Football League A set of functions to estimate outcomes of fourth down\n    plays in the National Football League and obtain fourth down plays\n    from <https://www.nfl.com/> and <https://www.espn.com/>.  "
  },
  {
    "id": 16884,
    "package_name": "nflfastR",
    "title": "Functions to Efficiently Access NFL Play by Play Data",
    "description": "A set of functions to access National Football\n    League play-by-play data from <https://www.nfl.com/>.",
    "version": "5.1.0",
    "maintainer": "Ben Baldwin <bbaldwin206@gmail.com>",
    "author": "Sebastian Carl [aut],\n  Ben Baldwin [cre, aut],\n  Lee Sharpe [ctb],\n  Maksim Horowitz [ctb],\n  Ron Yurko [ctb],\n  Samuel Ventura [ctb],\n  Tan Ho [ctb],\n  John Edwards [ctb]",
    "url": "https://www.nflfastr.com/, https://github.com/nflverse/nflfastR",
    "bug_reports": "https://github.com/nflverse/nflfastR/issues",
    "repository": "https://cran.r-project.org/package=nflfastR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nflfastR Functions to Efficiently Access NFL Play by Play Data A set of functions to access National Football\n    League play-by-play data from <https://www.nfl.com/>.  "
  },
  {
    "id": 16885,
    "package_name": "nflplotR",
    "title": "NFL Logo Plots in 'ggplot2' and 'gt'",
    "description": "A set of functions to visualize National Football League\n    analysis in 'ggplot2' plots and 'gt' tables.",
    "version": "1.6.0",
    "maintainer": "Sebastian Carl <mrcaseb@gmail.com>",
    "author": "Sebastian Carl [aut, cre]",
    "url": "https://nflplotr.nflverse.com,\nhttps://github.com/nflverse/nflplotR",
    "bug_reports": "https://github.com/nflverse/nflplotR/issues",
    "repository": "https://cran.r-project.org/package=nflplotR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nflplotR NFL Logo Plots in 'ggplot2' and 'gt' A set of functions to visualize National Football League\n    analysis in 'ggplot2' plots and 'gt' tables.  "
  },
  {
    "id": 16886,
    "package_name": "nflreadr",
    "title": "Download 'nflverse' Data",
    "description": "A minimal package for downloading data from 'GitHub'\n    repositories of the 'nflverse' project.",
    "version": "1.5.0",
    "maintainer": "Tan Ho <tan@tanho.ca>",
    "author": "Tan Ho [aut, cre, cph] (ORCID: <https://orcid.org/0000-0001-8388-5155>),\n  Sebastian Carl [aut],\n  John Edwards [ctb],\n  Ben Baldwin [ctb],\n  Thomas Mock [ctb],\n  Lee Sharpe [ctb],\n  Pranav Rajaram [ctb]",
    "url": "https://nflreadr.nflverse.com,\nhttps://github.com/nflverse/nflreadr",
    "bug_reports": "https://github.com/nflverse/nflreadr/issues",
    "repository": "https://cran.r-project.org/package=nflreadr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nflreadr Download 'nflverse' Data A minimal package for downloading data from 'GitHub'\n    repositories of the 'nflverse' project.  "
  },
  {
    "id": 16887,
    "package_name": "nflseedR",
    "title": "Functions to Efficiently Simulate and Evaluate NFL Seasons",
    "description": "A set of functions to simulate National Football League\n    seasons including the sophisticated tie-breaking procedures.",
    "version": "2.0.2",
    "maintainer": "Sebastian Carl <mrcaseb@gmail.com>",
    "author": "Sebastian Carl [cre, aut, cph],\n  Lee Sharpe [aut]",
    "url": "https://nflseedr.com, https://github.com/nflverse/nflseedR",
    "bug_reports": "https://github.com/nflverse/nflseedR/issues",
    "repository": "https://cran.r-project.org/package=nflseedR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nflseedR Functions to Efficiently Simulate and Evaluate NFL Seasons A set of functions to simulate National Football League\n    seasons including the sophisticated tie-breaking procedures.  "
  },
  {
    "id": 16888,
    "package_name": "nflverse",
    "title": "Easily Install and Load the 'nflverse'",
    "description": "The 'nflverse' is a set of packages dedicated to data of the\n    National Football League. This package is designed to make it easy to\n    install and load multiple 'nflverse' packages in a single step. Learn\n    more about the 'nflverse' at <https://nflverse.nflverse.com/>.",
    "version": "1.0.3",
    "maintainer": "Sebastian Carl <mrcaseb@gmail.com>",
    "author": "Sebastian Carl [aut, cre],\n  Ben Baldwin [aut],\n  Lee Sharpe [aut],\n  Tan Ho [aut] (ORCID: <https://orcid.org/0000-0001-8388-5155>),\n  John Edwards [aut] (ORCID: <https://orcid.org/0000-0002-8533-7504>)",
    "url": "https://nflverse.nflverse.com/,\nhttps://github.com/nflverse/nflverse",
    "bug_reports": "https://github.com/nflverse/nflverse/issues",
    "repository": "https://cran.r-project.org/package=nflverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nflverse Easily Install and Load the 'nflverse' The 'nflverse' is a set of packages dedicated to data of the\n    National Football League. This package is designed to make it easy to\n    install and load multiple 'nflverse' packages in a single step. Learn\n    more about the 'nflverse' at <https://nflverse.nflverse.com/>.  "
  },
  {
    "id": 16898,
    "package_name": "nhlapi",
    "title": "A Minimum-Dependency 'R' Interface to the 'NHL' API",
    "description": "Retrieves and processes the data exposed by the open 'NHL' API. This includes information on players, teams, games, tournaments, drafts, standings, schedules and other endpoints. A lower-level interface to access the data via URLs directly is also provided.",
    "version": "0.1.4",
    "maintainer": "Jozef Hajnala <jozef.hajnala@gmail.com>",
    "author": "Jozef Hajnala [aut, cre]",
    "url": "https://github.com/jozefhajnala/nhlapi",
    "bug_reports": "https://github.com/jozefhajnala/nhlapi/issues",
    "repository": "https://cran.r-project.org/package=nhlapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nhlapi A Minimum-Dependency 'R' Interface to the 'NHL' API Retrieves and processes the data exposed by the open 'NHL' API. This includes information on players, teams, games, tournaments, drafts, standings, schedules and other endpoints. A lower-level interface to access the data via URLs directly is also provided.  "
  },
  {
    "id": 17056,
    "package_name": "nosoi",
    "title": "A Forward Agent-Based Transmission Chain Simulator",
    "description": "The aim of 'nosoi' (pronounced no.si) is to provide a flexible agent-based stochastic transmission chain/epidemic simulator (Lequime et al. Methods in Ecology and Evolution 11:1002-1007). It is named after the daimones of plague, sickness and disease that escaped Pandora's jar in the Greek mythology. 'nosoi' is able to take into account the influence of multiple variable on the transmission process (e.g. dual-host systems (such as arboviruses), within-host viral dynamics, transportation, population structure), alone or taken together, to create complex but relatively intuitive epidemiological simulations.",
    "version": "1.1.2",
    "maintainer": "Sebastian Lequime <sebastian.lequime@gmail.com>",
    "author": "Sebastian Lequime [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3140-0651>),\n  Paul Bastide [aut] (ORCID: <https://orcid.org/0000-0002-8084-9893>),\n  Simon Dellicour [aut] (ORCID: <https://orcid.org/0000-0001-9558-1052>),\n  Philippe Lemey [aut] (ORCID: <https://orcid.org/0000-0003-2826-5353>),\n  Guy Baele [aut] (ORCID: <https://orcid.org/0000-0002-1915-7732>),\n  Thijs Janzen [ctb] (ORCID: <https://orcid.org/0000-0002-4162-1140>)",
    "url": "https://github.com/slequime/nosoi,\nhttps://slequime.github.io/nosoi/",
    "bug_reports": "https://github.com/slequime/nosoi/issues",
    "repository": "https://cran.r-project.org/package=nosoi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nosoi A Forward Agent-Based Transmission Chain Simulator The aim of 'nosoi' (pronounced no.si) is to provide a flexible agent-based stochastic transmission chain/epidemic simulator (Lequime et al. Methods in Ecology and Evolution 11:1002-1007). It is named after the daimones of plague, sickness and disease that escaped Pandora's jar in the Greek mythology. 'nosoi' is able to take into account the influence of multiple variable on the transmission process (e.g. dual-host systems (such as arboviruses), within-host viral dynamics, transportation, population structure), alone or taken together, to create complex but relatively intuitive epidemiological simulations.  "
  },
  {
    "id": 17073,
    "package_name": "nparcomp",
    "title": "Multiple Comparisons and Simultaneous Confidence Intervals",
    "description": "With this package, it is possible to compute nonparametric simultaneous confidence intervals for relative contrast effects in the unbalanced one way layout. Moreover, it computes simultaneous p-values. The simultaneous confidence intervals can be computed using multivariate normal distribution, multivariate t-distribution with a Satterthwaite Approximation of the degree of freedom or using multivariate range preserving transformations with Logit or Probit as transformation function. 2 sample comparisons can be performed with the same methods described above. There is no assumption on the underlying distribution function, only that the data have to be at least ordinal numbers. See Konietschke et al. (2015) <doi:10.18637/jss.v064.i09> for details.",
    "version": "3.0",
    "maintainer": "Kerstin Rubarth <kerstin.rubarth@charite.de>",
    "author": "Frank Konietschke [aut, cre], Kimihiro Noguchi [ctr], Kerstin Rubarth ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nparcomp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nparcomp Multiple Comparisons and Simultaneous Confidence Intervals With this package, it is possible to compute nonparametric simultaneous confidence intervals for relative contrast effects in the unbalanced one way layout. Moreover, it computes simultaneous p-values. The simultaneous confidence intervals can be computed using multivariate normal distribution, multivariate t-distribution with a Satterthwaite Approximation of the degree of freedom or using multivariate range preserving transformations with Logit or Probit as transformation function. 2 sample comparisons can be performed with the same methods described above. There is no assumption on the underlying distribution function, only that the data have to be at least ordinal numbers. See Konietschke et al. (2015) <doi:10.18637/jss.v064.i09> for details.  "
  },
  {
    "id": 17074,
    "package_name": "nparsurv",
    "title": "Nonparametric Tests for Main Effects, Simple Effects and\nInteraction Effect in a Factorial Design with Censored Data",
    "description": "Nonparametric Tests for Main Effects, Simple Effects and Interaction Effect with Censored Data and Two Factorial Influencing Variables.",
    "version": "0.1.0",
    "maintainer": "Christine Kroener <christinekroener@web.de>",
    "author": "Christine Kroener [aut, cre],\n  Sarah Friedrich [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nparsurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nparsurv Nonparametric Tests for Main Effects, Simple Effects and\nInteraction Effect in a Factorial Design with Censored Data Nonparametric Tests for Main Effects, Simple Effects and Interaction Effect with Censored Data and Two Factorial Influencing Variables.  "
  },
  {
    "id": 17114,
    "package_name": "nrlR",
    "title": "Functions to Scrape Rugby Data",
    "description": "Provides a set of functions to scrape and analyze rugby data. \n    Supports competitions including the National Rugby League, New South Wales Cup, \n    Queensland Cup, Super League, and various representative and women's competitions. \n    Includes functions to fetch player statistics, match results, ladders, venues, and coaching data. \n    Designed to assist analysts, fans, and researchers in exploring historical and current rugby league data. \n    See Woods et al. (2017) <doi:10.1123/ijspp.2016-0187> for an example of rugby league performance analysis methodology.",
    "version": "0.1.2",
    "maintainer": "Daniel Tomaro <danieltomaro@icloud.com>",
    "author": "Daniel Tomaro [aut, cre]",
    "url": "https://github.com/DanielTomaro13/nrlR",
    "bug_reports": "https://github.com/DanielTomaro13/nrlR/issues",
    "repository": "https://cran.r-project.org/package=nrlR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nrlR Functions to Scrape Rugby Data Provides a set of functions to scrape and analyze rugby data. \n    Supports competitions including the National Rugby League, New South Wales Cup, \n    Queensland Cup, Super League, and various representative and women's competitions. \n    Includes functions to fetch player statistics, match results, ladders, venues, and coaching data. \n    Designed to assist analysts, fans, and researchers in exploring historical and current rugby league data. \n    See Woods et al. (2017) <doi:10.1123/ijspp.2016-0187> for an example of rugby league performance analysis methodology.  "
  },
  {
    "id": 17164,
    "package_name": "obfuscatoR",
    "title": "Obfuscation Game Designs",
    "description": "When people make decisions, they may do so using a wide variety of decision rules. The package allows users to easily create obfuscation games to test the obfuscation hypothesis. It provides an easy to use interface and multiple options designed to vary the difficulty of the game and tailor it to the user's needs. For more detail: Chorus et al., 2021, Obfuscation maximization-based decision-making: Theory, methodology and first empirical evidence, Mathematical Social Sciences, 109, 28-44, <doi:10.1016/j.mathsocsci.2020.10.002>. ",
    "version": "0.2.2",
    "maintainer": "Erlend Dancke Sandorf <erlend.dancke.sandorf@nmbu.no>",
    "author": "Erlend Dancke Sandorf [aut, cre],\n  Caspar Chorus [aut],\n  Sander van Cranenburgh [aut]",
    "url": "https://obfuscator.edsandorf.me,\nhttps://github.com/edsandorf/obfuscatoR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=obfuscatoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "obfuscatoR Obfuscation Game Designs When people make decisions, they may do so using a wide variety of decision rules. The package allows users to easily create obfuscation games to test the obfuscation hypothesis. It provides an easy to use interface and multiple options designed to vary the difficulty of the game and tailor it to the user's needs. For more detail: Chorus et al., 2021, Obfuscation maximization-based decision-making: Theory, methodology and first empirical evidence, Mathematical Social Sciences, 109, 28-44, <doi:10.1016/j.mathsocsci.2020.10.002>.   "
  },
  {
    "id": 17198,
    "package_name": "odds.converter",
    "title": "Betting Odds Conversion",
    "description": "Conversion between the most common odds types for sports betting.\n    Hong Kong odds, US odds, Decimal odds, Indonesian odds, Malaysian odds, and raw\n    Probability are covered in this package.",
    "version": "1.4.8",
    "maintainer": "Marco Blume <marco.blume@pinnaclesports.com>",
    "author": "Marco Blume, Sascha Thomsen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=odds.converter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "odds.converter Betting Odds Conversion Conversion between the most common odds types for sports betting.\n    Hong Kong odds, US odds, Decimal odds, Indonesian odds, Malaysian odds, and raw\n    Probability are covered in this package.  "
  },
  {
    "id": 17200,
    "package_name": "oddsapiR",
    "title": "Access Live Sports Odds from the Odds API",
    "description": "A utility to quickly obtain clean and tidy sports\n    odds from The Odds API <https://the-odds-api.com>.",
    "version": "0.0.3",
    "maintainer": "Saiem Gilani <saiem.gilani@gmail.com>",
    "author": "Saiem Gilani [aut, cre]",
    "url": "https://oddsapiR.sportsdataverse.org/ (docs),\nhttps://github.com/sportsdataverse/oddsapiR (repo)",
    "bug_reports": "https://github.com/sportsdataverse/oddsapiR/issues",
    "repository": "https://cran.r-project.org/package=oddsapiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oddsapiR Access Live Sports Odds from the Odds API A utility to quickly obtain clean and tidy sports\n    odds from The Odds API <https://the-odds-api.com>.  "
  },
  {
    "id": 17234,
    "package_name": "olr",
    "title": "Optimal Linear Regression",
    "description": "The olr function systematically evaluates multiple linear regression models by exhaustively fitting all possible combinations of independent variables against the specified dependent variable. It selects the model that yields the highest adjusted R-squared (by default) or R-squared, depending on user preference. In model evaluation, both R-squared and adjusted R-squared are key metrics: R-squared measures the proportion of variance explained but tends to increase with the addition of predictors\u2014regardless of relevance\u2014potentially leading to overfitting. Adjusted R-squared compensates for this by penalizing model complexity, providing a more balanced view of fit quality. The goal of olr is to identify the most suitable model that captures the underlying structure of the data while avoiding unnecessary complexity. By comparing both metrics, it offers a robust evaluation framework that balances predictive power with model parsimony. Example Analogy: Imagine a gardener trying to understand what influences plant growth (the dependent variable). They might consider variables like sunlight, watering frequency, soil type, and nutrients (independent variables). Instead of manually guessing which combination works best, the olr function automatically tests every possible combination of predictors and identifies the most effective model\u2014based on either the highest R-squared or adjusted R-squared value. This saves the user from trial-and-error modeling and highlights only the most meaningful variables for explaining the outcome. A Python version is also available at <https://pypi.org/project/olr>.",
    "version": "1.2",
    "maintainer": "Mathew Fok <quiksilver67213@yahoo.com>",
    "author": "Mathew Fok [aut, cre]",
    "url": "https://github.com/MatHatter/olr_r, https://pypi.org/project/olr/",
    "bug_reports": "https://github.com/MatHatter/olr_r/issues",
    "repository": "https://cran.r-project.org/package=olr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "olr Optimal Linear Regression The olr function systematically evaluates multiple linear regression models by exhaustively fitting all possible combinations of independent variables against the specified dependent variable. It selects the model that yields the highest adjusted R-squared (by default) or R-squared, depending on user preference. In model evaluation, both R-squared and adjusted R-squared are key metrics: R-squared measures the proportion of variance explained but tends to increase with the addition of predictors\u2014regardless of relevance\u2014potentially leading to overfitting. Adjusted R-squared compensates for this by penalizing model complexity, providing a more balanced view of fit quality. The goal of olr is to identify the most suitable model that captures the underlying structure of the data while avoiding unnecessary complexity. By comparing both metrics, it offers a robust evaluation framework that balances predictive power with model parsimony. Example Analogy: Imagine a gardener trying to understand what influences plant growth (the dependent variable). They might consider variables like sunlight, watering frequency, soil type, and nutrients (independent variables). Instead of manually guessing which combination works best, the olr function automatically tests every possible combination of predictors and identifies the most effective model\u2014based on either the highest R-squared or adjusted R-squared value. This saves the user from trial-and-error modeling and highlights only the most meaningful variables for explaining the outcome. A Python version is also available at <https://pypi.org/project/olr>.  "
  },
  {
    "id": 17235,
    "package_name": "olsrr",
    "title": "Tools for Building OLS Regression Models",
    "description": "Tools designed to make it easier for users, particularly beginner/intermediate R users \n    to build ordinary least squares regression models. Includes comprehensive regression output, \n    heteroskedasticity tests, collinearity diagnostics, residual diagnostics, measures of influence, \n    model fit assessment and variable selection procedures.",
    "version": "0.6.1",
    "maintainer": "Aravind Hebbali <hebbali.aravind@gmail.com>",
    "author": "Aravind Hebbali [aut, cre]",
    "url": "https://olsrr.rsquaredacademy.com/,\nhttps://github.com/rsquaredacademy/olsrr",
    "bug_reports": "https://github.com/rsquaredacademy/olsrr/issues",
    "repository": "https://cran.r-project.org/package=olsrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "olsrr Tools for Building OLS Regression Models Tools designed to make it easier for users, particularly beginner/intermediate R users \n    to build ordinary least squares regression models. Includes comprehensive regression output, \n    heteroskedasticity tests, collinearity diagnostics, residual diagnostics, measures of influence, \n    model fit assessment and variable selection procedures.  "
  },
  {
    "id": 17254,
    "package_name": "oneinfl",
    "title": "Estimates OIPP and OIZTNB Regression Models",
    "description": "Estimates one-inflated positive Poisson (OIPP) and\n    one-inflated zero-truncated negative binomial (OIZTNB) regression\n    models. A suite of ancillary statistical tools are also provided,\n    including: estimation of positive Poisson (PP) and zero-truncated\n    negative binomial (ZTNB) models; marginal effects and their standard\n    errors; diagnostic likelihood ratio and Wald tests; plotting;\n    predicted counts and expected responses; and random variate\n    generation. The models and tools, as well as four applications, are\n    shown in Godwin, R. T. (2024). \"One-inflated zero-truncated count\n    regression models\" arXiv preprint <doi:10.48550/arXiv.2402.02272>.",
    "version": "1.0.2",
    "maintainer": "Ryan T. Godwin <ryan.godwin@umanitoba.ca>",
    "author": "Ryan T. Godwin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=oneinfl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oneinfl Estimates OIPP and OIZTNB Regression Models Estimates one-inflated positive Poisson (OIPP) and\n    one-inflated zero-truncated negative binomial (OIZTNB) regression\n    models. A suite of ancillary statistical tools are also provided,\n    including: estimation of positive Poisson (PP) and zero-truncated\n    negative binomial (ZTNB) models; marginal effects and their standard\n    errors; diagnostic likelihood ratio and Wald tests; plotting;\n    predicted counts and expected responses; and random variate\n    generation. The models and tools, as well as four applications, are\n    shown in Godwin, R. T. (2024). \"One-inflated zero-truncated count\n    regression models\" arXiv preprint <doi:10.48550/arXiv.2402.02272>.  "
  },
  {
    "id": 17305,
    "package_name": "opendotaR",
    "title": "Interface for OpenDota API",
    "description": "Enables the usage of the OpenDota API from <https://www.opendota.com/>, get game lists, and download JSON's of parsed replays from\n    the OpenDota API. Also has functionality to execute own code to extract the specific parts of the JSON file.",
    "version": "0.1.4",
    "maintainer": "Kari Gunnarsson <kari.gunnarsson@outlook.com>",
    "author": "Kari Gunnarsson",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=opendotaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opendotaR Interface for OpenDota API Enables the usage of the OpenDota API from <https://www.opendota.com/>, get game lists, and download JSON's of parsed replays from\n    the OpenDota API. Also has functionality to execute own code to extract the specific parts of the JSON file.  "
  },
  {
    "id": 17378,
    "package_name": "orders",
    "title": "Sampling from k-th Order Statistics of New Families of\nDistributions",
    "description": "Set of tools to generate samples of k-th order statistics and others quantities of interest from new families of distributions. \n    The main references for this package are: C. Kleiber and S. Kotz (2003) Statistical size distributions in economics and actuarial sciences; Gentle, J. (2009), Computational Statistics, Springer-Verlag; \n    Naradajah, S. and Rocha, R. (2016), <DOI:10.18637/jss.v069.i10> and Stasinopoulos, M. and Rigby, R. (2015), <DOI:10.1111/j.1467-9876.2005.00510.x>.\n    The families of distributions are: Benini distributions, Burr distributions, Dagum distributions, Feller-Pareto distributions, Generalized Pareto distributions, Inverse Pareto distributions, The Inverse Paralogistic distributions, Marshall-Olkin G distributions, exponentiated G distributions, beta G distributions, \n    gamma G distributions, Kumaraswamy G distributions, generalized beta G distributions, \n    beta extended G distributions, gamma G distributions, gamma uniform G distributions, beta exponential G distributions, Weibull G distributions, log gamma G I distributions, log gamma G II distributions, \n    exponentiated generalized G distributions, exponentiated Kumaraswamy G distributions, geometric exponential Poisson G distributions, truncated-exponential skew-symmetric G distributions, modified beta G distributions, \n    exponentiated exponential Poisson G distributions, Poisson-inverse gaussian distributions, Skew normal type 1 distributions, Skew student t distributions, Singh-Maddala distributions, Sinh-Arcsinh distributions, Sichel distributions, Zero inflated Poisson distributions. ",
    "version": "0.1.8",
    "maintainer": "Carlos Alberto Cardozo Delgado <cardozorpackages@gmail.com>",
    "author": "Carlos Alberto Cardozo Delgado",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=orders",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "orders Sampling from k-th Order Statistics of New Families of\nDistributions Set of tools to generate samples of k-th order statistics and others quantities of interest from new families of distributions. \n    The main references for this package are: C. Kleiber and S. Kotz (2003) Statistical size distributions in economics and actuarial sciences; Gentle, J. (2009), Computational Statistics, Springer-Verlag; \n    Naradajah, S. and Rocha, R. (2016), <DOI:10.18637/jss.v069.i10> and Stasinopoulos, M. and Rigby, R. (2015), <DOI:10.1111/j.1467-9876.2005.00510.x>.\n    The families of distributions are: Benini distributions, Burr distributions, Dagum distributions, Feller-Pareto distributions, Generalized Pareto distributions, Inverse Pareto distributions, The Inverse Paralogistic distributions, Marshall-Olkin G distributions, exponentiated G distributions, beta G distributions, \n    gamma G distributions, Kumaraswamy G distributions, generalized beta G distributions, \n    beta extended G distributions, gamma G distributions, gamma uniform G distributions, beta exponential G distributions, Weibull G distributions, log gamma G I distributions, log gamma G II distributions, \n    exponentiated generalized G distributions, exponentiated Kumaraswamy G distributions, geometric exponential Poisson G distributions, truncated-exponential skew-symmetric G distributions, modified beta G distributions, \n    exponentiated exponential Poisson G distributions, Poisson-inverse gaussian distributions, Skew normal type 1 distributions, Skew student t distributions, Singh-Maddala distributions, Sinh-Arcsinh distributions, Sichel distributions, Zero inflated Poisson distributions.   "
  },
  {
    "id": 17384,
    "package_name": "ordinalForest",
    "title": "Ordinal Forests: Prediction and Variable Ranking with Ordinal\nTarget Variables",
    "description": "The ordinal forest (OF) method allows ordinal regression with high-dimensional\n  and low-dimensional data. After having constructed an OF prediction rule using a training dataset, \n  it can be used to predict the values of the ordinal target variable for new observations.\n  Moreover, by means of the (permutation-based) variable importance measure of OF, it is also\n  possible to rank the covariates with respect to their importance in the prediction of the \n  values of the ordinal target variable.\n  OF is presented in Hornung (2020).\n  NOTE: Starting with package version 2.4, it is also possible to obtain class probability \n  predictions in addition to the class point predictions. Moreover, the variable importance values\n  can also be based on the class probability predictions. Preliminary results indicate that\n  this might lead to a better discrimination between influential and non-influential covariates.\n  The main functions of the package are: ordfor() (construction of OF) and predict.ordfor() \n  (prediction of the target variable values of new observations).\n  References:\n  Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4\u201317. \n  <doi:10.1007/s00357-018-9302-x>.",
    "version": "2.4-4",
    "maintainer": "Roman Hornung <hornung@ibe.med.uni-muenchen.de>",
    "author": "Roman Hornung [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ordinalForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ordinalForest Ordinal Forests: Prediction and Variable Ranking with Ordinal\nTarget Variables The ordinal forest (OF) method allows ordinal regression with high-dimensional\n  and low-dimensional data. After having constructed an OF prediction rule using a training dataset, \n  it can be used to predict the values of the ordinal target variable for new observations.\n  Moreover, by means of the (permutation-based) variable importance measure of OF, it is also\n  possible to rank the covariates with respect to their importance in the prediction of the \n  values of the ordinal target variable.\n  OF is presented in Hornung (2020).\n  NOTE: Starting with package version 2.4, it is also possible to obtain class probability \n  predictions in addition to the class point predictions. Moreover, the variable importance values\n  can also be based on the class probability predictions. Preliminary results indicate that\n  this might lead to a better discrimination between influential and non-influential covariates.\n  The main functions of the package are: ordfor() (construction of OF) and predict.ordfor() \n  (prediction of the target variable values of new observations).\n  References:\n  Hornung R. (2020) Ordinal Forests. Journal of Classification 37, 4\u201317. \n  <doi:10.1007/s00357-018-9302-x>.  "
  },
  {
    "id": 17555,
    "package_name": "panelSUR",
    "title": "Two-Way Error Component SUR Systems Estimation on Unbalanced\nPanel Data",
    "description": "Generalized Least Squares (GLS) estimation of Seemingly Unrelated Regression\n             (SUR) systems on unbalanced panel in the one/two-way cases also taking into \n             account the possibility of cross equation restrictions.  Methodological details\n             can be found in Bi\u00f8rn (2004) <doi:10.1016/j.jeconom.2003.10.023> and Platoni,\n             Sckokai, Moro (2012) <doi:10.1080/07474938.2011.607098>.",
    "version": "0.1.0",
    "maintainer": "Laura Barbieri <laura.barbieri@unicatt.it>",
    "author": "Laura Barbieri [aut, cre],\n  Silvia Platoni [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=panelSUR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "panelSUR Two-Way Error Component SUR Systems Estimation on Unbalanced\nPanel Data Generalized Least Squares (GLS) estimation of Seemingly Unrelated Regression\n             (SUR) systems on unbalanced panel in the one/two-way cases also taking into \n             account the possibility of cross equation restrictions.  Methodological details\n             can be found in Bi\u00f8rn (2004) <doi:10.1016/j.jeconom.2003.10.023> and Platoni,\n             Sckokai, Moro (2012) <doi:10.1080/07474938.2011.607098>.  "
  },
  {
    "id": 17587,
    "package_name": "paran",
    "title": "Horn's Test of Principal Components/Factors",
    "description": "An implementation of Horn's technique for numerically and graphically evaluating the components or factors retained in a principle components analysis (PCA) or common factor analysis (FA). Horn's method contrasts eigenvalues produced through a PCA or FA on a number of random data sets of uncorrelated variables with the same number of variables and observations as the experimental or observational data set to produce eigenvalues for components or factors that are adjusted for the sample error-induced inflation. Components with adjusted eigenvalues greater than one are retained. paran may also be used to conduct parallel analysis following Glorfeld's (1995) suggestions to reduce the likelihood of over-retention.",
    "version": "1.5.4",
    "maintainer": "Alexis Dinno <alexis.dinno@pdx.edu>",
    "author": "Alexis Dinno [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2560-310X>)",
    "url": "https://alexisdinno.com/Software/index.shtml#paran,\nhttps://alexisdinno.com/Software/files/PA_for_PCA_vs_FA.pdf",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=paran",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paran Horn's Test of Principal Components/Factors An implementation of Horn's technique for numerically and graphically evaluating the components or factors retained in a principle components analysis (PCA) or common factor analysis (FA). Horn's method contrasts eigenvalues produced through a PCA or FA on a number of random data sets of uncorrelated variables with the same number of variables and observations as the experimental or observational data set to produce eigenvalues for components or factors that are adjusted for the sample error-induced inflation. Components with adjusted eigenvalues greater than one are retained. paran may also be used to conduct parallel analysis following Glorfeld's (1995) suggestions to reduce the likelihood of over-retention.  "
  },
  {
    "id": 17706,
    "package_name": "pcse",
    "title": "Panel-Corrected Standard Error Estimation in R",
    "description": "A function to estimate\n        panel-corrected standard errors. Data may contain balanced or\n        unbalanced panels.",
    "version": "1.9.1.1",
    "maintainer": "Delia Bailey <delia.bailey@gmail.com>",
    "author": "Delia Bailey <delia.bailey@gmail.com> and Jonathan N. Katz\n        <jkatz@caltech.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pcse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcse Panel-Corrected Standard Error Estimation in R A function to estimate\n        panel-corrected standard errors. Data may contain balanced or\n        unbalanced panels.  "
  },
  {
    "id": 17731,
    "package_name": "pdynmc",
    "title": "Moment Condition Based Estimation of Linear Dynamic Panel Data\nModels",
    "description": "Linear dynamic panel data modeling based on linear and\n    nonlinear moment conditions as proposed by\n    Holtz-Eakin, Newey, and Rosen (1988) <doi:10.2307/1913103>,\n    Ahn and Schmidt (1995) <doi:10.1016/0304-4076(94)01641-C>,\n    and Arellano and Bover (1995) <doi:10.1016/0304-4076(94)01642-D>.\n    Estimation of the model parameters relies on the Generalized\n    Method of Moments (GMM) and instrumental variables (IV) estimation,\n    numerical optimization (when nonlinear moment conditions are\n    employed) and the computation of closed form solutions (when\n    estimation is based on linear moment conditions). One-step,\n    two-step and iterated estimation is available. For inference\n    and specification\n    testing, Windmeijer (2005) <doi:10.1016/j.jeconom.2004.02.005>\n    and doubly corrected standard errors\n    (Hwang, Kang, Lee, 2021 <doi:10.1016/j.jeconom.2020.09.010>)\n    are available. Additionally, serial correlation tests, tests for\n    overidentification, and Wald tests are provided. Functions for\n    visualizing panel data structures and modeling results obtained\n    from GMM estimation are also available. The plot methods include\n    functions to plot unbalanced panel structure, coefficient ranges\n    and coefficient paths across GMM iterations (the latter is\n    implemented according to the plot shown in\n    Hansen and Lee, 2021 <doi:10.3982/ECTA16274>).\n    For a more detailed description of the GMM-based functionality,\n    please see Fritsch, Pua, Schnurbus (2021) <doi:10.32614/RJ-2021-035>.\n    For more details on the IV-based estimation routines,\n    see Fritsch, Pua, and Schnurbus (WP, 2024) and\n    Han and Phillips (2010) <doi:10.1017/S026646660909063X>.",
    "version": "0.9.12",
    "maintainer": "Markus Fritsch <Markus.Fritsch@uni-Passau.de>",
    "author": "Markus Fritsch [aut, cre],\n  Joachim Schnurbus [aut],\n  Andrew Adrian Yu Pua [aut]",
    "url": "https://github.com/markusfritsch/pdynmc",
    "bug_reports": "https://github.com/markusfritsch/pdynmc/issues",
    "repository": "https://cran.r-project.org/package=pdynmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdynmc Moment Condition Based Estimation of Linear Dynamic Panel Data\nModels Linear dynamic panel data modeling based on linear and\n    nonlinear moment conditions as proposed by\n    Holtz-Eakin, Newey, and Rosen (1988) <doi:10.2307/1913103>,\n    Ahn and Schmidt (1995) <doi:10.1016/0304-4076(94)01641-C>,\n    and Arellano and Bover (1995) <doi:10.1016/0304-4076(94)01642-D>.\n    Estimation of the model parameters relies on the Generalized\n    Method of Moments (GMM) and instrumental variables (IV) estimation,\n    numerical optimization (when nonlinear moment conditions are\n    employed) and the computation of closed form solutions (when\n    estimation is based on linear moment conditions). One-step,\n    two-step and iterated estimation is available. For inference\n    and specification\n    testing, Windmeijer (2005) <doi:10.1016/j.jeconom.2004.02.005>\n    and doubly corrected standard errors\n    (Hwang, Kang, Lee, 2021 <doi:10.1016/j.jeconom.2020.09.010>)\n    are available. Additionally, serial correlation tests, tests for\n    overidentification, and Wald tests are provided. Functions for\n    visualizing panel data structures and modeling results obtained\n    from GMM estimation are also available. The plot methods include\n    functions to plot unbalanced panel structure, coefficient ranges\n    and coefficient paths across GMM iterations (the latter is\n    implemented according to the plot shown in\n    Hansen and Lee, 2021 <doi:10.3982/ECTA16274>).\n    For a more detailed description of the GMM-based functionality,\n    please see Fritsch, Pua, Schnurbus (2021) <doi:10.32614/RJ-2021-035>.\n    For more details on the IV-based estimation routines,\n    see Fritsch, Pua, and Schnurbus (WP, 2024) and\n    Han and Phillips (2010) <doi:10.1017/S026646660909063X>.  "
  },
  {
    "id": 17822,
    "package_name": "petersenlab",
    "title": "A Collection of R Functions by the Petersen Lab",
    "description": "A collection of R functions that are widely used by the Petersen\n    Lab. Included are functions for various purposes, including evaluating the\n    accuracy of judgments and predictions, performing scoring of assessments,\n    generating correlation matrices, conversion of data between various types,\n    data management, psychometric evaluation, extensions related to latent\n    variable modeling, various plotting capabilities, and other miscellaneous\n    useful functions. By making the package available, we hope to make our\n    methods reproducible and replicable by others and to help others perform\n    their data processing and analysis methods more easily and efficiently. The\n    codebase is provided in Petersen (2025) <doi:10.5281/zenodo.7602890> and on\n    'CRAN': <doi: 10.32614/CRAN.package.petersenlab>. The package is described\n    in \"Principles of Psychological Assessment: With Applied Examples in R\"\n    (Petersen, 2024, 2025a) <doi:10.1201/9781003357421>,\n    <doi:10.25820/work.007199>, <doi:10.5281/zenodo.6466589> and in \"Fantasy\n    Football Analytics: Statistics, Prediction, and Empiricism Using R\"\n    (Petersen, 2025b).",
    "version": "1.2.0",
    "maintainer": "Isaac T. Petersen <isaac-t-petersen@uiowa.edu>",
    "author": "Isaac T. Petersen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3072-6673>),\n  Developmental Psychopathology Lab at the University of Iowa [ctb],\n  Angela D. Staples [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7678-5794>),\n  Johanna Caskey [ctb] (ORCID: <https://orcid.org/0009-0001-8227-6603>),\n  Philipp Doebler [ctb] (ORCID: <https://orcid.org/0000-0002-2946-8526>),\n  Loreen Sabel [ctb] (ORCID: <https://orcid.org/0000-0002-9832-8842>)",
    "url": "https://github.com/DevPsyLab/petersenlab,\nhttps://devpsylab.github.io/petersenlab/",
    "bug_reports": "https://github.com/DevPsyLab/petersenlab/issues",
    "repository": "https://cran.r-project.org/package=petersenlab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "petersenlab A Collection of R Functions by the Petersen Lab A collection of R functions that are widely used by the Petersen\n    Lab. Included are functions for various purposes, including evaluating the\n    accuracy of judgments and predictions, performing scoring of assessments,\n    generating correlation matrices, conversion of data between various types,\n    data management, psychometric evaluation, extensions related to latent\n    variable modeling, various plotting capabilities, and other miscellaneous\n    useful functions. By making the package available, we hope to make our\n    methods reproducible and replicable by others and to help others perform\n    their data processing and analysis methods more easily and efficiently. The\n    codebase is provided in Petersen (2025) <doi:10.5281/zenodo.7602890> and on\n    'CRAN': <doi: 10.32614/CRAN.package.petersenlab>. The package is described\n    in \"Principles of Psychological Assessment: With Applied Examples in R\"\n    (Petersen, 2024, 2025a) <doi:10.1201/9781003357421>,\n    <doi:10.25820/work.007199>, <doi:10.5281/zenodo.6466589> and in \"Fantasy\n    Football Analytics: Statistics, Prediction, and Empiricism Using R\"\n    (Petersen, 2025b).  "
  },
  {
    "id": 17856,
    "package_name": "phase",
    "title": "Analyse Biological Time-Series Data",
    "description": "Compiles functions to trim, bin, visualise, and analyse activity/sleep time-series data collected from the Drosophila Activity Monitor (DAM) system (Trikinetics, USA). The following methods were used to compute periodograms - Chi-square periodogram: Sokolove and Bushell (1978) <doi:10.1016/0022-5193(78)90022-X>, Lomb-Scargle periodogram: Lomb (1976) <doi:10.1007/BF00648343>, Scargle (1982) <doi:10.1086/160554> and Ruf (1999) <doi:10.1076/brhm.30.2.178.1422>, and Autocorrelation: Eijzenbach et al. (1986) <doi:10.1111/j.1440-1681.1986.tb00943.x>. Identification of activity peaks is done after using a Savitzky-Golay filter (Savitzky and Golay (1964) <doi:10.1021/ac60214a047>) to smooth raw activity data. Three methods to estimate anticipation of activity are used based on the following papers - Slope method: Fernandez et al. (2020) <doi:10.1016/j.cub.2020.04.025>, Harrisingh method: Harrisingh et al. (2007) <doi:10.1523/JNEUROSCI.3680-07.2007>, and Stoleru method: Stoleru et al. (2004) <doi:10.1038/nature02926>. Rose plots and circular analysis are based on methods from - Batschelet (1981) <ISBN:0120810506> and Zar (2010) <ISBN:0321656865>.",
    "version": "1.2.9",
    "maintainer": "Lakshman Abhilash <labhilash@gc.cuny.edu>",
    "author": "Lakshman Abhilash [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9933-8989>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phase Analyse Biological Time-Series Data Compiles functions to trim, bin, visualise, and analyse activity/sleep time-series data collected from the Drosophila Activity Monitor (DAM) system (Trikinetics, USA). The following methods were used to compute periodograms - Chi-square periodogram: Sokolove and Bushell (1978) <doi:10.1016/0022-5193(78)90022-X>, Lomb-Scargle periodogram: Lomb (1976) <doi:10.1007/BF00648343>, Scargle (1982) <doi:10.1086/160554> and Ruf (1999) <doi:10.1076/brhm.30.2.178.1422>, and Autocorrelation: Eijzenbach et al. (1986) <doi:10.1111/j.1440-1681.1986.tb00943.x>. Identification of activity peaks is done after using a Savitzky-Golay filter (Savitzky and Golay (1964) <doi:10.1021/ac60214a047>) to smooth raw activity data. Three methods to estimate anticipation of activity are used based on the following papers - Slope method: Fernandez et al. (2020) <doi:10.1016/j.cub.2020.04.025>, Harrisingh method: Harrisingh et al. (2007) <doi:10.1523/JNEUROSCI.3680-07.2007>, and Stoleru method: Stoleru et al. (2004) <doi:10.1038/nature02926>. Rose plots and circular analysis are based on methods from - Batschelet (1981) <ISBN:0120810506> and Zar (2010) <ISBN:0321656865>.  "
  },
  {
    "id": 17899,
    "package_name": "photosynthesisLRC",
    "title": "Nonlinear Least Squares Models for Photosynthetic Light Response",
    "description": "Provides functions for modeling, comparing, and visualizing photosynthetic light response curves using established mechanistic and empirical models like the rectangular hyperbola Michaelis-Menton based models ((eq1 (Baly (1935) <doi:10.1098/rspb.1935.0026>)) (eq2 (Kaipiainenn (2009) <doi:10.1134/S1021443709040025>)) (eq3 (Smith (1936) <doi:10.1073/pnas.22.8.504>))), hyperbolic tangent based models ((eq4 (Jassby & Platt (1976) <doi:10.4319/LO.1976.21.4.0540>)) (eq5 (Abe et al. (2009) <doi:10.1111/j.1444-2906.2008.01619.x>))), the non-rectangular hyperbola model (eq6 (Prioul & Chartier (1977) <doi:10.1093/oxfordjournals.aob.a085354>)), exponential based models ((eq8 (Webb et al. (1974) <doi:10.1007/BF00345747>)), (eq9 (Prado & de Moraes (1997) <doi:10.1007/BF02982542>))), and finally the Ye model (eq11 (Ye (2007) <doi:10.1007/s11099-007-0110-5>)). Each of these nonlinear least squares models are commonly used to express photosynthetic response under changing light conditions and has been well supported in the literature, but distinctions in each mathematical model represent moderately different assumptions about physiology and trait relationships which ultimately produce different calculated functional trait values. These models were all thoughtfully discussed and curated by Lobo et al. (2013) <doi:10.1007/s11099-013-0045-y> to express the importance of selecting an appropriate model for analysis, and methods were established in Davis et al. (in review) to evaluate the impact of analytical choice in phylogenetic analysis of the function-valued traits. Gas exchange data on 28 wild sunflower species from Davis et al.are included as an example data set here. ",
    "version": "1.0.6",
    "maintainer": "Rebekah Davis <rebekah.davis.evoecophys@gmail.com>",
    "author": "Rebekah Davis [aut, cre],\n  Eric Goolsby [aut]",
    "url": "https://github.com/heliotropichuman/photosynthesisLRC",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=photosynthesisLRC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "photosynthesisLRC Nonlinear Least Squares Models for Photosynthetic Light Response Provides functions for modeling, comparing, and visualizing photosynthetic light response curves using established mechanistic and empirical models like the rectangular hyperbola Michaelis-Menton based models ((eq1 (Baly (1935) <doi:10.1098/rspb.1935.0026>)) (eq2 (Kaipiainenn (2009) <doi:10.1134/S1021443709040025>)) (eq3 (Smith (1936) <doi:10.1073/pnas.22.8.504>))), hyperbolic tangent based models ((eq4 (Jassby & Platt (1976) <doi:10.4319/LO.1976.21.4.0540>)) (eq5 (Abe et al. (2009) <doi:10.1111/j.1444-2906.2008.01619.x>))), the non-rectangular hyperbola model (eq6 (Prioul & Chartier (1977) <doi:10.1093/oxfordjournals.aob.a085354>)), exponential based models ((eq8 (Webb et al. (1974) <doi:10.1007/BF00345747>)), (eq9 (Prado & de Moraes (1997) <doi:10.1007/BF02982542>))), and finally the Ye model (eq11 (Ye (2007) <doi:10.1007/s11099-007-0110-5>)). Each of these nonlinear least squares models are commonly used to express photosynthetic response under changing light conditions and has been well supported in the literature, but distinctions in each mathematical model represent moderately different assumptions about physiology and trait relationships which ultimately produce different calculated functional trait values. These models were all thoughtfully discussed and curated by Lobo et al. (2013) <doi:10.1007/s11099-013-0045-y> to express the importance of selecting an appropriate model for analysis, and methods were established in Davis et al. (in review) to evaluate the impact of analytical choice in phylogenetic analysis of the function-valued traits. Gas exchange data on 28 wild sunflower species from Davis et al.are included as an example data set here.   "
  },
  {
    "id": 17915,
    "package_name": "phylometrics",
    "title": "Estimating Statistical Errors of Phylogenetic Metrics",
    "description": "Provides functions to estimate statistical errors of phylogenetic\n    metrics particularly to detect binary trait influence on diversification, as\n    well as a function to simulate trees with fixed number of sampled taxa and trait\n    prevalence.",
    "version": "0.0.1",
    "maintainer": "Xia Hua <xia.hua@anu.edu.au>",
    "author": "Xia Hua <xia.hua@anu.edu.au>, Lindell Bromham\n    <lindell.bromham@anu.edu.au>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phylometrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylometrics Estimating Statistical Errors of Phylogenetic Metrics Provides functions to estimate statistical errors of phylogenetic\n    metrics particularly to detect binary trait influence on diversification, as\n    well as a function to simulate trees with fixed number of sampled taxa and trait\n    prevalence.  "
  },
  {
    "id": 17942,
    "package_name": "piecepackr",
    "title": "Board Game Graphics",
    "description": "Functions to make board game graphics with the 'ggplot2', 'grid', 'rayrender', 'rayvertex', and 'rgl' packages.  Specializes in game diagrams, animations, and \"Print & Play\" layouts for the 'piecepack' <https://www.ludism.org/ppwiki> but can make graphics for other board game systems.  Includes configurations for several public domain game systems such as checkers, (double-18) dominoes, go, 'piecepack', playing cards, etc.",
    "version": "1.15.3",
    "maintainer": "Trevor L. Davis <trevor.l.davis@gmail.com>",
    "author": "Trevor L. Davis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6341-4639>),\n  Linux Foundation [dtc] (Uses some data from the \"SPDX License List\"\n    <https://github.com/spdx/license-list-XML>),\n  Delapouite <https://delapouite.com/> [ill] (Meeple shape extracted from\n    \"Meeple icon\" <https://game-icons.net/1x1/delapouite/meeple.html> /\n    \"CC BY 3.0\" <https://creativecommons.org/licenses/by/3.0/>),\n  Creative Commons [ill] (`save_print_and_play()` uses \"license badges\"\n    from Creative Commons to describe the generated print-and-play\n    file's license)",
    "url": "https://trevorldavis.com/piecepackr/ (blog),\nhttps://trevorldavis.com/R/piecepackr/ (pkgdown),\nhttps://groups.google.com/forum/#!forum/piecepackr (forum)",
    "bug_reports": "https://github.com/piecepackr/piecepackr/issues",
    "repository": "https://cran.r-project.org/package=piecepackr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "piecepackr Board Game Graphics Functions to make board game graphics with the 'ggplot2', 'grid', 'rayrender', 'rayvertex', and 'rgl' packages.  Specializes in game diagrams, animations, and \"Print & Play\" layouts for the 'piecepack' <https://www.ludism.org/ppwiki> but can make graphics for other board game systems.  Includes configurations for several public domain game systems such as checkers, (double-18) dominoes, go, 'piecepack', playing cards, etc.  "
  },
  {
    "id": 17951,
    "package_name": "pinnacle.data",
    "title": "Market Odds Data from Pinnacle",
    "description": "Market odds from from Pinnacle, an online sports betting bookmaker (see <https://www.pinnacle.com> for more information). Included are datasets for the Major League Baseball (MLB) 2016 season and the USA election 2016. These datasets can be used to build models and compare statistical information with the information from prediction markets.The Major League Baseball (MLB) 2016 dataset can be used for sabermetrics analysis and also can be used in conjunction with other popular Major League Baseball (MLB) datasets such as Retrosheets or the Lahman package by merging by GameID.",
    "version": "0.1.4",
    "maintainer": "Marco Blume <marco.blume@pinnaclesports.com>",
    "author": "Marco Blume, Michael Yan",
    "url": "https://github.com/marcoblume/pinnacle.data",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pinnacle.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pinnacle.data Market Odds Data from Pinnacle Market odds from from Pinnacle, an online sports betting bookmaker (see <https://www.pinnacle.com> for more information). Included are datasets for the Major League Baseball (MLB) 2016 season and the USA election 2016. These datasets can be used to build models and compare statistical information with the information from prediction markets.The Major League Baseball (MLB) 2016 dataset can be used for sabermetrics analysis and also can be used in conjunction with other popular Major League Baseball (MLB) datasets such as Retrosheets or the Lahman package by merging by GameID.  "
  },
  {
    "id": 17966,
    "package_name": "piratings",
    "title": "Calculate Pi Ratings for Teams Competing in Sport Matches",
    "description": "Calculate and optimize dynamic performance ratings of association football \n  teams competing in matches, in accordance with the method used in \n  the research paper \"Determining the level of ability of football teams by \n  dynamic ratings based on the relative discrepancies in scores between adversaries\", \n  by Constantinou and Fenton (2013) \n  <doi:10.1515/jqas-2012-0036>    \n  This dynamic rating system has proven to provide superior \n  results for predicting association football outcomes.",
    "version": "0.1.9",
    "maintainer": "Lars Van Cutsem <lars.vancutsem@hotmail.com>",
    "author": "Lars Van Cutsem",
    "url": "",
    "bug_reports": "https://github.com/larsvancutsem/piratings/issues",
    "repository": "https://cran.r-project.org/package=piratings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "piratings Calculate Pi Ratings for Teams Competing in Sport Matches Calculate and optimize dynamic performance ratings of association football \n  teams competing in matches, in accordance with the method used in \n  the research paper \"Determining the level of ability of football teams by \n  dynamic ratings based on the relative discrepancies in scores between adversaries\", \n  by Constantinou and Fenton (2013) \n  <doi:10.1515/jqas-2012-0036>    \n  This dynamic rating system has proven to provide superior \n  results for predicting association football outcomes.  "
  },
  {
    "id": 17975,
    "package_name": "pixelpuzzle",
    "title": "Puzzle Game for the R Console",
    "description": "Puzzle game that can be played in the R console.\n  Restore the pixel art by shifting rows.",
    "version": "1.0.1",
    "maintainer": "Roland Krasser <roland.krasser@gmail.com>",
    "author": "Roland Krasser [aut, cre]",
    "url": "https://github.com/rolkra/pixelpuzzle",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pixelpuzzle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pixelpuzzle Puzzle Game for the R Console Puzzle game that can be played in the R console.\n  Restore the pixel art by shifting rows.  "
  },
  {
    "id": 18011,
    "package_name": "play",
    "title": "Visualize Sports Data",
    "description": "Provides functions to visualise sports data. Converts data into a \n    format suitable for plotting charts. Helps to ease the process of working with\n    messy sports data to a more user friendly format. Football data is accessed \n    through 'worldfootballR' '<https://github.com/JaseZiv/worldfootballR>' which \n    gets data from 'FBref' <https://fbref.com/en>, 'Transfermarkt' <https://www.transfermarkt.com/>, \n    'Understat' <https://understat.com/>, and 'fotmob' <https://www.fotmob.com/>. ",
    "version": "0.1.3",
    "maintainer": "Joe Chelladurai <joe.chelladurai@yahoo.com>",
    "author": "Joe Chelladurai [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8477-3753>)",
    "url": "https://joe-chelladurai.github.io/play/",
    "bug_reports": "https://github.com/joe-chelladurai/play/issues",
    "repository": "https://cran.r-project.org/package=play",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "play Visualize Sports Data Provides functions to visualise sports data. Converts data into a \n    format suitable for plotting charts. Helps to ease the process of working with\n    messy sports data to a more user friendly format. Football data is accessed \n    through 'worldfootballR' '<https://github.com/JaseZiv/worldfootballR>' which \n    gets data from 'FBref' <https://fbref.com/en>, 'Transfermarkt' <https://www.transfermarkt.com/>, \n    'Understat' <https://understat.com/>, and 'fotmob' <https://www.fotmob.com/>.   "
  },
  {
    "id": 18012,
    "package_name": "player",
    "title": "Play Games in the Console",
    "description": "Games that can be played in the R console.  Includes coin\n    flip, hangman, jumble, magic 8 ball, poker, rock paper scissors, shut\n    the box, spelling bee, and 2048.",
    "version": "0.1.0",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>),\n  Alan Beale [dtc] (Word lists),\n  Kevin Atkinson [dtc] (Word lists),\n  Luis Von Ahn [dtc] (Word lists),\n  Flavia Rossell Hayes [ill],\n  Kristin Bott [ctb] (look_busy() status messages),\n  Daniel Chen [ctb] (ORCID: <https://orcid.org/0000-0003-3857-1741>,\n    look_busy() status messages),\n  Steven Smallberg [ctb] (look_busy() status messages)",
    "url": "https://github.com/rossellhayes/player",
    "bug_reports": "https://github.com/rossellhayes/player/issues",
    "repository": "https://cran.r-project.org/package=player",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "player Play Games in the Console Games that can be played in the R console.  Includes coin\n    flip, hangman, jumble, magic 8 ball, poker, rock paper scissors, shut\n    the box, spelling bee, and 2048.  "
  },
  {
    "id": 18018,
    "package_name": "pleiotest",
    "title": "Fast Sequential Pleiotropy Test",
    "description": "It performs a fast multi-trait genome-wide association analysis based on seemingly unrelated regressions. It tests for pleiotropic effects based on a series of Intersection-Union Wald tests. The package can handle large and unbalanced data and plot results.",
    "version": "1.0.0",
    "maintainer": "Fernando Aguate <fmaguate@gmail.com>",
    "author": "Fernando Aguate [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3608-8425>),\n  Gustavo de los Campos [aut] (ORCID:\n    <https://orcid.org/0000-0001-5692-7129>),\n  Alexander Grueneberg [ctb]",
    "url": "https://github.com/FerAguate/pleiotest",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pleiotest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pleiotest Fast Sequential Pleiotropy Test It performs a fast multi-trait genome-wide association analysis based on seemingly unrelated regressions. It tests for pleiotropic effects based on a series of Intersection-Union Wald tests. The package can handle large and unbalanced data and plot results.  "
  },
  {
    "id": 18080,
    "package_name": "plutor",
    "title": "Useful Functions for Visualization",
    "description": "In ancient Roman mythology, 'Pluto' was the ruler of the underworld\n  and presides over the afterlife. 'Pluto' was frequently conflated with \n  'Plutus', the god of wealth, because mineral wealth was found underground. \n  When plotting with R, you try once, twice, practice again and again, and finally \n  you get a pretty figure you want. It's a 'plot tour', a tour about repetition \n  and reward. Hope 'plutor' helps you on the tour!",
    "version": "0.1.0",
    "maintainer": "William Song <william_swl@163.com>",
    "author": "William Song [aut, cre]",
    "url": "https://github.com/william-swl/plutor",
    "bug_reports": "https://github.com/william-swl/plutor/issues",
    "repository": "https://cran.r-project.org/package=plutor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plutor Useful Functions for Visualization In ancient Roman mythology, 'Pluto' was the ruler of the underworld\n  and presides over the afterlife. 'Pluto' was frequently conflated with \n  'Plutus', the god of wealth, because mineral wealth was found underground. \n  When plotting with R, you try once, twice, practice again and again, and finally \n  you get a pretty figure you want. It's a 'plot tour', a tour about repetition \n  and reward. Hope 'plutor' helps you on the tour!  "
  },
  {
    "id": 18092,
    "package_name": "pmlbr",
    "title": "Interface to the Penn Machine Learning Benchmarks Data\nRepository",
    "description": "Check available classification and regression data sets from the PMLB repository and download them.\n    The PMLB repository (<https://github.com/EpistasisLab/pmlbr>) contains a curated collection of data sets for evaluating and comparing machine learning algorithms.\n    These data sets cover a range of applications, and include binary/multi-class classification problems and \n    regression problems, as well as combinations of categorical, ordinal, and continuous features.\n    There are currently over 150 datasets included in the PMLB repository.",
    "version": "0.3.0",
    "maintainer": "Trang Le <grixor@gmail.com>",
    "author": "Trang Le [aut, cre] (https://trang.page/),\n  makeyourownmaker [aut] (https://github.com/makeyourownmaker),\n  Jason Moore [aut] (http://www.epistasisblog.org/),\n  University of Pennsylvania [cph]",
    "url": "https://github.com/EpistasisLab/pmlbr",
    "bug_reports": "https://github.com/EpistasisLab/pmlbr/issues",
    "repository": "https://cran.r-project.org/package=pmlbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmlbr Interface to the Penn Machine Learning Benchmarks Data\nRepository Check available classification and regression data sets from the PMLB repository and download them.\n    The PMLB repository (<https://github.com/EpistasisLab/pmlbr>) contains a curated collection of data sets for evaluating and comparing machine learning algorithms.\n    These data sets cover a range of applications, and include binary/multi-class classification problems and \n    regression problems, as well as combinations of categorical, ordinal, and continuous features.\n    There are currently over 150 datasets included in the PMLB repository.  "
  },
  {
    "id": 18170,
    "package_name": "pompp",
    "title": "Presence-Only for Marked Point Process",
    "description": "Inspired by Moreira and Gamerman (2022) <doi:10.1214/21-AOAS1569>,\n    this methodology expands the idea by including Marks in the point process.\n    Using efficient 'C++' code, the estimation is possible and made faster with\n    'OpenMP' <https://www.openmp.org/> enabled computers. This package was\n    developed under the project PTDC/MAT-STA/28243/2017, supported by\n    Portuguese funds through the Portuguese Foundation for Science and\n    Technology (FCT).",
    "version": "0.1.3",
    "maintainer": "Guido Alberti Moreira <guidoalber@gmail.com>",
    "author": "Guido Alberti Moreira [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-7557-0874>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pompp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pompp Presence-Only for Marked Point Process Inspired by Moreira and Gamerman (2022) <doi:10.1214/21-AOAS1569>,\n    this methodology expands the idea by including Marks in the point process.\n    Using efficient 'C++' code, the estimation is possible and made faster with\n    'OpenMP' <https://www.openmp.org/> enabled computers. This package was\n    developed under the project PTDC/MAT-STA/28243/2017, supported by\n    Portuguese funds through the Portuguese Foundation for Science and\n    Technology (FCT).  "
  },
  {
    "id": 18215,
    "package_name": "postcard",
    "title": "Estimating Marginal Effects with Prognostic Covariate Adjustment",
    "description": "Conduct power analyses and inference of marginal effects. \n    Uses plug-in estimation and influence functions to perform robust inference,\n    optionally leveraging historical data to increase precision with\n    prognostic covariate adjustment. The methods are described in\n    H\u00f8jbjerre-Frandsen et al. (2025) <doi:10.48550/arXiv.2503.22284>.",
    "version": "1.1.0",
    "maintainer": "Mathias Lerbech Jeppesen <mathiasljeppesen@outlook.com>",
    "author": "Mathias Lerbech Jeppesen [aut, cre],\n  Emilie Hojbjerre-Frandsen [aut],\n  Novo Nordisk A/S [cph]",
    "url": "https://novonordisk-opensource.github.io/postcard/,\nhttps://github.com/NovoNordisk-OpenSource/postcard",
    "bug_reports": "https://github.com/NovoNordisk-OpenSource/postcard/issues",
    "repository": "https://cran.r-project.org/package=postcard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "postcard Estimating Marginal Effects with Prognostic Covariate Adjustment Conduct power analyses and inference of marginal effects. \n    Uses plug-in estimation and influence functions to perform robust inference,\n    optionally leveraging historical data to increase precision with\n    prognostic covariate adjustment. The methods are described in\n    H\u00f8jbjerre-Frandsen et al. (2025) <doi:10.48550/arXiv.2503.22284>.  "
  },
  {
    "id": 18248,
    "package_name": "powerjoin",
    "title": "Extensions of 'dplyr' and 'fuzzyjoin' Join Functions",
    "description": "We extend 'dplyr' and 'fuzzyjoin' join functions with\n    features to preprocess the data, apply various data checks, and deal with\n    conflicting columns.",
    "version": "0.1.0",
    "maintainer": "Antoine Fabri <antoine.fabri@gmail.com>",
    "author": "Antoine Fabri [aut, cre],\n  Hadley Wickham [ctb] (aut/cre of dplyr, ORCID:\n    <https://orcid.org/0000-0003-4757-117X>),\n  Romain Fran\u00e7ois [ctb] (aut of dplyr, ORCID:\n    <https://orcid.org/0000-0002-2444-4226>),\n  David Robinson [ctb] (aut of fuzzyjoin),\n  RStudio [cph, fnd] (cph/fnd dplyr)",
    "url": "https://github.com/moodymudskipper/powerjoin",
    "bug_reports": "https://github.com/moodymudskipper/powerjoin/issues",
    "repository": "https://cran.r-project.org/package=powerjoin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerjoin Extensions of 'dplyr' and 'fuzzyjoin' Join Functions We extend 'dplyr' and 'fuzzyjoin' join functions with\n    features to preprocess the data, apply various data checks, and deal with\n    conflicting columns.  "
  },
  {
    "id": 18266,
    "package_name": "pplot",
    "title": "Chronological and Ordered p-Plots for Empirical Data",
    "description": "Generates chronological and ordered p-plots for data vectors or vectors of p-values. The p-plot visualizes the evolution of the p-value of a significance test across the sampled data. It allows for assessing the consistency of the observed effects, for detecting the presence of potential moderator variables, and for estimating the influence of outlier values on the observed results. For non-significant findings, it can diagnose patterns indicative of underpowered study designs. The p-plot can thus either back the binary accept-vs-reject decision of common null-hypothesis significance tests, or it can qualify this decision and stimulate additional empirical work to arrive at more robust and replicable statistical inferences.",
    "version": "0.9",
    "maintainer": "Roland Pfister <mail@roland-pfister.net>",
    "author": "Roland Pfister [aut, cre],\n  Christian Frings [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pplot Chronological and Ordered p-Plots for Empirical Data Generates chronological and ordered p-plots for data vectors or vectors of p-values. The p-plot visualizes the evolution of the p-value of a significance test across the sampled data. It allows for assessing the consistency of the observed effects, for detecting the presence of potential moderator variables, and for estimating the influence of outlier values on the observed results. For non-significant findings, it can diagnose patterns indicative of underpowered study designs. The p-plot can thus either back the binary accept-vs-reject decision of common null-hypothesis significance tests, or it can qualify this decision and stimulate additional empirical work to arrive at more robust and replicable statistical inferences.  "
  },
  {
    "id": 18286,
    "package_name": "practicalSigni",
    "title": "Practical Significance Ranking of Regressors and Exact t Density",
    "description": "Consider a possibly nonlinear nonparametric regression\n   with p regressors. We provide evaluations by 13 methods to rank\n   regressors by their practical significance or importance using \n   various methods, including machine learning tools. Comprehensive\n   methods are as follows. \n   m6=Generalized partial correlation coefficient or\n   GPCC by Vinod (2021)<doi:10.1007/s10614-021-10190-x> and\n   Vinod (2022)<https://www.mdpi.com/1911-8074/15/1/32>.\n   m7= a generalization of psychologists' effect size incorporating \n   nonlinearity and many variables.\n   m8= local linear partial (dy/dxi) using the 'np' package for kernel \n   regressions.\n   m9= partial (dy/dxi) using the 'NNS' package.\n   m10= importance measure using the 'NNS' boost function.\n   m11= Shapley Value measure of importance (cooperative game theory).\n   m12 and m13= two versions of the random forest algorithm.\n   Taraldsen's exact density for sampling distribution of correlations added.",
    "version": "0.1.2",
    "maintainer": "Hrishikesh Vinod <vinod@fordham.edu>",
    "author": "Hrishikesh Vinod [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=practicalSigni",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "practicalSigni Practical Significance Ranking of Regressors and Exact t Density Consider a possibly nonlinear nonparametric regression\n   with p regressors. We provide evaluations by 13 methods to rank\n   regressors by their practical significance or importance using \n   various methods, including machine learning tools. Comprehensive\n   methods are as follows. \n   m6=Generalized partial correlation coefficient or\n   GPCC by Vinod (2021)<doi:10.1007/s10614-021-10190-x> and\n   Vinod (2022)<https://www.mdpi.com/1911-8074/15/1/32>.\n   m7= a generalization of psychologists' effect size incorporating \n   nonlinearity and many variables.\n   m8= local linear partial (dy/dxi) using the 'np' package for kernel \n   regressions.\n   m9= partial (dy/dxi) using the 'NNS' package.\n   m10= importance measure using the 'NNS' boost function.\n   m11= Shapley Value measure of importance (cooperative game theory).\n   m12 and m13= two versions of the random forest algorithm.\n   Taraldsen's exact density for sampling distribution of correlations added.  "
  },
  {
    "id": 18351,
    "package_name": "priceR",
    "title": "Economics and Pricing Tools",
    "description": "Functions to aid in micro and macro economic analysis and handling of price and\n    currency data. Includes extraction of relevant inflation and exchange rate data from World Bank\n    API, data cleaning/parsing, and standardisation. Inflation adjustment\n    calculations as found in Principles of Macroeconomics by Gregory Mankiw et al (2014). Current\n    and historical end of day exchange rates for 171 currencies from the European Central Bank\n    Statistical Data Warehouse (2020).",
    "version": "1.0.3",
    "maintainer": "Steve Condylios <steve.condylios@gmail.com>",
    "author": "Steve Condylios [aut, cre],\n  Bruno Mioto [ctb],\n  Bryan Shalloway [ctb]",
    "url": "https://github.com/stevecondylios/priceR",
    "bug_reports": "https://github.com/stevecondylios/priceR/issues",
    "repository": "https://cran.r-project.org/package=priceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "priceR Economics and Pricing Tools Functions to aid in micro and macro economic analysis and handling of price and\n    currency data. Includes extraction of relevant inflation and exchange rate data from World Bank\n    API, data cleaning/parsing, and standardisation. Inflation adjustment\n    calculations as found in Principles of Macroeconomics by Gregory Mankiw et al (2014). Current\n    and historical end of day exchange rates for 171 currencies from the European Central Bank\n    Statistical Data Warehouse (2020).  "
  },
  {
    "id": 18379,
    "package_name": "prithulib",
    "title": "Perform Random Experiments",
    "description": "Enables user to perform the following:\n    1. Roll 'n' number of die/dice (roll()).\n    2. Toss 'n' number of coin(s) (toss()).\n    3. Play the game of Rock, Paper, Scissors.\n    4. Choose 'n' number of card(s) from a pack of 52 playing cards (Joker optional).",
    "version": "1.0.2",
    "maintainer": "Prithul Chaturvedi <prithulc@gmail.com>",
    "author": "Prithul Chaturvedi <prithulc@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=prithulib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prithulib Perform Random Experiments Enables user to perform the following:\n    1. Roll 'n' number of die/dice (roll()).\n    2. Toss 'n' number of coin(s) (toss()).\n    3. Play the game of Rock, Paper, Scissors.\n    4. Choose 'n' number of card(s) from a pack of 52 playing cards (Joker optional).  "
  },
  {
    "id": 18430,
    "package_name": "promptr",
    "title": "Format and Complete Few-Shot LLM Prompts",
    "description": "Format and submit few-shot prompts to OpenAI's Large Language Models (LLMs). Designed to be particularly useful for text classification problems in the social sciences. Methods are described in Ornstein, Blasingame, and Truscott (2024) <https://joeornstein.github.io/publications/ornstein-blasingame-truscott.pdf>.",
    "version": "1.0.0",
    "maintainer": "Joe Ornstein <jornstein@uga.edu>",
    "author": "Joe Ornstein [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5704-2098>)",
    "url": "https://github.com/joeornstein/promptr",
    "bug_reports": "https://github.com/joeornstein/promptr/issues",
    "repository": "https://cran.r-project.org/package=promptr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "promptr Format and Complete Few-Shot LLM Prompts Format and submit few-shot prompts to OpenAI's Large Language Models (LLMs). Designed to be particularly useful for text classification problems in the social sciences. Methods are described in Ornstein, Blasingame, and Truscott (2024) <https://joeornstein.github.io/publications/ornstein-blasingame-truscott.pdf>.  "
  },
  {
    "id": 18449,
    "package_name": "proton",
    "title": "The Proton Game",
    "description": "'The Proton Game' is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  You have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  The knowledge of dplyr is not required but may be very helpful.\n  This game is linked with the ,,Pietraszko's Cave'' story available at http://biecek.pl/BetaBit/Warsaw.\n  It's a part of Beta and Bit series.\n  You will find more about the Beta and Bit series at http://biecek.pl/BetaBit.",
    "version": "1.0",
    "maintainer": "Przemys\u0142aw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemys\u0142aw Biecek [aut, cre],\n  Witold Chodor [trl],\n  Foundation SmarterPoland.pl [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=proton",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "proton The Proton Game 'The Proton Game' is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  You have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  The knowledge of dplyr is not required but may be very helpful.\n  This game is linked with the ,,Pietraszko's Cave'' story available at http://biecek.pl/BetaBit/Warsaw.\n  It's a part of Beta and Bit series.\n  You will find more about the Beta and Bit series at http://biecek.pl/BetaBit.  "
  },
  {
    "id": 18471,
    "package_name": "psBayesborrow",
    "title": "Bayesian Information Borrowing with Propensity Score Matching",
    "description": "Hybrid control design is a way to borrow information from external controls to augment concurrent controls in a randomized controlled trial and is expected to overcome the feasibility issue when adequate randomized controlled trials cannot be conducted. A major challenge in the hybrid control design is its inability to eliminate a prior-data conflict caused by systematic imbalances in measured or unmeasured confounding factors between patients in the concurrent treatment/control group and external controls. To prevent the prior-data conflict, a combined use of propensity score matching and Bayesian commensurate prior has been proposed in the context of hybrid control design. The propensity score matching is first performed to guarantee the balance in baseline characteristics, and then the Bayesian commensurate prior is constructed while discounting the information based on the similarity in outcomes between the concurrent and external controls. 'psBayesborrow' is a package to implement the propensity score matching and the Bayesian analysis with commensurate prior, as well as to conduct a simulation study to assess operating characteristics of the hybrid control design, where users can choose design parameters in flexible and straightforward ways depending on their own application.",
    "version": "1.1.0",
    "maintainer": "Yusuke Yamaguchi <yamagubed@gmail.com>",
    "author": "Yusuke Yamaguchi [aut, cre],\n  Jun Takeda [aut],\n  Kentaro Takeda [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psBayesborrow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psBayesborrow Bayesian Information Borrowing with Propensity Score Matching Hybrid control design is a way to borrow information from external controls to augment concurrent controls in a randomized controlled trial and is expected to overcome the feasibility issue when adequate randomized controlled trials cannot be conducted. A major challenge in the hybrid control design is its inability to eliminate a prior-data conflict caused by systematic imbalances in measured or unmeasured confounding factors between patients in the concurrent treatment/control group and external controls. To prevent the prior-data conflict, a combined use of propensity score matching and Bayesian commensurate prior has been proposed in the context of hybrid control design. The propensity score matching is first performed to guarantee the balance in baseline characteristics, and then the Bayesian commensurate prior is constructed while discounting the information based on the similarity in outcomes between the concurrent and external controls. 'psBayesborrow' is a package to implement the propensity score matching and the Bayesian analysis with commensurate prior, as well as to conduct a simulation study to assess operating characteristics of the hybrid control design, where users can choose design parameters in flexible and straightforward ways depending on their own application.  "
  },
  {
    "id": 18479,
    "package_name": "pscl",
    "title": "Political Science Computational Laboratory",
    "description": "Bayesian analysis of item-response theory (IRT) models,\n  roll call analysis; computing highest density regions; \n  maximum likelihood estimation of zero-inflated and hurdle models for count data;\n  goodness-of-fit measures for GLMs;\n  data sets used in writing\tand teaching; seats-votes curves.",
    "version": "1.5.9",
    "maintainer": "Simon Jackman <simon.jackman@sydney.edu.au>",
    "author": "Simon Jackman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7421-4034>),\n  Alex Tahk [ctb] (ORCID: <https://orcid.org/0000-0001-7895-9420>),\n  Achim Zeileis [ctb] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Christina Maimone [ctb] (ORCID:\n    <https://orcid.org/0000-0002-0402-6297>),\n  James Fearon [ctb],\n  Zoe Meers [ctb] (ORCID: <https://orcid.org/0000-0001-8045-6531>)",
    "url": "https://github.com/atahk/pscl",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pscl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pscl Political Science Computational Laboratory Bayesian analysis of item-response theory (IRT) models,\n  roll call analysis; computing highest density regions; \n  maximum likelihood estimation of zero-inflated and hurdle models for count data;\n  goodness-of-fit measures for GLMs;\n  data sets used in writing\tand teaching; seats-votes curves.  "
  },
  {
    "id": 18533,
    "package_name": "pttstability",
    "title": "Particle-Takens Stability",
    "description": "Includes a collection of functions presented in \"Measuring stability in ecological systems without static equilibria\" by Clark et al. (2022) <doi:10.1002/ecs2.4328> in Ecosphere.\n\tThese can be used to estimate the parameters of a stochastic state space model (i.e. a model where\n\ta time series is observed with error). The goal of this package is to estimate the variability\n\taround a deterministic process, both in terms of observation error - i.e. variability due to\n\timperfect observations that does not influence system state - and in terms of process noise - i.e.\n\tstochastic variation in the actual state of the process. Unlike classical methods for estimating\n\tvariability, this package does not necessarily assume that the deterministic state is fixed (i.e.\n\ta fixed-point equilibrium), meaning that variability around a dynamic trajectory can be estimated\n\t(e.g. stochastic fluctuations during predator-prey dynamics).",
    "version": "1.4",
    "maintainer": "Adam Clark <adam.tclark@gmail.com>",
    "author": "Adam Clark [aut, cre] (ORCID: <https://orcid.org/0000-0002-8843-3278>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pttstability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pttstability Particle-Takens Stability Includes a collection of functions presented in \"Measuring stability in ecological systems without static equilibria\" by Clark et al. (2022) <doi:10.1002/ecs2.4328> in Ecosphere.\n\tThese can be used to estimate the parameters of a stochastic state space model (i.e. a model where\n\ta time series is observed with error). The goal of this package is to estimate the variability\n\taround a deterministic process, both in terms of observation error - i.e. variability due to\n\timperfect observations that does not influence system state - and in terms of process noise - i.e.\n\tstochastic variation in the actual state of the process. Unlike classical methods for estimating\n\tvariability, this package does not necessarily assume that the deterministic state is fixed (i.e.\n\ta fixed-point equilibrium), meaning that variability around a dynamic trajectory can be estimated\n\t(e.g. stochastic fluctuations during predator-prey dynamics).  "
  },
  {
    "id": 18712,
    "package_name": "quadmesh",
    "title": "Quadrangle Mesh",
    "description": "Create surface forms from matrix or 'raster' data for flexible plotting and\n conversion to other mesh types. The functions 'quadmesh' or 'triangmesh'\n produce a continuous surface as a 'mesh3d' object as used by the 'rgl'\n package. This is used for plotting raster data in 3D (optionally with\n texture), and allows the application of a map projection without data loss and \n many processing applications that are restricted by inflexible regular grid rasters.\n There are discrete forms of these continuous surfaces available with\n 'dquadmesh' and 'dtriangmesh' functions.",
    "version": "0.5.5",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre]",
    "url": "https://github.com/hypertidy/quadmesh",
    "bug_reports": "https://github.com/hypertidy/quadmesh/issues",
    "repository": "https://cran.r-project.org/package=quadmesh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quadmesh Quadrangle Mesh Create surface forms from matrix or 'raster' data for flexible plotting and\n conversion to other mesh types. The functions 'quadmesh' or 'triangmesh'\n produce a continuous surface as a 'mesh3d' object as used by the 'rgl'\n package. This is used for plotting raster data in 3D (optionally with\n texture), and allows the application of a map projection without data loss and \n many processing applications that are restricted by inflexible regular grid rasters.\n There are discrete forms of these continuous surfaces available with\n 'dquadmesh' and 'dtriangmesh' functions.  "
  },
  {
    "id": 18767,
    "package_name": "quickregression",
    "title": "Quick Linear Regression",
    "description": "Helps to perform linear regression analysis by reducing manual effort. Reduces the independent variables based on specified p-value and Variance Inflation Factor (VIF) level.",
    "version": "0.2",
    "maintainer": "Darshan Maniyar <maniyar.darshan@gmail.com>",
    "author": "Darshan Maniyar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quickregression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quickregression Quick Linear Regression Helps to perform linear regression analysis by reducing manual effort. Reduces the independent variables based on specified p-value and Variance Inflation Factor (VIF) level.  "
  },
  {
    "id": 18836,
    "package_name": "rCoinbase",
    "title": "'Coinbase Advance Trade API Interface'",
    "description": "The 'Coinbase Advanced Trade API' <https://docs.cdp.coinbase.com/api-reference/advanced-trade-api/rest-api/introduction> lets you manage orders, portfolios, products, and fees with the new v3 endpoints.",
    "version": "1.0.0",
    "maintainer": "Jason Guevara <Jason.guevara.yt@gmail.com>",
    "author": "Jason Guevara [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rCoinbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rCoinbase 'Coinbase Advance Trade API Interface' The 'Coinbase Advanced Trade API' <https://docs.cdp.coinbase.com/api-reference/advanced-trade-api/rest-api/introduction> lets you manage orders, portfolios, products, and fees with the new v3 endpoints.  "
  },
  {
    "id": 18857,
    "package_name": "rJavaEnv",
    "title": "'Java' Environments for R Projects",
    "description": "Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.",
    "version": "0.3.0",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6690-5345>),\n  Chung-hong Chan [aut] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Mauricio Vargas [ctb] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\n  Hadley Wickham [ctb] (use_java feature suggestion and PR review),\n  Enrique Mondragon-Estrada [ctb] (ORCID:\n    <https://orcid.org/0009-0004-5592-1728>),\n  Jonas Lieth [ctb] (ORCID: <https://orcid.org/0000-0002-3451-3176>)",
    "url": "https://github.com/e-kotov/rJavaEnv,\nhttps://www.ekotov.pro/rJavaEnv/",
    "bug_reports": "https://github.com/e-kotov/rJavaEnv/issues",
    "repository": "https://cran.r-project.org/package=rJavaEnv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rJavaEnv 'Java' Environments for R Projects Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.  "
  },
  {
    "id": 18962,
    "package_name": "randomForestVIP",
    "title": "Tune Random Forests Based on Variable Importance & Plot Results",
    "description": "Functions for assessing variable relations and associations \n    prior to modeling with a Random Forest algorithm (although these are \n    relevant for any predictive model).\n    Metrics such as partial correlations and variance inflation factors\n    are tabulated as well as plotted for the user. A function is available\n    for tuning the main Random Forest hyper-parameter based on model performance \n    and variable importance metrics. This grid-search technique provides\n    tables and plots showing the effect of the main hyper-parameter on each \n    of the assessment metrics. It also returns each of the evaluated models \n    to the user. The package also provides superior variable importance plots \n    for individual models. All of the plots are developed so that the \n    user has the ability to edit and improve further upon the \n    plots. Derivations and methodology are described in Bladen (2022) \n    <https://digitalcommons.usu.edu/etd/8587/>.",
    "version": "0.1.3",
    "maintainer": "Kelvyn Bladen <kelvyn.bladen@usu.edu>",
    "author": "Kelvyn Bladen [aut, cre],\n  D. Richard Cutler [aut]",
    "url": "https://github.com/KelvynBladen/randomForestVIP",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=randomForestVIP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randomForestVIP Tune Random Forests Based on Variable Importance & Plot Results Functions for assessing variable relations and associations \n    prior to modeling with a Random Forest algorithm (although these are \n    relevant for any predictive model).\n    Metrics such as partial correlations and variance inflation factors\n    are tabulated as well as plotted for the user. A function is available\n    for tuning the main Random Forest hyper-parameter based on model performance \n    and variable importance metrics. This grid-search technique provides\n    tables and plots showing the effect of the main hyper-parameter on each \n    of the assessment metrics. It also returns each of the evaluated models \n    to the user. The package also provides superior variable importance plots \n    for individual models. All of the plots are developed so that the \n    user has the ability to edit and improve further upon the \n    plots. Derivations and methodology are described in Bladen (2022) \n    <https://digitalcommons.usu.edu/etd/8587/>.  "
  },
  {
    "id": 19059,
    "package_name": "rbedrock",
    "title": "Analysis and Manipulation of Data from Minecraft Bedrock Edition",
    "description": "Implements an interface to Minecraft (Bedrock Edition) worlds. Supports the analysis and management of these worlds and game saves.",
    "version": "0.4.2",
    "maintainer": "Reed Cartwright <racartwright@gmail.com>",
    "author": "Reed Cartwright [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0837-9380>),\n  Rich FitzJohn [ctb],\n  Christian Stigen Larsen [ctb],\n  The LevelDB Authors [cph]",
    "url": "https://reedacartwright.github.io/rbedrock/,\nhttps://github.com/reedacartwright/rbedrock",
    "bug_reports": "https://github.com/reedacartwright/rbedrock/issues",
    "repository": "https://cran.r-project.org/package=rbedrock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbedrock Analysis and Manipulation of Data from Minecraft Bedrock Edition Implements an interface to Minecraft (Bedrock Edition) worlds. Supports the analysis and management of these worlds and game saves.  "
  },
  {
    "id": 19073,
    "package_name": "rbounds",
    "title": "Perform Rosenbaum Bounds Sensitivity Tests for Matched and\nUnmatched Data",
    "description": "Takes matched and unmatched data and calculates Rosenbaum bounds for the treatment effect.  Calculates bounds for binary outcome data, Hodges-Lehmann point estimates, Wilcoxon signed-rank test for matched data and matched IV estimators, Wilcoxon sum rank test, and for data with multiple matched controls. The sensitivity analysis methods in this package are documented in Rosenbaum (2002) Observational Studies, <doi:10.1007/978-1-4757-3692-2>, Springer-Verlag.",
    "version": "2.2",
    "maintainer": "Luke J. Keele <luke.keele@gmail.com>",
    "author": "Luke J. Keele",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rbounds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbounds Perform Rosenbaum Bounds Sensitivity Tests for Matched and\nUnmatched Data Takes matched and unmatched data and calculates Rosenbaum bounds for the treatment effect.  Calculates bounds for binary outcome data, Hodges-Lehmann point estimates, Wilcoxon signed-rank test for matched data and matched IV estimators, Wilcoxon sum rank test, and for data with multiple matched controls. The sensitivity analysis methods in this package are documented in Rosenbaum (2002) Observational Studies, <doi:10.1007/978-1-4757-3692-2>, Springer-Verlag.  "
  },
  {
    "id": 19078,
    "package_name": "rbtt",
    "title": "Alternative Bootstrap-Based t-Test Aiming to Reduce Type-I Error\nfor Non-Negative, Zero-Inflated Data",
    "description": "Tu & Zhou (1999) <doi:10.1002/(SICI)1097-0258(19991030)18:20%3C2749::AID-SIM195%3E3.0.CO;2-C> showed that comparing the means of populations whose data-generating distributions are non-negative with excess zero observations is a problem of great importance in the analysis of medical cost data. In the same study, Tu & Zhou discuss that it can be difficult to control type-I error rates of general-purpose statistical tests for comparing the means of these particular data sets. This package allows users to perform a modified bootstrap-based t-test that aims to better control type-I error rates in these situations.",
    "version": "0.1.0",
    "maintainer": "Ian Waudby-Smith <iwaudbysmith@gmail.com>",
    "author": "Ian Waudby-Smith [aut, cre],\n  Pengfei Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rbtt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbtt Alternative Bootstrap-Based t-Test Aiming to Reduce Type-I Error\nfor Non-Negative, Zero-Inflated Data Tu & Zhou (1999) <doi:10.1002/(SICI)1097-0258(19991030)18:20%3C2749::AID-SIM195%3E3.0.CO;2-C> showed that comparing the means of populations whose data-generating distributions are non-negative with excess zero observations is a problem of great importance in the analysis of medical cost data. In the same study, Tu & Zhou discuss that it can be difficult to control type-I error rates of general-purpose statistical tests for comparing the means of these particular data sets. This package allows users to perform a modified bootstrap-based t-test that aims to better control type-I error rates in these situations.  "
  },
  {
    "id": 19145,
    "package_name": "rdlocrand",
    "title": "Local Randomization Methods for RD Designs",
    "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. Under the local randomization approach, RD designs can be interpreted as randomized experiments inside a window around the cutoff. This package provides tools to perform randomization inference for RD designs under local randomization: rdrandinf() to perform hypothesis testing using randomization inference, rdwinselect() to select a window around the cutoff in which randomization is likely to hold, rdsensitivity() to assess the sensitivity of the results to different window lengths and null hypotheses and rdrbounds() to construct Rosenbaum bounds for sensitivity to unobserved confounders. See Cattaneo, Titiunik and Vazquez-Bare (2016) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2016_Stata.pdf> for further methodological details.",
    "version": "1.1",
    "maintainer": "Gonzalo Vazquez-Bare <gvazquez@econ.ucsb.edu>",
    "author": "Matias D. Cattaneo [aut],\n  Rocio Titiunik [aut],\n  Gonzalo Vazquez-Bare [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rdlocrand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdlocrand Local Randomization Methods for RD Designs The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. Under the local randomization approach, RD designs can be interpreted as randomized experiments inside a window around the cutoff. This package provides tools to perform randomization inference for RD designs under local randomization: rdrandinf() to perform hypothesis testing using randomization inference, rdwinselect() to select a window around the cutoff in which randomization is likely to hold, rdsensitivity() to assess the sensitivity of the results to different window lengths and null hypotheses and rdrbounds() to construct Rosenbaum bounds for sensitivity to unobserved confounders. See Cattaneo, Titiunik and Vazquez-Bare (2016) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2016_Stata.pdf> for further methodological details.  "
  },
  {
    "id": 19167,
    "package_name": "read.gb",
    "title": "Open GenBank Files",
    "description": "Opens complete record(s) with .gb extension from the NCBI/GenBank Nucleotide database and returns a list containing shaped record(s). These kind of files contains detailed records of DNA samples (locus, organism, type of sequence, source of the sequence...). An example of record can be found at <https://www.ncbi.nlm.nih.gov/nuccore/HE799070>.",
    "version": "2.2",
    "maintainer": "Robin Mercier <robin.largon.mercier@hotmail.fr>",
    "author": "Robin Mercier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=read.gb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "read.gb Open GenBank Files Opens complete record(s) with .gb extension from the NCBI/GenBank Nucleotide database and returns a list containing shaped record(s). These kind of files contains detailed records of DNA samples (locus, organism, type of sequence, source of the sequence...). An example of record can be found at <https://www.ncbi.nlm.nih.gov/nuccore/HE799070>.  "
  },
  {
    "id": 19266,
    "package_name": "refseqR",
    "title": "Common Computational Operations Working with RefSeq Entries\n(GenBank)",
    "description": "Fetches NCBI data (RefSeq <https://www.ncbi.nlm.nih.gov/refseq/> database) and provides an environment to  \n    extract information at the level of gene, mRNA or protein accessions. ",
    "version": "1.1.5",
    "maintainer": "Jose V. Die <jose.die@uco.es>",
    "author": "Jose V. Die [aut, cre] (ORCID: <https://orcid.org/0000-0002-7506-8590>),\n  Llu\u00eds Revilla Sancho [ctb] (ORCID:\n    <https://orcid.org/0000-0001-9747-2570>)",
    "url": "https://github.com/jdieramon/refseqR",
    "bug_reports": "https://github.com/jdieramon/refseqR/issues",
    "repository": "https://cran.r-project.org/package=refseqR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refseqR Common Computational Operations Working with RefSeq Entries\n(GenBank) Fetches NCBI data (RefSeq <https://www.ncbi.nlm.nih.gov/refseq/> database) and provides an environment to  \n    extract information at the level of gene, mRNA or protein accessions.   "
  },
  {
    "id": 19337,
    "package_name": "remstats",
    "title": "Computes Statistics for Relational Event History Data",
    "description": "Computes a variety of statistics for relational event models. Relational event models enable researchers to investigate both exogenous and endogenous factors influencing the evolution of a time-ordered sequence of events. These models are categorized into tie-oriented models (Butts, C., 2008, <doi:10.1111/j.1467-9531.2008.00203.x>), where the probability of a dyad interacting next is modeled in a single step, and actor-oriented models (Stadtfeld, C., & Block, P., 2017, <doi:10.15195/v4.a14>), which first model the probability of a sender initiating an interaction and subsequently the probability of the sender's choice of receiver. The package is designed to compute a variety of statistics that summarize exogenous and endogenous influences on the event stream for both types of models.  ",
    "version": "3.2.4",
    "maintainer": "Giuseppe Arena <g.arena@uva.nl>",
    "author": "Giuseppe Arena [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5204-3326>),\n  Marlyne Meijerink-Bosman [aut],\n  Diana Karimova [ctb],\n  Rumana Lakdawala [ctb],\n  Mahdi Shafiee Kamalabad [ctb],\n  Fabio Generoso Vieira [ctb],\n  Roger Leenders [ctb],\n  Joris Mulder [ctb]",
    "url": "https://tilburgnetworkgroup.github.io/remstats/",
    "bug_reports": "https://github.com/TilburgNetworkGroup/remstats/issues",
    "repository": "https://cran.r-project.org/package=remstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "remstats Computes Statistics for Relational Event History Data Computes a variety of statistics for relational event models. Relational event models enable researchers to investigate both exogenous and endogenous factors influencing the evolution of a time-ordered sequence of events. These models are categorized into tie-oriented models (Butts, C., 2008, <doi:10.1111/j.1467-9531.2008.00203.x>), where the probability of a dyad interacting next is modeled in a single step, and actor-oriented models (Stadtfeld, C., & Block, P., 2017, <doi:10.15195/v4.a14>), which first model the probability of a sender initiating an interaction and subsequently the probability of the sender's choice of receiver. The package is designed to compute a variety of statistics that summarize exogenous and endogenous influences on the event stream for both types of models.    "
  },
  {
    "id": 19375,
    "package_name": "repurrrsive",
    "title": "Examples of Recursive Lists and Nested or Split Data Frames",
    "description": "Recursive lists in the form of R objects, 'JSON', and 'XML',\n    for use in teaching and examples. Examples include color palettes,\n    Game of Thrones characters, 'GitHub' users and repositories, music\n    collections, and entities from the Star Wars universe. Data from the\n    'gapminder' package is also included, as a simple data frame and in\n    nested and split forms.",
    "version": "1.1.0",
    "maintainer": "Jennifer Bryan <jenny@rstudio.com>",
    "author": "Jennifer Bryan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6983-2759>),\n  Charlotte Wickham [ctb],\n  Posit PBC [cph, fnd]",
    "url": "https://jennybc.github.io/repurrrsive/,\nhttps://github.com/jennybc/repurrrsive",
    "bug_reports": "https://github.com/jennybc/repurrrsive/issues",
    "repository": "https://cran.r-project.org/package=repurrrsive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "repurrrsive Examples of Recursive Lists and Nested or Split Data Frames Recursive lists in the form of R objects, 'JSON', and 'XML',\n    for use in teaching and examples. Examples include color palettes,\n    Game of Thrones characters, 'GitHub' users and repositories, music\n    collections, and entities from the Star Wars universe. Data from the\n    'gapminder' package is also included, as a simple data frame and in\n    nested and split forms.  "
  },
  {
    "id": 19387,
    "package_name": "reservoir",
    "title": "Tools for Analysis, Design, and Operation of Water Supply\nStorages",
    "description": "Measure single-storage water supply system performance using resilience,\n    reliability, and vulnerability metrics; assess storage-yield-reliability\n    relationships; determine no-fail storage with sequent peak analysis; optimize\n    release decisions for water supply, hydropower, and multi-objective reservoirs\n    using deterministic and stochastic dynamic programming; generate inflow\n    replicates using parametric and non-parametric models; evaluate inflow\n    persistence using the Hurst coefficient.",
    "version": "1.1.5",
    "maintainer": "Sean Turner <swd.turner@gmail.com>",
    "author": "Sean Turner [aut, cre],\n  Jia Yi Ng [aut],\n  Stefano Galelli [aut]",
    "url": "https://cran.r-project.org/package=reservoir",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reservoir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reservoir Tools for Analysis, Design, and Operation of Water Supply\nStorages Measure single-storage water supply system performance using resilience,\n    reliability, and vulnerability metrics; assess storage-yield-reliability\n    relationships; determine no-fail storage with sequent peak analysis; optimize\n    release decisions for water supply, hydropower, and multi-objective reservoirs\n    using deterministic and stochastic dynamic programming; generate inflow\n    replicates using parametric and non-parametric models; evaluate inflow\n    persistence using the Hurst coefficient.  "
  },
  {
    "id": 19422,
    "package_name": "retrosheet",
    "title": "Import Professional Baseball Data from 'Retrosheet'",
    "description": "A collection of tools to import and structure the (currently) single-season\n    event, game-log, roster, and schedule data available from <https://www.retrosheet.org>.\n    In particular, the event (a.k.a. play-by-play) files can be especially difficult to parse.\n    This package does the parsing on those files, returning the requested data in the most\n    practical R structure to use for sabermetric or other analyses.",
    "version": "1.1.6",
    "maintainer": "Colin Douglas <colin@douglas.science>",
    "author": "Colin Douglas [aut, cre, cph],\n  Richard Scriven [aut, cph]",
    "url": "https://github.com/colindouglas/retrosheet",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=retrosheet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "retrosheet Import Professional Baseball Data from 'Retrosheet' A collection of tools to import and structure the (currently) single-season\n    event, game-log, roster, and schedule data available from <https://www.retrosheet.org>.\n    In particular, the event (a.k.a. play-by-play) files can be especially difficult to parse.\n    This package does the parsing on those files, returning the requested data in the most\n    practical R structure to use for sabermetric or other analyses.  "
  },
  {
    "id": 19440,
    "package_name": "rfVarImpOOB",
    "title": "Unbiased Variable Importance for Random Forests",
    "description": "Computes a novel variable importance for random forests: Impurity reduction importance scores for out-of-bag (OOB) data complementing the existing inbag Gini importance, see also <doi: 10.1080/03610926.2020.1764042>. \n    The Gini impurities for inbag and OOB data are combined in three different ways, after which the information gain is computed at each split.\n    This gain is aggregated for each split variable in a tree and averaged across trees.",
    "version": "1.0.3",
    "maintainer": "Markus Loecher <Markus.Loecher@gmail.com>",
    "author": "Markus Loecher <Markus.Loecher@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rfVarImpOOB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfVarImpOOB Unbiased Variable Importance for Random Forests Computes a novel variable importance for random forests: Impurity reduction importance scores for out-of-bag (OOB) data complementing the existing inbag Gini importance, see also <doi: 10.1080/03610926.2020.1764042>. \n    The Gini impurities for inbag and OOB data are combined in three different ways, after which the information gain is computed at each split.\n    This gain is aggregated for each split variable in a tree and averaged across trees.  "
  },
  {
    "id": 19459,
    "package_name": "rgdax",
    "title": "Wrapper for 'Coinbase Pro (erstwhile GDAX)' Cryptocurrency\nExchange",
    "description": "Allow access to both public and private end points to Coinbase Pro (erstwhile GDAX) \n    cryptocurrency exchange. For authenticated flow, users must have valid api, secret and passphrase to be able to connect.",
    "version": "1.2.1",
    "maintainer": "Dheeraj Agarwal <dheeeraj.agarwal@gmail.com>",
    "author": "Dheeraj Agarwal [aut, cre]",
    "url": "https://github.com/DheerajAgarwal/rgdax/",
    "bug_reports": "https://github.com/DheerajAgarwal/rgdax/issues",
    "repository": "https://cran.r-project.org/package=rgdax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgdax Wrapper for 'Coinbase Pro (erstwhile GDAX)' Cryptocurrency\nExchange Allow access to both public and private end points to Coinbase Pro (erstwhile GDAX) \n    cryptocurrency exchange. For authenticated flow, users must have valid api, secret and passphrase to be able to connect.  "
  },
  {
    "id": 19508,
    "package_name": "ridgregextra",
    "title": "Ridge Regression Parameter Estimation",
    "description": "It is a package that provides alternative approach for finding optimum parameters of ridge regression. This package focuses on finding the ridge parameter value k which makes the variance inflation factors closest to 1, while keeping them above 1 as addressed by Michael Kutner, Christopher Nachtsheim, John Neter, William Li (2004, ISBN:978-0073108742). Moreover, the package offers end-to-end functionality to find optimum k value and presents the detailed ridge regression results. Finally it shows three sets of graphs consisting k versus variance inflation factors, regression coefficients and standard errors of them. ",
    "version": "0.1.1",
    "maintainer": "Olgun Aydin <olgun.aydin@pg.edu.pl>",
    "author": "Filiz Karadag [aut] (ORCID: <https://orcid.org/0000-0002-0116-7772>),\n  Hakan Savas Sazak [aut] (ORCID:\n    <https://orcid.org/0000-0001-6123-1214>),\n  Olgun Aydin [cre] (ORCID: <https://orcid.org/0000-0002-7090-0931>)",
    "url": "https://github.com/filizkrdg/ridgregextra",
    "bug_reports": "https://github.com/filizkrdg/ridgregextra/issues",
    "repository": "https://cran.r-project.org/package=ridgregextra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ridgregextra Ridge Regression Parameter Estimation It is a package that provides alternative approach for finding optimum parameters of ridge regression. This package focuses on finding the ridge parameter value k which makes the variance inflation factors closest to 1, while keeping them above 1 as addressed by Michael Kutner, Christopher Nachtsheim, John Neter, William Li (2004, ISBN:978-0073108742). Moreover, the package offers end-to-end functionality to find optimum k value and presents the detailed ridge regression results. Finally it shows three sets of graphs consisting k versus variance inflation factors, regression coefficients and standard errors of them.   "
  },
  {
    "id": 19512,
    "package_name": "rifreg",
    "title": "Estimate Recentered Influence Function Regression",
    "description": "Provides functions to compute recentered influence functions\n    (RIF) of a distributional variable at the mean, quantiles, variance,\n    gini or any custom functional of interest. The package allows to\n    regress the RIF on any number of covariates. Generic print, plot and\n    summary functions are also provided. Reference: Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. (2009) <doi:10.3982/ECTA6822>. \"Unconditional Quantile Regressions.\".",
    "version": "1.1.0",
    "maintainer": "Samuel Meier <samuel.meier+cran@immerda.ch>",
    "author": "David Gallusser [aut],\n  Samuel Meier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rifreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rifreg Estimate Recentered Influence Function Regression Provides functions to compute recentered influence functions\n    (RIF) of a distributional variable at the mean, quantiles, variance,\n    gini or any custom functional of interest. The package allows to\n    regress the RIF on any number of covariates. Generic print, plot and\n    summary functions are also provided. Reference: Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. (2009) <doi:10.3982/ECTA6822>. \"Unconditional Quantile Regressions.\".  "
  },
  {
    "id": 19549,
    "package_name": "rjade",
    "title": "A Clean, Whitespace-Sensitive Template Language for Writing HTML",
    "description": "Jade is a high performance template engine heavily influenced by\n    Haml and implemented with JavaScript for node and browsers.",
    "version": "0.1.1",
    "maintainer": "Jeroen Ooms <jeroen@berkeley.edu>",
    "author": "Jeroen Ooms, Forbes Lindesay",
    "url": "https://github.com/jeroen/rjade https://www.npmjs.com/package/jade",
    "bug_reports": "https://github.com/jeroen/rjade/issues",
    "repository": "https://cran.r-project.org/package=rjade",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjade A Clean, Whitespace-Sensitive Template Language for Writing HTML Jade is a high performance template engine heavily influenced by\n    Haml and implemented with JavaScript for node and browsers.  "
  },
  {
    "id": 19646,
    "package_name": "rnr",
    "title": "Rosenbaum and Rubin Sensitivity",
    "description": "Apply sensitivity analysis for offline policy evaluation, as\n    implemented in Jung et al. (2017) <arXiv:1702.04690> based\n    on Rosenbaum and Rubin (1983) <http://www.jstor.org/stable/2345524>.",
    "version": "0.2.1",
    "maintainer": "Jongbin Jung <me@jongbin.com>",
    "author": "Jongbin Jung",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rnr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rnr Rosenbaum and Rubin Sensitivity Apply sensitivity analysis for offline policy evaluation, as\n    implemented in Jung et al. (2017) <arXiv:1702.04690> based\n    on Rosenbaum and Rubin (1983) <http://www.jstor.org/stable/2345524>.  "
  },
  {
    "id": 19695,
    "package_name": "robustmeta",
    "title": "Robust Inference for Meta-Analysis with Influential Outlying\nStudies",
    "description": "Robust inference methods for fixed-effect and random-effects models of meta-analysis are implementable. The robust methods are developed using the density power divergence that is a robust estimating criterion developed in machine learning theory, and can effectively circumvent biases and misleading results caused by influential outliers. The density power divergence is originally introduced by Basu et al. (1998) <doi:10.1093/biomet/85.3.549>, and the meta-analysis methods are developed by Noma et al. (2022) <forthcoming>.",
    "version": "1.2-1",
    "maintainer": "Hisashi Noma <noma@ism.ac.jp>",
    "author": "Hisashi Noma [aut, cre],\n  Shonosuke Sugasawa [aut],\n  Toshi A. Furukawa [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=robustmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robustmeta Robust Inference for Meta-Analysis with Influential Outlying\nStudies Robust inference methods for fixed-effect and random-effects models of meta-analysis are implementable. The robust methods are developed using the density power divergence that is a robust estimating criterion developed in machine learning theory, and can effectively circumvent biases and misleading results caused by influential outliers. The density power divergence is originally introduced by Basu et al. (1998) <doi:10.1093/biomet/85.3.549>, and the meta-analysis methods are developed by Noma et al. (2022) <forthcoming>.  "
  },
  {
    "id": 19728,
    "package_name": "rollup",
    "title": "A Tidy Grouping Set Aggregation",
    "description": "A Tidy implementation of 'grouping sets', 'rollup' and 'cube' - extensions of the 'group_by' clause that allow for computing multiple 'group_by' clauses in a single statement. For more detailed information on these functions, please refer to \"Enhanced Aggregation, Cube, Grouping and Rollup\" <https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup>.",
    "version": "0.1.0",
    "maintainer": "Ju Young Ahn <juyoung.ahn@snu.ac.kr>",
    "author": "Ju Young Ahn [aut, cre] (ORCID:\n    <https://orcid.org/0009-0008-4613-3438>)",
    "url": "https://juyoungahn.github.io/rollup/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rollup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rollup A Tidy Grouping Set Aggregation A Tidy implementation of 'grouping sets', 'rollup' and 'cube' - extensions of the 'group_by' clause that allow for computing multiple 'group_by' clauses in a single statement. For more detailed information on these functions, please refer to \"Enhanced Aggregation, Cube, Grouping and Rollup\" <https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup>.  "
  },
  {
    "id": 19791,
    "package_name": "rpms",
    "title": "Recursive Partitioning for Modeling Survey Data",
    "description": "Functions to allow users to build and analyze design consistent \n    tree and random forest models using survey data from a complex sample \n    design.  The tree model algorithm can fit a linear model to survey data \n    in each node obtained by recursively partitioning the data.  The splitting \n    variables and selected splits are obtained using a randomized permutation \n    test procedure which adjusted for complex sample design features used to \n    obtain the data. Likewise the model fitting algorithm produces \n    design-consistent coefficients to any specified least squares linear model \n    between the dependent and independent variables used in the end nodes.\n    The main functions return the resulting binary tree or random forest as \n    an object of \"rpms\" or \"rpms_forest\" type. The package also provides methods\n    modeling a \"boosted\" tree or forest model and a tree model for zero-inflated\n    data as well as a number of functions and methods available for use with \n    these object types.",
    "version": "0.5.1",
    "maintainer": "Daniell Toth <danielltoth@yahoo.com>",
    "author": "Daniell Toth [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rpms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpms Recursive Partitioning for Modeling Survey Data Functions to allow users to build and analyze design consistent \n    tree and random forest models using survey data from a complex sample \n    design.  The tree model algorithm can fit a linear model to survey data \n    in each node obtained by recursively partitioning the data.  The splitting \n    variables and selected splits are obtained using a randomized permutation \n    test procedure which adjusted for complex sample design features used to \n    obtain the data. Likewise the model fitting algorithm produces \n    design-consistent coefficients to any specified least squares linear model \n    between the dependent and independent variables used in the end nodes.\n    The main functions return the resulting binary tree or random forest as \n    an object of \"rpms\" or \"rpms_forest\" type. The package also provides methods\n    modeling a \"boosted\" tree or forest model and a tree model for zero-inflated\n    data as well as a number of functions and methods available for use with \n    these object types.  "
  },
  {
    "id": 19855,
    "package_name": "rsmatch",
    "title": "Matching Methods for Time-Varying Observational Studies",
    "description": "Implements popular methods for matching in time-varying\n    observational studies. Matching is difficult in this scenario because\n    participants can be treated at different times which may have an\n    influence on the outcomes. The core methods include: \"Balanced Risk\n    Set Matching\" from Li, Propert, and Rosenbaum (2011)\n    <doi:10.1198/016214501753208573> and \"Propensity Score Matching with\n    Time-Dependent Covariates\" from Lu (2005)\n    <doi:10.1111/j.1541-0420.2005.00356.x>. Some functions use the\n    'Gurobi' optimization back-end to improve the optimization problem\n    speed; the 'gurobi' R package and associated software can be\n    downloaded from <https://www.gurobi.com> after obtaining a license.",
    "version": "0.2.1",
    "maintainer": "Sean Kent <skent259@gmail.com>",
    "author": "Sean Kent [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8697-9069>),\n  Mitchell Paukner [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-3839-5311>)",
    "url": "https://skent259.github.io/rsmatch/,\nhttps://github.com/skent259/rsmatch",
    "bug_reports": "https://github.com/skent259/rsmatch/issues",
    "repository": "https://cran.r-project.org/package=rsmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsmatch Matching Methods for Time-Varying Observational Studies Implements popular methods for matching in time-varying\n    observational studies. Matching is difficult in this scenario because\n    participants can be treated at different times which may have an\n    influence on the outcomes. The core methods include: \"Balanced Risk\n    Set Matching\" from Li, Propert, and Rosenbaum (2011)\n    <doi:10.1198/016214501753208573> and \"Propensity Score Matching with\n    Time-Dependent Covariates\" from Lu (2005)\n    <doi:10.1111/j.1541-0420.2005.00356.x>. Some functions use the\n    'Gurobi' optimization back-end to improve the optimization problem\n    speed; the 'gurobi' R package and associated software can be\n    downloaded from <https://www.gurobi.com> after obtaining a license.  "
  },
  {
    "id": 19895,
    "package_name": "rt3",
    "title": "Tic-Tac-Toe Package for R",
    "description": "Play the classic game of tic-tac-toe (naughts and crosses).",
    "version": "0.1.2",
    "maintainer": "Johan Jordaan <djjordaan@gmail.com>",
    "author": "Johan Jordaan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rt3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rt3 Tic-Tac-Toe Package for R Play the classic game of tic-tac-toe (naughts and crosses).  "
  },
  {
    "id": 19944,
    "package_name": "runexp",
    "title": "Softball Run Expectancy using Markov Chains and Simulation",
    "description": "Implements two methods of estimating runs scored in a softball \n    scenario: (1) theoretical expectation using discrete Markov chains and (2) empirical\n    distribution using multinomial random simulation.  Scores are based on player-specific input \n    probabilities (out, single, double, triple, walk, and homerun).  Optional inputs include probability\n    of attempting a steal, probability of succeeding in an attempted steal, and an indicator of whether\n    a player is \"fast\" (e.g. the player could stretch home).  These probabilities may be \n    calculated from common player statistics that are publicly available on team's webpages. \n    Scores are evaluated based on a nine-player lineup and may be used to compare lineups, \n    evaluate base scenarios, and compare the offensive potential of individual players.  \n    Manuscript forthcoming.  See Bukiet & Harold (1997) <doi:10.1287/opre.45.1.14> for \n    implementation of discrete Markov chains. ",
    "version": "0.2.1",
    "maintainer": "Annie Sauer <anniees@vt.edu>",
    "author": "Annie Sauer [aut, cre],\n  Sierra Merkes [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=runexp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "runexp Softball Run Expectancy using Markov Chains and Simulation Implements two methods of estimating runs scored in a softball \n    scenario: (1) theoretical expectation using discrete Markov chains and (2) empirical\n    distribution using multinomial random simulation.  Scores are based on player-specific input \n    probabilities (out, single, double, triple, walk, and homerun).  Optional inputs include probability\n    of attempting a steal, probability of succeeding in an attempted steal, and an indicator of whether\n    a player is \"fast\" (e.g. the player could stretch home).  These probabilities may be \n    calculated from common player statistics that are publicly available on team's webpages. \n    Scores are evaluated based on a nine-player lineup and may be used to compare lineups, \n    evaluate base scenarios, and compare the offensive potential of individual players.  \n    Manuscript forthcoming.  See Bukiet & Harold (1997) <doi:10.1287/opre.45.1.14> for \n    implementation of discrete Markov chains.   "
  },
  {
    "id": 19963,
    "package_name": "rvif",
    "title": "Collinearity Detection using Redefined Variance Inflation Factor\nand Graphical Methods",
    "description": "The detection of troubling approximate collinearity in a multiple linear regression model is a classical problem in Econometrics. This package is focused on determining whether or not the degree of approximate multicollinearity in a multiple linear regression model is of concern, meaning that it affects the statistical analysis (i.e. individual significance tests) of the model. This objective is achieved by using the variance inflation factor redefined and the scatterplot between the variance inflation factor and the coefficient of variation. For more details see Salmer\u00f3n R., Garc\u00eda C.B. and Garc\u00eda J. (2018) <doi:10.1080/00949655.2018.1463376>, Salmer\u00f3n, R., Rodr\u00edguez, A. and Garc\u00eda C. (2020) <doi:10.1007/s00180-019-00922-x>, Salmer\u00f3n, R., Garc\u00eda, C.B, Rodr\u00edguez, A. and Garc\u00eda, C. (2022) <doi:10.32614/RJ-2023-010>, Salmer\u00f3n, R., Garc\u00eda, C.B. and Garc\u00eda, J. (2025) <doi:10.1007/s10614-024-10575-8> and Salmer\u00f3n, R., Garc\u00eda, C.B, Garc\u00eda J. (2023, working paper) <doi:10.48550/arXiv.2005.02245>. You can also view the package vignette using 'browseVignettes(\"rvif\")', the package website (<https://www.ugr.es/local/romansg/rvif/index.html>) using 'browseURL(system.file(\"docs/index.html\", package = \"rvif\"))' or version control on GitHub (<https://github.com/rnoremlas/rvif_package>).",
    "version": "3.2",
    "maintainer": "R. Salmer\u00f3n <romansg@ugr.es>",
    "author": "R. Salmer\u00f3n [aut, cre],\n  C.B. Garc\u00eda [aut]",
    "url": "http://colldetreat.r-forge.r-project.org/,\nhttps://github.com/rnoremlas/rvif_package,\nhttps://www.ugr.es/local/romansg/rvif/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rvif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvif Collinearity Detection using Redefined Variance Inflation Factor\nand Graphical Methods The detection of troubling approximate collinearity in a multiple linear regression model is a classical problem in Econometrics. This package is focused on determining whether or not the degree of approximate multicollinearity in a multiple linear regression model is of concern, meaning that it affects the statistical analysis (i.e. individual significance tests) of the model. This objective is achieved by using the variance inflation factor redefined and the scatterplot between the variance inflation factor and the coefficient of variation. For more details see Salmer\u00f3n R., Garc\u00eda C.B. and Garc\u00eda J. (2018) <doi:10.1080/00949655.2018.1463376>, Salmer\u00f3n, R., Rodr\u00edguez, A. and Garc\u00eda C. (2020) <doi:10.1007/s00180-019-00922-x>, Salmer\u00f3n, R., Garc\u00eda, C.B, Rodr\u00edguez, A. and Garc\u00eda, C. (2022) <doi:10.32614/RJ-2023-010>, Salmer\u00f3n, R., Garc\u00eda, C.B. and Garc\u00eda, J. (2025) <doi:10.1007/s10614-024-10575-8> and Salmer\u00f3n, R., Garc\u00eda, C.B, Garc\u00eda J. (2023, working paper) <doi:10.48550/arXiv.2005.02245>. You can also view the package vignette using 'browseVignettes(\"rvif\")', the package website (<https://www.ugr.es/local/romansg/rvif/index.html>) using 'browseURL(system.file(\"docs/index.html\", package = \"rvif\"))' or version control on GitHub (<https://github.com/rnoremlas/rvif_package>).  "
  },
  {
    "id": 19968,
    "package_name": "rvolleydata",
    "title": "Extract Data from Professional Volleyball Leagues in North\nAmerica",
    "description": "Gather boxscore, play-by-play, and auxiliary data from Major League Volleyball (MLV) <https://provolleyball.com>, League One Volleyball Pro (LOVB Pro) <https://www.lovb.com/pro-league>, and Athletes Unlimited Pro Volleyball <https://auprosports.com/volleyball/> to create a repository of basic and advanced statistics for teams and players.",
    "version": "1.1.0",
    "maintainer": "David Awosoga <odo.awosoga@gmail.com>",
    "author": "David Awosoga [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2741-5770>),\n  Matthew Chow [aut] (ORCID: <https://orcid.org/0009-0009-6045-6839>),\n  Ryan Du [aut]",
    "url": "https://github.com/awosoga/rvolleydata,\nhttps://awosoga.github.io/rvolleydata/",
    "bug_reports": "https://github.com/awosoga/rvolleydata/issues",
    "repository": "https://cran.r-project.org/package=rvolleydata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvolleydata Extract Data from Professional Volleyball Leagues in North\nAmerica Gather boxscore, play-by-play, and auxiliary data from Major League Volleyball (MLV) <https://provolleyball.com>, League One Volleyball Pro (LOVB Pro) <https://www.lovb.com/pro-league>, and Athletes Unlimited Pro Volleyball <https://auprosports.com/volleyball/> to create a repository of basic and advanced statistics for teams and players.  "
  },
  {
    "id": 19971,
    "package_name": "rwarrior",
    "title": "R Warrior - An AI Programming Game",
    "description": "A port of Ruby Warrior.\n    Teaches R programming in a fun and interactive way.",
    "version": "0.4.1",
    "maintainer": "Rick M Tankard <rickmtankard@gmail.com>",
    "author": "Rick M Tankard [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-8847-9401>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rwarrior",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rwarrior R Warrior - An AI Programming Game A port of Ruby Warrior.\n    Teaches R programming in a fun and interactive way.  "
  },
  {
    "id": 20031,
    "package_name": "saeHB.ZIB",
    "title": "Small Area Estimation using Hierarchical Bayesian under Zero\nInflated Binomial Distribution",
    "description": "Provides function for area level of small area estimation using hierarchical Bayesian (HB) method with Zero-Inflated Binomial distribution for variables of interest. Some dataset produced by a data generation are also provided. The 'rjags' package is employed to obtain parameter estimates. Model-based estimators involves the HB estimators which include the mean and the variation of mean.",
    "version": "0.1.1",
    "maintainer": "Rizqina Rahmati <221810583@stis.ac.id>",
    "author": "Rizqina Rahmati, Azka Ubaidillah",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=saeHB.ZIB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saeHB.ZIB Small Area Estimation using Hierarchical Bayesian under Zero\nInflated Binomial Distribution Provides function for area level of small area estimation using hierarchical Bayesian (HB) method with Zero-Inflated Binomial distribution for variables of interest. Some dataset produced by a data generation are also provided. The 'rjags' package is employed to obtain parameter estimates. Model-based estimators involves the HB estimators which include the mean and the variation of mean.  "
  },
  {
    "id": 20044,
    "package_name": "saeczi",
    "title": "Small Area Estimation for Continuous Zero Inflated Data",
    "description": "Provides functionality to fit a zero-inflated estimator for small area estimation.\n    This estimator is a combines a linear mixed effects regression model and a logistic\n    mixed effects regression model via a two-stage modeling approach. The estimator's mean\n    squared error is estimated via a parametric bootstrap method. Chandra and others\n    (2012, <doi:10.1080/03610918.2011.598991>) introduce and describe this estimator and mean\n    squared error estimator. White and others (2024+, <doi:10.48550/arXiv.2402.03263>) describe the \n    applicability of this estimator to estimation of forest attributes and further assess the\n    estimator's properties. ",
    "version": "0.2.0",
    "maintainer": "Josh Yamamoto <joshuayamamoto5@gmail.com>",
    "author": "Josh Yamamoto [aut, cre],\n  Dinan Elsyad [aut],\n  Grayson White [aut],\n  Julian Schmitt [aut],\n  Niels Korsgaard [aut],\n  Kelly McConville [aut],\n  Kate Hu [aut]",
    "url": "https://harvard-ufds.github.io/saeczi/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=saeczi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saeczi Small Area Estimation for Continuous Zero Inflated Data Provides functionality to fit a zero-inflated estimator for small area estimation.\n    This estimator is a combines a linear mixed effects regression model and a logistic\n    mixed effects regression model via a two-stage modeling approach. The estimator's mean\n    squared error is estimated via a parametric bootstrap method. Chandra and others\n    (2012, <doi:10.1080/03610918.2011.598991>) introduce and describe this estimator and mean\n    squared error estimator. White and others (2024+, <doi:10.48550/arXiv.2402.03263>) describe the \n    applicability of this estimator to estimation of forest attributes and further assess the\n    estimator's properties.   "
  },
  {
    "id": 20067,
    "package_name": "sambia",
    "title": "A Collection of Techniques Correcting for Sample Selection Bias",
    "description": "A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods \"stochastic inverse-probability oversampling\" and \"parametric inverse-probability bagging\" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) <doi:10.1155/2017/7847531>. The methods may be used for further purposes where weighting and generation of new observations is needed.",
    "version": "0.1.0",
    "maintainer": "Norbert Krautenbacher <norbert.krautenbacher@tum.de>",
    "author": "Norbert Krautenbacher, Kevin Strauss, Maximilian Mandl, Christiane Fuchs",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sambia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sambia A Collection of Techniques Correcting for Sample Selection Bias A collection of various techniques correcting statistical models for sample selection bias is provided. In particular, the resampling-based methods \"stochastic inverse-probability oversampling\" and \"parametric inverse-probability bagging\" are placed at the disposal which generate synthetic observations for correcting classifiers for biased samples resulting from stratified random sampling. For further information, see the article Krautenbacher, Theis, and Fuchs (2017) <doi:10.1155/2017/7847531>. The methods may be used for further purposes where weighting and generation of new observations is needed.  "
  },
  {
    "id": 20076,
    "package_name": "samplesize",
    "title": "Sample Size Calculation for Various t-Tests and Wilcoxon-Test",
    "description": "Computes sample size for Student's t-test and for the Wilcoxon-Mann-Whitney test for categorical data. The t-test function allows paired and unpaired (balanced / unbalanced) designs as well as homogeneous and heterogeneous variances. The Wilcoxon function allows for ties.",
    "version": "0.2-4",
    "maintainer": "Ralph Scherer <shearer.ra76@gmail.com>",
    "author": "Ralph Scherer",
    "url": "https://github.com/shearer/samplesize",
    "bug_reports": "https://github.com/shearer/samplesize/issues",
    "repository": "https://cran.r-project.org/package=samplesize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "samplesize Sample Size Calculation for Various t-Tests and Wilcoxon-Test Computes sample size for Student's t-test and for the Wilcoxon-Mann-Whitney test for categorical data. The t-test function allows paired and unpaired (balanced / unbalanced) designs as well as homogeneous and heterogeneous variances. The Wilcoxon function allows for ties.  "
  },
  {
    "id": 20111,
    "package_name": "sapo",
    "title": "Spatial Association of Different Types of Polygon",
    "description": "In ecology, spatial data is often represented using polygons. These\n  polygons can represent a variety of spatial entities, such as ecological\n  patches, animal home ranges, or gaps in the forest canopy.  Researchers often\n  need to determine if two spatial processes, represented by these polygons, are\n  independent of each other. For instance, they might want to test if the home\n  range of a particular animal species is influenced by the presence of a\n  certain type of vegetation.  To address this, Godoy et al. (2022)\n  (<doi:10.1016/j.spasta.2022.100695>) developed conditional Monte Carlo\n  tests. These tests are designed to assess spatial independence while taking\n  into account the shape and size of the polygons.",
    "version": "0.8.0",
    "maintainer": "Lucas da Cunha Godoy <lcgodoy@duck.com>",
    "author": "Lucas da Cunha Godoy [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4265-972X>)",
    "url": "https://github.com/lcgodoy/sapo/",
    "bug_reports": "https://github.com/lcgodoy/sapo/issues/",
    "repository": "https://cran.r-project.org/package=sapo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sapo Spatial Association of Different Types of Polygon In ecology, spatial data is often represented using polygons. These\n  polygons can represent a variety of spatial entities, such as ecological\n  patches, animal home ranges, or gaps in the forest canopy.  Researchers often\n  need to determine if two spatial processes, represented by these polygons, are\n  independent of each other. For instance, they might want to test if the home\n  range of a particular animal species is influenced by the presence of a\n  certain type of vegetation.  To address this, Godoy et al. (2022)\n  (<doi:10.1016/j.spasta.2022.100695>) developed conditional Monte Carlo\n  tests. These tests are designed to assess spatial independence while taking\n  into account the shape and size of the polygons.  "
  },
  {
    "id": 20162,
    "package_name": "scModels",
    "title": "Fitting Discrete Distribution Models to Count Data",
    "description": "Provides functions for fitting discrete distribution models to count data.\n  Included are the Poisson, the negative binomial, the Poisson-inverse gaussian and, most importantly,\n  a new implementation of the Poisson-beta distribution (density, distribution and quantile\n  functions, and random number generator) together with a needed new implementation of\n  Kummer's function (also: confluent hypergeometric function of the first kind). Three\n  different implementations of the Gillespie algorithm allow data simulation based on the\n  basic, switching or bursting mRNA generating processes. Moreover, likelihood functions for\n  four variants of each of the three aforementioned distributions are also available.\n  The variants include one population and two population mixtures, both with and without\n  zero-inflation. The package depends on the 'MPFR' libraries (<https://www.mpfr.org/>) which need to be installed separately \n  (see description at <https://github.com/fuchslab/scModels>).\n  This package is supplement to the paper \"A mechanistic model for the negative binomial distribution of single-cell mRNA counts\" \n  by Lisa Amrhein, Kumar Harsha and Christiane Fuchs (2019) <doi:10.1101/657619> available on bioRxiv.",
    "version": "1.0.4",
    "maintainer": "Lisa Amrhein <amrheinlisa@gmail.com>",
    "author": "Lisa Amrhein [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0370-624X>),\n  Kumar Harsha [aut] (ORCID: <https://orcid.org/0000-0002-3865-5286>),\n  Christiane Fuchs [aut] (ORCID: <https://orcid.org/0000-0003-3565-8315>),\n  Pavel Holoborodko [ctb] (Author and copyright holder of 'mpreal.h')",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scModels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scModels Fitting Discrete Distribution Models to Count Data Provides functions for fitting discrete distribution models to count data.\n  Included are the Poisson, the negative binomial, the Poisson-inverse gaussian and, most importantly,\n  a new implementation of the Poisson-beta distribution (density, distribution and quantile\n  functions, and random number generator) together with a needed new implementation of\n  Kummer's function (also: confluent hypergeometric function of the first kind). Three\n  different implementations of the Gillespie algorithm allow data simulation based on the\n  basic, switching or bursting mRNA generating processes. Moreover, likelihood functions for\n  four variants of each of the three aforementioned distributions are also available.\n  The variants include one population and two population mixtures, both with and without\n  zero-inflation. The package depends on the 'MPFR' libraries (<https://www.mpfr.org/>) which need to be installed separately \n  (see description at <https://github.com/fuchslab/scModels>).\n  This package is supplement to the paper \"A mechanistic model for the negative binomial distribution of single-cell mRNA counts\" \n  by Lisa Amrhein, Kumar Harsha and Christiane Fuchs (2019) <doi:10.1101/657619> available on bioRxiv.  "
  },
  {
    "id": 20183,
    "package_name": "scan",
    "title": "Single-Case Data Analyses for Single and Multiple Baseline\nDesigns",
    "description": "A collection of procedures for analysing, visualising, \n  and managing single-case data. These include regression models \n  (multilevel, multivariate, bayesian), between case standardised mean difference, \n  overlap indices ('PND', 'PEM', 'PAND', 'PET', 'tau-u', 'IRD', 'baseline corrected tau', \n  'CDC'), and randomization tests. Data preparation functions support outlier \n  detection, handling missing values, scaling, and custom transformations. \n  An export function helps to generate html, word, and latex tables in a \n  publication friendly style. A shiny app allows to use scan in a graphical \n  user interface.\n  More details can be found in the online book 'Analyzing single-case data with \n  R and scan', Juergen Wilbert (2025)\n  <https://jazznbass.github.io/scan-Book/>.",
    "version": "0.67.0",
    "maintainer": "Juergen Wilbert <juergen.wilbert@uni-muenster.de>",
    "author": "Juergen Wilbert [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-8392-2873>),\n  Timo Lueke [aut] (ORCID: <https://orcid.org/0000-0002-2603-7341>)",
    "url": "https://github.com/jazznbass/scan/,\nhttps://jazznbass.github.io/scan-Book/,\nhttps://jazznbass.github.io/scan/",
    "bug_reports": "https://github.com/jazznbass/scan/issues",
    "repository": "https://cran.r-project.org/package=scan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scan Single-Case Data Analyses for Single and Multiple Baseline\nDesigns A collection of procedures for analysing, visualising, \n  and managing single-case data. These include regression models \n  (multilevel, multivariate, bayesian), between case standardised mean difference, \n  overlap indices ('PND', 'PEM', 'PAND', 'PET', 'tau-u', 'IRD', 'baseline corrected tau', \n  'CDC'), and randomization tests. Data preparation functions support outlier \n  detection, handling missing values, scaling, and custom transformations. \n  An export function helps to generate html, word, and latex tables in a \n  publication friendly style. A shiny app allows to use scan in a graphical \n  user interface.\n  More details can be found in the online book 'Analyzing single-case data with \n  R and scan', Juergen Wilbert (2025)\n  <https://jazznbass.github.io/scan-Book/>.  "
  },
  {
    "id": 20202,
    "package_name": "scdensity",
    "title": "Shape-Constrained Kernel Density Estimation",
    "description": "Implements methods for obtaining kernel density estimates\n    subject to a variety of shape constraints (unimodality, bimodality, \n    symmetry, tail monotonicity, bounds, and constraints on the number of \n    inflection points). Enforcing constraints can eliminate unwanted waves or \n    kinks in the estimate, which improves its subjective appearance and can \n    also improve statistical performance. The main function scdensity() is \n    very similar to the density() function in 'stats', allowing \n    shape-restricted estimates to be obtained with little effort. The \n    methods implemented in this package are described in Wolters and Braun \n    (2017) <doi:10.1080/03610918.2017.1288247>, Wolters (2012) \n    <doi:10.18637/jss.v047.i06>, and Hall and Huang (2002) \n    <https://www3.stat.sinica.edu.tw/statistica/j12n4/j12n41/j12n41.htm>.\n    See the scdensity() help for for full citations.",
    "version": "1.0.3",
    "maintainer": "Mark A. Wolters <mark@mwolters.com>",
    "author": "Mark A. Wolters [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7638-8222>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scdensity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scdensity Shape-Constrained Kernel Density Estimation Implements methods for obtaining kernel density estimates\n    subject to a variety of shape constraints (unimodality, bimodality, \n    symmetry, tail monotonicity, bounds, and constraints on the number of \n    inflection points). Enforcing constraints can eliminate unwanted waves or \n    kinks in the estimate, which improves its subjective appearance and can \n    also improve statistical performance. The main function scdensity() is \n    very similar to the density() function in 'stats', allowing \n    shape-restricted estimates to be obtained with little effort. The \n    methods implemented in this package are described in Wolters and Braun \n    (2017) <doi:10.1080/03610918.2017.1288247>, Wolters (2012) \n    <doi:10.18637/jss.v047.i06>, and Hall and Huang (2002) \n    <https://www3.stat.sinica.edu.tw/statistica/j12n4/j12n41/j12n41.htm>.\n    See the scdensity() help for for full citations.  "
  },
  {
    "id": 20246,
    "package_name": "scplot",
    "title": "Plot Function for Single-Case Data Frames",
    "description": "Add-on for the 'scan' package that creates plots \n  from single-case data frames ('scdf'). It includes functions for styling \n  single-case plots, adding phase-based lines to indicate various statistical \n  parameters, and predefined themes for presentations and publications. More \n  information and in depth examples can be found in the online book \n  \"Analyzing Single-Case Data with R and 'scan\" \n  J\u00fcrgen Wilbert (2025) <https://jazznbass.github.io/scan-Book/>.",
    "version": "0.6.0",
    "maintainer": "Juergen Wilbert <juergen.wilbert@uni-muenster.de>",
    "author": "Juergen Wilbert [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8392-2873>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scplot Plot Function for Single-Case Data Frames Add-on for the 'scan' package that creates plots \n  from single-case data frames ('scdf'). It includes functions for styling \n  single-case plots, adding phase-based lines to indicate various statistical \n  parameters, and predefined themes for presentations and publications. More \n  information and in depth examples can be found in the online book \n  \"Analyzing Single-Case Data with R and 'scan\" \n  J\u00fcrgen Wilbert (2025) <https://jazznbass.github.io/scan-Book/>.  "
  },
  {
    "id": 20335,
    "package_name": "segmag",
    "title": "Determine Event Boundaries in Event Segmentation Experiments",
    "description": "Contains functions that help to determine event\n    boundaries in event segmentation experiments by bootstrapping a critical\n    segmentation magnitude under the null hypothesis that all key presses were\n    randomly distributed across the experiment. Segmentation magnitude is\n    defined as the sum of Gaussians centered at the times of the segmentation\n    key presses performed by the participants. Within a participant, the maximum\n    of the overlaid Gaussians is used to prevent an excessive influence of a\n    single participant on the overall outcome (e.g. if a participant is pressing\n    the key multiple times in succession). Further functions are included, such\n    as plotting the results.",
    "version": "1.2.4",
    "maintainer": "Frank Papenmeier <frank.papenmeier@uni-tuebingen.de>",
    "author": "Frank Papenmeier [aut, cre],\n  Konstantin Sering [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=segmag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "segmag Determine Event Boundaries in Event Segmentation Experiments Contains functions that help to determine event\n    boundaries in event segmentation experiments by bootstrapping a critical\n    segmentation magnitude under the null hypothesis that all key presses were\n    randomly distributed across the experiment. Segmentation magnitude is\n    defined as the sum of Gaussians centered at the times of the segmentation\n    key presses performed by the participants. Within a participant, the maximum\n    of the overlaid Gaussians is used to prevent an excessive influence of a\n    single participant on the overall outcome (e.g. if a participant is pressing\n    the key multiple times in succession). Further functions are included, such\n    as plotting the results.  "
  },
  {
    "id": 20373,
    "package_name": "semfindr",
    "title": "Influential Cases in Structural Equation Modeling",
    "description": "Sensitivity analysis in structural equation modeling using\n    influence measures and diagnostic plots. Support leave-one-out casewise\n    sensitivity analysis presented by Pek and MacCallum (2011)\n    <doi:10.1080/00273171.2011.561068> and approximate casewise influence\n    using scores and casewise likelihood.",
    "version": "0.1.9",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "author": "Shu Fai Cheung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9871-9448>),\n  Mark Hok Chio Lai [aut] (ORCID:\n    <https://orcid.org/0000-0002-9196-7406>)",
    "url": "https://sfcheung.github.io/semfindr/",
    "bug_reports": "https://github.com/sfcheung/semfindr/issues",
    "repository": "https://cran.r-project.org/package=semfindr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semfindr Influential Cases in Structural Equation Modeling Sensitivity analysis in structural equation modeling using\n    influence measures and diagnostic plots. Support leave-one-out casewise\n    sensitivity analysis presented by Pek and MacCallum (2011)\n    <doi:10.1080/00273171.2011.561068> and approximate casewise influence\n    using scores and casewise likelihood.  "
  },
  {
    "id": 20400,
    "package_name": "sensiPhy",
    "title": "Sensitivity Analysis for Comparative Methods",
    "description": "An implementation of sensitivity analysis for phylogenetic comparative\n methods. The package is an umbrella of statistical and graphical methods that \n estimate and report different types of uncertainty in PCM:\n (i) Species Sampling uncertainty (sample size; influential species and clades).\n (ii) Phylogenetic uncertainty (different topologies and/or branch lengths).\n (iii) Data uncertainty (intraspecific variation and measurement error).",
    "version": "0.8.5",
    "maintainer": "Gustavo Paterno <paternogbc@gmail.com>",
    "author": "Gustavo Paterno [cre, aut],\n  Gijsbert Werner [aut],\n  Caterina Penone [aut],\n  Pablo Martinez [ctb]",
    "url": "https://github.com/paternogbc/sensiPhy",
    "bug_reports": "https://github.com/paternogbc/sensiPhy/issues",
    "repository": "https://cran.r-project.org/package=sensiPhy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensiPhy Sensitivity Analysis for Comparative Methods An implementation of sensitivity analysis for phylogenetic comparative\n methods. The package is an umbrella of statistical and graphical methods that \n estimate and report different types of uncertainty in PCM:\n (i) Species Sampling uncertainty (sample size; influential species and clades).\n (ii) Phylogenetic uncertainty (different topologies and/or branch lengths).\n (iii) Data uncertainty (intraspecific variation and measurement error).  "
  },
  {
    "id": 20405,
    "package_name": "sensitivityIxJ",
    "title": "Exact Nonparametric Sensitivity Analysis for I by J Contingency\nTables",
    "description": "Implements exact, normally approximated, and sampling-based sensitivity \n    analysis for observational studies with contingency tables. Includes exact \n    (kernel-based), normal approximation, and sequential importance sampling (SIS) \n    methods using 'Rcpp' for computational efficiency. The methods build upon the \n    framework introduced in Rosenbaum (2002) <doi:10.1007/978-1-4757-3692-2> and \n    the generalized design sensitivity framework developed by Chiu (2025) \n    <doi:10.48550/arXiv.2507.17207>.",
    "version": "0.1.5",
    "maintainer": "Elaine Chiu <kchiu4@wisc.edu>",
    "author": "Elaine Chiu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sensitivityIxJ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensitivityIxJ Exact Nonparametric Sensitivity Analysis for I by J Contingency\nTables Implements exact, normally approximated, and sampling-based sensitivity \n    analysis for observational studies with contingency tables. Includes exact \n    (kernel-based), normal approximation, and sequential importance sampling (SIS) \n    methods using 'Rcpp' for computational efficiency. The methods build upon the \n    framework introduced in Rosenbaum (2002) <doi:10.1007/978-1-4757-3692-2> and \n    the generalized design sensitivity framework developed by Chiu (2025) \n    <doi:10.48550/arXiv.2507.17207>.  "
  },
  {
    "id": 20406,
    "package_name": "sensitivityfull",
    "title": "Sensitivity Analysis for Full Matching in Observational Studies",
    "description": "Sensitivity to unmeasured biases in an observational study that is a full match.  Function senfm() performs tests and function senfmCI() creates confidence intervals.  The method uses Huber's M-statistics, including least squares, and is described in Rosenbaum (2007, Biometrics) <DOI:10.1111/j.1541-0420.2006.00717.x>.",
    "version": "1.5.6",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sensitivityfull",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensitivityfull Sensitivity Analysis for Full Matching in Observational Studies Sensitivity to unmeasured biases in an observational study that is a full match.  Function senfm() performs tests and function senfmCI() creates confidence intervals.  The method uses Huber's M-statistics, including least squares, and is described in Rosenbaum (2007, Biometrics) <DOI:10.1111/j.1541-0420.2006.00717.x>.  "
  },
  {
    "id": 20407,
    "package_name": "sensitivitymult",
    "title": "Sensitivity Analysis for Observational Studies with Multiple\nOutcomes",
    "description": "Sensitivity analysis for multiple outcomes in observational studies.  For instance, all linear combinations of several outcomes may be explored using Scheffe projections in the comparison() function; see Rosenbaum (2016, Annals of Applied Statistics) <doi:10.1214/16-AOAS942>.  Alternatively, attention may focus on a few principal components in the principal() function.  The package includes parallel methods for individual outcomes, including tests in the senm() function and confidence intervals in the senmCI() function.",
    "version": "1.0.2",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sensitivitymult",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensitivitymult Sensitivity Analysis for Observational Studies with Multiple\nOutcomes Sensitivity analysis for multiple outcomes in observational studies.  For instance, all linear combinations of several outcomes may be explored using Scheffe projections in the comparison() function; see Rosenbaum (2016, Annals of Applied Statistics) <doi:10.1214/16-AOAS942>.  Alternatively, attention may focus on a few principal components in the principal() function.  The package includes parallel methods for individual outcomes, including tests in the senm() function and confidence intervals in the senmCI() function.  "
  },
  {
    "id": 20408,
    "package_name": "sensitivitymv",
    "title": "Sensitivity Analysis in Observational Studies",
    "description": "The package performs a sensitivity analysis in an observational study using an M-statistic, for instance, the mean.  The main function in the package is senmv(), but amplify() and truncatedP() are also useful.  The method is developed in Rosenbaum Biometrics, 2007, 63, 456-464, <doi:10.1111/j.1541-0420.2006.00717.x>.",
    "version": "1.4.4",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sensitivitymv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensitivitymv Sensitivity Analysis in Observational Studies The package performs a sensitivity analysis in an observational study using an M-statistic, for instance, the mean.  The main function in the package is senmv(), but amplify() and truncatedP() are also useful.  The method is developed in Rosenbaum Biometrics, 2007, 63, 456-464, <doi:10.1111/j.1541-0420.2006.00717.x>.  "
  },
  {
    "id": 20409,
    "package_name": "sensitivitymw",
    "title": "Sensitivity Analysis for Observational Studies Using Weighted\nM-Statistics",
    "description": "Sensitivity analysis for tests, confidence intervals and estimates in matched observational studies with one or more controls using weighted or unweighted Huber-Maritz M-tests (including the permutational t-test).   The method is from Rosenbaum (2014) Weighted M-statistics with superior design sensitivity in matched observational studies with multiple controls JASA, 109(507), 1145-1158 <doi:10.1080/01621459.2013.879261>.",
    "version": "2.1",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sensitivitymw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensitivitymw Sensitivity Analysis for Observational Studies Using Weighted\nM-Statistics Sensitivity analysis for tests, confidence intervals and estimates in matched observational studies with one or more controls using weighted or unweighted Huber-Maritz M-tests (including the permutational t-test).   The method is from Rosenbaum (2014) Weighted M-statistics with superior design sensitivity in matched observational studies with multiple controls JASA, 109(507), 1145-1158 <doi:10.1080/01621459.2013.879261>.  "
  },
  {
    "id": 20413,
    "package_name": "senstrat",
    "title": "Sensitivity Analysis for Stratified Observational Studies",
    "description": "Sensitivity analysis in unmatched observational studies, with or without strata.  The main functions are sen2sample() and senstrat().  See Rosenbaum, P. R. and Krieger, A. M. (1990), JASA, 85, 493-498, <doi:10.1080/01621459.1990.10476226> and Gastwirth, Krieger and Rosenbaum (2000), JRSS-B, 62, 545\u2013555 <doi:10.1111/1467-9868.00249> .",
    "version": "1.0.3",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=senstrat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "senstrat Sensitivity Analysis for Stratified Observational Studies Sensitivity analysis in unmatched observational studies, with or without strata.  The main functions are sen2sample() and senstrat().  See Rosenbaum, P. R. and Krieger, A. M. (1990), JASA, 85, 493-498, <doi:10.1080/01621459.1990.10476226> and Gastwirth, Krieger and Rosenbaum (2000), JRSS-B, 62, 545\u2013555 <doi:10.1111/1467-9868.00249> .  "
  },
  {
    "id": 20423,
    "package_name": "seq2R",
    "title": "Simple Method to Detect Compositional Changes in Genomic\nSequences",
    "description": "This software is useful for loading '.fasta' or '.gbk' files, and for retrieving sequences from 'GenBank' dataset <https://www.ncbi.nlm.nih.gov/genbank/>. This package allows  to detect differences or asymmetries based on nucleotide composition by using local linear kernel smoothers. Also, it is possible to draw inference about critical points (i. e. maximum or minimum points) related with the derivative curves. Additionally, bootstrap methods have been used  for estimating confidence intervals and speed computational techniques (binning techniques) have been implemented in 'seq2R'.",
    "version": "2.0.1",
    "maintainer": "Nora M. Villanueva <nmvillanueva@uvigo.gal>",
    "author": "Nora M. Villanueva [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8085-2745>),\n  Marta Sestelo [aut] (ORCID: <https://orcid.org/0000-0003-4284-6509>),\n  Alan Miller [ctb] (FORTRAN code lsq.f90: weighted least-squares module)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=seq2R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seq2R Simple Method to Detect Compositional Changes in Genomic\nSequences This software is useful for loading '.fasta' or '.gbk' files, and for retrieving sequences from 'GenBank' dataset <https://www.ncbi.nlm.nih.gov/genbank/>. This package allows  to detect differences or asymmetries based on nucleotide composition by using local linear kernel smoothers. Also, it is possible to draw inference about critical points (i. e. maximum or minimum points) related with the derivative curves. Additionally, bootstrap methods have been used  for estimating confidence intervals and speed computational techniques (binning techniques) have been implemented in 'seq2R'.  "
  },
  {
    "id": 20516,
    "package_name": "shapper",
    "title": "Wrapper of Python Library 'shap'",
    "description": "Provides SHAP explanations of machine learning models. In applied machine learning, there is a strong belief that we need to strike a balance between interpretability and accuracy. However, in field of the Interpretable Machine Learning, there are more and more new ideas for explaining black-box models. One of the best known method for local explanations is SHapley Additive exPlanations (SHAP) introduced by Lundberg, S., et al., (2016) <arXiv:1705.07874> The SHAP method is used to calculate influences of variables on the particular observation. This method is based on Shapley values, a technique used in game theory. The R package 'shapper' is a port of the Python library 'shap'. ",
    "version": "0.1.3",
    "maintainer": "Szymon Maksymiuk <sz.maksymiuk@gmail.com>",
    "author": "Szymon Maksymiuk [aut, cre],\n  Alicja Gosiewska [aut],\n  Przemyslaw Biecek [aut],\n  Mateusz Staniak [ctb],\n  Michal Burdukiewicz [ctb]",
    "url": "https://github.com/ModelOriented/shapper",
    "bug_reports": "https://github.com/ModelOriented/shapper/issues",
    "repository": "https://cran.r-project.org/package=shapper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shapper Wrapper of Python Library 'shap' Provides SHAP explanations of machine learning models. In applied machine learning, there is a strong belief that we need to strike a balance between interpretability and accuracy. However, in field of the Interpretable Machine Learning, there are more and more new ideas for explaining black-box models. One of the best known method for local explanations is SHapley Additive exPlanations (SHAP) introduced by Lundberg, S., et al., (2016) <arXiv:1705.07874> The SHAP method is used to calculate influences of variables on the particular observation. This method is based on Shapley values, a technique used in game theory. The R package 'shapper' is a port of the Python library 'shap'.   "
  },
  {
    "id": 20622,
    "package_name": "shinykanban",
    "title": "Create Kanban Board in Shiny Applications",
    "description": "Provides an interactive Kanban board widget for 'shiny' applications. \n    It allows users to manage tasks using a drag-and-drop interface and offers customizable styling options. \n    'shinykanban' is ideal for project management, task tracking, and agile workflows within 'shiny' apps.",
    "version": "0.0.1",
    "maintainer": "Ugur Dar <ugurdarr@gmail.com>",
    "author": "Ugur Dar [aut, cre],\n  Brian Pillmore [aut, cph]",
    "url": "https://github.com/ugurdar/shinykanban",
    "bug_reports": "https://github.com/ugurdar/shinykanban/issues",
    "repository": "https://cran.r-project.org/package=shinykanban",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinykanban Create Kanban Board in Shiny Applications Provides an interactive Kanban board widget for 'shiny' applications. \n    It allows users to manage tasks using a drag-and-drop interface and offers customizable styling options. \n    'shinykanban' is ideal for project management, task tracking, and agile workflows within 'shiny' apps.  "
  },
  {
    "id": 20677,
    "package_name": "sigInt",
    "title": "Estimate the Parameters of a Discrete Crisis-Bargaining Game",
    "description": "Provides pseudo-likelihood methods for empirically analyzing common signaling games in international relations as described in Crisman-Cox and Gibilisco (2019) <doi:10.1017/psrm.2019.58>.",
    "version": "0.2.0",
    "maintainer": "Casey Crisman-Cox <ccrismancox@gmail.com>",
    "author": "Casey Crisman-Cox [aut, cre],\n  Michael Gibilisco [aut]",
    "url": "https://github.com/ccrismancox/sigint",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sigInt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sigInt Estimate the Parameters of a Discrete Crisis-Bargaining Game Provides pseudo-likelihood methods for empirically analyzing common signaling games in international relations as described in Crisman-Cox and Gibilisco (2019) <doi:10.1017/psrm.2019.58>.  "
  },
  {
    "id": 20692,
    "package_name": "siland",
    "title": "Spatial Influence of Landscape",
    "description": "Method to estimate the spatial influence scales of landscape variables on a response variable. The method is based on Chandler and Hepinstall-Cymerman (2016) Estimating the spatial scales of landscape effects on abundance, Landscape ecology, 31: 1383-1394, <doi:10.1007/s10980-016-0380-z>.",
    "version": "3.0.2",
    "maintainer": "Martin Olivier <olivier.martin@inrae.fr>",
    "author": "Carpentier F. and Martin O.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=siland",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "siland Spatial Influence of Landscape Method to estimate the spatial influence scales of landscape variables on a response variable. The method is based on Chandler and Hepinstall-Cymerman (2016) Estimating the spatial scales of landscape effects on abundance, Landscape ecology, 31: 1383-1394, <doi:10.1007/s10980-016-0380-z>.  "
  },
  {
    "id": 20733,
    "package_name": "simglm",
    "title": "Simulate Models Based on the Generalized Linear Model",
    "description": "Simulates regression models,\n    including both simple regression and generalized linear mixed\n    models with up to three level of nesting. Power simulations that are\n    flexible allowing the specification of missing data, unbalanced designs,\n    and different random error distributions are built into the package.",
    "version": "0.8.9",
    "maintainer": "Brandon LeBeau <lebebr01+simglm@gmail.com>",
    "author": "Brandon LeBeau [aut, cre]",
    "url": "https://github.com/lebebr01/simglm",
    "bug_reports": "https://github.com/lebebr01/simglm/issues",
    "repository": "https://cran.r-project.org/package=simglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simglm Simulate Models Based on the Generalized Linear Model Simulates regression models,\n    including both simple regression and generalized linear mixed\n    models with up to three level of nesting. Power simulations that are\n    flexible allowing the specification of missing data, unbalanced designs,\n    and different random error distributions are built into the package.  "
  },
  {
    "id": 20794,
    "package_name": "singleRcapture",
    "title": "Single-Source Capture-Recapture Models",
    "description": "Implementation of single-source capture-recapture methods for population size estimation using zero-truncated, zero-one truncated and zero-truncated one-inflated Poisson, Geometric and Negative Binomial regression as well as Zelterman's and Chao's regression. Package includes point and interval estimators for the population size with variances estimated using analytical or bootstrap method. Details can be found in: van der Heijden et all. (2003) <doi:10.1191/1471082X03st057oa>, B\u00f6hning and van der Heijden (2019) <doi:10.1214/18-AOAS1232>, B\u00f6hning et al. (2020) Capture-Recapture Methods for the Social and Medical Sciences or B\u00f6hning and Friedl (2021) <doi:10.1007/s10260-021-00556-8>.",
    "version": "0.2.3",
    "maintainer": "Maciej Ber\u0119sewicz <maciej.beresewicz@ue.poznan.pl>",
    "author": "Piotr Chlebicki [aut, ctb] (ORCID:\n    <https://orcid.org/0009-0006-4867-7434>),\n  Maciej Ber\u0119sewicz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8281-4301>)",
    "url": "https://github.com/ncn-foreigners/singleRcapture",
    "bug_reports": "https://github.com/ncn-foreigners/singleRcapture/issues",
    "repository": "https://cran.r-project.org/package=singleRcapture",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "singleRcapture Single-Source Capture-Recapture Models Implementation of single-source capture-recapture methods for population size estimation using zero-truncated, zero-one truncated and zero-truncated one-inflated Poisson, Geometric and Negative Binomial regression as well as Zelterman's and Chao's regression. Package includes point and interval estimators for the population size with variances estimated using analytical or bootstrap method. Details can be found in: van der Heijden et all. (2003) <doi:10.1191/1471082X03st057oa>, B\u00f6hning and van der Heijden (2019) <doi:10.1214/18-AOAS1232>, B\u00f6hning et al. (2020) Capture-Recapture Methods for the Social and Medical Sciences or B\u00f6hning and Friedl (2021) <doi:10.1007/s10260-021-00556-8>.  "
  },
  {
    "id": 20923,
    "package_name": "smovie",
    "title": "Some Movies to Illustrate Concepts in Statistics",
    "description": "Provides movies to help students to understand statistical \n  concepts.  The 'rpanel' package  <https://cran.r-project.org/package=rpanel> \n  is used to create interactive plots that move to illustrate key statistical \n  ideas and methods.  There are movies to: visualise probability distributions\n  (including user-supplied ones); illustrate sampling distributions of the\n  sample mean (central limit theorem), the median, the sample maximum \n  (extremal types theorem) and (the Fisher transformation of the) \n  product moment correlation coefficient; examine the influence of an \n  individual observation in simple linear regression; illustrate key concepts \n  in statistical hypothesis testing. Also provided are dpqr functions for the \n  distribution of the Fisher transformation of the correlation coefficient \n  under sampling from a bivariate normal distribution.",
    "version": "1.1.6",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "author": "Paul J. Northrop [aut, cre, cph]",
    "url": "https://paulnorthrop.github.io/smovie/,\nhttps://github.com/paulnorthrop/smovie/",
    "bug_reports": "https://github.com/paulnorthrop/smovie/issues",
    "repository": "https://cran.r-project.org/package=smovie",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smovie Some Movies to Illustrate Concepts in Statistics Provides movies to help students to understand statistical \n  concepts.  The 'rpanel' package  <https://cran.r-project.org/package=rpanel> \n  is used to create interactive plots that move to illustrate key statistical \n  ideas and methods.  There are movies to: visualise probability distributions\n  (including user-supplied ones); illustrate sampling distributions of the\n  sample mean (central limit theorem), the median, the sample maximum \n  (extremal types theorem) and (the Fisher transformation of the) \n  product moment correlation coefficient; examine the influence of an \n  individual observation in simple linear regression; illustrate key concepts \n  in statistical hypothesis testing. Also provided are dpqr functions for the \n  distribution of the Fisher transformation of the correlation coefficient \n  under sampling from a bivariate normal distribution.  "
  },
  {
    "id": 20942,
    "package_name": "snc",
    "title": "Strongest Neighbor Coherence",
    "description": "Computes Strongest Neighbor Coherence (SNC), a structural diagnostic that replaces Cronbach's alpha using top-k correlation structure.\n  For methodology, see Wells (2025) <https://github.com/TheotherDrWells/snc>.",
    "version": "0.1.0",
    "maintainer": "Kevin E. Wells <kevin.e.wells@usm.edu>",
    "author": "Kevin E. Wells [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=snc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snc Strongest Neighbor Coherence Computes Strongest Neighbor Coherence (SNC), a structural diagnostic that replaces Cronbach's alpha using top-k correlation structure.\n  For methodology, see Wells (2025) <https://github.com/TheotherDrWells/snc>.  "
  },
  {
    "id": 20964,
    "package_name": "socceR",
    "title": "Evaluating Sport Tournament Predictions",
    "description": "Functions for evaluating tournament predictions, simulating results from individual soccer matches and tournaments. See <http://sandsynligvis.dk/2018/08/03/world-cup-prediction-winners/> for more information.",
    "version": "0.1.1",
    "maintainer": "Claus Thorn Ekstr\u00f8m <ekstrom@sund.ku.dk>",
    "author": "Claus Thorn Ekstr\u00f8m [aut, cre]",
    "url": "https://github.com/ekstroem/socceR",
    "bug_reports": "https://github.com/ekstroem/socceR/issues",
    "repository": "https://cran.r-project.org/package=socceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "socceR Evaluating Sport Tournament Predictions Functions for evaluating tournament predictions, simulating results from individual soccer matches and tournaments. See <http://sandsynligvis.dk/2018/08/03/world-cup-prediction-winners/> for more information.  "
  },
  {
    "id": 20966,
    "package_name": "socialSim",
    "title": "Simulate and Analyse Social Interaction Data",
    "description": "\n    Provides tools to simulate and analyse datasets of social interactions \n    between individuals using hierarchical Bayesian models implemented in Stan. \n    The package interacts with Stan via 'cmdstanr' (available from \n    <https://mc-stan.org/r-packages/>) or 'rstan', depending on user setup. \n    Users can generate realistic interaction data where individual phenotypes influence \n    and respond to those of their partners, with control over sampling design parameters \n    such as the number of individuals, partners, and repeated dyads. The simulation \n    framework allows flexible control over variation and correlation in mean trait \n    values, social responsiveness, and social impact, making it suitable for research on \n    interacting phenotypes and on direct and indirect genetic effects ('DGEs' and 'IGEs'). \n    The package also includes functions to fit and compare alternative models of social \n    effects, including impact\u2013responsiveness, variance\u2013partitioning, and trait-based \n    models, and to summarise model performance in terms of bias and dispersion. For more \n    details on the study of social interactions and impact-responsiveness,\n    see Moore et al. (1997) <doi:10.1111/j.1558-5646.1997.tb01458.x> and \n    de Groot et al. (2022) <doi:10.1016/j.neubiorev.2022.104996>.",
    "version": "0.1.6",
    "maintainer": "Rori Efrain Wijnhorst <roriwijnhorst@gmail.com>",
    "author": "Rori Efrain Wijnhorst [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8902-592X>)",
    "url": "https://github.com/RoriWijnhorst/socialSim",
    "bug_reports": "https://github.com/RoriWijnhorst/socialSim/issues",
    "repository": "https://cran.r-project.org/package=socialSim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "socialSim Simulate and Analyse Social Interaction Data \n    Provides tools to simulate and analyse datasets of social interactions \n    between individuals using hierarchical Bayesian models implemented in Stan. \n    The package interacts with Stan via 'cmdstanr' (available from \n    <https://mc-stan.org/r-packages/>) or 'rstan', depending on user setup. \n    Users can generate realistic interaction data where individual phenotypes influence \n    and respond to those of their partners, with control over sampling design parameters \n    such as the number of individuals, partners, and repeated dyads. The simulation \n    framework allows flexible control over variation and correlation in mean trait \n    values, social responsiveness, and social impact, making it suitable for research on \n    interacting phenotypes and on direct and indirect genetic effects ('DGEs' and 'IGEs'). \n    The package also includes functions to fit and compare alternative models of social \n    effects, including impact\u2013responsiveness, variance\u2013partitioning, and trait-based \n    models, and to summarise model performance in terms of bias and dispersion. For more \n    details on the study of social interactions and impact-responsiveness,\n    see Moore et al. (1997) <doi:10.1111/j.1558-5646.1997.tb01458.x> and \n    de Groot et al. (2022) <doi:10.1016/j.neubiorev.2022.104996>.  "
  },
  {
    "id": 20968,
    "package_name": "socialranking",
    "title": "Social Ranking Solutions for Power Relations on Coalitions",
    "description": "The notion of power index has been widely used in literature to evaluate the influence of individual players (e.g., voters, political parties, nations, stockholders, etc.) involved in a collective decision situation like an electoral system, a parliament, a council, a management board, etc., where players may form coalitions. Traditionally this ranking is determined through numerical evaluation. More often than not however only ordinal data between coalitions is known. The package 'socialranking' offers a set of solutions to rank players based on a transitive ranking between coalitions, including through CP-Majority, ordinal Banzhaf or lexicographic excellence solution summarized by Tahar Allouche, Bruno Escoffier, Stefano Moretti and Meltem \u00d6zt\u00fcrk (2020, <doi:10.24963/ijcai.2020/3>).",
    "version": "1.2.0",
    "maintainer": "Felix Fritz <felix.fritz@dauphine.eu>",
    "author": "Felix Fritz [aut, cre],\n  Jochen Staudacher [aut, cph, ths],\n  Moretti Stefano [aut, cph, ths]",
    "url": "https://github.com/jassler/socialranking",
    "bug_reports": "https://github.com/jassler/socialranking/issues",
    "repository": "https://cran.r-project.org/package=socialranking",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "socialranking Social Ranking Solutions for Power Relations on Coalitions The notion of power index has been widely used in literature to evaluate the influence of individual players (e.g., voters, political parties, nations, stockholders, etc.) involved in a collective decision situation like an electoral system, a parliament, a council, a management board, etc., where players may form coalitions. Traditionally this ranking is determined through numerical evaluation. More often than not however only ordinal data between coalitions is known. The package 'socialranking' offers a set of solutions to rank players based on a transitive ranking between coalitions, including through CP-Majority, ordinal Banzhaf or lexicographic excellence solution summarized by Tahar Allouche, Bruno Escoffier, Stefano Moretti and Meltem \u00d6zt\u00fcrk (2020, <doi:10.24963/ijcai.2020/3>).  "
  },
  {
    "id": 20984,
    "package_name": "sokoban",
    "title": "Sokoban Game",
    "description": "Interactively play a game of sokoban ,which has nine game levels.Sokoban is a type of transport puzzle, in which the player pushes boxes or crates around in a warehouse, trying to get them to storage locations.",
    "version": "0.1.0",
    "maintainer": "Zhaoliang He <1024299329@qq.com>",
    "author": "Zhaoliang He <1024299329@qq.com>, Linsui Deng <1175520015@qq.com>, Kaiwen Tan <913596401@qq.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sokoban",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sokoban Sokoban Game Interactively play a game of sokoban ,which has nine game levels.Sokoban is a type of transport puzzle, in which the player pushes boxes or crates around in a warehouse, trying to get them to storage locations.  "
  },
  {
    "id": 21060,
    "package_name": "spantest",
    "title": "Mean-Variance Spanning Tests",
    "description": "Provides a comprehensive suite of portfolio spanning tests for asset \n             pricing, such as Huberman and Kandel (1987) <doi:10.1111/j.1540-6261.1987.tb03917.x>, \n             Gibbons et al. (1989) <doi:10.2307/1913625>, Kempf and Memmel (2006) <doi:10.1007/BF03396737>, \n             Pesaran and Yamagata (2024) <doi:10.1093/jjfinec/nbad002>, and Gungor and \n             Luger (2016) <doi:10.1080/07350015.2015.1019510>.",
    "version": "1.1-3",
    "maintainer": "David Ardia <david.ardia.ch@gmail.com>",
    "author": "David Ardia [aut, cre] (ORCID: <https://orcid.org/0000-0003-2823-782X>),\n  Benjamin Seguin [aut],\n  Rosnel Sessinou [ctb],\n  Richard Luger [ctb]",
    "url": "https://github.com/ArdiaD/spantest",
    "bug_reports": "https://github.com/ArdiaD/spantest/issues",
    "repository": "https://cran.r-project.org/package=spantest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spantest Mean-Variance Spanning Tests Provides a comprehensive suite of portfolio spanning tests for asset \n             pricing, such as Huberman and Kandel (1987) <doi:10.1111/j.1540-6261.1987.tb03917.x>, \n             Gibbons et al. (1989) <doi:10.2307/1913625>, Kempf and Memmel (2006) <doi:10.1007/BF03396737>, \n             Pesaran and Yamagata (2024) <doi:10.1093/jjfinec/nbad002>, and Gungor and \n             Luger (2016) <doi:10.1080/07350015.2015.1019510>.  "
  },
  {
    "id": 21101,
    "package_name": "sparsesurv",
    "title": "Forecasting and Early Outbreak Detection for Sparse Count Data",
    "description": "Functions for fitting, forecasting, and early detection of outbreaks in\n    sparse surveillance count time series. Supports negative binomial (NB),\n    self-exciting NB, generalise autoregressive moving average (GARMA) NB , zero-inflated NB (ZINB), self-exciting ZINB, generalise autoregressive moving average ZINB, and hurdle formulations. Climatic and environmental covariates\n    can be included in the regression component and/or the zero-modified components.\n    Includes outbreak-detection algorithms for NB, ZINB, and hurdle models, with\n    utilities for prediction and diagnostics.",
    "version": "0.1.1",
    "maintainer": "Alexandros Angelakis <alexandros.angelakis@swisstph.ch>",
    "author": "Alexandros Angelakis [aut, cre],\n  Bryan Nyawanda [aut],\n  Penelope Vounatsou [aut]",
    "url": "https://github.com/alexangelakis-ang/sparsesurv",
    "bug_reports": "https://github.com/alexangelakis-ang/sparsesurv/issues",
    "repository": "https://cran.r-project.org/package=sparsesurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparsesurv Forecasting and Early Outbreak Detection for Sparse Count Data Functions for fitting, forecasting, and early detection of outbreaks in\n    sparse surveillance count time series. Supports negative binomial (NB),\n    self-exciting NB, generalise autoregressive moving average (GARMA) NB , zero-inflated NB (ZINB), self-exciting ZINB, generalise autoregressive moving average ZINB, and hurdle formulations. Climatic and environmental covariates\n    can be included in the regression component and/or the zero-modified components.\n    Includes outbreak-detection algorithms for NB, ZINB, and hurdle models, with\n    utilities for prediction and diagnostics.  "
  },
  {
    "id": 21126,
    "package_name": "spatstat",
    "title": "Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests",
    "description": "Comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region. Also supports three-dimensional point patterns, space-time point patterns in any number of dimensions, point patterns on a linear network, and patterns of other geometrical objects. Supports spatial covariate data such as pixel images. \n\tContains over 3000 functions for plotting spatial data, exploratory data analysis, model-fitting, simulation, spatial sampling, model diagnostics, and formal inference. \n\tData types include point patterns, line segment patterns, spatial windows, pixel images, tessellations, and linear networks. \n\tExploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. \n\tA model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.",
    "version": "3.5-0",
    "maintainer": "Adrian Baddeley <Adrian.Baddeley@curtin.edu.au>",
    "author": "Adrian Baddeley [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9499-8382>),\n  Rolf Turner [aut] (ORCID: <https://orcid.org/0000-0001-5521-5218>),\n  Ege Rubak [aut] (ORCID: <https://orcid.org/0000-0002-6675-533X>)",
    "url": "http://spatstat.org/",
    "bug_reports": "https://github.com/spatstat/spatstat/issues",
    "repository": "https://cran.r-project.org/package=spatstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatstat Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests Comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region. Also supports three-dimensional point patterns, space-time point patterns in any number of dimensions, point patterns on a linear network, and patterns of other geometrical objects. Supports spatial covariate data such as pixel images. \n\tContains over 3000 functions for plotting spatial data, exploratory data analysis, model-fitting, simulation, spatial sampling, model diagnostics, and formal inference. \n\tData types include point patterns, line segment patterns, spatial windows, pixel images, tessellations, and linear networks. \n\tExploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. \n\tA model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.  "
  },
  {
    "id": 21132,
    "package_name": "spatstat.linnet",
    "title": "Linear Networks Functionality of the 'spatstat' Family",
    "description": "Defines types of spatial data on a linear network\n\t     and provides functionality for geometrical operations,\n\t     data analysis and modelling of data on a linear network,\n\t     in the 'spatstat' family of packages.\n\t     Contains definitions and support for linear networks, including creation of networks, geometrical measurements, topological connectivity, geometrical operations such as inserting and deleting vertices, intersecting a network with another object, and interactive editing of networks.\n\t     Data types defined on a network include point patterns, pixel images, functions, and tessellations.\n\t     Exploratory methods include kernel estimation of intensity on a network, K-functions and pair correlation functions on a network, simulation envelopes, nearest neighbour distance and empty space distance, relative risk estimation with cross-validated bandwidth selection. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the function lppm() similar to glm(). Only Poisson models are implemented so far. Models may involve dependence on covariates and dependence on marks. Models are fitted by maximum likelihood.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.\n\tRandom point patterns on a network can be generated using a variety of models.",
    "version": "3.4-0",
    "maintainer": "Adrian Baddeley <Adrian.Baddeley@curtin.edu.au>",
    "author": "Adrian Baddeley [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9499-8382>),\n  Rolf Turner [aut, cph] (ORCID: <https://orcid.org/0000-0001-5521-5218>),\n  Ege Rubak [aut, cph] (ORCID: <https://orcid.org/0000-0002-6675-533X>),\n  Greg McSwiggan [aut, cph],\n  Tilman Davies [ctb, cph],\n  Mehdi Moradi [ctb, cph],\n  Suman Rakshit [ctb, cph],\n  Ottmar Cronie [ctb]",
    "url": "http://spatstat.org/",
    "bug_reports": "https://github.com/spatstat/spatstat.linnet/issues",
    "repository": "https://cran.r-project.org/package=spatstat.linnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatstat.linnet Linear Networks Functionality of the 'spatstat' Family Defines types of spatial data on a linear network\n\t     and provides functionality for geometrical operations,\n\t     data analysis and modelling of data on a linear network,\n\t     in the 'spatstat' family of packages.\n\t     Contains definitions and support for linear networks, including creation of networks, geometrical measurements, topological connectivity, geometrical operations such as inserting and deleting vertices, intersecting a network with another object, and interactive editing of networks.\n\t     Data types defined on a network include point patterns, pixel images, functions, and tessellations.\n\t     Exploratory methods include kernel estimation of intensity on a network, K-functions and pair correlation functions on a network, simulation envelopes, nearest neighbour distance and empty space distance, relative risk estimation with cross-validated bandwidth selection. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the function lppm() similar to glm(). Only Poisson models are implemented so far. Models may involve dependence on covariates and dependence on marks. Models are fitted by maximum likelihood.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.\n\tRandom point patterns on a network can be generated using a variety of models.  "
  },
  {
    "id": 21134,
    "package_name": "spatstat.model",
    "title": "Parametric Statistical Modelling and Inference for the\n'spatstat' Family",
    "description": "Functionality for parametric statistical modelling and inference for spatial data,\n\t     mainly spatial point patterns, in the 'spatstat' family of packages.\n\t     (Excludes analysis of spatial data on a linear network,\n\t     which is covered by the separate package 'spatstat.linnet'.)\n\t     Supports parametric modelling, formal statistical inference, and model validation.\n\t     Parametric models include Poisson point processes, Cox point processes, Neyman-Scott cluster processes, Gibbs point processes and determinantal point processes. Models can be fitted to data using maximum likelihood, maximum pseudolikelihood, maximum composite likelihood and the method of minimum contrast. Fitted models can be simulated and predicted. Formal inference includes hypothesis tests (quadrat counting tests, Cressie-Read tests, Clark-Evans test, Berman test, Diggle-Cressie-Loosmore-Ford test, scan test, studentised permutation test, segregation test, ANOVA tests of fitted models, adjusted composite likelihood ratio test, envelope tests, Dao-Genton test, balanced independent two-stage test), confidence intervals for parameters, and prediction intervals for point counts. Model validation techniques include leverage, influence, partial residuals, added variable plots, diagnostic plots, pseudoscore residual plots, model compensators and Q-Q plots.",
    "version": "3.5-0",
    "maintainer": "Adrian Baddeley <Adrian.Baddeley@curtin.edu.au>",
    "author": "Adrian Baddeley [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9499-8382>),\n  Rolf Turner [aut, cph] (ORCID: <https://orcid.org/0000-0001-5521-5218>),\n  Ege Rubak [aut, cph] (ORCID: <https://orcid.org/0000-0002-6675-533X>),\n  Kasper Klitgaard Berthelsen [ctb],\n  Achmad Choiruddin [ctb, cph],\n  Jean-Francois Coeurjolly [ctb],\n  Ottmar Cronie [ctb],\n  Tilman Davies [ctb],\n  Julian Gilbey [ctb],\n  Yongtao Guan [ctb],\n  Ute Hahn [ctb],\n  Martin Hazelton [ctb],\n  Kassel Hingee [ctb],\n  Abdollah Jalilian [ctb],\n  Frederic Lavancier [ctb],\n  Marie-Colette van Lieshout [ctb],\n  Bethany Macdonald [ctb],\n  Greg McSwiggan [ctb],\n  Tuomas Rajala [ctb],\n  Suman Rakshit [ctb, cph],\n  Dominic Schuhmacher [ctb],\n  Rasmus Plenge Waagepetersen [ctb],\n  Hangsheng Wang [ctb]",
    "url": "http://spatstat.org/",
    "bug_reports": "https://github.com/spatstat/spatstat.model/issues",
    "repository": "https://cran.r-project.org/package=spatstat.model",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatstat.model Parametric Statistical Modelling and Inference for the\n'spatstat' Family Functionality for parametric statistical modelling and inference for spatial data,\n\t     mainly spatial point patterns, in the 'spatstat' family of packages.\n\t     (Excludes analysis of spatial data on a linear network,\n\t     which is covered by the separate package 'spatstat.linnet'.)\n\t     Supports parametric modelling, formal statistical inference, and model validation.\n\t     Parametric models include Poisson point processes, Cox point processes, Neyman-Scott cluster processes, Gibbs point processes and determinantal point processes. Models can be fitted to data using maximum likelihood, maximum pseudolikelihood, maximum composite likelihood and the method of minimum contrast. Fitted models can be simulated and predicted. Formal inference includes hypothesis tests (quadrat counting tests, Cressie-Read tests, Clark-Evans test, Berman test, Diggle-Cressie-Loosmore-Ford test, scan test, studentised permutation test, segregation test, ANOVA tests of fitted models, adjusted composite likelihood ratio test, envelope tests, Dao-Genton test, balanced independent two-stage test), confidence intervals for parameters, and prediction intervals for point counts. Model validation techniques include leverage, influence, partial residuals, added variable plots, diagnostic plots, pseudoscore residual plots, model compensators and Q-Q plots.  "
  },
  {
    "id": 21212,
    "package_name": "spinBayes",
    "title": "Semi-Parametric Gene-Environment Interaction via Bayesian\nVariable Selection",
    "description": "Many complex diseases are known to be affected by the interactions between \n   genetic variants and environmental exposures beyond the main genetic and environmental \n   effects. Existing Bayesian methods for gene-environment (G\u00d7E) interaction studies are \n   challenged by the high-dimensional nature of the study and the complexity of environmental \n   influences. We have developed a novel and powerful semi-parametric Bayesian variable \n   selection method that can accommodate linear and nonlinear G\u00d7E interactions simultaneously \n   (Ren et al. (2020) <doi:10.1002/sim.8434>). Furthermore, the proposed method can conduct \n   structural identification by distinguishing nonlinear interactions from main effects only \n   case within Bayesian framework. Spike-and-slab priors are incorporated on both individual \n   and group level to shrink coefficients corresponding to irrelevant main and interaction \n   effects to zero exactly. The Markov chain Monte Carlo algorithms of the proposed and \n   alternative methods are  efficiently implemented in C++. ",
    "version": "0.2.2",
    "maintainer": "Jie Ren <renjie0910@gmail.com>",
    "author": "Jie Ren [aut, cre],\n  Fei Zhou [aut],\n  Xiaoxi Li [aut],\n  Cen Wu [aut],\n  Yu Jiang [aut]",
    "url": "https://github.com/jrhub/spinBayes",
    "bug_reports": "https://github.com/jrhub/spinBayes/issues",
    "repository": "https://cran.r-project.org/package=spinBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spinBayes Semi-Parametric Gene-Environment Interaction via Bayesian\nVariable Selection Many complex diseases are known to be affected by the interactions between \n   genetic variants and environmental exposures beyond the main genetic and environmental \n   effects. Existing Bayesian methods for gene-environment (G\u00d7E) interaction studies are \n   challenged by the high-dimensional nature of the study and the complexity of environmental \n   influences. We have developed a novel and powerful semi-parametric Bayesian variable \n   selection method that can accommodate linear and nonlinear G\u00d7E interactions simultaneously \n   (Ren et al. (2020) <doi:10.1002/sim.8434>). Furthermore, the proposed method can conduct \n   structural identification by distinguishing nonlinear interactions from main effects only \n   case within Bayesian framework. Spike-and-slab priors are incorporated on both individual \n   and group level to shrink coefficients corresponding to irrelevant main and interaction \n   effects to zero exactly. The Markov chain Monte Carlo algorithms of the proposed and \n   alternative methods are  efficiently implemented in C++.   "
  },
  {
    "id": 21232,
    "package_name": "splitstackshape",
    "title": "Stack and Reshape Datasets After Splitting Concatenated Values",
    "description": "Online data collection tools like Google Forms often export\n    multiple-response questions with data concatenated in cells. The\n    concat.split (cSplit) family of functions splits such data into separate \n    cells. The package also includes functions to stack groups of columns and \n    to reshape wide data, even when the data are \"unbalanced\"---something \n    which reshape (from base R) does not handle, and which melt and dcast from \n    reshape2 do not easily handle.",
    "version": "1.4.8",
    "maintainer": "Ananda Mahto <mrdwab@gmail.com>",
    "author": "Ananda Mahto",
    "url": "http://github.com/mrdwab/splitstackshape",
    "bug_reports": "http://github.com/mrdwab/splitstackshape/issues",
    "repository": "https://cran.r-project.org/package=splitstackshape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "splitstackshape Stack and Reshape Datasets After Splitting Concatenated Values Online data collection tools like Google Forms often export\n    multiple-response questions with data concatenated in cells. The\n    concat.split (cSplit) family of functions splits such data into separate \n    cells. The package also includes functions to stack groups of columns and \n    to reshape wide data, even when the data are \"unbalanced\"---something \n    which reshape (from base R) does not handle, and which melt and dcast from \n    reshape2 do not easily handle.  "
  },
  {
    "id": 21252,
    "package_name": "sport",
    "title": "Sequential Pairwise Online Rating Techniques",
    "description": "Calculates ratings for two-player or \n  multi-player challenges. Methods included in package such as are able to \n  estimate ratings (players strengths) and their evolution in time, also able to \n  predict output of challenge. Algorithms are based on Bayesian Approximation \n  Method, and they don't involve any matrix inversions nor likelihood estimation. \n  Parameters are updated sequentially, and computation doesn't require any \n  additional RAM to make estimation feasible. Additionally, base of the package \n  is written in C++ what makes sport computation even faster. Methods used in the \n  package refer to Mark E. Glickman (1999) \n  <https://www.glicko.net/research/glicko.pdf>; \n  Mark E. Glickman (2001) <doi:10.1080/02664760120059219>; \n  Ruby C. Weng, Chih-Jen Lin (2011) <https://www.jmlr.org/papers/volume12/weng11a/weng11a.pdf>; \n  W. Penny, Stephen J. Roberts (1999) <doi:10.1109/IJCNN.1999.832603>.",
    "version": "0.2.2",
    "maintainer": "Dawid Ka\u0142\u0119dkowski <dawid.kaledkowski@gmail.com>",
    "author": "Dawid Ka\u0142\u0119dkowski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9533-457X>)",
    "url": "https://github.com/gogonzo/sport",
    "bug_reports": "https://github.com/gogonzo/sport/issues",
    "repository": "https://cran.r-project.org/package=sport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sport Sequential Pairwise Online Rating Techniques Calculates ratings for two-player or \n  multi-player challenges. Methods included in package such as are able to \n  estimate ratings (players strengths) and their evolution in time, also able to \n  predict output of challenge. Algorithms are based on Bayesian Approximation \n  Method, and they don't involve any matrix inversions nor likelihood estimation. \n  Parameters are updated sequentially, and computation doesn't require any \n  additional RAM to make estimation feasible. Additionally, base of the package \n  is written in C++ what makes sport computation even faster. Methods used in the \n  package refer to Mark E. Glickman (1999) \n  <https://www.glicko.net/research/glicko.pdf>; \n  Mark E. Glickman (2001) <doi:10.1080/02664760120059219>; \n  Ruby C. Weng, Chih-Jen Lin (2011) <https://www.jmlr.org/papers/volume12/weng11a/weng11a.pdf>; \n  W. Penny, Stephen J. Roberts (1999) <doi:10.1109/IJCNN.1999.832603>.  "
  },
  {
    "id": 21253,
    "package_name": "sportyR",
    "title": "Plot Scaled 'ggplot' Representations of Sports Playing Surfaces",
    "description": "Create scaled 'ggplot' representations of playing surfaces.\n    Playing surfaces are drawn pursuant to rule-book specifications.\n    This package should be used as a baseline plot for displaying any type of\n    tracking data.",
    "version": "2.2.3",
    "maintainer": "Ross Drucker <ross.a.drucker@gmail.com>",
    "author": "Ross Drucker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0688-0235>)",
    "url": "https://sportyr.sportsdataverse.org/,\nhttps://github.com/sportsdataverse/sportyR",
    "bug_reports": "https://github.com/sportsdataverse/sportyR/issues",
    "repository": "https://cran.r-project.org/package=sportyR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sportyR Plot Scaled 'ggplot' Representations of Sports Playing Surfaces Create scaled 'ggplot' representations of playing surfaces.\n    Playing surfaces are drawn pursuant to rule-book specifications.\n    This package should be used as a baseline plot for displaying any type of\n    tracking data.  "
  },
  {
    "id": 21297,
    "package_name": "sreg",
    "title": "Stratified Randomized Experiments",
    "description": "Estimate average treatment effects (ATEs) in stratified randomized experiments. `sreg` supports a wide range of stratification designs, including matched pairs, n-tuple designs, and larger strata with many units \u2014 possibly of unequal size across strata. 'sreg' is designed to accommodate scenarios with multiple treatments and cluster-level treatment assignments, and accommodates optimal linear covariate adjustment based on baseline observable characteristics. 'sreg' computes estimators and standard errors based on Bugni, Canay, Shaikh (2018) <doi:10.1080/01621459.2017.1375934>; Bugni, Canay, Shaikh, Tabord-Meehan (2024+) <doi:10.48550/arXiv.2204.08356>; Jiang, Linton, Tang, Zhang (2023+) <doi:10.48550/arXiv.2201.13004>; Bai, Jiang, Romano, Shaikh, and Zhang (2024) <doi:10.1016/j.jeconom.2024.105740>; Bai (2022) <doi:10.1257/aer.20201856>; Bai, Romano, and Shaikh (2022) <doi:10.1080/01621459.2021.1883437>; Liu (2024+) <doi:10.48550/arXiv.2301.09016>; and Cytrynbaum (2024) <doi:10.3982/QE2475>.",
    "version": "2.0.2",
    "maintainer": "Juri Trifonov <jutrifonov@u.northwestern.edu>",
    "author": "Juri Trifonov [aut, cre, cph],\n  Yuehao Bai [aut],\n  Azeem Shaikh [aut],\n  Max Tabord-Meehan [aut]",
    "url": "https://github.com/jutrifonov/sreg",
    "bug_reports": "https://github.com/jutrifonov/sreg/issues",
    "repository": "https://cran.r-project.org/package=sreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sreg Stratified Randomized Experiments Estimate average treatment effects (ATEs) in stratified randomized experiments. `sreg` supports a wide range of stratification designs, including matched pairs, n-tuple designs, and larger strata with many units \u2014 possibly of unequal size across strata. 'sreg' is designed to accommodate scenarios with multiple treatments and cluster-level treatment assignments, and accommodates optimal linear covariate adjustment based on baseline observable characteristics. 'sreg' computes estimators and standard errors based on Bugni, Canay, Shaikh (2018) <doi:10.1080/01621459.2017.1375934>; Bugni, Canay, Shaikh, Tabord-Meehan (2024+) <doi:10.48550/arXiv.2204.08356>; Jiang, Linton, Tang, Zhang (2023+) <doi:10.48550/arXiv.2201.13004>; Bai, Jiang, Romano, Shaikh, and Zhang (2024) <doi:10.1016/j.jeconom.2024.105740>; Bai (2022) <doi:10.1257/aer.20201856>; Bai, Romano, and Shaikh (2022) <doi:10.1080/01621459.2021.1883437>; Liu (2024+) <doi:10.48550/arXiv.2301.09016>; and Cytrynbaum (2024) <doi:10.3982/QE2475>.  "
  },
  {
    "id": 21329,
    "package_name": "ssmodels",
    "title": "Sample Selection Models",
    "description": "In order to facilitate the adjustment of the sample selection models\n  existing in the literature, we created the 'ssmodels' package. Our package\n  allows the adjustment of the classic Heckman model (Heckman (1976),\n  Heckman (1979) <doi:10.2307/1912352>), and the estimation of the parameters of\n  this model via the maximum likelihood method and two-step method, in addition\n  to the adjustment of the Heckman-t models introduced in the literature by\n  Marchenko and Genton (2012) <doi:10.1080/01621459.2012.656011> and the\n  Heckman-Skew model introduced in the literature by Ogundimu and Hutton (2016)\n  <doi:10.1111/sjos.12171>. We also implemented functions to adjust the\n  generalized version of the Heckman model, introduced by Bastos, Barreto-Souza,\n  and Genton (2021) <doi:10.5705/ss.202021.0068>, that allows the inclusion of\n  covariables to the dispersion and correlation parameters, and a function to\n  adjust the Heckman-BS model introduced by Bastos and Barreto-Souza (2020)\n  <doi:10.1080/02664763.2020.1780570> that uses the Birnbaum-Saunders\n  distribution as a joint distribution of the selection and primary regression\n  variables. This package extends and complements existing R packages such as \n  'sampleSelection' (Toomet and Henningsen, 2008) and 'ssmrob' (Zhelonkin et al., 2016), providing additional robust and flexible sample selection models.",
    "version": "2.0.1",
    "maintainer": "Fernando de Souza Bastos <fernando.bastos@ufv.br>",
    "author": "Fernando de Souza Bastos [aut, cre],\n  Wagner Barreto de Souza [aut]",
    "url": "https://fsbmat-ufv.github.io/ssmodels/",
    "bug_reports": "https://github.com/fsbmat-ufv/ssmodels/issues",
    "repository": "https://cran.r-project.org/package=ssmodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ssmodels Sample Selection Models In order to facilitate the adjustment of the sample selection models\n  existing in the literature, we created the 'ssmodels' package. Our package\n  allows the adjustment of the classic Heckman model (Heckman (1976),\n  Heckman (1979) <doi:10.2307/1912352>), and the estimation of the parameters of\n  this model via the maximum likelihood method and two-step method, in addition\n  to the adjustment of the Heckman-t models introduced in the literature by\n  Marchenko and Genton (2012) <doi:10.1080/01621459.2012.656011> and the\n  Heckman-Skew model introduced in the literature by Ogundimu and Hutton (2016)\n  <doi:10.1111/sjos.12171>. We also implemented functions to adjust the\n  generalized version of the Heckman model, introduced by Bastos, Barreto-Souza,\n  and Genton (2021) <doi:10.5705/ss.202021.0068>, that allows the inclusion of\n  covariables to the dispersion and correlation parameters, and a function to\n  adjust the Heckman-BS model introduced by Bastos and Barreto-Souza (2020)\n  <doi:10.1080/02664763.2020.1780570> that uses the Birnbaum-Saunders\n  distribution as a joint distribution of the selection and primary regression\n  variables. This package extends and complements existing R packages such as \n  'sampleSelection' (Toomet and Henningsen, 2008) and 'ssmrob' (Zhelonkin et al., 2016), providing additional robust and flexible sample selection models.  "
  },
  {
    "id": 21341,
    "package_name": "ssym",
    "title": "Fitting Semi-Parametric log-Symmetric Regression Models",
    "description": "Set of tools to fit a semi-parametric regression model suitable for analysis of data sets in which the response variable is continuous, strictly positive, asymmetric and possibly, censored. Under this setup, both the median and the skewness of the response variable distribution are explicitly modeled by using semi-parametric functions, whose non-parametric components may be approximated by natural cubic splines or P-splines. Supported distributions for the model error include log-normal, log-Student-t, log-power-exponential, log-hyperbolic, log-contaminated-normal, log-slash, Birnbaum-Saunders and Birnbaum-Saunders-t distributions.",
    "version": "1.5.8",
    "maintainer": "Luis Hernando Vanegas <hvanegasp@gmail.com>",
    "author": "Luis Hernando Vanegas <hvanegasp@gmail.com> and Gilberto A. Paula",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ssym",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ssym Fitting Semi-Parametric log-Symmetric Regression Models Set of tools to fit a semi-parametric regression model suitable for analysis of data sets in which the response variable is continuous, strictly positive, asymmetric and possibly, censored. Under this setup, both the median and the skewness of the response variable distribution are explicitly modeled by using semi-parametric functions, whose non-parametric components may be approximated by natural cubic splines or P-splines. Supported distributions for the model error include log-normal, log-Student-t, log-power-exponential, log-hyperbolic, log-contaminated-normal, log-slash, Birnbaum-Saunders and Birnbaum-Saunders-t distributions.  "
  },
  {
    "id": 21344,
    "package_name": "stR",
    "title": "Seasonal Trend Decomposition Using Regression",
    "description": "Methods for decomposing seasonal data: STR (a Seasonal-Trend \n  time series decomposition procedure based on Regression) and Robust STR. In \n  some ways, STR is similar to Ridge Regression and Robust STR can be related to \n  LASSO. They allow for multiple seasonal components, multiple linear covariates \n  with constant, flexible and seasonal influence. Seasonal patterns (for both \n  seasonal components and seasonal covariates) can be fractional and flexible \n  over time; moreover they can be either strictly periodic or have a more \n  complex topology. The methods provide confidence intervals for the estimated \n  components. The methods can also be used for forecasting.",
    "version": "0.7.1",
    "maintainer": "Rob Hyndman <Rob.Hyndman@monash.edu>",
    "author": "Alexander Dokumentov [aut] (ORCID:\n    <https://orcid.org/0000-0003-0478-0983>),\n  Rob Hyndman [aut, cre] (ORCID: <https://orcid.org/0000-0002-2140-5352>)",
    "url": "https://pkg.robjhyndman.com/stR/,\nhttps://github.com/robjhyndman/stR",
    "bug_reports": "https://github.com/robjhyndman/stR/issues",
    "repository": "https://cran.r-project.org/package=stR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stR Seasonal Trend Decomposition Using Regression Methods for decomposing seasonal data: STR (a Seasonal-Trend \n  time series decomposition procedure based on Regression) and Robust STR. In \n  some ways, STR is similar to Ridge Regression and Robust STR can be related to \n  LASSO. They allow for multiple seasonal components, multiple linear covariates \n  with constant, flexible and seasonal influence. Seasonal patterns (for both \n  seasonal components and seasonal covariates) can be fractional and flexible \n  over time; moreover they can be either strictly periodic or have a more \n  complex topology. The methods provide confidence intervals for the estimated \n  components. The methods can also be used for forecasting.  "
  },
  {
    "id": 21354,
    "package_name": "stablespec",
    "title": "Stable Specification Search in Structural Equation Models",
    "description": "An exploratory and heuristic approach for specification search in\n    Structural Equation Modeling. The basic idea is to subsample the original data\n    and then search for optimal models on each subset. Optimality is defined through\n    two objectives: model fit and parsimony. As these objectives are conflicting,\n    we apply a multi-objective optimization methods, specifically NSGA-II, to obtain\n    optimal models for the whole range of model complexities. From these optimal\n    models, we consider only the relevant model specifications (structures), i.e.,\n    those that are both stable (occur frequently) and parsimonious and use those to\n    infer a causal model.",
    "version": "0.3.0",
    "maintainer": "Ridho Rahmadi <r.rahmadi@cs.ru.nl>",
    "author": "Ridho Rahmadi [aut, cre],\n  Perry Groot [aut, ths],\n  Tom Heskes [aut, ths],\n  Christoph Stich [ctb]",
    "url": "https://github.com/rahmarid/stablespec",
    "bug_reports": "https://github.com/rahmarid/stablespec/issues",
    "repository": "https://cran.r-project.org/package=stablespec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stablespec Stable Specification Search in Structural Equation Models An exploratory and heuristic approach for specification search in\n    Structural Equation Modeling. The basic idea is to subsample the original data\n    and then search for optimal models on each subset. Optimality is defined through\n    two objectives: model fit and parsimony. As these objectives are conflicting,\n    we apply a multi-objective optimization methods, specifically NSGA-II, to obtain\n    optimal models for the whole range of model complexities. From these optimal\n    models, we consider only the relevant model specifications (structures), i.e.,\n    those that are both stable (occur frequently) and parsimonious and use those to\n    infer a causal model.  "
  },
  {
    "id": 21409,
    "package_name": "statioVAR",
    "title": "Trend Removal for Vector Autoregressive Workflows",
    "description": "Detrending multivariate time-series to approximate stationarity when dealing with intensive longitudinal data, prior to Vector Autoregressive (VAR) or multilevel-VAR estimation. Classical VAR assumes weak stationarity (constant first two moments), and deterministic trends inflate spurious autocorrelation, biasing Granger-causality and impulse-response analyses. All functions operate on raw panel data and write detrended columns back to the data set, but differ in the level at which the trend is estimated. See, for instance, Wang & Maxwell (2015) <doi:10.1037/met0000030>; Burger et al. (2022) <doi:10.4324/9781003111238-13>; Epskamp et al. (2018) <doi:10.1177/2167702617744325>.  ",
    "version": "0.1.3",
    "maintainer": "Giuseppe Corbelli <giuseppe.corbelli@uniroma1.it>",
    "author": "Giuseppe Corbelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2864-3548>)",
    "url": "https://github.com/g-corbelli/statioVAR",
    "bug_reports": "https://github.com/g-corbelli/statioVAR/issues",
    "repository": "https://cran.r-project.org/package=statioVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statioVAR Trend Removal for Vector Autoregressive Workflows Detrending multivariate time-series to approximate stationarity when dealing with intensive longitudinal data, prior to Vector Autoregressive (VAR) or multilevel-VAR estimation. Classical VAR assumes weak stationarity (constant first two moments), and deterministic trends inflate spurious autocorrelation, biasing Granger-causality and impulse-response analyses. All functions operate on raw panel data and write detrended columns back to the data set, but differ in the level at which the trend is estimated. See, for instance, Wang & Maxwell (2015) <doi:10.1037/met0000030>; Burger et al. (2022) <doi:10.4324/9781003111238-13>; Epskamp et al. (2018) <doi:10.1177/2167702617744325>.    "
  },
  {
    "id": 21450,
    "package_name": "stepgbm",
    "title": "Stepwise Variable Selection for Generalized Boosted Regression\nModeling",
    "description": "An introduction to a couple of novel predictive variable selection methods for generalised boosted regression modeling (gbm). They are based on various variable influence methods (i.e., relative variable influence (RVI) and knowledge informed RVI (i.e., KIRVI, and KIRVI2)) that adopted similar ideas as AVI, KIAVI and KIAVI2 in the 'steprf' package, and also based on predictive accuracy in stepwise algorithms. For details of the variable selection methods, please see: Li, J., Siwabessy, J., Huang, Z. and Nichol, S. (2019) <doi:10.3390/geosciences9040180>. Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R., Radke, L., Howard, F., Nichol, S. (2017). <DOI: 10.13140/RG.2.2.27686.22085>.",
    "version": "1.0.1",
    "maintainer": "Jin Li <jinli68@gmail.com>",
    "author": "Jin Li [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stepgbm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stepgbm Stepwise Variable Selection for Generalized Boosted Regression\nModeling An introduction to a couple of novel predictive variable selection methods for generalised boosted regression modeling (gbm). They are based on various variable influence methods (i.e., relative variable influence (RVI) and knowledge informed RVI (i.e., KIRVI, and KIRVI2)) that adopted similar ideas as AVI, KIAVI and KIAVI2 in the 'steprf' package, and also based on predictive accuracy in stepwise algorithms. For details of the variable selection methods, please see: Li, J., Siwabessy, J., Huang, Z. and Nichol, S. (2019) <doi:10.3390/geosciences9040180>. Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R., Radke, L., Howard, F., Nichol, S. (2017). <DOI: 10.13140/RG.2.2.27686.22085>.  "
  },
  {
    "id": 21452,
    "package_name": "stepmetrics",
    "title": "Calculate Step and Cadence Metrics from Wearable Data",
    "description": "Provides functions to calculate step- and cadence-based metrics from \n    timestamped accelerometer and wearable device data. Supports CSV and AGD files from \n    'ActiGraph' devices, CSV files from 'Fitbit' devices, and step counts derived   \n    with R package 'GGIR' <https://github.com/wadpac/GGIR>, with automatic handling \n    of epoch lengths from 1 to 60 seconds. Metrics include total steps, cadence \n    peaks, minutes and steps in predefined cadence bands, and time and steps in \n    moderate-to-vigorous physical activity (MVPA). Methods and thresholds are \n    informed by the literature, e.g., \n    Tudor-Locke and Rowe (2012) <doi:10.2165/11599170-000000000-00000>, \n    Barreira et al. (2012) <doi:10.1249/MSS.0b013e318254f2a3>, \n    and Tudor-Locke et al. (2018) <doi:10.1136/bjsports-2017-097628>. \n    The package record is also available on Zenodo (2023) <doi:10.5281/zenodo.7858094>.",
    "version": "1.0.3",
    "maintainer": "Jairo H Migueles <jairo@jhmigueles.com>",
    "author": "Jairo H Migueles [aut, cre],\n  Elroy J Aguiar [fnd] (Funding and data support),\n  University of Alabama [fnd]",
    "url": "https://github.com/jhmigueles/stepmetrics",
    "bug_reports": "https://github.com/jhmigueles/stepmetrics/issues",
    "repository": "https://cran.r-project.org/package=stepmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stepmetrics Calculate Step and Cadence Metrics from Wearable Data Provides functions to calculate step- and cadence-based metrics from \n    timestamped accelerometer and wearable device data. Supports CSV and AGD files from \n    'ActiGraph' devices, CSV files from 'Fitbit' devices, and step counts derived   \n    with R package 'GGIR' <https://github.com/wadpac/GGIR>, with automatic handling \n    of epoch lengths from 1 to 60 seconds. Metrics include total steps, cadence \n    peaks, minutes and steps in predefined cadence bands, and time and steps in \n    moderate-to-vigorous physical activity (MVPA). Methods and thresholds are \n    informed by the literature, e.g., \n    Tudor-Locke and Rowe (2012) <doi:10.2165/11599170-000000000-00000>, \n    Barreira et al. (2012) <doi:10.1249/MSS.0b013e318254f2a3>, \n    and Tudor-Locke et al. (2018) <doi:10.1136/bjsports-2017-097628>. \n    The package record is also available on Zenodo (2023) <doi:10.5281/zenodo.7858094>.  "
  },
  {
    "id": 21518,
    "package_name": "strategicplayers",
    "title": "Strategic Players",
    "description": "Identifies individuals in a social network who should be the intervention\n    subjects for a network intervention in which you have a group of targets, a\n    group of avoiders, and a group that is neither.",
    "version": "1.1",
    "maintainer": "Miles Ott <miles_ott@alumni.brown.edu>",
    "author": "Miles Ott",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=strategicplayers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strategicplayers Strategic Players Identifies individuals in a social network who should be the intervention\n    subjects for a network intervention in which you have a group of targets, a\n    group of avoiders, and a group that is neither.  "
  },
  {
    "id": 21561,
    "package_name": "stylest2",
    "title": "Estimating Speakers of Texts",
    "description": "Estimates the authors or speakers of texts. Methods developed in Huang, Perry, and Spirling (2020) <doi:10.1017/pan.2019.49>. The model is built on a Bayesian framework in which the distinctiveness of each speaker is defined by how different, on average, the speaker's terms are to everyone else in the corpus of texts. An optional cross-validation method is implemented to select the subset of terms that generate the most accurate speaker predictions. Once a set of terms is selected, the model can be estimated. Speaker distinctiveness and term influence can be recovered from parameters in the model using package functions. Once fitted, the model can be used to predict authorship of new texts.",
    "version": "0.1",
    "maintainer": "Christian Baehr <cbaehr@princeton.edu>",
    "author": "Christian Baehr [aut, cre, cph],\n  Arthur Spirling [aut, cph],\n  Leslie Huang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stylest2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stylest2 Estimating Speakers of Texts Estimates the authors or speakers of texts. Methods developed in Huang, Perry, and Spirling (2020) <doi:10.1017/pan.2019.49>. The model is built on a Bayesian framework in which the distinctiveness of each speaker is defined by how different, on average, the speaker's terms are to everyone else in the corpus of texts. An optional cross-validation method is implemented to select the subset of terms that generate the most accurate speaker predictions. Once a set of terms is selected, the model can be estimated. Speaker distinctiveness and term influence can be recovered from parameters in the model using package functions. Once fitted, the model can be used to predict authorship of new texts.  "
  },
  {
    "id": 21582,
    "package_name": "sudokuAlt",
    "title": "Tools for Making and Spoiling Sudoku Games",
    "description": "Tools for making, retrieving, displaying and solving sudoku games.\n    This package is an alternative to the earlier sudoku-solver package,\n    'sudoku'.  The present package uses a slightly different algorithm, has a\n    simpler coding and presents a few more sugar tools, such as plot and print\n    methods.  Solved sudoku games are of some interest in Experimental Design\n    as examples of Latin Square designs with additional balance constraints.",
    "version": "0.2-1",
    "maintainer": "Bill Venables <Bill.Venables@gmail.com>",
    "author": "Bill Venables <Bill.Venables@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sudokuAlt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sudokuAlt Tools for Making and Spoiling Sudoku Games Tools for making, retrieving, displaying and solving sudoku games.\n    This package is an alternative to the earlier sudoku-solver package,\n    'sudoku'.  The present package uses a slightly different algorithm, has a\n    simpler coding and presents a few more sugar tools, such as plot and print\n    methods.  Solved sudoku games are of some interest in Experimental Design\n    as examples of Latin Square designs with additional balance constraints.  "
  },
  {
    "id": 21646,
    "package_name": "survRM2perm",
    "title": "Permutation Test for Comparing Restricted Mean Survival Time",
    "description": "Performs the permutation test using difference in the restricted mean survival time (RMST) between groups as a summary measure of the survival time distribution. When the sample size is less than 50 per group, it has been shown that there is non-negligible inflation of the type I error rate in the commonly used asymptotic test for the RMST comparison. Generally, permutation tests can be useful in such a situation. However, when we apply the permutation test for the RMST comparison, particularly in small sample situations, there are some cases where the survival function in either group cannot be defined due to censoring in the permutation process. Horiguchi and Uno (2020) <doi:10.1002/sim.8565> have examined six workable solutions to handle this numerical issue. It performs permutation tests with implementation of the six methods outlined in the paper when the numerical issue arises during the permutation process. The result of the asymptotic test is also provided for a reference.",
    "version": "0.1.0",
    "maintainer": "Miki Horiguchi <horiguchimiki@gmail.com>",
    "author": "Miki Horiguchi, Hajime Uno",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survRM2perm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survRM2perm Permutation Test for Comparing Restricted Mean Survival Time Performs the permutation test using difference in the restricted mean survival time (RMST) between groups as a summary measure of the survival time distribution. When the sample size is less than 50 per group, it has been shown that there is non-negligible inflation of the type I error rate in the commonly used asymptotic test for the RMST comparison. Generally, permutation tests can be useful in such a situation. However, when we apply the permutation test for the RMST comparison, particularly in small sample situations, there are some cases where the survival function in either group cannot be defined due to censoring in the permutation process. Horiguchi and Uno (2020) <doi:10.1002/sim.8565> have examined six workable solutions to handle this numerical issue. It performs permutation tests with implementation of the six methods outlined in the paper when the numerical issue arises during the permutation process. The result of the asymptotic test is also provided for a reference.  "
  },
  {
    "id": 21680,
    "package_name": "survivalsurrogate",
    "title": "Evaluate a Longitudinal Surrogate with a Censored Outcome",
    "description": "Provides influence function-based methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting, with plug-in and targeted maximum likelihood estimation options. Details are described in: Agniel D and Parast L (2025). \"Robust Evaluation of Longitudinal Surrogate Markers with Censored Data.\" Journal of the Royal Statistical Society: Series B <doi:10.1093/jrsssb/qkae119>. A tutorial for this package can be found at <https://www.laylaparast.com/survivalsurrogate> and a Shiny App \n  implementing the package can be found at \n  <https://parastlab.shinyapps.io/survivalsurrogateApp/>.",
    "version": "1.1",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "author": "Denis Agniel [aut],\n  Layla Parast [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survivalsurrogate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survivalsurrogate Evaluate a Longitudinal Surrogate with a Censored Outcome Provides influence function-based methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting, with plug-in and targeted maximum likelihood estimation options. Details are described in: Agniel D and Parast L (2025). \"Robust Evaluation of Longitudinal Surrogate Markers with Censored Data.\" Journal of the Royal Statistical Society: Series B <doi:10.1093/jrsssb/qkae119>. A tutorial for this package can be found at <https://www.laylaparast.com/survivalsurrogate> and a Shiny App \n  implementing the package can be found at \n  <https://parastlab.shinyapps.io/survivalsurrogateApp/>.  "
  },
  {
    "id": 21682,
    "package_name": "survivoR",
    "title": "Data from all Seasons of Survivor (US) TV Series in Tidy Format",
    "description": "Datasets detailing the results, castaways, and events of each \n  season of Survivor for the US, Australia, South Africa, New Zealand, and the \n  UK. This includes details on the cast, voting history, immunity and reward \n  challenges, jury votes, boot order, advantage details, and episode ratings. \n  Use this for analysis of trends and statistics of the game.",
    "version": "2.3.8",
    "maintainer": "Daniel Oehm <danieloehm@gmail.com>",
    "author": "Daniel Oehm [aut, cre],\n  Carly Levitz [ctb]",
    "url": "https://github.com/doehm/survivoR",
    "bug_reports": "https://github.com/doehm/survivoR/issues",
    "repository": "https://cran.r-project.org/package=survivoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survivoR Data from all Seasons of Survivor (US) TV Series in Tidy Format Datasets detailing the results, castaways, and events of each \n  season of Survivor for the US, Australia, South Africa, New Zealand, and the \n  UK. This includes details on the cast, voting history, immunity and reward \n  challenges, jury votes, boot order, advantage details, and episode ratings. \n  Use this for analysis of trends and statistics of the game.  "
  },
  {
    "id": 21689,
    "package_name": "survstan",
    "title": "Fitting Survival Regression Models via 'Stan'",
    "description": "Parametric survival regression models under the maximum likelihood approach via 'Stan'. Implemented regression models include accelerated failure time models, proportional hazards models, proportional odds models, accelerated hazard models, Yang and Prentice models, and extended hazard models. Available baseline survival distributions include exponential, Weibull, log-normal, log-logistic, gamma, generalized gamma, rayleigh, Gompertz and fatigue (Birnbaum-Saunders) distributions. References: Lawless (2002) <ISBN:9780471372158>; Bennett (1982) <doi:10.1002/sim.4780020223>; Chen and Wang(2000) <doi:10.1080/01621459.2000.10474236>; Demarqui and Mayrink (2021) <doi:10.1214/20-BJPS471>.",
    "version": "0.0.7.1",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "author": "Fabio Demarqui [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9236-1986>),\n  Andrew Johnson [ctb]",
    "url": "https://github.com/fndemarqui/survstan,\nhttps://fndemarqui.github.io/survstan/",
    "bug_reports": "https://github.com/fndemarqui/survstan/issues",
    "repository": "https://cran.r-project.org/package=survstan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survstan Fitting Survival Regression Models via 'Stan' Parametric survival regression models under the maximum likelihood approach via 'Stan'. Implemented regression models include accelerated failure time models, proportional hazards models, proportional odds models, accelerated hazard models, Yang and Prentice models, and extended hazard models. Available baseline survival distributions include exponential, Weibull, log-normal, log-logistic, gamma, generalized gamma, rayleigh, Gompertz and fatigue (Birnbaum-Saunders) distributions. References: Lawless (2002) <ISBN:9780471372158>; Bennett (1982) <doi:10.1002/sim.4780020223>; Chen and Wang(2000) <doi:10.1080/01621459.2000.10474236>; Demarqui and Mayrink (2021) <doi:10.1214/20-BJPS471>.  "
  },
  {
    "id": 21721,
    "package_name": "svydiags",
    "title": "Regression Model Diagnostics for Survey Data",
    "description": "Diagnostics for fixed effects linear and general linear regression models fitted with survey data. Extensions of standard diagnostics to complex survey data are included: standardized residuals, leverages, Cook's D, dfbetas, dffits, condition indexes, and variance inflation factors as found in Li and Valliant (Surv. Meth., 2009, 35(1), pp. 15-24; Jnl. of Off. Stat., 2011, 27(1), pp. 99-119; Jnl. of Off. Stat., 2015, 31(1), pp. 61-75); Liao and Valliant (Surv. Meth., 2012, 38(1), pp. 53-62; Surv. Meth., 2012, 38(2), pp. 189-202).  Variance inflation factors and condition indexes are also computed for some general linear models as described in Liao (U. Maryland thesis, 2010).",
    "version": "0.7",
    "maintainer": "Richard Valliant <valliant@umich.edu>",
    "author": "Richard Valliant [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svydiags",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svydiags Regression Model Diagnostics for Survey Data Diagnostics for fixed effects linear and general linear regression models fitted with survey data. Extensions of standard diagnostics to complex survey data are included: standardized residuals, leverages, Cook's D, dfbetas, dffits, condition indexes, and variance inflation factors as found in Li and Valliant (Surv. Meth., 2009, 35(1), pp. 15-24; Jnl. of Off. Stat., 2011, 27(1), pp. 99-119; Jnl. of Off. Stat., 2015, 31(1), pp. 61-75); Liao and Valliant (Surv. Meth., 2012, 38(1), pp. 53-62; Surv. Meth., 2012, 38(2), pp. 189-202).  Variance inflation factors and condition indexes are also computed for some general linear models as described in Liao (U. Maryland thesis, 2010).  "
  },
  {
    "id": 21776,
    "package_name": "synthesizer",
    "title": "Fast, Robust, and High-Quality Synthetic Data Generation with a\nTuneable Privacy-Utility Trade-Off",
    "description": "Synthesize numeric, categorical, mixed and time series data. Data \n    circumstances including mixed (or zero-inflated) distributions and missing\n    data patterns are reproduced in the synthetic data. A single parameter allows\n    balancing between high-quality synthetic data that represents correlations of\n    the original data and lower quality but more privacy safe synthetic data\n    without correlations. Tuning can be done per variable or for the whole\n    dataset.",
    "version": "0.6.0",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9807-4686>)",
    "url": "https://github.com/markvanderloo/synthesizer",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=synthesizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "synthesizer Fast, Robust, and High-Quality Synthetic Data Generation with a\nTuneable Privacy-Utility Trade-Off Synthesize numeric, categorical, mixed and time series data. Data \n    circumstances including mixed (or zero-inflated) distributions and missing\n    data patterns are reproduced in the synthetic data. A single parameter allows\n    balancing between high-quality synthetic data that represents correlations of\n    the original data and lower quality but more privacy safe synthetic data\n    without correlations. Tuning can be done per variable or for the whole\n    dataset.  "
  },
  {
    "id": 21777,
    "package_name": "synthpop",
    "title": "Generating Synthetic Versions of Sensitive Microdata for\nStatistical Disclosure Control",
    "description": "A tool for producing synthetic versions of microdata containing confidential information so that they are safe to be released to users for exploratory analysis. The key objective of generating synthetic data is to replace sensitive original values with synthetic ones causing minimal distortion of the statistical information contained in the data set. Variables, which can be categorical or continuous, are synthesised one-by-one using sequential modelling. Replacements are generated by drawing from conditional distributions fitted to the original data using parametric or classification and regression trees models. Data are synthesised via the function syn() which can be largely automated, if default settings are used, or with methods defined by the user. Optional parameters can be used to influence the disclosure risk and the analytical quality of the synthesised data. For a description of the implemented method see Nowok, Raab and Dibben (2016) <doi:10.18637/jss.v074.i11>. Functions to assess identity and attribute disclosure for the original and for the synthetic data are included in the package, and their use is illustrated in a vignette on disclosure (Practical Privacy Metrics for Synthetic Data).",
    "version": "1.9-2",
    "maintainer": "Beata Nowok <beata.nowok@gmail.com>",
    "author": "Beata Nowok [aut, cre],\n  Gillian M Raab [aut],\n  Chris Dibben [ctb],\n  Joshua Snoke [ctb],\n  Caspar van Lissa [ctb],\n  Lotte Pater [ctb]",
    "url": "<https://www.synthpop.org.uk/>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=synthpop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "synthpop Generating Synthetic Versions of Sensitive Microdata for\nStatistical Disclosure Control A tool for producing synthetic versions of microdata containing confidential information so that they are safe to be released to users for exploratory analysis. The key objective of generating synthetic data is to replace sensitive original values with synthetic ones causing minimal distortion of the statistical information contained in the data set. Variables, which can be categorical or continuous, are synthesised one-by-one using sequential modelling. Replacements are generated by drawing from conditional distributions fitted to the original data using parametric or classification and regression trees models. Data are synthesised via the function syn() which can be largely automated, if default settings are used, or with methods defined by the user. Optional parameters can be used to influence the disclosure risk and the analytical quality of the synthesised data. For a description of the implemented method see Nowok, Raab and Dibben (2016) <doi:10.18637/jss.v074.i11>. Functions to assess identity and attribute disclosure for the original and for the synthetic data are included in the package, and their use is illustrated in a vignette on disclosure (Practical Privacy Metrics for Synthetic Data).  "
  },
  {
    "id": 21807,
    "package_name": "tableeasy",
    "title": "Tables of Clinical Study",
    "description": "Creates some tables of clinical study. 'Table 1' is created by table1() to describe baseline characteristics, which is essential in every clinical study. Created by table2(), the function of 'Table 2' is to explore influence factors. And 'Table 3' created by table3() is able to make stratified analysis.",
    "version": "1.1.2",
    "maintainer": "Jian Hang Zheng <1040854241@qq.com>",
    "author": "Jian Hang Zheng [aut, cre],\n  Yong Qi Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tableeasy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tableeasy Tables of Clinical Study Creates some tables of clinical study. 'Table 1' is created by table1() to describe baseline characteristics, which is essential in every clinical study. Created by table2(), the function of 'Table 2' is to explore influence factors. And 'Table 3' created by table3() is able to make stratified analysis.  "
  },
  {
    "id": 21835,
    "package_name": "tailTransform",
    "title": "Symmetric Transformation of Tails for Plotting Differences",
    "description": "When plotting treated-minus-control differences, after-minus-before changes, or difference-in-differences, the ttrans() function symmetrically transforms the positive and negative tails to aid plotting.  The package includes an observational study with three control groups and an unaffected outcome; see Rosenbaum (2022) <doi:10.1080/00031305.2022.2063944>.",
    "version": "2.0.0",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tailTransform",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tailTransform Symmetric Transformation of Tails for Plotting Differences When plotting treated-minus-control differences, after-minus-before changes, or difference-in-differences, the ttrans() function symmetrically transforms the positive and negative tails to aid plotting.  The package includes an observational study with three control groups and an unaffected outcome; see Rosenbaum (2022) <doi:10.1080/00031305.2022.2063944>.  "
  },
  {
    "id": 21867,
    "package_name": "tbea",
    "title": "Pre- And Post-Processing in Bayesian Evolutionary Analyses",
    "description": "Functions are provided for prior specification in divergence time\n    estimation using fossils as well as other kinds of data. It\n    provides tools for interacting with the input and output of Bayesian\n    platforms in evolutionary biology such as 'BEAST2', 'MrBayes', 'RevBayes',\n    or 'MCMCTree'.\n    It Implements a simple measure similarity between probability\n    density functions for comparing prior and\n    posterior Bayesian densities, as well as code for calculating the\n    combination of distributions using conflation of Hill (2008). Functions for estimating the\n    origination time in collections of distributions using the x-intercept (e.g., Draper and Smith, 1998) and\n    stratigraphic intervals (Marshall 2010) are also available.\n    Hill, T. 2008. \"Conflations of probability distributions\". Transactions of the American Mathematical Society, 363:3351-3372. <doi:10.48550/arXiv.0808.1808>,\n    Draper, N. R. and Smith, H. 1998. \"Applied Regression Analysis\". 1--706. Wiley Interscience, New York. <DOI:10.1002/9781118625590>,\n    Marshall, C. R. 2010. \"Using confidence intervals to quantify the uncertainty in the end-points of stratigraphic ranges\". Quantitative Methods in Paleobiology, 291--316. <DOI:10.1017/S1089332600001911>.",
    "version": "1.7.0",
    "maintainer": "Gustavo A. Ballen <gustavo.a.ballen@gmail.com>",
    "author": "Gustavo A. Ballen [aut, cre],\n  Sandra Reinales [aut]",
    "url": "https://github.com/gaballench/tbea,\nhttps://gaballench.github.io/tbea/",
    "bug_reports": "https://github.com/gaballench/tbea/issues",
    "repository": "https://cran.r-project.org/package=tbea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tbea Pre- And Post-Processing in Bayesian Evolutionary Analyses Functions are provided for prior specification in divergence time\n    estimation using fossils as well as other kinds of data. It\n    provides tools for interacting with the input and output of Bayesian\n    platforms in evolutionary biology such as 'BEAST2', 'MrBayes', 'RevBayes',\n    or 'MCMCTree'.\n    It Implements a simple measure similarity between probability\n    density functions for comparing prior and\n    posterior Bayesian densities, as well as code for calculating the\n    combination of distributions using conflation of Hill (2008). Functions for estimating the\n    origination time in collections of distributions using the x-intercept (e.g., Draper and Smith, 1998) and\n    stratigraphic intervals (Marshall 2010) are also available.\n    Hill, T. 2008. \"Conflations of probability distributions\". Transactions of the American Mathematical Society, 363:3351-3372. <doi:10.48550/arXiv.0808.1808>,\n    Draper, N. R. and Smith, H. 1998. \"Applied Regression Analysis\". 1--706. Wiley Interscience, New York. <DOI:10.1002/9781118625590>,\n    Marshall, C. R. 2010. \"Using confidence intervals to quantify the uncertainty in the end-points of stratigraphic ranges\". Quantitative Methods in Paleobiology, 291--316. <DOI:10.1017/S1089332600001911>.  "
  },
  {
    "id": 21905,
    "package_name": "teamcolors",
    "title": "Color Palettes for Pro Sports Teams",
    "description": "Provides color palettes corresponding to professional and amateur, \n    sports teams. These can be useful in creating data graphics that are themed \n    for particular teams. ",
    "version": "0.0.4",
    "maintainer": "Benjamin S. Baumer <ben.baumer@gmail.com>",
    "author": "Benjamin S. Baumer [aut, cre],\n  Gregory J. Matthews [aut],\n  Luke Benz [ctb],\n  Arielle Dror [ctb],\n  Clara Rosenberg [ctb],\n  Paige Patrick [ctb]",
    "url": "http://github.com/beanumber/teamcolors",
    "bug_reports": "https://github.com/beanumber/teamcolors/issues",
    "repository": "https://cran.r-project.org/package=teamcolors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "teamcolors Color Palettes for Pro Sports Teams Provides color palettes corresponding to professional and amateur, \n    sports teams. These can be useful in creating data graphics that are themed \n    for particular teams.   "
  },
  {
    "id": 21947,
    "package_name": "testDriveR",
    "title": "Teaching Data for Statistics and Data Science",
    "description": "Provides data sets for teaching statistics and data science courses. \n    It includes a sample of data from John Edmund Kerrich's famous \n    coinflip experiment. These are data that I used for statistics. The package \n    also contains an R Markdown template with the required formatting for \n    assignments in my former courses.",
    "version": "0.5.3",
    "maintainer": "Christopher Prener <chris.prener@gmail.com>",
    "author": "Christopher Prener [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4310-9888>),\n  Bill Bradley [dtc],\n  NORC at the University of Chicago [dtc],\n  UN Inter-agency Group for Child Mortality Estimation [dtc],\n  U.S. Department of Energy [dtc]",
    "url": "https://chris-prener.github.io/testDriveR/,\nhttps://github.com/chris-prener/testDriveR",
    "bug_reports": "https://github.com/chris-prener/testDriveR/issues",
    "repository": "https://cran.r-project.org/package=testDriveR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testDriveR Teaching Data for Statistics and Data Science Provides data sets for teaching statistics and data science courses. \n    It includes a sample of data from John Edmund Kerrich's famous \n    coinflip experiment. These are data that I used for statistics. The package \n    also contains an R Markdown template with the required formatting for \n    assignments in my former courses.  "
  },
  {
    "id": 21960,
    "package_name": "testtwice",
    "title": "Testing One Hypothesis Twice in Observational Studies",
    "description": "Tests one hypothesis with several test statistics, correcting for multiple testing.  The central function in the package is testtwice().  In a sensitivity analysis, the method has the largest design sensitivity of its component tests.  The package implements the method and examples in Rosenbaum, P. R. (2012) <doi:10.1093/biomet/ass032> Testing one hypothesis twice in observational studies. Biometrika, 99(4), 763-774.  ",
    "version": "1.0.3",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=testtwice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testtwice Testing One Hypothesis Twice in Observational Studies Tests one hypothesis with several test statistics, correcting for multiple testing.  The central function in the package is testtwice().  In a sensitivity analysis, the method has the largest design sensitivity of its component tests.  The package implements the method and examples in Rosenbaum, P. R. (2012) <doi:10.1093/biomet/ass032> Testing one hypothesis twice in observational studies. Biometrika, 99(4), 763-774.    "
  },
  {
    "id": 21961,
    "package_name": "tetRys",
    "title": "Game of 'tetRys'",
    "description": "A game inspired by 'Tetris'. Opens a plot window device and starts a game of 'tetRys' in it. Steer the tetrominos with the arrow keys, press Pause to pause and Esc to end the game.",
    "version": "1.2",
    "maintainer": "Carsten Croonenbroeck <carsten.croonenbroeck@uni-rostock.de>",
    "author": "Carsten Croonenbroeck [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tetRys",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tetRys Game of 'tetRys' A game inspired by 'Tetris'. Opens a plot window device and starts a game of 'tetRys' in it. Steer the tetrominos with the arrow keys, press Pause to pause and Esc to end the game.  "
  },
  {
    "id": 21997,
    "package_name": "textstem",
    "title": "Tools for Stemming and Lemmatizing Text",
    "description": "Tools that stem and lemmatize text.  Stemming is a process that removes\n         endings such as affixes.  Lemmatization is the process of grouping inflected\n         forms together as a single base form.",
    "version": "0.1.4",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker [aut, cre]",
    "url": "http://github.com/trinker/textstem",
    "bug_reports": "http://github.com/trinker/textstem/issues",
    "repository": "https://cran.r-project.org/package=textstem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textstem Tools for Stemming and Lemmatizing Text Tools that stem and lemmatize text.  Stemming is a process that removes\n         endings such as affixes.  Lemmatization is the process of grouping inflected\n         forms together as a single base form.  "
  },
  {
    "id": 22046,
    "package_name": "tictactoe",
    "title": "Tic-Tac-Toe Game",
    "description": "\n  Implements tic-tac-toe game to play on console, either with human or AI players.\n  Various levels of AI players are trained through the Q-learning algorithm.",
    "version": "0.2.2",
    "maintainer": "Kota Mori <kmori05@gmail.com>",
    "author": "Kota Mori [aut, cre]",
    "url": "https://github.com/kota7/tictactoe",
    "bug_reports": "https://github.com/kota7/tictactoe/issues",
    "repository": "https://cran.r-project.org/package=tictactoe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tictactoe Tic-Tac-Toe Game \n  Implements tic-tac-toe game to play on console, either with human or AI players.\n  Various levels of AI players are trained through the Q-learning algorithm.  "
  },
  {
    "id": 22147,
    "package_name": "tightenBlock",
    "title": "Tightens an Observational Block Design by Balanced Subset\nMatching",
    "description": "Tightens an observational block design into a smaller design with either smaller or fewer blocks while controlling for covariates. The method uses fine balance, optimal subset matching (Rosenbaum, 2012 <doi:10.1198/jcgs.2011.09219>) and two-criteria matching (Zhang et al 2023 <doi:10.1080/01621459.2021.1981337>).  The main function is tighten().  The suggested 'rrelaxiv' package for solving minimum cost flow problems: (i) derives from Bertsekas and Tseng (1988) <doi:10.1007/BF02288322>, (ii) is not available on CRAN due to its academic license, (iii) may be downloaded from GitHub at <https://github.com/josherrickson/rrelaxiv/>, (iv) is not essential to use the package.",
    "version": "0.1.7",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tightenBlock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tightenBlock Tightens an Observational Block Design by Balanced Subset\nMatching Tightens an observational block design into a smaller design with either smaller or fewer blocks while controlling for covariates. The method uses fine balance, optimal subset matching (Rosenbaum, 2012 <doi:10.1198/jcgs.2011.09219>) and two-criteria matching (Zhang et al 2023 <doi:10.1080/01621459.2021.1981337>).  The main function is tighten().  The suggested 'rrelaxiv' package for solving minimum cost flow problems: (i) derives from Bertsekas and Tseng (1988) <doi:10.1007/BF02288322>, (ii) is not available on CRAN due to its academic license, (iii) may be downloaded from GitHub at <https://github.com/josherrickson/rrelaxiv/>, (iv) is not essential to use the package.  "
  },
  {
    "id": 22181,
    "package_name": "tint",
    "title": "'tint' is not 'Tufte'",
    "description": "A 'tufte'-alike style for 'rmarkdown'.\n A modern take on the 'Tufte' design for pdf and html vignettes,\n building on the 'tufte' package with additional contributions\n from the 'knitr' and 'ggtufte' package, and also acknowledging\n the key influence of 'envisioned css'.",
    "version": "0.1.6",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Jonathan Gilligan [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1375-6686>)",
    "url": "https://github.com/eddelbuettel/tint/,\nhttps://dirk.eddelbuettel.com/code/tint.html",
    "bug_reports": "https://github.com/eddelbuettel/tint/issues",
    "repository": "https://cran.r-project.org/package=tint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tint 'tint' is not 'Tufte' A 'tufte'-alike style for 'rmarkdown'.\n A modern take on the 'Tufte' design for pdf and html vignettes,\n building on the 'tufte' package with additional contributions\n from the 'knitr' and 'ggtufte' package, and also acknowledging\n the key influence of 'envisioned css'.  "
  },
  {
    "id": 22202,
    "package_name": "tipr",
    "title": "Tipping Point Analyses",
    "description": "The strength of evidence provided by epidemiological and\n    observational studies is inherently limited by the potential for\n    unmeasured confounding.  We focus on three key quantities: the\n    observed bound of the confidence interval closest to the null, the\n    relationship between an unmeasured confounder and the outcome, for\n    example a plausible residual effect size for an unmeasured continuous\n    or binary confounder, and the relationship between an unmeasured\n    confounder and the exposure, for example a realistic mean difference\n    or prevalence difference for this hypothetical confounder between\n    exposure groups. Building on the methods put forth by Cornfield et al.\n    (1959), Bross (1966), Schlesselman (1978), Rosenbaum & Rubin (1983),\n    Lin et al. (1998), Lash et al. (2009), Rosenbaum (1986), Cinelli &\n    Hazlett (2020), VanderWeele & Ding (2017), and Ding & VanderWeele\n    (2016), we can use these quantities to assess how an unmeasured\n    confounder may tip our result to insignificance.",
    "version": "1.0.2",
    "maintainer": "Lucy D'Agostino McGowan <lucydagostino@gmail.com>",
    "author": "Lucy D'Agostino McGowan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6983-2759>),\n  Malcolm Barrett [aut] (ORCID: <https://orcid.org/0000-0003-0299-5825>)",
    "url": "https://r-causal.github.io/tipr/, https://github.com/r-causal/tipr",
    "bug_reports": "https://github.com/r-causal/tipr/issues",
    "repository": "https://cran.r-project.org/package=tipr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tipr Tipping Point Analyses The strength of evidence provided by epidemiological and\n    observational studies is inherently limited by the potential for\n    unmeasured confounding.  We focus on three key quantities: the\n    observed bound of the confidence interval closest to the null, the\n    relationship between an unmeasured confounder and the outcome, for\n    example a plausible residual effect size for an unmeasured continuous\n    or binary confounder, and the relationship between an unmeasured\n    confounder and the exposure, for example a realistic mean difference\n    or prevalence difference for this hypothetical confounder between\n    exposure groups. Building on the methods put forth by Cornfield et al.\n    (1959), Bross (1966), Schlesselman (1978), Rosenbaum & Rubin (1983),\n    Lin et al. (1998), Lash et al. (2009), Rosenbaum (1986), Cinelli &\n    Hazlett (2020), VanderWeele & Ding (2017), and Ding & VanderWeele\n    (2016), we can use these quantities to assess how an unmeasured\n    confounder may tip our result to insignificance.  "
  },
  {
    "id": 22203,
    "package_name": "tipsae",
    "title": "Tools for Handling Indices and Proportions in Small Area\nEstimation",
    "description": "It allows for mapping proportions and indicators defined on the unit interval. It implements Beta-based small area methods comprising the classical Beta regression models, the Flexible Beta model and Zero and/or One Inflated extensions (Janicki 2020 <doi:10.1080/03610926.2019.1570266>). Such methods, developed within a Bayesian framework through Stan <https://mc-stan.org/>, come equipped with a set of diagnostics and complementary tools, visualizing and exporting functions. A Shiny application with a user-friendly interface can be launched to further simplify the process. For further details, refer to De Nicol\u00f2 and Gardini (2024 <doi:10.18637/jss.v108.i01>).",
    "version": "1.0.3",
    "maintainer": "Silvia De Nicol\u00f2 <silvia.denicolo@unibo.it>",
    "author": "Silvia De Nicol\u00f2 [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5052-6527>),\n  Aldo Gardini [aut] (ORCID: <https://orcid.org/0000-0002-2164-5815>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tipsae",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tipsae Tools for Handling Indices and Proportions in Small Area\nEstimation It allows for mapping proportions and indicators defined on the unit interval. It implements Beta-based small area methods comprising the classical Beta regression models, the Flexible Beta model and Zero and/or One Inflated extensions (Janicki 2020 <doi:10.1080/03610926.2019.1570266>). Such methods, developed within a Bayesian framework through Stan <https://mc-stan.org/>, come equipped with a set of diagnostics and complementary tools, visualizing and exporting functions. A Shiny application with a user-friendly interface can be launched to further simplify the process. For further details, refer to De Nicol\u00f2 and Gardini (2024 <doi:10.18637/jss.v108.i01>).  "
  },
  {
    "id": 22275,
    "package_name": "toporanga",
    "title": "Topological Sort-Based Hierarchy Inference",
    "description": "Deciphering hierarchy of agents exhibiting observable dominance events is a crucial problem in several disciplines, in particular in behavioural analysis of social animals, but also in social sciences and game theory. This package implements an inference approach based on graph theory, namely to extract the optimal acyclic subset of a weighted graph of dominance; this allows for hierarchy estimation through topological sorting. The package also contains infrastructure to investigate partially defined hierarchies and hierarchy dynamics.",
    "version": "1.0.0",
    "maintainer": "Miron B. Kursa <m@mbq.me>",
    "author": "Miron B. Kursa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7672-648X>)",
    "url": "https://gitlab.com/mbq/toporanga",
    "bug_reports": "https://gitlab.com/mbq/toporanga/-/issues",
    "repository": "https://cran.r-project.org/package=toporanga",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "toporanga Topological Sort-Based Hierarchy Inference Deciphering hierarchy of agents exhibiting observable dominance events is a crucial problem in several disciplines, in particular in behavioural analysis of social animals, but also in social sciences and game theory. This package implements an inference approach based on graph theory, namely to extract the optimal acyclic subset of a weighted graph of dominance; this allows for hierarchy estimation through topological sorting. The package also contains infrastructure to investigate partially defined hierarchies and hierarchy dynamics.  "
  },
  {
    "id": 22300,
    "package_name": "toxEval",
    "title": "Exploring Biological Relevance of Environmental Chemistry\nObservations",
    "description": "Data analysis package for estimating potential biological effects from chemical concentrations in environmental samples. Included are a set of functions to analyze, visualize, and organize measured concentration data as it relates to user-selected chemical-biological interaction benchmark data such as water quality criteria. The intent of these analyses is to develop a better understanding of the potential biological relevance of environmental chemistry data. Results can be used to prioritize which chemicals at which sites may be of greatest concern. These methods are meant to be used as a screening technique to predict potential for biological influence from chemicals that ultimately need to be validated with direct biological assays. A description of the analysis can be found in Blackwell (2017) <doi:10.1021/acs.est.7b01613>.",
    "version": "1.4.1",
    "maintainer": "Laura DeCicco <ldecicco@usgs.gov>",
    "author": "Laura DeCicco [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3915-9487>),\n  Steven Corsi [aut] (ORCID: <https://orcid.org/0000-0003-0583-5536>),\n  Daniel Villeneuve [aut] (ORCID:\n    <https://orcid.org/0000-0003-2801-0203>),\n  Brett Blackwell [aut] (ORCID: <https://orcid.org/0000-0003-1296-4539>),\n  Gerald Ankley [aut] (ORCID: <https://orcid.org/0000-0002-9937-615X>),\n  Alison Appling [rev] (Reviewed for USGS),\n  Dalma Martinovic [rev] (Reviewed for USGS)",
    "url": "",
    "bug_reports": "https://github.com/DOI-USGS/toxEval/issues",
    "repository": "https://cran.r-project.org/package=toxEval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "toxEval Exploring Biological Relevance of Environmental Chemistry\nObservations Data analysis package for estimating potential biological effects from chemical concentrations in environmental samples. Included are a set of functions to analyze, visualize, and organize measured concentration data as it relates to user-selected chemical-biological interaction benchmark data such as water quality criteria. The intent of these analyses is to develop a better understanding of the potential biological relevance of environmental chemistry data. Results can be used to prioritize which chemicals at which sites may be of greatest concern. These methods are meant to be used as a screening technique to predict potential for biological influence from chemicals that ultimately need to be validated with direct biological assays. A description of the analysis can be found in Blackwell (2017) <doi:10.1021/acs.est.7b01613>.  "
  },
  {
    "id": 22332,
    "package_name": "trajeR",
    "title": "Group Based Modeling Trajectory",
    "description": "Estimation of group-based trajectory models, including finite mixture models for longitudinal data, \n    supporting censored normal, zero-inflated Poisson, logit, and beta distributions, \n    using expectation-maximization and quasi-Newton methods, with tools for model selection, \n    diagnostics, and visualization of latent trajectory groups,\n    <doi:10.4159/9780674041318>, Nagin, D. (2005). Group-Based Modeling of Development. Cambridge, MA: Harvard University Press.\n    and Noel (2022), <https://orbilu.uni.lu/>, thesis.",
    "version": "0.11.1",
    "maintainer": "C\u00e9dric Noel <cedric.noel@univ-lorraine.fr>",
    "author": "C\u00e9dric Noel [aut, cre],\n  Jang Schiltz [aut]",
    "url": "https://github.com/gitedric/trajeR",
    "bug_reports": "https://github.com/gitedric/trajeR/issues",
    "repository": "https://cran.r-project.org/package=trajeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trajeR Group Based Modeling Trajectory Estimation of group-based trajectory models, including finite mixture models for longitudinal data, \n    supporting censored normal, zero-inflated Poisson, logit, and beta distributions, \n    using expectation-maximization and quasi-Newton methods, with tools for model selection, \n    diagnostics, and visualization of latent trajectory groups,\n    <doi:10.4159/9780674041318>, Nagin, D. (2005). Group-Based Modeling of Development. Cambridge, MA: Harvard University Press.\n    and Noel (2022), <https://orbilu.uni.lu/>, thesis.  "
  },
  {
    "id": 22367,
    "package_name": "treats",
    "title": "Trees and Traits Simulations",
    "description": "A modular package for simulating phylogenetic trees and species traits jointly. Trees can be simulated using modular birth-death parameters (e.g. changing starting parameters or algorithm rules). Traits can be simulated in any way designed by the user. The growth of the tree and the traits can influence each other through modifiers objects providing rules for affecting each other. Finally, events can be created to modify both the tree and the traits under specific conditions ( Guillerme, 2024 <DOI:10.1111/2041-210X.14306>).",
    "version": "1.1.6",
    "maintainer": "Thomas Guillerme <guillert@tcd.ie>",
    "author": "Thomas Guillerme [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4325-1275>)",
    "url": "https://github.com/TGuillerme/treats",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=treats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "treats Trees and Traits Simulations A modular package for simulating phylogenetic trees and species traits jointly. Trees can be simulated using modular birth-death parameters (e.g. changing starting parameters or algorithm rules). Traits can be simulated in any way designed by the user. The growth of the tree and the traits can influence each other through modifiers objects providing rules for affecting each other. Finally, events can be created to modify both the tree and the traits under specific conditions ( Guillerme, 2024 <DOI:10.1111/2041-210X.14306>).  "
  },
  {
    "id": 22374,
    "package_name": "treePlotArea",
    "title": "Correction Factors for Tree Plot Areas Intersected by Stand\nBoundaries",
    "description": "The German national forest inventory uses angle count sampling, \n    a sampling method first published as `Bitterlich, W.: Die Winkelz\u00e4hlmessung.\n    Allgemeine Forst- und Holzwirtschaftliche Zeitung, 58. Jahrg., Folge 11/12 \n    vom Juni 1947` and extended by Grosenbaugh\n    (<https://academic.oup.com/jof/article-abstract/50/1/32/4684174>)\n    as probability proportional to size sampling.\n    When plots are located near stand boundaries, their sizes and hence\n    their probabilities need to be corrected.",
    "version": "3.1.0",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "author": "Andreas Dominik Cullmann [aut, cre],\n  Bernhard B\u00f6sch [ctb],\n  Christoph Fischer [ctb],\n  Gerald K\u00e4ndler [ctb]",
    "url": "https://gitlab.com/fvafrcu/treeplotarea.git",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=treePlotArea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "treePlotArea Correction Factors for Tree Plot Areas Intersected by Stand\nBoundaries The German national forest inventory uses angle count sampling, \n    a sampling method first published as `Bitterlich, W.: Die Winkelz\u00e4hlmessung.\n    Allgemeine Forst- und Holzwirtschaftliche Zeitung, 58. Jahrg., Folge 11/12 \n    vom Juni 1947` and extended by Grosenbaugh\n    (<https://academic.oup.com/jof/article-abstract/50/1/32/4684174>)\n    as probability proportional to size sampling.\n    When plots are located near stand boundaries, their sizes and hence\n    their probabilities need to be corrected.  "
  },
  {
    "id": 22394,
    "package_name": "trendsegmentR",
    "title": "Linear Trend Segmentation",
    "description": "Performs the detection of linear trend changes for univariate time series \n    by implementing the bottom-up unbalanced wavelet transformation proposed by \n    H. Maeng and P. Fryzlewicz (2023). The estimated number and locations of the \n    change-points are returned with the piecewise-linear estimator for signal.",
    "version": "1.3.0",
    "maintainer": "Hyeyoung Maeng <hyeyoung.maeng@durham.ac.uk>",
    "author": "Hyeyoung Maeng [aut, cre],\n  Piotr Fryzlewicz [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=trendsegmentR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trendsegmentR Linear Trend Segmentation Performs the detection of linear trend changes for univariate time series \n    by implementing the bottom-up unbalanced wavelet transformation proposed by \n    H. Maeng and P. Fryzlewicz (2023). The estimated number and locations of the \n    change-points are returned with the piecewise-linear estimator for signal.  "
  },
  {
    "id": 22396,
    "package_name": "trendtestR",
    "title": "Exploratory Trend Analysis and Visualization for Time-Series and\nGrouped Data",
    "description": "Provides a set of exploratory data analysis (EDA) tools for \n    visualizing trends, diagnosing data types for beginner-friendly workflows, \n    and automatically routing to suitable statistical tests or trend exploration \n    models. Includes unified plotting functions for trend lines, grouped boxplots, \n    and comparative scatterplots; automated statistical testing (e.g., t-test, \n    Wilcoxon, ANOVA, Kruskal-Wallis, Tukey, Dunn) with optional effect size \n    calculation; and model-based trend analysis using generalized additive \n    models (GAM) for count data, generalized linear models (GLM) for continuous \n    data, and zero-inflated models (ZIP/ZINB) for count data with potential \n    zero-inflation. \n    Also supports time-window continuity checks, cross-year \n    handling in compare_monthly_cases(), and ARIMA-ready preparation with \n    stationarity diagnostics, ensuring consistent parameter styles for \n    reproducible research and user-friendly workflows.Methods are \n    based on R Core Team (2024) <https://www.R-project.org/>, \n    Wood, S.N.(2017, ISBN:978-1498728331),\n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>, \n    Simon Jackman (2024) <https://github.com/atahk/pscl/>,    \n    Achim Zeileis, Christian Kleiber, Simon Jackman (2008) <doi:10.18637/jss.v027.i08>.",
    "version": "1.0.1",
    "maintainer": "Gelan Huang <huanggelan97@icloud.com>",
    "author": "Gelan Huang [aut, cre]",
    "url": "https://github.com/GrahnH/trendtestR",
    "bug_reports": "https://github.com/GrahnH/trendtestR/issues",
    "repository": "https://cran.r-project.org/package=trendtestR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trendtestR Exploratory Trend Analysis and Visualization for Time-Series and\nGrouped Data Provides a set of exploratory data analysis (EDA) tools for \n    visualizing trends, diagnosing data types for beginner-friendly workflows, \n    and automatically routing to suitable statistical tests or trend exploration \n    models. Includes unified plotting functions for trend lines, grouped boxplots, \n    and comparative scatterplots; automated statistical testing (e.g., t-test, \n    Wilcoxon, ANOVA, Kruskal-Wallis, Tukey, Dunn) with optional effect size \n    calculation; and model-based trend analysis using generalized additive \n    models (GAM) for count data, generalized linear models (GLM) for continuous \n    data, and zero-inflated models (ZIP/ZINB) for count data with potential \n    zero-inflation. \n    Also supports time-window continuity checks, cross-year \n    handling in compare_monthly_cases(), and ARIMA-ready preparation with \n    stationarity diagnostics, ensuring consistent parameter styles for \n    reproducible research and user-friendly workflows.Methods are \n    based on R Core Team (2024) <https://www.R-project.org/>, \n    Wood, S.N.(2017, ISBN:978-1498728331),\n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>, \n    Simon Jackman (2024) <https://github.com/atahk/pscl/>,    \n    Achim Zeileis, Christian Kleiber, Simon Jackman (2008) <doi:10.18637/jss.v027.i08>.  "
  },
  {
    "id": 22403,
    "package_name": "tricolore",
    "title": "A Flexible Color Scale for Ternary Compositions",
    "description": "Compositional data consisting of three-parts can be color\n  mapped with a ternary color scale. Such a scale is provided by\n  the tricolore packages with options for discrete and continuous\n  colors, mean-centering and scaling. See \n  Jonas Sch\u00f6ley (2021) \"The centered ternary balance scheme. A technique\n  to visualize surfaces of unbalanced three-part compositions\"\n  <doi:10.4054/DemRes.2021.44.19>,\n  Jonas Sch\u00f6ley, Frans Willekens (2017) \"Visualizing compositional data\n  on the Lexis surface\" <doi:10.4054/DemRes.2017.36.21>, and\n  Ilya Kashnitsky, Jonas Sch\u00f6ley (2018) \"Regional population structures\n  at a glance\" <doi:10.1016/S0140-6736(18)31194-2>.",
    "version": "1.2.6",
    "maintainer": "Jonas Sch\u00f6ley <jschoeley@gmail.com>",
    "author": "Jonas Sch\u00f6ley [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3340-8518>),\n  Ilya Kashnitsky [aut] (ORCID: <https://orcid.org/0000-0003-1835-8687>)",
    "url": "https://github.com/jschoeley/tricolore",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tricolore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tricolore A Flexible Color Scale for Ternary Compositions Compositional data consisting of three-parts can be color\n  mapped with a ternary color scale. Such a scale is provided by\n  the tricolore packages with options for discrete and continuous\n  colors, mean-centering and scaling. See \n  Jonas Sch\u00f6ley (2021) \"The centered ternary balance scheme. A technique\n  to visualize surfaces of unbalanced three-part compositions\"\n  <doi:10.4054/DemRes.2021.44.19>,\n  Jonas Sch\u00f6ley, Frans Willekens (2017) \"Visualizing compositional data\n  on the Lexis surface\" <doi:10.4054/DemRes.2017.36.21>, and\n  Ilya Kashnitsky, Jonas Sch\u00f6ley (2018) \"Regional population structures\n  at a glance\" <doi:10.1016/S0140-6736(18)31194-2>.  "
  },
  {
    "id": 22505,
    "package_name": "ttservice",
    "title": "A Service for Tidy Transcriptomics Software Suite",
    "description": "It provides generic methods that are used by more than one package, avoiding conflicts. This package will be imported by 'tidySingleCellExperiment' and 'tidyseurat'. ",
    "version": "0.5.3",
    "maintainer": "Stefano Mangiola <mangiolastefano@gmail.com>",
    "author": "Stefano Mangiola [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ttservice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ttservice A Service for Tidy Transcriptomics Software Suite It provides generic methods that are used by more than one package, avoiding conflicts. This package will be imported by 'tidySingleCellExperiment' and 'tidyseurat'.   "
  },
  {
    "id": 22543,
    "package_name": "twenty48",
    "title": "Play a Game of 2048 in the Console",
    "description": "Generates a game of 2048 that can be played in the\n    console.  Supports grids of arbitrary sizes, undoing the last move,\n    and resuming a game that was exited during the current session.",
    "version": "0.2.1",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>)",
    "url": "https://github.com/rossellhayes/twenty48",
    "bug_reports": "https://github.com/rossellhayes/twenty48/issues",
    "repository": "https://cran.r-project.org/package=twenty48",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twenty48 Play a Game of 2048 in the Console Generates a game of 2048 that can be played in the\n    console.  Supports grids of arbitrary sizes, undoing the last move,\n    and resuming a game that was exited during the current session.  "
  },
  {
    "id": 22557,
    "package_name": "twopartm",
    "title": "Two-Part Model with Marginal Effects",
    "description": "Fit two-part regression models for zero-inflated data. The models and their components are represented using S4 classes and methods. Average Marginal effects and predictive margins with standard errors and\n  confidence intervals can be calculated from two-part model objects. Belotti, F., Deb, P., Manning, W. G., & Norton, E. C. (2015) <doi:10.1177/1536867X1501500102>.",
    "version": "0.1.0",
    "maintainer": "Yajie Duan <yajieritaduan@gmail.com>",
    "author": "Yajie Duan [aut, cre],\n  Birol Emir [aut],\n  Griffith Bell [aut],\n  Javier Cabrera [aut],\n  Pfizer Inc. [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twopartm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twopartm Two-Part Model with Marginal Effects Fit two-part regression models for zero-inflated data. The models and their components are represented using S4 classes and methods. Average Marginal effects and predictive margins with standard errors and\n  confidence intervals can be calculated from two-part model objects. Belotti, F., Deb, P., Manning, W. G., & Norton, E. C. (2015) <doi:10.1177/1536867X1501500102>.  "
  },
  {
    "id": 22593,
    "package_name": "ui",
    "title": "Uncertainty Intervals and Sensitivity Analysis for Missing Data",
    "description": "Implements functions to derive uncertainty intervals for (i) regression (linear and probit) parameters when outcome is missing not at random (non-ignorable missingness) introduced in Genbaeck, M., Stanghellini, E., de Luna, X. (2015) <doi:10.1007/s00362-014-0610-x> and Genbaeck, M., Ng, N., Stanghellini, E., de Luna, X. (2018) <doi:10.1007/s10433-017-0448-x>; and (ii) double robust and outcome regression estimators of average causal effects (on the treated) with possibly unobserved confounding introduced in Genbaeck, M., de Luna, X. (2018) <doi:10.1111/biom.13001>.",
    "version": "0.1.1",
    "maintainer": "Minna Genbaeck <minna.genback@umu.se>",
    "author": "Minna Genbaeck [aut, cre],",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ui Uncertainty Intervals and Sensitivity Analysis for Missing Data Implements functions to derive uncertainty intervals for (i) regression (linear and probit) parameters when outcome is missing not at random (non-ignorable missingness) introduced in Genbaeck, M., Stanghellini, E., de Luna, X. (2015) <doi:10.1007/s00362-014-0610-x> and Genbaeck, M., Ng, N., Stanghellini, E., de Luna, X. (2018) <doi:10.1007/s10433-017-0448-x>; and (ii) double robust and outcome regression estimators of average causal effects (on the treated) with possibly unobserved confounding introduced in Genbaeck, M., de Luna, X. (2018) <doi:10.1111/biom.13001>.  "
  },
  {
    "id": 22612,
    "package_name": "unbalhaar",
    "title": "Function Estimation via Unbalanced Haar Wavelets",
    "description": "Top-down and bottom-up algorithms\n        for nonparametric function estimation in Gaussian noise using\n        Unbalanced Haar wavelets.",
    "version": "2.1",
    "maintainer": "Piotr Fryzlewicz <p.fryzlewicz@lse.ac.uk>",
    "author": "Piotr Fryzlewicz",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=unbalhaar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unbalhaar Function Estimation via Unbalanced Haar Wavelets Top-down and bottom-up algorithms\n        for nonparametric function estimation in Gaussian noise using\n        Unbalanced Haar wavelets.  "
  },
  {
    "id": 22613,
    "package_name": "uncertainUCDP",
    "title": "Parametric Mixture Models for Uncertainty Estimation of\nFatalities in UCDP Conflict Data",
    "description": "Provides functions for estimating uncertainty in the number of fatalities in the Uppsala Conflict Data Program (UCDP) data. The package implements a parametric reported-value Gumbel mixture distribution that accounts for the uncertainty in the number of fatalities in the UCDP data. The model is based on information from a survey on UCDP coders and how they view the uncertainty of the number of fatalities from UCDP events. The package provides functions for making random draws of fatalities from the mixture distribution, as well as to estimate percentiles, quantiles, means, and other statistics of the distribution. Full details on the survey and estimation procedure can be found in Vesco et al (2024).",
    "version": "0.6.1",
    "maintainer": "David Randahl <david.randahl@pcr.uu.se>",
    "author": "David Randahl [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=uncertainUCDP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uncertainUCDP Parametric Mixture Models for Uncertainty Estimation of\nFatalities in UCDP Conflict Data Provides functions for estimating uncertainty in the number of fatalities in the Uppsala Conflict Data Program (UCDP) data. The package implements a parametric reported-value Gumbel mixture distribution that accounts for the uncertainty in the number of fatalities in the UCDP data. The model is based on information from a survey on UCDP coders and how they view the uncertainty of the number of fatalities from UCDP events. The package provides functions for making random draws of fatalities from the mixture distribution, as well as to estimate percentiles, quantiles, means, and other statistics of the distribution. Full details on the survey and estimation procedure can be found in Vesco et al (2024).  "
  },
  {
    "id": 22614,
    "package_name": "uncmbb",
    "title": "UNC Men's Basketball Match Results Since 1949-1950 Season",
    "description": "Dataset contains select attributes for each match result since 1949-1950 season for UNC men's basketball team.",
    "version": "0.2.2",
    "maintainer": "Jay Lee <joongsup@gmail.com>",
    "author": "Jay Lee [aut, cre]",
    "url": "https://github.com/joongsup/uncmbb",
    "bug_reports": "https://github.com/joongsup/uncmbb/issues",
    "repository": "https://cran.r-project.org/package=uncmbb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uncmbb UNC Men's Basketball Match Results Since 1949-1950 Season Dataset contains select attributes for each match result since 1949-1950 season for UNC men's basketball team.  "
  },
  {
    "id": 22637,
    "package_name": "unine",
    "title": "Unine Light Stemmer",
    "description": "Implementation of \"light\" stemmers for French, German, Italian, Spanish, Portuguese, Finnish, Swedish. \n  They are based on the same work as the \"light\" stemmers found in 'SolR' <https://lucene.apache.org/solr/> or 'ElasticSearch' <https://www.elastic.co/fr/products/elasticsearch>. \n  A \"light\" stemmer consists in removing inflections only for noun and adjectives. \n  Indexing verbs for these languages is not of primary importance compared to nouns and adjectives. \n  The stemming procedure for French is described in (Savoy, 1999) <doi:10.1002/(SICI)1097-4571(1999)50:10%3C944::AID-ASI9%3E3.3.CO;2-H>.",
    "version": "0.2.0",
    "maintainer": "Micha\u00ebl Benesty <michael@benesty.fr>",
    "author": "Micha\u00ebl Benesty [aut, cre, cph],\n  Jacques Savoy [cph]",
    "url": "https://github.com/pommedeterresautee/unine,\nhttps://pommedeterresautee.github.io/unine/,\nhttp://members.unine.ch/jacques.savoy/clef/",
    "bug_reports": "https://github.com/pommedeterresautee/unine/issues",
    "repository": "https://cran.r-project.org/package=unine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unine Unine Light Stemmer Implementation of \"light\" stemmers for French, German, Italian, Spanish, Portuguese, Finnish, Swedish. \n  They are based on the same work as the \"light\" stemmers found in 'SolR' <https://lucene.apache.org/solr/> or 'ElasticSearch' <https://www.elastic.co/fr/products/elasticsearch>. \n  A \"light\" stemmer consists in removing inflections only for noun and adjectives. \n  Indexing verbs for these languages is not of primary importance compared to nouns and adjectives. \n  The stemming procedure for French is described in (Savoy, 1999) <doi:10.1002/(SICI)1097-4571(1999)50:10%3C944::AID-ASI9%3E3.3.CO;2-H>.  "
  },
  {
    "id": 22640,
    "package_name": "unitedR",
    "title": "Assessment and Evaluation of Formations in United",
    "description": "United is a software tool which can be downloaded at the following\n    website <http://www.schroepl.net/pbm/software/united/>. In general, it is\n    a virtual manager game for football teams. This package contains helpful\n    functions for determining an optimal formation for a virtual match in\n    United. E.g. knowing that the opponent has a strong defensive it is\n    advisable to beat him in the midfield. Furthermore, this package contains\n    functions for computing the optimal usage of hardness in a game.",
    "version": "0.4",
    "maintainer": "David Schindler <dv.schindler@gmail.com>",
    "author": "David Schindler [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=unitedR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unitedR Assessment and Evaluation of Formations in United United is a software tool which can be downloaded at the following\n    website <http://www.schroepl.net/pbm/software/united/>. In general, it is\n    a virtual manager game for football teams. This package contains helpful\n    functions for determining an optimal formation for a virtual match in\n    United. E.g. knowing that the opponent has a strong defensive it is\n    advisable to beat him in the midfield. Furthermore, this package contains\n    functions for computing the optimal usage of hardness in a game.  "
  },
  {
    "id": 22648,
    "package_name": "universals",
    "title": "S3 Generics for Bayesian Analyses",
    "description": "Provides S3 generic methods and some default implementations\n    for Bayesian analyses that generate Markov Chain Monte Carlo (MCMC)\n    samples.  The purpose of 'universals' is to reduce package\n    dependencies and conflicts.  The 'nlist' package implements many of\n    the methods for its 'nlist' class.",
    "version": "0.0.5",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "author": "Joe Thorley [aut, cre] (ORCID: <https://orcid.org/0000-0002-7683-4592>),\n  Kirill M\u00fcller [ctb] (ORCID: <https://orcid.org/0000-0002-1416-3412>),\n  Poisson Consulting [cph, fnd]",
    "url": "https://poissonconsulting.github.io/universals/,\nhttps://github.com/poissonconsulting/universals",
    "bug_reports": "https://github.com/poissonconsulting/universals/issues",
    "repository": "https://cran.r-project.org/package=universals",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "universals S3 Generics for Bayesian Analyses Provides S3 generic methods and some default implementations\n    for Bayesian analyses that generate Markov Chain Monte Carlo (MCMC)\n    samples.  The purpose of 'universals' is to reduce package\n    dependencies and conflicts.  The 'nlist' package implements many of\n    the methods for its 'nlist' class.  "
  },
  {
    "id": 22724,
    "package_name": "vaccinationimpact",
    "title": "Impact Study of Vaccination Campaigns",
    "description": "Tools to estimate the impact of vaccination campaigns at population level (number of events averted, number of avertable events, number needed to vaccinate). Inspired by the methodology proposed by Foppa et al. (2015) <doi:10.1016/j.vaccine.2015.02.042> and Machado et al. (2019) <doi:10.2807/1560-7917.ES.2019.24.45.1900268> for influenza vaccination impact.",
    "version": "0.1.0",
    "maintainer": "Yohann Mansiaux <y.mansiaux@epiconcept.fr>",
    "author": "Yohann Mansiaux [aut, cre],\n  Alexandre Blake [aut],\n  James Humphreys [aut],\n  Baltazar Nunes [aut]",
    "url": "https://github.com/Epiconcept-Paris/vaccinationimpact/,\nhttps://epiconcept-paris.github.io/vaccinationimpact/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vaccinationimpact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vaccinationimpact Impact Study of Vaccination Campaigns Tools to estimate the impact of vaccination campaigns at population level (number of events averted, number of avertable events, number needed to vaccinate). Inspired by the methodology proposed by Foppa et al. (2015) <doi:10.1016/j.vaccine.2015.02.042> and Machado et al. (2019) <doi:10.2807/1560-7917.ES.2019.24.45.1900268> for influenza vaccination impact.  "
  },
  {
    "id": 22747,
    "package_name": "valorate",
    "title": "Velocity and Accuracy of the LOg-RAnk TEst",
    "description": "The algorithm implemented in this package was\n    designed to quickly estimates the distribution of the \n    log-rank especially for heavy unbalanced groups. VALORATE \n    estimates the null distribution and the p-value of the \n    log-rank test based on a recent formulation. For a given \n    number of alterations that define the size of survival \n    groups, the estimation involves a weighted sum of \n    distributions that are conditional on a co-occurrence term \n    where mutations and events are both present. The estimation \n    of conditional distributions is quite fast allowing the \n    analysis of large datasets in few minutes \n    <https://bioinformatics.mx/index.php/bioinfo-tools/>.",
    "version": "1.0-5",
    "maintainer": "Victor Trevino <vtrevino@itesm.mx>",
    "author": "Victor Trevino [aut, cre]",
    "url": "https://bioinformatics.mx/index.php/bioinfo-tools/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=valorate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "valorate Velocity and Accuracy of the LOg-RAnk TEst The algorithm implemented in this package was\n    designed to quickly estimates the distribution of the \n    log-rank especially for heavy unbalanced groups. VALORATE \n    estimates the null distribution and the p-value of the \n    log-rank test based on a recent formulation. For a given \n    number of alterations that define the size of survival \n    groups, the estimation involves a weighted sum of \n    distributions that are conditional on a co-occurrence term \n    where mutations and events are both present. The estimation \n    of conditional distributions is quite fast allowing the \n    analysis of large datasets in few minutes \n    <https://bioinformatics.mx/index.php/bioinfo-tools/>.  "
  },
  {
    "id": 22748,
    "package_name": "valottery",
    "title": "Results from the Virginia Lottery Draw Games",
    "description": "Historical results for the state of Virginia lottery draw games. Data were downloaded from https://www.valottery.com/. ",
    "version": "0.0.1",
    "maintainer": "Clay Ford <joe.clayton.ford@gmail.com>",
    "author": "Clay Ford [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=valottery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "valottery Results from the Virginia Lottery Draw Games Historical results for the state of Virginia lottery draw games. Data were downloaded from https://www.valottery.com/.   "
  },
  {
    "id": 22828,
    "package_name": "verdata",
    "title": "Analyze Data from the Truth Commission in Colombia",
    "description": "Facilitates use and analysis of data about the armed conflict\n    in Colombia resulting from the joint project between La Jurisdicci\u00f3n\n    Especial para la Paz (JEP), La Comisi\u00f3n para el Esclarecimiento de la\n    Verdad, la Convivencia y la No repetici\u00f3n (CEV), and the Human Rights Data\n    Analysis Group (HRDAG). The data are 100 replicates from a multiple\n    imputation through chained equations as described in Van Buuren and\n    Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>. With the replicates\n    the user can examine four human rights violations that occurred in the\n    Colombian conflict accounting for the impact of missing fields and fully\n    missing observations.",
    "version": "1.0.1",
    "maintainer": "Maria Gargiulo <mariag@hrdag.org>",
    "author": "Maria Gargiulo [aut, cre],\n  Mar\u00eda Juliana Dur\u00e1n [aut],\n  Paula Andrea Amado [aut],\n  Patrick Ball [rev]",
    "url": "https://github.com/HRDAG/verdata",
    "bug_reports": "https://github.com/HRDAG/verdata/issues",
    "repository": "https://cran.r-project.org/package=verdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "verdata Analyze Data from the Truth Commission in Colombia Facilitates use and analysis of data about the armed conflict\n    in Colombia resulting from the joint project between La Jurisdicci\u00f3n\n    Especial para la Paz (JEP), La Comisi\u00f3n para el Esclarecimiento de la\n    Verdad, la Convivencia y la No repetici\u00f3n (CEV), and the Human Rights Data\n    Analysis Group (HRDAG). The data are 100 replicates from a multiple\n    imputation through chained equations as described in Van Buuren and\n    Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>. With the replicates\n    the user can examine four human rights violations that occurred in the\n    Colombian conflict accounting for the impact of missing fields and fully\n    missing observations.  "
  },
  {
    "id": 22885,
    "package_name": "vistla",
    "title": "Detecting Influence Paths with Information Theory",
    "description": "Traces information spread through interactions between features, utilising information theory measures and a higher-order generalisation of the concept of widest paths in graphs.\n  In particular, 'vistla' can be used to better understand the results of high-throughput biomedical experiments, by organising the effects of the investigated intervention in a tree-like hierarchy from direct to indirect ones, following the plausible information relay circuits.\n  Due to its higher-order nature, 'vistla' can handle multi-modality and assign multiple roles to a single feature.",
    "version": "2.1.2",
    "maintainer": "Miron B. Kursa <m@mbq.me>",
    "author": "Miron B. Kursa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7672-648X>)",
    "url": "https://gitlab.com/mbq/vistla",
    "bug_reports": "https://gitlab.com/mbq/vistla/-/issues",
    "repository": "https://cran.r-project.org/package=vistla",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vistla Detecting Influence Paths with Information Theory Traces information spread through interactions between features, utilising information theory measures and a higher-order generalisation of the concept of widest paths in graphs.\n  In particular, 'vistla' can be used to better understand the results of high-throughput biomedical experiments, by organising the effects of the investigated intervention in a tree-like hierarchy from direct to indirect ones, following the plausible information relay circuits.\n  Due to its higher-order nature, 'vistla' can handle multi-modality and assign multiple roles to a single feature.  "
  },
  {
    "id": 23049,
    "package_name": "wehoop",
    "title": "Access Women's Basketball Play by Play Data",
    "description": "A utility for working with women's basketball data. A scraping and aggregating interface for the WNBA Stats API <https://stats.wnba.com/> and ESPN's <https://www.espn.com> women's college basketball and WNBA statistics. It provides users with the capability to access the game play-by-plays, box scores, standings and results to analyze the data for themselves.",
    "version": "2.1.0",
    "maintainer": "Saiem Gilani <saiem.gilani@gmail.com>",
    "author": "Saiem Gilani [aut, cre],\n  Geoffery Hutchinson [aut]",
    "url": "https://wehoop.sportsdataverse.org,\nhttps://github.com/sportsdataverse/wehoop",
    "bug_reports": "https://github.com/sportsdataverse/wehoop/issues",
    "repository": "https://cran.r-project.org/package=wehoop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wehoop Access Women's Basketball Play by Play Data A utility for working with women's basketball data. A scraping and aggregating interface for the WNBA Stats API <https://stats.wnba.com/> and ESPN's <https://www.espn.com> women's college basketball and WNBA statistics. It provides users with the capability to access the game play-by-plays, box scores, standings and results to analyze the data for themselves.  "
  },
  {
    "id": 23054,
    "package_name": "weightedRank",
    "title": "Sensitivity Analysis Using Weighted Rank Statistics",
    "description": "Performs a sensitivity analysis using weighted rank tests in observational studies with I blocks of size J; see Rosenbaum (2024) <doi:10.1080/01621459.2023.2221402>.  The package can perform adaptive inference in block designs; see Rosenbaum (2012) <doi:10.1093/biomet/ass032>.  The main functions are wgtRank(), wgtRankCI() and wgtRanktt().",
    "version": "0.5.1",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=weightedRank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "weightedRank Sensitivity Analysis Using Weighted Rank Statistics Performs a sensitivity analysis using weighted rank tests in observational studies with I blocks of size J; see Rosenbaum (2024) <doi:10.1080/01621459.2023.2221402>.  The package can perform adaptive inference in block designs; see Rosenbaum (2012) <doi:10.1093/biomet/ass032>.  The main functions are wgtRank(), wgtRankCI() and wgtRanktt().  "
  },
  {
    "id": 23061,
    "package_name": "welo",
    "title": "Weighted and Standard Elo Rates",
    "description": "Estimates the standard and weighted Elo (WElo, Angelini et al., 2022 <doi:10.1016/j.ejor.2021.04.011>) rates. The current version provides Elo and WElo rates for tennis, according to different systems of weights (games or sets) and scale factors (constant, proportional to the number of matches, with more weight on Grand Slam matches or matches played on a specific surface). Moreover, the package gives the possibility of estimating the (bootstrap) standard errors for the rates. Finally, the package includes betting functions that automatically select the matches on which place a bet.",
    "version": "0.1.4",
    "maintainer": "Vincenzo Candila <vcandila@unisa.it>",
    "author": "Vincenzo Candila [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=welo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "welo Weighted and Standard Elo Rates Estimates the standard and weighted Elo (WElo, Angelini et al., 2022 <doi:10.1016/j.ejor.2021.04.011>) rates. The current version provides Elo and WElo rates for tennis, according to different systems of weights (games or sets) and scale factors (constant, proportional to the number of matches, with more weight on Grand Slam matches or matches played on a specific surface). Moreover, the package gives the possibility of estimating the (bootstrap) standard errors for the rates. Finally, the package includes betting functions that automatically select the matches on which place a bet.  "
  },
  {
    "id": 23086,
    "package_name": "whitechapelR",
    "title": "Advanced Policing Techniques for the Board Game \"Letters from\nWhitechapel\"",
    "description": "Provides a set of functions to make tracking the hidden movements \n  of the 'Jack' player easier. By tracking every possible path Jack might have \n  traveled from the point of the initial murder including special movement such \n  as through alleyways and via carriages, the police can more accurately narrow \n  the field of their search. Additionally, by tracking all possible hideouts from \n  round to round, rounds 3 and 4 should have a vastly reduced field of search.",
    "version": "0.3.0",
    "maintainer": "Mark Ewing <b.mark@ewingsonline.com>",
    "author": "Mark Ewing [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=whitechapelR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "whitechapelR Advanced Policing Techniques for the Board Game \"Letters from\nWhitechapel\" Provides a set of functions to make tracking the hidden movements \n  of the 'Jack' player easier. By tracking every possible path Jack might have \n  traveled from the point of the initial murder including special movement such \n  as through alleyways and via carriages, the police can more accurately narrow \n  the field of their search. Additionally, by tracking all possible hideouts from \n  round to round, rounds 3 and 4 should have a vastly reduced field of search.  "
  },
  {
    "id": 23107,
    "package_name": "wildpoker",
    "title": "Best Hand Analysis for Poker Variants Including Wildcards",
    "description": "Provides insight into how the best hand for a poker game changes based on the game dealt, players who stay in until the showdown and wildcards added to the base game.  At this time the package does not support player tactics, so draw poker variants are not included.",
    "version": "1.1",
    "maintainer": "Bradley Shanrock-Solberg <greblosb@gmail.com>",
    "author": "Bradley Shanrock-Solberg",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wildpoker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wildpoker Best Hand Analysis for Poker Variants Including Wildcards Provides insight into how the best hand for a poker game changes based on the game dealt, players who stay in until the showdown and wildcards added to the base game.  At this time the package does not support player tactics, so draw poker variants are not included.  "
  },
  {
    "id": 23138,
    "package_name": "wordPuzzleR",
    "title": "Word Puzzle Game",
    "description": "The word puzzle game requires you to find out the letters in a word within a limited number of guesses. In each round, if your guess hit any letters in the word, they reveal themselves. If all letters are revealed before your guesses run out, you win this game; otherwise you fail. You may run multiple games to guess different words.",
    "version": "0.1.1",
    "maintainer": "Xiurui Zhu <zxr6@163.com>",
    "author": "Xiurui Zhu [aut, cre],\n  @olivory [ctb] (Add `URL` and `BugReports` fields to `DESCRIPTION`.)",
    "url": "https://github.com/zhuxr11/wordPuzzleR",
    "bug_reports": "https://github.com/zhuxr11/wordPuzzleR/issues",
    "repository": "https://cran.r-project.org/package=wordPuzzleR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordPuzzleR Word Puzzle Game The word puzzle game requires you to find out the letters in a word within a limited number of guesses. In each round, if your guess hit any letters in the word, they reveal themselves. If all letters are revealed before your guesses run out, you win this game; otherwise you fail. You may run multiple games to guess different words.  "
  },
  {
    "id": 23141,
    "package_name": "wordler",
    "title": "The 'WORDLE' Game",
    "description": "The 'Wordle' game. Players have six attempts to guess a \n    five-letter word. After each guess, the player is informed which \n    letters in their guess are either: anywhere in the word; in the right \n    position in the word. This can be used to inform the next guess. Can be \n    played interactively in the console, or programmatically. Based on Josh \n    Wardle's game <https://www.powerlanguage.co.uk/wordle/>.",
    "version": "0.3.1",
    "maintainer": "David Smith <david.alex.smith@gmail.com>",
    "author": "David Smith [aut, cre],\n  Gethin Davies [ctb]",
    "url": "https://github.com/DavidASmith/wordler",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordler The 'WORDLE' Game The 'Wordle' game. Players have six attempts to guess a \n    five-letter word. After each guess, the player is informed which \n    letters in their guess are either: anywhere in the word; in the right \n    position in the word. This can be used to inform the next guess. Can be \n    played interactively in the console, or programmatically. Based on Josh \n    Wardle's game <https://www.powerlanguage.co.uk/wordle/>.  "
  },
  {
    "id": 23147,
    "package_name": "words",
    "title": "List of English Words from the Scrabble Dictionary",
    "description": "List of english scrabble words as listed in the OTCWL2014\n    <https://www.scrabbleplayers.org/w/Official_Tournament_and_Club_Word_List_2014_Edition>.\n    Words are collated from the 'Word Game Dictionary' <https://www.wordgamedictionary.com/word-lists/>.",
    "version": "1.0.1",
    "maintainer": "Conor Neilson <condwanaland@gmail.com>",
    "author": "Conor Neilson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=words",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "words List of English Words from the Scrabble Dictionary List of english scrabble words as listed in the OTCWL2014\n    <https://www.scrabbleplayers.org/w/Official_Tournament_and_Club_Word_List_2014_Edition>.\n    Words are collated from the 'Word Game Dictionary' <https://www.wordgamedictionary.com/word-lists/>.  "
  },
  {
    "id": 23153,
    "package_name": "worldfootballR",
    "title": "Extract and Clean World Football (Soccer) Data",
    "description": "Allow users to obtain clean and tidy\n    football (soccer) game, team and player data. Data is collected from a\n    number of popular sites, including 'FBref',\n    transfer and valuations data from\n    'Transfermarkt'<https://www.transfermarkt.com/> and shooting location\n    and other match stats data from 'Understat'<https://understat.com/>\n    and 'fotmob'<https://www.fotmob.com/>. It gives users the\n    ability to access data more efficiently, rather than having to export\n    data tables to files before being able to complete their analysis.",
    "version": "0.6.2",
    "maintainer": "Jason Zivkovic <jaseziv83@gmail.com>",
    "author": "Jason Zivkovic [aut, cre, cph],\n  Tony ElHabr [ctb],\n  Tan Ho [ctb],\n  Samuel H [ctb]",
    "url": "https://github.com/JaseZiv/worldfootballR",
    "bug_reports": "https://github.com/JaseZiv/worldfootballR/issues",
    "repository": "https://cran.r-project.org/package=worldfootballR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "worldfootballR Extract and Clean World Football (Soccer) Data Allow users to obtain clean and tidy\n    football (soccer) game, team and player data. Data is collected from a\n    number of popular sites, including 'FBref',\n    transfer and valuations data from\n    'Transfermarkt'<https://www.transfermarkt.com/> and shooting location\n    and other match stats data from 'Understat'<https://understat.com/>\n    and 'fotmob'<https://www.fotmob.com/>. It gives users the\n    ability to access data more efficiently, rather than having to export\n    data tables to files before being able to complete their analysis.  "
  },
  {
    "id": 23180,
    "package_name": "writeAlizer",
    "title": "Generate Predicted Writing Quality Scores",
    "description": "Imports variables from 'ReaderBench' (Dascalu et al.,\n    2018)<doi:10.1007/978-3-319-66610-5_48>, 'Coh-Metrix' (McNamara et\n    al., 2014)<doi:10.1017/CBO9780511894664>, and/or 'GAMET' (Crossley et\n    al., 2019) <doi:10.17239/jowr-2019.11.02.01> output files; downloads\n    predictive scoring models described in Mercer & Cannon\n    (2022)<doi:10.31244/jero.2022.01.03> and Mercer et\n    al.(2021)<doi:10.1177/0829573520987753>; and generates predicted\n    writing quality and curriculum-based measurement (McMaster & Espin,\n    2007)<doi:10.1177/00224669070410020301> scores.",
    "version": "1.7.2",
    "maintainer": "Sterett H. Mercer <sterett.mercer@ubc.ca>",
    "author": "Sterett H. Mercer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7940-4221>)",
    "url": "https://github.com/shmercer/writeAlizer/,\nhttps://shmercer.github.io/writeAlizer/",
    "bug_reports": "https://github.com/shmercer/writeAlizer/issues",
    "repository": "https://cran.r-project.org/package=writeAlizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "writeAlizer Generate Predicted Writing Quality Scores Imports variables from 'ReaderBench' (Dascalu et al.,\n    2018)<doi:10.1007/978-3-319-66610-5_48>, 'Coh-Metrix' (McNamara et\n    al., 2014)<doi:10.1017/CBO9780511894664>, and/or 'GAMET' (Crossley et\n    al., 2019) <doi:10.17239/jowr-2019.11.02.01> output files; downloads\n    predictive scoring models described in Mercer & Cannon\n    (2022)<doi:10.31244/jero.2022.01.03> and Mercer et\n    al.(2021)<doi:10.1177/0829573520987753>; and generates predicted\n    writing quality and curriculum-based measurement (McMaster & Espin,\n    2007)<doi:10.1177/00224669070410020301> scores.  "
  },
  {
    "id": 23203,
    "package_name": "xSub",
    "title": "Cross-National Data on Sub-National Violence",
    "description": "Tools to download and merge data files on sub-national conflict, violence and protests from <http://www.x-sub.org>.",
    "version": "3.0.2",
    "maintainer": "Yuri Zhukov <zhukov@umich.edu>",
    "author": "Yuri Zhukov [aut, cre],\n  Christian Davenport [aut],\n  Nadiya Kostyuk [aut]",
    "url": "https://github.com/zhukovyuri/xSub",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xSub",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xSub Cross-National Data on Sub-National Violence Tools to download and merge data files on sub-national conflict, violence and protests from <http://www.x-sub.org>.  "
  },
  {
    "id": 23318,
    "package_name": "zic",
    "title": "Bayesian Inference for Zero-Inflated Count Models",
    "description": "Provides MCMC algorithms for the analysis of\n        zero-inflated count models. The case of stochastic search\n        variable selection (SVS) is also considered.  All MCMC samplers\n        are coded in C++ for improved efficiency. A data set\n        considering the demand for health care is provided.",
    "version": "0.9.1",
    "maintainer": "Markus Jochmann <markus.jochmann@ncl.ac.uk>",
    "author": "Markus Jochmann <markus.jochmann@ncl.ac.uk>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=zic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zic Bayesian Inference for Zero-Inflated Count Models Provides MCMC algorithms for the analysis of\n        zero-inflated count models. The case of stochastic search\n        variable selection (SVS) is also considered.  All MCMC samplers\n        are coded in C++ for improved efficiency. A data set\n        considering the demand for health care is provided.  "
  },
  {
    "id": 23319,
    "package_name": "zigg",
    "title": "Lightweight Interfaces to the 'Ziggurat' Pseudo Random Number\nGenerator",
    "description": "The 'Ziggurat' pseudo-random number generator (or PRNG),\n introduced by Marsaglia and Tsang (2000, <doi:10.18637/jss.v005.i08>) and\n further improved by Leong et al (2005, <doi:10.18637/jss.v012.i07>), offers\n a lightweight and very fast PRNG for the normal, exponential, and uniform\n distributions. It is provided here in a small zero-dependency package. It can\n be used from R as well as from 'C/C++' code in other packages as is demonstrated\n by four included sample packages using four distinct methods to use the PRNG\n presented here in client package. The implementation is influenced by our\n package 'RcppZiggurat' which offers a comparison among multiple alternative\n implementations but presented here in a lighter-weight implementation that is\n easier to use by other packages. The PRNGs provided are generally faster than\n the ones in base R: on our machine, the relative gains for normal, exponential\n and uniform are on the order of 7.4, 5.2 and 4.7 times faster than base R.\n However, these generators are of potentially lesser quality and shorter period\n so if in doubt use of the base R functions remains the general recommendation.",
    "version": "0.0.2",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>)",
    "url": "https://github.com/eddelbuettel/zigg",
    "bug_reports": "https://github.com/eddelbuettel/zigg/issues",
    "repository": "https://cran.r-project.org/package=zigg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zigg Lightweight Interfaces to the 'Ziggurat' Pseudo Random Number\nGenerator The 'Ziggurat' pseudo-random number generator (or PRNG),\n introduced by Marsaglia and Tsang (2000, <doi:10.18637/jss.v005.i08>) and\n further improved by Leong et al (2005, <doi:10.18637/jss.v012.i07>), offers\n a lightweight and very fast PRNG for the normal, exponential, and uniform\n distributions. It is provided here in a small zero-dependency package. It can\n be used from R as well as from 'C/C++' code in other packages as is demonstrated\n by four included sample packages using four distinct methods to use the PRNG\n presented here in client package. The implementation is influenced by our\n package 'RcppZiggurat' which offers a comparison among multiple alternative\n implementations but presented here in a lighter-weight implementation that is\n easier to use by other packages. The PRNGs provided are generally faster than\n the ones in base R: on our machine, the relative gains for normal, exponential\n and uniform are on the order of 7.4, 5.2 and 4.7 times faster than base R.\n However, these generators are of potentially lesser quality and shorter period\n so if in doubt use of the base R functions remains the general recommendation.  "
  },
  {
    "id": 23324,
    "package_name": "zipsae",
    "title": "Small Area Estimation with Zero-Inflated Model",
    "description": "This function produces empirical best linier unbiased predictions (EBLUPs) for Zero-Inflated data and its Relative Standard Error. Small Area Estimation with Zero-Inflated Model (SAE-ZIP) is a model developed for Zero-Inflated data that can lead us to overdispersion situation. To handle this kind of situation, this model is created. The model in this package is based on Small Area Estimation with Zero-Inflated Poisson model proposed by Dian Christien Arisona (2018)<https://repository.ipb.ac.id/handle/123456789/92308>. For the data sample itself, we use combination method between Roberto Benavent and Domingo Morales (2015)<doi:10.1016/j.csda.2015.07.013> and Sabine Krieg, Harm Jan Boonstra and Marc Smeets (2016)<doi:10.1515/jos-2016-0051>.",
    "version": "1.0.2",
    "maintainer": "Fadheel Wisnu Utomo <221709671@stis.ac.id>",
    "author": "Fadheel Wisnu Utomo [aut, trl, cre],\n  Ika Yuni Wulansari [aut, ths]",
    "url": "https://github.com/dheel/zipsae",
    "bug_reports": "https://github.com/dheel/zipsae/issues",
    "repository": "https://cran.r-project.org/package=zipsae",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zipsae Small Area Estimation with Zero-Inflated Model This function produces empirical best linier unbiased predictions (EBLUPs) for Zero-Inflated data and its Relative Standard Error. Small Area Estimation with Zero-Inflated Model (SAE-ZIP) is a model developed for Zero-Inflated data that can lead us to overdispersion situation. To handle this kind of situation, this model is created. The model in this package is based on Small Area Estimation with Zero-Inflated Poisson model proposed by Dian Christien Arisona (2018)<https://repository.ipb.ac.id/handle/123456789/92308>. For the data sample itself, we use combination method between Roberto Benavent and Domingo Morales (2015)<doi:10.1016/j.csda.2015.07.013> and Sabine Krieg, Harm Jan Boonstra and Marc Smeets (2016)<doi:10.1515/jos-2016-0051>.  "
  },
  {
    "id": 23329,
    "package_name": "zoib",
    "title": "Bayesian Inference for Beta Regression and Zero-or-One Inflated\nBeta Regression",
    "description": "Fits beta regression and zero-or-one inflated beta regression and obtains Bayesian Inference of the model via the Markov Chain Monte Carlo approach implemented in JAGS.",
    "version": "1.6.1",
    "maintainer": "Fang Liu <fang.liu.131@nd.edu>",
    "author": "Fang Liu [aut, cre],\n  Yunchuan Kong [ctb]",
    "url": "https://www.r-project.org, https://www3.nd.edu/~fliu2/#tools",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=zoib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zoib Bayesian Inference for Beta Regression and Zero-or-One Inflated\nBeta Regression Fits beta regression and zero-or-one inflated beta regression and obtains Bayesian Inference of the model via the Markov Chain Monte Carlo approach implemented in JAGS.  "
  },
  {
    "id": 23330,
    "package_name": "zoid",
    "title": "Bayesian Zero-and-One Inflated Dirichlet Regression Modelling",
    "description": "Fits Dirichlet regression and zero-and-one inflated Dirichlet regression with Bayesian methods implemented in Stan. These models are sometimes referred to as trinomial mixture models; covariates and overdispersion can optionally be included.",
    "version": "1.3.1",
    "maintainer": "Eric J. Ward <eric.ward@noaa.gov>",
    "author": "Eric J. Ward [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4359-0296>),\n  Alexander J. Jensen [aut] (ORCID:\n    <https://orcid.org/0000-0002-2911-8884>),\n  Ryan P. Kelly [aut] (ORCID: <https://orcid.org/0000-0001-5037-2441>),\n  Andrew O. Shelton [aut] (ORCID:\n    <https://orcid.org/0000-0002-8045-6141>),\n  William H. Satterthwaite [aut] (ORCID:\n    <https://orcid.org/0000-0002-0436-7390>),\n  Eric C. Anderson [aut] (ORCID: <https://orcid.org/0000-0003-1326-0840>)",
    "url": "https://noaa-nwfsc.github.io/zoid/",
    "bug_reports": "https://github.com/noaa-nwfsc/zoid/issues",
    "repository": "https://cran.r-project.org/package=zoid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zoid Bayesian Zero-and-One Inflated Dirichlet Regression Modelling Fits Dirichlet regression and zero-and-one inflated Dirichlet regression with Bayesian methods implemented in Stan. These models are sometimes referred to as trinomial mixture models; covariates and overdispersion can optionally be included.  "
  },
  {
    "id": 23343,
    "package_name": "ztype",
    "title": "Run a Ztype Game Loaded with R Functions",
    "description": "How fast can you type R functions on your keyboard? Find out by running a 'zty.pe' game: export R functions as instructions to type to destroy opponents vessels.",
    "version": "0.1.0",
    "maintainer": "Vincent Guyader <vincent@thinkr.fr>",
    "author": "Vincent Guyader",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ztype",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ztype Run a Ztype Game Loaded with R Functions How fast can you type R functions on your keyboard? Find out by running a 'zty.pe' game: export R functions as instructions to type to destroy opponents vessels.  "
  }
]