[
  {
    "id": 794,
    "package_name": "marginaleffects",
    "title": "Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis\nTests",
    "description": "Compute and plot predictions, slopes, marginal means, and\ncomparisons (contrasts, risk ratios, odds, etc.) for over 100\nclasses of statistical and machine learning models in R.\nConduct linear and non-linear hypothesis tests, or equivalence\ntests. Calculate uncertainty estimates using the delta method,\nbootstrapping, or simulation-based inference. Details can be\nfound in Arel-Bundock, Greifer, and Heiss (2024)\n<doi:10.18637/jss.v111.i09>.",
    "version": "0.31.0.4",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>),\nNoah Greifer [ctb] (ORCID: <https://orcid.org/0000-0003-3067-7154>),\nEtienne Bacher [ctb] (ORCID: <https://orcid.org/0000-0002-9271-5075>),\nGrant McDermott [ctb] (ORCID: <https://orcid.org/0000-0001-7883-8573>),\nAndrew Heiss [ctb] (ORCID: <https://orcid.org/0000-0002-3948-3914>)",
    "url": "https://marginaleffects.com/",
    "bug_reports": "https://github.com/vincentarelbundock/marginaleffects/issues",
    "repository": "",
    "exports": [
      [
        "autodiff"
      ],
      [
        "avg_comparisons"
      ],
      [
        "avg_predictions"
      ],
      [
        "avg_slopes"
      ],
      [
        "comparisons"
      ],
      [
        "components"
      ],
      [
        "datagrid"
      ],
      [
        "expect_comparisons"
      ],
      [
        "expect_hypotheses"
      ],
      [
        "expect_margins"
      ],
      [
        "expect_predictions"
      ],
      [
        "expect_slopes"
      ],
      [
        "get_coef"
      ],
      [
        "get_dataset"
      ],
      [
        "get_draws"
      ],
      [
        "get_group_names"
      ],
      [
        "get_model_matrix"
      ],
      [
        "get_predict"
      ],
      [
        "get_vcov"
      ],
      [
        "glance"
      ],
      [
        "hypotheses"
      ],
      [
        "inferences"
      ],
      [
        "plot_comparisons"
      ],
      [
        "plot_predictions"
      ],
      [
        "plot_slopes"
      ],
      [
        "posterior_draws"
      ],
      [
        "predictions"
      ],
      [
        "prune"
      ],
      [
        "refit"
      ],
      [
        "set_coef"
      ],
      [
        "slopes"
      ],
      [
        "tidy"
      ]
    ],
    "topics": [],
    "score": 15.0743,
    "stars": 573,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "marginaleffects Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis\nTests Compute and plot predictions, slopes, marginal means, and\ncomparisons (contrasts, risk ratios, odds, etc.) for over 100\nclasses of statistical and machine learning models in R.\nConduct linear and non-linear hypothesis tests, or equivalence\ntests. Calculate uncertainty estimates using the delta method,\nbootstrapping, or simulation-based inference. Details can be\nfound in Arel-Bundock, Greifer, and Heiss (2024)\n<doi:10.18637/jss.v111.i09>. autodiff avg_comparisons avg_predictions avg_slopes comparisons components datagrid expect_comparisons expect_hypotheses expect_margins expect_predictions expect_slopes get_coef get_dataset get_draws get_group_names get_model_matrix get_predict get_vcov glance hypotheses inferences plot_comparisons plot_predictions plot_slopes posterior_draws predictions prune refit set_coef slopes tidy "
  },
  {
    "id": 1356,
    "package_name": "timetk",
    "title": "A Tool Kit for Working with Time Series",
    "description": "Easy visualization, wrangling, and feature engineering of\ntime series data for forecasting and machine learning\nprediction. Consolidates and extends time series functionality\nfrom packages including 'dplyr', 'stats', 'xts', 'forecast',\n'slider', 'padr', 'recipes', and 'rsample'.",
    "version": "2.9.1.9000",
    "maintainer": "Matt Dancho <mdancho@business-science.io>",
    "author": "Matt Dancho [aut, cre],\nDavis Vaughan [aut]",
    "url": "https://github.com/business-science/timetk,\nhttps://business-science.github.io/timetk/",
    "bug_reports": "https://github.com/business-science/timetk/issues",
    "repository": "",
    "exports": [
      [
        ":="
      ],
      [
        ".data"
      ],
      [
        "%-time%"
      ],
      [
        "%+time%"
      ],
      [
        "%||%"
      ],
      [
        "add_time"
      ],
      [
        "anomalize"
      ],
      [
        "as_label"
      ],
      [
        "as_name"
      ],
      [
        "auto_lambda"
      ],
      [
        "between_time"
      ],
      [
        "box_cox_inv_vec"
      ],
      [
        "box_cox_vec"
      ],
      [
        "condense_period"
      ],
      [
        "diff_inv_vec"
      ],
      [
        "diff_vec"
      ],
      [
        "enquo"
      ],
      [
        "enquos"
      ],
      [
        "expr"
      ],
      [
        "filter_by_time"
      ],
      [
        "filter_period"
      ],
      [
        "fourier_vec"
      ],
      [
        "future_frame"
      ],
      [
        "get_tk_time_scale_template"
      ],
      [
        "has_timetk_idx"
      ],
      [
        "is_date_class"
      ],
      [
        "lag_vec"
      ],
      [
        "lead_vec"
      ],
      [
        "log_interval_inv_vec"
      ],
      [
        "log_interval_vec"
      ],
      [
        "mutate_by_time"
      ],
      [
        "normalize_inv_vec"
      ],
      [
        "normalize_vec"
      ],
      [
        "pad_by_time"
      ],
      [
        "parse_date2"
      ],
      [
        "parse_datetime2"
      ],
      [
        "plot_acf_diagnostics"
      ],
      [
        "plot_anomalies"
      ],
      [
        "plot_anomalies_cleaned"
      ],
      [
        "plot_anomalies_decomp"
      ],
      [
        "plot_anomaly_diagnostics"
      ],
      [
        "plot_seasonal_diagnostics"
      ],
      [
        "plot_stl_diagnostics"
      ],
      [
        "plot_time_series"
      ],
      [
        "plot_time_series_boxplot"
      ],
      [
        "plot_time_series_cv_plan"
      ],
      [
        "plot_time_series_regression"
      ],
      [
        "set_tk_time_scale_template"
      ],
      [
        "slice_period"
      ],
      [
        "slidify"
      ],
      [
        "slidify_vec"
      ],
      [
        "smooth_vec"
      ],
      [
        "standardize_inv_vec"
      ],
      [
        "standardize_vec"
      ],
      [
        "step_box_cox"
      ],
      [
        "step_diff"
      ],
      [
        "step_fourier"
      ],
      [
        "step_holiday_signature"
      ],
      [
        "step_log_interval"
      ],
      [
        "step_slidify"
      ],
      [
        "step_slidify_augment"
      ],
      [
        "step_smooth"
      ],
      [
        "step_timeseries_signature"
      ],
      [
        "step_ts_clean"
      ],
      [
        "step_ts_impute"
      ],
      [
        "step_ts_pad"
      ],
      [
        "subtract_time"
      ],
      [
        "summarise_by_time"
      ],
      [
        "summarize_by_time"
      ],
      [
        "sym"
      ],
      [
        "syms"
      ],
      [
        "time_series_cv"
      ],
      [
        "time_series_split"
      ],
      [
        "tk_acf_diagnostics"
      ],
      [
        "tk_anomaly_diagnostics"
      ],
      [
        "tk_augment_differences"
      ],
      [
        "tk_augment_fourier"
      ],
      [
        "tk_augment_holiday_signature"
      ],
      [
        "tk_augment_lags"
      ],
      [
        "tk_augment_leads"
      ],
      [
        "tk_augment_slidify"
      ],
      [
        "tk_augment_timeseries_signature"
      ],
      [
        "tk_get_frequency"
      ],
      [
        "tk_get_holiday_signature"
      ],
      [
        "tk_get_holidays_by_year"
      ],
      [
        "tk_get_timeseries_signature"
      ],
      [
        "tk_get_timeseries_summary"
      ],
      [
        "tk_get_timeseries_unit_frequency"
      ],
      [
        "tk_get_timeseries_variables"
      ],
      [
        "tk_get_trend"
      ],
      [
        "tk_index"
      ],
      [
        "tk_make_future_timeseries"
      ],
      [
        "tk_make_holiday_sequence"
      ],
      [
        "tk_make_timeseries"
      ],
      [
        "tk_make_weekday_sequence"
      ],
      [
        "tk_make_weekend_sequence"
      ],
      [
        "tk_seasonal_diagnostics"
      ],
      [
        "tk_stl_diagnostics"
      ],
      [
        "tk_summary_diagnostics"
      ],
      [
        "tk_tbl"
      ],
      [
        "tk_time_scale_template"
      ],
      [
        "tk_time_series_cv_plan"
      ],
      [
        "tk_ts"
      ],
      [
        "tk_ts_"
      ],
      [
        "tk_ts_.data.frame"
      ],
      [
        "tk_ts_.default"
      ],
      [
        "tk_ts_dispatch_"
      ],
      [
        "tk_tsfeatures"
      ],
      [
        "tk_xts"
      ],
      [
        "tk_xts_"
      ],
      [
        "tk_zoo"
      ],
      [
        "tk_zoo_"
      ],
      [
        "tk_zooreg"
      ],
      [
        "tk_zooreg_"
      ],
      [
        "tk_zooreg_.data.frame"
      ],
      [
        "tk_zooreg_.default"
      ],
      [
        "tk_zooreg_dispatch_"
      ],
      [
        "ts_clean_vec"
      ],
      [
        "ts_impute_vec"
      ]
    ],
    "topics": [
      [
        "coercion"
      ],
      [
        "coercion-functions"
      ],
      [
        "data-mining"
      ],
      [
        "dplyr"
      ],
      [
        "forecast"
      ],
      [
        "forecasting"
      ],
      [
        "forecasting-models"
      ],
      [
        "machine-learning"
      ],
      [
        "series-decomposition"
      ],
      [
        "series-signature"
      ],
      [
        "tibble"
      ],
      [
        "tidy"
      ],
      [
        "tidyquant"
      ],
      [
        "tidyverse"
      ],
      [
        "time"
      ],
      [
        "time-series"
      ],
      [
        "timeseries"
      ]
    ],
    "score": 14.4312,
    "stars": 632,
    "primary_category": "tidyverse",
    "source_universe": "business-science",
    "search_text": "timetk A Tool Kit for Working with Time Series Easy visualization, wrangling, and feature engineering of\ntime series data for forecasting and machine learning\nprediction. Consolidates and extends time series functionality\nfrom packages including 'dplyr', 'stats', 'xts', 'forecast',\n'slider', 'padr', 'recipes', and 'rsample'. := .data %-time% %+time% %||% add_time anomalize as_label as_name auto_lambda between_time box_cox_inv_vec box_cox_vec condense_period diff_inv_vec diff_vec enquo enquos expr filter_by_time filter_period fourier_vec future_frame get_tk_time_scale_template has_timetk_idx is_date_class lag_vec lead_vec log_interval_inv_vec log_interval_vec mutate_by_time normalize_inv_vec normalize_vec pad_by_time parse_date2 parse_datetime2 plot_acf_diagnostics plot_anomalies plot_anomalies_cleaned plot_anomalies_decomp plot_anomaly_diagnostics plot_seasonal_diagnostics plot_stl_diagnostics plot_time_series plot_time_series_boxplot plot_time_series_cv_plan plot_time_series_regression set_tk_time_scale_template slice_period slidify slidify_vec smooth_vec standardize_inv_vec standardize_vec step_box_cox step_diff step_fourier step_holiday_signature step_log_interval step_slidify step_slidify_augment step_smooth step_timeseries_signature step_ts_clean step_ts_impute step_ts_pad subtract_time summarise_by_time summarize_by_time sym syms time_series_cv time_series_split tk_acf_diagnostics tk_anomaly_diagnostics tk_augment_differences tk_augment_fourier tk_augment_holiday_signature tk_augment_lags tk_augment_leads tk_augment_slidify tk_augment_timeseries_signature tk_get_frequency tk_get_holiday_signature tk_get_holidays_by_year tk_get_timeseries_signature tk_get_timeseries_summary tk_get_timeseries_unit_frequency tk_get_timeseries_variables tk_get_trend tk_index tk_make_future_timeseries tk_make_holiday_sequence tk_make_timeseries tk_make_weekday_sequence tk_make_weekend_sequence tk_seasonal_diagnostics tk_stl_diagnostics tk_summary_diagnostics tk_tbl tk_time_scale_template tk_time_series_cv_plan tk_ts tk_ts_ tk_ts_.data.frame tk_ts_.default tk_ts_dispatch_ tk_tsfeatures tk_xts tk_xts_ tk_zoo tk_zoo_ tk_zooreg tk_zooreg_ tk_zooreg_.data.frame tk_zooreg_.default tk_zooreg_dispatch_ ts_clean_vec ts_impute_vec coercion coercion-functions data-mining dplyr forecast forecasting forecasting-models machine-learning series-decomposition series-signature tibble tidy tidyquant tidyverse time time-series timeseries"
  },
  {
    "id": 1104,
    "package_name": "rgee",
    "title": "R Bindings for Calling the 'Earth Engine' API",
    "description": "Earth Engine <https://earthengine.google.com/> client\nlibrary for R. All of the 'Earth Engine' API classes, modules,\nand functions are made available. Additional functions\nimplemented include importing (exporting) of Earth Engine\nspatial objects, extraction of time series, interactive map\ndisplay, assets management interface, and metadata display. See\n<https://r-spatial.github.io/rgee/> for further details.",
    "version": "1.1.8.9000",
    "maintainer": "Matthieu Stigler <Matthieu.Stigler@gmail.com>",
    "author": "Cesar Aybar [aut] (ORCID: <https://orcid.org/0000-0003-2745-9535>),\nWu Qiusheng [ctb] (ORCID: <https://orcid.org/0000-0001-5437-4073>),\nLesly Bautista [ctb] (ORCID: <https://orcid.org/0000-0003-3523-8687>),\nRoy Yali [ctb] (ORCID: <https://orcid.org/0000-0003-4542-3755>),\nAntony Barja [ctb] (ORCID: <https://orcid.org/0000-0001-5921-2858>),\nKevin Ushey [ctb],\nJeroen Ooms [ctb] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nTim Appelhans [ctb],\nJJ Allaire [ctb],\nYuan Tang [ctb],\nSamapriya Roy [ctb],\nMariaElena Adauto [ctb] (ORCID:\n<https://orcid.org/0000-0002-2154-2429>),\nGabriel Carrasco [ctb] (ORCID: <https://orcid.org/0000-0002-6945-0419>),\nHenrik Bengtsson [ctb],\nJeffrey Hollister [rev] (Hollister reviewed the package for JOSS, see\nhttps://github.com/openjournals/joss-reviews/issues/2272/),\nGennadii Donchyts [rev] (Gena reviewed the package for JOSS, see\nhttps://github.com/openjournals/joss-reviews/issues/2272/),\nMatthieu Stigler [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6802-4290>),\nMarius Appel [rev] (Appel reviewed the package for JOSS, see\nhttps://github.com/openjournals/joss-reviews/issues/2272/)",
    "url": "https://github.com/r-spatial/rgee/,\nhttps://r-spatial.github.io/rgee/,\nhttps://github.com/google/earthengine-api/",
    "bug_reports": "https://github.com/r-spatial/rgee/issues/",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "ee"
      ],
      [
        "ee_as_rast"
      ],
      [
        "ee_as_raster"
      ],
      [
        "ee_as_sf"
      ],
      [
        "ee_as_stars"
      ],
      [
        "ee_as_thumbnail"
      ],
      [
        "ee_Authenticate"
      ],
      [
        "ee_check"
      ],
      [
        "ee_check_credentials"
      ],
      [
        "ee_check_gcloud"
      ],
      [
        "ee_check_python"
      ],
      [
        "ee_check_python_packages"
      ],
      [
        "ee_check_task_status"
      ],
      [
        "ee_clean_container"
      ],
      [
        "ee_clean_pyenv"
      ],
      [
        "ee_clean_user_credentials"
      ],
      [
        "ee_drive_to_local"
      ],
      [
        "ee_extract"
      ],
      [
        "ee_gcs_to_local"
      ],
      [
        "ee_get_assethome"
      ],
      [
        "ee_get_date_ic"
      ],
      [
        "ee_get_date_img"
      ],
      [
        "ee_get_earthengine_path"
      ],
      [
        "ee_help"
      ],
      [
        "ee_image_info"
      ],
      [
        "ee_image_to_asset"
      ],
      [
        "ee_image_to_drive"
      ],
      [
        "ee_image_to_gcs"
      ],
      [
        "ee_imagecollection_to_local"
      ],
      [
        "ee_Initialize"
      ],
      [
        "ee_install"
      ],
      [
        "ee_install_set_pyenv"
      ],
      [
        "ee_install_upgrade"
      ],
      [
        "ee_manage_asset_access"
      ],
      [
        "ee_manage_asset_size"
      ],
      [
        "ee_manage_assetlist"
      ],
      [
        "ee_manage_cancel_all_running_task"
      ],
      [
        "ee_manage_copy"
      ],
      [
        "ee_manage_create"
      ],
      [
        "ee_manage_delete"
      ],
      [
        "ee_manage_delete_properties"
      ],
      [
        "ee_manage_move"
      ],
      [
        "ee_manage_quota"
      ],
      [
        "ee_manage_set_properties"
      ],
      [
        "ee_manage_task"
      ],
      [
        "ee_monitoring"
      ],
      [
        "ee_print"
      ],
      [
        "ee_table_to_asset"
      ],
      [
        "ee_table_to_drive"
      ],
      [
        "ee_table_to_gcs"
      ],
      [
        "ee_user_info"
      ],
      [
        "ee_users"
      ],
      [
        "ee_utils_cog_metadata"
      ],
      [
        "ee_utils_create_json"
      ],
      [
        "ee_utils_create_manifest_image"
      ],
      [
        "ee_utils_create_manifest_table"
      ],
      [
        "ee_utils_dataset_display"
      ],
      [
        "ee_utils_future_value"
      ],
      [
        "ee_utils_get_crs"
      ],
      [
        "ee_utils_py_to_r"
      ],
      [
        "ee_utils_pyfunc"
      ],
      [
        "ee_utils_sak_copy"
      ],
      [
        "ee_utils_sak_validate"
      ],
      [
        "ee_utils_shp_to_zip"
      ],
      [
        "ee_version"
      ],
      [
        "eedate_to_rdate"
      ],
      [
        "gcs_to_ee_image"
      ],
      [
        "gcs_to_ee_table"
      ],
      [
        "local_to_gcs"
      ],
      [
        "Map"
      ],
      [
        "R6Map"
      ],
      [
        "raster_as_ee"
      ],
      [
        "rdate_to_eedate"
      ],
      [
        "sf_as_ee"
      ],
      [
        "stars_as_ee"
      ]
    ],
    "topics": [
      [
        "earth-engine"
      ],
      [
        "earthengine"
      ],
      [
        "google-earth-engine"
      ],
      [
        "googleearthengine"
      ],
      [
        "spatial-analysis"
      ],
      [
        "spatial-data"
      ]
    ],
    "score": 14.2851,
    "stars": 750,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "rgee R Bindings for Calling the 'Earth Engine' API Earth Engine <https://earthengine.google.com/> client\nlibrary for R. All of the 'Earth Engine' API classes, modules,\nand functions are made available. Additional functions\nimplemented include importing (exporting) of Earth Engine\nspatial objects, extraction of time series, interactive map\ndisplay, assets management interface, and metadata display. See\n<https://r-spatial.github.io/rgee/> for further details. %>% ee ee_as_rast ee_as_raster ee_as_sf ee_as_stars ee_as_thumbnail ee_Authenticate ee_check ee_check_credentials ee_check_gcloud ee_check_python ee_check_python_packages ee_check_task_status ee_clean_container ee_clean_pyenv ee_clean_user_credentials ee_drive_to_local ee_extract ee_gcs_to_local ee_get_assethome ee_get_date_ic ee_get_date_img ee_get_earthengine_path ee_help ee_image_info ee_image_to_asset ee_image_to_drive ee_image_to_gcs ee_imagecollection_to_local ee_Initialize ee_install ee_install_set_pyenv ee_install_upgrade ee_manage_asset_access ee_manage_asset_size ee_manage_assetlist ee_manage_cancel_all_running_task ee_manage_copy ee_manage_create ee_manage_delete ee_manage_delete_properties ee_manage_move ee_manage_quota ee_manage_set_properties ee_manage_task ee_monitoring ee_print ee_table_to_asset ee_table_to_drive ee_table_to_gcs ee_user_info ee_users ee_utils_cog_metadata ee_utils_create_json ee_utils_create_manifest_image ee_utils_create_manifest_table ee_utils_dataset_display ee_utils_future_value ee_utils_get_crs ee_utils_py_to_r ee_utils_pyfunc ee_utils_sak_copy ee_utils_sak_validate ee_utils_shp_to_zip ee_version eedate_to_rdate gcs_to_ee_image gcs_to_ee_table local_to_gcs Map R6Map raster_as_ee rdate_to_eedate sf_as_ee stars_as_ee earth-engine earthengine google-earth-engine googleearthengine spatial-analysis spatial-data"
  },
  {
    "id": 1348,
    "package_name": "tidyquant",
    "title": "Tidy Quantitative Financial Analysis",
    "description": "Bringing business and financial analysis to the\n'tidyverse'. The 'tidyquant' package provides a convenient\nwrapper to various 'xts', 'zoo', 'quantmod', 'TTR' and\n'PerformanceAnalytics' package functions and returns the\nobjects in the tidy 'tibble' format. The main advantage is\nbeing able to use quantitative functions with the 'tidyverse'\nfunctions including 'purrr', 'dplyr', 'tidyr', 'ggplot2',\n'lubridate', etc. See the 'tidyquant' website for more\ninformation, documentation and examples.",
    "version": "1.0.11.9000",
    "maintainer": "Matt Dancho <mdancho@business-science.io>",
    "author": "Matt Dancho [aut, cre],\nDavis Vaughan [aut]",
    "url": "https://business-science.github.io/tidyquant/,\nhttps://github.com/business-science/tidyquant",
    "bug_reports": "https://github.com/business-science/tidyquant/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "ABS"
      ],
      [
        "AS_DATE"
      ],
      [
        "AS_DATETIME"
      ],
      [
        "av_api_key"
      ],
      [
        "AVERAGE"
      ],
      [
        "AVERAGE_IFS"
      ],
      [
        "CEILING_DATE"
      ],
      [
        "CEILING_DAY"
      ],
      [
        "CEILING_MONTH"
      ],
      [
        "CEILING_QUARTER"
      ],
      [
        "CEILING_WEEK"
      ],
      [
        "CEILING_YEAR"
      ],
      [
        "CHANGE"
      ],
      [
        "CHANGE_FIRSTLAST"
      ],
      [
        "coord_x_date"
      ],
      [
        "coord_x_datetime"
      ],
      [
        "COR"
      ],
      [
        "COUNT"
      ],
      [
        "COUNT_DAYS"
      ],
      [
        "COUNT_IFS"
      ],
      [
        "COUNT_UNIQUE"
      ],
      [
        "COV"
      ],
      [
        "CREATE_IFS"
      ],
      [
        "CUMULATIVE_MAX"
      ],
      [
        "CUMULATIVE_MEAN"
      ],
      [
        "CUMULATIVE_MEDIAN"
      ],
      [
        "CUMULATIVE_MIN"
      ],
      [
        "CUMULATIVE_PRODUCT"
      ],
      [
        "CUMULATIVE_SUM"
      ],
      [
        "DATE"
      ],
      [
        "DATE_SEQUENCE"
      ],
      [
        "DATE_TO_DECIMAL"
      ],
      [
        "DATE_TO_NUMERIC"
      ],
      [
        "DATEVALUE"
      ],
      [
        "DAY"
      ],
      [
        "DMY"
      ],
      [
        "DMY_H"
      ],
      [
        "DMY_HM"
      ],
      [
        "DMY_HMS"
      ],
      [
        "DOM"
      ],
      [
        "DOW"
      ],
      [
        "EDATE"
      ],
      [
        "EOMONTH"
      ],
      [
        "EXP"
      ],
      [
        "FIRST"
      ],
      [
        "FLOOR_DATE"
      ],
      [
        "FLOOR_DAY"
      ],
      [
        "FLOOR_MONTH"
      ],
      [
        "FLOOR_QUARTER"
      ],
      [
        "FLOOR_WEEK"
      ],
      [
        "FLOOR_YEAR"
      ],
      [
        "FV"
      ],
      [
        "geom_barchart"
      ],
      [
        "geom_bbands"
      ],
      [
        "geom_bbands_"
      ],
      [
        "geom_candlestick"
      ],
      [
        "geom_ma"
      ],
      [
        "geom_ma_"
      ],
      [
        "HOLIDAY_SEQUENCE"
      ],
      [
        "HOLIDAY_TABLE"
      ],
      [
        "HOUR"
      ],
      [
        "IRR"
      ],
      [
        "LAG"
      ],
      [
        "LAST"
      ],
      [
        "LEAD"
      ],
      [
        "LOG"
      ],
      [
        "MAX"
      ],
      [
        "MAX_IFS"
      ],
      [
        "MDAY"
      ],
      [
        "MDY"
      ],
      [
        "MDY_H"
      ],
      [
        "MDY_HM"
      ],
      [
        "MDY_HMS"
      ],
      [
        "MEDIAN"
      ],
      [
        "MEDIAN_IFS"
      ],
      [
        "MIN"
      ],
      [
        "MIN_IFS"
      ],
      [
        "MINUTE"
      ],
      [
        "MONTH"
      ],
      [
        "MONTHDAY"
      ],
      [
        "NET_WORKDAYS"
      ],
      [
        "NOW"
      ],
      [
        "NPV"
      ],
      [
        "NTH"
      ],
      [
        "palette_dark"
      ],
      [
        "palette_green"
      ],
      [
        "palette_light"
      ],
      [
        "PCT_CHANGE"
      ],
      [
        "PCT_CHANGE_FIRSTLAST"
      ],
      [
        "pivot_table"
      ],
      [
        "PMT"
      ],
      [
        "PV"
      ],
      [
        "QDAY"
      ],
      [
        "quandl_api_key"
      ],
      [
        "quandl_search"
      ],
      [
        "QUARTER"
      ],
      [
        "QUARTERDAY"
      ],
      [
        "RATE"
      ],
      [
        "RETURN"
      ],
      [
        "ROUND_DATE"
      ],
      [
        "ROUND_DAY"
      ],
      [
        "ROUND_MONTH"
      ],
      [
        "ROUND_QUARTER"
      ],
      [
        "ROUND_WEEK"
      ],
      [
        "ROUND_YEAR"
      ],
      [
        "scale_color_tq"
      ],
      [
        "scale_colour_tq"
      ],
      [
        "scale_fill_tq"
      ],
      [
        "SECOND"
      ],
      [
        "SQRT"
      ],
      [
        "STDEV"
      ],
      [
        "SUM"
      ],
      [
        "SUM_IFS"
      ],
      [
        "theme_tq"
      ],
      [
        "theme_tq_dark"
      ],
      [
        "theme_tq_green"
      ],
      [
        "tidyquant_conflicts"
      ],
      [
        "tiingo_api_key"
      ],
      [
        "TODAY"
      ],
      [
        "tq_exchange"
      ],
      [
        "tq_exchange_options"
      ],
      [
        "tq_fund_holdings"
      ],
      [
        "tq_fund_source_options"
      ],
      [
        "tq_get"
      ],
      [
        "tq_get_options"
      ],
      [
        "tq_index"
      ],
      [
        "tq_index_options"
      ],
      [
        "tq_mutate"
      ],
      [
        "tq_mutate_"
      ],
      [
        "tq_mutate_fun_options"
      ],
      [
        "tq_mutate_xy"
      ],
      [
        "tq_mutate_xy_"
      ],
      [
        "tq_performance"
      ],
      [
        "tq_performance_"
      ],
      [
        "tq_performance_fun_options"
      ],
      [
        "tq_portfolio"
      ],
      [
        "tq_portfolio_"
      ],
      [
        "tq_repeat_df"
      ],
      [
        "tq_transform"
      ],
      [
        "tq_transform_xy"
      ],
      [
        "tq_transmute"
      ],
      [
        "tq_transmute_"
      ],
      [
        "tq_transmute_fun_options"
      ],
      [
        "tq_transmute_xy"
      ],
      [
        "tq_transmute_xy_"
      ],
      [
        "VAR"
      ],
      [
        "VLOOKUP"
      ],
      [
        "WDAY"
      ],
      [
        "WEEK"
      ],
      [
        "WEEKDAY"
      ],
      [
        "WEEKNUM"
      ],
      [
        "WEEKNUM_ISO"
      ],
      [
        "WORKDAY_SEQUENCE"
      ],
      [
        "YEAR"
      ],
      [
        "YEAR_ISO"
      ],
      [
        "YEARFRAC"
      ],
      [
        "YMD"
      ],
      [
        "YMD_H"
      ],
      [
        "YMD_HM"
      ],
      [
        "YMD_HMS"
      ]
    ],
    "topics": [
      [
        "dplyr"
      ],
      [
        "financial-analysis"
      ],
      [
        "financial-data"
      ],
      [
        "financial-statements"
      ],
      [
        "multiple-stocks"
      ],
      [
        "performance-analysis"
      ],
      [
        "performanceanalytics"
      ],
      [
        "quantmod"
      ],
      [
        "stock"
      ],
      [
        "stock-exchanges"
      ],
      [
        "stock-indexes"
      ],
      [
        "stock-lists"
      ],
      [
        "stock-performance"
      ],
      [
        "stock-prices"
      ],
      [
        "stock-symbol"
      ],
      [
        "tidyverse"
      ],
      [
        "time-series"
      ],
      [
        "timeseries"
      ],
      [
        "xts"
      ]
    ],
    "score": 13.2668,
    "stars": 897,
    "primary_category": "tidyverse",
    "source_universe": "business-science",
    "search_text": "tidyquant Tidy Quantitative Financial Analysis Bringing business and financial analysis to the\n'tidyverse'. The 'tidyquant' package provides a convenient\nwrapper to various 'xts', 'zoo', 'quantmod', 'TTR' and\n'PerformanceAnalytics' package functions and returns the\nobjects in the tidy 'tibble' format. The main advantage is\nbeing able to use quantitative functions with the 'tidyverse'\nfunctions including 'purrr', 'dplyr', 'tidyr', 'ggplot2',\n'lubridate', etc. See the 'tidyquant' website for more\ninformation, documentation and examples. %>% ABS AS_DATE AS_DATETIME av_api_key AVERAGE AVERAGE_IFS CEILING_DATE CEILING_DAY CEILING_MONTH CEILING_QUARTER CEILING_WEEK CEILING_YEAR CHANGE CHANGE_FIRSTLAST coord_x_date coord_x_datetime COR COUNT COUNT_DAYS COUNT_IFS COUNT_UNIQUE COV CREATE_IFS CUMULATIVE_MAX CUMULATIVE_MEAN CUMULATIVE_MEDIAN CUMULATIVE_MIN CUMULATIVE_PRODUCT CUMULATIVE_SUM DATE DATE_SEQUENCE DATE_TO_DECIMAL DATE_TO_NUMERIC DATEVALUE DAY DMY DMY_H DMY_HM DMY_HMS DOM DOW EDATE EOMONTH EXP FIRST FLOOR_DATE FLOOR_DAY FLOOR_MONTH FLOOR_QUARTER FLOOR_WEEK FLOOR_YEAR FV geom_barchart geom_bbands geom_bbands_ geom_candlestick geom_ma geom_ma_ HOLIDAY_SEQUENCE HOLIDAY_TABLE HOUR IRR LAG LAST LEAD LOG MAX MAX_IFS MDAY MDY MDY_H MDY_HM MDY_HMS MEDIAN MEDIAN_IFS MIN MIN_IFS MINUTE MONTH MONTHDAY NET_WORKDAYS NOW NPV NTH palette_dark palette_green palette_light PCT_CHANGE PCT_CHANGE_FIRSTLAST pivot_table PMT PV QDAY quandl_api_key quandl_search QUARTER QUARTERDAY RATE RETURN ROUND_DATE ROUND_DAY ROUND_MONTH ROUND_QUARTER ROUND_WEEK ROUND_YEAR scale_color_tq scale_colour_tq scale_fill_tq SECOND SQRT STDEV SUM SUM_IFS theme_tq theme_tq_dark theme_tq_green tidyquant_conflicts tiingo_api_key TODAY tq_exchange tq_exchange_options tq_fund_holdings tq_fund_source_options tq_get tq_get_options tq_index tq_index_options tq_mutate tq_mutate_ tq_mutate_fun_options tq_mutate_xy tq_mutate_xy_ tq_performance tq_performance_ tq_performance_fun_options tq_portfolio tq_portfolio_ tq_repeat_df tq_transform tq_transform_xy tq_transmute tq_transmute_ tq_transmute_fun_options tq_transmute_xy tq_transmute_xy_ VAR VLOOKUP WDAY WEEK WEEKDAY WEEKNUM WEEKNUM_ISO WORKDAY_SEQUENCE YEAR YEAR_ISO YEARFRAC YMD YMD_H YMD_HM YMD_HMS dplyr financial-analysis financial-data financial-statements multiple-stocks performance-analysis performanceanalytics quantmod stock stock-exchanges stock-indexes stock-lists stock-performance stock-prices stock-symbol tidyverse time-series timeseries xts"
  },
  {
    "id": 967,
    "package_name": "piggyback",
    "title": "Managing Larger Data on a GitHub Repository",
    "description": "Helps store files as GitHub release assets, which is a\nconvenient way for large/binary data files to piggyback onto\npublic and private GitHub repositories. Includes functions for\nfile downloads, uploads, and managing releases via the GitHub\nAPI.",
    "version": "0.1.5.9007",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nTan Ho [aut] (ORCID: <https://orcid.org/0000-0001-8388-5155>),\nMark Padgham [ctb] (ORCID: <https://orcid.org/0000-0003-2172-5265>),\nJeffrey O Hanson [ctb] (ORCID: <https://orcid.org/0000-0002-4716-6134>),\nKevin Kuo [ctb] (ORCID: <https://orcid.org/0000-0001-7803-7901>)",
    "url": "https://docs.ropensci.org/piggyback/,\nhttps://github.com/ropensci/piggyback",
    "bug_reports": "https://github.com/ropensci/piggyback/issues",
    "repository": "",
    "exports": [
      [
        ".pb_cache_clear"
      ],
      [
        "pb_delete"
      ],
      [
        "pb_download"
      ],
      [
        "pb_download_url"
      ],
      [
        "pb_list"
      ],
      [
        "pb_new_release"
      ],
      [
        "pb_read"
      ],
      [
        "pb_release_create"
      ],
      [
        "pb_release_delete"
      ],
      [
        "pb_releases"
      ],
      [
        "pb_upload"
      ],
      [
        "pb_write"
      ]
    ],
    "topics": [
      [
        "data-store"
      ],
      [
        "git-lfs"
      ],
      [
        "peer-reviewed"
      ]
    ],
    "score": 13.0253,
    "stars": 197,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "piggyback Managing Larger Data on a GitHub Repository Helps store files as GitHub release assets, which is a\nconvenient way for large/binary data files to piggyback onto\npublic and private GitHub repositories. Includes functions for\nfile downloads, uploads, and managing releases via the GitHub\nAPI. .pb_cache_clear pb_delete pb_download pb_download_url pb_list pb_new_release pb_read pb_release_create pb_release_delete pb_releases pb_upload pb_write data-store git-lfs peer-reviewed"
  },
  {
    "id": 1413,
    "package_name": "vcr",
    "title": "Record 'HTTP' Calls to Disk",
    "description": "Record test suite 'HTTP' requests and replays them during\nfuture runs. A port of the Ruby gem of the same name\n(<https://github.com/vcr/vcr/>). Works by recording real 'HTTP'\nrequests/responses on disk in 'cassettes', and then replaying\nmatching responses on subsequent requests.",
    "version": "2.1.0",
    "maintainer": "Scott Chamberlain <myrmecocystus@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nAaron Wolen [aut] (ORCID: <https://orcid.org/0000-0003-2542-2202>),\nMa\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nDaniel Possenriede [aut] (ORCID:\n<https://orcid.org/0000-0002-6738-9845>),\nHadley Wickham [aut],\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/vcr/,\nhttps://books.ropensci.org/http-testing/,\nhttps://docs.ropensci.org/vcr/",
    "bug_reports": "https://github.com/ropensci/vcr/issues",
    "repository": "",
    "exports": [
      [
        "Cassette"
      ],
      [
        "cassette_path"
      ],
      [
        "cassettes"
      ],
      [
        "check_cassette_names"
      ],
      [
        "current_cassette"
      ],
      [
        "current_cassette_recording"
      ],
      [
        "current_cassette_replaying"
      ],
      [
        "eject_cassette"
      ],
      [
        "insert_cassette"
      ],
      [
        "insert_example_cassette"
      ],
      [
        "is_recording"
      ],
      [
        "is_replaying"
      ],
      [
        "local_cassette"
      ],
      [
        "local_vcr_configure"
      ],
      [
        "local_vcr_configure_log"
      ],
      [
        "setup_knitr"
      ],
      [
        "skip_if_vcr_off"
      ],
      [
        "turn_off"
      ],
      [
        "turn_on"
      ],
      [
        "turned_off"
      ],
      [
        "turned_on"
      ],
      [
        "use_cassette"
      ],
      [
        "use_vcr"
      ],
      [
        "vcr_config_defaults"
      ],
      [
        "vcr_configuration"
      ],
      [
        "vcr_configure"
      ],
      [
        "vcr_configure_log"
      ],
      [
        "vcr_configure_reset"
      ],
      [
        "vcr_last_request"
      ],
      [
        "vcr_last_response"
      ],
      [
        "vcr_test_path"
      ]
    ],
    "topics": [
      [
        "http"
      ],
      [
        "https"
      ],
      [
        "api"
      ],
      [
        "web-services"
      ],
      [
        "curl"
      ],
      [
        "mock"
      ],
      [
        "mocking"
      ],
      [
        "http-mocking"
      ],
      [
        "testing"
      ],
      [
        "testing-tools"
      ],
      [
        "tdd"
      ],
      [
        "unit-testing"
      ],
      [
        "vcr"
      ]
    ],
    "score": 10.9253,
    "stars": 93,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "vcr Record 'HTTP' Calls to Disk Record test suite 'HTTP' requests and replays them during\nfuture runs. A port of the Ruby gem of the same name\n(<https://github.com/vcr/vcr/>). Works by recording real 'HTTP'\nrequests/responses on disk in 'cassettes', and then replaying\nmatching responses on subsequent requests. Cassette cassette_path cassettes check_cassette_names current_cassette current_cassette_recording current_cassette_replaying eject_cassette insert_cassette insert_example_cassette is_recording is_replaying local_cassette local_vcr_configure local_vcr_configure_log setup_knitr skip_if_vcr_off turn_off turn_on turned_off turned_on use_cassette use_vcr vcr_config_defaults vcr_configuration vcr_configure vcr_configure_log vcr_configure_reset vcr_last_request vcr_last_response vcr_test_path http https api web-services curl mock mocking http-mocking testing testing-tools tdd unit-testing vcr"
  },
  {
    "id": 1057,
    "package_name": "rb3",
    "title": "Download and Parse Public Data Released by B3 Exchange",
    "description": "Download and parse public files released by B3 and convert\nthem into useful formats and data structures common to data\nanalysis practitioners.",
    "version": "0.1.0",
    "maintainer": "Wilson Freitas <wilson.freitas@gmail.com>",
    "author": "Wilson Freitas [aut, cre],\nMarcelo Perlin [aut]",
    "url": "https://github.com/ropensci/rb3, https://ropensci.github.io/rb3/",
    "bug_reports": "https://github.com/ropensci/rb3/issues",
    "repository": "",
    "exports": [
      [
        "code2month"
      ],
      [
        "cotahist_filter_bdr"
      ],
      [
        "cotahist_filter_equity"
      ],
      [
        "cotahist_filter_equity_options"
      ],
      [
        "cotahist_filter_etf"
      ],
      [
        "cotahist_filter_fiagro"
      ],
      [
        "cotahist_filter_fidc"
      ],
      [
        "cotahist_filter_fii"
      ],
      [
        "cotahist_filter_fund_options"
      ],
      [
        "cotahist_filter_index"
      ],
      [
        "cotahist_filter_index_options"
      ],
      [
        "cotahist_filter_unit"
      ],
      [
        "cotahist_get"
      ],
      [
        "cotahist_options_by_symbols_get"
      ],
      [
        "download_marketdata"
      ],
      [
        "fetch_marketdata"
      ],
      [
        "futures_get"
      ],
      [
        "indexes_composition_get"
      ],
      [
        "indexes_current_portfolio_get"
      ],
      [
        "indexes_get"
      ],
      [
        "indexes_historical_data_get"
      ],
      [
        "indexes_theoretical_portfolio_get"
      ],
      [
        "list_templates"
      ],
      [
        "maturitycode2date"
      ],
      [
        "meta_db_connection"
      ],
      [
        "rb3_bootstrap"
      ],
      [
        "read_marketdata"
      ],
      [
        "template_dataset"
      ],
      [
        "template_meta_create_or_load"
      ],
      [
        "template_meta_load"
      ],
      [
        "template_meta_new"
      ],
      [
        "template_retrieve"
      ],
      [
        "yc_brl_get"
      ],
      [
        "yc_brl_with_futures_get"
      ],
      [
        "yc_get"
      ],
      [
        "yc_ipca_get"
      ],
      [
        "yc_ipca_with_futures_get"
      ],
      [
        "yc_usd_get"
      ],
      [
        "yc_usd_with_futures_get"
      ]
    ],
    "topics": [
      [
        "brazil"
      ],
      [
        "exchange-data"
      ],
      [
        "finance"
      ],
      [
        "financial-data"
      ],
      [
        "financial-services"
      ],
      [
        "market-data"
      ]
    ],
    "score": 8.6098,
    "stars": 87,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rb3 Download and Parse Public Data Released by B3 Exchange Download and parse public files released by B3 and convert\nthem into useful formats and data structures common to data\nanalysis practitioners. code2month cotahist_filter_bdr cotahist_filter_equity cotahist_filter_equity_options cotahist_filter_etf cotahist_filter_fiagro cotahist_filter_fidc cotahist_filter_fii cotahist_filter_fund_options cotahist_filter_index cotahist_filter_index_options cotahist_filter_unit cotahist_get cotahist_options_by_symbols_get download_marketdata fetch_marketdata futures_get indexes_composition_get indexes_current_portfolio_get indexes_get indexes_historical_data_get indexes_theoretical_portfolio_get list_templates maturitycode2date meta_db_connection rb3_bootstrap read_marketdata template_dataset template_meta_create_or_load template_meta_load template_meta_new template_retrieve yc_brl_get yc_brl_with_futures_get yc_get yc_ipca_get yc_ipca_with_futures_get yc_usd_get yc_usd_with_futures_get brazil exchange-data finance financial-data financial-services market-data"
  },
  {
    "id": 1290,
    "package_name": "swagger",
    "title": "Dynamically Generates Documentation from a 'Swagger' Compliant\nAPI",
    "description": "A collection of 'HTML', 'JavaScript', and 'CSS' assets\nthat dynamically generate beautiful documentation from a\n'Swagger' compliant API: <https://swagger.io/specification/>.",
    "version": "5.30.0",
    "maintainer": "Bruno Tremblay <cran@neoxone.com>",
    "author": "Barret Schloerke [aut] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\nJavier Luraschi [aut],\nBruno Tremblay [cre, ctb],\nRStudio [cph],\nSmartBear Software [aut, cph]",
    "url": "https://rstudio.github.io/swagger/,\nhttps://github.com/rstudio/swagger",
    "bug_reports": "https://github.com/rstudio/swagger/issues",
    "repository": "",
    "exports": [
      [
        "plumber_docs"
      ],
      [
        "swagger_index"
      ],
      [
        "swagger_path"
      ],
      [
        "swagger_spec"
      ]
    ],
    "topics": [],
    "score": 8.4674,
    "stars": 54,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "swagger Dynamically Generates Documentation from a 'Swagger' Compliant\nAPI A collection of 'HTML', 'JavaScript', and 'CSS' assets\nthat dynamically generate beautiful documentation from a\n'Swagger' compliant API: <https://swagger.io/specification/>. plumber_docs swagger_index swagger_path swagger_spec "
  },
  {
    "id": 1469,
    "package_name": "yfR",
    "title": "Downloads and Organizes Financial Data from Yahoo Finance",
    "description": "Facilitates download of financial data from Yahoo Finance\n<https://finance.yahoo.com/>, a vast repository of stock price\ndata across multiple financial exchanges. The package offers a\nlocal caching system and support for parallel computation.",
    "version": "1.1.2",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre],\nNic Crane [rev] (Nic reviewed the package (v. 0.0.5) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/523>),\nAlexander Fischer [rev] (Alexander reviewed the package (v. 0.0.5) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/523>)",
    "url": "https://github.com/ropensci/yfR, https://docs.ropensci.org/yfR/",
    "bug_reports": "https://github.com/ropensci/yfR/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "yf_cachefolder_get"
      ],
      [
        "yf_collection_get"
      ],
      [
        "yf_convert_to_wide"
      ],
      [
        "yf_get"
      ],
      [
        "yf_get_available_collections"
      ],
      [
        "yf_get_dividends"
      ],
      [
        "yf_index_composition"
      ],
      [
        "yf_index_list"
      ],
      [
        "yf_live_prices"
      ]
    ],
    "topics": [],
    "score": 8.0102,
    "stars": 47,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "yfR Downloads and Organizes Financial Data from Yahoo Finance Facilitates download of financial data from Yahoo Finance\n<https://finance.yahoo.com/>, a vast repository of stock price\ndata across multiple financial exchanges. The package offers a\nlocal caching system and support for parallel computation. %>% yf_cachefolder_get yf_collection_get yf_convert_to_wide yf_get yf_get_available_collections yf_get_dividends yf_index_composition yf_index_list yf_live_prices "
  },
  {
    "id": 1228,
    "package_name": "simulist",
    "title": "Simulate Disease Outbreak Line List and Contacts Data",
    "description": "Tools to simulate realistic raw case data for an epidemic\nin the form of line lists and contacts using a branching\nprocess. Simulated outbreaks are parameterised with\nepidemiological parameters and can have age-structured\npopulations, age-stratified hospitalisation and death risk and\ntime-varying case fatality risk.",
    "version": "0.6.0.9000",
    "maintainer": "Joshua W. Lambert <joshua.lambert@lshtm.ac.uk>",
    "author": "Joshua W. Lambert [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0001-5218-3046>),\nCarmen Tamayo Cuartero [aut] (ORCID:\n<https://orcid.org/0000-0003-4184-2864>),\nHugo Gruson [ctb, rev] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nPratik R. Gupte [ctb, rev] (ORCID:\n<https://orcid.org/0000-0001-5294-7819>),\nAdam Kucharski [rev] (ORCID: <https://orcid.org/0000-0001-8814-9421>),\nChris Hartgerink [rev] (ORCID: <https://orcid.org/0000-0003-1050-6809>),\nSebastian Funk [ctb] (ORCID: <https://orcid.org/0000-0002-2842-3406>),\nLondon School of Hygiene and Tropical Medicine, LSHTM [cph] (ROR:\n<https://ror.org/00a0jsq62>)",
    "url": "https://github.com/epiverse-trace/simulist,\nhttps://epiverse-trace.github.io/simulist/",
    "bug_reports": "https://github.com/epiverse-trace/simulist/issues",
    "repository": "",
    "exports": [
      [
        "censor_linelist"
      ],
      [
        "create_config"
      ],
      [
        "messy_linelist"
      ],
      [
        "sim_contacts"
      ],
      [
        "sim_linelist"
      ],
      [
        "sim_outbreak"
      ],
      [
        "truncate_linelist"
      ]
    ],
    "topics": [
      [
        "epidemiology"
      ],
      [
        "epiverse"
      ],
      [
        "linelist"
      ],
      [
        "outbreaks"
      ]
    ],
    "score": 7.96,
    "stars": 10,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "simulist Simulate Disease Outbreak Line List and Contacts Data Tools to simulate realistic raw case data for an epidemic\nin the form of line lists and contacts using a branching\nprocess. Simulated outbreaks are parameterised with\nepidemiological parameters and can have age-structured\npopulations, age-stratified hospitalisation and death risk and\ntime-varying case fatality risk. censor_linelist create_config messy_linelist sim_contacts sim_linelist sim_outbreak truncate_linelist epidemiology epiverse linelist outbreaks"
  },
  {
    "id": 1326,
    "package_name": "tfhub",
    "title": "Interface to 'TensorFlow' Hub",
    "description": "'TensorFlow' Hub is a library for the publication,\ndiscovery, and consumption of reusable parts of machine\nlearning models. A module is a self-contained piece of a\n'TensorFlow' graph, along with its weights and assets, that can\nbe reused across different tasks in a process known as transfer\nlearning. Transfer learning train a model with a smaller\ndataset, improve generalization, and speed up training.",
    "version": "0.8.1.9000",
    "maintainer": "Tomasz Kalinowski <tomasz.kalinowski@rstudio.com>",
    "author": "Tomasz Kalinowski [aut, cre],\nDaniel Falbel [aut],\nJJ Allaire [aut],\nRStudio [cph, fnd],\nGoogle Inc. [cph]",
    "url": "https://github.com/rstudio/tfhub",
    "bug_reports": "https://github.com/rstudio/tfhub/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "hub_image_embedding_column"
      ],
      [
        "hub_load"
      ],
      [
        "hub_sparse_text_embedding_column"
      ],
      [
        "hub_text_embedding_column"
      ],
      [
        "install_tensorflow"
      ],
      [
        "install_tfhub"
      ],
      [
        "layer_hub"
      ],
      [
        "shape"
      ],
      [
        "step_pretrained_text_embedding"
      ],
      [
        "tf"
      ]
    ],
    "topics": [],
    "score": 7.6379,
    "stars": 29,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "tfhub Interface to 'TensorFlow' Hub 'TensorFlow' Hub is a library for the publication,\ndiscovery, and consumption of reusable parts of machine\nlearning models. A module is a self-contained piece of a\n'TensorFlow' graph, along with its weights and assets, that can\nbe reused across different tasks in a process known as transfer\nlearning. Transfer learning train a model with a smaller\ndataset, improve generalization, and speed up training. %>% hub_image_embedding_column hub_load hub_sparse_text_embedding_column hub_text_embedding_column install_tensorflow install_tfhub layer_hub shape step_pretrained_text_embedding tf "
  },
  {
    "id": 221,
    "package_name": "aRxiv",
    "title": "Interface to the arXiv API",
    "description": "An interface to the API for 'arXiv', a repository of\nelectronic preprints for computer science, mathematics,\nphysics, quantitative biology, quantitative finance, and\nstatistics.",
    "version": "0.18",
    "maintainer": "Karl Broman <broman@wisc.edu>",
    "author": "Karthik Ram [aut] (ORCID: <https://orcid.org/0000-0002-0233-1757>),\nKarl Broman [aut, cre] (ORCID: <https://orcid.org/0000-0002-4914-6671>)",
    "url": "https://docs.ropensci.org/aRxiv/,\nhttps://github.com/ropensci/aRxiv",
    "bug_reports": "https://github.com/ropensci/aRxiv/issues",
    "repository": "",
    "exports": [
      [
        "arxiv_count"
      ],
      [
        "arxiv_open"
      ],
      [
        "arxiv_search"
      ],
      [
        "can_arxiv_connect"
      ]
    ],
    "topics": [
      [
        "arxiv"
      ],
      [
        "arxiv-analytics"
      ],
      [
        "arxiv-api"
      ],
      [
        "arxiv-org"
      ]
    ],
    "score": 7.4966,
    "stars": 63,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "aRxiv Interface to the arXiv API An interface to the API for 'arXiv', a repository of\nelectronic preprints for computer science, mathematics,\nphysics, quantitative biology, quantitative finance, and\nstatistics. arxiv_count arxiv_open arxiv_search can_arxiv_connect arxiv arxiv-analytics arxiv-api arxiv-org"
  },
  {
    "id": 553,
    "package_name": "fExtremes",
    "title": "Rmetrics - Modelling Extreme Events in Finance",
    "description": "Provides functions for analysing and modelling extreme\nevents in financial time Series. The topics include: (i) data\npre-processing, (ii) explorative data analysis, (iii) peak over\nthreshold modelling, (iv) block maxima modelling, (v)\nestimation of VaR and CVaR, and (vi) the computation of the\nextreme index.",
    "version": "4032.84",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "author": "Diethelm Wuertz [aut],\nTobias Setz [aut],\nYohan Chalabi [aut],\nPaul J. Northrop [cre, ctb]",
    "url": "https://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/projects/rmetrics",
    "repository": "",
    "exports": [
      [
        ".gevmleFit"
      ],
      [
        ".gevpwmFit"
      ],
      [
        ".gevrlevelLLH"
      ],
      [
        ".gummleFit"
      ],
      [
        ".gumpwmFit"
      ],
      [
        "blockMaxima"
      ],
      [
        "blockTheta"
      ],
      [
        "clusterTheta"
      ],
      [
        "CVaR"
      ],
      [
        "deCluster"
      ],
      [
        "dgev"
      ],
      [
        "dgpd"
      ],
      [
        "emdPlot"
      ],
      [
        "exindexesPlot"
      ],
      [
        "exindexPlot"
      ],
      [
        "ferrosegersTheta"
      ],
      [
        "findThreshold"
      ],
      [
        "gevFit"
      ],
      [
        "gevMoments"
      ],
      [
        "gevrlevelPlot"
      ],
      [
        "gevSim"
      ],
      [
        "gevSlider"
      ],
      [
        "ghMeanExcessFit"
      ],
      [
        "ghtMeanExcessFit"
      ],
      [
        "gpdFit"
      ],
      [
        "gpdMoments"
      ],
      [
        "gpdQPlot"
      ],
      [
        "gpdQuantPlot"
      ],
      [
        "gpdRiskMeasures"
      ],
      [
        "gpdSfallPlot"
      ],
      [
        "gpdShapePlot"
      ],
      [
        "gpdSim"
      ],
      [
        "gpdSlider"
      ],
      [
        "gpdTailPlot"
      ],
      [
        "gumbelFit"
      ],
      [
        "gumbelSim"
      ],
      [
        "hillPlot"
      ],
      [
        "hypMeanExcessFit"
      ],
      [
        "lilPlot"
      ],
      [
        "mePlot"
      ],
      [
        "mrlPlot"
      ],
      [
        "msratioPlot"
      ],
      [
        "mxfPlot"
      ],
      [
        "nigMeanExcessFit"
      ],
      [
        "normMeanExcessFit"
      ],
      [
        "pgev"
      ],
      [
        "pgpd"
      ],
      [
        "pointProcess"
      ],
      [
        "qgev"
      ],
      [
        "qgpd"
      ],
      [
        "qqparetoPlot"
      ],
      [
        "recordsPlot"
      ],
      [
        "rgev"
      ],
      [
        "rgpd"
      ],
      [
        "runTheta"
      ],
      [
        "shaparmDEHaan"
      ],
      [
        "shaparmHill"
      ],
      [
        "shaparmPickands"
      ],
      [
        "shaparmPlot"
      ],
      [
        "sllnPlot"
      ],
      [
        "ssrecordsPlot"
      ],
      [
        "tailPlot"
      ],
      [
        "tailRisk"
      ],
      [
        "tailSlider"
      ],
      [
        "thetaSim"
      ],
      [
        "VaR"
      ],
      [
        "xacfPlot"
      ]
    ],
    "topics": [],
    "score": 7.3466,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "fExtremes Rmetrics - Modelling Extreme Events in Finance Provides functions for analysing and modelling extreme\nevents in financial time Series. The topics include: (i) data\npre-processing, (ii) explorative data analysis, (iii) peak over\nthreshold modelling, (iv) block maxima modelling, (v)\nestimation of VaR and CVaR, and (vi) the computation of the\nextreme index. .gevmleFit .gevpwmFit .gevrlevelLLH .gummleFit .gumpwmFit blockMaxima blockTheta clusterTheta CVaR deCluster dgev dgpd emdPlot exindexesPlot exindexPlot ferrosegersTheta findThreshold gevFit gevMoments gevrlevelPlot gevSim gevSlider ghMeanExcessFit ghtMeanExcessFit gpdFit gpdMoments gpdQPlot gpdQuantPlot gpdRiskMeasures gpdSfallPlot gpdShapePlot gpdSim gpdSlider gpdTailPlot gumbelFit gumbelSim hillPlot hypMeanExcessFit lilPlot mePlot mrlPlot msratioPlot mxfPlot nigMeanExcessFit normMeanExcessFit pgev pgpd pointProcess qgev qgpd qqparetoPlot recordsPlot rgev rgpd runTheta shaparmDEHaan shaparmHill shaparmPickands shaparmPlot sllnPlot ssrecordsPlot tailPlot tailRisk tailSlider thetaSim VaR xacfPlot "
  },
  {
    "id": 556,
    "package_name": "fPortfolio",
    "title": "Rmetrics - Portfolio Selection and Optimization",
    "description": "A collection of functions to optimize portfolios and to\nanalyze them from different points of view.",
    "version": "4023.84.9000",
    "maintainer": "Stefan Theussl <Stefan.Theussl@R-project.org>",
    "author": "Diethelm Wuertz [aut],\nTobias Setz [aut],\nYohan Chalabi [aut],\nWilliam Chen [ctb],\nStefan Theussl [aut, cre]",
    "url": "https://r-forge.r-project.org/projects/rmetrics/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        ".fportfolio.plot.1"
      ],
      [
        ".fportfolio.plot.2"
      ],
      [
        ".fportfolio.plot.3"
      ],
      [
        ".fportfolio.plot.4"
      ],
      [
        ".fportfolio.plot.5"
      ],
      [
        ".fportfolio.plot.6"
      ],
      [
        ".fportfolio.plot.7"
      ],
      [
        ".fportfolio.plot.8"
      ],
      [
        "addRainbow"
      ],
      [
        "amplDataAdd"
      ],
      [
        "amplDataAddMatrix"
      ],
      [
        "amplDataAddValue"
      ],
      [
        "amplDataAddVector"
      ],
      [
        "amplDataOpen"
      ],
      [
        "amplDataSemicolon"
      ],
      [
        "amplDataShow"
      ],
      [
        "amplLP"
      ],
      [
        "amplLPControl"
      ],
      [
        "amplModelAdd"
      ],
      [
        "amplModelOpen"
      ],
      [
        "amplModelShow"
      ],
      [
        "amplNLP"
      ],
      [
        "amplNLPControl"
      ],
      [
        "amplOutShow"
      ],
      [
        "amplQP"
      ],
      [
        "amplQPControl"
      ],
      [
        "amplRunAdd"
      ],
      [
        "amplRunOpen"
      ],
      [
        "amplRunShow"
      ],
      [
        "backtestAssetsPlot"
      ],
      [
        "backtestDrawdownPlot"
      ],
      [
        "backtestPlot"
      ],
      [
        "backtestPortfolioPlot"
      ],
      [
        "backtestRebalancePlot"
      ],
      [
        "backtestReportPlot"
      ],
      [
        "backtestStats"
      ],
      [
        "backtestWeightsPlot"
      ],
      [
        "bcpAnalytics"
      ],
      [
        "budgetsModifiedES"
      ],
      [
        "budgetsModifiedVAR"
      ],
      [
        "budgetsNormalES"
      ],
      [
        "budgetsNormalVAR"
      ],
      [
        "budgetsSampleCOV"
      ],
      [
        "cmlLines"
      ],
      [
        "cmlPoints"
      ],
      [
        "covEstimator"
      ],
      [
        "covMcdEstimator"
      ],
      [
        "covOGKEstimator"
      ],
      [
        "covRisk"
      ],
      [
        "covRiskBudgetsLinePlot"
      ],
      [
        "covRiskBudgetsPie"
      ],
      [
        "covRiskBudgetsPlot"
      ],
      [
        "cvarRisk"
      ],
      [
        "Data"
      ],
      [
        "drawdownsAnalytics"
      ],
      [
        "efficientPortfolio"
      ],
      [
        "emaSmoother"
      ],
      [
        "eqsumWConstraints"
      ],
      [
        "equalWeightsPoints"
      ],
      [
        "equidistWindows"
      ],
      [
        "feasibleGrid"
      ],
      [
        "feasiblePortfolio"
      ],
      [
        "frontierPlot"
      ],
      [
        "frontierPlotControl"
      ],
      [
        "frontierPoints"
      ],
      [
        "garchAnalytics"
      ],
      [
        "getA"
      ],
      [
        "getA.fPFOLIOSPEC"
      ],
      [
        "getA.fPORTFOLIO"
      ],
      [
        "getAlpha"
      ],
      [
        "getAlpha.fPFOLIOSPEC"
      ],
      [
        "getAlpha.fPFOLIOVAL"
      ],
      [
        "getAlpha.fPORTFOLIO"
      ],
      [
        "getConstraints"
      ],
      [
        "getConstraints.fPORTFOLIO"
      ],
      [
        "getConstraintsTypes"
      ],
      [
        "getControl"
      ],
      [
        "getControl.fPFOLIOSPEC"
      ],
      [
        "getControl.fPORTFOLIO"
      ],
      [
        "getCov"
      ],
      [
        "getCov.fPFOLIODATA"
      ],
      [
        "getCov.fPORTFOLIO"
      ],
      [
        "getCovRiskBudgets"
      ],
      [
        "getCovRiskBudgets.fPFOLIOVAL"
      ],
      [
        "getCovRiskBudgets.fPORTFOLIO"
      ],
      [
        "getData"
      ],
      [
        "getData.fPFOLIODATA"
      ],
      [
        "getData.fPORTFOLIO"
      ],
      [
        "getEstimator"
      ],
      [
        "getEstimator.fPFOLIODATA"
      ],
      [
        "getEstimator.fPFOLIOSPEC"
      ],
      [
        "getEstimator.fPORTFOLIO"
      ],
      [
        "getMean"
      ],
      [
        "getMean.fPFOLIODATA"
      ],
      [
        "getMean.fPORTFOLIO"
      ],
      [
        "getMessages"
      ],
      [
        "getMessages.fPFOLIOBACKTEST"
      ],
      [
        "getMessages.fPFOLIOSPEC"
      ],
      [
        "getModel.fPFOLIOSPEC"
      ],
      [
        "getModel.fPORTFOLIO"
      ],
      [
        "getMu"
      ],
      [
        "getMu.fPFOLIODATA"
      ],
      [
        "getMu.fPORTFOLIO"
      ],
      [
        "getNAssets"
      ],
      [
        "getNAssets.fPFOLIODATA"
      ],
      [
        "getNAssets.fPORTFOLIO"
      ],
      [
        "getNFrontierPoints"
      ],
      [
        "getNFrontierPoints.fPFOLIOSPEC"
      ],
      [
        "getNFrontierPoints.fPFOLIOVAL"
      ],
      [
        "getNFrontierPoints.fPORTFOLIO"
      ],
      [
        "getObjective"
      ],
      [
        "getObjective.fPFOLIOSPEC"
      ],
      [
        "getObjective.fPORTFOLIO"
      ],
      [
        "getOptim"
      ],
      [
        "getOptim.fPFOLIOSPEC"
      ],
      [
        "getOptim.fPORTFOLIO"
      ],
      [
        "getOptimize"
      ],
      [
        "getOptimize.fPFOLIOSPEC"
      ],
      [
        "getOptimize.fPORTFOLIO"
      ],
      [
        "getOptions"
      ],
      [
        "getOptions.fPFOLIOSPEC"
      ],
      [
        "getOptions.fPORTFOLIO"
      ],
      [
        "getParams"
      ],
      [
        "getParams.fPFOLIOSPEC"
      ],
      [
        "getParams.fPORTFOLIO"
      ],
      [
        "getPortfolio"
      ],
      [
        "getPortfolio.fPFOLIOSPEC"
      ],
      [
        "getPortfolio.fPFOLIOVAL"
      ],
      [
        "getPortfolio.fPORTFOLIO"
      ],
      [
        "getRiskFreeRate"
      ],
      [
        "getRiskFreeRate.fPFOLIOSPEC"
      ],
      [
        "getRiskFreeRate.fPFOLIOVAL"
      ],
      [
        "getRiskFreeRate.fPORTFOLIO"
      ],
      [
        "getSeries"
      ],
      [
        "getSeries.fPFOLIODATA"
      ],
      [
        "getSeries.fPORTFOLIO"
      ],
      [
        "getSigma"
      ],
      [
        "getSigma.fPFOLIODATA"
      ],
      [
        "getSigma.fPORTFOLIO"
      ],
      [
        "getSmoother"
      ],
      [
        "getSmoother.fPFOLIOBACKTEST"
      ],
      [
        "getSmootherDoubleSmoothing"
      ],
      [
        "getSmootherDoubleSmoothing.fPFOLIOBACKTEST"
      ],
      [
        "getSmootherFun"
      ],
      [
        "getSmootherFun.fPFOLIOBACKTEST"
      ],
      [
        "getSmootherInitialWeights"
      ],
      [
        "getSmootherInitialWeights.fPFOLIOBACKTEST"
      ],
      [
        "getSmootherLambda"
      ],
      [
        "getSmootherLambda.fPFOLIOBACKTEST"
      ],
      [
        "getSmootherParams"
      ],
      [
        "getSmootherParams.fPFOLIOBACKTEST"
      ],
      [
        "getSmootherSkip"
      ],
      [
        "getSmootherSkip.fPFOLIOBACKTEST"
      ],
      [
        "getSolver"
      ],
      [
        "getSolver.fPFOLIOSPEC"
      ],
      [
        "getSolver.fPORTFOLIO"
      ],
      [
        "getSpec"
      ],
      [
        "getSpec.fPORTFOLIO"
      ],
      [
        "getStatistics"
      ],
      [
        "getStatistics.fPFOLIODATA"
      ],
      [
        "getStatistics.fPORTFOLIO"
      ],
      [
        "getStatus"
      ],
      [
        "getStatus.fPFOLIOSPEC"
      ],
      [
        "getStatus.fPFOLIOVAL"
      ],
      [
        "getStatus.fPORTFOLIO"
      ],
      [
        "getStrategy"
      ],
      [
        "getStrategy.fPFOLIOBACKTEST"
      ],
      [
        "getStrategyFun"
      ],
      [
        "getStrategyFun.fPFOLIOBACKTEST"
      ],
      [
        "getStrategyParams"
      ],
      [
        "getStrategyParams.fPFOLIOBACKTEST"
      ],
      [
        "getTailRisk"
      ],
      [
        "getTailRisk.fPFOLIODATA"
      ],
      [
        "getTailRisk.fPFOLIOSPEC"
      ],
      [
        "getTailRisk.fPORTFOLIO"
      ],
      [
        "getTailRiskBudgets"
      ],
      [
        "getTailRiskBudgets.fPORTFOLIO"
      ],
      [
        "getTargetReturn"
      ],
      [
        "getTargetReturn.fPFOLIOSPEC"
      ],
      [
        "getTargetReturn.fPFOLIOVAL"
      ],
      [
        "getTargetReturn.fPORTFOLIO"
      ],
      [
        "getTargetRisk"
      ],
      [
        "getTargetRisk.fPFOLIOSPEC"
      ],
      [
        "getTargetRisk.fPFOLIOVAL"
      ],
      [
        "getTargetRisk.fPORTFOLIO"
      ],
      [
        "getTrace"
      ],
      [
        "getTrace.fPFOLIOSPEC"
      ],
      [
        "getTrace.fPORTFOLIO"
      ],
      [
        "getType"
      ],
      [
        "getType.fPFOLIOSPEC"
      ],
      [
        "getType.fPORTFOLIO"
      ],
      [
        "getUnits.fPFOLIODATA"
      ],
      [
        "getUnits.fPORTFOLIO"
      ],
      [
        "getWeights"
      ],
      [
        "getWeights.fPFOLIOSPEC"
      ],
      [
        "getWeights.fPFOLIOVAL"
      ],
      [
        "getWeights.fPORTFOLIO"
      ],
      [
        "getWindows"
      ],
      [
        "getWindows.fPFOLIOBACKTEST"
      ],
      [
        "getWindowsFun"
      ],
      [
        "getWindowsFun.fPFOLIOBACKTEST"
      ],
      [
        "getWindowsHorizon"
      ],
      [
        "getWindowsHorizon.fPFOLIOBACKTEST"
      ],
      [
        "getWindowsParams"
      ],
      [
        "getWindowsParams.fPFOLIOBACKTEST"
      ],
      [
        "glpkLP"
      ],
      [
        "glpkLPControl"
      ],
      [
        "ipopQP"
      ],
      [
        "ipopQPControl"
      ],
      [
        "kendallEstimator"
      ],
      [
        "kestrelQP"
      ],
      [
        "kestrelQPControl"
      ],
      [
        "lambdaCVaR"
      ],
      [
        "listFConstraints"
      ],
      [
        "lpmEstimator"
      ],
      [
        "markowitzHull"
      ],
      [
        "maxBConstraints"
      ],
      [
        "maxBuyinConstraints"
      ],
      [
        "maxCardConstraints"
      ],
      [
        "maxddMap"
      ],
      [
        "maxFConstraints"
      ],
      [
        "maxratioPortfolio"
      ],
      [
        "maxreturnPortfolio"
      ],
      [
        "maxsumWConstraints"
      ],
      [
        "maxWConstraints"
      ],
      [
        "mcdEstimator"
      ],
      [
        "minBConstraints"
      ],
      [
        "minBuyinConstraints"
      ],
      [
        "minCardConstraints"
      ],
      [
        "minFConstraints"
      ],
      [
        "minriskPortfolio"
      ],
      [
        "minsumWConstraints"
      ],
      [
        "minvariancePoints"
      ],
      [
        "minvariancePortfolio"
      ],
      [
        "minWConstraints"
      ],
      [
        "modifiedVaR"
      ],
      [
        "monteCarloPoints"
      ],
      [
        "mveEstimator"
      ],
      [
        "nCardConstraints"
      ],
      [
        "neosLP"
      ],
      [
        "neosLPControl"
      ],
      [
        "neosQP"
      ],
      [
        "neosQPControl"
      ],
      [
        "netPerformance"
      ],
      [
        "nlminb2"
      ],
      [
        "nlminb2Control"
      ],
      [
        "nlminb2NLP"
      ],
      [
        "nlminb2NLPControl"
      ],
      [
        "nnveEstimator"
      ],
      [
        "normalVaR"
      ],
      [
        "parAnalytics"
      ],
      [
        "pcoutAnalytics"
      ],
      [
        "pfolioCVaR"
      ],
      [
        "pfolioCVaRoptim"
      ],
      [
        "pfolioCVaRplus"
      ],
      [
        "pfolioHist"
      ],
      [
        "pfolioMaxLoss"
      ],
      [
        "pfolioReturn"
      ],
      [
        "pfolioTargetReturn"
      ],
      [
        "pfolioTargetRisk"
      ],
      [
        "pfolioVaR"
      ],
      [
        "plot.fPORTFOLIO"
      ],
      [
        "portfolioBacktest"
      ],
      [
        "portfolioBacktesting"
      ],
      [
        "portfolioConstraints"
      ],
      [
        "portfolioData"
      ],
      [
        "portfolioFrontier"
      ],
      [
        "portfolioObjective"
      ],
      [
        "portfolioReturn"
      ],
      [
        "portfolioRisk"
      ],
      [
        "portfolioSmoothing"
      ],
      [
        "portfolioSpec"
      ],
      [
        "print.solver"
      ],
      [
        "quadprogQP"
      ],
      [
        "quadprogQPControl"
      ],
      [
        "ramplLP"
      ],
      [
        "ramplNLP"
      ],
      [
        "ramplQP"
      ],
      [
        "rglpkLP"
      ],
      [
        "ripop"
      ],
      [
        "ripopQP"
      ],
      [
        "riskBudgetsPlot"
      ],
      [
        "riskMap"
      ],
      [
        "riskmetricsAnalytics"
      ],
      [
        "rkestrelQP"
      ],
      [
        "rneosLP"
      ],
      [
        "rneosQP"
      ],
      [
        "rnlminb2"
      ],
      [
        "rnlminb2NLP"
      ],
      [
        "rollingCDaR"
      ],
      [
        "rollingCmlPortfolio"
      ],
      [
        "rollingCVaR"
      ],
      [
        "rollingDaR"
      ],
      [
        "rollingMinvariancePortfolio"
      ],
      [
        "rollingPortfolioFrontier"
      ],
      [
        "rollingSigma"
      ],
      [
        "rollingTangencyPortfolio"
      ],
      [
        "rollingVaR"
      ],
      [
        "rollingWindows"
      ],
      [
        "rquadprog"
      ],
      [
        "rquadprogQP"
      ],
      [
        "rsolnpNLP"
      ],
      [
        "rsolveLP"
      ],
      [
        "rsolveQP"
      ],
      [
        "rsymphonyLP"
      ],
      [
        "sampleCOV"
      ],
      [
        "sampleVaR"
      ],
      [
        "setAlpha<-"
      ],
      [
        "setEstimator<-"
      ],
      [
        "setNFrontierPoints<-"
      ],
      [
        "setObjective<-"
      ],
      [
        "setOptimize<-"
      ],
      [
        "setParams<-"
      ],
      [
        "setRiskFreeRate<-"
      ],
      [
        "setSmootherDoubleSmoothing<-"
      ],
      [
        "setSmootherFun<-"
      ],
      [
        "setSmootherInitialWeights<-"
      ],
      [
        "setSmootherLambda<-"
      ],
      [
        "setSmootherParams<-"
      ],
      [
        "setSmootherSkip<-"
      ],
      [
        "setSolver<-"
      ],
      [
        "setStatus<-"
      ],
      [
        "setStrategyFun<-"
      ],
      [
        "setStrategyParams<-"
      ],
      [
        "setTailRisk<-"
      ],
      [
        "setTargetReturn<-"
      ],
      [
        "setTargetRisk<-"
      ],
      [
        "setTrace<-"
      ],
      [
        "setType<-"
      ],
      [
        "setWeights<-"
      ],
      [
        "setWindowsFun<-"
      ],
      [
        "setWindowsHorizon<-"
      ],
      [
        "setWindowsParams<-"
      ],
      [
        "sharpeRatioLines"
      ],
      [
        "shrinkEstimator"
      ],
      [
        "singleAssetPoints"
      ],
      [
        "slpmEstimator"
      ],
      [
        "solnpNLP"
      ],
      [
        "solnpNLPControl"
      ],
      [
        "solveRampl.CVAR"
      ],
      [
        "solveRampl.MV"
      ],
      [
        "solveRglpk.CVAR"
      ],
      [
        "solveRglpk.MAD"
      ],
      [
        "solveRipop"
      ],
      [
        "solveRquadprog"
      ],
      [
        "solveRquadprog.CLA"
      ],
      [
        "solveRshortExact"
      ],
      [
        "solveRsocp"
      ],
      [
        "solveRsolnp"
      ],
      [
        "spearmanEstimator"
      ],
      [
        "stabilityAnalytics"
      ],
      [
        "summary.fPORTFOLIO"
      ],
      [
        "symphonyLP"
      ],
      [
        "symphonyLPControl"
      ],
      [
        "tailoredFrontierPlot"
      ],
      [
        "tailRiskBudgetsPie"
      ],
      [
        "tailRiskBudgetsPlot"
      ],
      [
        "tangencyLines"
      ],
      [
        "tangencyPoints"
      ],
      [
        "tangencyPortfolio"
      ],
      [
        "tangencyStrategy"
      ],
      [
        "ternaryCoord"
      ],
      [
        "ternaryFrontier"
      ],
      [
        "ternaryMap"
      ],
      [
        "ternaryPoints"
      ],
      [
        "ternaryWeights"
      ],
      [
        "turnsAnalytics"
      ],
      [
        "twoAssetsLines"
      ],
      [
        "varRisk"
      ],
      [
        "waveletSpectrum"
      ],
      [
        "weightedReturnsLinePlot"
      ],
      [
        "weightedReturnsPie"
      ],
      [
        "weightedReturnsPlot"
      ],
      [
        "weightsLinePlot"
      ],
      [
        "weightsPie"
      ],
      [
        "weightsPlot"
      ],
      [
        "weightsSlider"
      ]
    ],
    "topics": [],
    "score": 7.16,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "fPortfolio Rmetrics - Portfolio Selection and Optimization A collection of functions to optimize portfolios and to\nanalyze them from different points of view. .fportfolio.plot.1 .fportfolio.plot.2 .fportfolio.plot.3 .fportfolio.plot.4 .fportfolio.plot.5 .fportfolio.plot.6 .fportfolio.plot.7 .fportfolio.plot.8 addRainbow amplDataAdd amplDataAddMatrix amplDataAddValue amplDataAddVector amplDataOpen amplDataSemicolon amplDataShow amplLP amplLPControl amplModelAdd amplModelOpen amplModelShow amplNLP amplNLPControl amplOutShow amplQP amplQPControl amplRunAdd amplRunOpen amplRunShow backtestAssetsPlot backtestDrawdownPlot backtestPlot backtestPortfolioPlot backtestRebalancePlot backtestReportPlot backtestStats backtestWeightsPlot bcpAnalytics budgetsModifiedES budgetsModifiedVAR budgetsNormalES budgetsNormalVAR budgetsSampleCOV cmlLines cmlPoints covEstimator covMcdEstimator covOGKEstimator covRisk covRiskBudgetsLinePlot covRiskBudgetsPie covRiskBudgetsPlot cvarRisk Data drawdownsAnalytics efficientPortfolio emaSmoother eqsumWConstraints equalWeightsPoints equidistWindows feasibleGrid feasiblePortfolio frontierPlot frontierPlotControl frontierPoints garchAnalytics getA getA.fPFOLIOSPEC getA.fPORTFOLIO getAlpha getAlpha.fPFOLIOSPEC getAlpha.fPFOLIOVAL getAlpha.fPORTFOLIO getConstraints getConstraints.fPORTFOLIO getConstraintsTypes getControl getControl.fPFOLIOSPEC getControl.fPORTFOLIO getCov getCov.fPFOLIODATA getCov.fPORTFOLIO getCovRiskBudgets getCovRiskBudgets.fPFOLIOVAL getCovRiskBudgets.fPORTFOLIO getData getData.fPFOLIODATA getData.fPORTFOLIO getEstimator getEstimator.fPFOLIODATA getEstimator.fPFOLIOSPEC getEstimator.fPORTFOLIO getMean getMean.fPFOLIODATA getMean.fPORTFOLIO getMessages getMessages.fPFOLIOBACKTEST getMessages.fPFOLIOSPEC getModel.fPFOLIOSPEC getModel.fPORTFOLIO getMu getMu.fPFOLIODATA getMu.fPORTFOLIO getNAssets getNAssets.fPFOLIODATA getNAssets.fPORTFOLIO getNFrontierPoints getNFrontierPoints.fPFOLIOSPEC getNFrontierPoints.fPFOLIOVAL getNFrontierPoints.fPORTFOLIO getObjective getObjective.fPFOLIOSPEC getObjective.fPORTFOLIO getOptim getOptim.fPFOLIOSPEC getOptim.fPORTFOLIO getOptimize getOptimize.fPFOLIOSPEC getOptimize.fPORTFOLIO getOptions getOptions.fPFOLIOSPEC getOptions.fPORTFOLIO getParams getParams.fPFOLIOSPEC getParams.fPORTFOLIO getPortfolio getPortfolio.fPFOLIOSPEC getPortfolio.fPFOLIOVAL getPortfolio.fPORTFOLIO getRiskFreeRate getRiskFreeRate.fPFOLIOSPEC getRiskFreeRate.fPFOLIOVAL getRiskFreeRate.fPORTFOLIO getSeries getSeries.fPFOLIODATA getSeries.fPORTFOLIO getSigma getSigma.fPFOLIODATA getSigma.fPORTFOLIO getSmoother getSmoother.fPFOLIOBACKTEST getSmootherDoubleSmoothing getSmootherDoubleSmoothing.fPFOLIOBACKTEST getSmootherFun getSmootherFun.fPFOLIOBACKTEST getSmootherInitialWeights getSmootherInitialWeights.fPFOLIOBACKTEST getSmootherLambda getSmootherLambda.fPFOLIOBACKTEST getSmootherParams getSmootherParams.fPFOLIOBACKTEST getSmootherSkip getSmootherSkip.fPFOLIOBACKTEST getSolver getSolver.fPFOLIOSPEC getSolver.fPORTFOLIO getSpec getSpec.fPORTFOLIO getStatistics getStatistics.fPFOLIODATA getStatistics.fPORTFOLIO getStatus getStatus.fPFOLIOSPEC getStatus.fPFOLIOVAL getStatus.fPORTFOLIO getStrategy getStrategy.fPFOLIOBACKTEST getStrategyFun getStrategyFun.fPFOLIOBACKTEST getStrategyParams getStrategyParams.fPFOLIOBACKTEST getTailRisk getTailRisk.fPFOLIODATA getTailRisk.fPFOLIOSPEC getTailRisk.fPORTFOLIO getTailRiskBudgets getTailRiskBudgets.fPORTFOLIO getTargetReturn getTargetReturn.fPFOLIOSPEC getTargetReturn.fPFOLIOVAL getTargetReturn.fPORTFOLIO getTargetRisk getTargetRisk.fPFOLIOSPEC getTargetRisk.fPFOLIOVAL getTargetRisk.fPORTFOLIO getTrace getTrace.fPFOLIOSPEC getTrace.fPORTFOLIO getType getType.fPFOLIOSPEC getType.fPORTFOLIO getUnits.fPFOLIODATA getUnits.fPORTFOLIO getWeights getWeights.fPFOLIOSPEC getWeights.fPFOLIOVAL getWeights.fPORTFOLIO getWindows getWindows.fPFOLIOBACKTEST getWindowsFun getWindowsFun.fPFOLIOBACKTEST getWindowsHorizon getWindowsHorizon.fPFOLIOBACKTEST getWindowsParams getWindowsParams.fPFOLIOBACKTEST glpkLP glpkLPControl ipopQP ipopQPControl kendallEstimator kestrelQP kestrelQPControl lambdaCVaR listFConstraints lpmEstimator markowitzHull maxBConstraints maxBuyinConstraints maxCardConstraints maxddMap maxFConstraints maxratioPortfolio maxreturnPortfolio maxsumWConstraints maxWConstraints mcdEstimator minBConstraints minBuyinConstraints minCardConstraints minFConstraints minriskPortfolio minsumWConstraints minvariancePoints minvariancePortfolio minWConstraints modifiedVaR monteCarloPoints mveEstimator nCardConstraints neosLP neosLPControl neosQP neosQPControl netPerformance nlminb2 nlminb2Control nlminb2NLP nlminb2NLPControl nnveEstimator normalVaR parAnalytics pcoutAnalytics pfolioCVaR pfolioCVaRoptim pfolioCVaRplus pfolioHist pfolioMaxLoss pfolioReturn pfolioTargetReturn pfolioTargetRisk pfolioVaR plot.fPORTFOLIO portfolioBacktest portfolioBacktesting portfolioConstraints portfolioData portfolioFrontier portfolioObjective portfolioReturn portfolioRisk portfolioSmoothing portfolioSpec print.solver quadprogQP quadprogQPControl ramplLP ramplNLP ramplQP rglpkLP ripop ripopQP riskBudgetsPlot riskMap riskmetricsAnalytics rkestrelQP rneosLP rneosQP rnlminb2 rnlminb2NLP rollingCDaR rollingCmlPortfolio rollingCVaR rollingDaR rollingMinvariancePortfolio rollingPortfolioFrontier rollingSigma rollingTangencyPortfolio rollingVaR rollingWindows rquadprog rquadprogQP rsolnpNLP rsolveLP rsolveQP rsymphonyLP sampleCOV sampleVaR setAlpha<- setEstimator<- setNFrontierPoints<- setObjective<- setOptimize<- setParams<- setRiskFreeRate<- setSmootherDoubleSmoothing<- setSmootherFun<- setSmootherInitialWeights<- setSmootherLambda<- setSmootherParams<- setSmootherSkip<- setSolver<- setStatus<- setStrategyFun<- setStrategyParams<- setTailRisk<- setTargetReturn<- setTargetRisk<- setTrace<- setType<- setWeights<- setWindowsFun<- setWindowsHorizon<- setWindowsParams<- sharpeRatioLines shrinkEstimator singleAssetPoints slpmEstimator solnpNLP solnpNLPControl solveRampl.CVAR solveRampl.MV solveRglpk.CVAR solveRglpk.MAD solveRipop solveRquadprog solveRquadprog.CLA solveRshortExact solveRsocp solveRsolnp spearmanEstimator stabilityAnalytics summary.fPORTFOLIO symphonyLP symphonyLPControl tailoredFrontierPlot tailRiskBudgetsPie tailRiskBudgetsPlot tangencyLines tangencyPoints tangencyPortfolio tangencyStrategy ternaryCoord ternaryFrontier ternaryMap ternaryPoints ternaryWeights turnsAnalytics twoAssetsLines varRisk waveletSpectrum weightedReturnsLinePlot weightedReturnsPie weightedReturnsPlot weightsLinePlot weightsPie weightsPlot weightsSlider "
  },
  {
    "id": 433,
    "package_name": "daedalus",
    "title": "Model Health, Social, and Economic Costs of a Pandemic",
    "description": "Model the health, education, and economic costs of\ndirectly transmitted respiratory virus pandemics, under\ndifferent scenarios of prior vaccine investment, policy\ninterventions, and public behavioural change, using the\n'DAEDALUS' integrated health-economics model adapted from Haw\net al. (2022) <doi:10.1038/s43588-022-00233-0>.",
    "version": "0.3.2",
    "maintainer": "Pratik Gupte <p.gupte24@imperial.ac.uk>",
    "author": "Pratik Gupte [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-5294-7819>),\nPatrick Doohan [aut] (ORCID: <https://orcid.org/0000-0001-8076-1106>),\nRobert Johnson [aut] (ORCID: <https://orcid.org/0000-0002-7365-0042>),\nPablo Perez-Guzman [aut] (ORCID:\n<https://orcid.org/0000-0002-5277-5196>),\nRich FitzJohn [aut] (ORCID: <https://orcid.org/0000-0001-8888-3837>),\nEmma Russell [aut],\nDavid Mears [aut],\nKatharina Hauck [aut] (ORCID: <https://orcid.org/0000-0003-3138-4169>),\nAbdul Latif Jameel Institute for Disease and Emergency Analytics [fnd],\nImperial College of Science, Technology and Medicine [cph, fnd]",
    "url": "https://github.com/jameel-institute/daedalus,\nhttps://jameel-institute.github.io/daedalus/",
    "bug_reports": "https://github.com/jameel-institute/daedalus/issues",
    "repository": "",
    "exports": [
      [
        "daedalus"
      ],
      [
        "daedalus_country"
      ],
      [
        "daedalus_infection"
      ],
      [
        "daedalus_multi_infection"
      ],
      [
        "daedalus_new_behaviour"
      ],
      [
        "daedalus_npi"
      ],
      [
        "daedalus_old_behaviour"
      ],
      [
        "daedalus_timed_npi"
      ],
      [
        "daedalus_vaccination"
      ],
      [
        "get_costs"
      ],
      [
        "get_data"
      ],
      [
        "get_epidemic_summary"
      ],
      [
        "get_fiscal_costs"
      ],
      [
        "get_incidence"
      ],
      [
        "get_life_years_lost"
      ],
      [
        "get_new_vaccinations"
      ],
      [
        "is_daedalus_behaviour"
      ],
      [
        "is_daedalus_country"
      ],
      [
        "is_daedalus_infection"
      ],
      [
        "is_daedalus_npi"
      ],
      [
        "is_daedalus_vaccination"
      ],
      [
        "set_data"
      ]
    ],
    "topics": [
      [
        "decision-support"
      ],
      [
        "epidemiological-models"
      ],
      [
        "health-economics"
      ],
      [
        "pandemic-preparedness"
      ],
      [
        "public-health"
      ],
      [
        "rcpp"
      ],
      [
        "sdg-3"
      ],
      [
        "cpp"
      ],
      [
        "openmp"
      ]
    ],
    "score": 7.0527,
    "stars": 6,
    "primary_category": "epidemiology",
    "source_universe": "jameel-institute",
    "search_text": "daedalus Model Health, Social, and Economic Costs of a Pandemic Model the health, education, and economic costs of\ndirectly transmitted respiratory virus pandemics, under\ndifferent scenarios of prior vaccine investment, policy\ninterventions, and public behavioural change, using the\n'DAEDALUS' integrated health-economics model adapted from Haw\net al. (2022) <doi:10.1038/s43588-022-00233-0>. daedalus daedalus_country daedalus_infection daedalus_multi_infection daedalus_new_behaviour daedalus_npi daedalus_old_behaviour daedalus_timed_npi daedalus_vaccination get_costs get_data get_epidemic_summary get_fiscal_costs get_incidence get_life_years_lost get_new_vaccinations is_daedalus_behaviour is_daedalus_country is_daedalus_infection is_daedalus_npi is_daedalus_vaccination set_data decision-support epidemiological-models health-economics pandemic-preparedness public-health rcpp sdg-3 cpp openmp"
  },
  {
    "id": 1153,
    "package_name": "rsi",
    "title": "Efficiently Retrieve and Process Satellite Imagery",
    "description": "Downloads spatial data from spatiotemporal asset catalogs\n('STAC'), computes standard spectral indices from the Awesome\nSpectral Indices project (Montero et al. (2023)\n<doi:10.1038/s41597-023-02096-0>) against raster data, and\nglues the outputs together into predictor bricks. Methods focus\non interoperability with the broader spatial ecosystem;\nfunction arguments and outputs use classes from 'sf' and\n'terra', and data downloading functions support complex 'CQL2'\nqueries using 'rstac'.",
    "version": "0.3.2.9000",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Michael Mahoney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2402-304X>),\nFelipe Carvalho [rev] (Felipe reviewed the package (v. 0.3.0) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/636>),\nMichael Sumner [rev] (Michael reviewed the package (v. 0.3.0) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/636>),\nPermian Global [cph, fnd]",
    "url": "https://github.com/Permian-Global-Research/rsi,\nhttps://permian-global-research.github.io/rsi/",
    "bug_reports": "https://github.com/Permian-Global-Research/rsi/issues",
    "repository": "",
    "exports": [
      [
        "alos_palsar_mask_function"
      ],
      [
        "calculate_indices"
      ],
      [
        "default_query_function"
      ],
      [
        "filter_bands"
      ],
      [
        "filter_platforms"
      ],
      [
        "get_alos_palsar_imagery"
      ],
      [
        "get_dem"
      ],
      [
        "get_landsat_imagery"
      ],
      [
        "get_naip_imagery"
      ],
      [
        "get_sentinel1_imagery"
      ],
      [
        "get_sentinel2_imagery"
      ],
      [
        "get_stac_data"
      ],
      [
        "landsat_mask_function"
      ],
      [
        "landsat_platform_filter"
      ],
      [
        "rsi_download_rasters"
      ],
      [
        "rsi_gdal_config_options"
      ],
      [
        "rsi_gdalwarp_options"
      ],
      [
        "rsi_query_api"
      ],
      [
        "sentinel2_mask_function"
      ],
      [
        "sign_planetary_computer"
      ],
      [
        "spectral_indices"
      ],
      [
        "spectral_indices_url"
      ],
      [
        "stack_rasters"
      ]
    ],
    "topics": [],
    "score": 6.7404,
    "stars": 55,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rsi Efficiently Retrieve and Process Satellite Imagery Downloads spatial data from spatiotemporal asset catalogs\n('STAC'), computes standard spectral indices from the Awesome\nSpectral Indices project (Montero et al. (2023)\n<doi:10.1038/s41597-023-02096-0>) against raster data, and\nglues the outputs together into predictor bricks. Methods focus\non interoperability with the broader spatial ecosystem;\nfunction arguments and outputs use classes from 'sf' and\n'terra', and data downloading functions support complex 'CQL2'\nqueries using 'rstac'. alos_palsar_mask_function calculate_indices default_query_function filter_bands filter_platforms get_alos_palsar_imagery get_dem get_landsat_imagery get_naip_imagery get_sentinel1_imagery get_sentinel2_imagery get_stac_data landsat_mask_function landsat_platform_filter rsi_download_rasters rsi_gdal_config_options rsi_gdalwarp_options rsi_query_api sentinel2_mask_function sign_planetary_computer spectral_indices spectral_indices_url stack_rasters "
  },
  {
    "id": 558,
    "package_name": "fUnitRoots",
    "title": "Rmetrics - Modelling Trends and Unit Roots",
    "description": "Provides four addons for analyzing trends and unit roots\nin financial time series: (i) functions for the density and\nprobability of the augmented Dickey-Fuller Test, (ii) functions\nfor the density and probability of MacKinnon's unit root test\nstatistics, (iii) reimplementations for the ADF and MacKinnon\nTest, and (iv) an 'urca' Unit Root Test Interface for Pfaff's\nunit root test suite.",
    "version": "4040.81",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\nTobias Setz [aut],\nYohan Chalabi [aut],\nGeorgi N. Boshnakov [cre] (ORCID:\n<https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://r-forge.r-project.org/scm/viewvc.php/pkg/fUnitRoots/?root=rmetrics\n(devel), https://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/projects/rmetrics",
    "repository": "",
    "exports": [
      [
        "adfTable"
      ],
      [
        "adfTest"
      ],
      [
        "padf"
      ],
      [
        "punitroot"
      ],
      [
        "qadf"
      ],
      [
        "qunitroot"
      ],
      [
        "unitrootTable"
      ],
      [
        "unitrootTest"
      ],
      [
        "urdfTest"
      ],
      [
        "urersTest"
      ],
      [
        "urkpssTest"
      ],
      [
        "urppTest"
      ],
      [
        "urspTest"
      ],
      [
        "urzaTest"
      ]
    ],
    "topics": [
      [
        "fortran"
      ]
    ],
    "score": 6.733,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "fUnitRoots Rmetrics - Modelling Trends and Unit Roots Provides four addons for analyzing trends and unit roots\nin financial time series: (i) functions for the density and\nprobability of the augmented Dickey-Fuller Test, (ii) functions\nfor the density and probability of MacKinnon's unit root test\nstatistics, (iii) reimplementations for the ADF and MacKinnon\nTest, and (iv) an 'urca' Unit Root Test Interface for Pfaff's\nunit root test suite. adfTable adfTest padf punitroot qadf qunitroot unitrootTable unitrootTest urdfTest urersTest urkpssTest urppTest urspTest urzaTest fortran"
  },
  {
    "id": 384,
    "package_name": "coder",
    "title": "Deterministic Categorization of Items Based on External Code\nData",
    "description": "Fast categorization of items based on external code data\nidentified by regular expressions. A typical use case considers\npatient with medically coded data, such as codes from the\nInternational Classification of Diseases ('ICD') or the\nAnatomic Therapeutic Chemical ('ATC') classification system.\nFunctions of the package relies on a triad of objects: (1) case\ndata with unit id:s and possible dates of interest; (2)\nexternal code data for corresponding units in (1) and with\noptional dates of interest and; (3) a classification scheme\n('classcodes' object) with regular expressions to identify and\ncategorize relevant codes from (2). It is easy to introduce new\nclassification schemes ('classcodes' objects) or to use default\nschemes included in the package. Use cases includes patient\ncategorization based on 'comorbidity indices' such as\n'Charlson', 'Elixhauser', 'RxRisk V', or the\n'comorbidity-polypharmacy' score (CPS), as well as adverse\nevents after hip and knee replacement surgery.",
    "version": "1.0",
    "maintainer": "Erik Bulow <eriklgb@gmail.com>",
    "author": "Erik Bulow [aut, cre] (ORCID: <https://orcid.org/0000-0002-9973-456X>),\nEmely C Zabore [rev] (Emily reviewed the package (v. 0.12.1) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/381>),\nDavid Robinson [rev] (David reviewed the package (v. 0.12.1) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/381>)",
    "url": "https://docs.ropensci.org/coder/",
    "bug_reports": "https://github.com/ropensci/coder/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "all_classcodes"
      ],
      [
        "as.classcodes"
      ],
      [
        "categorize"
      ],
      [
        "classify"
      ],
      [
        "codebook"
      ],
      [
        "codebooks"
      ],
      [
        "codify"
      ],
      [
        "index"
      ],
      [
        "is.classcodes"
      ],
      [
        "set_classcodes"
      ],
      [
        "visualize"
      ]
    ],
    "topics": [
      [
        "classification"
      ],
      [
        "icd-10"
      ]
    ],
    "score": 6.4216,
    "stars": 22,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "coder Deterministic Categorization of Items Based on External Code\nData Fast categorization of items based on external code data\nidentified by regular expressions. A typical use case considers\npatient with medically coded data, such as codes from the\nInternational Classification of Diseases ('ICD') or the\nAnatomic Therapeutic Chemical ('ATC') classification system.\nFunctions of the package relies on a triad of objects: (1) case\ndata with unit id:s and possible dates of interest; (2)\nexternal code data for corresponding units in (1) and with\noptional dates of interest and; (3) a classification scheme\n('classcodes' object) with regular expressions to identify and\ncategorize relevant codes from (2). It is easy to introduce new\nclassification schemes ('classcodes' objects) or to use default\nschemes included in the package. Use cases includes patient\ncategorization based on 'comorbidity indices' such as\n'Charlson', 'Elixhauser', 'RxRisk V', or the\n'comorbidity-polypharmacy' score (CPS), as well as adverse\nevents after hip and knee replacement surgery. %>% all_classcodes as.classcodes categorize classify codebook codebooks codify index is.classcodes set_classcodes visualize classification icd-10"
  },
  {
    "id": 237,
    "package_name": "alphavantager",
    "title": "Lightweight Interface to the Alpha Vantage API",
    "description": "Alpha Vantage has free historical financial information.\nAll you need to do is get a free API key at\n<https://www.alphavantage.co>. Then you can use the R interface\nto retrieve free equity information. Refer to the Alpha Vantage\nwebsite for more information.",
    "version": "0.1.3",
    "maintainer": "Matt Dancho <mdancho@business-science.io>",
    "author": "Matt Dancho [aut, cre],\nDavis Vaughan [aut]",
    "url": "https://github.com/business-science/alphavantager",
    "bug_reports": "https://github.com/business-science/alphavantager/issues",
    "repository": "",
    "exports": [
      [
        "av_api_key"
      ],
      [
        "av_get"
      ]
    ],
    "topics": [
      [
        "alpha-vantage"
      ],
      [
        "financial-data"
      ],
      [
        "historical-financial-data"
      ]
    ],
    "score": 6.3423,
    "stars": 71,
    "primary_category": "tidyverse",
    "source_universe": "business-science",
    "search_text": "alphavantager Lightweight Interface to the Alpha Vantage API Alpha Vantage has free historical financial information.\nAll you need to do is get a free API key at\n<https://www.alphavantage.co>. Then you can use the R interface\nto retrieve free equity information. Refer to the Alpha Vantage\nwebsite for more information. av_api_key av_get alpha-vantage financial-data historical-financial-data"
  },
  {
    "id": 552,
    "package_name": "fAssets",
    "title": "Rmetrics - Analysing and Modelling Financial Assets",
    "description": "A collection of functions to manage, to investigate and to\nanalyze data sets of financial assets from different points of\nview.",
    "version": "4023.85.9000",
    "maintainer": "Stefan Theussl <Stefan.Theussl@R-project.org>",
    "author": "Diethelm Wuertz [aut],\nTobias Setz [aut],\nYohan Chalabi [aut],\nStefan Theussl [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6523-4620>)",
    "url": "https://r-forge.r-project.org/projects/rmetrics/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        ".baggedMeanCov"
      ],
      [
        ".bayesSteinMeanCov"
      ],
      [
        ".cov.arw"
      ],
      [
        ".cov.nnve"
      ],
      [
        ".cov.shrink"
      ],
      [
        ".donostahMeanCov"
      ],
      [
        ".ledoitWolfMeanCov"
      ],
      [
        ".rmtMeanCov"
      ],
      [
        ".studentMeanCov"
      ],
      [
        "abcArrange"
      ],
      [
        "assetsArrange"
      ],
      [
        "assetsBasicStatsPlot"
      ],
      [
        "assetsBoxPercentilePlot"
      ],
      [
        "assetsBoxPlot"
      ],
      [
        "assetsBoxStatsPlot"
      ],
      [
        "assetsCorEigenPlot"
      ],
      [
        "assetsCorgramPlot"
      ],
      [
        "assetsCorImagePlot"
      ],
      [
        "assetsCorTestPlot"
      ],
      [
        "assetsCumulatedPlot"
      ],
      [
        "assetsDendrogramPlot"
      ],
      [
        "assetsDist"
      ],
      [
        "assetsFit"
      ],
      [
        "assetsHistPairsPlot"
      ],
      [
        "assetsHistPlot"
      ],
      [
        "assetsLogDensityPlot"
      ],
      [
        "assetsLPM"
      ],
      [
        "assetsMeanCov"
      ],
      [
        "assetsMomentsPlot"
      ],
      [
        "assetsNIGFitPlot"
      ],
      [
        "assetsNIGShapeTrianglePlot"
      ],
      [
        "assetsOutliers"
      ],
      [
        "assetsPairsPlot"
      ],
      [
        "assetsQQNormPlot"
      ],
      [
        "assetsReturnPlot"
      ],
      [
        "assetsRiskReturnPlot"
      ],
      [
        "assetsSelect"
      ],
      [
        "assetsSeriesPlot"
      ],
      [
        "assetsSim"
      ],
      [
        "assetsSLPM"
      ],
      [
        "assetsStarsPlot"
      ],
      [
        "assetsTest"
      ],
      [
        "assetsTreePlot"
      ],
      [
        "binaryDist"
      ],
      [
        "braycurtisDist"
      ],
      [
        "canberraDist"
      ],
      [
        "corDist"
      ],
      [
        "covEllipsesPlot"
      ],
      [
        "euclideanDist"
      ],
      [
        "getCenterRob"
      ],
      [
        "getCovRob"
      ],
      [
        "hclustArrange"
      ],
      [
        "jaccardDist"
      ],
      [
        "kendallDist"
      ],
      [
        "mahalanobisDist"
      ],
      [
        "manhattanDist"
      ],
      [
        "maximumDist"
      ],
      [
        "minkowskiDist"
      ],
      [
        "mutinfoDist"
      ],
      [
        "mvenergyTest"
      ],
      [
        "mvshapiroTest"
      ],
      [
        "orderArrange"
      ],
      [
        "pcaArrange"
      ],
      [
        "sampleArrange"
      ],
      [
        "sorensenDist"
      ],
      [
        "spearmanDist"
      ],
      [
        "statsArrange"
      ]
    ],
    "topics": [],
    "score": 6.3138,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "fAssets Rmetrics - Analysing and Modelling Financial Assets A collection of functions to manage, to investigate and to\nanalyze data sets of financial assets from different points of\nview. .baggedMeanCov .bayesSteinMeanCov .cov.arw .cov.nnve .cov.shrink .donostahMeanCov .ledoitWolfMeanCov .rmtMeanCov .studentMeanCov abcArrange assetsArrange assetsBasicStatsPlot assetsBoxPercentilePlot assetsBoxPlot assetsBoxStatsPlot assetsCorEigenPlot assetsCorgramPlot assetsCorImagePlot assetsCorTestPlot assetsCumulatedPlot assetsDendrogramPlot assetsDist assetsFit assetsHistPairsPlot assetsHistPlot assetsLogDensityPlot assetsLPM assetsMeanCov assetsMomentsPlot assetsNIGFitPlot assetsNIGShapeTrianglePlot assetsOutliers assetsPairsPlot assetsQQNormPlot assetsReturnPlot assetsRiskReturnPlot assetsSelect assetsSeriesPlot assetsSim assetsSLPM assetsStarsPlot assetsTest assetsTreePlot binaryDist braycurtisDist canberraDist corDist covEllipsesPlot euclideanDist getCenterRob getCovRob hclustArrange jaccardDist kendallDist mahalanobisDist manhattanDist maximumDist minkowskiDist mutinfoDist mvenergyTest mvshapiroTest orderArrange pcaArrange sampleArrange sorensenDist spearmanDist statsArrange "
  },
  {
    "id": 1111,
    "package_name": "riingo",
    "title": "An R Interface to the 'Tiingo' Stock Price API",
    "description": "Functionality to download stock prices, cryptocurrency\ndata, and more from the 'Tiingo' API <https://api.tiingo.com/>.",
    "version": "0.3.1.9000",
    "maintainer": "Davis Vaughan <davis@rstudio.com>",
    "author": "Davis Vaughan [aut, cre],\nMatt Dancho [aut]",
    "url": "https://github.com/business-science/riingo",
    "bug_reports": "https://github.com/business-science/riingo/issues",
    "repository": "",
    "exports": [
      [
        "convert_to_local_time"
      ],
      [
        "is_supported_ticker"
      ],
      [
        "riingo_browse_documentation"
      ],
      [
        "riingo_browse_signup"
      ],
      [
        "riingo_browse_token"
      ],
      [
        "riingo_browse_usage"
      ],
      [
        "riingo_crypto_latest"
      ],
      [
        "riingo_crypto_meta"
      ],
      [
        "riingo_crypto_prices"
      ],
      [
        "riingo_crypto_quote"
      ],
      [
        "riingo_fundamentals_definitions"
      ],
      [
        "riingo_fundamentals_meta"
      ],
      [
        "riingo_fundamentals_metrics"
      ],
      [
        "riingo_fundamentals_statements"
      ],
      [
        "riingo_fx_prices"
      ],
      [
        "riingo_fx_quote"
      ],
      [
        "riingo_get_token"
      ],
      [
        "riingo_iex_latest"
      ],
      [
        "riingo_iex_prices"
      ],
      [
        "riingo_iex_quote"
      ],
      [
        "riingo_latest"
      ],
      [
        "riingo_meta"
      ],
      [
        "riingo_news"
      ],
      [
        "riingo_prices"
      ],
      [
        "riingo_set_token"
      ],
      [
        "supported_tickers"
      ]
    ],
    "topics": [],
    "score": 5.7715,
    "stars": 52,
    "primary_category": "tidyverse",
    "source_universe": "business-science",
    "search_text": "riingo An R Interface to the 'Tiingo' Stock Price API Functionality to download stock prices, cryptocurrency\ndata, and more from the 'Tiingo' API <https://api.tiingo.com/>. convert_to_local_time is_supported_ticker riingo_browse_documentation riingo_browse_signup riingo_browse_token riingo_browse_usage riingo_crypto_latest riingo_crypto_meta riingo_crypto_prices riingo_crypto_quote riingo_fundamentals_definitions riingo_fundamentals_meta riingo_fundamentals_metrics riingo_fundamentals_statements riingo_fx_prices riingo_fx_quote riingo_get_token riingo_iex_latest riingo_iex_prices riingo_iex_quote riingo_latest riingo_meta riingo_news riingo_prices riingo_set_token supported_tickers "
  },
  {
    "id": 555,
    "package_name": "fImport",
    "title": "Rmetrics - Importing Economic and Financial Data",
    "description": "Provides a collection of utility functions to download and\nmanage data sets from the Internet or from other sources.",
    "version": "4041.88",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\nTobias Setz [aut],\nYohan Chalabi [aut],\nGeorgi N. Boshnakov [cre, aut]",
    "url": "https://r-forge.r-project.org/scm/viewvc.php/pkg/fImport/?root=rmetrics\n(devel), https://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/projects/rmetrics",
    "repository": "",
    "exports": [
      [
        "charvecSplit"
      ],
      [
        "composeURL"
      ],
      [
        "dataSplit"
      ],
      [
        "fredImport"
      ],
      [
        "fredSeries"
      ],
      [
        "indexGrep"
      ],
      [
        "read.lines"
      ],
      [
        "read.links"
      ],
      [
        "read.lynx"
      ],
      [
        "read.w3m"
      ],
      [
        "show"
      ],
      [
        "stringSplit"
      ]
    ],
    "topics": [],
    "score": 5.6693,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "fImport Rmetrics - Importing Economic and Financial Data Provides a collection of utility functions to download and\nmanage data sets from the Internet or from other sources. charvecSplit composeURL dataSplit fredImport fredSeries indexGrep read.lines read.links read.lynx read.w3m show stringSplit "
  },
  {
    "id": 891,
    "package_name": "nser",
    "title": "Bhavcopy and Live Market Data from National Stock Exchange (NSE)\n& Bombay Stock Exchange (BSE) India",
    "description": "Download Current & Historical Bhavcopy. Get Live Market\ndata from NSE India of Equities and Derivatives (F&O) segment.\nData source <https://www.nseindia.com/>.",
    "version": "1.5.7",
    "maintainer": "Nandan Patil <tryanother609@gmail.com>",
    "author": "Nandan Patil [cre, aut]",
    "url": "https://github.com/nandp1/nser/",
    "bug_reports": "https://github.com/nandp1/nser/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "bhav"
      ],
      [
        "bhav1"
      ],
      [
        "bhavpr"
      ],
      [
        "bhavtoday"
      ],
      [
        "daytomonth"
      ],
      [
        "daytoweek"
      ],
      [
        "fdii"
      ],
      [
        "fobhav"
      ],
      [
        "fobhav1"
      ],
      [
        "fobhavtoday"
      ],
      [
        "nseindex"
      ],
      [
        "nseipo"
      ],
      [
        "nselive"
      ],
      [
        "nseopen"
      ],
      [
        "nsetree"
      ],
      [
        "optbanknifty"
      ],
      [
        "optnifty"
      ]
    ],
    "topics": [],
    "score": 5.4771,
    "stars": 2,
    "primary_category": "general",
    "source_universe": "cran",
    "search_text": "nser Bhavcopy and Live Market Data from National Stock Exchange (NSE)\n& Bombay Stock Exchange (BSE) India Download Current & Historical Bhavcopy. Get Live Market\ndata from NSE India of Equities and Derivatives (F&O) segment.\nData source <https://www.nseindia.com/>. %>% bhav bhav1 bhavpr bhavtoday daytomonth daytoweek fdii fobhav fobhav1 fobhavtoday nseindex nseipo nselive nseopen nsetree optbanknifty optnifty "
  },
  {
    "id": 53,
    "package_name": "FFD",
    "title": "Freedom from Disease",
    "description": "Functions, S4 classes/methods and a graphical user\ninterface (GUI) to design surveys to substantiate freedom from\ndisease using a modified hypergeometric function (see Cameron\nand Baldock, 1997, <doi:10.1016/s0167-5877(97)00081-0>). Herd\nsensitivities are computed according to sampling strategies\n\"individual sampling\" or \"limited sampling\" (see M. Ziller, T.\nSelhorst, J. Teuffert, M. Kramer and H. Schlueter, 2002,\n<doi:10.1016/S0167-5877(01)00245-8>). Methods to compute the\na-posteriori alpha-error are implemented. Risk-based targeted\nsampling is supported.",
    "version": "1.0-9",
    "maintainer": "Ian Kopacka <ian.kopacka@ages.at>",
    "author": "Ian Kopacka",
    "url": "http://ffd.r-forge.r-project.org, https://www.ages.at/startseite/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "computeAlpha"
      ],
      [
        "computeAlphaLimitedSampling"
      ],
      [
        "computeAposterioriError"
      ],
      [
        "computeAposterioriErrorRiskGroups"
      ],
      [
        "computeOptimalSampleSize"
      ],
      [
        "computeOptimalSampleSizeRiskGroups"
      ],
      [
        "computePValue"
      ],
      [
        "computePValueRiskGroups"
      ],
      [
        "FFD_GUI"
      ],
      [
        "HTML"
      ],
      [
        "indSampling"
      ],
      [
        "indSamplingSummary"
      ],
      [
        "lls"
      ],
      [
        "ltdSampling"
      ],
      [
        "ltdSamplingSummary"
      ],
      [
        "plot"
      ],
      [
        "sample"
      ],
      [
        "show"
      ],
      [
        "summary"
      ],
      [
        "surveyData"
      ]
    ],
    "topics": [],
    "score": 4.2041,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "FFD Freedom from Disease Functions, S4 classes/methods and a graphical user\ninterface (GUI) to design surveys to substantiate freedom from\ndisease using a modified hypergeometric function (see Cameron\nand Baldock, 1997, <doi:10.1016/s0167-5877(97)00081-0>). Herd\nsensitivities are computed according to sampling strategies\n\"individual sampling\" or \"limited sampling\" (see M. Ziller, T.\nSelhorst, J. Teuffert, M. Kramer and H. Schlueter, 2002,\n<doi:10.1016/S0167-5877(01)00245-8>). Methods to compute the\na-posteriori alpha-error are implemented. Risk-based targeted\nsampling is supported. computeAlpha computeAlphaLimitedSampling computeAposterioriError computeAposterioriErrorRiskGroups computeOptimalSampleSize computeOptimalSampleSizeRiskGroups computePValue computePValueRiskGroups FFD_GUI HTML indSampling indSamplingSummary lls ltdSampling ltdSamplingSummary plot sample show summary surveyData "
  },
  {
    "id": 1052,
    "package_name": "ramlegacy",
    "title": "Download and Read RAM Legacy Stock Assessment Database",
    "description": "Contains functions to download, cache and read in 'Excel'\nversion of the RAM Legacy Stock Assessment Data Base, an online\ncompilation of stock assessment results for commercially\nexploited marine populations from around the world. The\ndatabase is named after Dr. Ransom A. Myers whose original\nstock-recruitment database, is no longer being updated. More\ninformation about the database can be found at\n<https://ramlegacy.org/>. Ricard, D., Minto, C., Jensen, O.P.\nand Baum, J.K. (2012) <doi:10.1111/j.1467-2979.2011.00435.x>.",
    "version": "0.2.0",
    "maintainer": "Kshitiz Gupta <kshtzgupta1@berkeley.edu>",
    "author": "Carl Boettiger [aut, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nKshitiz Gupta [aut, cre, cph],\nSam Albers [rev] (ORCID: <https://orcid.org/0000-0002-9270-7884>),\nJamie Afflerbach [rev] (ORCID: <https://orcid.org/0000-0002-5215-9342>),\nRAM Legacy Stock Assessment Database [dtc]",
    "url": "https://docs.ropensci.org/ramlegacy,\nhttps://github.com/ropensci/ramlegacy",
    "bug_reports": "https://github.com/ropensci/ramlegacy/issues",
    "repository": "",
    "exports": [
      [
        "download_ramlegacy"
      ],
      [
        "load_ramlegacy"
      ],
      [
        "ram_dir"
      ]
    ],
    "topics": [
      [
        "fisheries"
      ],
      [
        "marine-biology"
      ],
      [
        "ramlegacy"
      ],
      [
        "ropensci"
      ],
      [
        "stock-assessment"
      ]
    ],
    "score": 4.1761,
    "stars": 5,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ramlegacy Download and Read RAM Legacy Stock Assessment Database Contains functions to download, cache and read in 'Excel'\nversion of the RAM Legacy Stock Assessment Data Base, an online\ncompilation of stock assessment results for commercially\nexploited marine populations from around the world. The\ndatabase is named after Dr. Ransom A. Myers whose original\nstock-recruitment database, is no longer being updated. More\ninformation about the database can be found at\n<https://ramlegacy.org/>. Ricard, D., Minto, C., Jensen, O.P.\nand Baum, J.K. (2012) <doi:10.1111/j.1467-2979.2011.00435.x>. download_ramlegacy load_ramlegacy ram_dir fisheries marine-biology ramlegacy ropensci stock-assessment"
  },
  {
    "id": 111,
    "package_name": "PST",
    "title": "Probabilistic Suffix Trees and Variable Length Markov Chains",
    "description": "Provides a framework for analysing state sequences with\nprobabilistic suffix trees (PST), the construction that stores\nvariable length Markov chains (VLMC). Besides functions for\nlearning and optimizing VLMC models, the PST library includes\nmany additional tools to analyse sequence data with these\nmodels: visualization tools, functions for sequence prediction\nand artificial sequences generation, as well as for context and\npattern mining. The package is specifically adapted to the\nfield of social sciences by allowing to learn VLMC models from\nsets of individual sequences possibly containing missing\nvalues, and by accounting for case weights. The library also\nallows to compute probabilistic divergence between two models,\nand to fit segmented VLMC, where sub-models fitted to distinct\nstrata of the learning sample are stored in a single PST. This\nsoftware results from research work executed within the\nframework of the Swiss National Centre of Competence in\nResearch LIVES, which is financed by the Swiss National Science\nFoundation. The authors are grateful to the Swiss National\nScience Foundation for its financial support.",
    "version": "0.95",
    "maintainer": "Alexis Gabadinho <alexis.gabadinho@wanadoo.fr>",
    "author": "Alexis Gabadinho [aut, cre, cph]",
    "url": "http://r-forge.r-project.org/projects/pst",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "cmine"
      ],
      [
        "cplot"
      ],
      [
        "cprob"
      ],
      [
        "generate"
      ],
      [
        "impute"
      ],
      [
        "logLik"
      ],
      [
        "nobs"
      ],
      [
        "nodenames"
      ],
      [
        "pdist"
      ],
      [
        "plot"
      ],
      [
        "pmine"
      ],
      [
        "ppplot"
      ],
      [
        "pqplot"
      ],
      [
        "predict"
      ],
      [
        "print"
      ],
      [
        "prune"
      ],
      [
        "pstree"
      ],
      [
        "query"
      ],
      [
        "subtree"
      ],
      [
        "summary"
      ],
      [
        "tune"
      ]
    ],
    "topics": [],
    "score": 3.5563,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "PST Probabilistic Suffix Trees and Variable Length Markov Chains Provides a framework for analysing state sequences with\nprobabilistic suffix trees (PST), the construction that stores\nvariable length Markov chains (VLMC). Besides functions for\nlearning and optimizing VLMC models, the PST library includes\nmany additional tools to analyse sequence data with these\nmodels: visualization tools, functions for sequence prediction\nand artificial sequences generation, as well as for context and\npattern mining. The package is specifically adapted to the\nfield of social sciences by allowing to learn VLMC models from\nsets of individual sequences possibly containing missing\nvalues, and by accounting for case weights. The library also\nallows to compute probabilistic divergence between two models,\nand to fit segmented VLMC, where sub-models fitted to distinct\nstrata of the learning sample are stored in a single PST. This\nsoftware results from research work executed within the\nframework of the Swiss National Centre of Competence in\nResearch LIVES, which is financed by the Swiss National Science\nFoundation. The authors are grateful to the Swiss National\nScience Foundation for its financial support. cmine cplot cprob generate impute logLik nobs nodenames pdist plot pmine ppplot pqplot predict print prune pstree query subtree summary tune "
  },
  {
    "id": 22,
    "package_name": "Colossus",
    "title": "\"Risk Model Regression and Analysis with Complex Non-Linear\nModels\"",
    "description": "Performs survival analysis using general non-linear models. Risk models can be the sum or product of terms. Each term is the product of exponential/linear functions of covariates. Additionally sub-terms can be defined as a sum of exponential, linear threshold, and step functions. Cox Proportional hazards <https://en.wikipedia.org/wiki/Proportional_hazards_model>, Poisson <https://en.wikipedia.org/wiki/Poisson_regression>, and Fine-Gray competing risks <https://www.publichealth.columbia.edu/research/population-health-methods/competing-risk-analysis> regression are supported. This work was sponsored by NASA Grants 80NSSC19M0161 and 80NSSC23M0129 through a subcontract from the National Council on Radiation Protection and Measurements (NCRP). The computing for this project was performed on the Beocat Research Cluster at Kansas State University, which is funded in part by NSF grants CNS-1006860, EPS-1006860, EPS-0919443, ACI-1440548, CHE-1726332, and NIH P20GM113109.",
    "version": "1.4.6",
    "maintainer": "Eric Giunta <egiunta@ksu.edu>",
    "author": "Eric Giunta [aut, cre] (ORCID: <https://orcid.org/0000-0002-1577-766X>),\n  Amir Bahadori [ctb] (ORCID: <https://orcid.org/0000-0002-4589-105X>),\n  Dan Andresen [ctb] (ORCID: <https://orcid.org/0000-0003-2345-6695>),\n  Linda Walsh [ctb] (ORCID: <https://orcid.org/0000-0001-7399-9191>),\n  Benjamin French [ctb] (ORCID: <https://orcid.org/0000-0001-9265-5378>),\n  Lawrence Dauer [ctb] (ORCID: <https://orcid.org/0000-0002-5629-8462>),\n  John Boice Jr [ctb] (ORCID: <https://orcid.org/0000-0002-8755-1299>),\n  Kansas State University [cph],\n  NASA [fnd],\n  NCRP [fnd],\n  NRC [fnd]",
    "url": "https://ericgiunta.github.io/Colossus/,\nhttps://github.com/ericgiunta/Colossus",
    "bug_reports": "https://github.com/ericgiunta/Colossus/issues",
    "repository": "https://cran.r-project.org/package=Colossus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Colossus \"Risk Model Regression and Analysis with Complex Non-Linear\nModels\" Performs survival analysis using general non-linear models. Risk models can be the sum or product of terms. Each term is the product of exponential/linear functions of covariates. Additionally sub-terms can be defined as a sum of exponential, linear threshold, and step functions. Cox Proportional hazards <https://en.wikipedia.org/wiki/Proportional_hazards_model>, Poisson <https://en.wikipedia.org/wiki/Poisson_regression>, and Fine-Gray competing risks <https://www.publichealth.columbia.edu/research/population-health-methods/competing-risk-analysis> regression are supported. This work was sponsored by NASA Grants 80NSSC19M0161 and 80NSSC23M0129 through a subcontract from the National Council on Radiation Protection and Measurements (NCRP). The computing for this project was performed on the Beocat Research Cluster at Kansas State University, which is funded in part by NSF grants CNS-1006860, EPS-1006860, EPS-0919443, ACI-1440548, CHE-1726332, and NIH P20GM113109.  "
  },
  {
    "id": 27,
    "package_name": "CoxBoost",
    "title": "Cox Models by Likelihood Based Boosting for a Single Survival\nEndpoint or Competing Risks",
    "description": "Provides routines for fitting Cox models by likelihood based\n    boosting for single event survival data with right censoring or in the\n    presence of competing risks. The methodology is described in Binder\n    and Schumacher (2008) <doi:10.1186/1471-2105-9-14> and Binder et al.\n    (2009) <doi:10.1093/bioinformatics/btp088>.",
    "version": "1.5.1",
    "maintainer": "John Zobolas <bblodfon@gmail.com>",
    "author": "John Zobolas [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-3609-8674>),\n  Harald Binder [aut] (ORCID: <https://orcid.org/0000-0002-5666-8662>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CoxBoost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CoxBoost Cox Models by Likelihood Based Boosting for a Single Survival\nEndpoint or Competing Risks Provides routines for fitting Cox models by likelihood based\n    boosting for single event survival data with right censoring or in the\n    presence of competing risks. The methodology is described in Binder\n    and Schumacher (2008) <doi:10.1186/1471-2105-9-14> and Binder et al.\n    (2009) <doi:10.1093/bioinformatics/btp088>.  "
  },
  {
    "id": 106,
    "package_name": "PINstimation",
    "title": "Estimation of the Probability of Informed Trading",
    "description": "A comprehensive bundle of utilities for the estimation of probability of informed trading models: original PIN in Easley and O'Hara (1992) and Easley et al. (1996); Multilayer PIN (MPIN) in Ersan (2016); Adjusted PIN (AdjPIN) in Duarte and Young (2009); and volume-synchronized PIN (VPIN) in Easley et al. (2011, 2012). Implementations of various estimation methods suggested in the literature are included. Additional compelling features comprise posterior probabilities, an implementation of an expectation-maximization (EM) algorithm, and PIN decomposition into layers, and into bad/good components. Versatile data simulation tools, and trade classification algorithms are among the supplementary utilities. The package provides fast, compact, and precise utilities to tackle the sophisticated, error-prone, and time-consuming estimation procedure of informed trading, and this solely using the raw trade-level data. ",
    "version": "0.2.0",
    "maintainer": "Montasser Ghachem <montasser.ghachem@pinstimation.com>",
    "author": "Montasser Ghachem [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6991-3316>),\n  Oguz Ersan [aut] (ORCID: <https://orcid.org/0000-0003-3135-5317>),\n  Alexandre Borentain [ctb]",
    "url": "https://www.pinstimation.com,\nhttps://github.com/monty-se/PINstimation",
    "bug_reports": "https://github.com/monty-se/PINstimation/issues",
    "repository": "https://cran.r-project.org/package=PINstimation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PINstimation Estimation of the Probability of Informed Trading A comprehensive bundle of utilities for the estimation of probability of informed trading models: original PIN in Easley and O'Hara (1992) and Easley et al. (1996); Multilayer PIN (MPIN) in Ersan (2016); Adjusted PIN (AdjPIN) in Duarte and Young (2009); and volume-synchronized PIN (VPIN) in Easley et al. (2011, 2012). Implementations of various estimation methods suggested in the literature are included. Additional compelling features comprise posterior probabilities, an implementation of an expectation-maximization (EM) algorithm, and PIN decomposition into layers, and into bad/good components. Versatile data simulation tools, and trade classification algorithms are among the supplementary utilities. The package provides fast, compact, and precise utilities to tackle the sophisticated, error-prone, and time-consuming estimation procedure of informed trading, and this solely using the raw trade-level data.   "
  },
  {
    "id": 107,
    "package_name": "PMwR",
    "title": "Portfolio Management with R",
    "description": "Tools for the practical management of financial\n  portfolios: backtesting investment and trading strategies,\n  computing profit/loss and returns, analysing trades,\n  handling lists of transactions, reporting, and more.  The\n  package provides a small set of reliable, efficient and\n  convenient tools for processing and analysing\n  trade/portfolio data.  The manual provides all the details;\n  it is available from\n  <https://enricoschumann.net/R/packages/PMwR/manual/PMwR.html>.\n  Examples and descriptions of new features are provided at\n  <https://enricoschumann.net/notes/PMwR/>.",
    "version": "1.2-0",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/PMwR/ ,\nhttps://git.sr.ht/~enricoschumann/PMwR ,\nhttps://gitlab.com/enricoschumann/PMwR ,\nhttps://github.com/enricoschumann/PMwR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PMwR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PMwR Portfolio Management with R Tools for the practical management of financial\n  portfolios: backtesting investment and trading strategies,\n  computing profit/loss and returns, analysing trades,\n  handling lists of transactions, reporting, and more.  The\n  package provides a small set of reliable, efficient and\n  convenient tools for processing and analysing\n  trade/portfolio data.  The manual provides all the details;\n  it is available from\n  <https://enricoschumann.net/R/packages/PMwR/manual/PMwR.html>.\n  Examples and descriptions of new features are provided at\n  <https://enricoschumann.net/notes/PMwR/>.  "
  },
  {
    "id": 120,
    "package_name": "ProbBreed",
    "title": "Probability Theory for Selecting Candidates in Plant Breeding",
    "description": "Use probability theory under the Bayesian framework for calculating the risk of selecting candidates in a multi-environment context. Contained are functions used to fit a Bayesian multi-environment model (based on the available presets), extract posterior values and maximum posterior values, compute the variance components, check the model\u2019s convergence, and calculate the probabilities. For both across and within-environments scopes, the package computes the probability of superior performance and the pairwise probability of superior performance. Furthermore, the probability of superior stability and the pairwise probability of superior stability across environments is estimated. A joint probability of superior performance and stability is also provided. ",
    "version": "1.0.4.9",
    "maintainer": "Saulo Chaves <saulochaves@usp.br>",
    "author": "Saulo Chaves [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0694-1798>),\n  Kaio Dias [aut, cph] (ORCID: <https://orcid.org/0000-0002-9171-1021>),\n  Matheus Krause [aut] (ORCID: <https://orcid.org/0000-0003-2411-9287>)",
    "url": "https://github.com/saulo-chaves/ProbBreed,\nhttps://saulo-chaves.github.io/ProbBreed_site/,\nhttps://saulo-chaves.github.io/ProbBreed/",
    "bug_reports": "https://github.com/saulo-chaves/ProbBreed/issues",
    "repository": "https://cran.r-project.org/package=ProbBreed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProbBreed Probability Theory for Selecting Candidates in Plant Breeding Use probability theory under the Bayesian framework for calculating the risk of selecting candidates in a multi-environment context. Contained are functions used to fit a Bayesian multi-environment model (based on the available presets), extract posterior values and maximum posterior values, compute the variance components, check the model\u2019s convergence, and calculate the probabilities. For both across and within-environments scopes, the package computes the probability of superior performance and the pairwise probability of superior performance. Furthermore, the probability of superior stability and the pairwise probability of superior stability across environments is estimated. A joint probability of superior performance and stability is also provided.   "
  },
  {
    "id": 151,
    "package_name": "RPEIF",
    "title": "Computation and Plots of Influence Functions for Risk and\nPerformance Measures",
    "description": "Computes the influence functions time series of the returns for the risk and \n             performance measures as mentioned in Chen and Martin (2018) \n             <https://www.ssrn.com/abstract=3085672>, as well as in Zhang et al. (2019)\n             <https://www.ssrn.com/abstract=3415903>. Also evaluates estimators influence\n             functions at a set of parameter values and plots them to display the shapes of \n             the influence functions.",
    "version": "1.2.5",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "author": "Anthony Christidis [aut, cre],\n  Shengyu Zhang [aut],\n  Douglas Martin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RPEIF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPEIF Computation and Plots of Influence Functions for Risk and\nPerformance Measures Computes the influence functions time series of the returns for the risk and \n             performance measures as mentioned in Chen and Martin (2018) \n             <https://www.ssrn.com/abstract=3085672>, as well as in Zhang et al. (2019)\n             <https://www.ssrn.com/abstract=3415903>. Also evaluates estimators influence\n             functions at a set of parameter values and plots them to display the shapes of \n             the influence functions.  "
  },
  {
    "id": 158,
    "package_name": "Rcan",
    "title": "Cancer Registry Data Analysis and Visualisation",
    "description": "Tools for basic and advance cancer statistics and graphics.\n\tGroups individual data, merges registry data and population data, calculates age-specific rate, age-standardized rate, cumulative risk, estimated annual percentage rate with standards error. Creates graphics across variable and\n    time, such as age-specific trends, bar chart and period-cohort trends.",
    "version": "1.3.92",
    "maintainer": "Mathieu Laversanne <laversannem@iarc.who.int>",
    "author": "Mathieu Laversanne [aut, cre],\n  Jerome Vignat [aut],\n  Cancer Surveillance Unit [cph]",
    "url": "https://github.com/timat35/Rcan",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rcan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rcan Cancer Registry Data Analysis and Visualisation Tools for basic and advance cancer statistics and graphics.\n\tGroups individual data, merges registry data and population data, calculates age-specific rate, age-standardized rate, cumulative risk, estimated annual percentage rate with standards error. Creates graphics across variable and\n    time, such as age-specific trends, bar chart and period-cohort trends.  "
  },
  {
    "id": 191,
    "package_name": "ShinyLink",
    "title": "'Shiny' Based Record Linkage Tool",
    "description": "A bridge is created between existing robust open-source record linkage algorithms and an urgently needed user-friendly platform that removes financial and technical barriers, setting a new standard for data interoperability in public health and bioinformatics. The 'fastLink' algorithms are used for matching. Ted Enamorado et al. (2019) <doi:10.1017/S0003055418000783>.",
    "version": "0.5.5",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre],\n  Shufu Chen [aut],\n  John Tyburski [aut],\n  Amrita Cheema [aut]",
    "url": "https://shinylink.org/, https://github.com/cdc-addm/ShinyLink/",
    "bug_reports": "https://github.com/cdc-addm/ShinyLink/issues/",
    "repository": "https://cran.r-project.org/package=ShinyLink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShinyLink 'Shiny' Based Record Linkage Tool A bridge is created between existing robust open-source record linkage algorithms and an urgently needed user-friendly platform that removes financial and technical barriers, setting a new standard for data interoperability in public health and bioinformatics. The 'fastLink' algorithms are used for matching. Ted Enamorado et al. (2019) <doi:10.1017/S0003055418000783>.  "
  },
  {
    "id": 192,
    "package_name": "Sim.DiffProc",
    "title": "Simulation of Diffusion Processes",
    "description": "It provides users with a wide range of tools to simulate, estimate, analyze, and visualize the dynamics of stochastic differential systems in both forms Ito and Stratonovich. Statistical analysis with parallel Monte Carlo and moment equations methods of SDEs <doi:10.18637/jss.v096.i02>. Enabled many searchers in different domains to use these equations to modeling practical problems in financial and actuarial modeling and other areas of application, e.g., modeling and simulate of first passage time problem in shallow water using the attractive center (Boukhetala K, 1996) ISBN:1-56252-342-2. ",
    "version": "5.0",
    "maintainer": "Arsalane Chouaib Guidoum <acguidoum@univ-tam.dz>",
    "author": "Arsalane Chouaib Guidoum [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-3781-2160>),\n  Kamal Boukhetala [aut]",
    "url": "https://github.com/acguidoum/Sim.DiffProc",
    "bug_reports": "https://github.com/acguidoum/Sim.DiffProc/issues",
    "repository": "https://cran.r-project.org/package=Sim.DiffProc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Sim.DiffProc Simulation of Diffusion Processes It provides users with a wide range of tools to simulate, estimate, analyze, and visualize the dynamics of stochastic differential systems in both forms Ito and Stratonovich. Statistical analysis with parallel Monte Carlo and moment equations methods of SDEs <doi:10.18637/jss.v096.i02>. Enabled many searchers in different domains to use these equations to modeling practical problems in financial and actuarial modeling and other areas of application, e.g., modeling and simulate of first passage time problem in shallow water using the attractive center (Boukhetala K, 1996) ISBN:1-56252-342-2.   "
  },
  {
    "id": 363,
    "package_name": "cifmodeling",
    "title": "Visualization and Polytomous Modeling of Survival and Competing\nRisks",
    "description": "A publication-ready toolkit for modern survival and competing risks\n    analysis with a minimal, formula-based interface. Both nonparametric\n    estimation and direct polytomous regression of cumulative incidence\n    functions (CIFs) are supported. The main functions 'cifcurve()', 'cifplot()',\n    and 'cifpanel()' estimate survival and CIF curves and produce high-quality\n    graphics with risk tables, censoring and competing-risk marks, and\n    multi-panel or inset layouts built on 'ggplot2' and 'ggsurvfit'. The modeling\n    function 'polyreg()' performs direct polytomous regression for coherent joint\n    modeling of all cause-specific CIFs to estimate risk ratios, odds ratios, or\n    subdistribution hazard ratios at user-specified time points. All core\n    functions adopt a formula-and-data syntax and return tidy and extensible\n    outputs that integrate smoothly with 'modelsummary', 'broom', and the broader\n    'tidyverse' ecosystem. Key numerical routines are implemented in C++ via\n    'Rcpp'.",
    "version": "0.9.6",
    "maintainer": "Shiro Tanaka <gestimation@gmail.com>",
    "author": "Shiro Tanaka [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6817-5235>),\n  Shigetaka Kobari [ctb],\n  Chisato Honda [ctb]",
    "url": "https://gestimation.github.io/cifmodeling/,\nhttps://github.com/gestimation/cifmodeling",
    "bug_reports": "https://github.com/gestimation/cifmodeling/issues",
    "repository": "https://cran.r-project.org/package=cifmodeling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cifmodeling Visualization and Polytomous Modeling of Survival and Competing\nRisks A publication-ready toolkit for modern survival and competing risks\n    analysis with a minimal, formula-based interface. Both nonparametric\n    estimation and direct polytomous regression of cumulative incidence\n    functions (CIFs) are supported. The main functions 'cifcurve()', 'cifplot()',\n    and 'cifpanel()' estimate survival and CIF curves and produce high-quality\n    graphics with risk tables, censoring and competing-risk marks, and\n    multi-panel or inset layouts built on 'ggplot2' and 'ggsurvfit'. The modeling\n    function 'polyreg()' performs direct polytomous regression for coherent joint\n    modeling of all cause-specific CIFs to estimate risk ratios, odds ratios, or\n    subdistribution hazard ratios at user-specified time points. All core\n    functions adopt a formula-and-data syntax and return tidy and extensible\n    outputs that integrate smoothly with 'modelsummary', 'broom', and the broader\n    'tidyverse' ecosystem. Key numerical routines are implemented in C++ via\n    'Rcpp'.  "
  },
  {
    "id": 429,
    "package_name": "csmpv",
    "title": "Biomarker Confirmation, Selection, Modelling, Prediction, and\nValidation",
    "description": "\n   There are diverse purposes such as biomarker confirmation, novel biomarker discovery, constructing predictive models, model-based prediction, and validation. \n   It handles binary, continuous, and time-to-event outcomes at the sample or patient level.\n   - Biomarker confirmation utilizes established functions like glm() from 'stats', coxph() from 'survival', surv_fit(), and ggsurvplot() from 'survminer'.\n   - Biomarker discovery and variable selection are facilitated by three LASSO-related functions LASSO2(), LASSO_plus(), and LASSO2plus(), leveraging the 'glmnet' R package with additional steps.\n   - Eight versatile modeling functions are offered, each designed for predictive models across various outcomes and data types.\n     1) LASSO2(), LASSO_plus(), LASSO2plus(), and LASSO2_reg() perform variable selection using LASSO methods and construct predictive models based on selected variables.\n     2) XGBtraining() employs 'XGBoost' for model building and is the only function not involving variable selection.\n     3) Functions like LASSO2_XGBtraining(), LASSOplus_XGBtraining(), and LASSO2plus_XGBtraining() combine LASSO-related variable selection with 'XGBoost' for model construction.\n   - All models support prediction and validation, requiring a testing dataset comparable to the training dataset.\n   Additionally, the package introduces XGpred() for risk prediction based on survival data, with the XGpred_predict() function available for predicting risk groups in new datasets.\n   The methodology is based on our new algorithms and various references:\n   - Hastie et al. (1992, ISBN 0 534 16765-9), \n   - Therneau et al. (2000, ISBN 0-387-98784-3), \n   - Kassambara et al. (2021) <https://CRAN.R-project.org/package=survminer>,\n   - Friedman et al. (2010) <doi:10.18637/jss.v033.i01>,\n   - Simon et al. (2011) <doi:10.18637/jss.v039.i05>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=rms>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=Hmisc>,\n   - Chen and Guestrin (2016) <doi:10.48550/arXiv.1603.02754>,\n   - Aoki et al. (2023) <doi:10.1200/JCO.23.01115>.",
    "version": "1.0.5",
    "maintainer": "Aixiang Jiang <aijiang@bccrc.ca>",
    "author": "Aixiang Jiang [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6153-7595>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csmpv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csmpv Biomarker Confirmation, Selection, Modelling, Prediction, and\nValidation \n   There are diverse purposes such as biomarker confirmation, novel biomarker discovery, constructing predictive models, model-based prediction, and validation. \n   It handles binary, continuous, and time-to-event outcomes at the sample or patient level.\n   - Biomarker confirmation utilizes established functions like glm() from 'stats', coxph() from 'survival', surv_fit(), and ggsurvplot() from 'survminer'.\n   - Biomarker discovery and variable selection are facilitated by three LASSO-related functions LASSO2(), LASSO_plus(), and LASSO2plus(), leveraging the 'glmnet' R package with additional steps.\n   - Eight versatile modeling functions are offered, each designed for predictive models across various outcomes and data types.\n     1) LASSO2(), LASSO_plus(), LASSO2plus(), and LASSO2_reg() perform variable selection using LASSO methods and construct predictive models based on selected variables.\n     2) XGBtraining() employs 'XGBoost' for model building and is the only function not involving variable selection.\n     3) Functions like LASSO2_XGBtraining(), LASSOplus_XGBtraining(), and LASSO2plus_XGBtraining() combine LASSO-related variable selection with 'XGBoost' for model construction.\n   - All models support prediction and validation, requiring a testing dataset comparable to the training dataset.\n   Additionally, the package introduces XGpred() for risk prediction based on survival data, with the XGpred_predict() function available for predicting risk groups in new datasets.\n   The methodology is based on our new algorithms and various references:\n   - Hastie et al. (1992, ISBN 0 534 16765-9), \n   - Therneau et al. (2000, ISBN 0-387-98784-3), \n   - Kassambara et al. (2021) <https://CRAN.R-project.org/package=survminer>,\n   - Friedman et al. (2010) <doi:10.18637/jss.v033.i01>,\n   - Simon et al. (2011) <doi:10.18637/jss.v039.i05>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=rms>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=Hmisc>,\n   - Chen and Guestrin (2016) <doi:10.48550/arXiv.1603.02754>,\n   - Aoki et al. (2023) <doi:10.1200/JCO.23.01115>.  "
  },
  {
    "id": 431,
    "package_name": "cvar",
    "title": "Compute Expected Shortfall and Value at Risk for Continuous\nDistributions",
    "description": "Compute expected shortfall (ES) and Value at Risk (VaR) from a\n    quantile function, distribution function, random number generator,\n    probability density function, or data.  ES is also known as Conditional\n    Value at Risk (CVaR). Virtually any continuous distribution can be\n    specified.  The functions are vectorized over the arguments. The\n    computations are done directly from the definitions, see e.g. Acerbi and\n    Tasche (2002) <doi:10.1111/1468-0300.00091>.  Some support for GARCH models\n    is provided, as well.",
    "version": "0.6",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Georgi N. Boshnakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/cvar/ (doc),\nhttps://github.com/GeoBosh/cvar (devel)",
    "bug_reports": "https://github.com/GeoBosh/cvar/issues",
    "repository": "https://cran.r-project.org/package=cvar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvar Compute Expected Shortfall and Value at Risk for Continuous\nDistributions Compute expected shortfall (ES) and Value at Risk (VaR) from a\n    quantile function, distribution function, random number generator,\n    probability density function, or data.  ES is also known as Conditional\n    Value at Risk (CVaR). Virtually any continuous distribution can be\n    specified.  The functions are vectorized over the arguments. The\n    computations are done directly from the definitions, see e.g. Acerbi and\n    Tasche (2002) <doi:10.1111/1468-0300.00091>.  Some support for GARCH models\n    is provided, as well.  "
  },
  {
    "id": 525,
    "package_name": "epiR",
    "title": "Tools for the Analysis of Epidemiological Data",
    "description": "Tools for the analysis of epidemiological and surveillance data. Contains functions for directly and indirectly adjusting measures of disease frequency, quantifying measures of association on the basis of single or multiple strata of count data presented in a contingency table, computation of confidence intervals around incidence risk and incidence rate estimates and sample size calculations for cross-sectional, case-control and cohort studies. Surveillance tools include functions to calculate an appropriate sample size for 1- and 2-stage representative freedom surveys, functions to estimate surveillance system sensitivity and functions to support scenario tree modelling analyses.   ",
    "version": "2.0.89",
    "maintainer": "Mark Stevenson <mark.stevenson1@unimelb.edu.au>",
    "author": "Mark Stevenson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1890-9784>),\n  Evan Sergeant [aut],\n  Cord Heuer [ctb],\n  Telmo Nunes [ctb],\n  Cord Heuer [ctb],\n  Jonathon Marshall [ctb],\n  Javier Sanchez [ctb],\n  Ron Thornton [ctb],\n  Jeno Reiczigel [ctb],\n  Jim Robison-Cox [ctb],\n  Paola Sebastiani [ctb],\n  Peter Solymos [ctb],\n  Kazuki Yoshida [ctb],\n  Geoff Jones [ctb],\n  Sarah Pirikahu [ctb],\n  Simon Firestone [ctb],\n  Ryan Kyle [ctb],\n  Johann Popp [ctb],\n  Mathew Jay [ctb],\n  Allison Cheung [ctb],\n  Nagendra Singanallur [ctb],\n  Aniko Szabo [ctb],\n  Ahmad Rabiee [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=epiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epiR Tools for the Analysis of Epidemiological Data Tools for the analysis of epidemiological and surveillance data. Contains functions for directly and indirectly adjusting measures of disease frequency, quantifying measures of association on the basis of single or multiple strata of count data presented in a contingency table, computation of confidence intervals around incidence risk and incidence rate estimates and sample size calculations for cross-sectional, case-control and cohort studies. Surveillance tools include functions to calculate an appropriate sample size for 1- and 2-stage representative freedom surveys, functions to estimate surveillance system sensitivity and functions to support scenario tree modelling analyses.     "
  },
  {
    "id": 554,
    "package_name": "fGarch",
    "title": "Rmetrics - Autoregressive Conditional Heteroskedastic Modelling",
    "description": "Analyze and model heteroskedastic behavior in financial time series.",
    "version": "4052.93",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\n  Yohan Chalabi [aut],\n  Tobias Setz [aut],\n  Martin Maechler [aut] (ORCID: <https://orcid.org/0000-0002-8685-9910>),\n  Chris Boudt [ctb],\n  Pierre Chausse [ctb],\n  Michal Miklovac [ctb],\n  Georgi N. Boshnakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/fGarchDoc/ (doc),\nhttps://CRAN.R-project.org/package=fGarch,\nhttps://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/tracker/?func=browse&group_id=156&atid=633",
    "repository": "https://cran.r-project.org/package=fGarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fGarch Rmetrics - Autoregressive Conditional Heteroskedastic Modelling Analyze and model heteroskedastic behavior in financial time series.  "
  },
  {
    "id": 706,
    "package_name": "impermanentlosscalc",
    "title": "Calculate Impermanent Loss in Automated Market Maker (AMM)\nLiquidity Pools",
    "description": "Computes the key metrics for assessing the performance of a liquidity provider (LP) position in a weighted multi-asset Automated Market Maker (AMM) pool. Calculates the nominal and percentage impermanent loss (IL) by comparing the portfolio value inside the pool (based on the weighted geometric mean of price ratios) against the value of simply holding the assets outside the pool (based on the weighted arithmetic mean). The primary function, `impermanent_loss()`, incorporates the effect of earned trading fees to provide the LP's net profit and loss relative to a holding strategy, using a methodology derived from Tiruviluamala, N., Port, A., and Lewis, E. (2022) <doi:10.48550/arXiv.2203.11352>.",
    "version": "0.1.0",
    "maintainer": "Amber Krause <amber32k@gmail.com>",
    "author": "Amber Krause [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7493-3909>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=impermanentlosscalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "impermanentlosscalc Calculate Impermanent Loss in Automated Market Maker (AMM)\nLiquidity Pools Computes the key metrics for assessing the performance of a liquidity provider (LP) position in a weighted multi-asset Automated Market Maker (AMM) pool. Calculates the nominal and percentage impermanent loss (IL) by comparing the portfolio value inside the pool (based on the weighted geometric mean of price ratios) against the value of simply holding the assets outside the pool (based on the weighted arithmetic mean). The primary function, `impermanent_loss()`, incorporates the effect of earned trading fees to provide the LP's net profit and loss relative to a holding strategy, using a methodology derived from Tiruviluamala, N., Port, A., and Lewis, E. (2022) <doi:10.48550/arXiv.2203.11352>.  "
  },
  {
    "id": 844,
    "package_name": "mpn.scorecard",
    "title": "Generate a scorecard with various measures of R package quality and risk",
    "description": "Collects various measures of quality and risk-of-use for an R package",
    "version": "0.5.3",
    "maintainer": "",
    "author": "",
    "url": "https://github.com/metrumresearchgroup/mpn.scorecard",
    "bug_reports": "https://github.com/metrumresearchgroup/mpn.scorecard/issues",
    "repository": "https://github.com/metrumresearchgroup/mpn.scorecard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 3,
    "primary_category": "pharmacometrics",
    "source_universe": "github:metrumresearchgroup",
    "search_text": "mpn.scorecard Generate a scorecard with various measures of R package quality and risk Collects various measures of quality and risk-of-use for an R package  "
  },
  {
    "id": 872,
    "package_name": "neighbours",
    "title": "Neighbourhood Functions for Local-Search Algorithms",
    "description": "Neighbourhood functions are key components of\n  local-search algorithms such as Simulated Annealing or\n  Threshold Accepting.  These functions take a solution and\n  return a slightly-modified copy of it, i.e. a neighbour.\n  The package provides a function neighbourfun() that\n  constructs such neighbourhood functions, based on\n  parameters such as admissible ranges for elements in a\n  solution.  Supported are numeric and logical solutions.\n  The algorithms were originally created for\n  portfolio-optimisation applications, but can be used for\n  other models as well.  Several recipes for neighbour\n  computations are taken from \"Numerical Methods and\n  Optimization in Finance\" by M. Gilli, D. Maringer and\n  E. Schumann (2019, ISBN:978-0128150658).",
    "version": "0.1-5",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/R/packages/neighbours/ ,\nhttps://sr.ht/~enricoschumann/neighbours/ ,\nhttps://github.com/enricoschumann/neighbours",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=neighbours",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neighbours Neighbourhood Functions for Local-Search Algorithms Neighbourhood functions are key components of\n  local-search algorithms such as Simulated Annealing or\n  Threshold Accepting.  These functions take a solution and\n  return a slightly-modified copy of it, i.e. a neighbour.\n  The package provides a function neighbourfun() that\n  constructs such neighbourhood functions, based on\n  parameters such as admissible ranges for elements in a\n  solution.  Supported are numeric and logical solutions.\n  The algorithms were originally created for\n  portfolio-optimisation applications, but can be used for\n  other models as well.  Several recipes for neighbour\n  computations are taken from \"Numerical Methods and\n  Optimization in Finance\" by M. Gilli, D. Maringer and\n  E. Schumann (2019, ISBN:978-0128150658).  "
  },
  {
    "id": 1036,
    "package_name": "qraLm",
    "title": "Functions to develop quantitative risk assessment for Listeria monocytogenes in foods",
    "description": "qraLm: A R package for quantitative risk assessment for Listeria monocytogenes in foods.",
    "version": "0.1.2",
    "maintainer": "Vasco Cadavez <vcadavez@ipb.pt>",
    "author": "",
    "url": "https://github.com/WorldHealthOrganization/qraLm",
    "bug_reports": "https://github.com/WorldHealthOrganization/qraLm/issues",
    "repository": "https://github.com/WorldHealthOrganization/qraLm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "github:WorldHealthOrganization",
    "search_text": "qraLm Functions to develop quantitative risk assessment for Listeria monocytogenes in foods qraLm: A R package for quantitative risk assessment for Listeria monocytogenes in foods.  "
  },
  {
    "id": 1142,
    "package_name": "rqlm",
    "title": "Modified Poisson Regression for Binary Outcome and Related\nMethods",
    "description": "Modified Poisson, logistic and least-squares regression analyses for binary outcomes of Zou (2004) <doi:10.1093/aje/kwh090>, Noma (2025)<Forthcoming>, and Cheung (2007) <doi:10.1093/aje/kwm223> have been standard multivariate analysis methods to estimate risk ratio and risk difference in clinical and epidemiological studies. This R package involves an easy-to-handle function to implement these analyses by simple commands. Missing data analysis tools (multiple imputation) are also involved. In addition, recent studies have shown the ordinary robust variance estimator possibly has serious bias under small or moderate sample size situations for these methods. This package also provides computational tools to calculate alternative accurate confidence intervals.",
    "version": "4.2-1",
    "maintainer": "Hisashi Noma <noma@ism.ac.jp>",
    "author": "Hisashi Noma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2520-9949>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rqlm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rqlm Modified Poisson Regression for Binary Outcome and Related\nMethods Modified Poisson, logistic and least-squares regression analyses for binary outcomes of Zou (2004) <doi:10.1093/aje/kwh090>, Noma (2025)<Forthcoming>, and Cheung (2007) <doi:10.1093/aje/kwm223> have been standard multivariate analysis methods to estimate risk ratio and risk difference in clinical and epidemiological studies. This R package involves an easy-to-handle function to implement these analyses by simple commands. Missing data analysis tools (multiple imputation) are also involved. In addition, recent studies have shown the ordinary robust variance estimator possibly has serious bias under small or moderate sample size situations for these methods. This package also provides computational tools to calculate alternative accurate confidence intervals.  "
  },
  {
    "id": 1355,
    "package_name": "timeSeries",
    "title": "Financial Time Series Objects (Rmetrics)",
    "description": "'S4' classes and various tools for financial time series:\n  Basic functions such as scaling and sorting, subsetting,\n  mathematical operations and statistical functions.",
    "version": "4052.112",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\n  Tobias Setz [aut],\n  Yohan Chalabi [aut],\n  Martin Maechler [ctb] (ORCID: <https://orcid.org/0000-0002-8685-9910>),\n  Georgi N. Boshnakov [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/timeSeriesDoc/ (doc),\nhttps://CRAN.R-project.org/package=timeSeries,\nhttps://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/tracker/?atid=633&group_id=156&func=browse",
    "repository": "https://cran.r-project.org/package=timeSeries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeSeries Financial Time Series Objects (Rmetrics) 'S4' classes and various tools for financial time series:\n  Basic functions such as scaling and sorting, subsetting,\n  mathematical operations and statistical functions.  "
  },
  {
    "id": 1552,
    "package_name": "AMISforInfectiousDiseases",
    "title": "Implement the AMIS Algorithm for Infectious Disease Models",
    "description": "Implements the Adaptive Multiple Importance Sampling (AMIS) algorithm, as described by Retkute et al. (2021, <doi:10.1214/21-AOAS1486>), to estimate key epidemiological parameters by combining outputs from a geostatistical model of infectious diseases (such as prevalence, incidence, or relative risk) with a disease transmission model. Utilising the resulting posterior distributions, the package enables forward projections at the local level.",
    "version": "0.1.0",
    "maintainer": "Simon Spencer <s.e.f.spencer@warwick.ac.uk>",
    "author": "Evandro Konzen [aut] (ORCID: <https://orcid.org/0000-0002-6275-1681>),\n  Renata Retkute [aut] (ORCID: <https://orcid.org/0000-0002-3877-6440>),\n  Raiha Browning [aut] (ORCID: <https://orcid.org/0000-0002-6175-2244>),\n  Thilbault Lestang [aut],\n  Simon Spencer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8375-5542>),\n  University of Warwick [cph],\n  Oxford Research Software Engineering [cph]",
    "url": "https://github.com/drsimonspencer/AMISforInfectiousDiseases-dev",
    "bug_reports": "https://github.com/drsimonspencer/AMISforInfectiousDiseases-dev/issues",
    "repository": "https://cran.r-project.org/package=AMISforInfectiousDiseases",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AMISforInfectiousDiseases Implement the AMIS Algorithm for Infectious Disease Models Implements the Adaptive Multiple Importance Sampling (AMIS) algorithm, as described by Retkute et al. (2021, <doi:10.1214/21-AOAS1486>), to estimate key epidemiological parameters by combining outputs from a geostatistical model of infectious diseases (such as prevalence, incidence, or relative risk) with a disease transmission model. Utilising the resulting posterior distributions, the package enables forward projections at the local level.  "
  },
  {
    "id": 1577,
    "package_name": "APtools",
    "title": "Average Positive Predictive Values (AP) for Binary Outcomes and\nCensored Event Times",
    "description": "We provide tools to estimate two prediction accuracy metrics,\n    the average positive predictive values (AP) as well as the well-known AUC\n    (the area under the receiver operator characteristic curve) for risk scores. \n    The outcome of interest is either binary or censored event time.\n    Note that for censored event time, our functions' estimates, the AP and the\n    AUC, are time-dependent for pre-specified time interval(s). A function that\n    compares the APs of two risk scores/markers is also included. Optional\n    outputs include positive predictive values and true positive fractions at\n    the specified marker cut-off values, and a plot of the time-dependent AP\n    versus time (available for event time data).",
    "version": "6.8.8",
    "maintainer": "Hengrui Cai <hengruicai@gmail.com>",
    "author": "Hengrui Cai <hengruicai@gmail.com>, Yan Yuan <yyuan@ualberta.ca>, Qian\n    Michelle Zhou <qz70@msstate.edu>, Bingying Li<dorisli1120@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=APtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "APtools Average Positive Predictive Values (AP) for Binary Outcomes and\nCensored Event Times We provide tools to estimate two prediction accuracy metrics,\n    the average positive predictive values (AP) as well as the well-known AUC\n    (the area under the receiver operator characteristic curve) for risk scores. \n    The outcome of interest is either binary or censored event time.\n    Note that for censored event time, our functions' estimates, the AP and the\n    AUC, are time-dependent for pre-specified time interval(s). A function that\n    compares the APs of two risk scores/markers is also included. Optional\n    outputs include positive predictive values and true positive fractions at\n    the specified marker cut-off values, and a plot of the time-dependent AP\n    versus time (available for event time data).  "
  },
  {
    "id": 1600,
    "package_name": "ASML",
    "title": "Algorithm Portfolio Selection with Machine Learning",
    "description": "A wrapper for machine learning (ML) methods to select among a portfolio of algorithms based on the value of a key performance indicator (KPI). A number of features is used to adjust a model to predict the value of the KPI for each algorithm, then, for a new value of the features the KPI is estimated and the algorithm with the best one is chosen. To learn it can use the regression methods in 'caret' package or a custom function defined by the user. Several graphics available to analyze the results obtained. This library has been used in Ghaddar et al. (2023) <doi:10.1287/ijoc.2022.0090>).",
    "version": "1.1.0",
    "maintainer": "Brais Gonz\u00e1lez-Rodr\u00edguez <brais.gonzalez.rodriguez@uvigo.gal>",
    "author": "Brais Gonz\u00e1lez-Rodr\u00edguez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5276-2320>),\n  Ignacio G\u00f3mez-Casares [aut] (ORCID:\n    <https://orcid.org/0000-0003-0420-7319>),\n  Beatriz Pateiro-L\u00f3pez [aut] (ORCID:\n    <https://orcid.org/0000-0002-7714-1835>),\n  Julio Gonz\u00e1lez-D\u00edaz [aut] (ORCID:\n    <https://orcid.org/0000-0002-4667-4348>),\n  Mar\u00eda Caseiro-Arias [ctb],\n  Antonio Fari\u00f1a-Elorza [ctb],\n  Manuel Timiraos-L\u00f3pez [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ASML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ASML Algorithm Portfolio Selection with Machine Learning A wrapper for machine learning (ML) methods to select among a portfolio of algorithms based on the value of a key performance indicator (KPI). A number of features is used to adjust a model to predict the value of the KPI for each algorithm, then, for a new value of the features the KPI is estimated and the algorithm with the best one is chosen. To learn it can use the regression methods in 'caret' package or a custom function defined by the user. Several graphics available to analyze the results obtained. This library has been used in Ghaddar et al. (2023) <doi:10.1287/ijoc.2022.0090>).  "
  },
  {
    "id": 1623,
    "package_name": "AccSamplingDesign",
    "title": "Acceptance Sampling Plans Design",
    "description": "Provides tools for designing and analyzing Acceptance Sampling plans.\n    Supports both Attributes Sampling (Binomial and Poisson distributions) and \n    Variables Sampling (Normal and Beta distributions), enabling quality control \n    for fractional and compositional data. Uses nonlinear programming for sampling \n    plan optimization, minimizing sample size while controlling producer's and \n    consumer's risks. Operating Characteristic curves are available for plan visualization.",
    "version": "0.0.7",
    "maintainer": "Ha Truong <truongvietha87@gmail.com>",
    "author": "Ha Truong [aut, cre, cph],\n  Victor Miranda [ths, rev],\n  Roger Kissling [ths, rev]",
    "url": "https://github.com/vietha/AccSamplingDesign",
    "bug_reports": "https://github.com/vietha/AccSamplingDesign/issues",
    "repository": "https://cran.r-project.org/package=AccSamplingDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AccSamplingDesign Acceptance Sampling Plans Design Provides tools for designing and analyzing Acceptance Sampling plans.\n    Supports both Attributes Sampling (Binomial and Poisson distributions) and \n    Variables Sampling (Normal and Beta distributions), enabling quality control \n    for fractional and compositional data. Uses nonlinear programming for sampling \n    plan optimization, minimizing sample size while controlling producer's and \n    consumer's risks. Operating Characteristic curves are available for plan visualization.  "
  },
  {
    "id": 1635,
    "package_name": "ActuarialM",
    "title": "Computation of Actuarial Measures Using Bell G Family",
    "description": "It computes two frequently applied actuarial measures, the expected shortfall and the value at risk. Seven well-known classical distributions in connection to the Bell generalized family are used as follows: Bell-exponential distribution, Bell-extended exponential distribution, Bell-Weibull distribution, Bell-extended Weibull distribution, Bell-Lomax distribution, Bell-Burr-12 distribution, and Bell-Burr-X distribution. Related works include:\n     a) Fayomi, A., Tahir, M. H., Algarni, A., Imran, M., & Jamal, F. (2022). \"A new useful exponential model with applications to quality control and \n        actuarial data\". Computational Intelligence and Neuroscience, 2022. <doi:10.1155/2022/2489998>.\n     b) Alsadat, N., Imran, M., Tahir, M. H., Jamal, F., Ahmad, H., & Elgarhy, M. (2023). \"Compounded Bell-G class of statistical models with applications to COVID-19 and actuarial data\". Open Physics, 21(1), 20220242. <doi:10.1515/phys-2022-0242>.",
    "version": "0.1.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  M.H. Tahir [aut],\n  Saima Shakoor [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ActuarialM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ActuarialM Computation of Actuarial Measures Using Bell G Family It computes two frequently applied actuarial measures, the expected shortfall and the value at risk. Seven well-known classical distributions in connection to the Bell generalized family are used as follows: Bell-exponential distribution, Bell-extended exponential distribution, Bell-Weibull distribution, Bell-extended Weibull distribution, Bell-Lomax distribution, Bell-Burr-12 distribution, and Bell-Burr-X distribution. Related works include:\n     a) Fayomi, A., Tahir, M. H., Algarni, A., Imran, M., & Jamal, F. (2022). \"A new useful exponential model with applications to quality control and \n        actuarial data\". Computational Intelligence and Neuroscience, 2022. <doi:10.1155/2022/2489998>.\n     b) Alsadat, N., Imran, M., Tahir, M. H., Jamal, F., Ahmad, H., & Elgarhy, M. (2023). \"Compounded Bell-G class of statistical models with applications to COVID-19 and actuarial data\". Open Physics, 21(1), 20220242. <doi:10.1515/phys-2022-0242>.  "
  },
  {
    "id": 1730,
    "package_name": "AssetPricing",
    "title": "Optimal Pricing of Assets with Fixed Expiry Date",
    "description": "Calculates the optimal price of assets (such as\n\tairline flight seats, hotel room bookings) whose value\n\tbecomes zero after a fixed ``expiry date''.  Assumes\n\tpotential customers arrive (possibly in groups) according\n\tto a known inhomogeneous Poisson process.  Also assumes a\n\tknown time-varying elasticity of demand (price sensitivity)\n\tfunction.  Uses elementary techniques based on ordinary\n\tdifferential equations.  Uses the package deSolve to effect\n\tthe solution of these differential equations.",
    "version": "1.0-3",
    "maintainer": "Rolf Turner <r.turner@auckland.ac.nz>",
    "author": "Rolf Turner <r.turner@auckland.ac.nz>",
    "url": "http://www.stat.auckland.ac.nz/~rolf/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AssetPricing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AssetPricing Optimal Pricing of Assets with Fixed Expiry Date Calculates the optimal price of assets (such as\n\tairline flight seats, hotel room bookings) whose value\n\tbecomes zero after a fixed ``expiry date''.  Assumes\n\tpotential customers arrive (possibly in groups) according\n\tto a known inhomogeneous Poisson process.  Also assumes a\n\tknown time-varying elasticity of demand (price sensitivity)\n\tfunction.  Uses elementary techniques based on ordinary\n\tdifferential equations.  Uses the package deSolve to effect\n\tthe solution of these differential equations.  "
  },
  {
    "id": 1748,
    "package_name": "AutoScore",
    "title": "An Interpretable Machine Learning-Based Automatic Clinical Score\nGenerator",
    "description": "A novel interpretable machine learning-based framework to automate the development of a clinical scoring model for predefined outcomes. Our novel framework consists of six modules: variable ranking with machine learning, variable transformation, score derivation, model selection, domain knowledge-based score fine-tuning, and performance evaluation.The details are described in our research paper<doi:10.2196/21798>. Users or clinicians could seamlessly generate parsimonious sparse-score risk models (i.e., risk scores), which can be easily implemented and validated in clinical practice. We hope to see its application in various medical case studies.",
    "version": "1.1.0",
    "maintainer": "Feng Xie <xief@u.duke.nus.edu>",
    "author": "Feng Xie [aut, cre] (ORCID: <https://orcid.org/0000-0002-0215-667X>),\n  Yilin Ning [aut] (ORCID: <https://orcid.org/0000-0002-6758-4472>),\n  Han Yuan [aut] (ORCID: <https://orcid.org/0000-0002-2674-6068>),\n  Mingxuan Liu [aut] (ORCID: <https://orcid.org/0000-0002-4274-9613>),\n  Siqi Li [aut] (ORCID: <https://orcid.org/0000-0002-1660-105X>),\n  Ehsan Saffari [aut] (ORCID: <https://orcid.org/0000-0002-6473-4375>),\n  Bibhas Chakraborty [aut] (ORCID:\n    <https://orcid.org/0000-0002-7366-0478>),\n  Nan Liu [aut] (ORCID: <https://orcid.org/0000-0003-3610-4883>)",
    "url": "https://github.com/nliulab/AutoScore",
    "bug_reports": "https://github.com/nliulab/AutoScore/issues",
    "repository": "https://cran.r-project.org/package=AutoScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AutoScore An Interpretable Machine Learning-Based Automatic Clinical Score\nGenerator A novel interpretable machine learning-based framework to automate the development of a clinical scoring model for predefined outcomes. Our novel framework consists of six modules: variable ranking with machine learning, variable transformation, score derivation, model selection, domain knowledge-based score fine-tuning, and performance evaluation.The details are described in our research paper<doi:10.2196/21798>. Users or clinicians could seamlessly generate parsimonious sparse-score risk models (i.e., risk scores), which can be easily implemented and validated in clinical practice. We hope to see its application in various medical case studies.  "
  },
  {
    "id": 1805,
    "package_name": "BCRA",
    "title": "Breast Cancer Risk Assessment",
    "description": "Functions provide risk projections of invasive breast cancer based on Gail model according to National Cancer Institute's Breast Cancer Risk Assessment Tool algorithm for specified race/ethnic groups and age intervals.\n             Gail MH, Brinton LA, et al (1989) <doi:10.1093/jnci/81.24.1879>. \n             Marthew PB, Gail MH, et al (2016) <doi:10.1093/jnci/djw215>.",
    "version": "2.1.2",
    "maintainer": "Fanni Zhang <rstatpackages@gmail.com>",
    "author": "Fanni Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BCRA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BCRA Breast Cancer Risk Assessment Functions provide risk projections of invasive breast cancer based on Gail model according to National Cancer Institute's Breast Cancer Risk Assessment Tool algorithm for specified race/ethnic groups and age intervals.\n             Gail MH, Brinton LA, et al (1989) <doi:10.1093/jnci/81.24.1879>. \n             Marthew PB, Gail MH, et al (2016) <doi:10.1093/jnci/djw215>.  "
  },
  {
    "id": 1806,
    "package_name": "BCT",
    "title": "Bayesian Context Trees for Discrete Time Series",
    "description": "An implementation of a collection of tools for exact Bayesian inference with discrete times series. This package contains functions that can be used for prediction, model selection, estimation, segmentation/change-point detection and other statistical tasks. Specifically, the functions provided can be used for the exact computation of the prior predictive likelihood of the data, for the identification of the a posteriori most likely (MAP) variable-memory Markov models, for calculating the exact posterior probabilities and the AIC and BIC scores of these models, for prediction with respect to log-loss and 0-1 loss and segmentation/change-point detection. Example data sets from finance, genetics, animal communication and meteorology are also provided. Detailed descriptions of the underlying theory and algorithms can be found in [Kontoyiannis et al. 'Bayesian Context Trees: Modelling and exact inference for discrete time series.' Journal of the Royal Statistical Society: Series B (Statistical Methodology), April 2022. Available at: <arXiv:2007.14900> [stat.ME], July 2020] and [Lungu et al. 'Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees' <arXiv:2203.04341> [stat.ME], March 2022]. ",
    "version": "1.2",
    "maintainer": "Valentinian Mihai Lungu <valentinian.mihai@gmail.com>",
    "author": "Ioannis Papageorgiou, Valentinian Mihai Lungu, Ioannis Kontoyiannis",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BCT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BCT Bayesian Context Trees for Discrete Time Series An implementation of a collection of tools for exact Bayesian inference with discrete times series. This package contains functions that can be used for prediction, model selection, estimation, segmentation/change-point detection and other statistical tasks. Specifically, the functions provided can be used for the exact computation of the prior predictive likelihood of the data, for the identification of the a posteriori most likely (MAP) variable-memory Markov models, for calculating the exact posterior probabilities and the AIC and BIC scores of these models, for prediction with respect to log-loss and 0-1 loss and segmentation/change-point detection. Example data sets from finance, genetics, animal communication and meteorology are also provided. Detailed descriptions of the underlying theory and algorithms can be found in [Kontoyiannis et al. 'Bayesian Context Trees: Modelling and exact inference for discrete time series.' Journal of the Royal Statistical Society: Series B (Statistical Methodology), April 2022. Available at: <arXiv:2007.14900> [stat.ME], July 2020] and [Lungu et al. 'Change-point Detection and Segmentation of Discrete Data using Bayesian Context Trees' <arXiv:2203.04341> [stat.ME], March 2022].   "
  },
  {
    "id": 1861,
    "package_name": "BLModel",
    "title": "Black-Litterman Posterior Distribution",
    "description": "Posterior distribution in the Black-Litterman model is computed from a prior distribution given in the form of a time series of asset returns and a continuous distribution of views provided by the user as an external function.",
    "version": "1.0.2",
    "maintainer": "Andrzej Palczewski <A.Palczewski@mimuw.edu.pl>",
    "author": "Andrzej Palczewski [aut, cre],\n  Jan Palczewski [aut],\n  Alicja Gosiewska [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BLModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BLModel Black-Litterman Posterior Distribution Posterior distribution in the Black-Litterman model is computed from a prior distribution given in the form of a time series of asset returns and a continuous distribution of views provided by the user as an external function.  "
  },
  {
    "id": 1886,
    "package_name": "BOIN",
    "title": "Bayesian Optimal INterval (BOIN) Design for Single-Agent and\nDrug- Combination Phase I Clinical Trials",
    "description": "The Bayesian optimal interval (BOIN) design is a novel phase I\n    clinical trial design for finding the maximum tolerated dose (MTD). It can be\n    used to design both single-agent and drug-combination trials. The BOIN design\n    is motivated by the top priority and concern of clinicians when testing a new\n    drug, which is to effectively treat patients and minimize the chance of exposing\n    them to subtherapeutic or overly toxic doses. The prominent advantage of the\n    BOIN design is that it achieves simplicity and superior performance at the same\n    time. The BOIN design is algorithm-based and can be implemented in a simple\n    way similar to the traditional 3+3 design. The BOIN design yields an average\n    performance that is comparable to that of the continual reassessment method\n    (CRM, one of the best model-based designs) in terms of selecting the MTD, but\n    has a substantially lower risk of assigning patients to subtherapeutic or overly\n    toxic doses. For tutorial, please check Yan et al. (2020) <doi:10.18637/jss.v094.i13>.",
    "version": "2.7.2",
    "maintainer": "Ying Yuan <yyuan@mdanderson.org>",
    "author": "Ying Yuan and Suyu Liu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BOIN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BOIN Bayesian Optimal INterval (BOIN) Design for Single-Agent and\nDrug- Combination Phase I Clinical Trials The Bayesian optimal interval (BOIN) design is a novel phase I\n    clinical trial design for finding the maximum tolerated dose (MTD). It can be\n    used to design both single-agent and drug-combination trials. The BOIN design\n    is motivated by the top priority and concern of clinicians when testing a new\n    drug, which is to effectively treat patients and minimize the chance of exposing\n    them to subtherapeutic or overly toxic doses. The prominent advantage of the\n    BOIN design is that it achieves simplicity and superior performance at the same\n    time. The BOIN design is algorithm-based and can be implemented in a simple\n    way similar to the traditional 3+3 design. The BOIN design yields an average\n    performance that is comparable to that of the continual reassessment method\n    (CRM, one of the best model-based designs) in terms of selecting the MTD, but\n    has a substantially lower risk of assigning patients to subtherapeutic or overly\n    toxic doses. For tutorial, please check Yan et al. (2020) <doi:10.18637/jss.v094.i13>.  "
  },
  {
    "id": 1898,
    "package_name": "BRINDA",
    "title": "Computation of BRINDA Adjusted Micronutrient Biomarkers for\nInflammation",
    "description": "Inflammation can affect many micronutrient biomarkers and can thus lead to incorrect diagnosis of individuals and to over- or under-estimate the prevalence of deficiency in a population. Biomarkers Reflecting Inflammation and Nutritional Determinants of Anemia (BRINDA) is a multi-agency and multi-country partnership designed to improve the interpretation of nutrient biomarkers in settings of inflammation and to generate context-specific estimates of risk factors for anemia (Suchdev (2016) <doi:10.3945/an.115.010215>). In the past few years, BRINDA published a series of papers to provide guidance on how to adjust micronutrient biomarkers, retinol binding protein, serum retinol, serum ferritin by Namaste (2020), soluble transferrin receptor (sTfR), serum zinc, serum and Red Blood Cell (RBC) folate, and serum B-12, using inflammation markers, alpha-1-acid glycoprotein (AGP) and/or C-Reactive Protein (CRP) by Namaste (2020) <doi:10.1093/ajcn/nqaa141>, Rohner (2017) <doi:10.3945/ajcn.116.142232>, McDonald (2020) <doi:10.1093/ajcn/nqz304>, and Young (2020) <doi:10.1093/ajcn/nqz303>. The BRINDA inflammation adjustment method mainly focuses on Women of Reproductive Age (WRA) and Preschool-age Children (PSC); however, the general principle of the BRINDA method might apply to other population groups. The BRINDA R package is a user-friendly all-in-one R package that uses a series of functions to implement BRINDA adjustment method, as described above. The BRINDA R package will first carry out rigorous checks and provides users guidance to correct data or input errors (if they occur) prior to inflammation adjustments. After no errors are detected, the package implements the BRINDA inflammation adjustment for up to five micronutrient biomarkers, namely retinol-binding-protein, serum retinol, serum ferritin, sTfR, and serum zinc (when appropriate), using inflammation indicators of AGP and/or CRP for various population groups. Of note, adjustment for serum and RBC folate and serum B-12 is not included in the R package, since evidence shows that no adjustment is needed for these micronutrient biomarkers in either WRA or PSC groups (Young (2020) <doi:10.1093/ajcn/nqz303>).",
    "version": "0.1.5",
    "maintainer": "Hanqi Luo <LUOHANQI@gmail.com>",
    "author": "Hanqi Luo [cre, aut] (ORCID: <https://orcid.org/0000-0001-6253-5818>),\n  O Yaw Addo [aut] (ORCID: <https://orcid.org/0000-0003-1269-759X>),\n  Jiaxi Geng [ctb]",
    "url": "https://github.com/hanqiluo/BRINDA",
    "bug_reports": "https://github.com/hanqiluo/BRINDA/issues",
    "repository": "https://cran.r-project.org/package=BRINDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BRINDA Computation of BRINDA Adjusted Micronutrient Biomarkers for\nInflammation Inflammation can affect many micronutrient biomarkers and can thus lead to incorrect diagnosis of individuals and to over- or under-estimate the prevalence of deficiency in a population. Biomarkers Reflecting Inflammation and Nutritional Determinants of Anemia (BRINDA) is a multi-agency and multi-country partnership designed to improve the interpretation of nutrient biomarkers in settings of inflammation and to generate context-specific estimates of risk factors for anemia (Suchdev (2016) <doi:10.3945/an.115.010215>). In the past few years, BRINDA published a series of papers to provide guidance on how to adjust micronutrient biomarkers, retinol binding protein, serum retinol, serum ferritin by Namaste (2020), soluble transferrin receptor (sTfR), serum zinc, serum and Red Blood Cell (RBC) folate, and serum B-12, using inflammation markers, alpha-1-acid glycoprotein (AGP) and/or C-Reactive Protein (CRP) by Namaste (2020) <doi:10.1093/ajcn/nqaa141>, Rohner (2017) <doi:10.3945/ajcn.116.142232>, McDonald (2020) <doi:10.1093/ajcn/nqz304>, and Young (2020) <doi:10.1093/ajcn/nqz303>. The BRINDA inflammation adjustment method mainly focuses on Women of Reproductive Age (WRA) and Preschool-age Children (PSC); however, the general principle of the BRINDA method might apply to other population groups. The BRINDA R package is a user-friendly all-in-one R package that uses a series of functions to implement BRINDA adjustment method, as described above. The BRINDA R package will first carry out rigorous checks and provides users guidance to correct data or input errors (if they occur) prior to inflammation adjustments. After no errors are detected, the package implements the BRINDA inflammation adjustment for up to five micronutrient biomarkers, namely retinol-binding-protein, serum retinol, serum ferritin, sTfR, and serum zinc (when appropriate), using inflammation indicators of AGP and/or CRP for various population groups. Of note, adjustment for serum and RBC folate and serum B-12 is not included in the R package, since evidence shows that no adjustment is needed for these micronutrient biomarkers in either WRA or PSC groups (Young (2020) <doi:10.1093/ajcn/nqz303>).  "
  },
  {
    "id": 1913,
    "package_name": "BSTZINB",
    "title": "Association Among Disease Counts and Socio-Environmental Factors",
    "description": "Estimation of association between disease or death counts (e.g. COVID-19) and socio-environmental risk factors using a zero-inflated Bayesian spatiotemporal model. Non-spatiotemporal models and/or models without zero-inflation are also included for comparison. Functions to produce corresponding maps are also included. See Chakraborty et al. (2022) <doi:10.1007/s13253-022-00487-1> for more details on the method.",
    "version": "2.0.1",
    "maintainer": "Suman Majumder <smajumd2@gmail.com>",
    "author": "Suman Majumder [cre, aut, cph],\n  Yoon-Bae Jun [aut, cph],\n  Sounak Chakraborty [ctb],\n  Chae-Young Lim [ctb],\n  Tanujit Dey [ctb]",
    "url": "https://github.com/SumanM47/BSTZINB",
    "bug_reports": "https://github.com/SumanM47/BSTZINB/issues",
    "repository": "https://cran.r-project.org/package=BSTZINB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BSTZINB Association Among Disease Counts and Socio-Environmental Factors Estimation of association between disease or death counts (e.g. COVID-19) and socio-environmental risk factors using a zero-inflated Bayesian spatiotemporal model. Non-spatiotemporal models and/or models without zero-inflation are also included for comparison. Functions to produce corresponding maps are also included. See Chakraborty et al. (2022) <doi:10.1007/s13253-022-00487-1> for more details on the method.  "
  },
  {
    "id": 1935,
    "package_name": "BacenAPI",
    "title": "Data Collection from the Central Bank of Brazil",
    "description": "Provides tools to facilitate the access and processing of data \n    from the Central Bank of Brazil API. The package allows users \n    to retrieve economic and financial data, transforming them into usable \n    tabular formats for further analysis. The data is obtained from the \n    Central Bank of Brazil API: <https://api.bcb.gov.br/dados/serie/bcdata.sgs.{series_code}/dados?formato=json&dataInicial={start_date}&dataFinal={end_date}>.",
    "version": "0.3.1",
    "maintainer": "Lissandro Sousa <lisandrosousa54@gmail.com>",
    "author": "Paulo Icaro [aut],\n  Lissandro Sousa [cre, aut],\n  Francisco Gildemir Ferreira da Silva [ths, aut]",
    "url": "https://github.com/LissandroSousa/BacenAPI.r",
    "bug_reports": "https://github.com/LissandroSousa/BacenAPI.r/issues",
    "repository": "https://cran.r-project.org/package=BacenAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BacenAPI Data Collection from the Central Bank of Brazil Provides tools to facilitate the access and processing of data \n    from the Central Bank of Brazil API. The package allows users \n    to retrieve economic and financial data, transforming them into usable \n    tabular formats for further analysis. The data is obtained from the \n    Central Bank of Brazil API: <https://api.bcb.gov.br/dados/serie/bcdata.sgs.{series_code}/dados?formato=json&dataInicial={start_date}&dataFinal={end_date}>.  "
  },
  {
    "id": 1947,
    "package_name": "BatchGetSymbols",
    "title": "Downloads and Organizes Financial Data for Multiple Tickers",
    "description": "Makes it easy to download financial data from Yahoo Finance <https://finance.yahoo.com/>.",
    "version": "2.6.4",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BatchGetSymbols",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BatchGetSymbols Downloads and Organizes Financial Data for Multiple Tickers Makes it easy to download financial data from Yahoo Finance <https://finance.yahoo.com/>.  "
  },
  {
    "id": 1995,
    "package_name": "BayesPieceHazSelect",
    "title": "Variable Selection in a Hierarchical Bayesian Model for a Hazard\nFunction",
    "description": "Fits a piecewise exponential hazard to survival data using a\n    Hierarchical Bayesian model with an Intrinsic Conditional Autoregressive\n    formulation for the spatial dependency in the hazard rates for each piece.\n    This function uses Metropolis- Hastings-Green MCMC to allow the number of split\n    points to vary and also uses Stochastic Search Variable Selection to determine\n    what covariates drive the risk of the event. This function outputs trace plots\n    depicting the number of split points in the hazard and the number of variables\n    included in the hazard. The function saves all posterior quantities to the\n    desired path.",
    "version": "1.1.0",
    "maintainer": "Andrew Chapple <AndrewChapple21@gmail.com>",
    "author": "Andrew Chapple [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesPieceHazSelect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesPieceHazSelect Variable Selection in a Hierarchical Bayesian Model for a Hazard\nFunction Fits a piecewise exponential hazard to survival data using a\n    Hierarchical Bayesian model with an Intrinsic Conditional Autoregressive\n    formulation for the spatial dependency in the hazard rates for each piece.\n    This function uses Metropolis- Hastings-Green MCMC to allow the number of split\n    points to vary and also uses Stochastic Search Variable Selection to determine\n    what covariates drive the risk of the event. This function outputs trace plots\n    depicting the number of split points in the hazard and the number of variables\n    included in the hazard. The function saves all posterior quantities to the\n    desired path.  "
  },
  {
    "id": 2038,
    "package_name": "BeeGUTS",
    "title": "General Unified Threshold Model of Survival for Bees using\nBayesian Inference",
    "description": "Tools to calibrate, validate, and make predictions with the\n    General Unified Threshold model of Survival adapted for Bee species. The\n    model is presented in the publication from Baas, J., Goussen, B., Miles, M.,\n    Preuss, T.G., Roessing, I. (2022) <doi:10.1002/etc.5423> and \n    Baas, J., Goussen, B., Taenzler, V., Roeben, V., Miles, M., Preuss, T.G., \n    van den Berg, S., Roessink, I. (2024) <doi:10.1002/etc.5871>, and is based on the \n    GUTS framework Jager, T., Albert, C., Preuss, T.G. and Ashauer, R. (2011) \n    <doi:10.1021/es103092a>.\n    The authors are grateful to Bayer A.G. for its financial support.",
    "version": "1.4.0",
    "maintainer": "Carlo Romoli <carlo.romoli@ibacon.com>",
    "author": "Benoit Goussen [aut] (ORCID: <https://orcid.org/0000-0001-7204-7981>),\n  Liubov Zakharova [ctb],\n  Carlo Romoli [aut, cre],\n  Bayer AG [cph, fnd],\n  ibacon GmbH [cph]",
    "url": "https://github.com/bgoussen/BeeGUTS",
    "bug_reports": "https://github.com/bgoussen/BeeGUTS/issues",
    "repository": "https://cran.r-project.org/package=BeeGUTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BeeGUTS General Unified Threshold Model of Survival for Bees using\nBayesian Inference Tools to calibrate, validate, and make predictions with the\n    General Unified Threshold model of Survival adapted for Bee species. The\n    model is presented in the publication from Baas, J., Goussen, B., Miles, M.,\n    Preuss, T.G., Roessing, I. (2022) <doi:10.1002/etc.5423> and \n    Baas, J., Goussen, B., Taenzler, V., Roeben, V., Miles, M., Preuss, T.G., \n    van den Berg, S., Roessink, I. (2024) <doi:10.1002/etc.5871>, and is based on the \n    GUTS framework Jager, T., Albert, C., Preuss, T.G. and Ashauer, R. (2011) \n    <doi:10.1021/es103092a>.\n    The authors are grateful to Bayer A.G. for its financial support.  "
  },
  {
    "id": 2082,
    "package_name": "BioM2",
    "title": "Biologically Explainable Machine Learning Framework",
    "description": "Biologically Explainable Machine Learning Framework for Phenotype Prediction using omics data described in Chen and Schwarz (2017) <doi:10.48550/arXiv.1712.00336>.Identifying reproducible and interpretable biological patterns from high-dimensional omics data is a critical factor in understanding the risk mechanism of complex disease. As such, explainable machine learning can offer biological insight in addition to personalized risk scoring.In this process, a feature space of biological pathways will be generated, and the feature space can also be subsequently analyzed using WGCNA (Described in Horvath and Zhang (2005) <doi:10.2202/1544-6115.1128> and Langfelder and Horvath (2008) <doi:10.1186/1471-2105-9-559> ) methods.",
    "version": "1.1.3",
    "maintainer": "Shunjie Zhang <zhang.shunjie@qq.com>",
    "author": "Shunjie Zhang [aut, cre],\n  Junfang Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BioM2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioM2 Biologically Explainable Machine Learning Framework Biologically Explainable Machine Learning Framework for Phenotype Prediction using omics data described in Chen and Schwarz (2017) <doi:10.48550/arXiv.1712.00336>.Identifying reproducible and interpretable biological patterns from high-dimensional omics data is a critical factor in understanding the risk mechanism of complex disease. As such, explainable machine learning can offer biological insight in addition to personalized risk scoring.In this process, a feature space of biological pathways will be generated, and the feature space can also be subsequently analyzed using WGCNA (Described in Horvath and Zhang (2005) <doi:10.2202/1544-6115.1128> and Langfelder and Horvath (2008) <doi:10.1186/1471-2105-9-559> ) methods.  "
  },
  {
    "id": 2088,
    "package_name": "BioProbability",
    "title": "Probability in Biostatistics",
    "description": "Several tools for analyzing diagnostic tests and 2x2 contingency tables are provided. In particular, positive and negative predictive values for a diagnostic tests can be calculated from prevalence, sensitivity and specificity values. For contingency tables, relative risk and odds ratio measures are estimated. Furthermore, confidence intervals are provided. ",
    "version": "1.0",
    "maintainer": "Paula Saavedra-Nieves <paula.saavedra@usc.es>",
    "author": "Alejandro Saavedra-Nieves, Paula Saavedra-Nieves",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BioProbability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioProbability Probability in Biostatistics Several tools for analyzing diagnostic tests and 2x2 contingency tables are provided. In particular, positive and negative predictive values for a diagnostic tests can be calculated from prevalence, sensitivity and specificity values. For contingency tables, relative risk and odds ratio measures are estimated. Furthermore, confidence intervals are provided.   "
  },
  {
    "id": 2110,
    "package_name": "Bivariate.Pareto",
    "title": "Bivariate Pareto Models",
    "description": "Perform competing risks analysis under bivariate Pareto models. See Shih et al. (2019) <doi:10.1080/03610926.2018.1425450> for details.",
    "version": "1.0.3",
    "maintainer": "Jia-Han Shih <tommy355097@gmail.com>",
    "author": "Jia-Han Shih, Wei Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Bivariate.Pareto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Bivariate.Pareto Bivariate Pareto Models Perform competing risks analysis under bivariate Pareto models. See Shih et al. (2019) <doi:10.1080/03610926.2018.1425450> for details.  "
  },
  {
    "id": 2117,
    "package_name": "BlockmodelingGUI",
    "title": "GUI for the Generalised Blockmodeling of Valued Networks",
    "description": "This app provides some useful tools for Offering an accessible GUI for generalised blockmodeling of single-relation, one-mode networks. The user can execute blockmodeling without having to write a line code by using the app's visual helps. Moreover, there are several ways to visualisations networks and their partitions. Finally, the results can be exported as if they were produced by writing code. The development of this package is financially supported by the Slovenian Research Agency (www.arrs.gov.si) within the research project J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).",
    "version": "1.8.4",
    "maintainer": "Fabio Ashtar Telarico <telaricof@fdv.uni-lj.si>",
    "author": "Fabio Ashtar Telarico [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>),\n  Ale\u0161 \u017diberna [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BlockmodelingGUI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BlockmodelingGUI GUI for the Generalised Blockmodeling of Valued Networks This app provides some useful tools for Offering an accessible GUI for generalised blockmodeling of single-relation, one-mode networks. The user can execute blockmodeling without having to write a line code by using the app's visual helps. Moreover, there are several ways to visualisations networks and their partitions. Finally, the results can be exported as if they were produced by writing code. The development of this package is financially supported by the Slovenian Research Agency (www.arrs.gov.si) within the research project J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).  "
  },
  {
    "id": 2119,
    "package_name": "BlueCarbon",
    "title": "Estimation of Organic Carbon Stocks and Sequestration Rates from\nSoil Core Data",
    "description": "Tools to estimate soil organic carbon stocks and sequestration rates \n    in blue carbon ecosystems. 'BlueCarbon' contains functions to estimate and correct\n    for core compaction, estimate sample thickness, estimate organic carbon content \n    from organic matter content, estimate organic carbon stocks and \n    sequestration rates, and visualize the error of carbon stock extrapolation. ",
    "version": "0.1.1",
    "maintainer": "Nerea Pi\u00f1eiro-Juncal <np.juncal@gmail.com>",
    "author": "Nerea Pi\u00f1eiro-Juncal [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3767-1812>),\n  Julen Astigarraga [aut] (ORCID:\n    <https://orcid.org/0000-0001-9520-3713>),\n  Valentina Costa [aut] (ORCID: <https://orcid.org/0000-0002-1513-0284>),\n  Marcio Martins [aut] (ORCID: <https://orcid.org/0000-0002-6969-2215>),\n  Francisco Rodriguez-Sanchez [aut] (ORCID:\n    <https://orcid.org/0000-0002-7981-1599>)",
    "url": "https://github.com/EcologyR/BlueCarbon,\nhttps://ecologyr.github.io/BlueCarbon/",
    "bug_reports": "https://github.com/EcologyR/BlueCarbon/issues",
    "repository": "https://cran.r-project.org/package=BlueCarbon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BlueCarbon Estimation of Organic Carbon Stocks and Sequestration Rates from\nSoil Core Data Tools to estimate soil organic carbon stocks and sequestration rates \n    in blue carbon ecosystems. 'BlueCarbon' contains functions to estimate and correct\n    for core compaction, estimate sample thickness, estimate organic carbon content \n    from organic matter content, estimate organic carbon stocks and \n    sequestration rates, and visualize the error of carbon stock extrapolation.   "
  },
  {
    "id": 2158,
    "package_name": "Buddle",
    "title": "A Deep Learning for Statistical Classification and Regression\nAnalysis with Random Effects",
    "description": "Statistical classification and regression have been popular among various fields and stayed in the limelight of scientists of those fields. Examples of the fields include clinical trials where the statistical classification of patients is indispensable to predict the clinical courses of diseases. Considering the negative impact of diseases on performing daily tasks, correctly classifying patients based on the clinical information is vital in that we need to identify patients of the high-risk group to develop a severe state and arrange medical treatment for them at an opportune moment. Deep learning - a part of artificial intelligence - has gained much attention, and research on it burgeons during past decades: see, e.g, Kazemi and Mirroshandel (2018) <DOI:10.1016/j.artmed.2017.12.001>. It is a veritable technique which was originally designed for the classification, and hence, the Buddle package can provide sublime solutions to various challenging classification and regression problems encountered in the clinical trials. The Buddle package is based on the back-propagation algorithm - together with various powerful techniques such as batch normalization and dropout - which performs a multi-layer feed-forward neural network: see Krizhevsky et. al (2017) <DOI:10.1145/3065386>, Schmidhuber (2015) <DOI:10.1016/j.neunet.2014.09.003> and LeCun et al. (1998) <DOI:10.1109/5.726791> for more details. This package contains two main functions: TrainBuddle() and FetchBuddle(). TrainBuddle() builds a feed-forward neural network model and trains the model. FetchBuddle() recalls the trained model which is the output of TrainBuddle(), classifies or regresses given data, and make a final prediction for the data.",
    "version": "2.0.1",
    "maintainer": "Jiwoong Kim <jwboys26@gmail.com>",
    "author": "Jiwoong Kim <jwboys26 at gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Buddle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Buddle A Deep Learning for Statistical Classification and Regression\nAnalysis with Random Effects Statistical classification and regression have been popular among various fields and stayed in the limelight of scientists of those fields. Examples of the fields include clinical trials where the statistical classification of patients is indispensable to predict the clinical courses of diseases. Considering the negative impact of diseases on performing daily tasks, correctly classifying patients based on the clinical information is vital in that we need to identify patients of the high-risk group to develop a severe state and arrange medical treatment for them at an opportune moment. Deep learning - a part of artificial intelligence - has gained much attention, and research on it burgeons during past decades: see, e.g, Kazemi and Mirroshandel (2018) <DOI:10.1016/j.artmed.2017.12.001>. It is a veritable technique which was originally designed for the classification, and hence, the Buddle package can provide sublime solutions to various challenging classification and regression problems encountered in the clinical trials. The Buddle package is based on the back-propagation algorithm - together with various powerful techniques such as batch normalization and dropout - which performs a multi-layer feed-forward neural network: see Krizhevsky et. al (2017) <DOI:10.1145/3065386>, Schmidhuber (2015) <DOI:10.1016/j.neunet.2014.09.003> and LeCun et al. (1998) <DOI:10.1109/5.726791> for more details. This package contains two main functions: TrainBuddle() and FetchBuddle(). TrainBuddle() builds a feed-forward neural network model and trains the model. FetchBuddle() recalls the trained model which is the output of TrainBuddle(), classifies or regresses given data, and make a final prediction for the data.  "
  },
  {
    "id": 2160,
    "package_name": "BurStFin",
    "title": "Burns Statistics Financial",
    "description": "A suite of functions for finance, including the estimation\n\tof variance matrices via a statistical factor model or\n\tLedoit-Wolf shrinkage.",
    "version": "1.3",
    "maintainer": "Pat Burns <patrick@burns-stat.com>",
    "author": "Burns Statistics",
    "url": "https://www.burns-stat.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BurStFin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BurStFin Burns Statistics Financial A suite of functions for finance, including the estimation\n\tof variance matrices via a statistical factor model or\n\tLedoit-Wolf shrinkage.  "
  },
  {
    "id": 2163,
    "package_name": "BuyseTest",
    "title": "Generalized Pairwise Comparisons",
    "description": "Implementation of the Generalized Pairwise Comparisons (GPC) as defined in Buyse (2010) <doi:10.1002/sim.3923> for complete observations, and extended in Peron (2018) <doi:10.1177/0962280216658320> to deal with right-censoring. GPC compare two groups of observations (intervention vs. control group) regarding several prioritized endpoints to estimate the probability that a random observation drawn from one group performs better/worse/equivalently than a random observation drawn from the other group. Summary statistics such as the net treatment benefit, win ratio, or win odds are then deduced from these probabilities. Confidence intervals and p-values are obtained based on asymptotic results (Ozenne 2021 <doi:10.1177/09622802211037067>), non-parametric bootstrap, or permutations. The software enables the use of thresholds of minimal importance difference, stratification, non-prioritized endpoints (O Brien test), and can handle right-censoring and competing-risks.",
    "version": "3.3.4",
    "maintainer": "Brice Ozenne <brice.mh.ozenne@gmail.com>",
    "author": "Brice Ozenne [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9694-2956>),\n  Eva Cantagallo [aut],\n  William Anderson [aut],\n  Julien Peron [ctb],\n  Johan Verbeeck [ctb]",
    "url": "https://github.com/bozenne/BuyseTest",
    "bug_reports": "https://github.com/bozenne/BuyseTest/issues",
    "repository": "https://cran.r-project.org/package=BuyseTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BuyseTest Generalized Pairwise Comparisons Implementation of the Generalized Pairwise Comparisons (GPC) as defined in Buyse (2010) <doi:10.1002/sim.3923> for complete observations, and extended in Peron (2018) <doi:10.1177/0962280216658320> to deal with right-censoring. GPC compare two groups of observations (intervention vs. control group) regarding several prioritized endpoints to estimate the probability that a random observation drawn from one group performs better/worse/equivalently than a random observation drawn from the other group. Summary statistics such as the net treatment benefit, win ratio, or win odds are then deduced from these probabilities. Confidence intervals and p-values are obtained based on asymptotic results (Ozenne 2021 <doi:10.1177/09622802211037067>), non-parametric bootstrap, or permutations. The software enables the use of thresholds of minimal importance difference, stratification, non-prioritized endpoints (O Brien test), and can handle right-censoring and competing-risks.  "
  },
  {
    "id": 2204,
    "package_name": "CBSr",
    "title": "Fits Cubic Bezier Spline Functions to Intertemporal and Risky\nChoice Data",
    "description": "Uses monotonically constrained Cubic Bezier Splines (CBS) to approximate latent utility functions in intertemporal choice and risky choice data. For more information, see Lee, Glaze, Bradlow, and Kable <doi:10.1007/s11336-020-09723-4>.",
    "version": "1.0.5",
    "maintainer": "Sangil Lee <sangillee3rd@gmail.com>",
    "author": "Sangil Lee [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CBSr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CBSr Fits Cubic Bezier Spline Functions to Intertemporal and Risky\nChoice Data Uses monotonically constrained Cubic Bezier Splines (CBS) to approximate latent utility functions in intertemporal choice and risky choice data. For more information, see Lee, Glaze, Bradlow, and Kable <doi:10.1007/s11336-020-09723-4>.  "
  },
  {
    "id": 2237,
    "package_name": "CFC",
    "title": "Cause-Specific Framework for Competing-Risk Analysis",
    "description": "Numerical integration of cause-specific survival curves to arrive at cause-specific cumulative incidence functions,\n    with three usage modes: 1) Convenient API for parametric survival regression followed by competing-risk analysis, 2) API for\n    CFC, accepting user-specified survival functions in R, and 3) Same as 2, but accepting survival functions in C++. For \n    mathematical details and software tutorial, see Mahani and Sharabiani (2019) <DOI:10.18637/jss.v089.i09>. ",
    "version": "1.2.1",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "author": "Mansour T.A. Sharabiani [aut],\n  Alireza S. Mahani [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CFC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CFC Cause-Specific Framework for Competing-Risk Analysis Numerical integration of cause-specific survival curves to arrive at cause-specific cumulative incidence functions,\n    with three usage modes: 1) Convenient API for parametric survival regression followed by competing-risk analysis, 2) API for\n    CFC, accepting user-specified survival functions in R, and 3) Same as 2, but accepting survival functions in C++. For \n    mathematical details and software tutorial, see Mahani and Sharabiani (2019) <DOI:10.18637/jss.v089.i09>.   "
  },
  {
    "id": 2251,
    "package_name": "CHNCapitalStock",
    "title": "Compute Chinese Capital Stocks",
    "description": "Compute Chinese capital stocks in provinces level, based on Zhang (2008) <DOI:10.1080/14765280802028302>. ",
    "version": "0.1.1",
    "maintainer": "Pu Chen <shengnehs@qq.com>",
    "author": "Pu Chen [aut, cre]",
    "url": "https://github.com/common2016/CapitalStock",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CHNCapitalStock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CHNCapitalStock Compute Chinese Capital Stocks Compute Chinese capital stocks in provinces level, based on Zhang (2008) <DOI:10.1080/14765280802028302>.   "
  },
  {
    "id": 2273,
    "package_name": "CJIVE",
    "title": "Canonical Joint and Individual Variation Explained (CJIVE)",
    "description": "Joint and Individual Variation Explained (JIVE) is a method for decomposing multiple datasets obtained on the same subjects into\n\t\tshared structure, structure unique to each dataset, and noise. The two most common implementations are R.JIVE, an iterative\n\t\tapproach, and AJIVE, which uses principal angle analysis. JIVE estimates subspaces but interpreting these subspaces can be\n\t\tchallenging with AJIVE or R.JIVE. We expand upon insights into AJIVE as a canonical correlation analysis (CCA) of principal component\n\t\tscores. This reformulation, which we call CJIVE, 1) provides an ordering of joint components by the degree of correlation between\n\t\tcorresponding canonical variables; 2) uses a computationally efficient permutation test for the number of joint components, which\n\t\tprovides a p-value for each component; and 3) can be used to predict subject scores for out-of-sample observations.\n\t\tPlease cite the following article when utilizing this package: \n\t\tMurden, R., Zhang, Z., Guo, Y., & Risk, B. (2022) <doi:10.3389/fnins.2022.969510>.",
    "version": "0.1.0",
    "maintainer": "Raphiel Murden <rmurden@emory.edu>",
    "author": "Raphiel Murden [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6396-9105>),\n  Benjamin Risk [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CJIVE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CJIVE Canonical Joint and Individual Variation Explained (CJIVE) Joint and Individual Variation Explained (JIVE) is a method for decomposing multiple datasets obtained on the same subjects into\n\t\tshared structure, structure unique to each dataset, and noise. The two most common implementations are R.JIVE, an iterative\n\t\tapproach, and AJIVE, which uses principal angle analysis. JIVE estimates subspaces but interpreting these subspaces can be\n\t\tchallenging with AJIVE or R.JIVE. We expand upon insights into AJIVE as a canonical correlation analysis (CCA) of principal component\n\t\tscores. This reformulation, which we call CJIVE, 1) provides an ordering of joint components by the degree of correlation between\n\t\tcorresponding canonical variables; 2) uses a computationally efficient permutation test for the number of joint components, which\n\t\tprovides a p-value for each component; and 3) can be used to predict subject scores for out-of-sample observations.\n\t\tPlease cite the following article when utilizing this package: \n\t\tMurden, R., Zhang, Z., Guo, Y., & Risk, B. (2022) <doi:10.3389/fnins.2022.969510>.  "
  },
  {
    "id": 2276,
    "package_name": "CLA",
    "title": "Critical Line Algorithm in Pure R",
    "description": "Implements 'Markowitz' Critical Line Algorithm ('CLA') for classical\n  mean-variance portfolio optimization, see Markowitz (1952) <doi:10.2307/2975974>.\n  Care has been taken for correctness in light of previous buggy implementations.",
    "version": "0.96-3",
    "maintainer": "Martin Maechler <maechler@stat.math.ethz.ch>",
    "author": "Yanhao Shi [aut],\n  Martin Maechler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8685-9910>)",
    "url": "https://gitlab.math.ethz.ch/maechler/CLA/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CLA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CLA Critical Line Algorithm in Pure R Implements 'Markowitz' Critical Line Algorithm ('CLA') for classical\n  mean-variance portfolio optimization, see Markowitz (1952) <doi:10.2307/2975974>.\n  Care has been taken for correctness in light of previous buggy implementations.  "
  },
  {
    "id": 2299,
    "package_name": "CNAIM",
    "title": "Common Network Asset Indices Methodology (CNAIM)",
    "description": "Implementation of the CNAIM standard in R. Contains a series of\n    algorithms which determine the probability of failure, consequences of\n    failure and monetary risk associated with electricity distribution\n    companies' assets such as transformers and cables. Results are visualized\n    in an easy-to-understand risk matrix.",
    "version": "2.1.4",
    "maintainer": "Mohsin Vindhani <mohsin@utiligize.com>",
    "author": "Emil Larsen [aut],\n  Kalle Hansen [aut],\n  Kenneth Rosenorn [aut],\n  Peter Larsen [aut],\n  Utiligize ApS [cph],\n  Mohsin Vindhani [aut, cre]",
    "url": "https://www.cnaim.io/",
    "bug_reports": "https://github.com/Utiligize/CNAIM/issues",
    "repository": "https://cran.r-project.org/package=CNAIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CNAIM Common Network Asset Indices Methodology (CNAIM) Implementation of the CNAIM standard in R. Contains a series of\n    algorithms which determine the probability of failure, consequences of\n    failure and monetary risk associated with electricity distribution\n    companies' assets such as transformers and cables. Results are visualized\n    in an easy-to-understand risk matrix.  "
  },
  {
    "id": 2312,
    "package_name": "COMBAT",
    "title": "A Combined Association Test for Genes using Summary Statistics",
    "description": "Genome-wide association studies (GWAS) have been widely used for identifying common variants associated with complex diseases. Due to the small effect sizes of common variants, the power to detect individual risk variants is generally low. Complementary to SNP-level analysis, a variety of gene-based association tests have been proposed. However, the power of existing gene-based tests is often dependent on the underlying genetic models, and it is not known a priori which test is optimal.  Here we proposed COMBined Association Test (COMBAT) to incorporate strengths from multiple existing gene-based tests, including VEGAS, GATES and simpleM. Compared to individual tests, COMBAT shows higher overall performance and robustness across a wide range of genetic models. The algorithm behind this method is described in Wang et al (2017) <doi:10.1534/genetics.117.300257>.",
    "version": "0.0.4",
    "maintainer": "Minghui Wang <m.h.wang@live.com>",
    "author": "Minghui Wang, Yiyuan Liu, Shizhong Han",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=COMBAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COMBAT A Combined Association Test for Genes using Summary Statistics Genome-wide association studies (GWAS) have been widely used for identifying common variants associated with complex diseases. Due to the small effect sizes of common variants, the power to detect individual risk variants is generally low. Complementary to SNP-level analysis, a variety of gene-based association tests have been proposed. However, the power of existing gene-based tests is often dependent on the underlying genetic models, and it is not known a priori which test is optimal.  Here we proposed COMBined Association Test (COMBAT) to incorporate strengths from multiple existing gene-based tests, including VEGAS, GATES and simpleM. Compared to individual tests, COMBAT shows higher overall performance and robustness across a wide range of genetic models. The algorithm behind this method is described in Wang et al (2017) <doi:10.1534/genetics.117.300257>.  "
  },
  {
    "id": 2328,
    "package_name": "CPBayes",
    "title": "Bayesian Meta Analysis for Studying Cross-Phenotype Genetic\nAssociations",
    "description": "A Bayesian meta-analysis method for studying cross-phenotype\n    genetic associations. It uses summary-level data across multiple phenotypes to\n    simultaneously measure the evidence of aggregate-level pleiotropic association\n    and estimate an optimal subset of traits associated with the risk locus. CPBayes\n    is based on a spike and slab prior. The methodology is available from: A Majumdar, T Haldar, S Bhattacharya, JS Witte (2018) <doi:10.1371/journal.pgen.1007139>.",
    "version": "1.1.0",
    "maintainer": "Arunabha Majumdar <statgen.arunabha@gmail.com>",
    "author": "Arunabha Majumdar <statgen.arunabha@gmail.com> [aut, cre],\n    Tanushree Haldar <tanushree.haldar@gmail.com> [aut],\n    John Witte [ctb]",
    "url": "https://github.com/ArunabhaCodes/CPBayes",
    "bug_reports": "https://github.com/ArunabhaCodes/CPBayes/issues",
    "repository": "https://cran.r-project.org/package=CPBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CPBayes Bayesian Meta Analysis for Studying Cross-Phenotype Genetic\nAssociations A Bayesian meta-analysis method for studying cross-phenotype\n    genetic associations. It uses summary-level data across multiple phenotypes to\n    simultaneously measure the evidence of aggregate-level pleiotropic association\n    and estimate an optimal subset of traits associated with the risk locus. CPBayes\n    is based on a spike and slab prior. The methodology is available from: A Majumdar, T Haldar, S Bhattacharya, JS Witte (2018) <doi:10.1371/journal.pgen.1007139>.  "
  },
  {
    "id": 2350,
    "package_name": "CSCNet",
    "title": "Fitting and Tuning Regularized Cause-Specific Cox Models with\nElastic-Net Penalty",
    "description": "Flexible tools to fit, tune and obtain absolute risk predictions from regularized cause-specific cox models with elastic-net penalty.",
    "version": "0.1.2",
    "maintainer": "Shahin Roshani <s.roshani@nki.nl>",
    "author": "Shahin Roshani [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7518-157X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CSCNet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CSCNet Fitting and Tuning Regularized Cause-Specific Cox Models with\nElastic-Net Penalty Flexible tools to fit, tune and obtain absolute risk predictions from regularized cause-specific cox models with elastic-net penalty.  "
  },
  {
    "id": 2381,
    "package_name": "CVrisk",
    "title": "Compute Risk Scores for Cardiovascular Diseases",
    "description": "Calculate various cardiovascular disease risk scores from the\n    Framingham Heart Study (FHS), the American College of Cardiology (ACC),\n    and the American Heart Association (AHA) as described in D\u2019agostino, et al\n    (2008) <doi:10.1161/circulationaha.107.699579>, Goff, et al (2013)\n    <doi:10.1161/01.cir.0000437741.48606.98>, and Mclelland, et al (2015)\n    <doi:10.1016/j.jacc.2015.08.035>.",
    "version": "1.1.1",
    "maintainer": "Victor Castro <vcastro@mgh.harvard.edu>",
    "author": "Victor Castro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7390-6354>)",
    "url": "https://github.com/vcastro/CVrisk",
    "bug_reports": "https://github.com/vcastro/CVrisk/issues",
    "repository": "https://cran.r-project.org/package=CVrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CVrisk Compute Risk Scores for Cardiovascular Diseases Calculate various cardiovascular disease risk scores from the\n    Framingham Heart Study (FHS), the American College of Cardiology (ACC),\n    and the American Heart Association (AHA) as described in D\u2019agostino, et al\n    (2008) <doi:10.1161/circulationaha.107.699579>, Goff, et al (2013)\n    <doi:10.1161/01.cir.0000437741.48606.98>, and Mclelland, et al (2015)\n    <doi:10.1016/j.jacc.2015.08.035>.  "
  },
  {
    "id": 2384,
    "package_name": "CaMeA",
    "title": "Causal Meta-Analysis for Aggregated Data",
    "description": "A tool for causal meta-analysis. This package implements the aggregation formulas and inference methods proposed in Berenfeld et al. (2025) <doi:10.48550/arXiv.2505.20168>. Users can input aggregated data across multiple studies and compute causally meaningful aggregated effects of their choice (risk difference, risk ratio, odds ratio, etc) under user-specified population weighting. The built-in function camea() allows to obtain precise variance estimates for these effects and to compare the latter to a classical meta-analysis aggregate, the random effect model, as implemented in the 'metafor' package <https://CRAN.R-project.org/package=metafor>.  ",
    "version": "0.1.1",
    "maintainer": "Clement Berenfeld <clement.berenfeld@inria.fr>",
    "author": "Clement Berenfeld [ctb, cre],\n  Ahmed Boughdiri [ctb],\n  Charif El Gataa [aut],\n  Julie Josse [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CaMeA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CaMeA Causal Meta-Analysis for Aggregated Data A tool for causal meta-analysis. This package implements the aggregation formulas and inference methods proposed in Berenfeld et al. (2025) <doi:10.48550/arXiv.2505.20168>. Users can input aggregated data across multiple studies and compute causally meaningful aggregated effects of their choice (risk difference, risk ratio, odds ratio, etc) under user-specified population weighting. The built-in function camea() allows to obtain precise variance estimates for these effects and to compare the latter to a classical meta-analysis aggregate, the random effect model, as implemented in the 'metafor' package <https://CRAN.R-project.org/package=metafor>.    "
  },
  {
    "id": 2388,
    "package_name": "CalcThemAll.PRM",
    "title": "Calculate Pesticide Risk Metric (PRM) Values from Multiple\nPesticides...Calc Them All",
    "description": "Contains functions which can be used to calculate Pesticide Risk Metric \n    values in aquatic environments from concentrations of multiple pesticides with known species sensitive \n    distributions (SSDs). Pesticides provided by this package have all be validated\n    however if the user has their own pesticides with SSD values they can append them\n    to the pesticide_info table to include them in estimates.",
    "version": "1.1.1",
    "maintainer": "Alexander Bezzina <alex.h.bezzina@gmail.com>",
    "author": "Alexander Bezzina [aut, cre, cph],\n  Jennifer Strauss [aut],\n  Catherine Neelamraju [aut],\n  Hayley Kaminski [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CalcThemAll.PRM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CalcThemAll.PRM Calculate Pesticide Risk Metric (PRM) Values from Multiple\nPesticides...Calc Them All Contains functions which can be used to calculate Pesticide Risk Metric \n    values in aquatic environments from concentrations of multiple pesticides with known species sensitive \n    distributions (SSDs). Pesticides provided by this package have all be validated\n    however if the user has their own pesticides with SSD values they can append them\n    to the pesticide_info table to include them in estimates.  "
  },
  {
    "id": 2400,
    "package_name": "CardioDataSets",
    "title": "A Comprehensive Collection of Cardiovascular and Heart Disease\nDatasets",
    "description": "Offers a diverse collection of datasets focused on cardiovascular and heart disease research, including heart failure, myocardial infarction, aortic dissection, transplant outcomes, cardiovascular risk factors, drug efficacy, and mortality trends. \n    Designed for researchers, clinicians, epidemiologists, and data scientists, the package features clinical, epidemiological, and simulated datasets covering a wide range of conditions and treatments such as statins, anticoagulants, and beta blockers. \n    It supports analyses related to disease progression, treatment effects, rehospitalization, and public health outcomes across various cardiovascular patient populations.",
    "version": "0.2.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/cardiodatasets,\nhttps://lightbluetitan.github.io/cardiodatasets/",
    "bug_reports": "https://github.com/lightbluetitan/cardiodatasets/issues",
    "repository": "https://cran.r-project.org/package=CardioDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CardioDataSets A Comprehensive Collection of Cardiovascular and Heart Disease\nDatasets Offers a diverse collection of datasets focused on cardiovascular and heart disease research, including heart failure, myocardial infarction, aortic dissection, transplant outcomes, cardiovascular risk factors, drug efficacy, and mortality trends. \n    Designed for researchers, clinicians, epidemiologists, and data scientists, the package features clinical, epidemiological, and simulated datasets covering a wide range of conditions and treatments such as statins, anticoagulants, and beta blockers. \n    It supports analyses related to disease progression, treatment effects, rehospitalization, and public health outcomes across various cardiovascular patient populations.  "
  },
  {
    "id": 2407,
    "package_name": "CaseCohortCoxSurvival",
    "title": "Case-Cohort Cox Survival Inference",
    "description": "Cox model inference for relative hazard and covariate-specific pure risk estimated \n             from stratified and unstratified case-cohort data as described in \n             Etievant, L., Gail, M.H. (Lifetime Data Analysis, 2024) <doi:10.1007/s10985-024-09621-2>.",
    "version": "0.0.36",
    "maintainer": "Lola Etievant <lola.etievant@gmail.com>",
    "author": "Lola Etievant [cre, aut],\n  Mitchell H. Gail [aut],\n  Bill Wheeler [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CaseCohortCoxSurvival",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CaseCohortCoxSurvival Case-Cohort Cox Survival Inference Cox model inference for relative hazard and covariate-specific pure risk estimated \n             from stratified and unstratified case-cohort data as described in \n             Etievant, L., Gail, M.H. (Lifetime Data Analysis, 2024) <doi:10.1007/s10985-024-09621-2>.  "
  },
  {
    "id": 2409,
    "package_name": "CatDyn",
    "title": "Fishery Stock Assessment by Catch Dynamics Models",
    "description": "Based on fishery Catch Dynamics instead of fish Population Dynamics (hence CatDyn) and using high-frequency or medium-frequency catch in biomass or numbers, fishing nominal effort, and mean fish body weight by time step, from one or two fishing fleets, estimate stock abundance, natural mortality rate, and fishing operational parameters. It includes methods for data organization, plotting standard exploratory and analytical plots, predictions, for 100 types of models of increasing complexity, and 72 likelihood models for the data.",
    "version": "1.1-1",
    "maintainer": "Ruben H. Roa-Ureta <ruben.roa.ureta@mail.com>",
    "author": "Ruben H. Roa-Ureta",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CatDyn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CatDyn Fishery Stock Assessment by Catch Dynamics Models Based on fishery Catch Dynamics instead of fish Population Dynamics (hence CatDyn) and using high-frequency or medium-frequency catch in biomass or numbers, fishing nominal effort, and mean fish body weight by time step, from one or two fishing fleets, estimate stock abundance, natural mortality rate, and fishing operational parameters. It includes methods for data organization, plotting standard exploratory and analytical plots, predictions, for 100 types of models of increasing complexity, and 72 likelihood models for the data.  "
  },
  {
    "id": 2456,
    "package_name": "ChileDataAPI",
    "title": "Access Chilean Data via APIs and Curated Datasets",
    "description": "Provides functions to access data from public RESTful APIs including\n    'FINDIC API', 'REST Countries API', 'World Bank API', and 'Nager.Date', \n    retrieving real-time or historical data related to Chile such as financial indicators, \n    holidays, international demographic and geopolitical indicators, and more. \n    Additionally, the package includes curated datasets related to Chile, covering topics \n    such as human rights violations during the Pinochet regime, electoral data, census samples, \n    health surveys, seismic events, territorial codes, and environmental measurements. \n    The package supports research and analysis focused on Chile by integrating open APIs with \n    high-quality datasets from multiple domains. For more information on the APIs, see: \n    'FINDIC' <https://findic.cl/>, \n    'REST Countries' <https://restcountries.com/>, \n    'World Bank API' <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    and 'Nager.Date' <https://date.nager.at/Api>.",
    "version": "0.2.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/chiledataapi,\nhttps://lightbluetitan.github.io/chiledataapi/",
    "bug_reports": "https://github.com/lightbluetitan/chiledataapi/issues",
    "repository": "https://cran.r-project.org/package=ChileDataAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ChileDataAPI Access Chilean Data via APIs and Curated Datasets Provides functions to access data from public RESTful APIs including\n    'FINDIC API', 'REST Countries API', 'World Bank API', and 'Nager.Date', \n    retrieving real-time or historical data related to Chile such as financial indicators, \n    holidays, international demographic and geopolitical indicators, and more. \n    Additionally, the package includes curated datasets related to Chile, covering topics \n    such as human rights violations during the Pinochet regime, electoral data, census samples, \n    health surveys, seismic events, territorial codes, and environmental measurements. \n    The package supports research and analysis focused on Chile by integrating open APIs with \n    high-quality datasets from multiple domains. For more information on the APIs, see: \n    'FINDIC' <https://findic.cl/>, \n    'REST Countries' <https://restcountries.com/>, \n    'World Bank API' <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    and 'Nager.Date' <https://date.nager.at/Api>.  "
  },
  {
    "id": 2497,
    "package_name": "ClinicalUtilityRecal",
    "title": "Recalibration Methods for Improved Clinical Utility of Risk\nScores",
    "description": "Recalibrate risk scores (predicting binary outcomes) to improve clinical utility of risk score using weighted logistic or constrained logistic recalibration methods. Additionally, produces plots to assess the potential for recalibration to improve the clinical utility of a risk model. Methods are described in detail in Mishra, A. (2019) \"Methods for Risk Markers that Incorporate Clinical Utility\" <http://hdl.handle.net/1773/44068>.",
    "version": "0.1.0",
    "maintainer": "Anu Mishra <anmishra@uw.edu>",
    "author": "Anu Mishra",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ClinicalUtilityRecal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClinicalUtilityRecal Recalibration Methods for Improved Clinical Utility of Risk\nScores Recalibrate risk scores (predicting binary outcomes) to improve clinical utility of risk score using weighted logistic or constrained logistic recalibration methods. Additionally, produces plots to assess the potential for recalibration to improve the clinical utility of a risk model. Methods are described in detail in Mishra, A. (2019) \"Methods for Risk Markers that Incorporate Clinical Utility\" <http://hdl.handle.net/1773/44068>.  "
  },
  {
    "id": 2532,
    "package_name": "CoRpower",
    "title": "Power Calculations for Assessing Correlates of Risk in Clinical\nEfficacy Trials",
    "description": "Calculates power for assessment of intermediate biomarker responses as correlates of risk in the active treatment group in clinical efficacy trials, as described in Gilbert, Janes, and Huang, Power/Sample Size Calculations for Assessing Correlates of Risk in Clinical Efficacy Trials (2016, Statistics in Medicine). The methods differ from past approaches by accounting for the level of clinical treatment efficacy overall and in biomarker response subgroups, which enables the correlates of risk results to be interpreted in terms of potential correlates of efficacy/protection. The methods also account for inter-individual variability of the observed biomarker response that is not biologically relevant (e.g., due to technical measurement error of the laboratory assay used to measure the biomarker response), which is important because power to detect a specified correlate of risk effect size is heavily affected by the biomarker's measurement error. The methods can be used for a general binary clinical endpoint model with a univariate dichotomous, trichotomous, or continuous biomarker response measured in active treatment recipients at a fixed timepoint after randomization, with either case-cohort Bernoulli sampling or case-control without-replacement sampling of the biomarker (a baseline biomarker is handled as a trivial special case). In a specified two-group trial design, the computeN() function can initially be used for calculating additional requisite design parameters pertaining to the target population of active treatment recipients observed to be at risk at the biomarker sampling timepoint. Subsequently, the power calculation employs an inverse probability weighted logistic regression model fitted by the tps() function in the 'osDesign' package. Power results as well as the relationship between the correlate of risk effect size and treatment efficacy can be visualized using various plotting functions. To link power calculations for detecting a correlate of risk and a correlate of treatment efficacy, a baseline immunogenicity predictor (BIP) can be simulated according to a specified classification rule (for dichotomous or trichotomous BIPs) or correlation with the biomarker response (for continuous BIPs), then outputted along with biomarker response data under assignment to treatment, and clinical endpoint data for both treatment and placebo groups.",
    "version": "1.0.4",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "author": "Stephanie Wu [aut],\n  Michal Juraska [aut, cre],\n  Peter Gilbert [aut],\n  Yunda Huang [aut]",
    "url": "https://github.com/mjuraska/CoRpower",
    "bug_reports": "https://github.com/mjuraska/CoRpower/issues",
    "repository": "https://cran.r-project.org/package=CoRpower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CoRpower Power Calculations for Assessing Correlates of Risk in Clinical\nEfficacy Trials Calculates power for assessment of intermediate biomarker responses as correlates of risk in the active treatment group in clinical efficacy trials, as described in Gilbert, Janes, and Huang, Power/Sample Size Calculations for Assessing Correlates of Risk in Clinical Efficacy Trials (2016, Statistics in Medicine). The methods differ from past approaches by accounting for the level of clinical treatment efficacy overall and in biomarker response subgroups, which enables the correlates of risk results to be interpreted in terms of potential correlates of efficacy/protection. The methods also account for inter-individual variability of the observed biomarker response that is not biologically relevant (e.g., due to technical measurement error of the laboratory assay used to measure the biomarker response), which is important because power to detect a specified correlate of risk effect size is heavily affected by the biomarker's measurement error. The methods can be used for a general binary clinical endpoint model with a univariate dichotomous, trichotomous, or continuous biomarker response measured in active treatment recipients at a fixed timepoint after randomization, with either case-cohort Bernoulli sampling or case-control without-replacement sampling of the biomarker (a baseline biomarker is handled as a trivial special case). In a specified two-group trial design, the computeN() function can initially be used for calculating additional requisite design parameters pertaining to the target population of active treatment recipients observed to be at risk at the biomarker sampling timepoint. Subsequently, the power calculation employs an inverse probability weighted logistic regression model fitted by the tps() function in the 'osDesign' package. Power results as well as the relationship between the correlate of risk effect size and treatment efficacy can be visualized using various plotting functions. To link power calculations for detecting a correlate of risk and a correlate of treatment efficacy, a baseline immunogenicity predictor (BIP) can be simulated according to a specified classification rule (for dichotomous or trichotomous BIPs) or correlation with the biomarker response (for continuous BIPs), then outputted along with biomarker response data under assignment to treatment, and clinical endpoint data for both treatment and placebo groups.  "
  },
  {
    "id": 2551,
    "package_name": "ComRiskModel",
    "title": "Fitting of Complementary Risk Models",
    "description": "Evaluates the probability density function (PDF), cumulative distribution function (CDF), quantile function (QF), random numbers and maximum likelihood estimates (MLEs) of well-known complementary binomial-G, complementary negative binomial-G and complementary geometric-G families of distributions taking baseline models such as exponential, extended exponential,  Weibull,  extended Weibull, Fisk, Lomax, Burr-XII and Burr-X. The functions also allow computing the goodness-of-fit measures namely the Akaike-information-criterion (AIC), the Bayesian-information-criterion (BIC), the minimum value of the negative log-likelihood (-2L) function, Anderson-Darling (A) test, Cramer-Von-Mises (W) test, Kolmogorov-Smirnov test, P-value and convergence status. Moreover, some commonly used data sets from the fields of actuarial, reliability, and medical science are also provided. Related works include: \n  a) Tahir, M. H., & Cordeiro, G. M. (2016). Compounding of distributions: a survey and new generalized classes. Journal of Statistical Distributions and Applications, 3, 1-35. \n  <doi:10.1186/s40488-016-0052-1>.  ",
    "version": "0.2.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  M.H Tahir [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ComRiskModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ComRiskModel Fitting of Complementary Risk Models Evaluates the probability density function (PDF), cumulative distribution function (CDF), quantile function (QF), random numbers and maximum likelihood estimates (MLEs) of well-known complementary binomial-G, complementary negative binomial-G and complementary geometric-G families of distributions taking baseline models such as exponential, extended exponential,  Weibull,  extended Weibull, Fisk, Lomax, Burr-XII and Burr-X. The functions also allow computing the goodness-of-fit measures namely the Akaike-information-criterion (AIC), the Bayesian-information-criterion (BIC), the minimum value of the negative log-likelihood (-2L) function, Anderson-Darling (A) test, Cramer-Von-Mises (W) test, Kolmogorov-Smirnov test, P-value and convergence status. Moreover, some commonly used data sets from the fields of actuarial, reliability, and medical science are also provided. Related works include: \n  a) Tahir, M. H., & Cordeiro, G. M. (2016). Compounding of distributions: a survey and new generalized classes. Journal of Statistical Distributions and Applications, 3, 1-35. \n  <doi:10.1186/s40488-016-0052-1>.    "
  },
  {
    "id": 2553,
    "package_name": "CombinePortfolio",
    "title": "Estimation of Optimal Portfolio Weights by Combining Simple\nPortfolio Strategies",
    "description": "Estimation of optimal portfolio weights as combination of simple portfolio strategies, like the tangency, global minimum variance (GMV) or naive (1/N) portfolio. It is based on a utility maximizing 8-fund rule. Popular special cases like the Kan-Zhou(2007) 2-fund and 3-fund rule or the Tu-Zhou(2011) estimator are nested.",
    "version": "0.4",
    "maintainer": "Florian Ziel <florian.ziel@uni-due.de>",
    "author": "Florian Ziel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CombinePortfolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CombinePortfolio Estimation of Optimal Portfolio Weights by Combining Simple\nPortfolio Strategies Estimation of optimal portfolio weights as combination of simple portfolio strategies, like the tangency, global minimum variance (GMV) or naive (1/N) portfolio. It is based on a utility maximizing 8-fund rule. Popular special cases like the Kan-Zhou(2007) 2-fund and 3-fund rule or the Tu-Zhou(2011) estimator are nested.  "
  },
  {
    "id": 2628,
    "package_name": "CopulaCenR",
    "title": "Copula-Based Regression Models for Multivariate Censored Data",
    "description": "Copula-based regression models for multivariate censored data, including \n bivariate right-censored data, bivariate interval-censored data, and right/interval-censored \n semi-competing risks data. Currently supports Clayton, Gumbel, Frank, Joe, AMH and \n Copula2 copula models. For marginal models, it supports parametric (Weibull, Loglogistic, \n Gompertz) and semiparametric (Cox and transformation) models. Includes methods for \n convenient prediction and plotting. Also provides a bivariate time-to-event simulation \n function and an information ratio-based goodness-of-fit test for copula. Method details \n can be found in Sun et.al (2019) Lifetime Data Analysis, Sun et.al (2021) Biostatistics, \n Sun et.al (2022) Statistical Methods in Medical Research, Sun et.al (2022) Biometrics, and\n Sun et al. (2023+) JRSSC.",
    "version": "1.2.4",
    "maintainer": "Tao Sun <sun.tao@ruc.edu.cn>",
    "author": "Tao Sun [aut, cre] (ORCID: <https://orcid.org/0000-0003-4447-3005>),\n  Ying Ding [aut] (ORCID: <https://orcid.org/0000-0003-1352-1000>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CopulaCenR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CopulaCenR Copula-Based Regression Models for Multivariate Censored Data Copula-based regression models for multivariate censored data, including \n bivariate right-censored data, bivariate interval-censored data, and right/interval-censored \n semi-competing risks data. Currently supports Clayton, Gumbel, Frank, Joe, AMH and \n Copula2 copula models. For marginal models, it supports parametric (Weibull, Loglogistic, \n Gompertz) and semiparametric (Cox and transformation) models. Includes methods for \n convenient prediction and plotting. Also provides a bivariate time-to-event simulation \n function and an information ratio-based goodness-of-fit test for copula. Method details \n can be found in Sun et.al (2019) Lifetime Data Analysis, Sun et.al (2021) Biostatistics, \n Sun et.al (2022) Statistical Methods in Medical Research, Sun et.al (2022) Biometrics, and\n Sun et al. (2023+) JRSSC.  "
  },
  {
    "id": 2662,
    "package_name": "CreditRisk",
    "title": "Evaluation of Credit Risk with Structural and Reduced Form\nModels",
    "description": "Evaluation of default probability of sovereign and corporate entities based on structural or intensity based models and calibration on market Credit Default Swap quotes. References: Damiano Brigo, Massimo Morini, Andrea Pallavicini (2013) <doi:10.1002/9781118818589>. Print ISBN: 9780470748466, Online ISBN: 9781118818589. \u00a9 2013 John Wiley & Sons Ltd.",
    "version": "0.1.7",
    "maintainer": "Alessandro Cimarelli <alessandro.cimarelli@icloud.com>",
    "author": "Alessandro Cimarelli [aut, cre],\n  Nicol\u00f2 Manca [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CreditRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CreditRisk Evaluation of Credit Risk with Structural and Reduced Form\nModels Evaluation of default probability of sovereign and corporate entities based on structural or intensity based models and calibration on market Credit Default Swap quotes. References: Damiano Brigo, Massimo Morini, Andrea Pallavicini (2013) <doi:10.1002/9781118818589>. Print ISBN: 9780470748466, Online ISBN: 9781118818589. \u00a9 2013 John Wiley & Sons Ltd.  "
  },
  {
    "id": 2679,
    "package_name": "CustomDerivative",
    "title": "Pricing Various Types of Custom Derivatives",
    "description": "A versatile R package for creating and pricing custom derivatives to suit your financial needs.",
    "version": "0.1.1",
    "maintainer": "Amit Kumar Jha <jha.8@iitj.ac.in>",
    "author": "Amit Kumar Jha [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CustomDerivative",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CustomDerivative Pricing Various Types of Custom Derivatives A versatile R package for creating and pricing custom derivatives to suit your financial needs.  "
  },
  {
    "id": 2683,
    "package_name": "CvmortalityMult",
    "title": "Cross-Validation for Multi-Population Mortality Models",
    "description": "Implementation of cross-validation method for testing the forecasting accuracy of several multi-population mortality models. The family of multi-population includes several multi-population mortality models proposed through the actuarial and demography literature. The package includes functions for fitting and forecast the mortality rates of several populations. Additionally, we include functions for testing the forecasting accuracy of different multi-population models.\n  References, <https://journal.r-project.org/articles/RJ-2025-018/>.\n  Atance, D., Debon, A., and Navarro, E. (2020) <doi:10.3390/math8091550>.\n  Bergmeir, C. & Benitez, J.M. (2012) <doi:10.1016/j.ins.2011.12.028>.\n  Debon, A., Montes, F., & Martinez-Ruiz, F. (2011) <doi:10.1007/s13385-011-0043-z>.\n  Lee, R.D. & Carter, L.R. (1992) <doi:10.1080/01621459.1992.10475265>.\n  Russolillo, M., Giordano, G., & Haberman, S. (2011) <doi:10.1080/03461231003611933>.\n  Santolino, M. (2023) <doi:10.3390/risks11100170>.",
    "version": "1.1.1",
    "maintainer": "David Atance <david.atance@uah.es>",
    "author": "David Atance [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5860-0584>),\n  Ana Deb\u00f3n [aut] (ORCID: <https://orcid.org/0000-0002-5116-289X>)",
    "url": "https://github.com/davidAtance/CvmortalityMult",
    "bug_reports": "https://github.com/davidAtance/CvmortalityMult/issues",
    "repository": "https://cran.r-project.org/package=CvmortalityMult",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CvmortalityMult Cross-Validation for Multi-Population Mortality Models Implementation of cross-validation method for testing the forecasting accuracy of several multi-population mortality models. The family of multi-population includes several multi-population mortality models proposed through the actuarial and demography literature. The package includes functions for fitting and forecast the mortality rates of several populations. Additionally, we include functions for testing the forecasting accuracy of different multi-population models.\n  References, <https://journal.r-project.org/articles/RJ-2025-018/>.\n  Atance, D., Debon, A., and Navarro, E. (2020) <doi:10.3390/math8091550>.\n  Bergmeir, C. & Benitez, J.M. (2012) <doi:10.1016/j.ins.2011.12.028>.\n  Debon, A., Montes, F., & Martinez-Ruiz, F. (2011) <doi:10.1007/s13385-011-0043-z>.\n  Lee, R.D. & Carter, L.R. (1992) <doi:10.1080/01621459.1992.10475265>.\n  Russolillo, M., Giordano, G., & Haberman, S. (2011) <doi:10.1080/03461231003611933>.\n  Santolino, M. (2023) <doi:10.3390/risks11100170>.  "
  },
  {
    "id": 2803,
    "package_name": "DJL",
    "title": "Distance Measure Based Judgment and Learning",
    "description": "Implements various decision support tools related to the Econometrics & Technometrics.\n             Subroutines include correlation reliability test, Mahalanobis distance measure for outlier detection, combinatorial search (all possible subset regression), non-parametric efficiency analysis measures: DDF (directional distance function), DEA (data envelopment analysis), HDF (hyperbolic distance function), SBM (slack-based measure), and SF (shortage function), benchmarking, Malmquist productivity analysis, risk analysis, technology adoption model, new product target setting, network DEA, dynamic DEA, intertemporal budgeting, etc.",
    "version": "3.9",
    "maintainer": "Dong-Joon Lim <tgno3.com@gmail.com>",
    "author": "Dong-Joon Lim, Ph.D. <technometrics.org>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DJL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DJL Distance Measure Based Judgment and Learning Implements various decision support tools related to the Econometrics & Technometrics.\n             Subroutines include correlation reliability test, Mahalanobis distance measure for outlier detection, combinatorial search (all possible subset regression), non-parametric efficiency analysis measures: DDF (directional distance function), DEA (data envelopment analysis), HDF (hyperbolic distance function), SBM (slack-based measure), and SF (shortage function), benchmarking, Malmquist productivity analysis, risk analysis, technology adoption model, new product target setting, network DEA, dynamic DEA, intertemporal budgeting, etc.  "
  },
  {
    "id": 2827,
    "package_name": "DOSPortfolio",
    "title": "Dynamic Optimal Shrinkage Portfolio",
    "description": "\n  Constructs dynamic optimal shrinkage estimators for the weights of the global \n  minimum variance portfolio which are reconstructed at given reallocation \n  points as derived in Bodnar, Parolya, and Thors\u00e9n (2021) (<arXiv:2106.02131>).\n  Two dynamic shrinkage estimators are available in this package. One using \n  overlapping samples while the other use nonoverlapping samples.",
    "version": "0.1.0",
    "maintainer": "Erik Thors\u00e9n <erik.thorsen@math.su.se>",
    "author": "Taras Bodnar [aut] (ORCID: <https://orcid.org/0000-0001-7855-8221>),\n  Nestor Parolya [aut] (ORCID: <https://orcid.org/0000-0003-2147-2288>),\n  Erik Thors\u00e9n [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5992-1216>)",
    "url": "https://github.com/Statistics-In-Portfolio-Theory/DOSportfolio",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DOSPortfolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DOSPortfolio Dynamic Optimal Shrinkage Portfolio \n  Constructs dynamic optimal shrinkage estimators for the weights of the global \n  minimum variance portfolio which are reconstructed at given reallocation \n  points as derived in Bodnar, Parolya, and Thors\u00e9n (2021) (<arXiv:2106.02131>).\n  Two dynamic shrinkage estimators are available in this package. One using \n  overlapping samples while the other use nonoverlapping samples.  "
  },
  {
    "id": 2843,
    "package_name": "DRHotNet",
    "title": "Differential Risk Hotspots in a Linear Network",
    "description": "Performs the identification of differential risk hotspots (Briz-Redon et al. 2019) <doi:10.1016/j.aap.2019.105278> along a linear network. Given a marked point pattern lying on the linear network, the method implemented uses a network-constrained version of kernel density estimation (McSwiggan et al. 2017) <doi:10.1111/sjos.12255> to approximate the probability of occurrence across space for the type of event specified by the user through the marks of the pattern (Kelsall and Diggle 1995) <doi:10.2307/3318678>. The goal is to detect microzones of the linear network where the type of event indicated by the user is overrepresented.",
    "version": "2.3",
    "maintainer": "Alvaro Briz-Redon <alvaro.briz@uv.es>",
    "author": "Alvaro Briz-Redon",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DRHotNet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DRHotNet Differential Risk Hotspots in a Linear Network Performs the identification of differential risk hotspots (Briz-Redon et al. 2019) <doi:10.1016/j.aap.2019.105278> along a linear network. Given a marked point pattern lying on the linear network, the method implemented uses a network-constrained version of kernel density estimation (McSwiggan et al. 2017) <doi:10.1111/sjos.12255> to approximate the probability of occurrence across space for the type of event specified by the user through the marks of the pattern (Kelsall and Diggle 1995) <doi:10.2307/3318678>. The goal is to detect microzones of the linear network where the type of event indicated by the user is overrepresented.  "
  },
  {
    "id": 2872,
    "package_name": "DTDA.cif",
    "title": "Doubly Truncated Data Analysis, Cumulative Incidence Functions",
    "description": "Nonparametric estimator of the cumulative incidences of competing risks under double truncation. The estimator generalizes the Efron-Petrosian NPMLE (Non-Parametric Maximun Likelihood Estimator) to the competing risks setting. Efron, B. and Petrosian, V. (1999) <doi:10.2307/2669997>. ",
    "version": "1.0.2",
    "maintainer": "Jos\u00e9 Carlos Soage Gonz\u00e1lez <jsoage@uvigo.es>",
    "author": "Jacobo de U\u00f1a \u00c1lvarez [aut],\n  Jos\u00e9 Carlos Soage Gonz\u00e1lez [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DTDA.cif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DTDA.cif Doubly Truncated Data Analysis, Cumulative Incidence Functions Nonparametric estimator of the cumulative incidences of competing risks under double truncation. The estimator generalizes the Efron-Petrosian NPMLE (Non-Parametric Maximun Likelihood Estimator) to the competing risks setting. Efron, B. and Petrosian, V. (1999) <doi:10.2307/2669997>.   "
  },
  {
    "id": 2887,
    "package_name": "DUToolkit",
    "title": "Visualizing and Quantifying Decision Uncertainty",
    "description": "A suite of tools to help modelers and decision-makers effectively\n interpret and communicate decision risk when evaluating multiple policy options.\n It uses model outputs from uncertainty analysis for baseline scenarios and policy\n alternatives to generate visual representations of uncertainty and quantitative\n measures for assessing associated risks. For more details see \n Wiggins and colleagues (2025) <doi:10.1371/journal.pone.0332522> and <https://dut.ihe.ca/>.",
    "version": "1.0.2",
    "maintainer": "Megan Wiggins <mwiggins@ihe.ca>",
    "author": "Megan Wiggins [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-0290-4950>),\n  Marie Betsy Varughese [aut] (ORCID:\n    <https://orcid.org/0000-0002-8432-6539>),\n  Ellen Rafferty [aut] (ORCID: <https://orcid.org/0000-0001-9974-8016>),\n  Sasha van Katwyk [aut] (ORCID: <https://orcid.org/0000-0003-3026-2063>),\n  Christopher McCabe [aut] (ORCID:\n    <https://orcid.org/0000-0001-5728-4129>),\n  Jeff Round [aut] (ORCID: <https://orcid.org/0000-0002-3103-6984>),\n  Erin Kirwin [aut] (ORCID: <https://orcid.org/0000-0001-6957-7236>),\n  Institute of Health Economics [cph, aut],\n  Canadian Network for Modelling Infectious Diseases [fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DUToolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DUToolkit Visualizing and Quantifying Decision Uncertainty A suite of tools to help modelers and decision-makers effectively\n interpret and communicate decision risk when evaluating multiple policy options.\n It uses model outputs from uncertainty analysis for baseline scenarios and policy\n alternatives to generate visual representations of uncertainty and quantitative\n measures for assessing associated risks. For more details see \n Wiggins and colleagues (2025) <doi:10.1371/journal.pone.0332522> and <https://dut.ihe.ca/>.  "
  },
  {
    "id": 2891,
    "package_name": "DWaveNARDL",
    "title": "Dual Wavelet Based NARDL Model",
    "description": "Dual Wavelet based Nonlinear Autoregressive Distributed Lag model has been developed for noisy time series analysis. This package is designed to capture both short-run and long-run relationships in time series data, while incorporating wavelet transformations. The methodology combines the NARDL model with wavelet decomposition to better capture the nonlinear dynamics of the series and exogenous variables. The package is useful for analyzing economic and financial time series data that exhibit both long-term trends and short-term fluctuations. This package has been developed using algorithm of Jammazi et al. <doi:10.1016/j.intfin.2014.11.011>.",
    "version": "0.1.0",
    "maintainer": "Md Yeasin <yeasin.iasri@gmail.com>",
    "author": "Md Yeasin [aut, cre],\n  Ranjit Kumar Paul [aut],\n  Ranjit Kumar Upadhyay [aut],\n  Anita Sarkar [aut],\n  Amrit Kumar Paul [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DWaveNARDL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DWaveNARDL Dual Wavelet Based NARDL Model Dual Wavelet based Nonlinear Autoregressive Distributed Lag model has been developed for noisy time series analysis. This package is designed to capture both short-run and long-run relationships in time series data, while incorporating wavelet transformations. The methodology combines the NARDL model with wavelet decomposition to better capture the nonlinear dynamics of the series and exogenous variables. The package is useful for analyzing economic and financial time series data that exhibit both long-term trends and short-term fluctuations. This package has been developed using algorithm of Jammazi et al. <doi:10.1016/j.intfin.2014.11.011>.  "
  },
  {
    "id": 2910,
    "package_name": "DataSetsUni",
    "title": "A Collection of Univariate Data Sets",
    "description": "A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.     ",
    "version": "0.1",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  M.H Tahir [ctb],\n  Farrukh Jamal [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DataSetsUni",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataSetsUni A Collection of Univariate Data Sets A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.       "
  },
  {
    "id": 2911,
    "package_name": "DataSetsVerse",
    "title": "A Metapackage for Thematic and Domain-Specific Datasets",
    "description": "A metapackage that brings together a curated collection \n    of R packages containing domain-specific datasets. It includes time series data, \n    educational metrics, crime records, medical datasets, and oncology research data. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to structured and well-documented datasets, this metapackage \n    facilitates reproducible research, data exploration, and teaching applications across \n    a wide range of domains.\n    Included packages:\n    - 'timeSeriesDataSets': Time series data from economics, finance, energy, and healthcare.\n    - 'educationR': Datasets related to education, learning outcomes, and school metrics.\n    - 'crimedatasets': Datasets on global and local crime and criminal behavior.\n    - 'MedDataSets': Datasets related to medicine, public health, treatments, and clinical trials.\n    - 'OncoDataSets': Datasets focused on cancer research, survival, genetics, and biomarkers.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre]",
    "url": "https://github.com/lightbluetitan/datasetsverse,\nhttps://lightbluetitan.github.io/datasetsverse/",
    "bug_reports": "https://github.com/lightbluetitan/datasetsverse/issues",
    "repository": "https://cran.r-project.org/package=DataSetsVerse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataSetsVerse A Metapackage for Thematic and Domain-Specific Datasets A metapackage that brings together a curated collection \n    of R packages containing domain-specific datasets. It includes time series data, \n    educational metrics, crime records, medical datasets, and oncology research data. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to structured and well-documented datasets, this metapackage \n    facilitates reproducible research, data exploration, and teaching applications across \n    a wide range of domains.\n    Included packages:\n    - 'timeSeriesDataSets': Time series data from economics, finance, energy, and healthcare.\n    - 'educationR': Datasets related to education, learning outcomes, and school metrics.\n    - 'crimedatasets': Datasets on global and local crime and criminal behavior.\n    - 'MedDataSets': Datasets related to medicine, public health, treatments, and clinical trials.\n    - 'OncoDataSets': Datasets focused on cancer research, survival, genetics, and biomarkers.  "
  },
  {
    "id": 2920,
    "package_name": "DatastreamR",
    "title": "Datastream API",
    "description": "Access Datastream content through <https://product.datastream.com/dswsclient/Docs/Default.aspx>., our historical financial database with over 35 million individual instruments or indicators across all major asset classes, including over 19 million active economic indicators. It features 120 years of data, across 175 countries \u2013 the information you need to interpret market trends, economic cycles, and the impact of world events.    Data spans bond indices, bonds, commodities, convertibles, credit default swaps, derivatives, economics, energy, equities, equity indices, ESG, estimates, exchange rates, fixed income, funds, fundamentals, interest rates, and investment trusts. Unique content includes I/B/E/S Estimates, Worldscope Fundamentals, point-in-time data, and Reuters Polls.    Alongside the content, sit a set of powerful analytical tools for exploring relationships between different asset types, with a library of customizable analytical functions.    In-house timeseries can also be uploaded using the package to comingle with Datastream maintained datasets, use with these analytical tools and displayed in Datastream\u2019s flexible charting facilities in Microsoft Office. ",
    "version": "2.0.4",
    "maintainer": "LSEG (Datastream) <datastreamapi@lseg.com>",
    "author": "LSEG (Datastream) [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DatastreamR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatastreamR Datastream API Access Datastream content through <https://product.datastream.com/dswsclient/Docs/Default.aspx>., our historical financial database with over 35 million individual instruments or indicators across all major asset classes, including over 19 million active economic indicators. It features 120 years of data, across 175 countries \u2013 the information you need to interpret market trends, economic cycles, and the impact of world events.    Data spans bond indices, bonds, commodities, convertibles, credit default swaps, derivatives, economics, energy, equities, equity indices, ESG, estimates, exchange rates, fixed income, funds, fundamentals, interest rates, and investment trusts. Unique content includes I/B/E/S Estimates, Worldscope Fundamentals, point-in-time data, and Reuters Polls.    Alongside the content, sit a set of powerful analytical tools for exploring relationships between different asset types, with a library of customizable analytical functions.    In-house timeseries can also be uploaded using the package to comingle with Datastream maintained datasets, use with these analytical tools and displayed in Datastream\u2019s flexible charting facilities in Microsoft Office.   "
  },
  {
    "id": 2956,
    "package_name": "DetLifeInsurance",
    "title": "Life Insurance Premium and Reserves Valuation",
    "description": "Methods for valuation of life insurance premiums and reserves (including variable-benefit and fractional coverage) based on  \"Actuarial Mathematics\" by Bowers, H.U. Gerber, J.C. Hickman, D.A. Jones and C.J. Nesbitt (1997, ISBN: 978-0938959465), \"Actuarial Mathematics for Life Contingent Risks\" by Dickson, David C. M., Hardy, Mary R. and Waters, Howard R  (2009) <doi:10.1017/CBO9780511800146>  and \"Life Contingencies\" by Jordan, C. W (1952) <doi:10.1017/S002026810005410X>. It also contains functions for  equivalent interest and discount rate calculation, present and future values of annuities, and loan amortization schedule.",
    "version": "0.1.3",
    "maintainer": "Joaquin Auza <auzajoaquin@gmail.com>",
    "author": "Joaquin Auza [aut, cre],\n  Maria Sol Alvarez [aut]",
    "url": "https://github.com/JoaquinAuza/DetLifeInsurance",
    "bug_reports": "https://github.com/JoaquinAuza/DetLifeInsurance/issues",
    "repository": "https://cran.r-project.org/package=DetLifeInsurance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DetLifeInsurance Life Insurance Premium and Reserves Valuation Methods for valuation of life insurance premiums and reserves (including variable-benefit and fractional coverage) based on  \"Actuarial Mathematics\" by Bowers, H.U. Gerber, J.C. Hickman, D.A. Jones and C.J. Nesbitt (1997, ISBN: 978-0938959465), \"Actuarial Mathematics for Life Contingent Risks\" by Dickson, David C. M., Hardy, Mary R. and Waters, Howard R  (2009) <doi:10.1017/CBO9780511800146>  and \"Life Contingencies\" by Jordan, C. W (1952) <doi:10.1017/S002026810005410X>. It also contains functions for  equivalent interest and discount rate calculation, present and future values of annuities, and loan amortization schedule.  "
  },
  {
    "id": 3008,
    "package_name": "Distributacalcul",
    "title": "Probability Distribution Functions",
    "description": "Calculates expected values, variance, different moments (kth \n    moment, truncated mean), stop-loss, mean excess loss, Value-at-Risk (VaR)\n    and Tail Value-at-Risk (TVaR) as well as some density and cumulative \n    (survival) functions of continuous, discrete and compound distributions. \n    This package also includes a visual 'Shiny' component to enable students \n    to visualize distributions and understand the impact of their parameters.\n    This package is intended to expand the 'stats' package so as\n    to enable students to develop an intuition for probability.",
    "version": "0.4.0",
    "maintainer": "Alec James van Rassel <alec.van-rassel.1@ulaval.ca>",
    "author": "Alec James van Rassel [aut, cre, cph],\n  Gabriel Cr\u00e9peault-Cauchon [aut, ccp],\n  \u00c9tienne Marceau [tch, sad],\n  H\u00e9l\u00e8ne Cossette [tch, sad],\n  Laboratoire Act & Risk [fnd, sht],\n  \u00c9cole d'actuariat de l'Universit\u00e9 Laval [fnd, his, uvp],\n  Natural Sciences and Engineering Research Council of Canada [fnd],\n  Marc-Andr\u00e9 Devost [ccp]",
    "url": "https://alec42.github.io/Distributacalcul_Package/",
    "bug_reports": "https://github.com/alec42/Distributacalcul_Package/issues",
    "repository": "https://cran.r-project.org/package=Distributacalcul",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Distributacalcul Probability Distribution Functions Calculates expected values, variance, different moments (kth \n    moment, truncated mean), stop-loss, mean excess loss, Value-at-Risk (VaR)\n    and Tail Value-at-Risk (TVaR) as well as some density and cumulative \n    (survival) functions of continuous, discrete and compound distributions. \n    This package also includes a visual 'Shiny' component to enable students \n    to visualize distributions and understand the impact of their parameters.\n    This package is intended to expand the 'stats' package so as\n    to enable students to develop an intuition for probability.  "
  },
  {
    "id": 3015,
    "package_name": "DiversificationR",
    "title": "Econometric Tools to Measure Portfolio Diversification",
    "description": "Diversification is one of the most important concepts in portfolio management. This framework offers scholars, practitioners and policymakers a useful toolbox to measure diversification. Specifically, this framework provides recent diversification measures from the recent literature. These diversification measures are based on the works of Rudin and Morgan (2006) <doi:10.3905/jpm.2006.611807>, Choueifaty and Coignard (2008) <doi:10.3905/JPM.2008.35.1.40>, Vermorken et al. (2012) <doi:10.3905/jpm.2012.39.1.067>, Flores et al. (2017) <doi:10.3905/jpm.2017.43.4.112>, Calvet et al. (2007) <doi:10.1086/524204>, and Candelon, Fuerst and Hasse (2020).",
    "version": "0.1.0",
    "maintainer": "Jean-Baptiste Hasse <jb-hasse@hotmail.fr>",
    "author": "Jean-Baptiste Hasse [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiversificationR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiversificationR Econometric Tools to Measure Portfolio Diversification Diversification is one of the most important concepts in portfolio management. This framework offers scholars, practitioners and policymakers a useful toolbox to measure diversification. Specifically, this framework provides recent diversification measures from the recent literature. These diversification measures are based on the works of Rudin and Morgan (2006) <doi:10.3905/jpm.2006.611807>, Choueifaty and Coignard (2008) <doi:10.3905/JPM.2008.35.1.40>, Vermorken et al. (2012) <doi:10.3905/jpm.2012.39.1.067>, Flores et al. (2017) <doi:10.3905/jpm.2017.43.4.112>, Calvet et al. (2007) <doi:10.1086/524204>, and Candelon, Fuerst and Hasse (2020).  "
  },
  {
    "id": 3019,
    "package_name": "DoEstRare",
    "title": "Rare Variant Association Test Based on Position Density\nEstimation",
    "description": "Rare variant association test integrating variant position information. It aims to identify the presence of clusters of disease-risk variants in specific gene regions. For more details, please read the publication from Persyn et al. (2017)  <doi:10.1371/journal.pone.0179364>.",
    "version": "0.2",
    "maintainer": "Elodie Persyn <elodie.persyn@univ-nantes.fr>",
    "author": "Elodie Persyn",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DoEstRare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DoEstRare Rare Variant Association Test Based on Position Density\nEstimation Rare variant association test integrating variant position information. It aims to identify the presence of clusters of disease-risk variants in specific gene regions. For more details, please read the publication from Persyn et al. (2017)  <doi:10.1371/journal.pone.0179364>.  "
  },
  {
    "id": 3030,
    "package_name": "Dowd",
    "title": "Functions Ported from 'MMR2' Toolbox Offered in Kevin Dowd's\nBook Measuring Market Risk",
    "description": "'Kevin Dowd's' book Measuring Market Risk is a widely read book \n          in the area of risk measurement by students and \n          practitioners alike. As he claims, 'MATLAB' indeed might have been the most \n          suitable language when he originally wrote the functions, but,\n          with growing popularity of R it is not entirely \n\t  valid. As 'Dowd's' code was not intended to be error free and were mainly \n\t  for reference, some functions in this package have inherited those \n\t  errors. An attempt will be made in future releases to identify and correct \n\t  them. 'Dowd's' original code can be downloaded from www.kevindowd.org/measuring-market-risk/. \n          It should be noted that 'Dowd' offers both\n          'MMR2' and 'MMR1' toolboxes. Only 'MMR2' was ported to R. 'MMR2' is more \n          recent version of 'MMR1' toolbox and they both have mostly similar \n          function. The toolbox mainly contains different parametric and non \n\t  parametric methods for measurement of market risk as well as \n\t  backtesting risk measurement methods.",
    "version": "0.12",
    "maintainer": "Dinesh Acharya <dines.acharya@gmail.com>",
    "author": "Dinesh Acharya <dines.acharya@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Dowd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Dowd Functions Ported from 'MMR2' Toolbox Offered in Kevin Dowd's\nBook Measuring Market Risk 'Kevin Dowd's' book Measuring Market Risk is a widely read book \n          in the area of risk measurement by students and \n          practitioners alike. As he claims, 'MATLAB' indeed might have been the most \n          suitable language when he originally wrote the functions, but,\n          with growing popularity of R it is not entirely \n\t  valid. As 'Dowd's' code was not intended to be error free and were mainly \n\t  for reference, some functions in this package have inherited those \n\t  errors. An attempt will be made in future releases to identify and correct \n\t  them. 'Dowd's' original code can be downloaded from www.kevindowd.org/measuring-market-risk/. \n          It should be noted that 'Dowd' offers both\n          'MMR2' and 'MMR1' toolboxes. Only 'MMR2' was ported to R. 'MMR2' is more \n          recent version of 'MMR1' toolbox and they both have mostly similar \n          function. The toolbox mainly contains different parametric and non \n\t  parametric methods for measurement of market risk as well as \n\t  backtesting risk measurement methods.  "
  },
  {
    "id": 3065,
    "package_name": "EBPRS",
    "title": "Derive Polygenic Risk Score Based on Emprical Bayes Theory",
    "description": "EB-PRS is a novel method that leverages information for effect sizes across all the markers to improve the prediction accuracy.  No parameter tuning is needed in the method, and no external information is needed. This R-package provides the calculation of polygenic risk scores from the given training summary statistics and testing data. We can use EB-PRS to extract main information, estimate Empirical Bayes parameters, derive polygenic risk scores for  each individual in testing data, and evaluate the PRS according to AUC and predictive r2. See Song et al. (2020) <doi:10.1371/journal.pcbi.1007565> for a detailed presentation of the method.",
    "version": "2.1.0",
    "maintainer": "Shuang Song <song-s19@mails.tsinghua.edu.cn>",
    "author": "Shuang Song [aut, cre], Wei Jiang [aut], Lin Hou [aut] and Hongyu Zhao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EBPRS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EBPRS Derive Polygenic Risk Score Based on Emprical Bayes Theory EB-PRS is a novel method that leverages information for effect sizes across all the markers to improve the prediction accuracy.  No parameter tuning is needed in the method, and no external information is needed. This R-package provides the calculation of polygenic risk scores from the given training summary statistics and testing data. We can use EB-PRS to extract main information, estimate Empirical Bayes parameters, derive polygenic risk scores for  each individual in testing data, and evaluate the PRS according to AUC and predictive r2. See Song et al. (2020) <doi:10.1371/journal.pcbi.1007565> for a detailed presentation of the method.  "
  },
  {
    "id": 3133,
    "package_name": "EMP",
    "title": "Expected Maximum Profit Classification Performance Measure",
    "description": "Functions for estimating EMP (Expected Maximum Profit Measure) in Credit Risk Scoring and Customer Churn Prediction, according to Verbraken et al (2013, 2014) <DOI:10.1109/TKDE.2012.50>, <DOI:10.1016/j.ejor.2014.04.001>.",
    "version": "2.0.6",
    "maintainer": "Cristian Bravo <cbravoro@uwo.ca>",
    "author": "Cristian Bravo [aut, cre],\n  Seppe vanden Broucke [ctb],\n  Thomas Verbraken [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EMP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EMP Expected Maximum Profit Classification Performance Measure Functions for estimating EMP (Expected Maximum Profit Measure) in Credit Risk Scoring and Customer Churn Prediction, according to Verbraken et al (2013, 2014) <DOI:10.1109/TKDE.2012.50>, <DOI:10.1016/j.ejor.2014.04.001>.  "
  },
  {
    "id": 3147,
    "package_name": "EQRN",
    "title": "Extreme Quantile Regression Neural Networks for Risk Forecasting",
    "description": "This framework enables forecasting and extrapolating measures of conditional risk\n    (e.g. of extreme or unprecedented events), including quantiles and exceedance probabilities,\n    using extreme value statistics and flexible neural network architectures.\n    It allows for capturing complex multivariate dependencies,\n    including dependencies between observations, such as sequential dependence (time-series).\n    The methodology was introduced in Pasche and Engelke (2024) <doi:10.1214/24-AOAS1907>\n    (also available in preprint: Pasche and Engelke (2022) <doi:10.48550/arXiv.2208.07590>).",
    "version": "0.1.2",
    "maintainer": "Olivier C. Pasche <olivier_pasche@alumni.epfl.ch>",
    "author": "Olivier C. Pasche [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1202-9199>)",
    "url": "https://github.com/opasche/EQRN, https://opasche.github.io/EQRN/",
    "bug_reports": "https://github.com/opasche/EQRN/issues",
    "repository": "https://cran.r-project.org/package=EQRN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EQRN Extreme Quantile Regression Neural Networks for Risk Forecasting This framework enables forecasting and extrapolating measures of conditional risk\n    (e.g. of extreme or unprecedented events), including quantiles and exceedance probabilities,\n    using extreme value statistics and flexible neural network architectures.\n    It allows for capturing complex multivariate dependencies,\n    including dependencies between observations, such as sequential dependence (time-series).\n    The methodology was introduced in Pasche and Engelke (2024) <doi:10.1214/24-AOAS1907>\n    (also available in preprint: Pasche and Engelke (2022) <doi:10.48550/arXiv.2208.07590>).  "
  },
  {
    "id": 3160,
    "package_name": "ESG",
    "title": "A Package for Asset Projection",
    "description": "Presents a \"Scenarios\" class containing\n        general parameters, risk parameters and projection results.\n        Risk parameters are gathered together into a ParamsScenarios\n        sub-object. The general process for using this package is to\n        set all needed parameters in a Scenarios object, use the\n        customPathsGeneration method to proceed to the projection, then\n        use xxx_PriceDistribution() methods to get asset prices.",
    "version": "1.3",
    "maintainer": "Wassim Youssef <Wassim.G.Youssef@gmail.com>",
    "author": "Jean-Charles Croix, Thierry Moudiki, Fr\u00e9d\u00e9ric Planchet, Wassim\n        Youssef",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ESG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ESG A Package for Asset Projection Presents a \"Scenarios\" class containing\n        general parameters, risk parameters and projection results.\n        Risk parameters are gathered together into a ParamsScenarios\n        sub-object. The general process for using this package is to\n        set all needed parameters in a Scenarios object, use the\n        customPathsGeneration method to proceed to the projection, then\n        use xxx_PriceDistribution() methods to get asset prices.  "
  },
  {
    "id": 3175,
    "package_name": "EWS",
    "title": "Early Warning System",
    "description": "The purpose of Early Warning Systems (EWS) is to detect accurately the occurrence of a crisis, which is represented by a binary variable which takes the value of one when the event occurs, and the value of zero otherwise. EWS are a toolbox for policymakers to prevent or attenuate the impact of economic downturns. Modern EWS are based on the econometric framework of Kauppi and Saikkonen (2008) <doi:10.1162/rest.90.4.777>. Specifically, this framework includes four dichotomous models, relying on a logit approach to model the relationship between yield spreads and future recessions, controlling for recession risk factors. These models can be estimated in a univariate or a balanced panel framework as in Candelon, Dumitrescu and Hurlin (2014) <doi:10.1016/j.ijforecast.2014.03.015>. This package provides both methods for estimating these models and a dataset covering 13 OECD countries over a period of 45 years. In addition, this package also provides methods for the analysis of the propagation mechanisms of an exogenous shock, as well as robust confidence intervals for these response functions using a block-bootstrap method as in Lajaunie (2021). This package constitutes a useful toolbox (data and functions) for scholars as well as policymakers.",
    "version": "0.2.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Jean-Baptiste Hasse [aut], Quentin Lajaunie [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EWS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EWS Early Warning System The purpose of Early Warning Systems (EWS) is to detect accurately the occurrence of a crisis, which is represented by a binary variable which takes the value of one when the event occurs, and the value of zero otherwise. EWS are a toolbox for policymakers to prevent or attenuate the impact of economic downturns. Modern EWS are based on the econometric framework of Kauppi and Saikkonen (2008) <doi:10.1162/rest.90.4.777>. Specifically, this framework includes four dichotomous models, relying on a logit approach to model the relationship between yield spreads and future recessions, controlling for recession risk factors. These models can be estimated in a univariate or a balanced panel framework as in Candelon, Dumitrescu and Hurlin (2014) <doi:10.1016/j.ijforecast.2014.03.015>. This package provides both methods for estimating these models and a dataset covering 13 OECD countries over a period of 45 years. In addition, this package also provides methods for the analysis of the propagation mechanisms of an exogenous shock, as well as robust confidence intervals for these response functions using a block-bootstrap method as in Lajaunie (2021). This package constitutes a useful toolbox (data and functions) for scholars as well as policymakers.  "
  },
  {
    "id": 3235,
    "package_name": "EnviroPRA2",
    "title": "Environmental Probabilistic Risk Assessment Tools",
    "description": "It contains functions for dose calculation for different routes, fitting data to probability distributions, random number generation (Monte Carlo simulation) and calculation of systemic and carcinogenic risks. For more information see the publication: Barrio-Parra et al. (2019) \"Human-health probabilistic risk assessment: the role of exposure factors in an urban garden scenario\" <doi:10.1016/j.landurbplan.2019.02.005>. ",
    "version": "1.0.1",
    "maintainer": "Fernando Barrio-Parra <fernando.barrio@upm.es>",
    "author": "Fernando Barrio-Parra [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5475-3567>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EnviroPRA2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EnviroPRA2 Environmental Probabilistic Risk Assessment Tools It contains functions for dose calculation for different routes, fitting data to probability distributions, random number generation (Monte Carlo simulation) and calculation of systemic and carcinogenic risks. For more information see the publication: Barrio-Parra et al. (2019) \"Human-health probabilistic risk assessment: the role of exposure factors in an urban garden scenario\" <doi:10.1016/j.landurbplan.2019.02.005>.   "
  },
  {
    "id": 3246,
    "package_name": "EpiPvr",
    "title": "Estimating Plant Pathogen Epidemiology Parameters from\nLaboratory Assays",
    "description": "Provides functions for estimating plant pathogen parameters from\n    access period (AP) experiments. Separate functions are implemented for\n    semi-persistently transmitted (SPT) and persistently transmitted (PT)\n    pathogens. The common AP experiment exposes insect cohorts to infected source\n    plants, healthy test plants, and intermediate plants (for PT pathogens). The\n    package allows estimation of acquisition and inoculation rates during feeding,\n    recovery rates, and latent progression rates (for PT pathogens). Additional\n    functions support inference of epidemic risk from pathogen and local\n    parameters, and also simulate AP experiment data. The functions implement\n    probability models for epidemiological analysis, as derived in Donnelly et\n    al. (2025), <doi:10.32942/X29K9P>. These models were originally\n    implemented in the 'EpiPv' 'GitHub' package.",
    "version": "0.0.1",
    "maintainer": "Ruairi Donnelly <rd501@cam.ac.uk>",
    "author": "Ruairi Donnelly [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7642-0317>)",
    "url": "https://CRAN.R-project.org/package=EpiPvr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpiPvr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpiPvr Estimating Plant Pathogen Epidemiology Parameters from\nLaboratory Assays Provides functions for estimating plant pathogen parameters from\n    access period (AP) experiments. Separate functions are implemented for\n    semi-persistently transmitted (SPT) and persistently transmitted (PT)\n    pathogens. The common AP experiment exposes insect cohorts to infected source\n    plants, healthy test plants, and intermediate plants (for PT pathogens). The\n    package allows estimation of acquisition and inoculation rates during feeding,\n    recovery rates, and latent progression rates (for PT pathogens). Additional\n    functions support inference of epidemic risk from pathogen and local\n    parameters, and also simulate AP experiment data. The functions implement\n    probability models for epidemiological analysis, as derived in Donnelly et\n    al. (2025), <doi:10.32942/X29K9P>. These models were originally\n    implemented in the 'EpiPv' 'GitHub' package.  "
  },
  {
    "id": 3263,
    "package_name": "EstimateBreed",
    "title": "Estimation of Environmental Variables and Genetic Parameters",
    "description": "Performs analyzes and estimates of environmental covariates \n    and genetic parameters related to selection strategies and development \n    of superior genotypes. It has two main functionalities, the first being \n    about prediction models of covariates and environmental processes, while \n    the second deals with the estimation of genetic parameters and selection \n    strategies. Designed for researchers and professionals in genetics and \n    environmental sciences, the package combines statistical methods\n    for modeling and data analysis. This includes the plastochron estimate \n    proposed by Porta et al. (2024) <doi:10.1590/1807-1929/agriambi.v28n10e278299>,\n    Stress indices for genotype selection referenced by Ghazvini et al. (2024)\n    <doi:10.1007/s10343-024-00981-1>, the Environmental Stress Index described by\n    Tazzo et al. (2024) <https://revistas.ufg.br/vet/article/view/77035>,\n    industrial quality indices of wheat genotypes (Szareski et al., 2019), \n    <doi:10.4238/gmr18223>, Ear Indexes estimation (Rigotti et al., 2024),\n    <doi:10.13083/reveng.v32i1.17394>, Selection index for protein and grain yield\n    (de Pelegrin et al., 2017), <doi:10.4236/ajps.2017.813224>, Estimation of the\n    ISGR - Genetic Selection Index for Resilience for environmental resilience\n    (Bandeira et al., 2024) <https://www.cropj.com/Carvalho_18_12_2024_825_830.pdf>,\n    estimation of Leaf Area Index (Meira et al., 2015) \n    <https://www.fag.edu.br/upload/revista/cultivando_o_saber/55d1ef202e494.pdf>,\n    Restriction of control variability (Carvalho et al., 2023) \n    <doi:10.4025/actasciagron.v45i1.56156>, Risk of Disease Occurrence in Soybeans\n    described by Engers et al. (2024) <doi:10.1007/s40858-024-00649-1> and \n    estimation of genetic parameters for selection based on balanced experiments\n    (Yadav et al., 2024) <doi:10.1155/2024/9946332>.",
    "version": "1.0.2",
    "maintainer": "Willyan Junior Adorian Bandeira <bandeira.wjab@gmail.com>",
    "author": "Willyan Junior Adorian Bandeira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9430-3664>),\n  Ivan Ricardo Carvalho [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-7947-4900>),\n  Murilo Vieira Loro [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-0241-4226>),\n  Leonardo Cesar Pradebon [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-7827-6312>),\n  Jose Antonio Gonzalez da Silva [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-9335-2421>)",
    "url": "https://github.com/willyanjnr/EstimateBreed",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EstimateBreed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EstimateBreed Estimation of Environmental Variables and Genetic Parameters Performs analyzes and estimates of environmental covariates \n    and genetic parameters related to selection strategies and development \n    of superior genotypes. It has two main functionalities, the first being \n    about prediction models of covariates and environmental processes, while \n    the second deals with the estimation of genetic parameters and selection \n    strategies. Designed for researchers and professionals in genetics and \n    environmental sciences, the package combines statistical methods\n    for modeling and data analysis. This includes the plastochron estimate \n    proposed by Porta et al. (2024) <doi:10.1590/1807-1929/agriambi.v28n10e278299>,\n    Stress indices for genotype selection referenced by Ghazvini et al. (2024)\n    <doi:10.1007/s10343-024-00981-1>, the Environmental Stress Index described by\n    Tazzo et al. (2024) <https://revistas.ufg.br/vet/article/view/77035>,\n    industrial quality indices of wheat genotypes (Szareski et al., 2019), \n    <doi:10.4238/gmr18223>, Ear Indexes estimation (Rigotti et al., 2024),\n    <doi:10.13083/reveng.v32i1.17394>, Selection index for protein and grain yield\n    (de Pelegrin et al., 2017), <doi:10.4236/ajps.2017.813224>, Estimation of the\n    ISGR - Genetic Selection Index for Resilience for environmental resilience\n    (Bandeira et al., 2024) <https://www.cropj.com/Carvalho_18_12_2024_825_830.pdf>,\n    estimation of Leaf Area Index (Meira et al., 2015) \n    <https://www.fag.edu.br/upload/revista/cultivando_o_saber/55d1ef202e494.pdf>,\n    Restriction of control variability (Carvalho et al., 2023) \n    <doi:10.4025/actasciagron.v45i1.56156>, Risk of Disease Occurrence in Soybeans\n    described by Engers et al. (2024) <doi:10.1007/s40858-024-00649-1> and \n    estimation of genetic parameters for selection based on balanced experiments\n    (Yadav et al., 2024) <doi:10.1155/2024/9946332>.  "
  },
  {
    "id": 3278,
    "package_name": "EventWinRatios",
    "title": "Event-Specific Win Ratios for Terminal and Non-Terminal Events",
    "description": "Provides several confidence interval and testing procedures using\n    event-specific win ratios for semi-competing risks data with non-terminal\n    and terminal events, as developed in Yang et al. (2021<doi:10.1002/sim.9266>). \n    Compared with conventional methods for survival data, these procedures are \n    designed to utilize more data for improved inference procedures with \n    semi-competing risks data. The event-specific win ratios were introduced in \n    Yang and Troendle (2021<doi:10.1177/1740774520972408>). In this package, \n    the event-specific win ratios and confidence intervals are obtained for each \n    event type, and several testing procedures are developed for the global null \n    of no treatment effect on either terminal or non-terminal events. Furthermore,\n    a test of proportional hazard assumptions, under which the event-specific win \n    ratios converge to the hazard ratios, and a test of equal hazard ratios are \n    provided. For summarizing the treatment effect on all events, confidence \n    intervals for linear combinations of the event-specific win ratios are available\n    using pre-determined or data-driven weights. Asymptotic properties of these \n    inference procedures are discussed in Yang et al (2021<doi:10.1002/sim.9266>). \n    Also, transformations are used to yield better control of the type one error \n    rates for moderately sized data sets.",
    "version": "1.0.0",
    "maintainer": "Daewoo Pak <dpak@yonsei.ac.kr>",
    "author": "Daewoo Pak [aut, cre],\n  Song Yang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EventWinRatios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EventWinRatios Event-Specific Win Ratios for Terminal and Non-Terminal Events Provides several confidence interval and testing procedures using\n    event-specific win ratios for semi-competing risks data with non-terminal\n    and terminal events, as developed in Yang et al. (2021<doi:10.1002/sim.9266>). \n    Compared with conventional methods for survival data, these procedures are \n    designed to utilize more data for improved inference procedures with \n    semi-competing risks data. The event-specific win ratios were introduced in \n    Yang and Troendle (2021<doi:10.1177/1740774520972408>). In this package, \n    the event-specific win ratios and confidence intervals are obtained for each \n    event type, and several testing procedures are developed for the global null \n    of no treatment effect on either terminal or non-terminal events. Furthermore,\n    a test of proportional hazard assumptions, under which the event-specific win \n    ratios converge to the hazard ratios, and a test of equal hazard ratios are \n    provided. For summarizing the treatment effect on all events, confidence \n    intervals for linear combinations of the event-specific win ratios are available\n    using pre-determined or data-driven weights. Asymptotic properties of these \n    inference procedures are discussed in Yang et al (2021<doi:10.1002/sim.9266>). \n    Also, transformations are used to yield better control of the type one error \n    rates for moderately sized data sets.  "
  },
  {
    "id": 3295,
    "package_name": "ExactVaRTest",
    "title": "Exact Finite-Sample Value-at-Risk Back-Testing",
    "description": "Provides fast dynamic-programming algorithms in 'C++'/'Rcpp'\n    (with pure 'R' fallbacks) for the exact finite-sample distributions\n    and p-values of Christoffersen (1998) independence (IND) and\n    conditional-coverage (CC) VaR backtests. For completeness, it also\n    provides the exact unconditional-coverage (UC) test following\n    Kupiec (1995) via a closed-form binomial enumeration. See\n    Christoffersen (1998) <doi:10.2307/2527341> and Kupiec (1995)\n    <doi:10.3905/jod.1995.407942>.",
    "version": "0.1.3",
    "maintainer": "Yujian Chen <yjc4996@gmail.com>",
    "author": "Yujian Chen [aut, cre]",
    "url": "https://github.com/YujianCHEN219/ExactVaRTest",
    "bug_reports": "https://github.com/YujianCHEN219/ExactVaRTest/issues",
    "repository": "https://cran.r-project.org/package=ExactVaRTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExactVaRTest Exact Finite-Sample Value-at-Risk Back-Testing Provides fast dynamic-programming algorithms in 'C++'/'Rcpp'\n    (with pure 'R' fallbacks) for the exact finite-sample distributions\n    and p-values of Christoffersen (1998) independence (IND) and\n    conditional-coverage (CC) VaR backtests. For completeness, it also\n    provides the exact unconditional-coverage (UC) test following\n    Kupiec (1995) via a closed-form binomial enumeration. See\n    Christoffersen (1998) <doi:10.2307/2527341> and Kupiec (1995)\n    <doi:10.3905/jod.1995.407942>.  "
  },
  {
    "id": 3319,
    "package_name": "ExtremeRisks",
    "title": "Extreme Risk Measures",
    "description": "A set of procedures for estimating risks related to extreme events via risk measures such as Expectile, Value-at-Risk, etc. is provided. Estimation methods for univariate independent observations and temporal dependent observations are available. The methodology is extended to the case of independent multidimensional observations.  The statistical inference is performed through parametric and non-parametric estimators. Inferential procedures such as confidence intervals, confidence regions and hypothesis testing are obtained by exploiting the asymptotic theory. Adapts the methodologies derived in Padoan and Stupfler (2022) <doi:10.3150/21-BEJ1375>, Davison et al. (2023) <doi:10.1080/07350015.2022.2078332>, Daouia et al. (2018) <doi:10.1111/rssb.12254>, Drees (2000) <doi:10.1214/aoap/1019487617>, Drees (2003) <doi:10.3150/bj/1066223272>, de Haan and Ferreira (2006) <doi:10.1007/0-387-34471-3>, de Haan et al. (2016) <doi:10.1007/s00780-015-0287-6>, Padoan and Rizzelli (2024) <doi:10.3150/23-BEJ1668>, Daouia et al. (2024) <doi:10.3150/23-BEJ1632>.",
    "version": "0.0.5",
    "maintainer": "Simone Padoan <simone.padoan@unibocconi.it>",
    "author": "Simone Padoan [cre, aut],\n  Gilles Stupfler [aut],\n  Carlotta Pacifici [aut]",
    "url": "https://faculty.unibocconi.it/simonepadoan/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ExtremeRisks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExtremeRisks Extreme Risk Measures A set of procedures for estimating risks related to extreme events via risk measures such as Expectile, Value-at-Risk, etc. is provided. Estimation methods for univariate independent observations and temporal dependent observations are available. The methodology is extended to the case of independent multidimensional observations.  The statistical inference is performed through parametric and non-parametric estimators. Inferential procedures such as confidence intervals, confidence regions and hypothesis testing are obtained by exploiting the asymptotic theory. Adapts the methodologies derived in Padoan and Stupfler (2022) <doi:10.3150/21-BEJ1375>, Davison et al. (2023) <doi:10.1080/07350015.2022.2078332>, Daouia et al. (2018) <doi:10.1111/rssb.12254>, Drees (2000) <doi:10.1214/aoap/1019487617>, Drees (2003) <doi:10.3150/bj/1066223272>, de Haan and Ferreira (2006) <doi:10.1007/0-387-34471-3>, de Haan et al. (2016) <doi:10.1007/s00780-015-0287-6>, Padoan and Rizzelli (2024) <doi:10.3150/23-BEJ1668>, Daouia et al. (2024) <doi:10.3150/23-BEJ1632>.  "
  },
  {
    "id": 3331,
    "package_name": "FARS",
    "title": "Factor-Augmented Regression Scenarios",
    "description": "Provides a comprehensive framework in R for modeling and forecasting economic scenarios based on multi-level dynamic factor model. The package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the factor-augmented quantile regressions together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.",
    "version": "0.7.1",
    "maintainer": "Gian Pietro Bellocca <gbellocc@est-econ.uc3m.es>",
    "author": "Gian Pietro Bellocca [aut, cre],\n  Ignacio Garr\u00f3n [aut],\n  Vladimir Rodr\u00edguez-Caballero [aut],\n  Esther Ruiz [aut]",
    "url": "https://arxiv.org/abs/2507.10679",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FARS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FARS Factor-Augmented Regression Scenarios Provides a comprehensive framework in R for modeling and forecasting economic scenarios based on multi-level dynamic factor model. The package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the factor-augmented quantile regressions together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.  "
  },
  {
    "id": 3357,
    "package_name": "FER",
    "title": "Financial Engineering in R",
    "description": "R implementations of standard financial engineering codes;\n  vanilla option pricing models such as Black-Scholes, Bachelier, CEV, and\n  SABR.",
    "version": "0.94",
    "maintainer": "Jaehyuk Choi <pyfe@eml.cc>",
    "author": "Jaehyuk Choi [aut, cre]",
    "url": "https://github.com/PyFE/FE-R",
    "bug_reports": "https://github.com/PyFE/FE-R/issues",
    "repository": "https://cran.r-project.org/package=FER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FER Financial Engineering in R R implementations of standard financial engineering codes;\n  vanilla option pricing models such as Black-Scholes, Bachelier, CEV, and\n  SABR.  "
  },
  {
    "id": 3358,
    "package_name": "FESta",
    "title": "Fishing Effort Standardisation",
    "description": "Original idea was presented in the reference paper. Varghese et al. (2020, 74(1):35-42) \"Bayesian State-space Implementation of Schaefer Production Model for Assessment of Stock Status for Multi-gear Fishery\". Marine fisheries governance and management practices are very essential to ensure the sustainability of the marine resources. A widely accepted resource management strategy towards this is to derive sustainable fish harvest levels based on the status of marine fish stock. Various fish stock assessment models that describe the biomass dynamics using time series data on fish catch and fishing effort are generally used for this purpose. In the scenario of complex multi-species marine fishery in which different species are caught by a number of fishing gears and each gear harvests a number of species make it difficult to obtain the fishing effort corresponding to each fish species. Since the capacity of the gears varies, the effort made to catch a resource cannot be considered as the sum of efforts expended by different fishing gears. This necessitates standardisation of fishing effort in unit base.",
    "version": "1.0.0",
    "maintainer": "Eldho Varghese <eldhoiasri@gmail.com>",
    "author": "Eldho Varghese [aut, cre],\n  Sathianandan T V [aut],\n  Jayasankar J [aut],\n  Reshma Gills [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FESta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FESta Fishing Effort Standardisation Original idea was presented in the reference paper. Varghese et al. (2020, 74(1):35-42) \"Bayesian State-space Implementation of Schaefer Production Model for Assessment of Stock Status for Multi-gear Fishery\". Marine fisheries governance and management practices are very essential to ensure the sustainability of the marine resources. A widely accepted resource management strategy towards this is to derive sustainable fish harvest levels based on the status of marine fish stock. Various fish stock assessment models that describe the biomass dynamics using time series data on fish catch and fishing effort are generally used for this purpose. In the scenario of complex multi-species marine fishery in which different species are caught by a number of fishing gears and each gear harvests a number of species make it difficult to obtain the fishing effort corresponding to each fish species. Since the capacity of the gears varies, the effort made to catch a resource cannot be considered as the sum of efforts expended by different fishing gears. This necessitates standardisation of fishing effort in unit base.  "
  },
  {
    "id": 3370,
    "package_name": "FILEST",
    "title": "Fine-Level Structure Simulator",
    "description": "A population genetic simulator, which is able to generate synthetic datasets for single-nucleotide polymorphisms (SNP) for multiple populations. The genetic distances among populations can be set according to the Fixation Index (Fst) as explained in Balding and Nichols (1995) <doi:10.1007/BF01441146>. This tool is able to simulate outlying individuals and missing SNPs can be specified. For Genome-wide association study (GWAS), disease status can be set in desired level according risk ratio.",
    "version": "1.1.2",
    "maintainer": "Kridsadakorn Chaichoompu <kridsadakorn@biostatgen.org>",
    "author": "Kridsadakorn Chaichoompu [aut, cre],\n  Kristel Van Steen [aut],\n  Fentaw Abegaz [aut]",
    "url": "https://gitlab.com/kris.ccp/filest",
    "bug_reports": "https://gitlab.com/kris.ccp/filest/-/issues",
    "repository": "https://cran.r-project.org/package=FILEST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FILEST Fine-Level Structure Simulator A population genetic simulator, which is able to generate synthetic datasets for single-nucleotide polymorphisms (SNP) for multiple populations. The genetic distances among populations can be set according to the Fixation Index (Fst) as explained in Balding and Nichols (1995) <doi:10.1007/BF01441146>. This tool is able to simulate outlying individuals and missing SNPs can be specified. For Genome-wide association study (GWAS), disease status can be set in desired level according risk ratio.  "
  },
  {
    "id": 3401,
    "package_name": "FRAPO",
    "title": "Financial Risk Modelling and Portfolio Optimisation with R",
    "description": "Accompanying package of the book 'Financial Risk Modelling\n        and Portfolio Optimisation with R', second edition. The data sets used in the book are contained in this package.",
    "version": "0.4-1",
    "maintainer": "Bernhard Pfaff <bernhard@pfaffikus.de>",
    "author": "Bernhard Pfaff [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FRAPO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FRAPO Financial Risk Modelling and Portfolio Optimisation with R Accompanying package of the book 'Financial Risk Modelling\n        and Portfolio Optimisation with R', second edition. The data sets used in the book are contained in this package.  "
  },
  {
    "id": 3411,
    "package_name": "FSA",
    "title": "Simple Fisheries Stock Assessment Methods",
    "description": "A variety of simple fish stock assessment methods.",
    "version": "0.10.0",
    "maintainer": "Derek H. Ogle <DerekOgle51@gmail.com>",
    "author": "Derek H. Ogle [aut, cre],\n  Jason C. Doll [aut],\n  A. Powell Wheeler [aut],\n  Alexis Dinno [aut] (Provided base functionality of dunnTest())",
    "url": "https://fishr-core-team.github.io/FSA/",
    "bug_reports": "https://github.com/fishR-Core-Team/FSA/issues",
    "repository": "https://cran.r-project.org/package=FSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FSA Simple Fisheries Stock Assessment Methods A variety of simple fish stock assessment methods.  "
  },
  {
    "id": 3412,
    "package_name": "FSAdata",
    "title": "Data to Support Fish Stock Assessment ('FSA') Package",
    "description": "The datasets to support the Fish Stock Assessment ('FSA') package.",
    "version": "0.4.1",
    "maintainer": "Derek Ogle <derekogle51@gmail.com>",
    "author": "Derek Ogle [aut, cre] (ORCID: <https://orcid.org/0000-0002-0370-9299>)",
    "url": "https://fishr-core-team.github.io/FSAdata/",
    "bug_reports": "https://github.com/fishR-Core-Team/FSAdata/issues",
    "repository": "https://cran.r-project.org/package=FSAdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FSAdata Data to Support Fish Stock Assessment ('FSA') Package The datasets to support the Fish Stock Assessment ('FSA') package.  "
  },
  {
    "id": 3415,
    "package_name": "FSK2R",
    "title": "An Interface Between the 'FSKX' Standard and 'R'",
    "description": "Functions for importing, creating, editing and\n    exporting 'FSK' files <https://foodrisklabs.bfr.bund.de/fskx-food-safety-knowledge-exchange-format/>\n    using the 'R' programming environment. Furthermore, it enables users\n    to run simulations contained in the 'FSK' files and visualize the results.",
    "version": "0.2.0",
    "maintainer": "Alberto Garre <garre.alberto@gmail.com>",
    "author": "Alberto Garre [aut, cre],\n  Miguel de Alba Aparicio [aut],\n  Thomas Schueler [aut],\n  Pablo S. Fernandez [aut],\n  Matthias Filter [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FSK2R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FSK2R An Interface Between the 'FSKX' Standard and 'R' Functions for importing, creating, editing and\n    exporting 'FSK' files <https://foodrisklabs.bfr.bund.de/fskx-food-safety-knowledge-exchange-format/>\n    using the 'R' programming environment. Furthermore, it enables users\n    to run simulations contained in the 'FSK' files and visualize the results.  "
  },
  {
    "id": 3450,
    "package_name": "FastJM",
    "title": "Semi-Parametric Joint Modeling of Longitudinal and Survival Data",
    "description": "A joint model for large-scale, competing risks time-to-event data with singular or multiple longitudinal biomarkers, implemented with the efficient algorithms developed by Li and colleagues (2022) <doi:10.1155/2022/1362913> and <doi:10.48550/arXiv.2506.12741>.\n             The time-to-event data is modelled using a (cause-specific) Cox \n             proportional hazards regression model with time-fixed covariates. \n             The longitudinal biomarkers are modelled using a linear mixed \n             effects model. The association between the longitudinal submodel \n             and the survival submodel is captured through shared random \n             effects. It allows researchers to analyze large-scale data to \n             model biomarker trajectories, estimate their effects on event \n             outcomes, and dynamically predict future events from patients\u2019 \n             past histories. A function for simulating survival and longitudinal \n             data for multiple biomarkers is also included alongside built-in \n             datasets.",
    "version": "1.5.3",
    "maintainer": "Shanpeng Li <lishanpeng0913@ucla.edu>",
    "author": "Shanpeng Li [aut, cre],\n  Ning Li [ctb],\n  Emily Ouyang [ctb],\n  Hong Wang [ctb],\n  Jin Zhou [ctb],\n  Hua Zhou [ctb],\n  Gang Li [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FastJM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FastJM Semi-Parametric Joint Modeling of Longitudinal and Survival Data A joint model for large-scale, competing risks time-to-event data with singular or multiple longitudinal biomarkers, implemented with the efficient algorithms developed by Li and colleagues (2022) <doi:10.1155/2022/1362913> and <doi:10.48550/arXiv.2506.12741>.\n             The time-to-event data is modelled using a (cause-specific) Cox \n             proportional hazards regression model with time-fixed covariates. \n             The longitudinal biomarkers are modelled using a linear mixed \n             effects model. The association between the longitudinal submodel \n             and the survival submodel is captured through shared random \n             effects. It allows researchers to analyze large-scale data to \n             model biomarker trajectories, estimate their effects on event \n             outcomes, and dynamically predict future events from patients\u2019 \n             past histories. A function for simulating survival and longitudinal \n             data for multiple biomarkers is also included alongside built-in \n             datasets.  "
  },
  {
    "id": 3459,
    "package_name": "FatTailsR",
    "title": "Kiener Distributions and Fat Tails in Finance and Neuroscience",
    "description": "Kiener distributions K1, K2, K3, K4 and K7 to characterize\n    distributions with left and right, symmetric or asymmetric fat tails in \n    finance, neuroscience and other disciplines. Two algorithms to estimate the\n    distribution parameters, quantiles, value-at-risk and expected shortfall.\n    IMPORTANT: Standardization has been changed in versions >= 2.0.0 to get \n\tsd = 1 when kappa = Inf rather than 2*pi/sqrt(3) in versions <= 1.8.6. \n\tThis affects parameter g (other parameters stay unchanged). Do not update \n\tif you need consistent comparisons with previous results for the g parameter. ",
    "version": "2.0.0",
    "maintainer": "Patrice Kiener <fattailsr@inmodelia.com>",
    "author": "Patrice Kiener [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0505-9920>)",
    "url": "https://www.inmodelia.com/fattailsr-en.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FatTailsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FatTailsR Kiener Distributions and Fat Tails in Finance and Neuroscience Kiener distributions K1, K2, K3, K4 and K7 to characterize\n    distributions with left and right, symmetric or asymmetric fat tails in \n    finance, neuroscience and other disciplines. Two algorithms to estimate the\n    distribution parameters, quantiles, value-at-risk and expected shortfall.\n    IMPORTANT: Standardization has been changed in versions >= 2.0.0 to get \n\tsd = 1 when kappa = Inf rather than 2*pi/sqrt(3) in versions <= 1.8.6. \n\tThis affects parameter g (other parameters stay unchanged). Do not update \n\tif you need consistent comparisons with previous results for the g parameter.   "
  },
  {
    "id": 3461,
    "package_name": "FaultTree",
    "title": "Fault Trees for Risk and Reliability Analysis",
    "description": "Construction, calculation and display of fault trees. Methods derived from Clifton A. Ericson II (2005, ISBN: 9780471739425) <DOI:10.1002/0471739421>, Antoine Rauzy (1993) <DOI:10.1016/0951-8320(93)90060-C>, Tim Bedford and Roger Cooke (2012, ISBN: 9780511813597) <DOI:10.1017/CBO9780511813597>, Nikolaos Limnios,  (2007, ISBN: 9780470612484) <DOI: 10.1002/9780470612484>.",
    "version": "1.0.1",
    "maintainer": "Jacob Ormerod <jake@openreliability.org>",
    "author": "David Silkworth [aut],\n  Jacob Ormerod [cre],\n  OpenReliability.org [cph]",
    "url": "http://www.openreliability.org/fault-tree-analysis-on-r/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FaultTree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FaultTree Fault Trees for Risk and Reliability Analysis Construction, calculation and display of fault trees. Methods derived from Clifton A. Ericson II (2005, ISBN: 9780471739425) <DOI:10.1002/0471739421>, Antoine Rauzy (1993) <DOI:10.1016/0951-8320(93)90060-C>, Tim Bedford and Roger Cooke (2012, ISBN: 9780511813597) <DOI:10.1017/CBO9780511813597>, Nikolaos Limnios,  (2007, ISBN: 9780470612484) <DOI: 10.1002/9780470612484>.  "
  },
  {
    "id": 3475,
    "package_name": "FinAna",
    "title": "Financial Analysis and Regression Diagnostic Analysis",
    "description": "Functions for financial analysis and financial modeling, \n             including batch graphs generation, beta calculation, \n             descriptive statistics, annuity calculation, bond pricing \n             and financial data download.",
    "version": "0.1.2",
    "maintainer": "Xuanhua(Peter) Yin <peteryin.sju@hotmail.com>",
    "author": "Xuanhua(Peter) Yin <peteryin.sju@hotmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FinAna",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinAna Financial Analysis and Regression Diagnostic Analysis Functions for financial analysis and financial modeling, \n             including batch graphs generation, beta calculation, \n             descriptive statistics, annuity calculation, bond pricing \n             and financial data download.  "
  },
  {
    "id": 3476,
    "package_name": "FinCal",
    "title": "Time Value of Money, Time Series Analysis and Computational\nFinance",
    "description": "Package for time value of money calculation, time series analysis and computational finance.",
    "version": "0.6.3",
    "maintainer": "Felix Yanhui Fan <nolanfyh@gmail.com>",
    "author": "Felix Yanhui Fan <nolanfyh@gmail.com>",
    "url": "http://felixfan.github.io/FinCal/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FinCal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinCal Time Value of Money, Time Series Analysis and Computational\nFinance Package for time value of money calculation, time series analysis and computational finance.  "
  },
  {
    "id": 3477,
    "package_name": "FinCovRegularization",
    "title": "Covariance Matrix Estimation and Regularization for Finance",
    "description": "Estimation and regularization for covariance matrix of asset\n    returns. For covariance matrix estimation, three major types of factor\n    models are included: macroeconomic factor model, fundamental factor model and\n    statistical factor model. For covariance matrix regularization, four regularized\n    estimators are included: banding, tapering, hard-thresholding and soft-\n    thresholding. The tuning parameters of these regularized estimators are selected\n    via cross-validation.",
    "version": "1.1.0",
    "maintainer": "YaChen Yan <yanyachen21@gmail.com>",
    "author": "YaChen Yan [aut, cre],\n  FangZhu Lin [aut]",
    "url": "http://github.com/yanyachen/FinCovRegularization",
    "bug_reports": "http://github.com/yanyachen/FinCovRegularization/issues",
    "repository": "https://cran.r-project.org/package=FinCovRegularization",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinCovRegularization Covariance Matrix Estimation and Regularization for Finance Estimation and regularization for covariance matrix of asset\n    returns. For covariance matrix estimation, three major types of factor\n    models are included: macroeconomic factor model, fundamental factor model and\n    statistical factor model. For covariance matrix regularization, four regularized\n    estimators are included: banding, tapering, hard-thresholding and soft-\n    thresholding. The tuning parameters of these regularized estimators are selected\n    via cross-validation.  "
  },
  {
    "id": 3478,
    "package_name": "FinNet",
    "title": "Quickly Build and Manipulate Financial Networks",
    "description": "Providing classes, methods, and functions to deal with financial networks. \n    Users can easily store information about both physical and legal persons by using pre-made classes that are studied for integration with scraping packages such as 'rvest' and 'RSelenium'.\n    Moreover, the package assists in creating various types of financial networks depending on the type of relation between its units depending on the relation under scrutiny (ownership, board interlocks, etc.), the desired tie type (valued or binary), and renders them in the most common formats (adjacency matrix, incidence matrix, edge list, 'igraph', 'network').\n    There are also ad-hoc functions for the Fiedler value, global network efficiency, and cascade-failure analysis.",
    "version": "0.2.1",
    "maintainer": "Fabio Ashtar Telarico <Fabio-Ashtar.Telarico@fdv.uni-lj.si>",
    "author": "Fabio Ashtar Telarico [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>)",
    "url": "https://fatelarico.github.io/FinNet.html",
    "bug_reports": "https://github.com/FATelarico/FinNet/issues",
    "repository": "https://cran.r-project.org/package=FinNet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinNet Quickly Build and Manipulate Financial Networks Providing classes, methods, and functions to deal with financial networks. \n    Users can easily store information about both physical and legal persons by using pre-made classes that are studied for integration with scraping packages such as 'rvest' and 'RSelenium'.\n    Moreover, the package assists in creating various types of financial networks depending on the type of relation between its units depending on the relation under scrutiny (ownership, board interlocks, etc.), the desired tie type (valued or binary), and renders them in the most common formats (adjacency matrix, incidence matrix, edge list, 'igraph', 'network').\n    There are also ad-hoc functions for the Fiedler value, global network efficiency, and cascade-failure analysis.  "
  },
  {
    "id": 3479,
    "package_name": "FinTS",
    "title": "Companion to Tsay (2005) Analysis of Financial Time Series",
    "description": "R companion to Tsay (2005) Analysis of Financial Time\n   Series, second edition (Wiley).  Includes data sets, functions and\n   script files required to work some of the examples.  Version 0.3-x\n   includes R objects for all data files used in the text and script\n   files to recreate most of the analyses in chapters 1-3 and 9 plus\n   parts of chapters 4 and 11.",
    "version": "0.4-9",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Spencer Graves [aut],\n  Georgi N. Boshnakov [cre, ctb]",
    "url": "https://geobosh.github.io/FinTSDoc/ (doc),\nhttps://r-forge.r-project.org/projects/fints/ (devel)",
    "bug_reports": "https://r-forge.r-project.org/tracker/?group_id=84&atid=380",
    "repository": "https://cran.r-project.org/package=FinTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinTS Companion to Tsay (2005) Analysis of Financial Time Series R companion to Tsay (2005) Analysis of Financial Time\n   Series, second edition (Wiley).  Includes data sets, functions and\n   script files required to work some of the examples.  Version 0.3-x\n   includes R objects for all data files used in the text and script\n   files to recreate most of the analyses in chapters 1-3 and 9 plus\n   parts of chapters 4 and 11.  "
  },
  {
    "id": 3480,
    "package_name": "FinancialMath",
    "title": "Financial Mathematics for Actuaries",
    "description": "Contains financial math functions and introductory derivative functions included in the Society of Actuaries and Casualty Actuarial Society 'Financial Mathematics' exam, and some topics in the 'Models for Financial Economics' exam.",
    "version": "0.1.1",
    "maintainer": "Kameron Penn <kameron.penn.financialmath@gmail.com>",
    "author": "Kameron Penn [aut, cre],\n  Jack Schmidt [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FinancialMath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinancialMath Financial Mathematics for Actuaries Contains financial math functions and introductory derivative functions included in the Society of Actuaries and Casualty Actuarial Society 'Financial Mathematics' exam, and some topics in the 'Models for Financial Economics' exam.  "
  },
  {
    "id": 3510,
    "package_name": "ForCausality",
    "title": "A Curated Collection of 'Causal Inference' Datasets and Tools",
    "description": "Provides a comprehensive set of datasets and tools for 'causal inference' research. \n    The package includes data from clinical trials, cancer studies, epidemiological surveys, environmental exposures, and health-related observational studies.\n    Designed to facilitate causal analysis, risk assessment, and advanced statistical modeling, \n    it leverages datasets from packages such as 'causalOT', 'survival', 'causalPAF', 'evident', 'melt', and 'sanon'. \n    The package is inspired by the foundational work of Pearl (2009) <doi:10.1017/CBO9780511803161> on causal inference frameworks.",
    "version": "0.1.0",
    "maintainer": "Tom\u00e1s Valderrama <tomasvm2004@gmail.com>",
    "author": "Tom\u00e1s Valderrama [aut, cre]",
    "url": "https://github.com/Toby-codigos/ForCausality,\nhttps://toby-codigos.github.io/ForCausality/",
    "bug_reports": "https://github.com/Toby-codigos/ForCausality/issues",
    "repository": "https://cran.r-project.org/package=ForCausality",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForCausality A Curated Collection of 'Causal Inference' Datasets and Tools Provides a comprehensive set of datasets and tools for 'causal inference' research. \n    The package includes data from clinical trials, cancer studies, epidemiological surveys, environmental exposures, and health-related observational studies.\n    Designed to facilitate causal analysis, risk assessment, and advanced statistical modeling, \n    it leverages datasets from packages such as 'causalOT', 'survival', 'causalPAF', 'evident', 'melt', and 'sanon'. \n    The package is inspired by the foundational work of Pearl (2009) <doi:10.1017/CBO9780511803161> on causal inference frameworks.  "
  },
  {
    "id": 3515,
    "package_name": "ForeComp",
    "title": "Size-Power Tradeoff Visualization for Equal Predictive Ability\nof Two Forecasts",
    "description": "Offers a set of tools for visualizing and analyzing size and power properties of the test for equal predictive accuracy, the Diebold-Mariano test that is based on heteroskedasticity and autocorrelation-robust (HAR) inference. A typical HAR inference is involved with non-parametric estimation of the long-run variance, and one of its tuning parameters, the truncation parameter, trades off a size and power. Lazarus, Lewis, and Stock (2021)<doi:10.3982/ECTA15404> theoretically characterize the size-power frontier for the Gaussian multivariate location model. 'ForeComp' computes and visualizes the finite-sample size-power frontier of the Diebold-Mariano test based on fixed-b asymptotics together with the Bartlett kernel. To compute the finite-sample size and power, it works with the best approximating ARMA process to the given dataset. It informs the user how their choice of the truncation parameter performs and how robust the testing outcomes are.",
    "version": "0.9.0",
    "maintainer": "Minchul Shin <visiblehand@gmail.com>",
    "author": "Nathan Schor [aut],\n  Minchul Shin [aut, cre, cph]",
    "url": "https://github.com/mcmcs/ForeComp",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ForeComp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForeComp Size-Power Tradeoff Visualization for Equal Predictive Ability\nof Two Forecasts Offers a set of tools for visualizing and analyzing size and power properties of the test for equal predictive accuracy, the Diebold-Mariano test that is based on heteroskedasticity and autocorrelation-robust (HAR) inference. A typical HAR inference is involved with non-parametric estimation of the long-run variance, and one of its tuning parameters, the truncation parameter, trades off a size and power. Lazarus, Lewis, and Stock (2021)<doi:10.3982/ECTA15404> theoretically characterize the size-power frontier for the Gaussian multivariate location model. 'ForeComp' computes and visualizes the finite-sample size-power frontier of the Diebold-Mariano test based on fixed-b asymptotics together with the Bartlett kernel. To compute the finite-sample size and power, it works with the best approximating ARMA process to the given dataset. It informs the user how their choice of the truncation parameter performs and how robust the testing outcomes are.  "
  },
  {
    "id": 3578,
    "package_name": "GALLO",
    "title": "Genomic Annotation in Livestock for Positional Candidate LOci",
    "description": "The accurate annotation of genes and Quantitative Trait Loci (QTLs) located within candidate markers and/or regions (haplotypes, windows, CNVs, etc) is a crucial step the most common genomic analyses performed in livestock, such as Genome-Wide Association Studies or transcriptomics. The Genomic Annotation in Livestock for positional candidate LOci (GALLO) is an R package designed to provide an intuitive and straightforward environment to annotate positional candidate genes and QTLs from high-throughput genetic studies in livestock. Moreover, GALLO allows the graphical visualization of gene and QTL annotation results, data comparison among different grouping factors (e.g., methods, breeds, tissues, statistical models, studies, etc.), and QTL enrichment in different livestock species including cattle, pigs, sheep, and chicken, among others.",
    "version": "1.5",
    "maintainer": "Pablo Fonseca <pfonseca@uoguelph.ca>",
    "author": "Pablo Fonseca [aut, cre],\n  Aroa Suarez-Vega [aut],\n  Gabriele Marras [aut],\n  Angela C\u00e1novas [aut]",
    "url": "<https://github.com/pablobio/GALLO>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GALLO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GALLO Genomic Annotation in Livestock for Positional Candidate LOci The accurate annotation of genes and Quantitative Trait Loci (QTLs) located within candidate markers and/or regions (haplotypes, windows, CNVs, etc) is a crucial step the most common genomic analyses performed in livestock, such as Genome-Wide Association Studies or transcriptomics. The Genomic Annotation in Livestock for positional candidate LOci (GALLO) is an R package designed to provide an intuitive and straightforward environment to annotate positional candidate genes and QTLs from high-throughput genetic studies in livestock. Moreover, GALLO allows the graphical visualization of gene and QTL annotation results, data comparison among different grouping factors (e.g., methods, breeds, tissues, statistical models, studies, etc.), and QTL enrichment in different livestock species including cattle, pigs, sheep, and chicken, among others.  "
  },
  {
    "id": 3598,
    "package_name": "GCPM",
    "title": "Generalized Credit Portfolio Model",
    "description": "Analyze the default risk of credit portfolios. Commonly known models, \n\t\tlike CreditRisk+ or the CreditMetrics model are implemented in their very basic settings.\n\t\tThe portfolio loss distribution can be achieved either by simulation or analytically \n\t\tin case of the classic CreditRisk+ model. Models are only implemented to respect losses\n\t\tcaused by defaults, i.e. migration risk is not included. The package structure is kept\n\t\tflexible especially with respect to distributional assumptions in order to quantify the\n\t\tsensitivity of risk figures with respect to several assumptions. Therefore the package\n\t\tcan be used to determine the credit risk of a given portfolio as well as to quantify\n\t\tmodel sensitivities.",
    "version": "1.2.2",
    "maintainer": "Kevin Jakob <Kevin.Jakob.Research@gmail.com>",
    "author": "Kevin Jakob",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GCPM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GCPM Generalized Credit Portfolio Model Analyze the default risk of credit portfolios. Commonly known models, \n\t\tlike CreditRisk+ or the CreditMetrics model are implemented in their very basic settings.\n\t\tThe portfolio loss distribution can be achieved either by simulation or analytically \n\t\tin case of the classic CreditRisk+ model. Models are only implemented to respect losses\n\t\tcaused by defaults, i.e. migration risk is not included. The package structure is kept\n\t\tflexible especially with respect to distributional assumptions in order to quantify the\n\t\tsensitivity of risk figures with respect to several assumptions. Therefore the package\n\t\tcan be used to determine the credit risk of a given portfolio as well as to quantify\n\t\tmodel sensitivities.  "
  },
  {
    "id": 3617,
    "package_name": "GEInter",
    "title": "Robust Gene-Environment Interaction Analysis",
    "description": "Description: For the risk, progression, and response to treatment of many complex diseases, it has been increasingly recognized that gene-environment interactions play important roles beyond the main genetic and environmental effects. In practical interaction analyses, outliers in response variables and covariates are not uncommon. In addition, missingness in environmental factors is routinely encountered in epidemiological studies. The developed package consists of five robust approaches to address the outliers problems, among which two approaches can also accommodate missingness in environmental factors. Both continuous and right censored responses are considered. The proposed approaches are based on penalization and sparse boosting techniques for identifying important interactions, which are realized using efficient algorithms. Beyond the gene-environment analysis, the developed package can also be adopted to conduct analysis on interactions between other types of low-dimensional and high-dimensional data. (Mengyun Wu et al (2017), <doi:10.1080/00949655.2018.1523411>; Mengyun Wu et al (2017), <doi:10.1002/gepi.22055>; Yaqing Xu et al (2018), <doi:10.1080/00949655.2018.1523411>; Yaqing Xu et al (2019), <doi:10.1016/j.ygeno.2018.07.006>; Mengyun Wu et al (2021), <doi:10.1093/bioinformatics/btab318>). ",
    "version": "0.3.2",
    "maintainer": "Xing Qin <qin.xing@163.sufe.edu.cn>",
    "author": "Mengyun Wu [aut],\n  Xing Qin [aut, cre],\n  Shuangge Ma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GEInter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GEInter Robust Gene-Environment Interaction Analysis Description: For the risk, progression, and response to treatment of many complex diseases, it has been increasingly recognized that gene-environment interactions play important roles beyond the main genetic and environmental effects. In practical interaction analyses, outliers in response variables and covariates are not uncommon. In addition, missingness in environmental factors is routinely encountered in epidemiological studies. The developed package consists of five robust approaches to address the outliers problems, among which two approaches can also accommodate missingness in environmental factors. Both continuous and right censored responses are considered. The proposed approaches are based on penalization and sparse boosting techniques for identifying important interactions, which are realized using efficient algorithms. Beyond the gene-environment analysis, the developed package can also be adopted to conduct analysis on interactions between other types of low-dimensional and high-dimensional data. (Mengyun Wu et al (2017), <doi:10.1080/00949655.2018.1523411>; Mengyun Wu et al (2017), <doi:10.1002/gepi.22055>; Yaqing Xu et al (2018), <doi:10.1080/00949655.2018.1523411>; Yaqing Xu et al (2019), <doi:10.1016/j.ygeno.2018.07.006>; Mengyun Wu et al (2021), <doi:10.1093/bioinformatics/btab318>).   "
  },
  {
    "id": 3635,
    "package_name": "GFDrmtl",
    "title": "Multiple RMTL-Based Tests for Competing Risks Data in General\nFactorial Designs",
    "description": "We implemented multiple tests based on the restricted mean time lost (RMTL) for general factorial designs as described in Munko et al. (2024) <doi:10.48550/arXiv.2409.07917>. Therefore, an asymptotic test and a permutation test are incorporated with a Wald-type test statistic. The asymptotic test takes the asymptotic exact dependence structure of the test statistics into account to gain more power. Furthermore, confidence intervals for RMTL contrasts can be calculated and plotted and a stepwise extension that can improve the power of the multiple tests is available.",
    "version": "0.1.0",
    "maintainer": "Merle Munko <merle.munko@ovgu.de>",
    "author": "Marc Ditzhaus [aut],\n  Dennis Dobler [aut],\n  Merle Munko [aut, cre],\n  Jannes Walter [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GFDrmtl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GFDrmtl Multiple RMTL-Based Tests for Competing Risks Data in General\nFactorial Designs We implemented multiple tests based on the restricted mean time lost (RMTL) for general factorial designs as described in Munko et al. (2024) <doi:10.48550/arXiv.2409.07917>. Therefore, an asymptotic test and a permutation test are incorporated with a Wald-type test statistic. The asymptotic test takes the asymptotic exact dependence structure of the test statistics into account to gain more power. Furthermore, confidence intervals for RMTL contrasts can be calculated and plotted and a stepwise extension that can improve the power of the multiple tests is available.  "
  },
  {
    "id": 3638,
    "package_name": "GFGM.copula",
    "title": "Generalized Farlie-Gumbel-Morgenstern Copula",
    "description": "Compute bivariate dependence measures and perform bivariate competing risks analysis under the generalized Farlie-Gumbel-Morgenstern (FGM) copula. See Shih and Emura (2018) <doi:10.1007/s00180-018-0804-0> and Shih and Emura (2019) <doi:10.1007/s00362-016-0865-5> for details.",
    "version": "1.0.4",
    "maintainer": "Jia-Han Shih <tommy355097@gmail.com>",
    "author": "Jia-Han Shih",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GFGM.copula",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GFGM.copula Generalized Farlie-Gumbel-Morgenstern Copula Compute bivariate dependence measures and perform bivariate competing risks analysis under the generalized Farlie-Gumbel-Morgenstern (FGM) copula. See Shih and Emura (2018) <doi:10.1007/s00180-018-0804-0> and Shih and Emura (2019) <doi:10.1007/s00362-016-0865-5> for details.  "
  },
  {
    "id": 3639,
    "package_name": "GFM",
    "title": "Generalized Factor Model",
    "description": "Generalized factor model is implemented for ultra-high dimensional data with mixed-type variables.\n    Two algorithms, variational EM and alternate maximization, are designed to implement the generalized factor model,\n    respectively. The factor matrix and loading matrix together with the number of factors can be well estimated. \n    This model can be employed in social and behavioral sciences, economy and finance, and  genomics, \n    to extract interpretable nonlinear factors. More details can be referred to \n    Wei Liu, Huazhen Lin, Shurong Zheng and Jin Liu. (2021) <doi:10.1080/01621459.2021.1999818>. ",
    "version": "1.2.1",
    "maintainer": "Wei Liu <LiuWeideng@gmail.com>",
    "author": "Wei Liu [aut, cre],\n  Huazhen Lin [aut],\n  Shurong Zheng [aut],\n  Jin Liu [aut],\n  Jinyu Nie [aut]",
    "url": "https://github.com/feiyoung/GFM",
    "bug_reports": "https://github.com/feiyoung/GFM/issues",
    "repository": "https://cran.r-project.org/package=GFM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GFM Generalized Factor Model Generalized factor model is implemented for ultra-high dimensional data with mixed-type variables.\n    Two algorithms, variational EM and alternate maximization, are designed to implement the generalized factor model,\n    respectively. The factor matrix and loading matrix together with the number of factors can be well estimated. \n    This model can be employed in social and behavioral sciences, economy and finance, and  genomics, \n    to extract interpretable nonlinear factors. More details can be referred to \n    Wei Liu, Huazhen Lin, Shurong Zheng and Jin Liu. (2021) <doi:10.1080/01621459.2021.1999818>.   "
  },
  {
    "id": 3653,
    "package_name": "GHRmodel",
    "title": "Bayesian Hierarchical Modelling of Spatio-Temporal Health Data",
    "description": "Supports modeling health outcomes using Bayesian hierarchical \n  spatio-temporal models with complex covariate effects (e.g., linear, \n  non-linear, interactions, distributed lag linear and non-linear \n  models) in the 'INLA' framework. It is designed to help users identify key \n  drivers and  predictors of disease risk by enabling streamlined model \n  exploration, comparison, and visualization of complex covariate effects. \n  See an application of the modelling framework in Lowe, Lee, O'Reilly et al. (2021)\n  <doi:10.1016/S2542-5196(20)30292-8>.",
    "version": "0.1.1",
    "maintainer": "Carles Mil\u00e0 <carles.milagarcia@bsc.es>",
    "author": "Carles Mil\u00e0 [aut, cre] (ORCID: <https://orcid.org/0000-0003-0470-0760>),\n  Giovenale Moirano [aut] (ORCID:\n    <https://orcid.org/0000-0001-8748-3321>),\n  Anna B. Kawiecki [aut] (ORCID: <https://orcid.org/0000-0002-0499-2612>),\n  Rachel Lowe [aut] (ORCID: <https://orcid.org/0000-0003-3939-7343>)",
    "url": "https://gitlab.earth.bsc.es/ghr/ghrmodel,\nhttps://bsc-es.github.io/GHRtools/docs/GHRmodel/GHRmodel",
    "bug_reports": "https://gitlab.earth.bsc.es/ghr/ghrmodel/-/issues",
    "repository": "https://cran.r-project.org/package=GHRmodel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GHRmodel Bayesian Hierarchical Modelling of Spatio-Temporal Health Data Supports modeling health outcomes using Bayesian hierarchical \n  spatio-temporal models with complex covariate effects (e.g., linear, \n  non-linear, interactions, distributed lag linear and non-linear \n  models) in the 'INLA' framework. It is designed to help users identify key \n  drivers and  predictors of disease risk by enabling streamlined model \n  exploration, comparison, and visualization of complex covariate effects. \n  See an application of the modelling framework in Lowe, Lee, O'Reilly et al. (2021)\n  <doi:10.1016/S2542-5196(20)30292-8>.  "
  },
  {
    "id": 3739,
    "package_name": "GRSxE",
    "title": "Testing Gene-Environment Interactions Through Genetic Risk\nScores",
    "description": "Statistical testing procedures for detecting\n  GxE (gene-environment) interactions. The main focus lies on\n  GRSxE interaction tests that aim at detecting GxE interactions\n  through GRS (genetic risk scores). Moreover, a novel testing\n  procedure based on bagging and OOB (out-of-bag) predictions is\n  implemented for incorporating all available observations at\n  both GRS construction and GxE testing (Lau et al., 2023,\n  <doi:10.1038/s41598-023-28172-4>).",
    "version": "1.0.1",
    "maintainer": "Michael Lau <michael.lau@hhu.de>",
    "author": "Michael Lau [aut, cre] (ORCID: <https://orcid.org/0000-0002-5327-8351>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GRSxE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GRSxE Testing Gene-Environment Interactions Through Genetic Risk\nScores Statistical testing procedures for detecting\n  GxE (gene-environment) interactions. The main focus lies on\n  GRSxE interaction tests that aim at detecting GxE interactions\n  through GRS (genetic risk scores). Moreover, a novel testing\n  procedure based on bagging and OOB (out-of-bag) predictions is\n  implemented for incorporating all available observations at\n  both GRS construction and GxE testing (Lau et al., 2023,\n  <doi:10.1038/s41598-023-28172-4>).  "
  },
  {
    "id": 3833,
    "package_name": "GeoTox",
    "title": "Spatiotemporal Mixture Risk Assessment",
    "description": "Connecting spatiotemporal exposure to individual and\n    population-level risk via source-to-outcome continuum modeling. The package,\n    methods, and case-studies are described in Messier, Reif, and Marvel (2024)\n    <doi:10.1101/2024.09.23.24314096> and Eccles et al. (2023)\n    <doi:10.1016/j.scitotenv.2022.158905>.",
    "version": "0.2.0",
    "maintainer": "Kyle Messier <kyle.messier@nih.gov>",
    "author": "Skylar Marvel [aut] (ORCID: <https://orcid.org/0000-0002-2971-9743>),\n  David Reif [aut] (ORCID: <https://orcid.org/0000-0001-7815-6767>),\n  Kyle Messier [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-9508-9623>),\n  Spatiotemporal Exposures and Toxicology Group [cph]",
    "url": "https://niehs.github.io/GeoTox/, https://github.com/NIEHS/GeoTox",
    "bug_reports": "https://github.com/NIEHS/GeoTox/issues",
    "repository": "https://cran.r-project.org/package=GeoTox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeoTox Spatiotemporal Mixture Risk Assessment Connecting spatiotemporal exposure to individual and\n    population-level risk via source-to-outcome continuum modeling. The package,\n    methods, and case-studies are described in Messier, Reif, and Marvel (2024)\n    <doi:10.1101/2024.09.23.24314096> and Eccles et al. (2023)\n    <doi:10.1016/j.scitotenv.2022.158905>.  "
  },
  {
    "id": 3840,
    "package_name": "GetDFPData",
    "title": "Reading Annual Financial Reports from Bovespa's DFP, FRE and FCA\nSystem",
    "description": "Reads annual financial reports including assets, liabilities, dividends history, stockholder composition and much more from Bovespa's DFP, FRE and FCA systems <http://www.b3.com.br/pt_br/produtos-e-servicos/negociacao/renda-variavel/empresas-listadas.htm>.\n These are web based interfaces for all financial reports of companies traded at Bovespa. The package is specially designed for large scale data importation, keeping a tabular (long) structure for easier processing.  ",
    "version": "1.6",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre]",
    "url": "https://github.com/msperlin/GetDFPData/",
    "bug_reports": "https://github.com/msperlin/GetDFPData/issues",
    "repository": "https://cran.r-project.org/package=GetDFPData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GetDFPData Reading Annual Financial Reports from Bovespa's DFP, FRE and FCA\nSystem Reads annual financial reports including assets, liabilities, dividends history, stockholder composition and much more from Bovespa's DFP, FRE and FCA systems <http://www.b3.com.br/pt_br/produtos-e-servicos/negociacao/renda-variavel/empresas-listadas.htm>.\n These are web based interfaces for all financial reports of companies traded at Bovespa. The package is specially designed for large scale data importation, keeping a tabular (long) structure for easier processing.    "
  },
  {
    "id": 3841,
    "package_name": "GetDFPData2",
    "title": "Reading Annual and Quarterly Financial Reports from B3",
    "description": "Reads annual and quarterly financial reports from companies traded at B3, the Brazilian exchange \n            <https://www.b3.com.br/>. \n            All data is downloaded and imported from CVM's public ftp site <https://dados.cvm.gov.br/dados/CIA_ABERTA/>.",
    "version": "0.6.3",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre],\n  Guilherme Kirch [aut]",
    "url": "https://github.com/msperlin/GetDFPData2/",
    "bug_reports": "https://github.com/msperlin/GetDFPData2/issues/",
    "repository": "https://cran.r-project.org/package=GetDFPData2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GetDFPData2 Reading Annual and Quarterly Financial Reports from B3 Reads annual and quarterly financial reports from companies traded at B3, the Brazilian exchange \n            <https://www.b3.com.br/>. \n            All data is downloaded and imported from CVM's public ftp site <https://dados.cvm.gov.br/dados/CIA_ABERTA/>.  "
  },
  {
    "id": 3904,
    "package_name": "GxEprs",
    "title": "Genotype-by-Environment Interaction in Polygenic Score Models",
    "description": "A novel PRS model is introduced to enhance the prediction accuracy by utilising GxE effects. This package performs Genome Wide Association Studies (GWAS) and Genome Wide Environment Interaction Studies (GWEIS) using a discovery dataset. The package has the ability to obtain polygenic risk scores (PRSs) for a target sample. Finally it predicts the risk values of each individual in the target sample. Users have the choice of using existing models (Li et al., 2015) <doi:10.1093/annonc/mdu565>, (Pandis et al., 2013) <doi:10.1093/ejo/cjt054>, (Peyrot et al., 2018) <doi:10.1016/j.biopsych.2017.09.009> and (Song et al., 2022) <doi:10.1038/s41467-022-32407-9>, as well as newly proposed models for genomic risk prediction (refer to the URL for more details).",
    "version": "1.2",
    "maintainer": "Dovini Jayasinghe <dovini.jayasinghe@mymail.unisa.edu.au>",
    "author": "Dovini Jayasinghe [aut, cre, cph],\n  Hong Lee [aut, cph],\n  Moksedul Momin [aut, cph]",
    "url": "https://github.com/DoviniJ/GxEprs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GxEprs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GxEprs Genotype-by-Environment Interaction in Polygenic Score Models A novel PRS model is introduced to enhance the prediction accuracy by utilising GxE effects. This package performs Genome Wide Association Studies (GWAS) and Genome Wide Environment Interaction Studies (GWEIS) using a discovery dataset. The package has the ability to obtain polygenic risk scores (PRSs) for a target sample. Finally it predicts the risk values of each individual in the target sample. Users have the choice of using existing models (Li et al., 2015) <doi:10.1093/annonc/mdu565>, (Pandis et al., 2013) <doi:10.1093/ejo/cjt054>, (Peyrot et al., 2018) <doi:10.1016/j.biopsych.2017.09.009> and (Song et al., 2022) <doi:10.1038/s41467-022-32407-9>, as well as newly proposed models for genomic risk prediction (refer to the URL for more details).  "
  },
  {
    "id": 3929,
    "package_name": "HDMFA",
    "title": "High-Dimensional Matrix Factor Analysis",
    "description": "High-dimensional matrix factor models have drawn much attention in view of the fact that observations are usually well structured to be an array such as in macroeconomics and finance. In addition, data often exhibit heavy-tails and thus it is also important to develop robust procedures. We aim to address this issue by replacing the least square loss with Huber loss function. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's Frobenius norm, which leads to a weighted iterative projection approach to compute and learn the parameters and thereby named as Robust-Matrix-Factor-Analysis (RMFA), see the details in He et al. (2023)<doi:10.1080/07350015.2023.2191676>. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm (IHR), see the details in He et al. (2023) <arXiv:2306.03317>. In this package, we also provide the algorithm for alpha-PCA by Chen & Fan (2021) <doi:10.1080/01621459.2021.1970569>, the Projected estimation (PE) method by Yu et al. (2022)<doi:10.1016/j.jeconom.2021.04.001>. In addition, the methods for determining the pair of factor numbers are also given.",
    "version": "0.1.1",
    "maintainer": "Ran Zhao <Zhaoran@mail.sdu.edu.cn>",
    "author": "Yong He [aut],\n  Changwei Zhao [aut],\n  Ran Zhao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HDMFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDMFA High-Dimensional Matrix Factor Analysis High-dimensional matrix factor models have drawn much attention in view of the fact that observations are usually well structured to be an array such as in macroeconomics and finance. In addition, data often exhibit heavy-tails and thus it is also important to develop robust procedures. We aim to address this issue by replacing the least square loss with Huber loss function. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's Frobenius norm, which leads to a weighted iterative projection approach to compute and learn the parameters and thereby named as Robust-Matrix-Factor-Analysis (RMFA), see the details in He et al. (2023)<doi:10.1080/07350015.2023.2191676>. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm (IHR), see the details in He et al. (2023) <arXiv:2306.03317>. In this package, we also provide the algorithm for alpha-PCA by Chen & Fan (2021) <doi:10.1080/01621459.2021.1970569>, the Projected estimation (PE) method by Yu et al. (2022)<doi:10.1016/j.jeconom.2021.04.001>. In addition, the methods for determining the pair of factor numbers are also given.  "
  },
  {
    "id": 3933,
    "package_name": "HDRFA",
    "title": "High-Dimensional Robust Factor Analysis",
    "description": "Factor models have been widely applied in areas such as economics and finance, and the well-known heavy-tailedness of macroeconomic/financial data should be taken into account when conducting factor analysis. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's L2 norm, which turns out to do Principal Component Analysis (PCA) on the weighted sample covariance matrix and thereby named as Huber PCA. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm. In this package we also provide the code for traditional PCA, the Robust Two Step (RTS) method by He et al. (2022) and the Quantile Factor Analysis (QFA) method by Chen et al. (2021) and He et al. (2023).",
    "version": "0.1.5",
    "maintainer": "Dong Liu <liudong_stat@163.com>",
    "author": "Yong He [aut],\n  Lingxiao Li [aut],\n  Dong Liu [aut, cre],\n  Wenxin Zhou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HDRFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDRFA High-Dimensional Robust Factor Analysis Factor models have been widely applied in areas such as economics and finance, and the well-known heavy-tailedness of macroeconomic/financial data should be taken into account when conducting factor analysis. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's L2 norm, which turns out to do Principal Component Analysis (PCA) on the weighted sample covariance matrix and thereby named as Huber PCA. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm. In this package we also provide the code for traditional PCA, the Robust Two Step (RTS) method by He et al. (2022) and the Quantile Factor Analysis (QFA) method by Chen et al. (2021) and He et al. (2023).  "
  },
  {
    "id": 3934,
    "package_name": "HDShOP",
    "title": "High-Dimensional Shrinkage Optimal Portfolios",
    "description": "Constructs shrinkage estimators of high-dimensional mean-variance portfolios and performs \n    high-dimensional tests on optimality of a given portfolio. The techniques developed in \n    Bodnar et al. (2018 <doi:10.1016/j.ejor.2017.09.028>, 2019 <doi:10.1109/TSP.2019.2929964>, \n    2020 <doi:10.1109/TSP.2020.3037369>, 2021 <doi:10.1080/07350015.2021.2004897>) \n    are central to the package. They provide simple and feasible estimators and tests for optimal \n    portfolio weights, which are applicable for 'large p and large n' situations where p is the \n    portfolio dimension (number of stocks) and n is the sample size. The package also includes tools\n    for constructing portfolios based on shrinkage estimators of the mean vector and covariance matrix\n    as well as a new Bayesian estimator for the Markowitz efficient frontier recently developed by \n    Bauder et al. (2021) <doi:10.1080/14697688.2020.1748214>.",
    "version": "0.1.7",
    "maintainer": "Dmitry Otryakhin <d.otryakhin.acad@protonmail.ch>",
    "author": "Taras Bodnar [aut] (ORCID: <https://orcid.org/0000-0001-7855-8221>),\n  Solomiia Dmytriv [aut] (ORCID: <https://orcid.org/0000-0003-1855-3044>),\n  Yarema Okhrin [aut] (ORCID: <https://orcid.org/0000-0003-4704-5233>),\n  Dmitry Otryakhin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4700-7221>),\n  Nestor Parolya [aut] (ORCID: <https://orcid.org/0000-0003-2147-2288>)",
    "url": "https://github.com/Otryakhin-Dmitry/global-minimum-variance-portfolio",
    "bug_reports": "https://github.com/Otryakhin-Dmitry/global-minimum-variance-portfolio/issues",
    "repository": "https://cran.r-project.org/package=HDShOP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDShOP High-Dimensional Shrinkage Optimal Portfolios Constructs shrinkage estimators of high-dimensional mean-variance portfolios and performs \n    high-dimensional tests on optimality of a given portfolio. The techniques developed in \n    Bodnar et al. (2018 <doi:10.1016/j.ejor.2017.09.028>, 2019 <doi:10.1109/TSP.2019.2929964>, \n    2020 <doi:10.1109/TSP.2020.3037369>, 2021 <doi:10.1080/07350015.2021.2004897>) \n    are central to the package. They provide simple and feasible estimators and tests for optimal \n    portfolio weights, which are applicable for 'large p and large n' situations where p is the \n    portfolio dimension (number of stocks) and n is the sample size. The package also includes tools\n    for constructing portfolios based on shrinkage estimators of the mean vector and covariance matrix\n    as well as a new Bayesian estimator for the Markowitz efficient frontier recently developed by \n    Bauder et al. (2021) <doi:10.1080/14697688.2020.1748214>.  "
  },
  {
    "id": 4007,
    "package_name": "Haplin",
    "title": "Analyzing Case-Parent Triad and/or Case-Control Data with SNP\nHaplotypes",
    "description": "Performs genetic association analyses of case-parent triad (trio) data with multiple markers. It can also incorporate complete or incomplete control triads, for instance independent control children. Estimation is based on haplotypes, for instance SNP haplotypes, even though phase is not known from the genetic data. 'Haplin' estimates relative risk (RR + conf.int.) and p-value associated with each haplotype. It uses maximum likelihood estimation to make optimal use of data from triads with missing genotypic data, for instance if some SNPs has not been typed for some individuals. 'Haplin' also allows estimation of effects of maternal haplotypes and parent-of-origin effects, particularly appropriate in perinatal epidemiology. 'Haplin' allows special models, like X-inactivation, to be fitted on the X-chromosome. A GxE analysis allows testing interactions between environment and all estimated genetic effects. The models were originally described in \"Gjessing HK and Lie RT. Case-parent triads: Estimating single- and double-dose effects of fetal and maternal disease gene haplotypes. Annals of Human Genetics (2006) 70, pp. 382-396\".",
    "version": "7.3.2",
    "maintainer": "Hakon K. Gjessing <hakon.gjessing@uib.no>",
    "author": "Hakon K. Gjessing [aut, cre],\n  Miriam Gjerdevik [ctb] (functions 'lineByLine', 'cbindFiles',\n    'rbindFiles', 'snpPower', 'snpSampleSize', 'hapSim', 'hapRun',\n    'hapPower', 'hapPowerAsymp', and 'hapRelEff'),\n  Julia Romanowska [ctb] (ORCID: <https://orcid.org/0000-0001-6733-1953>,\n    new data format, parallelisation, new documentation),\n  Oivind Skare [ctb] (TDT tests)",
    "url": "https://haplin.bitbucket.io",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Haplin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Haplin Analyzing Case-Parent Triad and/or Case-Control Data with SNP\nHaplotypes Performs genetic association analyses of case-parent triad (trio) data with multiple markers. It can also incorporate complete or incomplete control triads, for instance independent control children. Estimation is based on haplotypes, for instance SNP haplotypes, even though phase is not known from the genetic data. 'Haplin' estimates relative risk (RR + conf.int.) and p-value associated with each haplotype. It uses maximum likelihood estimation to make optimal use of data from triads with missing genotypic data, for instance if some SNPs has not been typed for some individuals. 'Haplin' also allows estimation of effects of maternal haplotypes and parent-of-origin effects, particularly appropriate in perinatal epidemiology. 'Haplin' allows special models, like X-inactivation, to be fitted on the X-chromosome. A GxE analysis allows testing interactions between environment and all estimated genetic effects. The models were originally described in \"Gjessing HK and Lie RT. Case-parent triads: Estimating single- and double-dose effects of fetal and maternal disease gene haplotypes. Annals of Human Genetics (2006) 70, pp. 382-396\".  "
  },
  {
    "id": 4015,
    "package_name": "HazardDiff",
    "title": "Conditional Treatment Effect for Competing Risks",
    "description": "The conditional treatment effect for competing risks data in observational studies is estimated. While it is described as a constant difference between the hazard functions given the covariates, we do not assume specific functional forms for the covariates. Rava, D. and Xu, R. (2021) <arXiv:2112.09535>.",
    "version": "0.1.0",
    "maintainer": "Denise Rava <drava@ucsd.edu>",
    "author": "Denise Rava [aut, cre, cph],\n  Ronghui Xu [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HazardDiff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HazardDiff Conditional Treatment Effect for Competing Risks The conditional treatment effect for competing risks data in observational studies is estimated. While it is described as a constant difference between the hazard functions given the covariates, we do not assume specific functional forms for the covariates. Rava, D. and Xu, R. (2021) <arXiv:2112.09535>.  "
  },
  {
    "id": 4033,
    "package_name": "HierPortfolios",
    "title": "Hierarchical Risk Clustering Portfolio Allocation Strategies",
    "description": "Machine learning hierarchical risk clustering portfolio allocation strategies. \n The implemented methods are:\n  Hierarchical risk parity (De Prado, 2016) <DOI: 10.3905/jpm.2016.42.4.059>.\n  Hierarchical clustering-based asset allocation (Raffinot, 2017)  \n  <DOI: 10.3905/jpm.2018.44.2.089>.\n  Hierarchical equal risk contribution portfolio (Raffinot, 2018)\n  <DOI: 10.2139/ssrn.3237540>.\n  A Constrained Hierarchical Risk Parity Algorithm with Cluster-based Capital Allocation (Pfitzingera and Katzke, 2019)\n  <https://www.ekon.sun.ac.za/wpapers/2019/wp142019/wp142019.pdf>.",
    "version": "1.0.2",
    "maintainer": "Carlos Trucios <ctrucios@unicamp.br>",
    "author": "Carlos Trucios [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8746-8877>),\n  Moon Jun Kwon [aut],\n  S\u00e3o Paulo Research Foundation (FAPESP), grant 2022/09122-0 [fnd],\n  Programa de Incentivo a Novos Docentes da UNICAMP (PIND), grant 2525/23\n    [fnd]",
    "url": "https://github.com/ctruciosm/HierPortfolios",
    "bug_reports": "https://github.com/ctruciosm/HierPortfolios/issues",
    "repository": "https://cran.r-project.org/package=HierPortfolios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HierPortfolios Hierarchical Risk Clustering Portfolio Allocation Strategies Machine learning hierarchical risk clustering portfolio allocation strategies. \n The implemented methods are:\n  Hierarchical risk parity (De Prado, 2016) <DOI: 10.3905/jpm.2016.42.4.059>.\n  Hierarchical clustering-based asset allocation (Raffinot, 2017)  \n  <DOI: 10.3905/jpm.2018.44.2.089>.\n  Hierarchical equal risk contribution portfolio (Raffinot, 2018)\n  <DOI: 10.2139/ssrn.3237540>.\n  A Constrained Hierarchical Risk Parity Algorithm with Cluster-based Capital Allocation (Pfitzingera and Katzke, 2019)\n  <https://www.ekon.sun.ac.za/wpapers/2019/wp142019/wp142019.pdf>.  "
  },
  {
    "id": 4055,
    "package_name": "HyRiM",
    "title": "Multicriteria Risk Management using Zero-Sum Games with\nVector-Valued Payoffs that are Probability Distributions",
    "description": "Construction and analysis of multivalued zero-sum matrix games over the abstract space of probability distributions, which describe the losses in each scenario of defense vs. attack action. The distributions can be compiled directly from expert opinions or other empirical data (insofar available). The package implements the methods put forth in the EU project HyRiM (Hybrid Risk Management for Utility Networks), FP7 EU Project Number 608090. The method has been published in Rass, S., K\u00f6nig, S., Schauer, S., 2016. Decisions with Uncertain Consequences-A Total Ordering on Loss-Distributions. PLoS ONE 11, e0168583. <doi:10.1371/journal.pone.0168583>, and applied for advanced persistent thread modeling in Rass, S., K\u00f6nig, S., Schauer, S., 2017. Defending Against Advanced Persistent Threats Using Game-Theory. PLoS ONE 12, e0168675. <doi:10.1371/journal.pone.0168675>. A volume covering the wider range of aspects of risk management, partially based on the theory implemented in the package is the book edited by S. Rass and S. Schauer, 2018. Game Theory for Security and Risk Management: From Theory to Practice. Springer, <doi:10.1007/978-3-319-75268-6>, ISBN 978-3-319-75267-9.",
    "version": "2.0.2",
    "maintainer": "\"Stefan Rass, on behalf of the Austrian Institute of Technology\" <stefan.rass@jku.at>",
    "author": "Stefan Rass, Sandra Koenig, Ali Alshawish",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HyRiM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HyRiM Multicriteria Risk Management using Zero-Sum Games with\nVector-Valued Payoffs that are Probability Distributions Construction and analysis of multivalued zero-sum matrix games over the abstract space of probability distributions, which describe the losses in each scenario of defense vs. attack action. The distributions can be compiled directly from expert opinions or other empirical data (insofar available). The package implements the methods put forth in the EU project HyRiM (Hybrid Risk Management for Utility Networks), FP7 EU Project Number 608090. The method has been published in Rass, S., K\u00f6nig, S., Schauer, S., 2016. Decisions with Uncertain Consequences-A Total Ordering on Loss-Distributions. PLoS ONE 11, e0168583. <doi:10.1371/journal.pone.0168583>, and applied for advanced persistent thread modeling in Rass, S., K\u00f6nig, S., Schauer, S., 2017. Defending Against Advanced Persistent Threats Using Game-Theory. PLoS ONE 12, e0168675. <doi:10.1371/journal.pone.0168675>. A volume covering the wider range of aspects of risk management, partially based on the theory implemented in the package is the book edited by S. Rass and S. Schauer, 2018. Game Theory for Security and Risk Management: From Theory to Practice. Springer, <doi:10.1007/978-3-319-75268-6>, ISBN 978-3-319-75267-9.  "
  },
  {
    "id": 4075,
    "package_name": "IBMPopSim",
    "title": "Individual Based Model Population Simulation",
    "description": "Simulation of the random evolution of heterogeneous populations using stochastic Individual-Based Models (IBMs) <doi:10.48550/arXiv.2303.06183>. \n    The package enables users to simulate population evolution, in which individuals are characterized by their age and some characteristics, and the population is modified by different types of events, including births/arrivals, death/exit events, or changes of characteristics. The frequency at which an event can occur to an individual can depend on their age and characteristics, but also on the characteristics of other individuals (interactions). \n    Such models have a wide range of applications. For instance, IBMs can be used for simulating the evolution of a heterogeneous insurance portfolio with selection or for validating  mortality forecasts. \n    This package overcomes the limitations of time-consuming IBMs simulations by implementing new efficient algorithms  based on thinning methods, which are compiled using the 'Rcpp' package while providing a user-friendly interface.",
    "version": "1.1.0",
    "maintainer": "Daphn\u00e9 Giorgi <daphne.giorgi@sorbonne-universite.fr>",
    "author": "Daphn\u00e9 Giorgi [aut, cre],\n  Sarah Kaakai [aut],\n  Vincent Lemaire [aut]",
    "url": "https://github.com/DaphneGiorgi/IBMPopSim,\nhttps://DaphneGiorgi.github.io/IBMPopSim/",
    "bug_reports": "https://github.com/DaphneGiorgi/IBMPopSim/issues",
    "repository": "https://cran.r-project.org/package=IBMPopSim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IBMPopSim Individual Based Model Population Simulation Simulation of the random evolution of heterogeneous populations using stochastic Individual-Based Models (IBMs) <doi:10.48550/arXiv.2303.06183>. \n    The package enables users to simulate population evolution, in which individuals are characterized by their age and some characteristics, and the population is modified by different types of events, including births/arrivals, death/exit events, or changes of characteristics. The frequency at which an event can occur to an individual can depend on their age and characteristics, but also on the characteristics of other individuals (interactions). \n    Such models have a wide range of applications. For instance, IBMs can be used for simulating the evolution of a heterogeneous insurance portfolio with selection or for validating  mortality forecasts. \n    This package overcomes the limitations of time-consuming IBMs simulations by implementing new efficient algorithms  based on thinning methods, which are compiled using the 'Rcpp' package while providing a user-friendly interface.  "
  },
  {
    "id": 4085,
    "package_name": "ICDS",
    "title": "Identification of Cancer Dysfunctional Subpathway with Omics\nData",
    "description": "Identify Cancer Dysfunctional Sub-pathway by integrating gene expression, DNA methylation and copy number variation, and pathway topological information. 1)We firstly calculate the gene risk scores by integrating three kinds of data: DNA methylation, copy number variation, and gene expression.  2)Secondly, we perform a greedy search algorithm to identify the key dysfunctional sub-pathways within the pathways for which the discriminative scores were locally maximal. 3)Finally, the permutation test was used to calculate statistical significance level for these key dysfunctional sub-pathways.",
    "version": "0.1.3",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [cre],\n  Baotong Zheng [aut],\n  Siyao Liu [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ICDS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICDS Identification of Cancer Dysfunctional Subpathway with Omics\nData Identify Cancer Dysfunctional Sub-pathway by integrating gene expression, DNA methylation and copy number variation, and pathway topological information. 1)We firstly calculate the gene risk scores by integrating three kinds of data: DNA methylation, copy number variation, and gene expression.  2)Secondly, we perform a greedy search algorithm to identify the key dysfunctional sub-pathways within the pathways for which the discriminative scores were locally maximal. 3)Finally, the permutation test was used to calculate statistical significance level for these key dysfunctional sub-pathways.  "
  },
  {
    "id": 4102,
    "package_name": "ICcalib",
    "title": "Cox Model with Interval-Censored Starting Time of a Covariate",
    "description": "Calibration and risk-set calibration methods for fitting Cox proportional hazard model when a binary covariate is measured intermittently. Methods include functions to fit calibration models from interval-censored data and modified partial likelihood for the proportional hazard model, Nevo et al. (2018+) <arXiv:1801.01529>.",
    "version": "1.0.8",
    "maintainer": "Daniel Nevo <danielnevo@gmail.com>",
    "author": "Daniel Nevo",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ICcalib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICcalib Cox Model with Interval-Censored Starting Time of a Covariate Calibration and risk-set calibration methods for fitting Cox proportional hazard model when a binary covariate is measured intermittently. Methods include functions to fit calibration models from interval-censored data and modified partial likelihood for the proportional hazard model, Nevo et al. (2018+) <arXiv:1801.01529>.  "
  },
  {
    "id": 4156,
    "package_name": "INFOSET",
    "title": "Computing a New Informative Distribution Set of Asset Returns",
    "description": "Estimation of the most-left informative set of gross returns \n             (i.e., the informative set).\n             The procedure to compute the informative set adjusts the method \n             proposed by \n             Mariani et al. (2022a) <doi:10.1007/s11205-020-02440-6> \n             and \n             Mariani et al. (2022b) <doi:10.1007/s10287-022-00422-2> \n             to gross returns of financial assets. \n             This is accomplished through an adaptive algorithm\n             that identifies sub-groups of gross returns in \n             each iteration by approximating their distribution with a\n             sequence of two-component log-normal mixtures. \n             These sub-groups emerge when a significant change\n             in the distribution occurs below the median of the \n             financial returns, with their boundary termed as\n             the \u201cchange point\" of the mixture. \n             The process concludes when no further change points are detected.\n             The outcome encompasses parameters of the leftmost mixture \n             distributions and change points of the\n             analyzed financial time series.\n             The functionalities of the INFOSET package include: (i) modelling asset distribution\n             detecting the parameters which describe left tail behaviour (infoset function), (ii) clustering, (iii) labeling of the financial\n             series for predictive and classification purposes through a Left Risk measure based on the first change point (LR_cp function)\n             (iv) portfolio construction (ptf_construction function).\n             The package also provide a specific function to construct rolling windows of different length size and overlapping time.",
    "version": "4.1",
    "maintainer": "Gloria Polinesi <g.polinesi@staff.univpm.it>",
    "author": "Gloria Polinesi [aut, cre],\n  Francesca Mariani [aut],\n  Maria Cristina Recchioni [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=INFOSET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "INFOSET Computing a New Informative Distribution Set of Asset Returns Estimation of the most-left informative set of gross returns \n             (i.e., the informative set).\n             The procedure to compute the informative set adjusts the method \n             proposed by \n             Mariani et al. (2022a) <doi:10.1007/s11205-020-02440-6> \n             and \n             Mariani et al. (2022b) <doi:10.1007/s10287-022-00422-2> \n             to gross returns of financial assets. \n             This is accomplished through an adaptive algorithm\n             that identifies sub-groups of gross returns in \n             each iteration by approximating their distribution with a\n             sequence of two-component log-normal mixtures. \n             These sub-groups emerge when a significant change\n             in the distribution occurs below the median of the \n             financial returns, with their boundary termed as\n             the \u201cchange point\" of the mixture. \n             The process concludes when no further change points are detected.\n             The outcome encompasses parameters of the leftmost mixture \n             distributions and change points of the\n             analyzed financial time series.\n             The functionalities of the INFOSET package include: (i) modelling asset distribution\n             detecting the parameters which describe left tail behaviour (infoset function), (ii) clustering, (iii) labeling of the financial\n             series for predictive and classification purposes through a Left Risk measure based on the first change point (LR_cp function)\n             (iv) portfolio construction (ptf_construction function).\n             The package also provide a specific function to construct rolling windows of different length size and overlapping time.  "
  },
  {
    "id": 4158,
    "package_name": "INLAjoint",
    "title": "Multivariate Joint Modeling for Longitudinal and Time-to-Event\nOutcomes with 'INLA'",
    "description": "Estimation of joint models for multivariate longitudinal markers (with various distributions available) and survival outcomes (possibly accounting for competing risks) with Integrated Nested Laplace Approximations (INLA). The flexible and user friendly function joint() facilitates the use of the fast and reliable inference technique implemented in the 'INLA' package for joint modeling. More details are given in the help page of the joint() function (accessible via ?joint in the R console) and the vignette associated to the joint() function (accessible via vignette(\"INLAjoint\") in the R console).",
    "version": "25.11.10",
    "maintainer": "Denis Rustand <INLAjoint@gmail.com>",
    "author": "Denis Rustand [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-9708-5220>),\n  Elias Teixeira Krainski [aut] (ORCID:\n    <https://orcid.org/0000-0002-7063-2615>),\n  Haavard Rue [aut] (ORCID: <https://orcid.org/0000-0002-0222-1881>),\n  Janet van Niekerk [aut] (ORCID:\n    <https://orcid.org/0000-0002-4334-2057>)",
    "url": "https://github.com/DenisRustand/INLAjoint",
    "bug_reports": "https://github.com/DenisRustand/INLAjoint/issues",
    "repository": "https://cran.r-project.org/package=INLAjoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "INLAjoint Multivariate Joint Modeling for Longitudinal and Time-to-Event\nOutcomes with 'INLA' Estimation of joint models for multivariate longitudinal markers (with various distributions available) and survival outcomes (possibly accounting for competing risks) with Integrated Nested Laplace Approximations (INLA). The flexible and user friendly function joint() facilitates the use of the fast and reliable inference technique implemented in the 'INLA' package for joint modeling. More details are given in the help page of the joint() function (accessible via ?joint in the R console) and the vignette associated to the joint() function (accessible via vignette(\"INLAjoint\") in the R console).  "
  },
  {
    "id": 4168,
    "package_name": "IPDfromKM",
    "title": "Map Digitized Survival Curves Back to Individual Patient Data",
    "description": "\n      An implementation to reconstruct individual patient data from Kaplan-Meier (K-M) survival curves, visualize and assess the accuracy of the reconstruction, then perform secondary analysis on the reconstructed data. We involve a simple function to extract the coordinates form the published K-M curves. The function is developed based on Poisot T. \u2019s digitize package (2011)  <doi:10.32614/RJ-2011-004> . For more complex and tangled together graphs, digitizing software, such as 'DigitizeIt' (for MAC or windows) or 'ScanIt'(for windows) can be used to get the coordinates. Additional information should also be involved to increase the accuracy, like numbers of patients at risk (often reported at 5-10 time points under the x-axis of the K-M graph), total number of patients, and total number of events. The package implements the modified iterative K-M estimation algorithm (modified-iKM) improved upon the approach proposed by Guyot (2012) <doi:10.1186/1471-2288-12-9> with some modifications. ",
    "version": "0.1.10",
    "maintainer": "Na Liu <nliu1104@gmail.com>",
    "author": "Na Liu [aut, cre],\n  J.Jack Lee [aut, ths],\n  Yanhong Zhou [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IPDfromKM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IPDfromKM Map Digitized Survival Curves Back to Individual Patient Data \n      An implementation to reconstruct individual patient data from Kaplan-Meier (K-M) survival curves, visualize and assess the accuracy of the reconstruction, then perform secondary analysis on the reconstructed data. We involve a simple function to extract the coordinates form the published K-M curves. The function is developed based on Poisot T. \u2019s digitize package (2011)  <doi:10.32614/RJ-2011-004> . For more complex and tangled together graphs, digitizing software, such as 'DigitizeIt' (for MAC or windows) or 'ScanIt'(for windows) can be used to get the coordinates. Additional information should also be involved to increase the accuracy, like numbers of patients at risk (often reported at 5-10 time points under the x-axis of the K-M graph), total number of patients, and total number of events. The package implements the modified iterative K-M estimation algorithm (modified-iKM) improved upon the approach proposed by Guyot (2012) <doi:10.1186/1471-2288-12-9> with some modifications.   "
  },
  {
    "id": 4170,
    "package_name": "IPEDS",
    "title": "Data from the Integrated Post-Secondary Education Data System",
    "description": "Contains data on Post-Secondary Institution Statistics in 2020 <https://nces.ed.gov/ipeds/use-the-data>. The package allows easy access to a wide variety of information regarding Post-secondary Institutions, its students, faculty, and their demographics, financial aid, educational and recreational offerings, and completions. This package can be used by students, college counselors, or involved parents interested in pursuing higher education, considering their options, and securing admission into their school of choice.",
    "version": "0.1.0",
    "maintainer": "Aushanae Haller <aushanaenhaller@gmail.com>",
    "author": "Aushanae Haller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2090-1952>),\n  Alejandra Munoz [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IPEDS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IPEDS Data from the Integrated Post-Secondary Education Data System Contains data on Post-Secondary Institution Statistics in 2020 <https://nces.ed.gov/ipeds/use-the-data>. The package allows easy access to a wide variety of information regarding Post-secondary Institutions, its students, faculty, and their demographics, financial aid, educational and recreational offerings, and completions. This package can be used by students, college counselors, or involved parents interested in pursuing higher education, considering their options, and securing admission into their school of choice.  "
  },
  {
    "id": 4247,
    "package_name": "InfoTrad",
    "title": "Calculates the Probability of Informed Trading (PIN)",
    "description": "Estimates the probability of informed trading (PIN) initially introduced by Easley et. al. (1996) <doi:10.1111/j.1540-6261.1996.tb04074.x> . Contribution of the package is that it uses likelihood factorizations of Easley et. al. (2010) <doi:10.1017/S0022109010000074> (EHO factorization) and Lin and Ke (2011) <doi:10.1016/j.finmar.2011.03.001> (LK factorization). Moreover, the package uses different estimation algorithms. Specifically, the grid-search algorithm proposed by Yan and Zhang (2012) <doi:10.1016/j.jbankfin.2011.08.003> , hierarchical agglomerative clustering approach proposed by Gan et. al. (2015) <doi:10.1080/14697688.2015.1023336> and later extended by Ersan and Alici (2016) <doi:10.1016/j.intfin.2016.04.001> .",
    "version": "1.2",
    "maintainer": "Murat Tinic <tinic@bilkent.edu.tr>",
    "author": "Duygu Celik and Murat Tinic",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=InfoTrad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "InfoTrad Calculates the Probability of Informed Trading (PIN) Estimates the probability of informed trading (PIN) initially introduced by Easley et. al. (1996) <doi:10.1111/j.1540-6261.1996.tb04074.x> . Contribution of the package is that it uses likelihood factorizations of Easley et. al. (2010) <doi:10.1017/S0022109010000074> (EHO factorization) and Lin and Ke (2011) <doi:10.1016/j.finmar.2011.03.001> (LK factorization). Moreover, the package uses different estimation algorithms. Specifically, the grid-search algorithm proposed by Yan and Zhang (2012) <doi:10.1016/j.jbankfin.2011.08.003> , hierarchical agglomerative clustering approach proposed by Gan et. al. (2015) <doi:10.1080/14697688.2015.1023336> and later extended by Ersan and Alici (2016) <doi:10.1016/j.intfin.2016.04.001> .  "
  },
  {
    "id": 4249,
    "package_name": "InformativeCensoring",
    "title": "Multiple Imputation for Informative Censoring",
    "description": "Multiple Imputation for Informative Censoring.\n    This package implements two methods. Gamma Imputation\n    described in <DOI:10.1002/sim.6274> and Risk Score Imputation\n    described in <DOI:10.1002/sim.3480>.",
    "version": "0.3.6",
    "maintainer": "Jonathan Bartlett <jonathan.bartlett1@lshtm.ac.uk>",
    "author": "David Ruau [aut],\n  Nikolas Burkoff [aut],\n  Jonathan Bartlett [aut, cre],\n  Dan Jackson [aut],\n  Edmund Jones [aut],\n  Martin Law [aut],\n  Paul Metcalfe [aut]",
    "url": "https://github.com/jwb133/InformativeCensoring",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=InformativeCensoring",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "InformativeCensoring Multiple Imputation for Informative Censoring Multiple Imputation for Informative Censoring.\n    This package implements two methods. Gamma Imputation\n    described in <DOI:10.1002/sim.6274> and Risk Score Imputation\n    described in <DOI:10.1002/sim.3480>.  "
  },
  {
    "id": 4297,
    "package_name": "JFE",
    "title": "Tools for Analyzing Time Series Data of Just Finance and\nEconometrics",
    "description": "Offer procedures to download financial-economic time series data and enhanced procedures for computing the investment performance indices of Bacon (2004) <DOI:10.1002/9781119206309>.",
    "version": "2.5.11",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "author": "Ho Tsung-wu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JFE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JFE Tools for Analyzing Time Series Data of Just Finance and\nEconometrics Offer procedures to download financial-economic time series data and enhanced procedures for computing the investment performance indices of Bacon (2004) <DOI:10.1002/9781119206309>.  "
  },
  {
    "id": 4303,
    "package_name": "JMH",
    "title": "Joint Model of Heterogeneous Repeated Measures and Survival Data",
    "description": "Maximum likelihood estimation for the semi-parametric joint modeling of competing risks and longitudinal data in the presence of heterogeneous within-subject variability, proposed by Li and colleagues (2023) <arXiv:2301.06584>.\n             The proposed method models the within-subject variability of the biomarker and associates it with the risk of the competing risks event. The time-to-event data is modeled using a (cause-specific) Cox proportional hazards regression model with time-fixed covariates. \n             The longitudinal outcome is modeled using a mixed-effects location and scale model. The association is captured by shared random effects. The model \n             is estimated using an Expectation Maximization algorithm.",
    "version": "1.0.3",
    "maintainer": "Shanpeng Li <lishanpeng0913@ucla.edu>",
    "author": "Shanpeng Li [aut, cre],\n  Jin Zhou [ctb],\n  Hua Zhou [ctb],\n  Gang Li [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JMH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JMH Joint Model of Heterogeneous Repeated Measures and Survival Data Maximum likelihood estimation for the semi-parametric joint modeling of competing risks and longitudinal data in the presence of heterogeneous within-subject variability, proposed by Li and colleagues (2023) <arXiv:2301.06584>.\n             The proposed method models the within-subject variability of the biomarker and associates it with the risk of the competing risks event. The time-to-event data is modeled using a (cause-specific) Cox proportional hazards regression model with time-fixed covariates. \n             The longitudinal outcome is modeled using a mixed-effects location and scale model. The association is captured by shared random effects. The model \n             is estimated using an Expectation Maximization algorithm.  "
  },
  {
    "id": 4306,
    "package_name": "JMbayes2",
    "title": "Extended Joint Models for Longitudinal and Time-to-Event Data",
    "description": "Fit joint models for longitudinal and time-to-event data under the Bayesian approach. Multiple longitudinal outcomes of mixed type (continuous/categorical) and multiple event times (competing risks and multi-state processes) are accommodated. Rizopoulos (2012, ISBN:9781439872864).",
    "version": "0.5-7",
    "maintainer": "Dimitris Rizopoulos <d.rizopoulos@erasmusmc.nl>",
    "author": "Dimitris Rizopoulos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9397-0900>),\n  Pedro Miranda Afonso [aut],\n  Grigorios Papageorgiou [aut]",
    "url": "https://drizopoulos.github.io/JMbayes2/,\nhttps://github.com/drizopoulos/JMbayes2",
    "bug_reports": "https://github.com/drizopoulos/JMbayes2/issues",
    "repository": "https://cran.r-project.org/package=JMbayes2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JMbayes2 Extended Joint Models for Longitudinal and Time-to-Event Data Fit joint models for longitudinal and time-to-event data under the Bayesian approach. Multiple longitudinal outcomes of mixed type (continuous/categorical) and multiple event times (competing risks and multi-state processes) are accommodated. Rizopoulos (2012, ISBN:9781439872864).  "
  },
  {
    "id": 4329,
    "package_name": "Jcvrisk",
    "title": "Risk Calculator for Cardiovascular Disease in Japan",
    "description": "A calculation tool to obtain the 5-year or 10-year risk of cardiovascular disease from various risk models.",
    "version": "0.1.3",
    "maintainer": "Hiroshi Okumiyama <xzxz2019@iCloud.com>",
    "author": "Hiroshi Okumiyama [aut, cre],\n  Ryosuke Fujii [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Jcvrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Jcvrisk Risk Calculator for Cardiovascular Disease in Japan A calculation tool to obtain the 5-year or 10-year risk of cardiovascular disease from various risk models.  "
  },
  {
    "id": 4330,
    "package_name": "Jdmbs",
    "title": "Monte Carlo Option Pricing Algorithms for Jump Diffusion Models\nwith Correlational Companies",
    "description": "Option is a one of the financial derivatives and its pricing is an important problem in practice. The process of stock prices are represented as Geometric Brownian motion [Black (1973) <doi:10.1086/260062>] or jump diffusion processes [Kou (2002) <doi:10.1287/mnsc.48.8.1086.166>]. In this package, algorithms and visualizations are implemented by Monte Carlo method in order to calculate European option price for three equations by Geometric Brownian motion and jump diffusion processes and furthermore a model that presents jumps among companies affect each other.",
    "version": "1.4",
    "maintainer": "Masashi Okada <okadaalgorithm@gmail.com>",
    "author": "Masashi Okada [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Jdmbs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Jdmbs Monte Carlo Option Pricing Algorithms for Jump Diffusion Models\nwith Correlational Companies Option is a one of the financial derivatives and its pricing is an important problem in practice. The process of stock prices are represented as Geometric Brownian motion [Black (1973) <doi:10.1086/260062>] or jump diffusion processes [Kou (2002) <doi:10.1287/mnsc.48.8.1086.166>]. In this package, algorithms and visualizations are implemented by Monte Carlo method in order to calculate European option price for three equations by Geometric Brownian motion and jump diffusion processes and furthermore a model that presents jumps among companies affect each other.  "
  },
  {
    "id": 4335,
    "package_name": "JointFPM",
    "title": "A Parametric Model for Estimating the Mean Number of Events",
    "description": "Implementation of a parametric joint model for modelling recurrent\n  and competing event processes using generalised survival models as described in\n  Entrop et al., (2025) <doi:10.1002/bimj.70038>. The joint  model can \n  subsequently be used to predict the mean number of events in the\n  presence of competing risks at different time points. Comparisons of the mean\n  number of event functions, e.g. the differences in mean number of events\n  between two exposure groups, are also available.",
    "version": "1.3.0",
    "maintainer": "Joshua P. Entrop <joshuaentrop@posteo.de>",
    "author": "Joshua P. Entrop [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1614-8096>),\n  Alessandro Gasparini [ctb],\n  Mark Clements [ctb]",
    "url": "https://github.com/entjos/JointFPM,\nhttps://entjos.github.io/JointFPM/",
    "bug_reports": "https://github.com/entjos/JointFPM/issues",
    "repository": "https://cran.r-project.org/package=JointFPM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JointFPM A Parametric Model for Estimating the Mean Number of Events Implementation of a parametric joint model for modelling recurrent\n  and competing event processes using generalised survival models as described in\n  Entrop et al., (2025) <doi:10.1002/bimj.70038>. The joint  model can \n  subsequently be used to predict the mean number of events in the\n  presence of competing risks at different time points. Comparisons of the mean\n  number of event functions, e.g. the differences in mean number of events\n  between two exposure groups, are also available.  "
  },
  {
    "id": 4358,
    "package_name": "KMunicate",
    "title": "KMunicate-Style Kaplan\u2013Meier Plots",
    "description": "Produce Kaplan\u2013Meier plots in the style recommended\n    following the KMunicate study by Morris et al. (2019)\n    <doi:10.1136/bmjopen-2019-030215>. The KMunicate style consists of\n    Kaplan-Meier curves with confidence intervals to quantify uncertainty\n    and an extended risk table (per treatment arm) depicting the number of\n    study subjects at risk, events, and censored observations over time.\n    The resulting plots are built using 'ggplot2' and can be further\n    customised to a certain extent, including themes, fonts, and colour\n    scales.",
    "version": "0.2.5",
    "maintainer": "Alessandro Gasparini <alessandro@ellessenne.xyz>",
    "author": "Alessandro Gasparini [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8319-7624>),\n  Ary Serpa Neto [ctb],\n  Peter Dutey [ctb] (ORCID: <https://orcid.org/0000-0002-8942-9836>)",
    "url": "https://ellessenne.github.io/KMunicate-package/,\nhttps://github.com/ellessenne/KMunicate-package",
    "bug_reports": "https://github.com/ellessenne/KMunicate-package/issues",
    "repository": "https://cran.r-project.org/package=KMunicate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KMunicate KMunicate-Style Kaplan\u2013Meier Plots Produce Kaplan\u2013Meier plots in the style recommended\n    following the KMunicate study by Morris et al. (2019)\n    <doi:10.1136/bmjopen-2019-030215>. The KMunicate style consists of\n    Kaplan-Meier curves with confidence intervals to quantify uncertainty\n    and an extended risk table (per treatment arm) depicting the number of\n    study subjects at risk, events, and censored observations over time.\n    The resulting plots are built using 'ggplot2' and can be further\n    customised to a certain extent, including themes, fonts, and colour\n    scales.  "
  },
  {
    "id": 4407,
    "package_name": "KrakenR",
    "title": "Comprehensive R Interface for Accessing Kraken Cryptocurrency\nExchange REST API",
    "description": "A comprehensive R interface to access data from the Kraken cryptocurrency exchange REST API <https://docs.kraken.com/api/>.\n  It allows users to retrieve various market data, such as asset information, trading pairs, and price data.\n  The package is designed to facilitate efficient data access for analysis, strategy development, and monitoring of cryptocurrency market trends.",
    "version": "1.0.0",
    "maintainer": "Nathana\u00ebl D\u00fcrst <nathanael.durst@unil.ch>",
    "author": "Nathana\u00ebl D\u00fcrst [aut, cre]",
    "url": "https://github.com/nathanael-g-durst/KrakenR",
    "bug_reports": "https://github.com/nathanael-g-durst/KrakenR/issues",
    "repository": "https://cran.r-project.org/package=KrakenR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KrakenR Comprehensive R Interface for Accessing Kraken Cryptocurrency\nExchange REST API A comprehensive R interface to access data from the Kraken cryptocurrency exchange REST API <https://docs.kraken.com/api/>.\n  It allows users to retrieve various market data, such as asset information, trading pairs, and price data.\n  The package is designed to facilitate efficient data access for analysis, strategy development, and monitoring of cryptocurrency market trends.  "
  },
  {
    "id": 4408,
    "package_name": "KraljicMatrix",
    "title": "A Quantified Implementation of the Kraljic Matrix",
    "description": "Implements a quantified approach to the Kraljic Matrix (Kraljic, 1983, <https://hbr.org/1983/09/purchasing-must-become-supply-management>)\n    for strategically analyzing a firm\u2019s purchasing portfolio. It combines multi-objective decision analysis to measure purchasing characteristics and\n    uses this information to place products and services within the Kraljic Matrix.",
    "version": "0.2.1",
    "maintainer": "Bradley Boehmke <bradleyboehmke@gmail.com>",
    "author": "Bradley Boehmke [aut, cre],\n  Brandon Greenwell [aut],\n  Andrew McCarthy [aut],\n  Robert Montgomery [ctb]",
    "url": "https://github.com/koalaverse/KraljicMatrix",
    "bug_reports": "https://github.com/koalaverse/KraljicMatrix/issues",
    "repository": "https://cran.r-project.org/package=KraljicMatrix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KraljicMatrix A Quantified Implementation of the Kraljic Matrix Implements a quantified approach to the Kraljic Matrix (Kraljic, 1983, <https://hbr.org/1983/09/purchasing-must-become-supply-management>)\n    for strategically analyzing a firm\u2019s purchasing portfolio. It combines multi-objective decision analysis to measure purchasing characteristics and\n    uses this information to place products and services within the Kraljic Matrix.  "
  },
  {
    "id": 4424,
    "package_name": "LARisk",
    "title": "Estimation of Lifetime Attributable Risk of Cancer from\nRadiation Exposure",
    "description": "Compute lifetime attributable risk of \n    radiation-induced cancer reveals that it can be helpful with \n    enhancement of the flexibility in research with fast calculation \n    and various options. Important reference papers include\n    Berrington de Gonzalez et al. (2012) <doi:10.1088/0952-4746/32/3/205>,\n    National Research Council (2006, ISBN:978-0-309-09156-5).",
    "version": "1.0.0",
    "maintainer": "Juhee Lee <ljh988488@gmail.com>",
    "author": "Juhee Lee [aut, cre],\n  Young-Min Kim [aut],\n  YeongWoo Park [aut],\n  Eunjin Jang [aut],\n  Jinkyung Yoo [aut],\n  Songwon Seo [ctb],\n  Eun-Shil Cha [ctb],\n  Won-Jin Lee [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LARisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LARisk Estimation of Lifetime Attributable Risk of Cancer from\nRadiation Exposure Compute lifetime attributable risk of \n    radiation-induced cancer reveals that it can be helpful with \n    enhancement of the flexibility in research with fast calculation \n    and various options. Important reference papers include\n    Berrington de Gonzalez et al. (2012) <doi:10.1088/0952-4746/32/3/205>,\n    National Research Council (2006, ISBN:978-0-309-09156-5).  "
  },
  {
    "id": 4513,
    "package_name": "LSMRealOptions",
    "title": "Value American and Real Options Through LSM Simulation",
    "description": "The least-squares Monte Carlo (LSM) simulation method is a popular method for the approximation of the value of early and multiple exercise options. 'LSMRealOptions' provides implementations of the LSM simulation method to value American option products and capital investment projects through real options analysis. 'LSMRealOptions' values capital investment projects with cash flows dependent upon underlying state variables that are stochastically evolving, providing analysis into the timing and critical values at which investment is optimal. 'LSMRealOptions' provides flexibility in the stochastic processes followed by underlying assets, the number of state variables, basis functions and underlying asset characteristics to allow a broad range of assets to be valued through the LSM simulation method. Real options projects are further able to be valued whilst considering construction periods, time-varying initial capital expenditures and path-dependent operational flexibility including the ability to temporarily shutdown or permanently abandon projects after initial investment has occurred. The LSM simulation method was first presented in the prolific work of Longstaff and Schwartz (2001) <doi:10.1093/rfs/14.1.113>.",
    "version": "0.2.1",
    "maintainer": "Thomas Aspinall <tomaspinall2512@gmail.com>",
    "author": "Thomas Aspinall [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6968-1989>),\n  Adrian Gepp [aut] (ORCID: <https://orcid.org/0000-0003-1666-5501>),\n  Geoff Harris [aut] (ORCID: <https://orcid.org/0000-0003-4284-8619>),\n  Simone Kelly [aut] (ORCID: <https://orcid.org/0000-0002-6528-8557>),\n  Colette Southam [aut] (ORCID: <https://orcid.org/0000-0001-7263-2347>),\n  Bruce Vanstone [aut] (ORCID: <https://orcid.org/0000-0002-3977-2468>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LSMRealOptions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSMRealOptions Value American and Real Options Through LSM Simulation The least-squares Monte Carlo (LSM) simulation method is a popular method for the approximation of the value of early and multiple exercise options. 'LSMRealOptions' provides implementations of the LSM simulation method to value American option products and capital investment projects through real options analysis. 'LSMRealOptions' values capital investment projects with cash flows dependent upon underlying state variables that are stochastically evolving, providing analysis into the timing and critical values at which investment is optimal. 'LSMRealOptions' provides flexibility in the stochastic processes followed by underlying assets, the number of state variables, basis functions and underlying asset characteristics to allow a broad range of assets to be valued through the LSM simulation method. Real options projects are further able to be valued whilst considering construction periods, time-varying initial capital expenditures and path-dependent operational flexibility including the ability to temporarily shutdown or permanently abandon projects after initial investment has occurred. The LSM simulation method was first presented in the prolific work of Longstaff and Schwartz (2001) <doi:10.1093/rfs/14.1.113>.  "
  },
  {
    "id": 4526,
    "package_name": "LTFGRS",
    "title": "Implementation of Several Phenotype-Based Family Genetic Risk\nScores",
    "description": "Implementation of several phenotype-based family genetic risk scores \n  with unified input data and data preparation functions to help facilitate\n  the required data preparation and management. The implemented family genetic\n  risk scores are the extended liability threshold model conditional on family history\n  from Pedersen (2022) <doi:10.1016/j.ajhg.2022.01.009> and Pedersen (2023)  <https://www.nature.com/articles/s41467-023-41210-z>,\n  Pearson-Aitken Family Genetic Risk Scores from Krebs (2024) <doi:10.1016/j.ajhg.2024.09.009>,\n  and family genetic risk score from Kendler (2021) <doi:10.1001/jamapsychiatry.2021.0336>.",
    "version": "1.0.1",
    "maintainer": "Emil Michael Pedersen <emp@ph.au.dk>",
    "author": "Emil Michael Pedersen [aut, cre],\n  Jette Steinbach [aut],\n  Lucas Rasmussen [ctb],\n  Morten Dybdahl Krebs [aut]",
    "url": "https://emilmip.github.io/LTFGRS/",
    "bug_reports": "https://github.com/EmilMiP/LTFGRS/issues",
    "repository": "https://cran.r-project.org/package=LTFGRS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LTFGRS Implementation of Several Phenotype-Based Family Genetic Risk\nScores Implementation of several phenotype-based family genetic risk scores \n  with unified input data and data preparation functions to help facilitate\n  the required data preparation and management. The implemented family genetic\n  risk scores are the extended liability threshold model conditional on family history\n  from Pedersen (2022) <doi:10.1016/j.ajhg.2022.01.009> and Pedersen (2023)  <https://www.nature.com/articles/s41467-023-41210-z>,\n  Pearson-Aitken Family Genetic Risk Scores from Krebs (2024) <doi:10.1016/j.ajhg.2024.09.009>,\n  and family genetic risk score from Kendler (2021) <doi:10.1001/jamapsychiatry.2021.0336>.  "
  },
  {
    "id": 4545,
    "package_name": "Landmarking",
    "title": "Analysis using Landmark Models",
    "description": "The landmark approach allows survival predictions to be\n\tupdated dynamically as new measurements from an individual are recorded.\n\tThe idea is to set predefined time points, known as \"landmark times\",\n\tand form a model at each landmark time using only the individuals in the\n\trisk set. This package allows the longitudinal data to be modelled\n\teither using the last observation carried forward or linear mixed\n\teffects modelling. There is also the option to model competing risks,\n\teither through cause-specific Cox regression or Fine-Gray regression.\n\tTo find out more about the methods in this package, please see \n\t<https://isobelbarrott.github.io/Landmarking/articles/Landmarking>.",
    "version": "1.0.0",
    "maintainer": "Isobel Barrott <isobel.barrott@gmail.com>",
    "author": "Isobel Barrott [aut, cre],\n  Jessica Barrett [aut],\n  Ruth Keogh [ctb],\n  Michael Sweeting [ctb],\n  David Stevens [ctb]",
    "url": "https://github.com/isobelbarrott/Landmarking/",
    "bug_reports": "https://github.com/isobelbarrott/Landmarking/issues",
    "repository": "https://cran.r-project.org/package=Landmarking",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Landmarking Analysis using Landmark Models The landmark approach allows survival predictions to be\n\tupdated dynamically as new measurements from an individual are recorded.\n\tThe idea is to set predefined time points, known as \"landmark times\",\n\tand form a model at each landmark time using only the individuals in the\n\trisk set. This package allows the longitudinal data to be modelled\n\teither using the last observation carried forward or linear mixed\n\teffects modelling. There is also the option to model competing risks,\n\teither through cause-specific Cox regression or Fine-Gray regression.\n\tTo find out more about the methods in this package, please see \n\t<https://isobelbarrott.github.io/Landmarking/articles/Landmarking>.  "
  },
  {
    "id": 4553,
    "package_name": "Latamverse",
    "title": "Latin American Data via 'RESTful' APIs and Curated Datasets",
    "description": "Brings together a comprehensive collection \n    of R packages providing access to API functions and curated datasets from Argentina, Brazil, \n    Chile, Colombia, and Peru. Includes real-time and historical data through public \n    'RESTful' APIs ('Nager.Date', World Bank API, REST Countries API, and country-specific APIs) and \n    extensive curated collections of open datasets covering economics, demographics, public health, \n    environmental data, political indicators, social metrics, and cultural information. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to Latin American data sources, facilitating \n    reproducible research, comparative analysis, and teaching applications focused \n    on these five major Latin American countries.\n    Included packages:\n    - 'ArgentinAPI': API functions and curated datasets for Argentina covering exchange rates, inflation, political figures, national holidays and more.\n    - 'BrazilDataAPI': API functions and curated datasets for Brazil covering postal codes, banks, economic indicators, holidays, company registrations and more.\n    - 'ChileDataAPI': API functions and curated datasets for Chile covering financial indicators ('UF', UTM, Dollar, Euro, Yen, Copper, Bitcoin, 'IPSA' index), holidays and more.\n    - 'ColombiAPI': API functions and curated datasets for Colombia covering geographic locations, cultural attractions, economic indicators, demographic data, national holidays and more.\n    - 'PeruAPIs': API functions and curated datasets for Peru covering economic indicators, demographics, national holidays, administrative divisions, electoral data, biodiversity and more.\n    For more information on the APIs, see: \n    'Nager.Date' <https://date.nager.at/Api>, \n    World Bank API <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    REST Countries API <https://restcountries.com/>,\n    'ArgentinaDatos' API <https://argentinadatos.com/>,\n    'BrasilAPI' <https://brasilapi.com.br/>,\n    'FINDIC' <https://findic.cl/>,\n    and API-Colombia <https://api-colombia.com/>.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/latamverse,\nhttps://lightbluetitan.github.io/latamverse/",
    "bug_reports": "https://github.com/lightbluetitan/latamverse/issues",
    "repository": "https://cran.r-project.org/package=Latamverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Latamverse Latin American Data via 'RESTful' APIs and Curated Datasets Brings together a comprehensive collection \n    of R packages providing access to API functions and curated datasets from Argentina, Brazil, \n    Chile, Colombia, and Peru. Includes real-time and historical data through public \n    'RESTful' APIs ('Nager.Date', World Bank API, REST Countries API, and country-specific APIs) and \n    extensive curated collections of open datasets covering economics, demographics, public health, \n    environmental data, political indicators, social metrics, and cultural information. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to Latin American data sources, facilitating \n    reproducible research, comparative analysis, and teaching applications focused \n    on these five major Latin American countries.\n    Included packages:\n    - 'ArgentinAPI': API functions and curated datasets for Argentina covering exchange rates, inflation, political figures, national holidays and more.\n    - 'BrazilDataAPI': API functions and curated datasets for Brazil covering postal codes, banks, economic indicators, holidays, company registrations and more.\n    - 'ChileDataAPI': API functions and curated datasets for Chile covering financial indicators ('UF', UTM, Dollar, Euro, Yen, Copper, Bitcoin, 'IPSA' index), holidays and more.\n    - 'ColombiAPI': API functions and curated datasets for Colombia covering geographic locations, cultural attractions, economic indicators, demographic data, national holidays and more.\n    - 'PeruAPIs': API functions and curated datasets for Peru covering economic indicators, demographics, national holidays, administrative divisions, electoral data, biodiversity and more.\n    For more information on the APIs, see: \n    'Nager.Date' <https://date.nager.at/Api>, \n    World Bank API <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392>, \n    REST Countries API <https://restcountries.com/>,\n    'ArgentinaDatos' API <https://argentinadatos.com/>,\n    'BrasilAPI' <https://brasilapi.com.br/>,\n    'FINDIC' <https://findic.cl/>,\n    and API-Colombia <https://api-colombia.com/>.  "
  },
  {
    "id": 4579,
    "package_name": "LifeInsureR",
    "title": "Modelling Traditional Life Insurance Contracts",
    "description": "R6 classes to model traditional life insurance\n    contracts like annuities, whole life insurances or endowments. Such life\n    insurance contracts provide a guaranteed interest and are not directly linked\n    to the performance of a particular investment vehicle, but they typically\n    provide (discretionary) profit participation. This package provides a framework\n    to model such contracts in a very generic (cash-flow-based) way and includes\n    modelling profit participation schemes, dynamic increases or more general\n    contract layers, as well as contract changes (like sum increases or premium\n    waivers). All relevant quantities like premium decomposition, reserves and \n    benefits over the whole contract period are calculated and potentially \n    exported to 'Excel'. Mortality rates are given using the 'MortalityTables' package.",
    "version": "1.0.1",
    "maintainer": "Reinhold Kainhofer <reinhold@kainhofer.com>",
    "author": "Reinhold Kainhofer [aut, cre, cph]",
    "url": "https://gitlab.open-tools.net/R/LifeInsureR,\nhttps://github.com/kainhofer/LifeInsureR",
    "bug_reports": "https://gitlab.open-tools.net/R/LifeInsureR/-/issues",
    "repository": "https://cran.r-project.org/package=LifeInsureR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LifeInsureR Modelling Traditional Life Insurance Contracts R6 classes to model traditional life insurance\n    contracts like annuities, whole life insurances or endowments. Such life\n    insurance contracts provide a guaranteed interest and are not directly linked\n    to the performance of a particular investment vehicle, but they typically\n    provide (discretionary) profit participation. This package provides a framework\n    to model such contracts in a very generic (cash-flow-based) way and includes\n    modelling profit participation schemes, dynamic increases or more general\n    contract layers, as well as contract changes (like sum increases or premium\n    waivers). All relevant quantities like premium decomposition, reserves and \n    benefits over the whole contract period are calculated and potentially \n    exported to 'Excel'. Mortality rates are given using the 'MortalityTables' package.  "
  },
  {
    "id": 4608,
    "package_name": "LocalControl",
    "title": "Nonparametric Methods for Generating High Quality Comparative\nEffectiveness Evidence",
    "description": "Implements novel nonparametric approaches to address\n    biases and confounding when comparing treatments or exposures in\n    observational studies of outcomes. While designed and appropriate for use\n    in studies involving medicine and the life sciences, the package can be\n    used in other situations involving outcomes with multiple confounders.\n    The package implements a family of methods for non-parametric bias correction\n    when comparing treatments in observational studies, including survival\n    analysis settings, where competing risks and/or censoring may be present.\n    The approach extends to bias-corrected personalized predictions of treatment\n    outcome differences, and analysis of heterogeneity of treatment effect-sizes\n    across patient subgroups. For further details, please see: \n    Lauve NR, Nelson SJ, Young SS, Obenchain RL, Lambert CG. LocalControl:\n    An R Package for Comparative Safety and Effectiveness Research.\n    Journal of Statistical Software. 2020. p. 1\u201332. Available from <doi:10.18637/jss.v096.i04>.",
    "version": "1.1.4",
    "maintainer": "Christophe G. Lambert <cglambert@salud.unm.edu>",
    "author": "Nicolas R. Lauve [aut] (ORCID: <https://orcid.org/0000-0002-9348-0319>),\n  Stuart J. Nelson [aut] (ORCID: <https://orcid.org/0000-0002-8756-0179>),\n  S. Stanley Young [aut] (ORCID: <https://orcid.org/0000-0001-9449-5478>),\n  Robert L. Obenchain [aut] (ORCID:\n    <https://orcid.org/0000-0002-8395-1666>),\n  Melania Pintilie [ctb],\n  Martin Kutz [ctb],\n  Christophe G. Lambert [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1994-2893>)",
    "url": "https://github.com/OHDSI/LocalControl",
    "bug_reports": "https://github.com/OHDSI/LocalControl/issues",
    "repository": "https://cran.r-project.org/package=LocalControl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LocalControl Nonparametric Methods for Generating High Quality Comparative\nEffectiveness Evidence Implements novel nonparametric approaches to address\n    biases and confounding when comparing treatments or exposures in\n    observational studies of outcomes. While designed and appropriate for use\n    in studies involving medicine and the life sciences, the package can be\n    used in other situations involving outcomes with multiple confounders.\n    The package implements a family of methods for non-parametric bias correction\n    when comparing treatments in observational studies, including survival\n    analysis settings, where competing risks and/or censoring may be present.\n    The approach extends to bias-corrected personalized predictions of treatment\n    outcome differences, and analysis of heterogeneity of treatment effect-sizes\n    across patient subgroups. For further details, please see: \n    Lauve NR, Nelson SJ, Young SS, Obenchain RL, Lambert CG. LocalControl:\n    An R Package for Comparative Safety and Effectiveness Research.\n    Journal of Statistical Software. 2020. p. 1\u201332. Available from <doi:10.18637/jss.v096.i04>.  "
  },
  {
    "id": 4617,
    "package_name": "LogisticEnsembles",
    "title": "Automatically Runs 24 Logistic Models (Individual and Ensembles)",
    "description": "Automatically returns 24 logistic models including 13 individual models and 11 ensembles of models of logistic data. The package also returns 25 plots, 5 tables, and a summary report. The package automatically\n    builds all 24 models, reports all results, and provides graphics to show how the models performed. This can be used for a wide range of data, such as sports or medical data. The package includes medical data (the Pima Indians data set), and\n    information about the performance of Lebron James. The package can be used to analyze many other examples, such as stock market data. The package automatically returns many values for each model, such as\n    True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, Positive Predictive Value, Negative Predictive Value, F1 Score, Area Under the Curve. The package also returns 36 Receiver\n    Operating Characteristic (ROC) curves for each of the 24 models.",
    "version": "0.8.2",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "https://github.com/InfiniteCuriosity/LogisticEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/LogisticEnsembles/issues",
    "repository": "https://cran.r-project.org/package=LogisticEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LogisticEnsembles Automatically Runs 24 Logistic Models (Individual and Ensembles) Automatically returns 24 logistic models including 13 individual models and 11 ensembles of models of logistic data. The package also returns 25 plots, 5 tables, and a summary report. The package automatically\n    builds all 24 models, reports all results, and provides graphics to show how the models performed. This can be used for a wide range of data, such as sports or medical data. The package includes medical data (the Pima Indians data set), and\n    information about the performance of Lebron James. The package can be used to analyze many other examples, such as stock market data. The package automatically returns many values for each model, such as\n    True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, Positive Predictive Value, Negative Predictive Value, F1 Score, Area Under the Curve. The package also returns 36 Receiver\n    Operating Characteristic (ROC) curves for each of the 24 models.  "
  },
  {
    "id": 4621,
    "package_name": "LongDecompHE",
    "title": "Longitudinal Decomposition of Health Expectancy by Age and Cause",
    "description": "Provides tools to decompose differences in cohort health expectancy (HE)\n    by age and cause using longitudinal data. The package implements a novel\n    longitudinal attribution method based on a semiparametric additive hazards\n    model with time-dependent covariates, specifically designed to address interval\n    censoring and semi-competing risks via a copula framework. The resulting\n    age-cause-specific contributions to disability prevalence and death probability\n    can be used to quantify and decompose differences in cohort HE between groups.\n    The package supports stepwise replacement decomposition algorithms and is\n    applicable to cohort-based health disparity research across diverse populations.\n    Related methods include\n    Sun et al. (2023) <doi:10.1177/09622802221133552>.",
    "version": "0.1.0",
    "maintainer": "Huiping Zheng <zhenghuiping@ruc.edu.cn>",
    "author": "Huiping Zheng [aut, cre],\n  Tao Sun [aut],\n  Xiaojun Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LongDecompHE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LongDecompHE Longitudinal Decomposition of Health Expectancy by Age and Cause Provides tools to decompose differences in cohort health expectancy (HE)\n    by age and cause using longitudinal data. The package implements a novel\n    longitudinal attribution method based on a semiparametric additive hazards\n    model with time-dependent covariates, specifically designed to address interval\n    censoring and semi-competing risks via a copula framework. The resulting\n    age-cause-specific contributions to disability prevalence and death probability\n    can be used to quantify and decompose differences in cohort HE between groups.\n    The package supports stepwise replacement decomposition algorithms and is\n    applicable to cohort-based health disparity research across diverse populations.\n    Related methods include\n    Sun et al. (2023) <doi:10.1177/09622802221133552>.  "
  },
  {
    "id": 4676,
    "package_name": "MBESS",
    "title": "The MBESS R Package",
    "description": "Implements methods that are useful in designing research studies and analyzing data, with \n\tparticular emphasis on methods that are developed for or used within the behavioral, \n\teducational, and social sciences (broadly defined). That being said, many of the methods \n\timplemented within MBESS are applicable to a wide variety of disciplines. MBESS has a \n\tsuite of functions for a variety of related topics, such as effect sizes, confidence intervals \n\tfor effect sizes (including standardized effect sizes and noncentral effect sizes), sample size\n\tplanning (from the accuracy in parameter estimation [AIPE], power analytic, equivalence, and \n\tminimum-risk point estimation perspectives), mediation analysis, various properties of \n\tdistributions, and a variety of utility functions. MBESS (pronounced 'em-bes') was originally \n\tan acronym for 'Methods for the Behavioral, Educational, and Social Sciences,' but MBESS became\n\tmore general and now contains methods applicable and used in a wide variety of fields and is an \n\torphan acronym, in the sense that what was an acronym is now literally its name. MBESS has \n\tgreatly benefited from others, see <https://www3.nd.edu/~kkelley/r-packages.html> for a detailed \n\tlist of those that have contributed and other details.",
    "version": "4.9.41",
    "maintainer": "Ken Kelley <kkelley@nd.edu>",
    "author": "Ken Kelley [aut, cre]",
    "url": "https://www3.nd.edu/~kkelley/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MBESS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MBESS The MBESS R Package Implements methods that are useful in designing research studies and analyzing data, with \n\tparticular emphasis on methods that are developed for or used within the behavioral, \n\teducational, and social sciences (broadly defined). That being said, many of the methods \n\timplemented within MBESS are applicable to a wide variety of disciplines. MBESS has a \n\tsuite of functions for a variety of related topics, such as effect sizes, confidence intervals \n\tfor effect sizes (including standardized effect sizes and noncentral effect sizes), sample size\n\tplanning (from the accuracy in parameter estimation [AIPE], power analytic, equivalence, and \n\tminimum-risk point estimation perspectives), mediation analysis, various properties of \n\tdistributions, and a variety of utility functions. MBESS (pronounced 'em-bes') was originally \n\tan acronym for 'Methods for the Behavioral, Educational, and Social Sciences,' but MBESS became\n\tmore general and now contains methods applicable and used in a wide variety of fields and is an \n\torphan acronym, in the sense that what was an acronym is now literally its name. MBESS has \n\tgreatly benefited from others, see <https://www3.nd.edu/~kkelley/r-packages.html> for a detailed \n\tlist of those that have contributed and other details.  "
  },
  {
    "id": 4700,
    "package_name": "MCPAN",
    "title": "Multiple Comparisons Using Normal Approximation",
    "description": "Multiple contrast tests and simultaneous confidence\n intervals based on normal approximation. With implementations for\n binomial proportions in a 2xk setting (risk difference and odds ratio),\n poly-3-adjusted tumour rates, biodiversity indices (multinomial data) \n and expected values under lognormal assumption. Approximative power \n calculation for multiple contrast tests of binomial and Gaussian data.",
    "version": "1.1-21",
    "maintainer": "Frank Schaarschmidt <schaarschmidt@biostat.uni-hannover.de>",
    "author": "Frank Schaarschmidt [aut, cre],\n  Daniel Gerhard [aut],\n  Martin Sill [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MCPAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MCPAN Multiple Comparisons Using Normal Approximation Multiple contrast tests and simultaneous confidence\n intervals based on normal approximation. With implementations for\n binomial proportions in a 2xk setting (risk difference and odds ratio),\n poly-3-adjusted tumour rates, biodiversity indices (multinomial data) \n and expected values under lognormal assumption. Approximative power \n calculation for multiple contrast tests of binomial and Gaussian data.  "
  },
  {
    "id": 4753,
    "package_name": "MGPSDK",
    "title": "Interact with the Maxar 'MGP' Application Programming Interfaces",
    "description": "Provides an interface to the Maxar Geospatial Platform (MGP) Application Programming Interface. <https://www.maxar.com/maxar-geospatial-platform>\n    It facilitates imagery searches using the MGP Streaming Application Programming Interface via the Web Feature Service (WFS) method, and supports image downloads through Web Map Service (WMS) and Web Map Tile Service (WMTS)\n    Open Geospatial Consortium (OGC) methods. \n    Additionally, it integrates with the Maxar Geospatial Platform Basemaps Application Programming Interface for accessing Maxar basemaps imagery and seamlines. \n    The package also offers seamless integration with the Maxar Geospatial Platform Discovery Application Programming Interface, allowing users to search, filter, and sort Maxar content, \n    while retrieving detailed metadata in formats like SpatioTemporal Asset Catalog (STAC) and GeoJSON.",
    "version": "1.0.0",
    "maintainer": "Nathan Carr <nathan.carr@maxar.com>",
    "author": "Nathan Carr [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MGPSDK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MGPSDK Interact with the Maxar 'MGP' Application Programming Interfaces Provides an interface to the Maxar Geospatial Platform (MGP) Application Programming Interface. <https://www.maxar.com/maxar-geospatial-platform>\n    It facilitates imagery searches using the MGP Streaming Application Programming Interface via the Web Feature Service (WFS) method, and supports image downloads through Web Map Service (WMS) and Web Map Tile Service (WMTS)\n    Open Geospatial Consortium (OGC) methods. \n    Additionally, it integrates with the Maxar Geospatial Platform Basemaps Application Programming Interface for accessing Maxar basemaps imagery and seamlines. \n    The package also offers seamless integration with the Maxar Geospatial Platform Discovery Application Programming Interface, allowing users to search, filter, and sort Maxar content, \n    while retrieving detailed metadata in formats like SpatioTemporal Asset Catalog (STAC) and GeoJSON.  "
  },
  {
    "id": 4779,
    "package_name": "MKclass",
    "title": "Statistical Classification",
    "description": "Performance measures and scores for statistical classification such as accuracy, sensitivity, specificity, recall, similarity coefficients, AUC, GINI index, Brier score and many more. Calculation of optimal cut-offs and decision stumps (Iba and Langley (1991), <doi:10.1016/B978-1-55860-247-2.50035-8>) for all implemented performance measures. Hosmer-Lemeshow goodness of fit tests (Lemeshow and Hosmer (1982), <doi:10.1093/oxfordjournals.aje.a113284>; Hosmer et al (1997), <doi:10.1002/(SICI)1097-0258(19970515)16:9%3C965::AID-SIM509%3E3.0.CO;2-O>). Statistical and epidemiological risk measures such as relative risk, odds ratio, number needed to treat (Porta (2014), <doi:10.1093%2Facref%2F9780199976720.001.0001>).",
    "version": "0.5",
    "maintainer": "Matthias Kohl <Matthias.Kohl@stamats.de>",
    "author": "Matthias Kohl [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9514-8910>)",
    "url": "https://github.com/stamats/MKclass",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MKclass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MKclass Statistical Classification Performance measures and scores for statistical classification such as accuracy, sensitivity, specificity, recall, similarity coefficients, AUC, GINI index, Brier score and many more. Calculation of optimal cut-offs and decision stumps (Iba and Langley (1991), <doi:10.1016/B978-1-55860-247-2.50035-8>) for all implemented performance measures. Hosmer-Lemeshow goodness of fit tests (Lemeshow and Hosmer (1982), <doi:10.1093/oxfordjournals.aje.a113284>; Hosmer et al (1997), <doi:10.1002/(SICI)1097-0258(19970515)16:9%3C965::AID-SIM509%3E3.0.CO;2-O>). Statistical and epidemiological risk measures such as relative risk, odds ratio, number needed to treat (Porta (2014), <doi:10.1093%2Facref%2F9780199976720.001.0001>).  "
  },
  {
    "id": 4781,
    "package_name": "MKendall",
    "title": "Matrix Kendall's Tau and Matrix Elliptical Factor Model",
    "description": "Large-scale matrix-variate data have been widely observed nowadays in various research areas such as finance, signal processing and medical imaging. Modelling matrix-valued data by matrix-elliptical family not only provides a flexible way to handle heavy-tail property and tail dependencies, but also maintains the intrinsic row and column structure of random matrices. We proposed a new tool named matrix Kendall's tau which is efficient for analyzing random elliptical matrices. By applying this new type of Kendell\u2019s tau to the matrix elliptical factor model, we propose a Matrix-type Robust Two-Step (MRTS) method to estimate the loading and factor spaces. See the details in He at al. (2022) <arXiv:2207.09633>. In this package, we provide the algorithms for calculating sample matrix Kendall's tau, the MRTS method and the Matrix Kendall's tau Eigenvalue-Ratio (MKER) method which is used for determining the number of factors.",
    "version": "1.5-4",
    "maintainer": "Yalin Wang <wangyalin@mail.sdu.edu.cn>",
    "author": "Yong He [aut],\n  Yalin Wang [aut, cre],\n  Long Yu [aut],\n  Wang Zhou [aut],\n  Wenxin Zhou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MKendall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MKendall Matrix Kendall's Tau and Matrix Elliptical Factor Model Large-scale matrix-variate data have been widely observed nowadays in various research areas such as finance, signal processing and medical imaging. Modelling matrix-valued data by matrix-elliptical family not only provides a flexible way to handle heavy-tail property and tail dependencies, but also maintains the intrinsic row and column structure of random matrices. We proposed a new tool named matrix Kendall's tau which is efficient for analyzing random elliptical matrices. By applying this new type of Kendell\u2019s tau to the matrix elliptical factor model, we propose a Matrix-type Robust Two-Step (MRTS) method to estimate the loading and factor spaces. See the details in He at al. (2022) <arXiv:2207.09633>. In this package, we provide the algorithms for calculating sample matrix Kendall's tau, the MRTS method and the Matrix Kendall's tau Eigenvalue-Ratio (MKER) method which is used for determining the number of factors.  "
  },
  {
    "id": 4795,
    "package_name": "MLEcens",
    "title": "Computation of the MLE for Bivariate Interval Censored Data",
    "description": "We provide functions to compute the nonparametric \n  maximum likelihood estimator (MLE) for \n  the bivariate distribution of (X,Y), when \n  realizations of (X,Y) cannot be observed directly. \n  To be more precise, we consider the situation \n  where we observe a set of rectangles in R^2 that are known \n  to contain the unobservable realizations of (X,Y). We\n  compute the MLE based on such a set of rectangles. \n  The methods can also be used for univariate censored data (see data set\n  'cosmesis'), and for \n  censored data with competing risks (see data set 'menopause'). \n  We also provide functions to visualize the observed data and the MLE. ",
    "version": "0.1-7.1",
    "maintainer": "Marloes Maathuis <marloesmaathuis@gmail.com>",
    "author": "Marloes Maathuis [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MLEcens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MLEcens Computation of the MLE for Bivariate Interval Censored Data We provide functions to compute the nonparametric \n  maximum likelihood estimator (MLE) for \n  the bivariate distribution of (X,Y), when \n  realizations of (X,Y) cannot be observed directly. \n  To be more precise, we consider the situation \n  where we observe a set of rectangles in R^2 that are known \n  to contain the unobservable realizations of (X,Y). We\n  compute the MLE based on such a set of rectangles. \n  The methods can also be used for univariate censored data (see data set\n  'cosmesis'), and for \n  censored data with competing risks (see data set 'menopause'). \n  We also provide functions to visualize the observed data and the MLE.   "
  },
  {
    "id": 4913,
    "package_name": "MTS",
    "title": "All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models",
    "description": "Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model. ",
    "version": "1.2.1",
    "maintainer": "Ruey S. Tsay <ruey.tsay@chicagobooth.edu>",
    "author": "Ruey S. Tsay [aut, cre], David Wood [aut], Jon Lachmann [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MTS All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model.   "
  },
  {
    "id": 4916,
    "package_name": "MUACz",
    "title": "Generate MUAC and BMI z-Scores and Percentiles for Children and\nAdolescents",
    "description": "Generates mid upper arm circumference (MUAC) and body mass index (BMI) \n            for age z-scores and percentiles based on LMS method for children and \n            adolescents up to 19 years that can be used to assess nutritional and health status and\n            define risk of adverse health events. ",
    "version": "2.1.0",
    "maintainer": "Lazarus Mramba <lmramba@gmail.com>",
    "author": "Lazarus Mramba [aut, cre],\n  James Berkley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MUACz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MUACz Generate MUAC and BMI z-Scores and Percentiles for Children and\nAdolescents Generates mid upper arm circumference (MUAC) and body mass index (BMI) \n            for age z-scores and percentiles based on LMS method for children and \n            adolescents up to 19 years that can be used to assess nutritional and health status and\n            define risk of adverse health events.   "
  },
  {
    "id": 4918,
    "package_name": "MUS",
    "title": "Monetary Unit Sampling and Estimation Methods, Widely Used in\nAuditing",
    "description": "Sampling and evaluation methods to apply Monetary Unit Sampling (or in older literature Dollar Unit Sampling) during an audit of financial statements.",
    "version": "0.1.6",
    "maintainer": "Henning Pr\u00f6mpers <henning@proempers.net>",
    "author": "Henning Pr\u00f6mpers, Andr\u00e9 Guimar\u00e3es",
    "url": "",
    "bug_reports": "https://github.com/alsguimaraes/MUS",
    "repository": "https://cran.r-project.org/package=MUS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MUS Monetary Unit Sampling and Estimation Methods, Widely Used in\nAuditing Sampling and evaluation methods to apply Monetary Unit Sampling (or in older literature Dollar Unit Sampling) during an audit of financial statements.  "
  },
  {
    "id": 4950,
    "package_name": "Mangrove",
    "title": "Risk Prediction on Trees",
    "description": "Methods for performing genetic risk\n        prediction from genotype data.  You can use it to perform risk\n        prediction for individuals, or for families with missing data.",
    "version": "1.21",
    "maintainer": "Luke Jostins <luke.jostins@kennedy.ox.ac.uk>",
    "author": "Luke Jostins",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Mangrove",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Mangrove Risk Prediction on Trees Methods for performing genetic risk\n        prediction from genotype data.  You can use it to perform risk\n        prediction for individuals, or for families with missing data.  "
  },
  {
    "id": 4955,
    "package_name": "ManyIVsNets",
    "title": "Environmental Phillips Curve Analysis with Multiple Instrumental\nVariables and Networks",
    "description": "Comprehensive toolkit for Environmental Phillips Curve analysis \n    featuring multidimensional instrumental variable creation, transfer entropy \n    causal discovery, network analysis, and state-of-the-art econometric methods.\n    Implements geographic, technological, migration, geopolitical, financial, \n    and natural risk instruments with robust diagnostics and visualization.\n    Provides 24 different instrumental variable approaches with empirical validation.\n    Methods based on Phillips (1958) <doi:10.1111/j.1468-0335.1958.tb00003.x>,\n    transfer entropy by Schreiber (2000) <doi:10.1103/PhysRevLett.85.461>, and \n    weak instrument tests by Stock and Yogo (2005) <doi:10.1017/CBO9780511614491.006>.",
    "version": "0.1.1",
    "maintainer": "Avishek Bhandari <bavisek@gmail.com>",
    "author": "Avishek Bhandari [aut, cre, cph]",
    "url": "https://github.com/avishekb9/ManyIVsNets,\nhttps://avishekb9.github.io/ManyIVsNets/",
    "bug_reports": "https://github.com/avishekb9/ManyIVsNets/issues",
    "repository": "https://cran.r-project.org/package=ManyIVsNets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ManyIVsNets Environmental Phillips Curve Analysis with Multiple Instrumental\nVariables and Networks Comprehensive toolkit for Environmental Phillips Curve analysis \n    featuring multidimensional instrumental variable creation, transfer entropy \n    causal discovery, network analysis, and state-of-the-art econometric methods.\n    Implements geographic, technological, migration, geopolitical, financial, \n    and natural risk instruments with robust diagnostics and visualization.\n    Provides 24 different instrumental variable approaches with empirical validation.\n    Methods based on Phillips (1958) <doi:10.1111/j.1468-0335.1958.tb00003.x>,\n    transfer entropy by Schreiber (2000) <doi:10.1103/PhysRevLett.85.461>, and \n    weak instrument tests by Stock and Yogo (2005) <doi:10.1017/CBO9780511614491.006>.  "
  },
  {
    "id": 4968,
    "package_name": "MarkowitzR",
    "title": "Statistical Significance of the Markowitz Portfolio",
    "description": "A collection of tools for analyzing significance of \n    Markowitz portfolios, using the delta method on the second moment\n    matrix, <arxiv:1312.0557>.",
    "version": "1.0.3",
    "maintainer": "Steven E. Pav <shabbychef@gmail.com>",
    "author": "Steven E. Pav [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4197-6195>)",
    "url": "https://github.com/shabbychef/MarkowitzR",
    "bug_reports": "https://github.com/shabbychef/MarkowitzR/issues",
    "repository": "https://cran.r-project.org/package=MarkowitzR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MarkowitzR Statistical Significance of the Markowitz Portfolio A collection of tools for analyzing significance of \n    Markowitz portfolios, using the delta method on the second moment\n    matrix, <arxiv:1312.0557>.  "
  },
  {
    "id": 5019,
    "package_name": "MetaLandSim",
    "title": "Landscape and Range Expansion Simulation",
    "description": "Tools to generate random landscape graphs, evaluate species\n    occurrence in dynamic landscapes, simulate future landscape occupation and\n    evaluate range expansion when new empty patches are available (e.g. as a\n    result of climate change). References: Mestre, F., Canovas, F., Pita, R., \n    Mira, A., Beja, P. (2016) <doi:10.1016/j.envsoft.2016.03.007>; Mestre, F., \n    Risk, B., Mira, A., Beja, P., Pita, R. (2017) \n    <doi:10.1016/j.ecolmodel.2017.06.013>; Mestre, F., Pita, R., Mira, A., Beja,\n    P. (2020) <doi:10.1186/s12898-019-0273-5>.",
    "version": "2.0.0",
    "maintainer": "Frederico Mestre <mestre.frederico@gmail.com>",
    "author": "Frederico Mestre, Fernando Canovas, Benjamin Risk, Ricardo Pita, \n\tAntonio Mira, Pedro Beja.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MetaLandSim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MetaLandSim Landscape and Range Expansion Simulation Tools to generate random landscape graphs, evaluate species\n    occurrence in dynamic landscapes, simulate future landscape occupation and\n    evaluate range expansion when new empty patches are available (e.g. as a\n    result of climate change). References: Mestre, F., Canovas, F., Pita, R., \n    Mira, A., Beja, P. (2016) <doi:10.1016/j.envsoft.2016.03.007>; Mestre, F., \n    Risk, B., Mira, A., Beja, P., Pita, R. (2017) \n    <doi:10.1016/j.ecolmodel.2017.06.013>; Mestre, F., Pita, R., Mira, A., Beja,\n    P. (2020) <doi:10.1186/s12898-019-0273-5>.  "
  },
  {
    "id": 5029,
    "package_name": "MetabolicSurv",
    "title": "A Biomarker Validation Approach for Classification and\nPredicting Survival Using Metabolomics Signature",
    "description": "An approach to identifies metabolic biomarker signature for metabolic data by discovering predictive metabolite for predicting survival and classifying patients into risk groups. \n Classifiers are constructed as a linear combination of predictive/important metabolites, prognostic factors and treatment effects if necessary. \n Several methods were implemented to reduce the metabolomics matrix such as the  principle component analysis of Wold Svante et al. (1987) <doi:10.1016/0169-7439(87)80084-9> , \n the LASSO method by Robert Tibshirani (1998) <doi:10.1002/(SICI)1097-0258(19970228)16:4%3C385::AID-SIM380%3E3.0.CO;2-3>, the \n elastic net approach by Hui Zou and Trevor Hastie (2005) <doi:10.1111/j.1467-9868.2005.00503.x>. \n Sensitivity analysis on the quantile used for the classification can also be accessed to check the deviation of the classification group based on the quantile specified. \n Large scale cross validation can be performed in  order to investigate the mostly selected predictive metabolites and for internal validation. During the evaluation process, validation is accessed using the hazard ratios (HR) distribution of the test set and inference is mainly based on resampling and permutations technique.",
    "version": "1.1.2",
    "maintainer": "Olajumoke Evangelina Owokotomo <olajumoke.owokotomo@uhasselt.be>",
    "author": "Olajumoke Evangelina Owokotomo [aut, cre],\n  Ziv Shkedy [ctb]",
    "url": "https://github.com/OlajumokeEvangelina/MetabolicSurv",
    "bug_reports": "https://github.com/OlajumokeEvangelina/MetabolicSurv/issues/new",
    "repository": "https://cran.r-project.org/package=MetabolicSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MetabolicSurv A Biomarker Validation Approach for Classification and\nPredicting Survival Using Metabolomics Signature An approach to identifies metabolic biomarker signature for metabolic data by discovering predictive metabolite for predicting survival and classifying patients into risk groups. \n Classifiers are constructed as a linear combination of predictive/important metabolites, prognostic factors and treatment effects if necessary. \n Several methods were implemented to reduce the metabolomics matrix such as the  principle component analysis of Wold Svante et al. (1987) <doi:10.1016/0169-7439(87)80084-9> , \n the LASSO method by Robert Tibshirani (1998) <doi:10.1002/(SICI)1097-0258(19970228)16:4%3C385::AID-SIM380%3E3.0.CO;2-3>, the \n elastic net approach by Hui Zou and Trevor Hastie (2005) <doi:10.1111/j.1467-9868.2005.00503.x>. \n Sensitivity analysis on the quantile used for the classification can also be accessed to check the deviation of the classification group based on the quantile specified. \n Large scale cross validation can be performed in  order to investigate the mostly selected predictive metabolites and for internal validation. During the evaluation process, validation is accessed using the hazard ratios (HR) distribution of the test set and inference is mainly based on resampling and permutations technique.  "
  },
  {
    "id": 5046,
    "package_name": "MiRSEA",
    "title": "'MicroRNA' Set Enrichment Analysis",
    "description": "The tools for 'MicroRNA Set Enrichment Analysis' can identify risk pathways(or prior gene sets) regulated by microRNA set in the context of microRNA expression data. (1) This package constructs a correlation profile of microRNA and pathways by the hypergeometric statistic test. The gene sets of pathways derived from the three public databases (Kyoto Encyclopedia of Genes and Genomes ('KEGG'); 'Reactome'; 'Biocarta') and the target gene sets of microRNA are provided by four databases('TarBaseV6.0'; 'mir2Disease'; 'miRecords'; 'miRTarBase';). (2) This package can quantify the change of correlation between microRNA for each pathway(or prior gene set) based on a microRNA expression data with cases and controls. (3) This package uses the weighted Kolmogorov-Smirnov statistic to calculate an enrichment score (ES) of a microRNA set that co-regulate to a pathway , which reflects the degree to which a given pathway is associated with the specific phenotype. (4) This package can provide the visualization of the results.",
    "version": "1.1.1",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han, Siyao Liu ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MiRSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MiRSEA 'MicroRNA' Set Enrichment Analysis The tools for 'MicroRNA Set Enrichment Analysis' can identify risk pathways(or prior gene sets) regulated by microRNA set in the context of microRNA expression data. (1) This package constructs a correlation profile of microRNA and pathways by the hypergeometric statistic test. The gene sets of pathways derived from the three public databases (Kyoto Encyclopedia of Genes and Genomes ('KEGG'); 'Reactome'; 'Biocarta') and the target gene sets of microRNA are provided by four databases('TarBaseV6.0'; 'mir2Disease'; 'miRecords'; 'miRTarBase';). (2) This package can quantify the change of correlation between microRNA for each pathway(or prior gene set) based on a microRNA expression data with cases and controls. (3) This package uses the weighted Kolmogorov-Smirnov statistic to calculate an enrichment score (ES) of a microRNA set that co-regulate to a pathway , which reflects the degree to which a given pathway is associated with the specific phenotype. (4) This package can provide the visualization of the results.  "
  },
  {
    "id": 5056,
    "package_name": "MicrobiomeSurv",
    "title": "Biomarker Validation for Microbiome-Based Survival\nClassification and Prediction",
    "description": "An approach to identify microbiome biomarker for time to event data by discovering microbiome for predicting survival and classifying subjects into risk groups.\n Classifiers are constructed as a linear combination of important microbiome and treatment effects if necessary. \n Several methods were implemented to estimate the microbiome risk score such as the LASSO method by Robert Tibshirani (1998) <doi:10.1002/(SICI)1097-0258(19970228)16:4%3C385::AID-SIM380%3E3.0.CO;2-3>, Elastic net approach by Hui Zou and Trevor Hastie (2005) <doi:10.1111/j.1467-9868.2005.00503.x>, supervised principle component analysis of Wold Svante et al. (1987) <doi:10.1016/0169-7439(87)80084-9>, and supervised partial least squares analysis by Inge S. Helland <https://www.jstor.org/stable/4616159>.\n Sensitivity analysis on the quantile used for the classification can also be accessed to check the deviation of the classification group based on the quantile specified. Large scale cross validation can be performed in order to investigate the mostly selected microbiome and for internal validation.\n During the evaluation process, validation is accessed using the hazard ratios (HR) distribution of the test set and inference is mainly based on resampling and permutations technique.",
    "version": "0.1.0",
    "maintainer": "Thi Huyen Nguyen <thihuyen.nguyen@uhasselt.be>",
    "author": "Thi Huyen Nguyen [aut, cre],\n  Olajumoke Evangelina Owokotomo [aut],\n  Ziv Shkedy [aut]",
    "url": "https://github.com/N-T-Huyen/MicrobiomeSurv",
    "bug_reports": "https://github.com/N-T-Huyen/MicrobiomeSurv/issues/new",
    "repository": "https://cran.r-project.org/package=MicrobiomeSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MicrobiomeSurv Biomarker Validation for Microbiome-Based Survival\nClassification and Prediction An approach to identify microbiome biomarker for time to event data by discovering microbiome for predicting survival and classifying subjects into risk groups.\n Classifiers are constructed as a linear combination of important microbiome and treatment effects if necessary. \n Several methods were implemented to estimate the microbiome risk score such as the LASSO method by Robert Tibshirani (1998) <doi:10.1002/(SICI)1097-0258(19970228)16:4%3C385::AID-SIM380%3E3.0.CO;2-3>, Elastic net approach by Hui Zou and Trevor Hastie (2005) <doi:10.1111/j.1467-9868.2005.00503.x>, supervised principle component analysis of Wold Svante et al. (1987) <doi:10.1016/0169-7439(87)80084-9>, and supervised partial least squares analysis by Inge S. Helland <https://www.jstor.org/stable/4616159>.\n Sensitivity analysis on the quantile used for the classification can also be accessed to check the deviation of the classification group based on the quantile specified. Large scale cross validation can be performed in order to investigate the mostly selected microbiome and for internal validation.\n During the evaluation process, validation is accessed using the hazard ratios (HR) distribution of the test set and inference is mainly based on resampling and permutations technique.  "
  },
  {
    "id": 5145,
    "package_name": "MultiATSM",
    "title": "Multicountry Term Structure of Interest Rates Models",
    "description": "Package for estimating, analyzing, and forecasting multi-country macro-finance affine term structure models (ATSMs). All setups build on the single-country unspanned macroeconomic risk framework from Joslin, Priebsch, and Singleton (2014, JF) <doi:10.1111/jofi.12131>. Multicountry extensions by Jotikasthira, Le, and Lundblad (2015, JFE) <doi:10.1016/j.jfineco.2014.09.004>, Candelon and Moura (2023, EM) <doi:10.1016/j.econmod.2023.106453>, and Candelon and Moura (2024, JFEC) <doi:10.1093/jjfinec/nbae008> are also available. The package also provides tools for bias correction as in Bauer Rudebusch and Wu (2012, JBES) <doi:10.1080/07350015.2012.693855>, bootstrap analysis, and several graphical/numerical outputs. ",
    "version": "1.5.1",
    "maintainer": "Rubens Moura <rubens.gtmoura@gmail.com>",
    "author": "Rubens Moura [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8105-4729>)",
    "url": "https://github.com/rubensmoura87/MultiATSM,\nhttps://rubensmoura87.github.io/MultiATSM/",
    "bug_reports": "https://github.com/rubensmoura87/MultiATSM/issues",
    "repository": "https://cran.r-project.org/package=MultiATSM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiATSM Multicountry Term Structure of Interest Rates Models Package for estimating, analyzing, and forecasting multi-country macro-finance affine term structure models (ATSMs). All setups build on the single-country unspanned macroeconomic risk framework from Joslin, Priebsch, and Singleton (2014, JF) <doi:10.1111/jofi.12131>. Multicountry extensions by Jotikasthira, Le, and Lundblad (2015, JFE) <doi:10.1016/j.jfineco.2014.09.004>, Candelon and Moura (2023, EM) <doi:10.1016/j.econmod.2023.106453>, and Candelon and Moura (2024, JFEC) <doi:10.1093/jjfinec/nbae008> are also available. The package also provides tools for bias correction as in Bauer Rudebusch and Wu (2012, JBES) <doi:10.1080/07350015.2012.693855>, bootstrap analysis, and several graphical/numerical outputs.   "
  },
  {
    "id": 5216,
    "package_name": "NFCP",
    "title": "N-Factor Commodity Pricing Through Term Structure Estimation",
    "description": "Commodity pricing models are (systems of) stochastic differential equations that are utilized for the valuation and hedging of commodity contingent claims (i.e. derivative products on the commodity) and other commodity related investments. Commodity pricing models that capture market dynamics are of great importance to commodity market participants in order to exercise sound investment and risk-management strategies. Parameters of commodity pricing models are estimated through maximum likelihood estimation, using available term structure futures data of a commodity. 'NFCP' (n-factor commodity pricing) provides a framework for the modeling, parameter estimation, probabilistic forecasting, option valuation and simulation of commodity prices through state space and Monte Carlo methods, risk-neutral valuation and Kalman filtering. 'NFCP' allows the commodity pricing model to consist of n correlated factors, with both random walk and mean-reverting elements. The n-factor commodity pricing model framework was first presented in the work of Cortazar and Naranjo (2006) <doi:10.1002/fut.20198>. Examples presented in 'NFCP' replicate the two-factor crude oil commodity pricing model presented in the prolific work of Schwartz and Smith (2000) <doi:10.1287/mnsc.46.7.893.12034> with the approximate term structure futures data applied within this study provided in the 'NFCP' package.",
    "version": "1.2.2",
    "maintainer": "Thomas Aspinall <tomaspinall2512@gmail.com>",
    "author": "Thomas Aspinall [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6968-1989>),\n  Adrian Gepp [aut] (ORCID: <https://orcid.org/0000-0003-1666-5501>),\n  Geoff Harris [aut] (ORCID: <https://orcid.org/0000-0003-4284-8619>),\n  Simone Kelly [aut] (ORCID: <https://orcid.org/0000-0002-6528-8557>),\n  Colette Southam [aut] (ORCID: <https://orcid.org/0000-0001-7263-2347>),\n  Bruce Vanstone [aut] (ORCID: <https://orcid.org/0000-0002-3977-2468>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NFCP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NFCP N-Factor Commodity Pricing Through Term Structure Estimation Commodity pricing models are (systems of) stochastic differential equations that are utilized for the valuation and hedging of commodity contingent claims (i.e. derivative products on the commodity) and other commodity related investments. Commodity pricing models that capture market dynamics are of great importance to commodity market participants in order to exercise sound investment and risk-management strategies. Parameters of commodity pricing models are estimated through maximum likelihood estimation, using available term structure futures data of a commodity. 'NFCP' (n-factor commodity pricing) provides a framework for the modeling, parameter estimation, probabilistic forecasting, option valuation and simulation of commodity prices through state space and Monte Carlo methods, risk-neutral valuation and Kalman filtering. 'NFCP' allows the commodity pricing model to consist of n correlated factors, with both random walk and mean-reverting elements. The n-factor commodity pricing model framework was first presented in the work of Cortazar and Naranjo (2006) <doi:10.1002/fut.20198>. Examples presented in 'NFCP' replicate the two-factor crude oil commodity pricing model presented in the prolific work of Schwartz and Smith (2000) <doi:10.1287/mnsc.46.7.893.12034> with the approximate term structure futures data applied within this study provided in the 'NFCP' package.  "
  },
  {
    "id": 5246,
    "package_name": "NMOF",
    "title": "Numerical Methods and Optimization in Finance",
    "description": "Functions, examples and data from the first and\n  the second edition of \"Numerical Methods and Optimization in\n  Finance\" by M. Gilli, D. Maringer and E. Schumann (2019,\n  ISBN:978-0128150658).  The package provides implementations\n  of optimisation heuristics (Differential Evolution, Genetic\n  Algorithms, Particle Swarm Optimisation, Simulated Annealing\n  and Threshold Accepting), and other optimisation tools, such\n  as grid search and greedy search.  There are also functions\n  for the valuation of financial instruments such as bonds and\n  options, for portfolio selection and functions that help\n  with stochastic simulations.",
    "version": "2.11-0",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/NMOF.htm , https://gitlab.com/NMOF ,\nhttps://git.sr.ht/~enricoschumann/NMOF ,\nhttps://github.com/enricoschumann/NMOF",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NMOF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NMOF Numerical Methods and Optimization in Finance Functions, examples and data from the first and\n  the second edition of \"Numerical Methods and Optimization in\n  Finance\" by M. Gilli, D. Maringer and E. Schumann (2019,\n  ISBN:978-0128150658).  The package provides implementations\n  of optimisation heuristics (Differential Evolution, Genetic\n  Algorithms, Particle Swarm Optimisation, Simulated Annealing\n  and Threshold Accepting), and other optimisation tools, such\n  as grid search and greedy search.  There are also functions\n  for the valuation of financial instruments such as bonds and\n  options, for portfolio selection and functions that help\n  with stochastic simulations.  "
  },
  {
    "id": 5252,
    "package_name": "NMsim",
    "title": "Seamless 'Nonmem' Simulation Platform",
    "description": "A complete and seamless 'Nonmem' simulation interface within R. Turns 'Nonmem' control streams into simulation control streams, executes them with specified simulation input data and returns the results. The simulation is performed by 'Nonmem', eliminating manual work and risks of re-implementation of models in other tools.",
    "version": "0.2.6",
    "maintainer": "Philip Delff <philip@delff.dk>",
    "author": "Philip Delff [aut, cre],\n  Brian Reilly [ctb],\n  Sanaya Shroff [ctb],\n  Boris Grinshpun [ctb]",
    "url": "https://nmautoverse.github.io/NMsim/",
    "bug_reports": "https://github.com/nmautoverse/NMsim/issues",
    "repository": "https://cran.r-project.org/package=NMsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NMsim Seamless 'Nonmem' Simulation Platform A complete and seamless 'Nonmem' simulation interface within R. Turns 'Nonmem' control streams into simulation control streams, executes them with specified simulation input data and returns the results. The simulation is performed by 'Nonmem', eliminating manual work and risks of re-implementation of models in other tools.  "
  },
  {
    "id": 5270,
    "package_name": "NPMLEcmprsk",
    "title": "Type-Specific Failure Rate and Hazard Rate on Competing Risks\nData",
    "description": "Given a failure type, the function computes covariate-specific probability of failure over time and covariate-specific conditional hazard rate based on possibly right-censored competing risk data. Specifically, it computes the non-parametric maximum-likelihood estimates of these quantities and their asymptotic variances in a semi-parametric mixture model for competing-risks data, as described in Chang et al. (2007a).",
    "version": "3.0",
    "maintainer": "Chung-Hsing Chen <chchen@nhri.org.tw>",
    "author": "Chung-Hsing Chen, I-Shou Chang and Chao A. Hsiung",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NPMLEcmprsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NPMLEcmprsk Type-Specific Failure Rate and Hazard Rate on Competing Risks\nData Given a failure type, the function computes covariate-specific probability of failure over time and covariate-specific conditional hazard rate based on possibly right-censored competing risk data. Specifically, it computes the non-parametric maximum-likelihood estimates of these quantities and their asymptotic variances in a semi-parametric mixture model for competing-risks data, as described in Chang et al. (2007a).  "
  },
  {
    "id": 5326,
    "package_name": "NetworkRiskMeasures",
    "title": "Risk Measures for (Financial) Networks",
    "description": "Implements some risk measures for (financial) networks, such as DebtRank, Impact Susceptibility, Impact Diffusion and Impact Fluidity. ",
    "version": "0.1.7",
    "maintainer": "Carlos Cinelli <carloscinelli@hotmail.com>",
    "author": "Carlos Cinelli [aut, cre],\n  Thiago Cristiano Silva [aut]",
    "url": "https://github.com/carloscinelli/NetworkRiskMeasures",
    "bug_reports": "https://github.com/carloscinelli/NetworkRiskMeasures/issues",
    "repository": "https://cran.r-project.org/package=NetworkRiskMeasures",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetworkRiskMeasures Risk Measures for (Financial) Networks Implements some risk measures for (financial) networks, such as DebtRank, Impact Susceptibility, Impact Diffusion and Impact Fluidity.   "
  },
  {
    "id": 5479,
    "package_name": "OptHedging",
    "title": "Estimation of value and hedging strategy of call and put\noptions",
    "description": "Estimation of value and hedging strategy of call and put options, based on optimal hedging and Monte Carlo method, from Chapter 3 of 'Statistical Methods for Financial Engineering', by Bruno Remillard, CRC Press, (2013).",
    "version": "1.0",
    "maintainer": "Bruno Remillard <bruno.remillard@hec.ca>",
    "author": "Bruno Remillard",
    "url": "http://www.r-project.org, http://www.brunoremillard.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OptHedging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OptHedging Estimation of value and hedging strategy of call and put\noptions Estimation of value and hedging strategy of call and put options, based on optimal hedging and Monte Carlo method, from Chapter 3 of 'Statistical Methods for Financial Engineering', by Bruno Remillard, CRC Press, (2013).  "
  },
  {
    "id": 5480,
    "package_name": "OptHoldoutSize",
    "title": "Estimation of Optimal Size for a Holdout Set for Updating a\nPredictive Score",
    "description": "Predictive scores must be updated with care, because actions taken on the basis of existing risk scores causes bias in risk estimates from the updated score. A holdout set is a straightforward way to manage this problem: a proportion of the population is 'held-out' from computation of the previous risk score. This package provides tools to estimate a size for this holdout set and associated errors. Comprehensive vignettes are included. Please see: Haidar-Wehbe S, Emerson SR, Aslett LJM, Liley J (2022) <doi:10.48550/arXiv.2202.06374> (to appear in Annals of Applied Statistics) for details of methods. ",
    "version": "0.1.0.1",
    "maintainer": "James Liley <james.liley@durham.ac.uk>",
    "author": "Sami Haidar-Wehbe [aut],\n  Sam Emerson [aut] (ORCID: <https://orcid.org/0000-0002-8379-2781>),\n  Louis Aslett [aut] (ORCID: <https://orcid.org/0000-0003-2211-233X>),\n  James Liley [cre, aut] (ORCID: <https://orcid.org/0000-0002-0049-8238>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OptHoldoutSize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OptHoldoutSize Estimation of Optimal Size for a Holdout Set for Updating a\nPredictive Score Predictive scores must be updated with care, because actions taken on the basis of existing risk scores causes bias in risk estimates from the updated score. A holdout set is a straightforward way to manage this problem: a proportion of the population is 'held-out' from computation of the previous risk score. This package provides tools to estimate a size for this holdout set and associated errors. Comprehensive vignettes are included. Please see: Haidar-Wehbe S, Emerson SR, Aslett LJM, Liley J (2022) <doi:10.48550/arXiv.2202.06374> (to appear in Annals of Applied Statistics) for details of methods.   "
  },
  {
    "id": 5525,
    "package_name": "PANPRSnext",
    "title": "Building PRS Models Based on Summary Statistics of GWAs",
    "description": "Shrinkage estimator for polygenic risk prediction (PRS) models based on summary statistics of genome-wide association (GWA) studies. Based upon the methods and original 'PANPRS' package as found in: Chen, Chatterjee, Landi, and Shi (2020) <doi:10.1080/01621459.2020.1764849>.",
    "version": "1.2.1",
    "maintainer": "Katherine Luo <hluo224@uwo.ca>",
    "author": "Katherine Luo [aut, cre],\n  Osvaldo Espin-Garcia [aut],\n  Ting-Huei Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PANPRSnext",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PANPRSnext Building PRS Models Based on Summary Statistics of GWAs Shrinkage estimator for polygenic risk prediction (PRS) models based on summary statistics of genome-wide association (GWA) studies. Based upon the methods and original 'PANPRS' package as found in: Chen, Chatterjee, Landi, and Shi (2020) <doi:10.1080/01621459.2020.1764849>.  "
  },
  {
    "id": 5542,
    "package_name": "PBtDesigns",
    "title": "Partially Balanced t-Designs (PBtDesigns)",
    "description": "The t-designs represent a generalized class of balanced incomplete block designs in which the number of blocks in which any t-tuple of treatments (t >= 2) occur together is a constant. When the focus of an experiment lies in grading and selecting treatment subgroups, t-designs would be preferred over the conventional ones, as they have the additional advantage of t-tuple balance. t-designs can be advantageously used in identifying the best crop-livestock combination for a particular location in Integrated Farming Systems that will help in generating maximum profit. But as the number of components increases, the number of possible t-component combinations will also increase. Most often, combinations derived from specific components are only practically feasible, for example, in a specific locality, farmers may not be interested in keeping a pig or goat and hence combinations involving these may not be of any use in that locality. In such situations partially balanced t-designs with few selected combinations appearing in a constant number of blocks (while others not at all appearing) may be useful (Sayantani Karmakar, Cini Varghese, Seema Jaggi & Mohd Harun (2021)<doi:10.1080/03610918.2021.2008436>). Further, every location may not have the resources to form equally sized homogeneous blocks. Partially balanced t-designs with unequal block sizes (Damaraju Raghavarao & Bei Zhou (1998)<doi:10.1080/03610929808832657>. Sayantani Karmakar, Cini Varghese, Seema Jaggi & Mohd Harun (2022).\" Partially Balanced t-designs with unequal block sizes\") prove to be more suitable for such situations.This package generates three series of partially balanced t-designs namely Series 1, Series 2 and Series 3. Series 1 and Series 2 are designs having equal block sizes and with treatment structures 4(t + 1) and a prime number, respectively. Series 3 consists of designs with unequal block sizes and with treatment structure n(n-1)/2. This package is based on the function named PBtD() for generating partially balanced t-designs along with their parameters, information matrices, average variance factors and canonical efficiency factors.",
    "version": "1.0.0",
    "maintainer": "Ashutosh Dalal <ashutosh.dalal97@gmail.com>",
    "author": "Sayantani Karmakar [aut, ctb],\n  Cini Varghese [aut, ctb],\n  Ashutosh Dalal [aut, cre],\n  Vinaykumar LN [aut, ctb],\n  Seema Jaggi [aut, ctb],\n  Mohd Harun [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PBtDesigns",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PBtDesigns Partially Balanced t-Designs (PBtDesigns) The t-designs represent a generalized class of balanced incomplete block designs in which the number of blocks in which any t-tuple of treatments (t >= 2) occur together is a constant. When the focus of an experiment lies in grading and selecting treatment subgroups, t-designs would be preferred over the conventional ones, as they have the additional advantage of t-tuple balance. t-designs can be advantageously used in identifying the best crop-livestock combination for a particular location in Integrated Farming Systems that will help in generating maximum profit. But as the number of components increases, the number of possible t-component combinations will also increase. Most often, combinations derived from specific components are only practically feasible, for example, in a specific locality, farmers may not be interested in keeping a pig or goat and hence combinations involving these may not be of any use in that locality. In such situations partially balanced t-designs with few selected combinations appearing in a constant number of blocks (while others not at all appearing) may be useful (Sayantani Karmakar, Cini Varghese, Seema Jaggi & Mohd Harun (2021)<doi:10.1080/03610918.2021.2008436>). Further, every location may not have the resources to form equally sized homogeneous blocks. Partially balanced t-designs with unequal block sizes (Damaraju Raghavarao & Bei Zhou (1998)<doi:10.1080/03610929808832657>. Sayantani Karmakar, Cini Varghese, Seema Jaggi & Mohd Harun (2022).\" Partially Balanced t-designs with unequal block sizes\") prove to be more suitable for such situations.This package generates three series of partially balanced t-designs namely Series 1, Series 2 and Series 3. Series 1 and Series 2 are designs having equal block sizes and with treatment structures 4(t + 1) and a prime number, respectively. Series 3 consists of designs with unequal block sizes and with treatment structure n(n-1)/2. This package is based on the function named PBtD() for generating partially balanced t-designs along with their parameters, information matrices, average variance factors and canonical efficiency factors.  "
  },
  {
    "id": 5554,
    "package_name": "PCLassoReg",
    "title": "Group Regression Models for Risk Protein Complex Identification",
    "description": "Two protein complex-based group regression models (PCLasso and PCLasso2) for risk protein complex identification. PCLasso is a prognostic model that identifies risk protein complexes associated with survival. PCLasso2 is a classification model that identifies risk protein complexes associated with classes. For more information, see Wang and Liu (2021) <doi:10.1093/bib/bbab212>. ",
    "version": "1.0.0",
    "maintainer": "Wei Liu <freelw@qq.com>",
    "author": "Wei Liu [aut, cre] (ORCID: <https://orcid.org/0000-0002-5496-3641>)",
    "url": "https://github.com/weiliu123/PCLassoReg",
    "bug_reports": "https://github.com/weiliu123/PCLassoReg/issues",
    "repository": "https://cran.r-project.org/package=PCLassoReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCLassoReg Group Regression Models for Risk Protein Complex Identification Two protein complex-based group regression models (PCLasso and PCLasso2) for risk protein complex identification. PCLasso is a prognostic model that identifies risk protein complexes associated with survival. PCLasso2 is a classification model that identifies risk protein complexes associated with classes. For more information, see Wang and Liu (2021) <doi:10.1093/bib/bbab212>.   "
  },
  {
    "id": 5560,
    "package_name": "PCRA",
    "title": "Companion to Portfolio Construction and Risk Analysis",
    "description": "A collection of functions and data sets that support teaching\n  a quantitative finance MS level course on Portfolio Construction and Risk\n  Analysis, and the writing of a textbook for such a course.  The package is\n  unique in providing several real-world data sets that may be used for problem\n  assignments and student projects.  The data sets include cross-sections of\n  stock data from the Center for Research on Security Prices, LLC (CRSP),\n  corresponding factor exposures data from S&P Global, and several SP500 data\n  sets.",
    "version": "1.2",
    "maintainer": "Doug Martin <martinrd3d@gmail.com>",
    "author": "Doug Martin [cre, aut],\n  Alexios Galanos [ctb],\n  Kirk Li [aut, ctb],\n  Jon Spinney [ctb],\n  Thomas Philips [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PCRA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCRA Companion to Portfolio Construction and Risk Analysis A collection of functions and data sets that support teaching\n  a quantitative finance MS level course on Portfolio Construction and Risk\n  Analysis, and the writing of a textbook for such a course.  The package is\n  unique in providing several real-world data sets that may be used for problem\n  assignments and student projects.  The data sets include cross-sections of\n  stock data from the Center for Research on Security Prices, LLC (CRSP),\n  corresponding factor exposures data from S&P Global, and several SP500 data\n  sets.  "
  },
  {
    "id": 5570,
    "package_name": "PDMIF",
    "title": "Fits Heterogeneous Panel Data Models",
    "description": "Fits heterogeneous panel data models with interactive effects for linear regression, logistic, count, probit, quantile, and clustering. Based on Ando, T. and Bai, J. (2015) \"A simple new test for slope homogeneity in panel data models with interactive effects\" <doi: 10.1016/j.econlet.2015.09.019>, Ando, T. and Bai, J. (2015) \"Asset Pricing with a General Multifactor Structure\" <doi: 10.1093/jjfinex/nbu026> , Ando, T. and Bai, J. (2016) \"Panel data models with grouped factor structure under unknown group membership\" <doi: 10.1002/jae.2467>, Ando, T. and Bai, J. (2017) \"Clustering huge number of financial time series: A panel data approach with high-dimensional predictors and factor structures\" <doi: 10.1080/01621459.2016.1195743>, Ando, T. and Bai, J. (2020) \"Quantile co-movement in financial markets\" <doi: 10.1080/01621459.2018.1543598>, Ando, T., Bai, J. and Li, K. (2021) \"Bayesian and maximum likelihood analysis of large-scale panel choice models with unobserved heterogeneity\" <doi: 10.1016/j.jeconom.2020.11.013.>.",
    "version": "0.1.0",
    "maintainer": "Tomohiro Ando <t.ando@mbs.edu>",
    "author": "Tomohiro Ando [aut, cre],\n  Hani Fayad [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PDMIF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PDMIF Fits Heterogeneous Panel Data Models Fits heterogeneous panel data models with interactive effects for linear regression, logistic, count, probit, quantile, and clustering. Based on Ando, T. and Bai, J. (2015) \"A simple new test for slope homogeneity in panel data models with interactive effects\" <doi: 10.1016/j.econlet.2015.09.019>, Ando, T. and Bai, J. (2015) \"Asset Pricing with a General Multifactor Structure\" <doi: 10.1093/jjfinex/nbu026> , Ando, T. and Bai, J. (2016) \"Panel data models with grouped factor structure under unknown group membership\" <doi: 10.1002/jae.2467>, Ando, T. and Bai, J. (2017) \"Clustering huge number of financial time series: A panel data approach with high-dimensional predictors and factor structures\" <doi: 10.1080/01621459.2016.1195743>, Ando, T. and Bai, J. (2020) \"Quantile co-movement in financial markets\" <doi: 10.1080/01621459.2018.1543598>, Ando, T., Bai, J. and Li, K. (2021) \"Bayesian and maximum likelihood analysis of large-scale panel choice models with unobserved heterogeneity\" <doi: 10.1016/j.jeconom.2020.11.013.>.  "
  },
  {
    "id": 5575,
    "package_name": "PDtoolkit",
    "title": "Collection of Tools for PD Rating Model Development and\nValidation",
    "description": "The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.",
    "version": "1.2.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/PDtoolkit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PDtoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PDtoolkit Collection of Tools for PD Rating Model Development and\nValidation The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.  "
  },
  {
    "id": 5582,
    "package_name": "PERK",
    "title": "Predicting Environmental Concentration and Risk",
    "description": "A Shiny Web Application to predict and visualize concentrations of pharmaceuticals in the aqueous environment. \n    Jagadeesan K., Barden R. and Kasprzyk-Hordern B. (2022) <https://www.ssrn.com/abstract=4306129>.",
    "version": "0.0.9.2",
    "maintainer": "Kishore Kumar Jagadeesan <jkkishore85@gmail.com>",
    "author": "Kishore Kumar Jagadeesan [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-0916-256X>)",
    "url": "https://github.com/jkkishore85/PERK/,\nhttps://jkkishore85.github.io/PERK/",
    "bug_reports": "https://github.com/jkkishore85/PERK/issues",
    "repository": "https://cran.r-project.org/package=PERK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PERK Predicting Environmental Concentration and Risk A Shiny Web Application to predict and visualize concentrations of pharmaceuticals in the aqueous environment. \n    Jagadeesan K., Barden R. and Kasprzyk-Hordern B. (2022) <https://www.ssrn.com/abstract=4306129>.  "
  },
  {
    "id": 5629,
    "package_name": "PMAPscore",
    "title": "Identify Prognosis-Related Pathways Altered by Somatic Mutation",
    "description": "We innovatively defined a pathway mutation accumulate perturbation score (PMAPscore) to reflect the position and the cumulative effect of the genetic mutations at the pathway level. Based on the PMAPscore of pathways, identified prognosis-related pathways altered by somatic mutation and predict immunotherapy efficacy by constructing a multiple-pathway-based risk model (Tarca, Adi Laurentiu et al (2008) <doi:10.1093/bioinformatics/btn577>).",
    "version": "0.1.1",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [aut, cre, cph],\n  Yalan He [aut],\n  Xiangmei Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PMAPscore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PMAPscore Identify Prognosis-Related Pathways Altered by Somatic Mutation We innovatively defined a pathway mutation accumulate perturbation score (PMAPscore) to reflect the position and the cumulative effect of the genetic mutations at the pathway level. Based on the PMAPscore of pathways, identified prognosis-related pathways altered by somatic mutation and predict immunotherapy efficacy by constructing a multiple-pathway-based risk model (Tarca, Adi Laurentiu et al (2008) <doi:10.1093/bioinformatics/btn577>).  "
  },
  {
    "id": 5667,
    "package_name": "PRA",
    "title": "Project Risk Analysis",
    "description": "Data analysis for Project Risk Management via the Second Moment Method, \n  Monte Carlo Simulation, Contingency Analysis, Sensitivity Analysis, Earned Value Management, \n  Learning Curves, Design Structure Matrices, and more.",
    "version": "0.3.0",
    "maintainer": "Paul Govan <paul.govan2@gmail.com>",
    "author": "Paul Govan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1821-8492>)",
    "url": "https://paulgovan.github.io/PRA/, https://github.com/paulgovan/PRA",
    "bug_reports": "https://github.com/paulgovan/PRA/issues",
    "repository": "https://cran.r-project.org/package=PRA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRA Project Risk Analysis Data analysis for Project Risk Management via the Second Moment Method, \n  Monte Carlo Simulation, Contingency Analysis, Sensitivity Analysis, Earned Value Management, \n  Learning Curves, Design Structure Matrices, and more.  "
  },
  {
    "id": 5669,
    "package_name": "PRDA",
    "title": "Conduct a Prospective or Retrospective Design Analysis",
    "description": "An implementation of the \"Design Analysis\" proposed by \n    Gelman and Carlin (2014) <doi:10.1177/1745691614551642>. It combines \n    the evaluation of Power-Analysis with other inferential-risks as \n    Type-M error (i.e. Magnitude) and Type-S error (i.e. Sign). See also\n    Alto\u00e8 et al. (2020) <doi:10.3389/fpsyg.2019.02893> and \n    Bertoldo et al. (2020) <doi:10.31234/osf.io/q9f86>.",
    "version": "1.0.2",
    "maintainer": "Claudio Zandonella Callegher <claudiozandonella@gmail.com>",
    "author": "Claudio Zandonella Callegher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7721-6318>),\n  Massimiliano Pastore [aut] (ORCID:\n    <https://orcid.org/0000-0002-7922-6365>),\n  Angela Andreella [aut] (ORCID: <https://orcid.org/0000-0002-1141-3041>),\n  Anna Vesely [aut] (ORCID: <https://orcid.org/0000-0001-6696-2390>),\n  Enrico Toffalini [aut] (ORCID: <https://orcid.org/0000-0002-1404-5133>),\n  Giulia Bertoldo [aut] (ORCID: <https://orcid.org/0000-0002-6960-3980>),\n  Gianmarco Alto\u00e8 [aut] (ORCID: <https://orcid.org/0000-0003-1154-9528>)",
    "url": "https://claudiozandonella.github.io/PRDA/,\nhttps://github.com/ClaudioZandonella/PRDA",
    "bug_reports": "https://github.com/ClaudioZandonella/PRDA/issues",
    "repository": "https://cran.r-project.org/package=PRDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRDA Conduct a Prospective or Retrospective Design Analysis An implementation of the \"Design Analysis\" proposed by \n    Gelman and Carlin (2014) <doi:10.1177/1745691614551642>. It combines \n    the evaluation of Power-Analysis with other inferential-risks as \n    Type-M error (i.e. Magnitude) and Type-S error (i.e. Sign). See also\n    Alto\u00e8 et al. (2020) <doi:10.3389/fpsyg.2019.02893> and \n    Bertoldo et al. (2020) <doi:10.31234/osf.io/q9f86>.  "
  },
  {
    "id": 5682,
    "package_name": "PROSPER",
    "title": "Simulation of Weed Population Dynamics",
    "description": "An environment to simulate the development of annual plant populations with regard to population dynamics and genetics, especially herbicide resistance. It combines genetics on the individual level (Renton et al. 2011) with a stochastic development on the population level (Daedlow, 2015).\n\tRenton, M, Diggle, A, Manalil, S and Powles, S (2011) <doi:10.1016/j.jtbi.2011.05.010> \n\tDaedlow, Daniel (2015, doctoral dissertation: University of Rostock, Faculty of Agriculture and Environmental Sciences.)",
    "version": "0.3.3",
    "maintainer": "Christoph v. Redwitz <christoph.redwitz@posteo.de>",
    "author": "Christoph v. Redwitz [aut, cre],\n  Friederike de Mol [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PROSPER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROSPER Simulation of Weed Population Dynamics An environment to simulate the development of annual plant populations with regard to population dynamics and genetics, especially herbicide resistance. It combines genetics on the individual level (Renton et al. 2011) with a stochastic development on the population level (Daedlow, 2015).\n\tRenton, M, Diggle, A, Manalil, S and Powles, S (2011) <doi:10.1016/j.jtbi.2011.05.010> \n\tDaedlow, Daniel (2015, doctoral dissertation: University of Rostock, Faculty of Agriculture and Environmental Sciences.)  "
  },
  {
    "id": 5765,
    "package_name": "Pareto",
    "title": "The Pareto, Piecewise Pareto and Generalized Pareto Distribution",
    "description": "Utilities for the Pareto, piecewise Pareto and generalized Pareto distribution\n    that are useful for reinsurance pricing. In particular, the package provides\n    a non-trivial algorithm that can be used to match the expected losses of a \n    tower of reinsurance layers with a layer-independent collective risk model.\n    The theoretical background of the matching algorithm and most other methods\n    are described in Ulrich Riegel (2018) <doi:10.1007/s13385-018-0177-3>.",
    "version": "2.4.5",
    "maintainer": "Ulrich Riegel <ulrich.riegel@gmx.de>",
    "author": "Ulrich Riegel [aut, cre]",
    "url": "https://github.com/ulrichriegel/Pareto#pareto",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Pareto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Pareto The Pareto, Piecewise Pareto and Generalized Pareto Distribution Utilities for the Pareto, piecewise Pareto and generalized Pareto distribution\n    that are useful for reinsurance pricing. In particular, the package provides\n    a non-trivial algorithm that can be used to match the expected losses of a \n    tower of reinsurance layers with a layer-independent collective risk model.\n    The theoretical background of the matching algorithm and most other methods\n    are described in Ulrich Riegel (2018) <doi:10.1007/s13385-018-0177-3>.  "
  },
  {
    "id": 5798,
    "package_name": "PerformanceAnalytics",
    "title": "Econometric Tools for Performance and Risk Analysis",
    "description": "Collection of econometric functions for performance and risk \n    analysis. In addition to standard risk and performance metrics, this \n    package aims to aid practitioners and researchers in utilizing the latest\n    research in analysis of non-normal return streams.  In general, it is most \n    tested on return (rather than price) data on a regular scale, but most \n    functions will work with irregular return data as well, and increasing\n    numbers of functions will work with P&L or price data where possible.",
    "version": "2.0.8",
    "maintainer": "Brian G. Peterson <brian@braverock.com>",
    "author": "Brian G. Peterson [cre, aut, cph],\n  Peter Carl [aut, cph],\n  Kris Boudt [ctb, cph],\n  Ross Bennett [ctb],\n  Joshua Ulrich [ctb],\n  Eric Zivot [ctb],\n  Dries Cornilly [ctb],\n  Eric Hung [ctb],\n  Matthieu Lestel [ctb],\n  Kyle Balkissoon [ctb],\n  Diethelm Wuertz [ctb],\n  Anthony Alexander Christidis [ctb],\n  R. Douglas Martin [ctb],\n  Zeheng Zenith Zhou [ctb],\n  Justin M. Shea [ctb],\n  Dhairya Jain [ctb]",
    "url": "https://github.com/braverock/PerformanceAnalytics",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PerformanceAnalytics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PerformanceAnalytics Econometric Tools for Performance and Risk Analysis Collection of econometric functions for performance and risk \n    analysis. In addition to standard risk and performance metrics, this \n    package aims to aid practitioners and researchers in utilizing the latest\n    research in analysis of non-normal return streams.  In general, it is most \n    tested on return (rather than price) data on a regular scale, but most \n    functions will work with irregular return data as well, and increasing\n    numbers of functions will work with P&L or price data where possible.  "
  },
  {
    "id": 5815,
    "package_name": "PheNorm",
    "title": "Unsupervised Gold-Standard Label Free Phenotyping Algorithm for\nEHR Data",
    "description": "The algorithm combines the most predictive variable, such as count of the main International Classification of Diseases (ICD) codes, and other Electronic Health Record (EHR) features (e.g. health utilization and processed clinical note data), to obtain a score for accurate risk prediction and disease classification. In particular, it normalizes the surrogate to resemble gaussian mixture and leverages the remaining features through random corruption denoising. Background and details about the method can be found at Yu et al. (2018) <doi:10.1093/jamia/ocx111>.",
    "version": "0.1.0",
    "maintainer": "Clara-Lea Bonzel <clbonzel@hsph.harvard.edu>",
    "author": "Sheng Yu [aut],\n  Victor Castro [aut],\n  Clara-Lea Bonzel [aut, cre],\n  Molei Liu [aut],\n  Chuan Hong [aut],\n  Tianxi Cai [aut],\n  PARSE LTD [aut]",
    "url": "https://github.com/celehs/PheNorm",
    "bug_reports": "https://github.com/celehs/PheNorm/issues",
    "repository": "https://cran.r-project.org/package=PheNorm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PheNorm Unsupervised Gold-Standard Label Free Phenotyping Algorithm for\nEHR Data The algorithm combines the most predictive variable, such as count of the main International Classification of Diseases (ICD) codes, and other Electronic Health Record (EHR) features (e.g. health utilization and processed clinical note data), to obtain a score for accurate risk prediction and disease classification. In particular, it normalizes the surrogate to resemble gaussian mixture and leverages the remaining features through random corruption denoising. Background and details about the method can be found at Yu et al. (2018) <doi:10.1093/jamia/ocx111>.  "
  },
  {
    "id": 5833,
    "package_name": "PhytosanitaryCalculator",
    "title": "Phytosanitary Calculator for Inspection Plans Based on Risks",
    "description": "A 'Shiny' application for calculating phytosanitary inspection plans based on risks. It generates a diagram of pallets in a lot, highlights the units to be sampled, and documents them based on the selected sampling method (simple random or systematic sampling). ",
    "version": "1.1.3",
    "maintainer": "Gustavo Ramirez-Valverde <gustavoramirezvalverde@gmail.com>",
    "author": "Gustavo Ramirez-Valverde [aut, cre],\n  Luis Gabriel Otero-Prevost [aut],\n  Pedro Macias-Canales [ctb],\n  Juan A. Villanueva-Jimenez [ctb],\n  Jorge Luis Leyva-Vazquez [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PhytosanitaryCalculator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhytosanitaryCalculator Phytosanitary Calculator for Inspection Plans Based on Risks A 'Shiny' application for calculating phytosanitary inspection plans based on risks. It generates a diagram of pallets in a lot, highlights the units to be sampled, and documents them based on the selected sampling method (simple random or systematic sampling).   "
  },
  {
    "id": 5834,
    "package_name": "PiC",
    "title": "Pointcloud Interactive Computation",
    "description": "Provides advanced algorithms for analyzing pointcloud data from terrestrial laser scanner in\n    forestry applications. Key features include fast voxelization of\n    large datasets; segmentation of point clouds into forest floor,\n    understorey, canopy, and wood components. The package enables\n    efficient processing of large-scale forest pointcloud data, offering\n    insights into forest structure, connectivity, and fire risk\n    assessment. Algorithms to analyze pointcloud data (.xyz input file).\n    For more details, see Ferrara & Arrizza (2025) <https://hdl.handle.net/20.500.14243/533471>.\n    For single tree segmentation details, see Ferrara et al. (2018) \n    <doi:10.1016/j.agrformet.2018.04.008>.",
    "version": "1.2.7",
    "maintainer": "Roberto Ferrara <roberto.ferrara@cnr.it>",
    "author": "Roberto Ferrara [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-3627-6867>),\n  Stefano Arrizza [ctb] (ORCID: <https://orcid.org/0009-0009-2290-3650>)",
    "url": "https://github.com/rupppy/PiC",
    "bug_reports": "https://github.com/rupppy/PiC/issues",
    "repository": "https://cran.r-project.org/package=PiC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PiC Pointcloud Interactive Computation Provides advanced algorithms for analyzing pointcloud data from terrestrial laser scanner in\n    forestry applications. Key features include fast voxelization of\n    large datasets; segmentation of point clouds into forest floor,\n    understorey, canopy, and wood components. The package enables\n    efficient processing of large-scale forest pointcloud data, offering\n    insights into forest structure, connectivity, and fire risk\n    assessment. Algorithms to analyze pointcloud data (.xyz input file).\n    For more details, see Ferrara & Arrizza (2025) <https://hdl.handle.net/20.500.14243/533471>.\n    For single tree segmentation details, see Ferrara et al. (2018) \n    <doi:10.1016/j.agrformet.2018.04.008>.  "
  },
  {
    "id": 5857,
    "package_name": "PoPdesign",
    "title": "Posterior Predictive (PoP) Design for Phase I Clinical Trials",
    "description": "The primary goal of phase I clinical trials is to find the maximum tolerated dose (MTD). To reach this objective, we introduce a new design for phase I clinical trials, the posterior predictive (PoP) design. The PoP design is an innovative model-assisted design that is as simply as the conventional algorithmic designs as its decision rules can be pre-tabulated prior to the onset of trial, but is of more flexibility of selecting diverse target toxicity rates and cohort sizes. The PoP design has desirable properties, such as coherence and consistency. Moreover, the PoP design provides better empirical performance than the BOIN and Keyboard design with respect to high average probabilities of choosing the MTD and slightly lower risk of treating patients at subtherapeutic or overly toxic doses. ",
    "version": "1.1.0",
    "maintainer": "Xinying Fang <xf.research@outlook.com>",
    "author": "Chenqi Fu [aut],\n  Xinying Fang [aut, cre],\n  Shouhao Zhou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PoPdesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PoPdesign Posterior Predictive (PoP) Design for Phase I Clinical Trials The primary goal of phase I clinical trials is to find the maximum tolerated dose (MTD). To reach this objective, we introduce a new design for phase I clinical trials, the posterior predictive (PoP) design. The PoP design is an innovative model-assisted design that is as simply as the conventional algorithmic designs as its decision rules can be pre-tabulated prior to the onset of trial, but is of more flexibility of selecting diverse target toxicity rates and cohort sizes. The PoP design has desirable properties, such as coherence and consistency. Moreover, the PoP design provides better empirical performance than the BOIN and Keyboard design with respect to high average probabilities of choosing the MTD and slightly lower risk of treating patients at subtherapeutic or overly toxic doses.   "
  },
  {
    "id": 5860,
    "package_name": "PogromcyDanych",
    "title": "DataCrunchers (PogromcyDanych) is the Massive Online Open Course\nthat Brings R and Statistics to the People",
    "description": "The data sets used in the online course ,,PogromcyDanych''. You can process data in many ways. The course Data Crunchers will introduce you to this variety. For this reason we will work on datasets of different size (from several to several hundred thousand rows), with various level of complexity (from two to two thousand columns) and prepared in different formats (text data, quantitative data and qualitative data). All of these data sets were gathered in a single big package called PogromcyDanych to facilitate access to them. It contains all sorts of data sets such as data about offer prices of cars, results of opinion polls, information about changes in stock market indices, data about names given to newborn babies, ski jumping results or information about outcomes of breast cancer patients treatment.",
    "version": "1.7.1",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PogromcyDanych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PogromcyDanych DataCrunchers (PogromcyDanych) is the Massive Online Open Course\nthat Brings R and Statistics to the People The data sets used in the online course ,,PogromcyDanych''. You can process data in many ways. The course Data Crunchers will introduce you to this variety. For this reason we will work on datasets of different size (from several to several hundred thousand rows), with various level of complexity (from two to two thousand columns) and prepared in different formats (text data, quantitative data and qualitative data). All of these data sets were gathered in a single big package called PogromcyDanych to facilitate access to them. It contains all sorts of data sets such as data about offer prices of cars, results of opinion polls, information about changes in stock market indices, data about names given to newborn babies, ski jumping results or information about outcomes of breast cancer patients treatment.  "
  },
  {
    "id": 5874,
    "package_name": "PolicyPortfolios",
    "title": "Tools for Managing, Measuring and Visualizing Policy Portfolios",
    "description": "Tools for simplifying the creation and management of data structures\n    suitable for dealing with policy portfolios, that is, two-dimensional spaces \n    of policy instruments and policy targets. The package also allows to generate measures of\n    portfolio characteristics and facilitates their visualization.",
    "version": "0.5",
    "maintainer": "Xavier Fern\u00e1ndez i Mar\u00edn <xavier.fim@gmail.com>",
    "author": "Xavier Fern\u00e1ndez i Mar\u00edn [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9522-8870>)",
    "url": "http://xavier-fim.net/packages/PolicyPortfolios/,\nhttps://github.com/xfim/PolicyPortfolios",
    "bug_reports": "https://github.com/xfim/PolicyPortfolios/issues",
    "repository": "https://cran.r-project.org/package=PolicyPortfolios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PolicyPortfolios Tools for Managing, Measuring and Visualizing Policy Portfolios Tools for simplifying the creation and management of data structures\n    suitable for dealing with policy portfolios, that is, two-dimensional spaces \n    of policy instruments and policy targets. The package also allows to generate measures of\n    portfolio characteristics and facilitates their visualization.  "
  },
  {
    "id": 5886,
    "package_name": "PooledCohort",
    "title": "Predicted Risk for CVD using Pooled Cohort Equations, PREVENT\nEquations, and Other Contemporary CVD Risk Calculators",
    "description": "The 2017 American College of Cardiology and American Heart\n  Association blood pressure guideline recommends using 10-year predicted \n  atherosclerotic cardiovascular disease risk to guide the decision to \n  initiate or intensify antihypertensive medication. The guideline recommends \n  using the Pooled Cohort risk prediction equations to predict 10-year \n  atherosclerotic cardiovascular disease risk. This package implements the \n  original Pooled Cohort risk prediction equations and also incorporates \n  updated versions based on more contemporary data and statistical methods.",
    "version": "0.0.2",
    "maintainer": "Byron Jaeger <bjaeger@wakehealth.edu>",
    "author": "Byron Jaeger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7399-2299>)",
    "url": "https://github.com/bcjaeger/PooledCohort,\nhttps://bcjaeger.github.io/PooledCohort/",
    "bug_reports": "https://github.com/bcjaeger/PooledCohort/issues",
    "repository": "https://cran.r-project.org/package=PooledCohort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PooledCohort Predicted Risk for CVD using Pooled Cohort Equations, PREVENT\nEquations, and Other Contemporary CVD Risk Calculators The 2017 American College of Cardiology and American Heart\n  Association blood pressure guideline recommends using 10-year predicted \n  atherosclerotic cardiovascular disease risk to guide the decision to \n  initiate or intensify antihypertensive medication. The guideline recommends \n  using the Pooled Cohort risk prediction equations to predict 10-year \n  atherosclerotic cardiovascular disease risk. This package implements the \n  original Pooled Cohort risk prediction equations and also incorporates \n  updated versions based on more contemporary data and statistical methods.  "
  },
  {
    "id": 5896,
    "package_name": "PortfolioAnalytics",
    "title": "Portfolio Analysis, Including Numerical Methods for Optimization\nof Portfolios",
    "description": "Portfolio optimization and analysis routines and graphics.",
    "version": "2.1.0",
    "maintainer": "Brian G. Peterson <brian@braverock.com>",
    "author": "Brian G. Peterson [cre, aut, cph],\n  Peter Carl [aut, cph],\n  Ross Bennett [ctb, cph],\n  Kris Boudt [ctb, cph],\n  Xinran Zhao [cph],\n  R. Douglas Martin [ctb],\n  Guy Yollin [ctb],\n  Hezky Varon [ctb],\n  Xiaokang Feng [ctb],\n  Yifu Kang [ctb]",
    "url": "https://github.com/braverock/PortfolioAnalytics",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PortfolioAnalytics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PortfolioAnalytics Portfolio Analysis, Including Numerical Methods for Optimization\nof Portfolios Portfolio optimization and analysis routines and graphics.  "
  },
  {
    "id": 5897,
    "package_name": "PortfolioOptim",
    "title": "Small/Large Sample Portfolio Optimization",
    "description": "Two functions for financial portfolio optimization by linear programming are provided. One function implements Benders decomposition algorithm and can be used for very large data sets. The other, applicable for moderate sample sizes, finds optimal portfolio which has the smallest distance to a given benchmark portfolio.",
    "version": "1.1.1",
    "maintainer": "Andrzej Palczewski <A.Palczewski@mimuw.edu.pl>",
    "author": "Andrzej Palczewski [aut, cre],\n  Aleksandra Dabrowska [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PortfolioOptim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PortfolioOptim Small/Large Sample Portfolio Optimization Two functions for financial portfolio optimization by linear programming are provided. One function implements Benders decomposition algorithm and can be used for very large data sets. The other, applicable for moderate sample sizes, finds optimal portfolio which has the smallest distance to a given benchmark portfolio.  "
  },
  {
    "id": 5898,
    "package_name": "PortfolioTesteR",
    "title": "Test Investment Strategies with English-Like Code",
    "description": "Design, backtest, and analyze portfolio strategies using simple,\n    English-like function chains. Includes technical indicators, flexible stock\n    selection, portfolio construction methods (equal weighting, signal weighting,\n    inverse volatility, hierarchical risk parity), and a compact backtesting\n    engine for portfolio returns, drawdowns, and summary metrics.",
    "version": "0.1.4",
    "maintainer": "Alberto Pallotta <pallottaalberto@gmail.com>",
    "author": "Alberto Pallotta [aut, cre]",
    "url": "https://github.com/AlbertoPallotta/PortfolioTesteR",
    "bug_reports": "https://github.com/AlbertoPallotta/PortfolioTesteR/issues",
    "repository": "https://cran.r-project.org/package=PortfolioTesteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PortfolioTesteR Test Investment Strategies with English-Like Code Design, backtest, and analyze portfolio strategies using simple,\n    English-like function chains. Includes technical indicators, flexible stock\n    selection, portfolio construction methods (equal weighting, signal weighting,\n    inverse volatility, hierarchical risk parity), and a compact backtesting\n    engine for portfolio returns, drawdowns, and summary metrics.  "
  },
  {
    "id": 5918,
    "package_name": "PredictABEL",
    "title": "Assessment of Risk Prediction Models",
    "description": "We included functions to assess the performance of\n risk models. The package contains functions for the various measures that are\n used in empirical studies, including univariate and multivariate odds ratios\n (OR) of the predictors, the c-statistic (or area under the receiver operating\n characteristic (ROC) curve (AUC)), Hosmer-Lemeshow goodness of fit test,\n reclassification table, net reclassification improvement (NRI) and\n integrated discrimination improvement (IDI). Also included are functions\n to create plots, such as risk distributions, ROC curves, calibration plot,\n discrimination box plot and predictiveness curves. In addition to functions\n to assess the performance of risk models, the package includes functions to\n obtain weighted and unweighted risk scores as well as predicted risks using\n logistic regression analysis. These logistic regression functions are\n specifically written for models that include genetic variables, but they\n can also be applied to models that are based on non-genetic risk factors only.\n Finally, the package includes function to construct a simulated dataset with \n genotypes, genetic risks, and disease status for a hypothetical population, which \n is used for the evaluation of genetic risk models.",
    "version": "1.2-4",
    "maintainer": "Suman Kundu <suman_math@yahoo.com>",
    "author": "Suman Kundu, Yurii S. Aulchenko, A. Cecile J.W. Janssens",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PredictABEL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PredictABEL Assessment of Risk Prediction Models We included functions to assess the performance of\n risk models. The package contains functions for the various measures that are\n used in empirical studies, including univariate and multivariate odds ratios\n (OR) of the predictors, the c-statistic (or area under the receiver operating\n characteristic (ROC) curve (AUC)), Hosmer-Lemeshow goodness of fit test,\n reclassification table, net reclassification improvement (NRI) and\n integrated discrimination improvement (IDI). Also included are functions\n to create plots, such as risk distributions, ROC curves, calibration plot,\n discrimination box plot and predictiveness curves. In addition to functions\n to assess the performance of risk models, the package includes functions to\n obtain weighted and unweighted risk scores as well as predicted risks using\n logistic regression analysis. These logistic regression functions are\n specifically written for models that include genetic variables, but they\n can also be applied to models that are based on non-genetic risk factors only.\n Finally, the package includes function to construct a simulated dataset with \n genotypes, genetic risks, and disease status for a hypothetical population, which \n is used for the evaluation of genetic risk models.  "
  },
  {
    "id": 5929,
    "package_name": "ProPublicaR",
    "title": "Access Functions for ProPublica's APIs",
    "description": "Provides wrapper functions to access the ProPublica's Congress and Campaign Finance APIs.\n    The Congress API provides near real-time access to legislative data from the House of \n    Representatives, the Senate and the Library of Congress.\n    The Campaign Finance API provides data from United States Federal Election Commission \n    filings and other sources. The API covers summary information for candidates and \n    committees, as well as certain types of itemized data.\n    For more information about these APIs go to: <https://www.propublica.org/datastore/apis>.",
    "version": "1.1.4",
    "maintainer": "Aleksander Dietrichson <dietrichson@gmail.com>",
    "author": "Aleksander Dietrichson [aut, cre],\n  Joselina Davit [aut]",
    "url": "",
    "bug_reports": "https://github.com/dietrichson/ProPublicaR/issues",
    "repository": "https://cran.r-project.org/package=ProPublicaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProPublicaR Access Functions for ProPublica's APIs Provides wrapper functions to access the ProPublica's Congress and Campaign Finance APIs.\n    The Congress API provides near real-time access to legislative data from the House of \n    Representatives, the Senate and the Library of Congress.\n    The Campaign Finance API provides data from United States Federal Election Commission \n    filings and other sources. The API covers summary information for candidates and \n    committees, as well as certain types of itemized data.\n    For more information about these APIs go to: <https://www.propublica.org/datastore/apis>.  "
  },
  {
    "id": 5991,
    "package_name": "QDiabetes",
    "title": "Type 2 Diabetes Risk Calculator",
    "description": "Calculate the risk of developing type 2 diabetes using risk prediction algorithms derived by 'ClinRisk'.",
    "version": "1.0-2",
    "maintainer": "Benjamin G. Feakins <benjamin.feakins@ndph.ox.ac.uk>",
    "author": "Benjamin G. Feakins [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3928-6750>),\n  Sarah L. Lay-Flurrie [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1094-8455>),\n  Richard J. Stevens [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9258-4060>),\n  Trish Greenhalgh [ctb] (ORCID: <https://orcid.org/0000-0003-2369-8088>),\n  Tim A. Holt [ctb] (ORCID: <https://orcid.org/0000-0002-1214-1682>),\n  Evangelos Kontopantelis [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6450-5815>),\n  Dianna M. Smith [ctb] (ORCID: <https://orcid.org/0000-0002-0650-6606>),\n  Bernard C. Gudgin [ctb],\n  Benjamin J. Cairns [csl, sad] (ORCID:\n    <https://orcid.org/0000-0001-7994-8213>),\n  National Institute for Health Research School for Primary Care Research\n    [fnd],\n  University of Oxford [cph, sht]",
    "url": "https://github.com/Feakster/qdiabetes",
    "bug_reports": "https://github.com/Feakster/qdiabetes/issues",
    "repository": "https://cran.r-project.org/package=QDiabetes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QDiabetes Type 2 Diabetes Risk Calculator Calculate the risk of developing type 2 diabetes using risk prediction algorithms derived by 'ClinRisk'.  "
  },
  {
    "id": 5994,
    "package_name": "QFRM",
    "title": "Pricing of Vanilla and Exotic Option Contracts",
    "description": "\n  Option pricing (financial derivatives) techniques mainly following textbook 'Options, Futures and Other Derivatives', 9ed by John C.Hull, 2014. Prentice Hall. Implementations are via binomial tree option model (BOPM), Black-Scholes model, Monte Carlo simulations, etc. \n  This package is a result of Quantitative Financial Risk Management course (STAT 449 and STAT 649) at Rice University, Houston, TX, USA, taught by Oleg Melnikov, statistics PhD student, as of Spring 2015.",
    "version": "1.0.1",
    "maintainer": "Oleg Melnikov <XisReal@gmail.com>",
    "author": "Oleg Melnikov [aut, cre],\n  Max Lee [ctb],\n  Robert Abramov [ctb],\n  Richard Huang [ctb],\n  Liu Tong [ctb],\n  Jake Kornblau [ctb],\n  Xinnan Lu [ctb],\n  Kiryl Novikau [ctb],\n  Tongyue Luo [ctb],\n  Le You [ctb],\n  Jin Chen [ctb],\n  Chengwei Ge [ctb],\n  Jiayao Huang [ctb],\n  Kim Raath [ctb]",
    "url": "http://Oleg.Rice.edu",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QFRM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QFRM Pricing of Vanilla and Exotic Option Contracts \n  Option pricing (financial derivatives) techniques mainly following textbook 'Options, Futures and Other Derivatives', 9ed by John C.Hull, 2014. Prentice Hall. Implementations are via binomial tree option model (BOPM), Black-Scholes model, Monte Carlo simulations, etc. \n  This package is a result of Quantitative Financial Risk Management course (STAT 449 and STAT 649) at Rice University, Houston, TX, USA, taught by Oleg Melnikov, statistics PhD student, as of Spring 2015.  "
  },
  {
    "id": 5999,
    "package_name": "QHScrnomo",
    "title": "Construct Nomograms for Competing Risks Regression Models",
    "description": "Nomograms are constructed to predict the cumulative incidence\n    rate which is calculated after adjusting for competing causes to the event of interest. \n    K-fold cross-validation is implemented to validate predictive accuracy using a competing-risk version of the concordance index.\n    Methods are as described in: Kattan MW, Heller G, \n    Brennan MF (2003).",
    "version": "3.0.2",
    "maintainer": "Zajichek Alex <zajicha2@ccf.org>",
    "author": "Michael Kattan [aut],\n  Changhong Yu [aut],\n  Xinge Ji [aut],\n  Zajichek Alex [cre]",
    "url": "https://github.com/ClevelandClinicQHS/QHScrnomo",
    "bug_reports": "https://github.com/ClevelandClinicQHS/QHScrnomo/issues",
    "repository": "https://cran.r-project.org/package=QHScrnomo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QHScrnomo Construct Nomograms for Competing Risks Regression Models Nomograms are constructed to predict the cumulative incidence\n    rate which is calculated after adjusting for competing causes to the event of interest. \n    K-fold cross-validation is implemented to validate predictive accuracy using a competing-risk version of the concordance index.\n    Methods are as described in: Kattan MW, Heller G, \n    Brennan MF (2003).  "
  },
  {
    "id": 6005,
    "package_name": "QRAGadget",
    "title": "A 'Shiny' Gadget for Interactive 'QRA' Visualizations",
    "description": "Upload raster data and easily create interactive quantitative risk analysis 'QRA' visualizations. Select\n    from numerous color palettes, base-maps, and different configurations.",
    "version": "0.3.0",
    "maintainer": "Paul Govan <pgovan1@aggienetwork.com>",
    "author": "Paul Govan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1821-8492>)",
    "url": "https://github.com/paulgovan/qragadget,\nhttp://paulgovan.github.io/QRAGadget/",
    "bug_reports": "https://github.com/paulgovan/qragadget/issues",
    "repository": "https://cran.r-project.org/package=QRAGadget",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QRAGadget A 'Shiny' Gadget for Interactive 'QRA' Visualizations Upload raster data and easily create interactive quantitative risk analysis 'QRA' visualizations. Select\n    from numerous color palettes, base-maps, and different configurations.  "
  },
  {
    "id": 6006,
    "package_name": "QRISK3",
    "title": "10-Year Cardiovascular Disease Risk Calculator (QRISK3 2017)",
    "description": "This function aims to calculate risk of developing cardiovascular disease of individual patients in next 10 years. This unofficial package was based on published open-sourced free risk prediction algorithm QRISK3-2017 <https://qrisk.org/src.php>. ",
    "version": "0.6.0",
    "maintainer": "Yan Li <bluefatterplaydota@gmail.com>",
    "author": "Yan Li <bluefatterplaydota@gmail.com> [aut, cre, trl],\n        Matthew Sperrin [aut, ctb],\n\t\tClinRisk Ltd. [cph],\n\t\tTjeerd Pieter van Staa [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QRISK3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QRISK3 10-Year Cardiovascular Disease Risk Calculator (QRISK3 2017) This function aims to calculate risk of developing cardiovascular disease of individual patients in next 10 years. This unofficial package was based on published open-sourced free risk prediction algorithm QRISK3-2017 <https://qrisk.org/src.php>.   "
  },
  {
    "id": 6008,
    "package_name": "QRM",
    "title": "Provides R-Language Code to Examine Quantitative Risk Management\nConcepts",
    "description": "Provides functions/methods to accompany the book\n Quantitative Risk Management: Concepts, Techniques and Tools by\n Alexander J. McNeil, Ruediger Frey, and Paul Embrechts.",
    "version": "0.4-35",
    "maintainer": "Bernhard Pfaff <bernhard@pfaffikus.de>",
    "author": "Bernhard Pfaff [aut, cre],\n  Marius Hofert [ctb],\n  Alexander McNeil) [aut] (QRMlib),\n  Scott Ulmann [trl] (First R port as package QRMlib)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QRM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QRM Provides R-Language Code to Examine Quantitative Risk Management\nConcepts Provides functions/methods to accompany the book\n Quantitative Risk Management: Concepts, Techniques and Tools by\n Alexander J. McNeil, Ruediger Frey, and Paul Embrechts.  "
  },
  {
    "id": 6032,
    "package_name": "QualityMeasure",
    "title": "Methods for Analyzing Quality Measure Performance",
    "description": "Quality of care is compared across accountable entities, including hospitals, provider groups, and insurance plans, using standardized quality measures. However, observed variations in quality measure performance might be the result of chance sampling or measurement errors. Contains functions for estimating the reliability of unadjusted and risk-standardized quality measures.",
    "version": "2.0.1",
    "maintainer": "Kenneth Nieser <nieser@stanford.edu>",
    "author": "Kenneth Nieser [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QualityMeasure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QualityMeasure Methods for Analyzing Quality Measure Performance Quality of care is compared across accountable entities, including hospitals, provider groups, and insurance plans, using standardized quality measures. However, observed variations in quality measure performance might be the result of chance sampling or measurement errors. Contains functions for estimating the reliability of unadjusted and risk-standardized quality measures.  "
  },
  {
    "id": 6035,
    "package_name": "QuantBondCurves",
    "title": "Calculates Bond Values and Interest Rate Curves for Finance",
    "description": "Values different types of assets and calibrates discount curves \n    for quantitative financial analysis. It covers fixed coupon assets, \n    floating note assets, interest and cross currency swaps with different \n    payment frequencies. Enables the calibration of spot, instantaneous forward \n    and basis curves, making it a powerful tool for accurate and flexible bond \n    valuation and curve generation. The valuation and calibration techniques \n    presented here are consistent with industry standards and incorporates \n    author's own calculations. Tuckman, B., Serrat, A. (2022, ISBN: 978-1-119-83555-4).",
    "version": "0.3.2",
    "maintainer": "Camilo D\u00edaz <kamodiaz@gmail.com>",
    "author": "Camilo D\u00edaz [aut, cre, com],\n  Andr\u00e9s Galeano [aut],\n  Juli\u00e1n Rojas [aut],\n  Quantil S.A.S [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QuantBondCurves",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QuantBondCurves Calculates Bond Values and Interest Rate Curves for Finance Values different types of assets and calibrates discount curves \n    for quantitative financial analysis. It covers fixed coupon assets, \n    floating note assets, interest and cross currency swaps with different \n    payment frequencies. Enables the calibration of spot, instantaneous forward \n    and basis curves, making it a powerful tool for accurate and flexible bond \n    valuation and curve generation. The valuation and calibration techniques \n    presented here are consistent with industry standards and incorporates \n    author's own calculations. Tuckman, B., Serrat, A. (2022, ISBN: 978-1-119-83555-4).  "
  },
  {
    "id": 6076,
    "package_name": "R4GoodPersonalFinances",
    "title": "Make Optimal Financial Decisions",
    "description": "Make optimal decisions for your personal or\n    household finances. Use tools and methods that are selected carefully\n    to align with academic consensus, bridging the gap between theoretical\n    knowledge and practical application. They help you find\n    your own personalized optimal discretionary spending or\n    optimal asset allocation, and prepare you for retirement \n    or financial independence.\n    The optimal solution to this problems is extremely complex,\n    and we only have a single lifetime to get it right.\n    Fortunately, we now have the user-friendly tools implemented,\n    that integrate life-cycle models \n    with single-period net-worth mean-variance optimization models. \n    Those tools can be used by anyone who wants to see \n    what highly-personalized optimal decisions can look like.\n    For more details see: \n    Idzorek T., Kaplan P. (2024, ISBN:9781952927379),\n    Haghani V., White J. (2023, ISBN:9781119747918).",
    "version": "1.2.0",
    "maintainer": "Kamil Wais <kamil.wais@gmail.com>",
    "author": "Kamil Wais [aut, cre, cph, fnd] (ORCID:\n    <https://orcid.org/0000-0002-4062-055X>),\n  Olesia Wais [aut] (ORCID: <https://orcid.org/0000-0002-8741-8674>)",
    "url": "https://www.r4good.academy/,\nhttps://r4goodacademy.github.io/R4GoodPersonalFinances/,\nhttps://github.com/R4GoodAcademy/R4GoodPersonalFinances",
    "bug_reports": "https://github.com/R4GoodAcademy/R4GoodPersonalFinances/issues",
    "repository": "https://cran.r-project.org/package=R4GoodPersonalFinances",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R4GoodPersonalFinances Make Optimal Financial Decisions Make optimal decisions for your personal or\n    household finances. Use tools and methods that are selected carefully\n    to align with academic consensus, bridging the gap between theoretical\n    knowledge and practical application. They help you find\n    your own personalized optimal discretionary spending or\n    optimal asset allocation, and prepare you for retirement \n    or financial independence.\n    The optimal solution to this problems is extremely complex,\n    and we only have a single lifetime to get it right.\n    Fortunately, we now have the user-friendly tools implemented,\n    that integrate life-cycle models \n    with single-period net-worth mean-variance optimization models. \n    Those tools can be used by anyone who wants to see \n    what highly-personalized optimal decisions can look like.\n    For more details see: \n    Idzorek T., Kaplan P. (2024, ISBN:9781952927379),\n    Haghani V., White J. (2023, ISBN:9781119747918).  "
  },
  {
    "id": 6109,
    "package_name": "RBNZ",
    "title": "Download Data from the Reserve Bank of New Zealand Website",
    "description": "Provides a convenient way of accessing data published by the Reserve Bank of New Zealand (RBNZ) on their website, <https://www.rbnz.govt.nz/statistics>. A range of financial and economic data is provided in spreadsheet format including exchange and interest rates, commercial lending statistics, Reserve Bank market operations, financial institution statistics, household financial data, New Zealand debt security information, and economic indicators. This package provides a method to download those spreadsheets and read them directly into R.",
    "version": "3.0.0",
    "maintainer": "Jasper Watson <jasper.g.watson@gmail.com>",
    "author": "Jasper Watson [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/rntq472/RBNZ/issues",
    "repository": "https://cran.r-project.org/package=RBNZ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RBNZ Download Data from the Reserve Bank of New Zealand Website Provides a convenient way of accessing data published by the Reserve Bank of New Zealand (RBNZ) on their website, <https://www.rbnz.govt.nz/statistics>. A range of financial and economic data is provided in spreadsheet format including exchange and interest rates, commercial lending statistics, Reserve Bank market operations, financial institution statistics, household financial data, New Zealand debt security information, and economic indicators. This package provides a method to download those spreadsheets and read them directly into R.  "
  },
  {
    "id": 6173,
    "package_name": "REGENT",
    "title": "Risk Estimation for Genetic and Environmental Traits",
    "description": "Produces population distribution of disease risk and statistical risk categories, and predicts risks for individuals with genotype information.",
    "version": "1.0.6",
    "maintainer": "Daniel Crouch <djmcrouch@gmail.com>",
    "author": "Daniel J.M. Crouch, Graham H.M. Goddard & Cathryn M. Lewis",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=REGENT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REGENT Risk Estimation for Genetic and Environmental Traits Produces population distribution of disease risk and statistical risk categories, and predicts risks for individuals with genotype information.  "
  },
  {
    "id": 6176,
    "package_name": "REN",
    "title": "Regularization Ensemble for Robust Portfolio Optimization",
    "description": "Portfolio optimization is achieved through a combination of regularization techniques and ensemble methods that are designed to generate stable out-of-sample return predictions, particularly in the presence of strong correlations among assets. The package includes functions for data preparation, parallel processing, and portfolio analysis using methods such as Mean-Variance, James-Stein, LASSO, Ridge Regression, and Equal Weighting. It also provides visualization tools and performance metrics, such as the Sharpe ratio, volatility, and maximum drawdown, to assess the results.",
    "version": "0.1.0",
    "maintainer": "Bonsoo Koo <bonsoo.koo@monash.edu>",
    "author": "Hardik Dixit [aut],\n  Shijia Wang [aut],\n  Bonsoo Koo [aut, cre],\n  Cash Looi [aut],\n  Hong Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=REN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REN Regularization Ensemble for Robust Portfolio Optimization Portfolio optimization is achieved through a combination of regularization techniques and ensemble methods that are designed to generate stable out-of-sample return predictions, particularly in the presence of strong correlations among assets. The package includes functions for data preparation, parallel processing, and portfolio analysis using methods such as Mean-Variance, James-Stein, LASSO, Ridge Regression, and Equal Weighting. It also provides visualization tools and performance metrics, such as the Sharpe ratio, volatility, and maximum drawdown, to assess the results.  "
  },
  {
    "id": 6204,
    "package_name": "RGAP",
    "title": "Production Function Output Gap Estimation",
    "description": "The output gap indicates the percentage difference between the actual output of an economy and its potential. Since potential output is a latent process, the estimation of the output gap poses a challenge and numerous filtering techniques have been proposed. 'RGAP' facilitates the estimation of a Cobb-Douglas production function type output gap, as suggested by the European Commission (Havik et al. 2014) <https://ideas.repec.org/p/euf/ecopap/0535.html>. To that end, the non-accelerating wage rate of unemployment (NAWRU) and the trend of total factor productivity (TFP) can be estimated in two bivariate unobserved component models by means of Kalman filtering and smoothing. 'RGAP' features a flexible modeling framework for the appropriate state-space models and offers frequentist as well as Bayesian estimation techniques. Additional functionalities include direct access to the 'AMECO' <https://economy-finance.ec.europa.eu/economic-research-and-databases/economic-databases/ameco-database_en> database and automated model selection procedures. See the paper by Streicher (2022) <http://hdl.handle.net/20.500.11850/552089> for details. ",
    "version": "0.1.1",
    "maintainer": "Sina Streicher <streicher@kof.ethz.ch>",
    "author": "Sina Streicher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7848-1842>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RGAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RGAP Production Function Output Gap Estimation The output gap indicates the percentage difference between the actual output of an economy and its potential. Since potential output is a latent process, the estimation of the output gap poses a challenge and numerous filtering techniques have been proposed. 'RGAP' facilitates the estimation of a Cobb-Douglas production function type output gap, as suggested by the European Commission (Havik et al. 2014) <https://ideas.repec.org/p/euf/ecopap/0535.html>. To that end, the non-accelerating wage rate of unemployment (NAWRU) and the trend of total factor productivity (TFP) can be estimated in two bivariate unobserved component models by means of Kalman filtering and smoothing. 'RGAP' features a flexible modeling framework for the appropriate state-space models and offers frequentist as well as Bayesian estimation techniques. Additional functionalities include direct access to the 'AMECO' <https://economy-finance.ec.europa.eu/economic-research-and-databases/economic-databases/ameco-database_en> database and automated model selection procedures. See the paper by Streicher (2022) <http://hdl.handle.net/20.500.11850/552089> for details.   "
  },
  {
    "id": 6272,
    "package_name": "RM2006",
    "title": "RiskMetrics 2006 Methodology",
    "description": "Estimation of the conditional covariance matrix using the RiskMetrics 2006 methodology of Zumbach (2007) <doi:10.2139/ssrn.1420185>.",
    "version": "0.1.1",
    "maintainer": "Carlos Trucios <ctrucios@gmail.com>",
    "author": "Carlos Trucios",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RM2006",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RM2006 RiskMetrics 2006 Methodology Estimation of the conditional covariance matrix using the RiskMetrics 2006 methodology of Zumbach (2007) <doi:10.2139/ssrn.1420185>.  "
  },
  {
    "id": 6283,
    "package_name": "RMOPI",
    "title": "Risk Management and Optimization for Portfolio Investment",
    "description": "Provides functions for risk management and portfolio investment of securities with practical tools for data processing and plotting. Moreover, it contains functions which perform the COS Method, an option pricing method based on the Fourier-cosine series (Fang, F. (2008) <doi:10.1137/080718061>).",
    "version": "1.1",
    "maintainer": "Wei Ling <lingwei3418@163.com>",
    "author": "Wei Ling [aut, cre],\n  Yang Liu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RMOPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMOPI Risk Management and Optimization for Portfolio Investment Provides functions for risk management and portfolio investment of securities with practical tools for data processing and plotting. Moreover, it contains functions which perform the COS Method, an option pricing method based on the Fourier-cosine series (Fang, F. (2008) <doi:10.1137/080718061>).  "
  },
  {
    "id": 6308,
    "package_name": "RND",
    "title": "Risk Neutral Density Extraction Package",
    "description": "Extract the implied risk neutral density from options using various methods.",
    "version": "1.2",
    "maintainer": "Kam Hamidieh <khamidieh@gmail.com>",
    "author": "Kam Hamidieh <khamidieh@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RND",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RND Risk Neutral Density Extraction Package Extract the implied risk neutral density from options using various methods.  "
  },
  {
    "id": 6371,
    "package_name": "RQuantLib",
    "title": "R Interface to the 'QuantLib' Library",
    "description": "The 'RQuantLib' package makes parts of 'QuantLib' accessible from R\n The 'QuantLib' project aims to provide a comprehensive software framework\n for quantitative finance. The goal is to provide a standard open source library\n for quantitative analysis, modeling, trading, and risk management of financial\n assets.",
    "version": "0.4.26",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Khanh Nguyen [aut] (2009-2010),\n  Terry Leitch [aut] (ORCID: <https://orcid.org/0000-0003-1706-4070>,\n    since 2016)",
    "url": "https://github.com/eddelbuettel/rquantlib,\nhttps://dirk.eddelbuettel.com/code/rquantlib.html",
    "bug_reports": "https://github.com/eddelbuettel/rquantlib/issues",
    "repository": "https://cran.r-project.org/package=RQuantLib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RQuantLib R Interface to the 'QuantLib' Library The 'RQuantLib' package makes parts of 'QuantLib' accessible from R\n The 'QuantLib' project aims to provide a comprehensive software framework\n for quantitative finance. The goal is to provide a standard open source library\n for quantitative analysis, modeling, trading, and risk management of financial\n assets.  "
  },
  {
    "id": 6394,
    "package_name": "RSDC",
    "title": "Regime-Switching Dynamic Correlation Models",
    "description": "Estimation, forecasting, simulation, and portfolio construction for \n    regime-switching models with exogenous variables as in \n    Pelletier (2006) <doi:10.1016/j.jeconom.2005.01.013>.",
    "version": "1.1-2",
    "maintainer": "David Ardia <david.ardia.ch@gmail.com>",
    "author": "David Ardia [aut, cre] (ORCID: <https://orcid.org/0000-0003-2823-782X>),\n  Benjamin Seguin [aut]",
    "url": "https://github.com/ArdiaD/RSDC",
    "bug_reports": "https://github.com/ArdiaD/RSDC/issues",
    "repository": "https://cran.r-project.org/package=RSDC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSDC Regime-Switching Dynamic Correlation Models Estimation, forecasting, simulation, and portfolio construction for \n    regime-switching models with exogenous variables as in \n    Pelletier (2006) <doi:10.1016/j.jeconom.2005.01.013>.  "
  },
  {
    "id": 6421,
    "package_name": "RSurveillance",
    "title": "Design and Analysis of Disease Surveillance Activities",
    "description": "A range of functions for the design and\n    analysis of disease surveillance activities. These functions were\n    originally developed for animal health surveillance activities but can be\n    equally applied to aquatic animal, wildlife, plant and human health\n    surveillance activities. Utilities are included for sample size calculation\n    and analysis of representative surveys for disease freedom, risk-based\n    studies for disease freedom and for prevalence estimation.\n    This package is based on Cameron A., Conraths F., Frohlich A., Schauer B.,\n    Schulz K., Sergeant E., Sonnenburg J., Staubach C. (2015). R package of \n    functions for risk-based surveillance. Deliverable 6.24, WP 6 - Decision \n    making tools for implementing risk-based surveillance, Grant Number \n    no. 310806, RISKSUR (<https://www.fp7-risksur.eu/sites/default/files/documents/Deliverables/RISKSUR_%28310806%29_D6.24.pdf>). \n    Many of the 'RSurveillance' functions are incorporated into the 'epitools'\n    website: Sergeant, ESG, 2019. Epitools epidemiological calculators. \n    Ausvet Pty Ltd. Available at: <http://epitools.ausvet.com.au>.",
    "version": "0.2.1",
    "maintainer": "Rohan Sadler <rohan.sadler@ausvet.com.au>",
    "author": "Evan Sergeant",
    "url": "https://github.com/roStats/RSurveillance",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSurveillance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSurveillance Design and Analysis of Disease Surveillance Activities A range of functions for the design and\n    analysis of disease surveillance activities. These functions were\n    originally developed for animal health surveillance activities but can be\n    equally applied to aquatic animal, wildlife, plant and human health\n    surveillance activities. Utilities are included for sample size calculation\n    and analysis of representative surveys for disease freedom, risk-based\n    studies for disease freedom and for prevalence estimation.\n    This package is based on Cameron A., Conraths F., Frohlich A., Schauer B.,\n    Schulz K., Sergeant E., Sonnenburg J., Staubach C. (2015). R package of \n    functions for risk-based surveillance. Deliverable 6.24, WP 6 - Decision \n    making tools for implementing risk-based surveillance, Grant Number \n    no. 310806, RISKSUR (<https://www.fp7-risksur.eu/sites/default/files/documents/Deliverables/RISKSUR_%28310806%29_D6.24.pdf>). \n    Many of the 'RSurveillance' functions are incorporated into the 'epitools'\n    website: Sergeant, ESG, 2019. Epitools epidemiological calculators. \n    Ausvet Pty Ltd. Available at: <http://epitools.ausvet.com.au>.  "
  },
  {
    "id": 6425,
    "package_name": "RTFA",
    "title": "Robust Factor Analysis for Tensor Time Series",
    "description": "Tensor Factor Models (TFM) are appealing dimension reduction tools for high-order tensor time series, and have wide applications in economics, finance and medical imaging. We propose an one-step projection estimator by minimizing the least-square loss function, and further propose a robust estimator with an iterative weighted projection technique by utilizing the Huber loss function. The methods are discussed in Barigozzi et al. (2022) <arXiv:2206.09800>, and Barigozzi et al. (2023) <arXiv:2303.18163>.",
    "version": "0.1.0",
    "maintainer": "Lingxiao Li <lilingxiao@mail.sdu.edu.cn>",
    "author": "Matteo Barigozzi [aut],\n  Yong He [aut],\n  Lorenzo Trapani [aut],\n  Lingxiao Li [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RTFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTFA Robust Factor Analysis for Tensor Time Series Tensor Factor Models (TFM) are appealing dimension reduction tools for high-order tensor time series, and have wide applications in economics, finance and medical imaging. We propose an one-step projection estimator by minimizing the least-square loss function, and further propose a robust estimator with an iterative weighted projection technique by utilizing the Huber loss function. The methods are discussed in Barigozzi et al. (2022) <arXiv:2206.09800>, and Barigozzi et al. (2023) <arXiv:2303.18163>.  "
  },
  {
    "id": 6426,
    "package_name": "RTL",
    "title": "Risk Tool Library - Trading, Risk, Analytics for Commodities",
    "description": "A toolkit for Commodities 'analytics', risk management and\n    trading professionals. Includes functions for API calls to\n    <https://commodities.morningstar.com/#/>, <https://developer.genscape.com/>,\n    and <https://www.bankofcanada.ca/valet/docs>.",
    "version": "1.3.7",
    "maintainer": "Philippe Cote <pcote@ualberta.ca>",
    "author": "Philippe Cote [aut, cre],\n  Nima Safaian [aut]",
    "url": "https://github.com/risktoollib/RTL",
    "bug_reports": "https://github.com/risktoollib/RTL/issues",
    "repository": "https://cran.r-project.org/package=RTL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTL Risk Tool Library - Trading, Risk, Analytics for Commodities A toolkit for Commodities 'analytics', risk management and\n    trading professionals. Includes functions for API calls to\n    <https://commodities.morningstar.com/#/>, <https://developer.genscape.com/>,\n    and <https://www.bankofcanada.ca/valet/docs>.  "
  },
  {
    "id": 6434,
    "package_name": "RTransferEntropy",
    "title": "Measuring Information Flow Between Time Series with Shannon and\nRenyi Transfer Entropy",
    "description": "Measuring information flow between time series with Shannon and R\u00e9nyi transfer entropy. See also Dimpfl and Peter (2013) <doi:10.1515/snde-2012-0044> and Dimpfl and Peter (2014) <doi:10.1016/j.intfin.2014.03.004> for theory and applications to financial time series. Additional references can be found in the theory part of the vignette.",
    "version": "0.2.21",
    "maintainer": "David Zimmermann <david_j_zimmermann@hotmail.com>",
    "author": "David Zimmermann [aut, cre],\n  Simon Behrendt [aut],\n  Thomas Dimpfl [aut],\n  Franziska Peter [aut]",
    "url": "https://github.com/BZPaper/RTransferEntropy",
    "bug_reports": "https://github.com/BZPaper/RTransferEntropy/issues",
    "repository": "https://cran.r-project.org/package=RTransferEntropy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTransferEntropy Measuring Information Flow Between Time Series with Shannon and\nRenyi Transfer Entropy Measuring information flow between time series with Shannon and R\u00e9nyi transfer entropy. See also Dimpfl and Peter (2013) <doi:10.1515/snde-2012-0044> and Dimpfl and Peter (2014) <doi:10.1016/j.intfin.2014.03.004> for theory and applications to financial time series. Additional references can be found in the theory part of the vignette.  "
  },
  {
    "id": 6525,
    "package_name": "RcmdrPlugin.EBM",
    "title": "Rcmdr Evidence Based Medicine Plug-in Package",
    "description": "Rcmdr plug-in GUI extension for Evidence Based Medicine medical indicators calculations (Sensitivity, specificity, absolute risk reduction, relative risk, ...).",
    "version": "1.0-10",
    "maintainer": "Daniel-Corneliu Leucuta <danny.ldc@gmail.com>",
    "author": "Daniel-Corneliu Leucuta <danny.ldc@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.EBM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.EBM Rcmdr Evidence Based Medicine Plug-in Package Rcmdr plug-in GUI extension for Evidence Based Medicine medical indicators calculations (Sensitivity, specificity, absolute risk reduction, relative risk, ...).  "
  },
  {
    "id": 6537,
    "package_name": "RcmdrPlugin.RiskDemo",
    "title": "R Commander Plug-in for Risk Demonstration",
    "description": "R Commander plug-in to demonstrate various actuarial and financial risks. It includes valuation of bonds and stocks, portfolio optimization, classical ruin theory, demography and epidemic. ",
    "version": "3.2",
    "maintainer": "Arto Luoma <arto.luoma@wippies.com>",
    "author": "Arto Luoma",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.RiskDemo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.RiskDemo R Commander Plug-in for Risk Demonstration R Commander plug-in to demonstrate various actuarial and financial risks. It includes valuation of bonds and stocks, portfolio optimization, classical ruin theory, demography and epidemic.   "
  },
  {
    "id": 6570,
    "package_name": "RcppDist",
    "title": "'Rcpp' Integration of Additional Probability Distributions",
    "description": "The 'Rcpp' package provides a C++ library to make it easier\n    to use C++ with R. R and 'Rcpp' provide functions for a variety of\n    statistical distributions. Several R packages make functions\n    available to R for additional statistical distributions. However,\n    to access these functions from C++ code, a costly call to the R\n    functions must be made. 'RcppDist' provides a header-only C++ library\n    with functions for additional statistical distributions that can be\n    called from C++ when writing code using 'Rcpp' or 'RcppArmadillo'.\n    Functions are available that return a 'NumericVector' as well as\n    doubles, and for multivariate or matrix distributions, 'Armadillo'\n    vectors and matrices. 'RcppDist' provides functions for the following\n    distributions: the four parameter beta distribution; the location-\n    scale t distribution; the truncated normal distribution; the\n    truncated t distribution; a truncated location-scale t distribution;\n    the triangle distribution; the multivariate normal distribution*;\n    the multivariate t distribution*; the Wishart distribution*; and\n    the inverse Wishart distribution*. Distributions marked with an\n    asterisk rely on 'RcppArmadillo'.",
    "version": "0.1.1.1",
    "maintainer": "JB Duck-Mayr <j.duckmayr@gmail.com>",
    "author": "JB Duck-Mayr [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2231-1294>)",
    "url": "https://github.com/duckmayr/RcppDist",
    "bug_reports": "https://github.com/duckmayr/RcppDist/issues",
    "repository": "https://cran.r-project.org/package=RcppDist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppDist 'Rcpp' Integration of Additional Probability Distributions The 'Rcpp' package provides a C++ library to make it easier\n    to use C++ with R. R and 'Rcpp' provide functions for a variety of\n    statistical distributions. Several R packages make functions\n    available to R for additional statistical distributions. However,\n    to access these functions from C++ code, a costly call to the R\n    functions must be made. 'RcppDist' provides a header-only C++ library\n    with functions for additional statistical distributions that can be\n    called from C++ when writing code using 'Rcpp' or 'RcppArmadillo'.\n    Functions are available that return a 'NumericVector' as well as\n    doubles, and for multivariate or matrix distributions, 'Armadillo'\n    vectors and matrices. 'RcppDist' provides functions for the following\n    distributions: the four parameter beta distribution; the location-\n    scale t distribution; the truncated normal distribution; the\n    truncated t distribution; a truncated location-scale t distribution;\n    the triangle distribution; the multivariate normal distribution*;\n    the multivariate t distribution*; the Wishart distribution*; and\n    the inverse Wishart distribution*. Distributions marked with an\n    asterisk rely on 'RcppArmadillo'.  "
  },
  {
    "id": 6650,
    "package_name": "RegAssure",
    "title": "Streamlined Integration of Regression Assumption",
    "description": "It streamlines the evaluation of regression model\n    assumptions, enhancing result reliability. With integrated tools for\n    assessing key aspects like linearity, homoscedasticity, and more. It's\n    a valuable asset for researchers and analysts working with regression\n    models.",
    "version": "1.0.0",
    "maintainer": "Nicol\u00e1s Rubio Garc\u00eda <nrubiogar@gmail.com>",
    "author": "Nicol\u00e1s Rubio Garc\u00eda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3502-0784>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RegAssure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RegAssure Streamlined Integration of Regression Assumption It streamlines the evaluation of regression model\n    assumptions, enhancing result reliability. With integrated tools for\n    assessing key aspects like linearity, homoscedasticity, and more. It's\n    a valuable asset for researchers and analysts working with regression\n    models.  "
  },
  {
    "id": 6716,
    "package_name": "Riex",
    "title": "IEX Stocks and Market Data",
    "description": "Retrieves efficiently and reliably Investors Exchange ('IEX') stock and market data using 'IEX Cloud API'. The platform is offered by Investors Exchange Group (IEX Group).\n    Main goal is to leverage 'R' capabilities including existing packages to effectively provide financial and statistical analysis as well as visualization in support of fact-based decisions.\n    In addition, continuously improve and enhance 'Riex' by applying best practices and being in tune with users' feedback and requirements.\n    Please, make sure to review and acknowledge Investors Exchange Group (IEX Group) terms and conditions before using 'Riex' (<https://iexcloud.io/terms/>).",
    "version": "1.0.2",
    "maintainer": "Myriam Ibrahim <ibrahimmyriam7@gmail.com>",
    "author": "Myriam Ibrahim [aut, cre]",
    "url": "https://github.com/TheEliteAnalyst/Riex",
    "bug_reports": "https://github.com/TheEliteAnalyst/Riex/issues",
    "repository": "https://cran.r-project.org/package=Riex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Riex IEX Stocks and Market Data Retrieves efficiently and reliably Investors Exchange ('IEX') stock and market data using 'IEX Cloud API'. The platform is offered by Investors Exchange Group (IEX Group).\n    Main goal is to leverage 'R' capabilities including existing packages to effectively provide financial and statistical analysis as well as visualization in support of fact-based decisions.\n    In addition, continuously improve and enhance 'Riex' by applying best practices and being in tune with users' feedback and requirements.\n    Please, make sure to review and acknowledge Investors Exchange Group (IEX Group) terms and conditions before using 'Riex' (<https://iexcloud.io/terms/>).  "
  },
  {
    "id": 6720,
    "package_name": "Risk",
    "title": "Computes 26 Financial Risk Measures for Any Continuous\nDistribution",
    "description": "Computes 26 financial risk measures for any continuous distribution.  The 26 financial risk measures  include value at risk, expected shortfall due to Artzner et al. (1999) <DOI:10.1007/s10957-011-9968-2>, tail conditional median due to Kou et al. (2013) <DOI:10.1287/moor.1120.0577>, expectiles due to Newey and Powell (1987) <DOI:10.2307/1911031>, beyond value at risk due to Longin (2001) <DOI:10.3905/jod.2001.319161>, expected proportional shortfall due to Belzunce et al. (2012) <DOI:10.1016/j.insmatheco.2012.05.003>, elementary risk measure due to Ahmadi-Javid (2012) <DOI:10.1007/s10957-011-9968-2>, omega due to Shadwick and Keating (2002), sortino ratio due to Rollinger and Hoffman (2013), kappa  due to Kaplan and Knowles  (2004), Wang (1998)'s <DOI:10.1080/10920277.1998.10595708> risk measures, Stone (1973)'s <DOI:10.2307/2978638> risk measures, Luce (1980)'s <DOI:10.1007/BF00135033> risk measures, Sarin (1987)'s <DOI:10.1007/BF00126387> risk measures, Bronshtein and Kurelenkova (2009)'s risk measures.",
    "version": "1.0",
    "maintainer": "Saralees Nadarajah <mbbsssn2@manchester.ac.uk>",
    "author": "Saralees Nadarajah, Stephen Chan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Risk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Risk Computes 26 Financial Risk Measures for Any Continuous\nDistribution Computes 26 financial risk measures for any continuous distribution.  The 26 financial risk measures  include value at risk, expected shortfall due to Artzner et al. (1999) <DOI:10.1007/s10957-011-9968-2>, tail conditional median due to Kou et al. (2013) <DOI:10.1287/moor.1120.0577>, expectiles due to Newey and Powell (1987) <DOI:10.2307/1911031>, beyond value at risk due to Longin (2001) <DOI:10.3905/jod.2001.319161>, expected proportional shortfall due to Belzunce et al. (2012) <DOI:10.1016/j.insmatheco.2012.05.003>, elementary risk measure due to Ahmadi-Javid (2012) <DOI:10.1007/s10957-011-9968-2>, omega due to Shadwick and Keating (2002), sortino ratio due to Rollinger and Hoffman (2013), kappa  due to Kaplan and Knowles  (2004), Wang (1998)'s <DOI:10.1080/10920277.1998.10595708> risk measures, Stone (1973)'s <DOI:10.2307/2978638> risk measures, Luce (1980)'s <DOI:10.1007/BF00135033> risk measures, Sarin (1987)'s <DOI:10.1007/BF00126387> risk measures, Bronshtein and Kurelenkova (2009)'s risk measures.  "
  },
  {
    "id": 6722,
    "package_name": "RiskPortfolios",
    "title": "Computation of Risk-Based Portfolios",
    "description": "Collection of functions designed to compute risk-based portfolios as described \n    in Ardia et al. (2017) <doi:10.1007/s10479-017-2474-7> and Ardia et al. (2017) <doi:10.21105/joss.00171>.",
    "version": "2.1.7",
    "maintainer": "David Ardia <david.ardia.ch@gmail.com>",
    "author": "David Ardia [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2823-782X>),\n  Kris Boudt [aut],\n  Jean-Philippe Gagnon-Fleury [aut]",
    "url": "https://github.com/ArdiaD/RiskPortfolios",
    "bug_reports": "https://github.com/ArdiaD/RiskPortfolios/issues",
    "repository": "https://cran.r-project.org/package=RiskPortfolios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RiskPortfolios Computation of Risk-Based Portfolios Collection of functions designed to compute risk-based portfolios as described \n    in Ardia et al. (2017) <doi:10.1007/s10479-017-2474-7> and Ardia et al. (2017) <doi:10.21105/joss.00171>.  "
  },
  {
    "id": 6723,
    "package_name": "RiskScorescvd",
    "title": "Cardiovascular Risk Scores Calculator",
    "description": "A tool to calculate Cardiovascular Risk Scores in large data frames as published in Perez-Vicencio, et al (2024) <doi:10.1136/openhrt-2024-002755>. Cardiovascular risk scores are statistical tools used to assess an individual's likelihood of developing a cardiovascular disease based on various risk factors, such as age, gender, blood pressure, cholesterol levels, and smoking. Here we bring together the six most commonly used in the emergency department. Using 'RiskScorescvd', you can calculate all the risk scores in an extended dataset in seconds. PCE (ASCVD) described in Goff, et al (2013) <doi:10.1161/01.cir.0000437741.48606.98>. EDACS described in Mark DG, et al (2016) <doi:10.1016/j.jacc.2017.11.064>. GRACE described in Fox KA, et al (2006) <doi:10.1136/bmj.38985.646481.55>. HEART is described in Mahler SA, et al (2017) <doi:10.1016/j.clinbiochem.2017.01.003>. SCORE2/OP described in SCORE2 working group and ESC Cardiovascular risk collaboration (2021) <doi:10.1093/eurheartj/ehab309>. TIMI described in Antman EM, et al (2000) <doi:10.1001/jama.284.7.835>. SCORE2-Diabetes described in SCORE2-Diabetes working group and ESC Cardiovascular risk collaboration (2023) <doi:10.1093/eurheartj/ehab260>. SCORE2/OP with CKD add-on described in Kunihiro M et al (2022) <doi:10.1093/eurjpc/zwac176>.   ",
    "version": "0.3.1",
    "maintainer": "Daniel Perez-Vicencio <dvicencio947@gmail.com>",
    "author": "Daniel Perez-Vicencio [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2903-1129>),\n  Dimitrios Doudesis [aut],\n  Alexander JF Thurston [aut],\n  Jeremy Selva [aut] (ORCID: <https://orcid.org/0000-0002-4498-2662>)",
    "url": "https://github.com/dvicencio/RiskScorescvd,\nhttps://dvicencio.github.io/RiskScorescvd/",
    "bug_reports": "https://github.com/dvicencio/RiskScorescvd/issues",
    "repository": "https://cran.r-project.org/package=RiskScorescvd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RiskScorescvd Cardiovascular Risk Scores Calculator A tool to calculate Cardiovascular Risk Scores in large data frames as published in Perez-Vicencio, et al (2024) <doi:10.1136/openhrt-2024-002755>. Cardiovascular risk scores are statistical tools used to assess an individual's likelihood of developing a cardiovascular disease based on various risk factors, such as age, gender, blood pressure, cholesterol levels, and smoking. Here we bring together the six most commonly used in the emergency department. Using 'RiskScorescvd', you can calculate all the risk scores in an extended dataset in seconds. PCE (ASCVD) described in Goff, et al (2013) <doi:10.1161/01.cir.0000437741.48606.98>. EDACS described in Mark DG, et al (2016) <doi:10.1016/j.jacc.2017.11.064>. GRACE described in Fox KA, et al (2006) <doi:10.1136/bmj.38985.646481.55>. HEART is described in Mahler SA, et al (2017) <doi:10.1016/j.clinbiochem.2017.01.003>. SCORE2/OP described in SCORE2 working group and ESC Cardiovascular risk collaboration (2021) <doi:10.1093/eurheartj/ehab309>. TIMI described in Antman EM, et al (2000) <doi:10.1001/jama.284.7.835>. SCORE2-Diabetes described in SCORE2-Diabetes working group and ESC Cardiovascular risk collaboration (2023) <doi:10.1093/eurheartj/ehab260>. SCORE2/OP with CKD add-on described in Kunihiro M et al (2022) <doi:10.1093/eurjpc/zwac176>.     "
  },
  {
    "id": 6764,
    "package_name": "RobPC",
    "title": "Robust Panel Clustering Algorithm",
    "description": "Performs both classical and robust panel clustering by applying Principal Component Analysis (PCA) for dimensionality reduction and clustering via standard K-Means or Trimmed K-Means. The method is designed to ensure stable and reliable clustering, even in the presence of outliers. Suitable for analyzing panel data in domains such as economic research, financial time-series, healthcare analytics, and social sciences. The package allows users to choose between classical K-Means for standard clustering and Trimmed K-Means for robust clustering, making it a flexible tool for various applications. For this package, we have benefited from the studies Rencher (2003), Wang and Lu (2021) <DOI:10.25236/AJBM.2021.031018>, Cuesta-Albertos et al. (1997) <https://www.jstor.org/stable/2242558?seq=1>.",
    "version": "1.4",
    "maintainer": "Hasan Bulut <hasan.bulut@omu.edu.tr>",
    "author": "Hasan Bulut [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobPC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobPC Robust Panel Clustering Algorithm Performs both classical and robust panel clustering by applying Principal Component Analysis (PCA) for dimensionality reduction and clustering via standard K-Means or Trimmed K-Means. The method is designed to ensure stable and reliable clustering, even in the presence of outliers. Suitable for analyzing panel data in domains such as economic research, financial time-series, healthcare analytics, and social sciences. The package allows users to choose between classical K-Means for standard clustering and Trimmed K-Means for robust clustering, making it a flexible tool for various applications. For this package, we have benefited from the studies Rencher (2003), Wang and Lu (2021) <DOI:10.25236/AJBM.2021.031018>, Cuesta-Albertos et al. (1997) <https://www.jstor.org/stable/2242558?seq=1>.  "
  },
  {
    "id": 6771,
    "package_name": "RobinHood",
    "title": "Interface for the RobinHood.com No Commission Investing Platform",
    "description": "Execute API calls to the RobinHood <https://robinhood.com> investing platform. Functionality includes accessing account data and current holdings, retrieving investment statistics and quotes, placing and canceling orders, getting market trading hours, searching investments by popular tag, and interacting with watch lists.",
    "version": "1.7.0",
    "maintainer": "Joseph Blubaugh <jestonblu@gmail.com>",
    "author": "Joseph Blubaugh",
    "url": "https://github.com/JestonBlu/RobinHood",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobinHood",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobinHood Interface for the RobinHood.com No Commission Investing Platform Execute API calls to the RobinHood <https://robinhood.com> investing platform. Functionality includes accessing account data and current holdings, retrieving investment statistics and quotes, placing and canceling orders, getting market trading hours, searching investments by popular tag, and interacting with watch lists.  "
  },
  {
    "id": 6788,
    "package_name": "RolWinWavCor",
    "title": "Estimate Rolling Window Wavelet Correlation Between Two Time\nSeries",
    "description": "Estimates and plots as a heat map the rolling window wavelet correlation (RWWC) coefficients statistically significant (within the 95% CI) between two regular (evenly spaced) time series. 'RolWinWavCor' also plots at the same graphic the time series under study. The 'RolWinWavCor' was designed for financial time series, but this software can be used with other kinds of data (e.g., climatic, ecological, geological, etc). The functions contained in 'RolWinWavCor' are highly flexible since these contains some parameters to personalize the time series under analysis and the heat maps of the rolling window wavelet correlation coefficients. Moreover, we have also included a data set (named EU_stock_markets) that contains nine European stock market indices to exemplify the use of the functions contained in 'RolWinWavCor'. Methods derived from Polanco-Mart\u00ednez et al (2018) <doi:10.1016/j.physa.2017.08.065>). ",
    "version": "0.4.0",
    "maintainer": "Josu\u00e9 M. Polanco-Mart\u00ednez <josue.m.polanco@gmail.com>",
    "author": "Josu\u00e9 M. Polanco-Mart\u00ednez [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-7164-0185>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RolWinWavCor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RolWinWavCor Estimate Rolling Window Wavelet Correlation Between Two Time\nSeries Estimates and plots as a heat map the rolling window wavelet correlation (RWWC) coefficients statistically significant (within the 95% CI) between two regular (evenly spaced) time series. 'RolWinWavCor' also plots at the same graphic the time series under study. The 'RolWinWavCor' was designed for financial time series, but this software can be used with other kinds of data (e.g., climatic, ecological, geological, etc). The functions contained in 'RolWinWavCor' are highly flexible since these contains some parameters to personalize the time series under analysis and the heat maps of the rolling window wavelet correlation coefficients. Moreover, we have also included a data set (named EU_stock_markets) that contains nine European stock market indices to exemplify the use of the functions contained in 'RolWinWavCor'. Methods derived from Polanco-Mart\u00ednez et al (2018) <doi:10.1016/j.physa.2017.08.065>).   "
  },
  {
    "id": 6808,
    "package_name": "Rprofet",
    "title": "WOE Transformation and Scorecard Builder",
    "description": "Performs all steps in the credit scoring process. This package allows the user to follow all the necessary steps for building an effective scorecard. It provides the user functions for coarse binning of variables, Weights of Evidence (WOE) transformation, variable clustering, custom binning, visualization, and scaling of logistic regression coefficients. The results will generate a scorecard that can be used as an effective credit scoring tool to evaluate risk. For complete details on the credit scoring process, see Siddiqi (2005, ISBN:047175451X).  ",
    "version": "3.1.1",
    "maintainer": "Thomas Brandenburger <thomas.brandenburger@sdstate.edu>",
    "author": "Thomas Brandenburger [cre],\n  Eric Stratman [aut],\n  Krystal Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rprofet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rprofet WOE Transformation and Scorecard Builder Performs all steps in the credit scoring process. This package allows the user to follow all the necessary steps for building an effective scorecard. It provides the user functions for coarse binning of variables, Weights of Evidence (WOE) transformation, variable clustering, custom binning, visualization, and scaling of logistic regression coefficients. The results will generate a scorecard that can be used as an effective credit scoring tool to evaluate risk. For complete details on the credit scoring process, see Siddiqi (2005, ISBN:047175451X).    "
  },
  {
    "id": 6833,
    "package_name": "Rtauchen",
    "title": "Discretization of AR(1) Processes",
    "description": "Discretize AR(1) process following Tauchen (1986) <http://www.sciencedirect.com/science/article/pii/0165176586901680>. A discrete Markov chain that approximates in the sense of weak convergence a continuous-valued univariate Autoregressive process of first order is generated. It is a popular method used in economics and in finance. ",
    "version": "1.0",
    "maintainer": "David Zarruk Valencia <davidzarruk@gmail.com>",
    "author": "David Zarruk Valencia & Rodrigo Azuero Melo",
    "url": "https://github.com/davidzarruk/Rtauchen",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rtauchen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rtauchen Discretization of AR(1) Processes Discretize AR(1) process following Tauchen (1986) <http://www.sciencedirect.com/science/article/pii/0165176586901680>. A discrete Markov chain that approximates in the sense of weak convergence a continuous-valued univariate Autoregressive process of first order is generated. It is a popular method used in economics and in finance.   "
  },
  {
    "id": 6854,
    "package_name": "SACCR",
    "title": "SA Counterparty Credit Risk under CRR2",
    "description": "Computes the Exposure-At-Default based on  the standardized approach\n    of CRR2 (SA-CCR). The simplified version of SA-CCR has been included, as well as the OEM methodology.\n\tMultiple trade types of all the five major asset classes are being supported including the Other Exposure and, given the inheritance-\n    based structure of the application, the addition of further trade types\n    is straightforward. The application returns a list of trees per Counterparty and CSA after\n    automatically separating the trades based on the Counterparty, the CSAs, the hedging sets, the\n    netting sets and the risk factors. The basis and volatility transactions are\n    also identified and treated in specific hedging sets whereby the corresponding \n    penalty factors are applied. All the examples appearing on the\n    regulatory papers (both for the margined and the unmargined workflow) have been\n    implemented including the latest CRR2 developments.",
    "version": "3.3",
    "maintainer": "Tasos Grivas <info@openriskcalculator.com>",
    "author": "Tasos Grivas [aut, cre]",
    "url": "https://openriskcalculator.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SACCR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SACCR SA Counterparty Credit Risk under CRR2 Computes the Exposure-At-Default based on  the standardized approach\n    of CRR2 (SA-CCR). The simplified version of SA-CCR has been included, as well as the OEM methodology.\n\tMultiple trade types of all the five major asset classes are being supported including the Other Exposure and, given the inheritance-\n    based structure of the application, the addition of further trade types\n    is straightforward. The application returns a list of trees per Counterparty and CSA after\n    automatically separating the trades based on the Counterparty, the CSAs, the hedging sets, the\n    netting sets and the risk factors. The basis and volatility transactions are\n    also identified and treated in specific hedging sets whereby the corresponding \n    penalty factors are applied. All the examples appearing on the\n    regulatory papers (both for the margined and the unmargined workflow) have been\n    implemented including the latest CRR2 developments.  "
  },
  {
    "id": 6860,
    "package_name": "SAFEPG",
    "title": "A Novel SAFE Model for Predicting Climate-Related Extreme Losses",
    "description": "The goal of 'SAFEPG' is to predict climate-related extreme losses by fitting a frequency-severity model. It improves predictive performance by introducing a sign-aligned regularization term, which ensures consistent signs for the coefficients across the frequency and severity components. This enhancement not only increases model accuracy but also enhances its interpretability, making it more suitable for practical applications in risk assessment.",
    "version": "0.0.1",
    "maintainer": "Qian Tang <qian-tang@uiowa.edu>",
    "author": "Qian Tang [aut, cre],\n  Yikai Zhang [aut],\n  Boxiang Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SAFEPG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SAFEPG A Novel SAFE Model for Predicting Climate-Related Extreme Losses The goal of 'SAFEPG' is to predict climate-related extreme losses by fitting a frequency-severity model. It improves predictive performance by introducing a sign-aligned regularization term, which ensures consistent signs for the coefficients across the frequency and severity components. This enhancement not only increases model accuracy but also enhances its interpretability, making it more suitable for practical applications in risk assessment.  "
  },
  {
    "id": 6870,
    "package_name": "SAMTx",
    "title": "Sensitivity Assessment to Unmeasured Confounding with Multiple\nTreatments",
    "description": "A sensitivity analysis approach for unmeasured confounding in observational data with multiple treatments and a binary outcome. This approach derives the general bias formula and provides adjusted causal effect estimates in response to various assumptions about the degree of unmeasured confounding. Nested multiple imputation is embedded within the Bayesian framework to integrate   uncertainty about the sensitivity parameters and sampling variability.  Bayesian Additive Regression Model (BART) is used for outcome modeling. The causal estimands are the conditional average treatment effects (CATE) based on the risk difference.  For more details, see paper: Hu L et al. (2020) A flexible sensitivity analysis approach for unmeasured confounding with multiple treatments and a binary outcome with application to SEER-Medicare lung cancer data <arXiv:2012.06093>. ",
    "version": "0.3.0",
    "maintainer": "Jiayi Ji <Jiayi.Ji@mountsinai.org>",
    "author": "Liangyuan Hu [aut],\n  Jungang Zou [aut],\n  Jiayi Ji [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SAMTx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SAMTx Sensitivity Assessment to Unmeasured Confounding with Multiple\nTreatments A sensitivity analysis approach for unmeasured confounding in observational data with multiple treatments and a binary outcome. This approach derives the general bias formula and provides adjusted causal effect estimates in response to various assumptions about the degree of unmeasured confounding. Nested multiple imputation is embedded within the Bayesian framework to integrate   uncertainty about the sensitivity parameters and sampling variability.  Bayesian Additive Regression Model (BART) is used for outcome modeling. The causal estimands are the conditional average treatment effects (CATE) based on the risk difference.  For more details, see paper: Hu L et al. (2020) A flexible sensitivity analysis approach for unmeasured confounding with multiple treatments and a binary outcome with application to SEER-Medicare lung cancer data <arXiv:2012.06093>.   "
  },
  {
    "id": 6873,
    "package_name": "SAMtool",
    "title": "Stock Assessment Methods Toolkit",
    "description": "Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. \n  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, \n  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.",
    "version": "1.9.0",
    "maintainer": "Quang Huynh <quang@bluematterscience.com>",
    "author": "Quang Huynh [aut, cre],\n  Tom Carruthers [aut],\n  Adrian Hordyk [aut]",
    "url": "https://openmse.com, https://samtool.openmse.com,\nhttps://github.com/Blue-Matter/SAMtool",
    "bug_reports": "https://github.com/Blue-Matter/SAMtool/issues",
    "repository": "https://cran.r-project.org/package=SAMtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SAMtool Stock Assessment Methods Toolkit Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. \n  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, \n  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.  "
  },
  {
    "id": 6916,
    "package_name": "SCORNET",
    "title": "Semi-Supervised Calibration of Risk with Noisy Event Times",
    "description": "A consistent, semi-supervised, non-parametric survival curve estimator optimized for efficient use of Electronic Health Record (EHR) data with a limited number of current status labels. See van der Laan and Robins (1997) <doi:10.2307/2670119>.",
    "version": "0.1.1",
    "maintainer": "Yuri Ahuja <Yuri_Ahuja@hms.harvard.edu>",
    "author": "Yuri Ahuja [aut, cre]",
    "url": "https://github.com/celehs/SCORNET",
    "bug_reports": "https://github.com/celehs/SCORNET/issues",
    "repository": "https://cran.r-project.org/package=SCORNET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SCORNET Semi-Supervised Calibration of Risk with Noisy Event Times A consistent, semi-supervised, non-parametric survival curve estimator optimized for efficient use of Electronic Health Record (EHR) data with a limited number of current status labels. See van der Laan and Robins (1997) <doi:10.2307/2670119>.  "
  },
  {
    "id": 6929,
    "package_name": "SDCNway",
    "title": "Tools to Evaluate Disclosure Risk",
    "description": "Tools for calculating disclosure risk measures for microdata,\n    including record-level and file-level measures. The record-level disclosure\n    risk is estimated primarily using exhaustive tabulation. The file-level\n    disclosure risk is estimated by fitting loglinear models on the observed \n    sample counts in cells formed by key variables and their interactions. \n    Funded by the National Center for Education Statistics. See Skinner and\n    Shlomo (2008) <doi:10.1198/016214507000001328> for a description of the\n    file-level risk measures and the loglinear model approach.",
    "version": "1.0.1",
    "maintainer": "John Riddles <JohnRiddles@westat.com>",
    "author": "John Riddles [aut, cre],\n  Westat [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SDCNway",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SDCNway Tools to Evaluate Disclosure Risk Tools for calculating disclosure risk measures for microdata,\n    including record-level and file-level measures. The record-level disclosure\n    risk is estimated primarily using exhaustive tabulation. The file-level\n    disclosure risk is estimated by fitting loglinear models on the observed \n    sample counts in cells formed by key variables and their interactions. \n    Funded by the National Center for Education Statistics. See Skinner and\n    Shlomo (2008) <doi:10.1198/016214507000001328> for a description of the\n    file-level risk measures and the loglinear model approach.  "
  },
  {
    "id": 6945,
    "package_name": "SEERaBomb",
    "title": "SEER and Atomic Bomb Survivor Data Analysis Tools",
    "description": "Creates SEER (Surveillance, Epidemiology and End Results) and \n    A-bomb data binaries from ASCII sources and provides tools for estimating\n    SEER second cancer risks. Methods are described in <doi:10.1038/leu.2015.258>.",
    "version": "2019.2",
    "maintainer": "Tomas Radivoyevitch <radivot@ccf.org>",
    "author": "Tomas Radivoyevitch [aut, cre],\n  R. Molenaar [ctb]",
    "url": "http://epbi-radivot.cwru.edu/SEERaBomb/SEERaBomb.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SEERaBomb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SEERaBomb SEER and Atomic Bomb Survivor Data Analysis Tools Creates SEER (Surveillance, Epidemiology and End Results) and \n    A-bomb data binaries from ASCII sources and provides tools for estimating\n    SEER second cancer risks. Methods are described in <doi:10.1038/leu.2015.258>.  "
  },
  {
    "id": 6999,
    "package_name": "SIRmcmc",
    "title": "Compartmental Susceptible-Infectious-Recovered (SIR) Model of\nCommunity and Household Infection",
    "description": "We build an Susceptible-Infectious-Recovered (SIR) model where the rate of infection is the sum of the household rate and the community rate. We estimate the posterior distribution of the parameters using the Metropolis algorithm. Further details may be found in: F Scott Dahlgren, Ivo M Foppa, Melissa S Stockwell, Celibell Y Vargas, Philip LaRussa, Carrie Reed (2021) \"Household transmission of influenza A and B within a prospective cohort during the 2013-2014 and 2014-2015 seasons\" <doi:10.1002/sim.9181>.",
    "version": "1.1.1",
    "maintainer": "F Scott Dahlgren <fdahlgr@gmail.com>",
    "author": "F Scott Dahlgren and Ivo M Foppa",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SIRmcmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIRmcmc Compartmental Susceptible-Infectious-Recovered (SIR) Model of\nCommunity and Household Infection We build an Susceptible-Infectious-Recovered (SIR) model where the rate of infection is the sum of the household rate and the community rate. We estimate the posterior distribution of the parameters using the Metropolis algorithm. Further details may be found in: F Scott Dahlgren, Ivo M Foppa, Melissa S Stockwell, Celibell Y Vargas, Philip LaRussa, Carrie Reed (2021) \"Household transmission of influenza A and B within a prospective cohort during the 2013-2014 and 2014-2015 seasons\" <doi:10.1002/sim.9181>.  "
  },
  {
    "id": 7052,
    "package_name": "SNPassoc",
    "title": "SNPs-Based Whole Genome Association Studies",
    "description": "Functions to perform most of the common analysis in genome \n    association studies are implemented. These analyses include descriptive \n    statistics and exploratory analysis of missing values, calculation of \n    Hardy-Weinberg equilibrium, analysis of association based on generalized\n    linear models (either for quantitative or binary traits), and analysis\n    of multiple SNPs (haplotype and epistasis analysis). Permutation test \n    and related tests (sum statistic and truncated product) are also \n    implemented. Max-statistic and genetic risk-allele score exact \n    distributions are also possible to be estimated. The methods are \n    described in Gonzalez JR et al., 2007 <doi: 10.1093/bioinformatics/btm025>.",
    "version": "2.1-2",
    "maintainer": "Dolors Pelegri <dolors.pelegri@isglobal.org>",
    "author": "Victor Moreno [aut],\n  Juan R Gonzalez [aut] (ORCID: <https://orcid.org/0000-0003-3267-2146>),\n  Dolors Pelegri [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5993-3003>)",
    "url": "https://github.com/isglobal-brge/SNPassoc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SNPassoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SNPassoc SNPs-Based Whole Genome Association Studies Functions to perform most of the common analysis in genome \n    association studies are implemented. These analyses include descriptive \n    statistics and exploratory analysis of missing values, calculation of \n    Hardy-Weinberg equilibrium, analysis of association based on generalized\n    linear models (either for quantitative or binary traits), and analysis\n    of multiple SNPs (haplotype and epistasis analysis). Permutation test \n    and related tests (sum statistic and truncated product) are also \n    implemented. Max-statistic and genetic risk-allele score exact \n    distributions are also possible to be estimated. The methods are \n    described in Gonzalez JR et al., 2007 <doi: 10.1093/bioinformatics/btm025>.  "
  },
  {
    "id": 7058,
    "package_name": "SNSequate",
    "title": "Standard and Nonstandard Statistical Models and Methods for Test\nEquating",
    "description": "Contains functions to perform various models and\n    methods for test equating (Kolen and Brennan, 2014 \n    <doi:10.1007/978-1-4939-0317-7> ; Gonzalez and Wiberg, 2017 \n    <doi:10.1007/978-3-319-51824-4> ; von Davier et. al, 2004 \n    <doi:10.1007/b97446>). It currently implements the traditional mean, linear \n    and equipercentile equating methods. Both IRT observed-score and true-score \n    equating are also supported, as well as the mean-mean, mean-sigma, Haebara \n    and Stocking-Lord IRT linking methods. It also supports newest methods such \n    that local equating, kernel equating (using Gaussian, logistic, \n    Epanechnikov, uniform and adaptive kernels) with presmoothing, and IRT \n    parameter linking methods based on asymmetric item characteristic functions. \n    Functions to obtain both standard error of equating (SEE) and standard error \n    of equating differences between two equating functions (SEED) are also \n    implemented for the kernel method of equating.",
    "version": "1.3-5",
    "maintainer": "Jorge Gonzalez <jorge.gonzalez@mat.uc.cl>",
    "author": "Jorge Gonzalez [cre, aut],\n  Daniel Leon Acuna [ctb]",
    "url": "https://www.mat.uc.cl/~jorge.gonzalez/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SNSequate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SNSequate Standard and Nonstandard Statistical Models and Methods for Test\nEquating Contains functions to perform various models and\n    methods for test equating (Kolen and Brennan, 2014 \n    <doi:10.1007/978-1-4939-0317-7> ; Gonzalez and Wiberg, 2017 \n    <doi:10.1007/978-3-319-51824-4> ; von Davier et. al, 2004 \n    <doi:10.1007/b97446>). It currently implements the traditional mean, linear \n    and equipercentile equating methods. Both IRT observed-score and true-score \n    equating are also supported, as well as the mean-mean, mean-sigma, Haebara \n    and Stocking-Lord IRT linking methods. It also supports newest methods such \n    that local equating, kernel equating (using Gaussian, logistic, \n    Epanechnikov, uniform and adaptive kernels) with presmoothing, and IRT \n    parameter linking methods based on asymmetric item characteristic functions. \n    Functions to obtain both standard error of equating (SEE) and standard error \n    of equating differences between two equating functions (SEED) are also \n    implemented for the kernel method of equating.  "
  },
  {
    "id": 7076,
    "package_name": "SPARRAfairness",
    "title": "Analysis of Differential Behaviour of SPARRA Score Across\nDemographic Groups",
    "description": "The SPARRA risk score (Scottish Patients At Risk of admission and Re-Admission) estimates yearly risk of emergency hospital admission using electronic health records on a monthly basis for most of the Scottish population. This package implements a suite of functions used to analyse the behaviour and performance of the score, focusing particularly on differential performance over demographically-defined groups. It includes useful utility functions to plot receiver-operator-characteristic, precision-recall and calibration curves, draw stock human figures, estimate counterfactual quantities without the need to re-compute risk scores, to simulate a semi-realistic dataset. Our manuscript can be found at: <doi:10.1371/journal.pdig.0000675>.",
    "version": "0.1.0.0",
    "maintainer": "James Liley <james.liley@durham.ac.uk>",
    "author": "Ioanna Thoma [aut] (ORCID: <https://orcid.org/0000-0001-6928-2198>),\n  Catalina Vallejos [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3638-1960>),\n  Louis Aslett [ctb] (ORCID: <https://orcid.org/0000-0003-2211-233X>),\n  Jill Ireland [ctb] (ORCID: <https://orcid.org/0009-0009-5324-6630>),\n  Simon Rogers [ctb] (ORCID: <https://orcid.org/0000-0003-3578-4477>),\n  James Liley [cre, aut] (ORCID: <https://orcid.org/0000-0002-0049-8238>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPARRAfairness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPARRAfairness Analysis of Differential Behaviour of SPARRA Score Across\nDemographic Groups The SPARRA risk score (Scottish Patients At Risk of admission and Re-Admission) estimates yearly risk of emergency hospital admission using electronic health records on a monthly basis for most of the Scottish population. This package implements a suite of functions used to analyse the behaviour and performance of the score, focusing particularly on differential performance over demographically-defined groups. It includes useful utility functions to plot receiver-operator-characteristic, precision-recall and calibration curves, draw stock human figures, estimate counterfactual quantities without the need to re-compute risk scores, to simulate a semi-realistic dataset. Our manuscript can be found at: <doi:10.1371/journal.pdig.0000675>.  "
  },
  {
    "id": 7093,
    "package_name": "SPLICE",
    "title": "Synthetic Paid Loss and Incurred Cost Experience (SPLICE)\nSimulator",
    "description": "An extension to the individual claim simulator called 'SynthETIC'\n    (on CRAN), to simulate the evolution of case estimates of incurred losses\n    through the lifetime of an insurance claim. The transactional simulation\n    output now comprises key dates, and both claim payments and revisions of\n    estimated incurred losses. An initial set of test parameters, designed to\n    mirror the experience of a real insurance portfolio, were set up and applied\n    by default to generate a realistic test data set of incurred histories (see\n    vignette). However, the distributional assumptions used to generate this\n    data set can be easily modified by users to match their experiences.\n    Reference: Avanzi B, Taylor G, Wang M (2021) \"SPLICE: A Synthetic Paid Loss\n    and Incurred Cost Experience Simulator\" <arXiv:2109.04058>.",
    "version": "1.1.2",
    "maintainer": "Melantha Wang <wang.melantha@gmail.com>",
    "author": "Benjamin Avanzi [aut],\n  Greg Taylor [aut],\n  Melantha Wang [aut, cre],\n  William Ho [aut]",
    "url": "https://github.com/agi-lab/SPLICE",
    "bug_reports": "https://github.com/agi-lab/SPLICE/issues",
    "repository": "https://cran.r-project.org/package=SPLICE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPLICE Synthetic Paid Loss and Incurred Cost Experience (SPLICE)\nSimulator An extension to the individual claim simulator called 'SynthETIC'\n    (on CRAN), to simulate the evolution of case estimates of incurred losses\n    through the lifetime of an insurance claim. The transactional simulation\n    output now comprises key dates, and both claim payments and revisions of\n    estimated incurred losses. An initial set of test parameters, designed to\n    mirror the experience of a real insurance portfolio, were set up and applied\n    by default to generate a realistic test data set of incurred histories (see\n    vignette). However, the distributional assumptions used to generate this\n    data set can be easily modified by users to match their experiences.\n    Reference: Avanzi B, Taylor G, Wang M (2021) \"SPLICE: A Synthetic Paid Loss\n    and Incurred Cost Experience Simulator\" <arXiv:2109.04058>.  "
  },
  {
    "id": 7096,
    "package_name": "SPPcomb",
    "title": "Combining Different Spatial Datasets in Cancer Risk Estimation",
    "description": "We propose a novel two-step procedure to combine epidemiological\n    data obtained from diverse sources with the aim to quantify risk factors\n    affecting the probability that an individual develops certain disease such as\n    cancer. See  Hui Huang, Xiaomei Ma, Rasmus Waagepetersen, Theodore R. Holford,\n    Rong Wang, Harvey Risch, Lloyd Mueller & Yongtao Guan (2014) A New Estimation Approach\n    for Combining Epidemiological Data From Multiple Sources, Journal of the American Statistical\n    Association, 109:505, 11-23,  <doi:10.1080/01621459.2013.870904>.",
    "version": "0.1",
    "maintainer": "Ming Wang <willnju@gmail.com>",
    "author": "Ming Wang, Yongtao Guan, Kun Xu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPPcomb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPPcomb Combining Different Spatial Datasets in Cancer Risk Estimation We propose a novel two-step procedure to combine epidemiological\n    data obtained from diverse sources with the aim to quantify risk factors\n    affecting the probability that an individual develops certain disease such as\n    cancer. See  Hui Huang, Xiaomei Ma, Rasmus Waagepetersen, Theodore R. Holford,\n    Rong Wang, Harvey Risch, Lloyd Mueller & Yongtao Guan (2014) A New Estimation Approach\n    for Combining Epidemiological Data From Multiple Sources, Journal of the American Statistical\n    Association, 109:505, 11-23,  <doi:10.1080/01621459.2013.870904>.  "
  },
  {
    "id": 7097,
    "package_name": "SPREDA",
    "title": "Statistical Package for Reliability Data Analysis",
    "description": "The Statistical Package for REliability Data Analysis (SPREDA) implements recently-developed statistical methods for the analysis of reliability data. Modern technological developments, such as sensors and smart chips, allow us to dynamically track product/system usage as well as other environmental variables, such as temperature and humidity. We refer to these variables as dynamic covariates. The package contains functions for the analysis of time-to-event data with dynamic covariates and degradation data with dynamic covariates. The package also contains functions that can be used for analyzing time-to-event data with right censoring, and with left truncation and right censoring. Financial support from NSF and DuPont are acknowledged.  ",
    "version": "1.2",
    "maintainer": "Yili Hong <yilihong@vt.edu>",
    "author": "Yili Hong [aut, cre],\n  Yimeng Xie [aut],\n  Zhibing Xu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPREDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPREDA Statistical Package for Reliability Data Analysis The Statistical Package for REliability Data Analysis (SPREDA) implements recently-developed statistical methods for the analysis of reliability data. Modern technological developments, such as sensors and smart chips, allow us to dynamically track product/system usage as well as other environmental variables, such as temperature and humidity. We refer to these variables as dynamic covariates. The package contains functions for the analysis of time-to-event data with dynamic covariates and degradation data with dynamic covariates. The package also contains functions that can be used for analyzing time-to-event data with right censoring, and with left truncation and right censoring. Financial support from NSF and DuPont are acknowledged.    "
  },
  {
    "id": 7137,
    "package_name": "SSplots",
    "title": "Stock Status Plots (SSPs)",
    "description": "Pauly et al. (2008) <http://legacy.seaaroundus.s3.amazonaws.com/doc/Researcher+Publications/dpauly/PDF/2008/Books%26Chapters/FisheriesInLargeMarineEcosystems.pdf> created (and coined the name) 'Stock Status Plots' for a UNEP compendium on Large Marine Ecosystems(LMEs, Sherman and Hempel (2009)<https://marineinfo.org/imis?module=ref&refid=142061&printversion=1&dropIMIStitle=1>). Stock status plots are bivariate graphs summarizing the status (e.g., developing, fully exploited, overexploited, etc.), through time, of the multispecies fisheries of a fished area or ecosystem. This package contains three functions to generate stock status plots viz., SSplots_pauly() (as per the criteria proposed by Pauly et al.,2008), SSplots_kleisner() (as per the criteria proposed by Kleisner and Pauly (2011) <http://www.ecomarres.com/downloads/regional.pdf> and Kleisner et al. (2013) <doi:10.1111/j.1467-2979.2012.00469.x>)and SSplots_EPI() (as per the criteria proposed by Jayasankar et al.,2021 <https://eprints.cmfri.org.in/11364/>).",
    "version": "0.1.2",
    "maintainer": "Eldho Varghese <eldhoiasri@gmail.com>",
    "author": "Eldho Varghese [aut, cre],\n  J Jayasankar [aut],\n  Sreepriya V [aut],\n  Reshma Gills [ctb],\n  Ashutosh Dalal [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SSplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSplots Stock Status Plots (SSPs) Pauly et al. (2008) <http://legacy.seaaroundus.s3.amazonaws.com/doc/Researcher+Publications/dpauly/PDF/2008/Books%26Chapters/FisheriesInLargeMarineEcosystems.pdf> created (and coined the name) 'Stock Status Plots' for a UNEP compendium on Large Marine Ecosystems(LMEs, Sherman and Hempel (2009)<https://marineinfo.org/imis?module=ref&refid=142061&printversion=1&dropIMIStitle=1>). Stock status plots are bivariate graphs summarizing the status (e.g., developing, fully exploited, overexploited, etc.), through time, of the multispecies fisheries of a fished area or ecosystem. This package contains three functions to generate stock status plots viz., SSplots_pauly() (as per the criteria proposed by Pauly et al.,2008), SSplots_kleisner() (as per the criteria proposed by Kleisner and Pauly (2011) <http://www.ecomarres.com/downloads/regional.pdf> and Kleisner et al. (2013) <doi:10.1111/j.1467-2979.2012.00469.x>)and SSplots_EPI() (as per the criteria proposed by Jayasankar et al.,2021 <https://eprints.cmfri.org.in/11364/>).  "
  },
  {
    "id": 7171,
    "package_name": "SWIM",
    "title": "Scenario Weights for Importance Measurement",
    "description": "An efficient sensitivity analysis for stochastic models based on \n    Monte Carlo samples. Provides weights on simulated scenarios from a \n    stochastic model, such that stressed random variables fulfil given \n    probabilistic constraints (e.g. specified values for risk measures), \n    under the new scenario weights. Scenario weights are selected by \n    constrained minimisation of the relative entropy to the baseline model. \n    The 'SWIM' package is based on Pesenti S.M., Millossovich P., Tsanakas A. (2019)\n    \"Reverse Sensitivity Testing: What does it take to break the model\" \n    <openaccess.city.ac.uk/id/eprint/18896/> and Pesenti S.M. (2021) \n    \"Reverse Sensitivity Analysis for Risk Modelling\" <https://www.ssrn.com/abstract=3878879>.",
    "version": "1.0.0",
    "maintainer": "Silvana M. Pesenti <swimpackage@gmail.com>",
    "author": "Silvana M. Pesenti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6661-6970>),\n  Alberto Bettini [aut],\n  Pietro Millossovich [aut] (ORCID:\n    <https://orcid.org/0000-0001-8269-7507>),\n  Andreas Tsanakas [aut] (ORCID: <https://orcid.org/0000-0003-4552-5532>),\n  Zhuomin Mao [ctb],\n  Kent Wu [ctb]",
    "url": "https://github.com/spesenti/SWIM,\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3515274,\nhttps://utstat.toronto.edu/pesenti/?page_id=138",
    "bug_reports": "https://github.com/spesenti/SWIM/issues",
    "repository": "https://cran.r-project.org/package=SWIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SWIM Scenario Weights for Importance Measurement An efficient sensitivity analysis for stochastic models based on \n    Monte Carlo samples. Provides weights on simulated scenarios from a \n    stochastic model, such that stressed random variables fulfil given \n    probabilistic constraints (e.g. specified values for risk measures), \n    under the new scenario weights. Scenario weights are selected by \n    constrained minimisation of the relative entropy to the baseline model. \n    The 'SWIM' package is based on Pesenti S.M., Millossovich P., Tsanakas A. (2019)\n    \"Reverse Sensitivity Testing: What does it take to break the model\" \n    <openaccess.city.ac.uk/id/eprint/18896/> and Pesenti S.M. (2021) \n    \"Reverse Sensitivity Analysis for Risk Modelling\" <https://www.ssrn.com/abstract=3878879>.  "
  },
  {
    "id": 7221,
    "package_name": "SemiCompRisks",
    "title": "Hierarchical Models for Parametric and Semi-Parametric Analyses\nof Semi-Competing Risks Data",
    "description": "Hierarchical multistate models are considered to perform the analysis of independent/clustered semi-competing risks data. The package allows to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions and cluster-specific random effects distribution; a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation approach for several parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.",
    "version": "3.4",
    "maintainer": "Kyu Ha Lee <klee15239@gmail.com>",
    "author": "Kyu Ha Lee, Catherine Lee, Danilo Alvares, and Sebastien Haneuse",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SemiCompRisks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SemiCompRisks Hierarchical Models for Parametric and Semi-Parametric Analyses\nof Semi-Competing Risks Data Hierarchical multistate models are considered to perform the analysis of independent/clustered semi-competing risks data. The package allows to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions and cluster-specific random effects distribution; a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation approach for several parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.  "
  },
  {
    "id": 7232,
    "package_name": "SentimentAnalysis",
    "title": "Dictionary-Based Sentiment Analysis",
    "description": "Performs a sentiment analysis of textual contents in R. This implementation\n    utilizes various existing dictionaries, such as Harvard IV, or finance-specific \n    dictionaries. Furthermore, it can also create customized dictionaries. The latter \n    uses LASSO regularization as a statistical approach to select relevant terms based on \n    an exogenous response variable. ",
    "version": "1.3-5",
    "maintainer": "Nicolas Proellochs <nicolas@nproellochs.com>",
    "author": "Nicolas Proellochs [aut, cre],\n  Stefan Feuerriegel [aut]",
    "url": "https://github.com/sfeuerriegel/SentimentAnalysis",
    "bug_reports": "https://github.com/sfeuerriegel/SentimentAnalysis/issues",
    "repository": "https://cran.r-project.org/package=SentimentAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SentimentAnalysis Dictionary-Based Sentiment Analysis Performs a sentiment analysis of textual contents in R. This implementation\n    utilizes various existing dictionaries, such as Harvard IV, or finance-specific \n    dictionaries. Furthermore, it can also create customized dictionaries. The latter \n    uses LASSO regularization as a statistical approach to select relevant terms based on \n    an exogenous response variable.   "
  },
  {
    "id": 7253,
    "package_name": "SharpeR",
    "title": "Statistical Significance of the Sharpe Ratio",
    "description": "A collection of tools for analyzing significance of assets,\n    funds, and trading strategies, based on the Sharpe ratio and overfit \n    of the same. Provides density, distribution, quantile and random generation \n    of the Sharpe ratio distribution based on normal returns, as well\n    as the optimal Sharpe ratio over multiple assets. Computes confidence intervals\n    on the Sharpe and provides a test of equality of Sharpe ratios based on \n    the Delta method. The statistical foundations of the Sharpe can be found in\n    the author's Short Sharpe Course  <doi:10.2139/ssrn.3036276>.",
    "version": "1.4.0",
    "maintainer": "Steven E. Pav <shabbychef@gmail.com>",
    "author": "Steven E. Pav [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4197-6195>)",
    "url": "https://github.com/shabbychef/SharpeR",
    "bug_reports": "https://github.com/shabbychef/SharpeR/issues",
    "repository": "https://cran.r-project.org/package=SharpeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SharpeR Statistical Significance of the Sharpe Ratio A collection of tools for analyzing significance of assets,\n    funds, and trading strategies, based on the Sharpe ratio and overfit \n    of the same. Provides density, distribution, quantile and random generation \n    of the Sharpe ratio distribution based on normal returns, as well\n    as the optimal Sharpe ratio over multiple assets. Computes confidence intervals\n    on the Sharpe and provides a test of equality of Sharpe ratios based on \n    the Delta method. The statistical foundations of the Sharpe can be found in\n    the author's Short Sharpe Course  <doi:10.2139/ssrn.3036276>.  "
  },
  {
    "id": 7262,
    "package_name": "ShinyTester",
    "title": "Functions to Minimize Bonehead Moves While Working with 'shiny'",
    "description": "It's my experience that working with 'shiny' is intuitive once you're\n    into it, but can be quite daunting at first. Several common mistakes are fairly\n    predictable, and therefore we can control for these. The functions in this\n    package help match up the assets listed in the UI and the SERVER files, and\n    Visualize the ad hoc structure of the 'shiny' App.",
    "version": "0.1.0",
    "maintainer": "Amit Kohli <amit@amitkohli.com>",
    "author": "Amit Kohli",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ShinyTester",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShinyTester Functions to Minimize Bonehead Moves While Working with 'shiny' It's my experience that working with 'shiny' is intuitive once you're\n    into it, but can be quite daunting at first. Several common mistakes are fairly\n    predictable, and therefore we can control for these. The functions in this\n    package help match up the assets listed in the UI and the SERVER files, and\n    Visualize the ad hoc structure of the 'shiny' App.  "
  },
  {
    "id": 7269,
    "package_name": "SiMRiv",
    "title": "Simulating Multistate Movements in River/Heterogeneous\nLandscapes",
    "description": "Provides functions to generate and analyze spatially-explicit individual-based multistate movements in rivers,\n  heterogeneous and homogeneous spaces. This is done by incorporating landscape bias on local behaviour, based on\n  resistance rasters. Although originally conceived and designed to simulate trajectories of species constrained to\n  linear habitats/dendritic ecological networks (e.g. river networks), the simulation algorithm is built to be\n  highly flexible and can be applied to any (aquatic, semi-aquatic or terrestrial) organism, independently on the\n  landscape in which it moves. Thus, the user will be able to use the package to simulate movements either in\n  homogeneous landscapes, heterogeneous landscapes (e.g. semi-aquatic animal moving mainly along rivers but also using\n  the matrix), or even in highly contrasted landscapes (e.g. fish in a river network). The algorithm and its input\n  parameters are the same for all cases, so that results are comparable. Simulated trajectories can then be used as\n  mechanistic null models (Potts & Lewis 2014, <DOI:10.1098/rspb.2014.0231>) to test a variety of 'Movement Ecology'\n  hypotheses (Nathan et al. 2008, <DOI:10.1073/pnas.0800375105>), including landscape effects (e.g. resources, \n  infrastructures) on animal movement and species site fidelity, or for predictive purposes (e.g. road mortality risk,\n  dispersal/connectivity). The package should be relevant to explore a broad spectrum of ecological phenomena, such as\n  those at the interface of animal behaviour, management, landscape and movement ecology, disease and invasive species\n  spread, and population dynamics.",
    "version": "1.0.7",
    "maintainer": "Miguel Porto <mpbertolo@gmail.com>",
    "author": "Lorenzo Quaglietta [aut],\n  Miguel Porto [aut, cre],\n  Erida Gjini [ctb]",
    "url": "https://www.r-project.org, https://github.com/miguel-porto/SiMRiv",
    "bug_reports": "https://github.com/miguel-porto/SiMRiv/issues",
    "repository": "https://cran.r-project.org/package=SiMRiv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SiMRiv Simulating Multistate Movements in River/Heterogeneous\nLandscapes Provides functions to generate and analyze spatially-explicit individual-based multistate movements in rivers,\n  heterogeneous and homogeneous spaces. This is done by incorporating landscape bias on local behaviour, based on\n  resistance rasters. Although originally conceived and designed to simulate trajectories of species constrained to\n  linear habitats/dendritic ecological networks (e.g. river networks), the simulation algorithm is built to be\n  highly flexible and can be applied to any (aquatic, semi-aquatic or terrestrial) organism, independently on the\n  landscape in which it moves. Thus, the user will be able to use the package to simulate movements either in\n  homogeneous landscapes, heterogeneous landscapes (e.g. semi-aquatic animal moving mainly along rivers but also using\n  the matrix), or even in highly contrasted landscapes (e.g. fish in a river network). The algorithm and its input\n  parameters are the same for all cases, so that results are comparable. Simulated trajectories can then be used as\n  mechanistic null models (Potts & Lewis 2014, <DOI:10.1098/rspb.2014.0231>) to test a variety of 'Movement Ecology'\n  hypotheses (Nathan et al. 2008, <DOI:10.1073/pnas.0800375105>), including landscape effects (e.g. resources, \n  infrastructures) on animal movement and species site fidelity, or for predictive purposes (e.g. road mortality risk,\n  dispersal/connectivity). The package should be relevant to explore a broad spectrum of ecological phenomena, such as\n  those at the interface of animal behaviour, management, landscape and movement ecology, disease and invasive species\n  spread, and population dynamics.  "
  },
  {
    "id": 7298,
    "package_name": "SimRDS",
    "title": "Simulation of Respondent Driven Samples",
    "description": "Simulate populations  with desired \n  properties and extract respondent driven samples. To better understand the \n  usage of the package and the algorithm used, please refer to Perera, A., and \n  Ramanayake, A. (2019) \n  <https://www.aimr.tirdiconference.com/assets/images/portfolio/Conference-Proceeding-AIMR-19.pdf>.",
    "version": "2.0.0",
    "maintainer": "Ayesha Perera <gaayesha93@gmail.com>",
    "author": "Ayesha Perera [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimRDS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimRDS Simulation of Respondent Driven Samples Simulate populations  with desired \n  properties and extract respondent driven samples. To better understand the \n  usage of the package and the algorithm used, please refer to Perera, A., and \n  Ramanayake, A. (2019) \n  <https://www.aimr.tirdiconference.com/assets/images/portfolio/Conference-Proceeding-AIMR-19.pdf>.  "
  },
  {
    "id": 7328,
    "package_name": "SmartSVA",
    "title": "Fast and Robust Surrogate Variable Analysis",
    "description": "Introduces a fast and efficient Surrogate Variable Analysis algorithm that captures variation of unknown sources (batch effects) for high-dimensional data sets. The algorithm is built on the 'irwsva.build' function of the 'sva' package and proposes a revision on it that achieves an order of magnitude faster running time while trading no accuracy loss in return.",
    "version": "0.1.3",
    "maintainer": "Jun Chen <Chen.Jun2@mayo.edu>",
    "author": "Jun Chen <Chen.Jun2@mayo.edu>, Ehsan Behnam <behnamgh@usc.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SmartSVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SmartSVA Fast and Robust Surrogate Variable Analysis Introduces a fast and efficient Surrogate Variable Analysis algorithm that captures variation of unknown sources (batch effects) for high-dimensional data sets. The algorithm is built on the 'irwsva.build' function of the 'sva' package and proposes a revision on it that achieves an order of magnitude faster running time while trading no accuracy loss in return.  "
  },
  {
    "id": 7346,
    "package_name": "SoilManageR",
    "title": "Calculate Soil Management Indicators for Agricultural Practice\nAssessment",
    "description": "Calculate numerical agricultural soil management indicators from on a management timeline of an arable field. Currently, indicators for carbon (C) input into the soil system, soil tillage intensity rating (STIR), number of soil cover and living plant cover days, N fertilization and livestock intensity, and plant diversity are implemented.\n    The functions can also be used independently of the management timeline to calculate some indicators. The package contains tables with reference information for the functions, as well as a '*.xlsx' template to collect the management data.",
    "version": "1.1.0",
    "maintainer": "Olivier Heller <olivier.heller@agroscope.admin.ch>",
    "author": "Olivier Heller [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-5918-4161>),\n  Rapha\u00ebl Wittwer [ctb] (ORCID: <https://orcid.org/0000-0002-2129-7195>)",
    "url": "https://gitlab.com/SoilManageR/",
    "bug_reports": "https://gitlab.com/SoilManageR/SoilManageR/-/issues",
    "repository": "https://cran.r-project.org/package=SoilManageR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SoilManageR Calculate Soil Management Indicators for Agricultural Practice\nAssessment Calculate numerical agricultural soil management indicators from on a management timeline of an arable field. Currently, indicators for carbon (C) input into the soil system, soil tillage intensity rating (STIR), number of soil cover and living plant cover days, N fertilization and livestock intensity, and plant diversity are implemented.\n    The functions can also be used independently of the management timeline to calculate some indicators. The package contains tables with reference information for the functions, as well as a '*.xlsx' template to collect the management data.  "
  },
  {
    "id": 7398,
    "package_name": "SpatialKWD",
    "title": "Spatial KWD for Large Spatial Maps",
    "description": "Contains efficient implementations of Discrete Optimal Transport algorithms for the computation of Kantorovich-Wasserstein distances between pairs of large spatial maps (Bassetti, Gualandi, Veneroni (2020), <doi:10.1137/19M1261195>). All the algorithms are based on an ad-hoc implementation of the Network Simplex algorithm. The package has four main helper functions: compareOneToOne() (to compare two spatial maps), compareOneToMany() (to compare a reference map with a list of other maps), compareAll() (to compute a matrix of distances between a list of maps), and focusArea() (to compute the KWD distance within a focus area). In non-convex maps, the helper functions first build the convex-hull of the input bins and pad the weights with zeros.",
    "version": "0.4.1",
    "maintainer": "Stefano Gualandi <stefano.gualandi@gmail.com>",
    "author": "Stefano Gualandi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatialKWD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatialKWD Spatial KWD for Large Spatial Maps Contains efficient implementations of Discrete Optimal Transport algorithms for the computation of Kantorovich-Wasserstein distances between pairs of large spatial maps (Bassetti, Gualandi, Veneroni (2020), <doi:10.1137/19M1261195>). All the algorithms are based on an ad-hoc implementation of the Network Simplex algorithm. The package has four main helper functions: compareOneToOne() (to compare two spatial maps), compareOneToMany() (to compare a reference map with a list of other maps), compareAll() (to compute a matrix of distances between a list of maps), and focusArea() (to compute the KWD distance within a focus area). In non-convex maps, the helper functions first build the convex-hull of the input bins and pad the weights with zeros.  "
  },
  {
    "id": 7410,
    "package_name": "SpeTestNP",
    "title": "Non-Parametric Tests of Parametric Specifications",
    "description": "\n    Performs non-parametric tests of parametric specifications.\n    Five tests are available. \n    Specific bandwidth and kernel methods can be chosen along with many other options. \n    Allows parallel computing to quickly compute p-values based on the bootstrap. \n    Methods implemented in the package are H.J. Bierens (1982) <doi:10.1016/0304-4076(82)90105-1>,\n    J.C. Escanciano (2006) <doi:10.1017/S0266466606060506>,\n    P.L. Gozalo (1997) <doi:10.1016/S0304-4076(97)86571-2>,\n    P. Lavergne and V. Patilea (2008) <doi:10.1016/j.jeconom.2007.08.014>,\n    P. Lavergne and V. Patilea (2012) <doi:10.1198/jbes.2011.07152>,\n    J.H. Stock and M.W. Watson (2006) <doi:10.1111/j.1538-4616.2007.00014.x>,\n    C.F.J. Wu (1986) <doi:10.1214/aos/1176350142>,\n    J. Yin, Z. Geng, R. Li, H. Wang (2010) <https://www.jstor.org/stable/24309002>\n    and J.X. Zheng (1996) <doi:10.1016/0304-4076(95)01760-7>.",
    "version": "1.1.0",
    "maintainer": "Hippolyte Boucher <Hippolyte.Boucher@outlook.com>",
    "author": "Hippolyte Boucher [aut, cre],\n  Pascal Lavergne [aut]",
    "url": "https://github.com/HippolyteBoucher/SpeTestNP",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpeTestNP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpeTestNP Non-Parametric Tests of Parametric Specifications \n    Performs non-parametric tests of parametric specifications.\n    Five tests are available. \n    Specific bandwidth and kernel methods can be chosen along with many other options. \n    Allows parallel computing to quickly compute p-values based on the bootstrap. \n    Methods implemented in the package are H.J. Bierens (1982) <doi:10.1016/0304-4076(82)90105-1>,\n    J.C. Escanciano (2006) <doi:10.1017/S0266466606060506>,\n    P.L. Gozalo (1997) <doi:10.1016/S0304-4076(97)86571-2>,\n    P. Lavergne and V. Patilea (2008) <doi:10.1016/j.jeconom.2007.08.014>,\n    P. Lavergne and V. Patilea (2012) <doi:10.1198/jbes.2011.07152>,\n    J.H. Stock and M.W. Watson (2006) <doi:10.1111/j.1538-4616.2007.00014.x>,\n    C.F.J. Wu (1986) <doi:10.1214/aos/1176350142>,\n    J. Yin, Z. Geng, R. Li, H. Wang (2010) <https://www.jstor.org/stable/24309002>\n    and J.X. Zheng (1996) <doi:10.1016/0304-4076(95)01760-7>.  "
  },
  {
    "id": 7473,
    "package_name": "StochBlock",
    "title": "Stochastic Blockmodeling of One-Mode and Linked Networks",
    "description": "Stochastic blockmodeling of one-mode and linked networks as presented in \u0160kulj and \u017diberna (2022) <doi:10.1016/j.socnet.2022.02.001>. The optimization is done via CEM (Classification Expectation Maximization) algorithm that can be initialized by random partitions or the results of k-means algorithm. The development of this package is financially supported by the Slovenian Research Agency (<https://www.arrs.si/>) within the research programs P5-0168 and the research projects J7-8279 (Blockmodeling multilevel and temporal networks) and J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).",
    "version": "0.1.5",
    "maintainer": "Ale\u0161 \u017diberna <ales.ziberna@fdv.uni-lj.si>",
    "author": "Ale\u0161 \u017diberna [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1534-6971>),\n  Fabio Ashtar Telarico [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StochBlock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StochBlock Stochastic Blockmodeling of One-Mode and Linked Networks Stochastic blockmodeling of one-mode and linked networks as presented in \u0160kulj and \u017diberna (2022) <doi:10.1016/j.socnet.2022.02.001>. The optimization is done via CEM (Classification Expectation Maximization) algorithm that can be initialized by random partitions or the results of k-means algorithm. The development of this package is financially supported by the Slovenian Research Agency (<https://www.arrs.si/>) within the research programs P5-0168 and the research projects J7-8279 (Blockmodeling multilevel and temporal networks) and J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).  "
  },
  {
    "id": 7474,
    "package_name": "StockDistFit",
    "title": "Fit Stock Price Distributions",
    "description": "The 'StockDistFit' package provides functions for fitting probability distributions to stock price data. The package uses maximum likelihood estimation to find the best-fitting distribution for a given stock. It also offers a function to fit several distributions to one or more assets and compare the distribution with the Akaike Information Criterion (AIC) and then pick the best distribution. References are as follows: Siew et al. (2008) <https://www.jstage.jst.go.jp/article/jappstat/37/1/37_1_1/_pdf/-char/ja> and Benth et al. (2008) <https://books.google.co.ke/books?hl=en&lr=&id=MHNpDQAAQBAJ&oi=fnd&pg=PR7&dq=Stochastic+modeling+of+commodity+prices+using+the+Variance+Gamma+(VG)+model.+&ots=YNIL2QmEYg&sig=XZtGU0lp4oqXHVyPZ-O8x5i7N3w&redir_esc=y#v=onepage&q&f=false>.",
    "version": "1.0.0",
    "maintainer": "Brian Njuguna <briannjuguna133@gmail.com>",
    "author": "Brian Njuguna [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-2119-904X>),\n  Stanely Sayianka [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StockDistFit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StockDistFit Fit Stock Price Distributions The 'StockDistFit' package provides functions for fitting probability distributions to stock price data. The package uses maximum likelihood estimation to find the best-fitting distribution for a given stock. It also offers a function to fit several distributions to one or more assets and compare the distribution with the Akaike Information Criterion (AIC) and then pick the best distribution. References are as follows: Siew et al. (2008) <https://www.jstage.jst.go.jp/article/jappstat/37/1/37_1_1/_pdf/-char/ja> and Benth et al. (2008) <https://books.google.co.ke/books?hl=en&lr=&id=MHNpDQAAQBAJ&oi=fnd&pg=PR7&dq=Stochastic+modeling+of+commodity+prices+using+the+Variance+Gamma+(VG)+model.+&ots=YNIL2QmEYg&sig=XZtGU0lp4oqXHVyPZ-O8x5i7N3w&redir_esc=y#v=onepage&q&f=false>.  "
  },
  {
    "id": 7480,
    "package_name": "Strategy",
    "title": "Generic Framework to Analyze Trading Strategies",
    "description": "Users can build and test customized quantitative trading strategies. Some quantitative trading strategies are already implemented, e.g. various moving-average filters with trend following approaches.\n    The implemented class called \"Strategy\" allows users to access several methods to analyze performance figures, plots and backtest the strategies.\n    Furthermore, custom strategies can be added, a generic template is available. The custom strategies require a certain input and output so they can be called from the Strategy-constructor.",
    "version": "1.0.1",
    "maintainer": "Julian Busch <jb@quants.ch>",
    "author": "Julian Busch",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Strategy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Strategy Generic Framework to Analyze Trading Strategies Users can build and test customized quantitative trading strategies. Some quantitative trading strategies are already implemented, e.g. various moving-average filters with trend following approaches.\n    The implemented class called \"Strategy\" allows users to access several methods to analyze performance figures, plots and backtest the strategies.\n    Furthermore, custom strategies can be added, a generic template is available. The custom strategies require a certain input and output so they can be called from the Strategy-constructor.  "
  },
  {
    "id": 7505,
    "package_name": "SummaryLasso",
    "title": "Building Polygenic Risk Score Using GWAS Summary Statistics",
    "description": "Shrinkage estimator for polygenic risk prediction models based on summary statistics of genome-wide association studies.",
    "version": "1.2.1",
    "maintainer": "Ting-Huei Chen <tingstat22@gmail.com>",
    "author": "Ting-Huei Chen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SummaryLasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SummaryLasso Building Polygenic Risk Score Using GWAS Summary Statistics Shrinkage estimator for polygenic risk prediction models based on summary statistics of genome-wide association studies.  "
  },
  {
    "id": 7513,
    "package_name": "SuperRanker",
    "title": "Sequential Rank Agreement",
    "description": "Tools for analysing the agreement of two or more rankings of the same items. Examples are importance rankings of predictor variables and risk predictions of subjects. Benchmarks for agreement are computed based on random permutation and bootstrap. See Ekstr\u00f8m CT, Gerds TA, Jensen, AK (2018). \"Sequential rank agreement methods for comparison of ranked lists.\" _Biostatistics_, *20*(4), 582-598 <doi:10.1093/biostatistics/kxy017> for more information.",
    "version": "1.2.1",
    "maintainer": "Claus Thorn Ekstr\u00f8m <ekstrom@sund.ku.dk>",
    "author": "Claus Thorn Ekstr\u00f8m [aut, cre],\n  Thomas Alexander Gerds [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SuperRanker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SuperRanker Sequential Rank Agreement Tools for analysing the agreement of two or more rankings of the same items. Examples are importance rankings of predictor variables and risk predictions of subjects. Benchmarks for agreement are computed based on random permutation and bootstrap. See Ekstr\u00f8m CT, Gerds TA, Jensen, AK (2018). \"Sequential rank agreement methods for comparison of ranked lists.\" _Biostatistics_, *20*(4), 582-598 <doi:10.1093/biostatistics/kxy017> for more information.  "
  },
  {
    "id": 7561,
    "package_name": "SynthETIC",
    "title": "Synthetic Experience Tracking Insurance Claims",
    "description": "Creation of an individual claims simulator which generates various\n    features of non-life insurance claims. An initial set of test parameters,\n    designed to mirror the experience of an Auto Liability portfolio, were set\n    up and applied by default to generate a realistic test data set of\n    individual claims (see vignette). The simulated data set then allows\n    practitioners to back-test the validity of various reserving models and to\n    prove and/or disprove certain actuarial assumptions made in claims\n    modelling. The distributional assumptions used to generate this data set can\n    be easily modified by users to match their experiences. Reference: Avanzi B,\n    Taylor G, Wang M, Wong B (2020) \"SynthETIC: an individual insurance claim\n    simulator with feature control\" <doi:10.48550/arXiv.2008.05693>.",
    "version": "1.1.1",
    "maintainer": "Melantha Wang <wang.melantha@gmail.com>",
    "author": "Benjamin Avanzi [aut],\n  William Ho [aut],\n  Greg Taylor [aut],\n  Melantha Wang [aut, cre],\n  Bernard Wong [aut]",
    "url": "https://github.com/agi-lab/SynthETIC",
    "bug_reports": "https://github.com/agi-lab/SynthETIC/issues",
    "repository": "https://cran.r-project.org/package=SynthETIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SynthETIC Synthetic Experience Tracking Insurance Claims Creation of an individual claims simulator which generates various\n    features of non-life insurance claims. An initial set of test parameters,\n    designed to mirror the experience of an Auto Liability portfolio, were set\n    up and applied by default to generate a realistic test data set of\n    individual claims (see vignette). The simulated data set then allows\n    practitioners to back-test the validity of various reserving models and to\n    prove and/or disprove certain actuarial assumptions made in claims\n    modelling. The distributional assumptions used to generate this data set can\n    be easily modified by users to match their experiences. Reference: Avanzi B,\n    Taylor G, Wang M, Wong B (2020) \"SynthETIC: an individual insurance claim\n    simulator with feature control\" <doi:10.48550/arXiv.2008.05693>.  "
  },
  {
    "id": 7564,
    "package_name": "SystemicR",
    "title": "Monitoring Systemic Risk",
    "description": "The past decade has demonstrated an increased need to better understand risks leading to systemic crises. This framework offers scholars, practitioners and policymakers a useful toolbox to explore such risks in financial systems. Specifically, this framework provides popular econometric and network measures to monitor systemic risk and to measure the consequences of regulatory decisions. These systemic risk measures are based on the frameworks of Adrian and Brunnermeier (2016) <doi:10.1257/aer.20120555> and Billio, Getmansky, Lo and Pelizzon (2012) <doi:10.1016/j.jfineco.2011.12.010>.",
    "version": "0.1.0",
    "maintainer": "Jean-Baptiste Hasse <jb-hasse@hotmail.fr>",
    "author": "Jean-Baptiste Hasse [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SystemicR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SystemicR Monitoring Systemic Risk The past decade has demonstrated an increased need to better understand risks leading to systemic crises. This framework offers scholars, practitioners and policymakers a useful toolbox to explore such risks in financial systems. Specifically, this framework provides popular econometric and network measures to monitor systemic risk and to measure the consequences of regulatory decisions. These systemic risk measures are based on the frameworks of Adrian and Brunnermeier (2016) <doi:10.1257/aer.20120555> and Billio, Getmansky, Lo and Pelizzon (2012) <doi:10.1016/j.jfineco.2011.12.010>.  "
  },
  {
    "id": 7580,
    "package_name": "TBFmultinomial",
    "title": "TBF Methodology Extension for Multinomial Outcomes",
    "description": "Extends the test-based Bayes factor (TBF) methodology to multinomial regression models and discrete time-to-event models with competing risks. The TBF methodology has been well developed and implemented for the generalised linear model [Held et al. (2015) <doi:10.1214/14-STS510>] and for the Cox model [Held et al. (2016) <doi:10.1002/sim.7089>].",
    "version": "0.1.3",
    "maintainer": "Rachel Heyard <rachel.heyard@uzh.ch>",
    "author": "Rachel Heyard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TBFmultinomial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TBFmultinomial TBF Methodology Extension for Multinomial Outcomes Extends the test-based Bayes factor (TBF) methodology to multinomial regression models and discrete time-to-event models with competing risks. The TBF methodology has been well developed and implemented for the generalised linear model [Held et al. (2015) <doi:10.1214/14-STS510>] and for the Cox model [Held et al. (2016) <doi:10.1002/sim.7089>].  "
  },
  {
    "id": 7607,
    "package_name": "TFRE",
    "title": "A Tuning-Free Robust and Efficient Approach to High-Dimensional\nRegression",
    "description": "Provide functions to estimate the coefficients in high-dimensional linear regressions via a tuning-free and robust approach. The method was published in Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), \"A Tuning-free Robust and Efficient Approach to High-dimensional Regression\", Journal of the American Statistical Association, 115:532, 1700-1714(JASA\u2019s discussion paper), <doi:10.1080/01621459.2020.1840989>. See also Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), \"Rejoinder to \u201cA tuning-free robust and efficient approach to high-dimensional regression\". Journal of the American Statistical Association, 115, 1726-1729, <doi:10.1080/01621459.2020.1843865>; Peng, B. and Wang, L. (2015), \"An Iterative Coordinate Descent Algorithm for High-Dimensional Nonconvex Penalized Quantile Regression\", Journal of Computational and Graphical Statistics, 24:3, 676-694, <doi:10.1080/10618600.2014.913516>; Cl\u00e9men\u00e7on, S., Colin, I., and Bellet, A. (2016), \"Scaling-up empirical risk minimization: optimization of incomplete u-statistics\", The Journal of Machine Learning Research, 17(1):2682\u20132717; Fan, J. and Li, R. (2001), \"Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties\", Journal of the American Statistical Association, 96:456, 1348-1360, <doi:10.1198/016214501753382273>. ",
    "version": "0.1.0",
    "maintainer": "Yunan Wu <yunan.wu@utdallas.edu>",
    "author": "Yunan Wu [aut, cre, cph],\n  Lan Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TFRE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TFRE A Tuning-Free Robust and Efficient Approach to High-Dimensional\nRegression Provide functions to estimate the coefficients in high-dimensional linear regressions via a tuning-free and robust approach. The method was published in Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), \"A Tuning-free Robust and Efficient Approach to High-dimensional Regression\", Journal of the American Statistical Association, 115:532, 1700-1714(JASA\u2019s discussion paper), <doi:10.1080/01621459.2020.1840989>. See also Wang, L., Peng, B., Bradic, J., Li, R. and Wu, Y. (2020), \"Rejoinder to \u201cA tuning-free robust and efficient approach to high-dimensional regression\". Journal of the American Statistical Association, 115, 1726-1729, <doi:10.1080/01621459.2020.1843865>; Peng, B. and Wang, L. (2015), \"An Iterative Coordinate Descent Algorithm for High-Dimensional Nonconvex Penalized Quantile Regression\", Journal of Computational and Graphical Statistics, 24:3, 676-694, <doi:10.1080/10618600.2014.913516>; Cl\u00e9men\u00e7on, S., Colin, I., and Bellet, A. (2016), \"Scaling-up empirical risk minimization: optimization of incomplete u-statistics\", The Journal of Machine Learning Research, 17(1):2682\u20132717; Fan, J. and Li, R. (2001), \"Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties\", Journal of the American Statistical Association, 96:456, 1348-1360, <doi:10.1198/016214501753382273>.   "
  },
  {
    "id": 7612,
    "package_name": "TGST",
    "title": "Targeted Gold Standard Testing",
    "description": "Functions for implementing the targeted gold standard (GS) testing. You provide the true disease or treatment failure status and the risk score, tell 'TGST' the availability of GS tests and which method to use, and it returns the optimal tripartite rules. Please refer to Liu et al. (2013) <doi:10.1080/01621459.2013.810149> for more details.",
    "version": "1.0",
    "maintainer": "Yizhen Xu <yizhen_xu@alumni.brown.edu>",
    "author": "Yizhen Xu [aut, cre],\n  Tao Liu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TGST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TGST Targeted Gold Standard Testing Functions for implementing the targeted gold standard (GS) testing. You provide the true disease or treatment failure status and the risk score, tell 'TGST' the availability of GS tests and which method to use, and it returns the optimal tripartite rules. Please refer to Liu et al. (2013) <doi:10.1080/01621459.2013.810149> for more details.  "
  },
  {
    "id": 7662,
    "package_name": "TSEtools",
    "title": "Manage Data from Stock Exchange Markets",
    "description": "Tools to perform some descriptive data analysis for assets. Manage the portfolio and capital of assets. It also downloads and organizes data from the Tehran Stock Exchange (TSE).",
    "version": "0.2.2",
    "maintainer": "Ali Saeb <ali.saeb@gmail.com>",
    "author": "Ali Saeb",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSEtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSEtools Manage Data from Stock Exchange Markets Tools to perform some descriptive data analysis for assets. Manage the portfolio and capital of assets. It also downloads and organizes data from the Tehran Stock Exchange (TSE).  "
  },
  {
    "id": 7690,
    "package_name": "TTR",
    "title": "Technical Trading Rules",
    "description": "A collection of over 50 technical indicators for creating technical trading rules. The package also provides fast implementations of common rolling-window functions, and several volatility calculations.",
    "version": "0.24.4",
    "maintainer": "Joshua Ulrich <josh.m.ulrich@gmail.com>",
    "author": "Joshua Ulrich [cre, aut],\n  Ethan B. Smith [ctb]",
    "url": "https://github.com/joshuaulrich/TTR",
    "bug_reports": "https://github.com/joshuaulrich/TTR/issues",
    "repository": "https://cran.r-project.org/package=TTR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TTR Technical Trading Rules A collection of over 50 technical indicators for creating technical trading rules. The package also provides fast implementations of common rolling-window functions, and several volatility calculations.  "
  },
  {
    "id": 7698,
    "package_name": "TVMVP",
    "title": "Time-Varying Minimum Variance Portfolio",
    "description": "Provides the estimation of a time-dependent covariance matrix of returns with the intended use for portfolio optimization. The package offers methods for determining the optimal number of factors to be used in the covariance estimation, a hypothesis test of time-varying covariance, and user-friendly functions for portfolio optimization and rolling window evaluation. The local PCA method, method for determining the number of factors, and associated hypothesis test are based on Su and Wang (2017) <doi:10.1016/j.jeconom.2016.12.004>. The approach to time-varying portfolio optimization follows Fan et al. (2024) <doi:10.1016/j.jeconom.2022.08.007>. The regularisation applied to the residual covariance matrix adopts the technique introduced by Chen et al. (2019) <doi:10.1016/j.jeconom.2019.04.025>.",
    "version": "1.0.5",
    "maintainer": "Erik Lillrank <erik.lillrank@gmail.com>",
    "author": "Erik Lillrank [aut, cre] (ORCID:\n    <https://orcid.org/0009-0001-3345-7694>),\n  Yukai Yang [aut] (ORCID: <https://orcid.org/0000-0002-2623-8549>)",
    "url": "https://github.com/erilill/TV-MVP",
    "bug_reports": "https://github.com/erilill/TV-MVP/issues",
    "repository": "https://cran.r-project.org/package=TVMVP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TVMVP Time-Varying Minimum Variance Portfolio Provides the estimation of a time-dependent covariance matrix of returns with the intended use for portfolio optimization. The package offers methods for determining the optimal number of factors to be used in the covariance estimation, a hypothesis test of time-varying covariance, and user-friendly functions for portfolio optimization and rolling window evaluation. The local PCA method, method for determining the number of factors, and associated hypothesis test are based on Su and Wang (2017) <doi:10.1016/j.jeconom.2016.12.004>. The approach to time-varying portfolio optimization follows Fan et al. (2024) <doi:10.1016/j.jeconom.2022.08.007>. The regularisation applied to the residual covariance matrix adopts the technique introduced by Chen et al. (2019) <doi:10.1016/j.jeconom.2019.04.025>.  "
  },
  {
    "id": 7789,
    "package_name": "ToxCrit",
    "title": "Calculates Safety Stopping Boundaries for a Single-Arm Trial\nusing Bayes",
    "description": "Computation of stopping boundaries for a single-arm trial using a\n    Bayesian criterion; i.e., for each m<=n (n= total patient number of the \n    trial) the smallest number of observed toxicities is calculated\n    leading to the termination of the trial/accrual according to the specified \n    criteria. The probabilities of stopping the trial/accrual at and up until \n    (resp.) the m-th patient (m<=n) is also calculated. This design is more \n    conservative than the frequentist approach (using Clopper Pearson CIs)\n    which might be preferred as it concerns safety.See also Aamot et.al.(2010) \n    \"Continuous monitoring of toxicity in clinical trials - simulating the risk \n    of stopping prematurely\" <doi:10.5414/cpp48476>.",
    "version": "1.0",
    "maintainer": "Lisa-Marie Lanz <studienzentrale@nct-heidelberg.de>",
    "author": "NCT Trial Center, Heidelberg [aut],\n  Lisa-Marie Lanz [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ToxCrit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ToxCrit Calculates Safety Stopping Boundaries for a Single-Arm Trial\nusing Bayes Computation of stopping boundaries for a single-arm trial using a\n    Bayesian criterion; i.e., for each m<=n (n= total patient number of the \n    trial) the smallest number of observed toxicities is calculated\n    leading to the termination of the trial/accrual according to the specified \n    criteria. The probabilities of stopping the trial/accrual at and up until \n    (resp.) the m-th patient (m<=n) is also calculated. This design is more \n    conservative than the frequentist approach (using Clopper Pearson CIs)\n    which might be preferred as it concerns safety.See also Aamot et.al.(2010) \n    \"Continuous monitoring of toxicity in clinical trials - simulating the risk \n    of stopping prematurely\" <doi:10.5414/cpp48476>.  "
  },
  {
    "id": 7795,
    "package_name": "Trading",
    "title": "Trade Objects, Advanced Correlation & Beta Estimates, Betting\nStrategies",
    "description": "Contains performance analysis metrics of track records including entropy-based\n            correlation and dynamic beta based on a state/space algorithm. The normalized sample entropy method\n            has been implemented which produces accurate entropy estimation even on smaller datasets.\n            On a separate stream, trades from the five major assets classes and also\n            functionality to use pricing curves, rating tables, Credit Support Annex and add-on tables. The\n            implementation follows an object oriented logic whereby each trade inherits from\n            more abstract classes while also the curves/tables are objects. Furthermore, odds calculators\n            and P&L back-testing functionality has been implemented for the most widely used betting/trading\n            strategies including martingale, 'DAlembert', 'Labouchere' and Fibonacci. Back testing has also been included for the 'EuroMillions',\n      \t\t\tthe 'EuroJackpot', the UK Lotto, the Set For Life and the UK 'ThunderBall' lotteries.\n      \t\t\tFurthermore, some basic functionality about climate risk has been included. ",
    "version": "3.2",
    "maintainer": "Tasos Grivas <info@openriskcalculator.com>",
    "author": "Tasos Grivas [aut, cre]",
    "url": "https://openriskcalculator.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Trading",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Trading Trade Objects, Advanced Correlation & Beta Estimates, Betting\nStrategies Contains performance analysis metrics of track records including entropy-based\n            correlation and dynamic beta based on a state/space algorithm. The normalized sample entropy method\n            has been implemented which produces accurate entropy estimation even on smaller datasets.\n            On a separate stream, trades from the five major assets classes and also\n            functionality to use pricing curves, rating tables, Credit Support Annex and add-on tables. The\n            implementation follows an object oriented logic whereby each trade inherits from\n            more abstract classes while also the curves/tables are objects. Furthermore, odds calculators\n            and P&L back-testing functionality has been implemented for the most widely used betting/trading\n            strategies including martingale, 'DAlembert', 'Labouchere' and Fibonacci. Back testing has also been included for the 'EuroMillions',\n      \t\t\tthe 'EuroJackpot', the UK Lotto, the Set For Life and the UK 'ThunderBall' lotteries.\n      \t\t\tFurthermore, some basic functionality about climate risk has been included.   "
  },
  {
    "id": 7806,
    "package_name": "TreatmentSelection",
    "title": "Evaluate Treatment Selection Biomarkers",
    "description": "A suite of descriptive and inferential methods designed to evaluate one or more biomarkers for their ability to guide patient treatment recommendations.  Package includes functions to assess the calibration of risk models; and plot, evaluate, and compare markers. Please see the reference Janes H, Brown MD, Huang Y, et al. (2014) <doi:10.1515/ijb-2012-0052> for further details. ",
    "version": "2.1.1",
    "maintainer": "Marshall Brown <mdbrown@fhcrc.org>",
    "author": "Marshall Brown and Holly Janes",
    "url": "http://rpubs.com/mdbrown/TreatmentSelection",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TreatmentSelection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TreatmentSelection Evaluate Treatment Selection Biomarkers A suite of descriptive and inferential methods designed to evaluate one or more biomarkers for their ability to guide patient treatment recommendations.  Package includes functions to assess the calibration of risk models; and plot, evaluate, and compare markers. Please see the reference Janes H, Brown MD, Huang Y, et al. (2014) <doi:10.1515/ijb-2012-0052> for further details.   "
  },
  {
    "id": 7833,
    "package_name": "TropFishR",
    "title": "Tropical Fisheries Analysis",
    "description": "A compilation of fish stock assessment methods for the\n    analysis of length-frequency data in the context of data-poor\n    fisheries. Includes methods and examples included in the FAO\n    Manual by P. Sparre and S.C. Venema (1998), \"Introduction to tropical fish\n    stock assessment\" (<https://openknowledge.fao.org/server/api/core/bitstreams/bc7c37b6-30df-49c0-b5b4-8367a872c97e/content>),\n    as well as other more recent methods.",
    "version": "1.6.6",
    "maintainer": "Tobias K. Mildenberger <t.k.mildenberger@gmail.com>",
    "author": "Tobias K. Mildenberger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6631-7524>),\n  Marc H. Taylor [aut],\n  Matthias Wolff [aut]",
    "url": "https://github.com/tokami/TropFishR",
    "bug_reports": "https://github.com/tokami/TropFishR/issues",
    "repository": "https://cran.r-project.org/package=TropFishR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TropFishR Tropical Fisheries Analysis A compilation of fish stock assessment methods for the\n    analysis of length-frequency data in the context of data-poor\n    fisheries. Includes methods and examples included in the FAO\n    Manual by P. Sparre and S.C. Venema (1998), \"Introduction to tropical fish\n    stock assessment\" (<https://openknowledge.fao.org/server/api/core/bitstreams/bc7c37b6-30df-49c0-b5b4-8367a872c97e/content>),\n    as well as other more recent methods.  "
  },
  {
    "id": 7838,
    "package_name": "TrumpetPlots",
    "title": "Visualization of Genetic Association Studies",
    "description": "Visualizes the relationship between allele frequency and effect size in genetic association studies. The input is a data frame containing association results. The output is a plot with the effect size of risk variants in the Y axis, and the allele frequency spectrum in the X axis. Corte et al (2023) <doi:10.1101/2023.04.21.23288923>.",
    "version": "0.0.1.1",
    "maintainer": "Judit Garc\u00eda-Gonz\u00e1lez <judit.garciagonzalez@mssm.edu>",
    "author": "Judit Garc\u00eda-Gonz\u00e1lez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6245-740X>),\n  Lathan Liou [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TrumpetPlots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TrumpetPlots Visualization of Genetic Association Studies Visualizes the relationship between allele frequency and effect size in genetic association studies. The input is a data frame containing association results. The output is a plot with the effect size of risk variants in the Y axis, and the allele frequency spectrum in the X axis. Corte et al (2023) <doi:10.1101/2023.04.21.23288923>.  "
  },
  {
    "id": 7845,
    "package_name": "Tushare",
    "title": "Interface to 'Tushare Pro' API",
    "description": "Helps the R users to get data from 'Tushare Pro'<https://tushare.pro>.\n    'Tushare Pro' is a platform as well as a community with a lot of staffs working in financial area.\n    We support financial data such as stock price, financial report statements and digital coins data.",
    "version": "0.1.4",
    "maintainer": "Feifei ZHANG<tushare_pro@163.com>",
    "author": "Feifei ZHANG",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Tushare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Tushare Interface to 'Tushare Pro' API Helps the R users to get data from 'Tushare Pro'<https://tushare.pro>.\n    'Tushare Pro' is a platform as well as a community with a lot of staffs working in financial area.\n    We support financial data such as stock price, financial report statements and digital coins data.  "
  },
  {
    "id": 7852,
    "package_name": "TwoTimeScales",
    "title": "Analysis of Event Data with Two Time Scales",
    "description": "Analyse time to event data with two time scales by estimating a smooth hazard that varies over two time scales and also, if covariates are available, to estimate a proportional hazards model with such a two-dimensional baseline hazard. \n  Functions are provided to prepare the raw data for estimation, to estimate and to plot the two-dimensional smooth hazard. \n  Extension to a competing risks model are implemented. For details about the method please refer to Carollo et al. (2024) <doi:10.1002/sim.10297>.",
    "version": "1.0.0",
    "maintainer": "Angela Carollo <carollo@demogr.mpg.de>",
    "author": "Angela Carollo [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5625-6553>),\n  Paul H.C. Eilers [aut],\n  Jutta Gampe [aut]",
    "url": "https://github.com/AngelaCar/TwoTimeScales,\nhttps://angelacar.github.io/TwoTimeScales/",
    "bug_reports": "https://github.com/AngelaCar/TwoTimeScales/issues",
    "repository": "https://cran.r-project.org/package=TwoTimeScales",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TwoTimeScales Analysis of Event Data with Two Time Scales Analyse time to event data with two time scales by estimating a smooth hazard that varies over two time scales and also, if covariates are available, to estimate a proportional hazards model with such a two-dimensional baseline hazard. \n  Functions are provided to prepare the raw data for estimation, to estimate and to plot the two-dimensional smooth hazard. \n  Extension to a competing risks model are implemented. For details about the method please refer to Carollo et al. (2024) <doi:10.1002/sim.10297>.  "
  },
  {
    "id": 7917,
    "package_name": "VBJM",
    "title": "Variational Inference for Joint Model",
    "description": "The shared random effects joint model is one of the most widely used approaches to study the associations between longitudinal biomarkers and a survival outcome and make dynamic risk predictions using the longitudinally measured biomarkers. \n    One major limitation of joint models is that they could be computationally expensive for complex models where the number of the shared random effects is large.  \n    This package can be used to fit complex multivariate joint models using our newly developed algorithm Jieqi Tu and Jiehuan Sun (2023) <doi:10.1002/sim.9619>, which is based on Gaussian variational approximate inference and is computationally efficient.",
    "version": "0.1.0",
    "maintainer": "Jiehuan Sun <jiehuan.sun@gmail.com>",
    "author": "Jiehuan Sun [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VBJM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VBJM Variational Inference for Joint Model The shared random effects joint model is one of the most widely used approaches to study the associations between longitudinal biomarkers and a survival outcome and make dynamic risk predictions using the longitudinally measured biomarkers. \n    One major limitation of joint models is that they could be computationally expensive for complex models where the number of the shared random effects is large.  \n    This package can be used to fit complex multivariate joint models using our newly developed algorithm Jieqi Tu and Jiehuan Sun (2023) <doi:10.1002/sim.9619>, which is based on Gaussian variational approximate inference and is computationally efficient.  "
  },
  {
    "id": 7921,
    "package_name": "VBV",
    "title": "The Generalized Berlin Method for Time Series Decomposition",
    "description": "Time series decomposition for univariate time series using the\n    \"Verallgemeinerte Berliner Verfahren\" (Generalized Berlin Method) as\n    described in 'Kontinuierliche Messgr\u00f6\u00dfen und Stichprobenstrategien in\n    Raum und Zeit mit Anwendungen in den Natur-, Umwelt-, Wirtschafts-\n    und Finanzwissenschaften', by\n    Hebbel and Steuer, Springer Berlin Heidelberg, 2022\n    <doi:10.1007/978-3-662-65638-9>, or\n    'Decomposition of Time Series using the Generalised Berlin \n    Method (VBV)' by Hebbel and Steuer, in Jan Beran, Yuanhua Feng, Hartmut\n    Hebbel (Eds.): Empirical Economic and Financial Research - Theory,\n    Methods and Practice, Festschrift in Honour of Prof. Siegfried Heiler.\n    Series: Advanced Studies in Theoretical and Applied Econometrics.\n    Springer 2014, p. 9-40.",
    "version": "0.6.2",
    "maintainer": "Detlef Steuer <steuer@hsu-hh.de>",
    "author": "Detlef Steuer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2676-5290>),\n  Hartmut Hebbel [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VBV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VBV The Generalized Berlin Method for Time Series Decomposition Time series decomposition for univariate time series using the\n    \"Verallgemeinerte Berliner Verfahren\" (Generalized Berlin Method) as\n    described in 'Kontinuierliche Messgr\u00f6\u00dfen und Stichprobenstrategien in\n    Raum und Zeit mit Anwendungen in den Natur-, Umwelt-, Wirtschafts-\n    und Finanzwissenschaften', by\n    Hebbel and Steuer, Springer Berlin Heidelberg, 2022\n    <doi:10.1007/978-3-662-65638-9>, or\n    'Decomposition of Time Series using the Generalised Berlin \n    Method (VBV)' by Hebbel and Steuer, in Jan Beran, Yuanhua Feng, Hartmut\n    Hebbel (Eds.): Empirical Economic and Financial Research - Theory,\n    Methods and Practice, Festschrift in Honour of Prof. Siegfried Heiler.\n    Series: Advanced Studies in Theoretical and Applied Econometrics.\n    Springer 2014, p. 9-40.  "
  },
  {
    "id": 7960,
    "package_name": "VaRES",
    "title": "Computes Value at Risk and Expected Shortfall for over 100\nParametric Distributions",
    "description": "Computes Value at risk and expected shortfall, two most popular measures of financial risk, for over one hundred parametric distributions, including all commonly known distributions.  Also computed are the corresponding probability density function and cumulative distribution function. See Chan, Nadarajah and Afuecheta (2015) <doi:10.1080/03610918.2014.944658> for more details.",
    "version": "1.0.2",
    "maintainer": "Leo Belzile <belzilel@gmail.com>",
    "author": "Leo Belzile [cre] (ORCID: <https://orcid.org/0000-0002-9135-014X>),\n  Saralees Nadarajah [aut],\n  Stephen Chan [aut],\n  Emmanuel Afuecheta [aut] (ORCID:\n    <https://orcid.org/0000-0002-9223-0799>)",
    "url": "",
    "bug_reports": "https://github.com/lbelzile/VaRES/issues/",
    "repository": "https://cran.r-project.org/package=VaRES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VaRES Computes Value at Risk and Expected Shortfall for over 100\nParametric Distributions Computes Value at risk and expected shortfall, two most popular measures of financial risk, for over one hundred parametric distributions, including all commonly known distributions.  Also computed are the corresponding probability density function and cumulative distribution function. See Chan, Nadarajah and Afuecheta (2015) <doi:10.1080/03610918.2014.944658> for more details.  "
  },
  {
    "id": 7975,
    "package_name": "VetResearchLMM",
    "title": "Linear Mixed Models - An Introduction with Applications in\nVeterinary Research",
    "description": "R Codes and Datasets for Duchateau, L. and Janssen, P. and Rowlands, G. J. (1998). Linear Mixed Models. An Introduction with applications in Veterinary Research. International Livestock Research Institute.",
    "version": "1.0.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre],\n  Luc Duchateau [ctb],\n  Paul Janssen [ctb],\n  John Rowlands [ctb]",
    "url": "https://github.com/MYaseen208/VetResearchLMM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VetResearchLMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VetResearchLMM Linear Mixed Models - An Introduction with Applications in\nVeterinary Research R Codes and Datasets for Duchateau, L. and Janssen, P. and Rowlands, G. J. (1998). Linear Mixed Models. An Introduction with applications in Veterinary Research. International Livestock Research Institute.  "
  },
  {
    "id": 7986,
    "package_name": "VisualDom",
    "title": "Visualize Dominant Variables in Wavelet Multiple Correlation",
    "description": "Estimates and plots as a heat map the correlation coefficients obtained via the wavelet local multiple correlation 'WLMC' (Fern\u00e1ndez-Macho 2018) and the 'dominant' variable/s, i.e., the variable/s that maximizes the multiple correlation through time and scale (Polanco-Mart\u00ednez et al. 2020, Polanco-Mart\u00ednez 2022). We improve the graphical outputs of WLMC proposing a didactic and useful way to visualize the 'dominant' variable(s) for a set of time series. The WLMC was designed for financial time series, but other kinds of data (e.g., climatic, ecological, etc.) can be used. The functions contained in 'VisualDom' are highly flexible since these contains several parameters to personalize the time series under analysis and the heat maps. In addition, we have also included two data sets (named 'rdata_climate' and 'rdata_Lorenz') to exemplify the use of the functions contained in 'VisualDom'. Methods derived from Fern\u00e1ndez-Macho (2018) <doi:10.1016/j.physa.2017.11.050>, Polanco-Mart\u00ednez et al. (2020) <doi:10.1038/s41598-020-77767-8> and Polanco-Mart\u00ednez (2023, in press). ",
    "version": "0.8.0",
    "maintainer": "Josu\u00e9 M. Polanco-Mart\u00ednez <josue.m.polanco@gmail.com>",
    "author": "Josu\u00e9 M. Polanco-Mart\u00ednez [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-7164-0185>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VisualDom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VisualDom Visualize Dominant Variables in Wavelet Multiple Correlation Estimates and plots as a heat map the correlation coefficients obtained via the wavelet local multiple correlation 'WLMC' (Fern\u00e1ndez-Macho 2018) and the 'dominant' variable/s, i.e., the variable/s that maximizes the multiple correlation through time and scale (Polanco-Mart\u00ednez et al. 2020, Polanco-Mart\u00ednez 2022). We improve the graphical outputs of WLMC proposing a didactic and useful way to visualize the 'dominant' variable(s) for a set of time series. The WLMC was designed for financial time series, but other kinds of data (e.g., climatic, ecological, etc.) can be used. The functions contained in 'VisualDom' are highly flexible since these contains several parameters to personalize the time series under analysis and the heat maps. In addition, we have also included two data sets (named 'rdata_climate' and 'rdata_Lorenz') to exemplify the use of the functions contained in 'VisualDom'. Methods derived from Fern\u00e1ndez-Macho (2018) <doi:10.1016/j.physa.2017.11.050>, Polanco-Mart\u00ednez et al. (2020) <doi:10.1038/s41598-020-77767-8> and Polanco-Mart\u00ednez (2023, in press).   "
  },
  {
    "id": 8006,
    "package_name": "WCM",
    "title": "Water Cloud Model (WCM) for the Simulation of Leaf Area Index\n(LAI) and Soil Moisture (SM) from Microwave Backscattering",
    "description": "Retrieval the leaf area index (LAI) and soil moisture (SM) from  microwave backscattering data using water cloud model (WCM) model . The WCM algorithm attributed to Pervot et al.(1993) <doi:10.1016/0034-4257(93)90053-Z>. The authors are grateful to SAC, ISRO, Ahmedabad for providing financial support to Dr. Prashant K Srivastava to conduct this research work.",
    "version": "0.2.2",
    "maintainer": "Ujjwal Singh <ujjwalrsmt@gmail.com>",
    "author": "Ujjwal Singh <ujjwalrsmt@gmail.com>\n        Prashant K Srivastava <prashant.just@gmail.com)>\n        Dharmendra Kumar Pandey <Dkp@sac.isro.gov.in>\n        Sumit Kumar Chaudhary <sumit.mathe@gmail.com>\n        Dileep Kumar Gupta <dileepgupta85@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WCM Water Cloud Model (WCM) for the Simulation of Leaf Area Index\n(LAI) and Soil Moisture (SM) from Microwave Backscattering Retrieval the leaf area index (LAI) and soil moisture (SM) from  microwave backscattering data using water cloud model (WCM) model . The WCM algorithm attributed to Pervot et al.(1993) <doi:10.1016/0034-4257(93)90053-Z>. The authors are grateful to SAC, ISRO, Ahmedabad for providing financial support to Dr. Prashant K Srivastava to conduct this research work.  "
  },
  {
    "id": 8009,
    "package_name": "WEGE",
    "title": "A Metric to Rank Locations for Biodiversity Conservation",
    "description": "Calculates the WEGE (Weighted Endemism including Global \n    Endangerment index) index for a particular area. Additionally it also \n    calculates rasters of KBA's (Key Biodiversity Area) criteria (A1a, A1b, A1e, \n    and B1), Weighted endemism (WE), the EDGE (Evolutionarily Distinct and\n    Globally Endangered) score, Evolutionary Distinctiveness (ED) and Extinction\n    risk (ER). Farooq, H., Azevedo, J., Belluardo F., Nanvonamuquitxo, C.,\n    Bennett, D., Moat, J., Soares, A., Faurby, S. & Antonelli, A. (2020)\n    <doi:10.1101/2020.01.17.910299>.",
    "version": "0.1.0",
    "maintainer": "Harith Farooq <harithmorgadinho@gmail.com>",
    "author": "Harith Farooq [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9031-2785>),\n  Josu\u00e9 Azevedo [aut],\n  Francesco Belluardo [aut],\n  Crist\u00f3v\u00e3o Nanvonamuquitxo [aut],\n  Dom Bennett [aut] (ORCID: <https://orcid.org/0000-0003-2722-1359>),\n  Jason Moat [aut],\n  Amadeu Soares [aut],\n  S\u00f8ren Faurby [aut],\n  Alexandre Antonelli [aut] (ORCID:\n    <https://orcid.org/0000-0003-1842-9297>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WEGE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WEGE A Metric to Rank Locations for Biodiversity Conservation Calculates the WEGE (Weighted Endemism including Global \n    Endangerment index) index for a particular area. Additionally it also \n    calculates rasters of KBA's (Key Biodiversity Area) criteria (A1a, A1b, A1e, \n    and B1), Weighted endemism (WE), the EDGE (Evolutionarily Distinct and\n    Globally Endangered) score, Evolutionary Distinctiveness (ED) and Extinction\n    risk (ER). Farooq, H., Azevedo, J., Belluardo F., Nanvonamuquitxo, C.,\n    Bennett, D., Moat, J., Soares, A., Faurby, S. & Antonelli, A. (2020)\n    <doi:10.1101/2020.01.17.910299>.  "
  },
  {
    "id": 8012,
    "package_name": "WH",
    "title": "Enhanced Implementation of Whittaker-Henderson Smoothing",
    "description": "An enhanced implementation of Whittaker-Henderson smoothing for the graduation \n    of one-dimensional and two-dimensional actuarial tables used to quantify Life Insurance risks.\n    'WH' is based on the methods described in Biessy (2025) <doi:10.48550/arXiv.2306.06932>.\n    Among other features, it generalizes the original smoothing algorithm to maximum likelihood estimation, \n    automatically selects the smoothing parameter(s) and extrapolates beyond the range of data.",
    "version": "2.0.0",
    "maintainer": "Guillaume Biessy <guillaume.biessy78@gmail.com>",
    "author": "Guillaume Biessy [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3756-7345>)",
    "url": "https://github.com/GuillaumeBiessy/WH",
    "bug_reports": "https://github.com/GuillaumeBiessy/WH/issues",
    "repository": "https://cran.r-project.org/package=WH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WH Enhanced Implementation of Whittaker-Henderson Smoothing An enhanced implementation of Whittaker-Henderson smoothing for the graduation \n    of one-dimensional and two-dimensional actuarial tables used to quantify Life Insurance risks.\n    'WH' is based on the methods described in Biessy (2025) <doi:10.48550/arXiv.2306.06932>.\n    Among other features, it generalizes the original smoothing algorithm to maximum likelihood estimation, \n    automatically selects the smoothing parameter(s) and extrapolates beyond the range of data.  "
  },
  {
    "id": 8116,
    "package_name": "YRmisc",
    "title": "Y&R Miscellaneous R Functions",
    "description": "Miscellaneous functions for data analysis, portfolio management, graphics, data manipulation, statistical investigation, including descriptive statistics, creating leading and lagging variables, portfolio return analysis, time series difference and percentage change calculation, stacking data for higher efficient analysis.",
    "version": "0.1.6",
    "maintainer": "Xuanhua (Peter) Yin <peteryin.sju@hotmail.com>",
    "author": "Manuel Russon <RUSSONM@stjohns.edu>, Xuanhua (Peter) Yin <peteryin.sju@hotmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=YRmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "YRmisc Y&R Miscellaneous R Functions Miscellaneous functions for data analysis, portfolio management, graphics, data manipulation, statistical investigation, including descriptive statistics, creating leading and lagging variables, portfolio return analysis, time series difference and percentage change calculation, stacking data for higher efficient analysis.  "
  },
  {
    "id": 8145,
    "package_name": "aLBI",
    "title": "Estimating Length-Based Indicators for Fish Stock",
    "description": "Provides tools for estimating length-based indicators from length frequency data to assess fish stock status and manage fisheries sustainably. Implements methods from Cope and Punt (2009) <doi:10.1577/C08-025.1> for data-limited stock assessment and Froese (2004) <doi:10.1111/j.1467-2979.2004.00144.x> for detecting overfishing using simple indicators. Key functions include:\n    FrequencyTable(): Calculate the frequency table from the collected and also the extract the length frequency data from the frequency table with the upper length_range. A numeric value specifying the bin width for class intervals. If not provided, the bin width is automatically calculated using Sturges (1926) <doi:10.1080/01621459.1926.10502161> formula.\n    CalPar(): Calculates various lengths used in fish stock assessment as biological length indicators such as asymptotic length (Linf), maximum length (Lmax), length at sexual maturity (Lm), and optimal length (Lopt).\n    FishPar(): Calculates length-based indicators (LBIs) proposed by Froese (2004) <doi:10.1111/j.1467-2979.2004.00144.x> such as the percentage of mature fish (Pmat), percentage of optimal length fish (Popt), percentage of mega spawners (Pmega), and the sum of these as Pobj. This function also estimates confidence intervals for different lengths, visualizes length frequency distributions, and provides data frames containing calculated values.\n    FishSS(): Makes decisions based on input from Cope and Punt (2009) <doi:10.1577/C08-025.1> and parameters calculated by FishPar() (e.g., Pobj, Pmat, Popt, LM_ratio) to determine stock status as target spawning biomass (TSB40) and limit spawning biomass (LSB25).\n    LWR(): Fits and visualizes length-weight relationships using linear regression, with options for log-transformation and customizable plotting.",
    "version": "0.1.8",
    "maintainer": "Ataher Ali <ataher.cu.ms@gmail.com>",
    "author": "Ataher Ali [aut, cre],\n  Mohammed Shahidul Alam [aut]",
    "url": "https://github.com/Ataher76/aLBI",
    "bug_reports": "https://github.com/Ataher76/aLBI/issues",
    "repository": "https://cran.r-project.org/package=aLBI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aLBI Estimating Length-Based Indicators for Fish Stock Provides tools for estimating length-based indicators from length frequency data to assess fish stock status and manage fisheries sustainably. Implements methods from Cope and Punt (2009) <doi:10.1577/C08-025.1> for data-limited stock assessment and Froese (2004) <doi:10.1111/j.1467-2979.2004.00144.x> for detecting overfishing using simple indicators. Key functions include:\n    FrequencyTable(): Calculate the frequency table from the collected and also the extract the length frequency data from the frequency table with the upper length_range. A numeric value specifying the bin width for class intervals. If not provided, the bin width is automatically calculated using Sturges (1926) <doi:10.1080/01621459.1926.10502161> formula.\n    CalPar(): Calculates various lengths used in fish stock assessment as biological length indicators such as asymptotic length (Linf), maximum length (Lmax), length at sexual maturity (Lm), and optimal length (Lopt).\n    FishPar(): Calculates length-based indicators (LBIs) proposed by Froese (2004) <doi:10.1111/j.1467-2979.2004.00144.x> such as the percentage of mature fish (Pmat), percentage of optimal length fish (Popt), percentage of mega spawners (Pmega), and the sum of these as Pobj. This function also estimates confidence intervals for different lengths, visualizes length frequency distributions, and provides data frames containing calculated values.\n    FishSS(): Makes decisions based on input from Cope and Punt (2009) <doi:10.1577/C08-025.1> and parameters calculated by FishPar() (e.g., Pobj, Pmat, Popt, LM_ratio) to determine stock status as target spawning biomass (TSB40) and limit spawning biomass (LSB25).\n    LWR(): Fits and visualizes length-weight relationships using linear regression, with options for log-transformation and customizable plotting.  "
  },
  {
    "id": 8150,
    "package_name": "aRD",
    "title": "Adjusted Risk Differences via Specifically Penalized Likelihood",
    "description": "Fits a linear-binomial model using a modified Newton-type \n    algorithm for solving the maximum likelihood \n    estimation problem under linear box constraints. Similar methods are \n    described in Wagenpfeil, Sch\u00f6pe and Bekhit (2025, ISBN:9783111341972) \n    \"Estimation of adjusted relative risks in log-binomial regression \n    using the BSW algorithm\". In: Mau, Mukhin, Wang and Xu (Eds.), \n    Biokybernetika. De Gruyter, Berlin, pp. 665\u2013676.",
    "version": "0.1.0",
    "maintainer": "Thomas Wolf <imbei@med-imbei.uni-saarland.de>",
    "author": "Thomas Wolf [aut, cre],\n  Julius Johannes Weise [aut],\n  Stefan Wagenpfeil [aut]",
    "url": "https://github.com/UdS-MF-IMBEI/aRD",
    "bug_reports": "https://github.com/UdS-MF-IMBEI/aRD/issues",
    "repository": "https://cran.r-project.org/package=aRD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aRD Adjusted Risk Differences via Specifically Penalized Likelihood Fits a linear-binomial model using a modified Newton-type \n    algorithm for solving the maximum likelihood \n    estimation problem under linear box constraints. Similar methods are \n    described in Wagenpfeil, Sch\u00f6pe and Bekhit (2025, ISBN:9783111341972) \n    \"Estimation of adjusted relative risks in log-binomial regression \n    using the BSW algorithm\". In: Mau, Mukhin, Wang and Xu (Eds.), \n    Biokybernetika. De Gruyter, Berlin, pp. 665\u2013676.  "
  },
  {
    "id": 8163,
    "package_name": "abcrlda",
    "title": "Asymptotically Bias-Corrected Regularized Linear Discriminant\nAnalysis",
    "description": "Offers methods to perform asymptotically bias-corrected regularized linear discriminant analysis (ABC_RLDA) for cost-sensitive binary classification. The bias-correction is an estimate of the bias term added to regularized discriminant analysis (RLDA) that minimizes the overall risk. The default magnitude of misclassification costs are equal and set to 0.5; however, the package also offers the options to set them to some predetermined values or, alternatively, take them as hyperparameters to tune.\n    A. Zollanvari, M. Abdirash, A. Dadlani and B. Abibullaev (2019) <doi:10.1109/LSP.2019.2918485>.",
    "version": "1.0.3",
    "maintainer": "Dmitriy Fedorov <dmitriy.fedorov@nu.edu.kz>",
    "author": "Dmitriy Fedorov [aut, cre],\n  Amin Zollanvari [aut],\n  Aresh Dadlani [aut],\n  Berdakh Abibullaev [aut]",
    "url": "https://ieeexplore.ieee.org/document/8720003/,\nhttps://dx.doi.org/10.1109/LSP.2019.2918485",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=abcrlda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "abcrlda Asymptotically Bias-Corrected Regularized Linear Discriminant\nAnalysis Offers methods to perform asymptotically bias-corrected regularized linear discriminant analysis (ABC_RLDA) for cost-sensitive binary classification. The bias-correction is an estimate of the bias term added to regularized discriminant analysis (RLDA) that minimizes the overall risk. The default magnitude of misclassification costs are equal and set to 0.5; however, the package also offers the options to set them to some predetermined values or, alternatively, take them as hyperparameters to tune.\n    A. Zollanvari, M. Abdirash, A. Dadlani and B. Abibullaev (2019) <doi:10.1109/LSP.2019.2918485>.  "
  },
  {
    "id": 8233,
    "package_name": "actuaRE",
    "title": "Handling Hierarchically Structured Risk Factors using Random\nEffects Models",
    "description": "Using this package, you can fit a random effects model using either the hierarchical credibility model, a combination of the hierarchical credibility model with a generalized linear model or a Tweedie generalized linear mixed model. See Campo, B.D.C. and Antonio, K. (2023) <doi:10.1080/03461238.2022.2161413>.",
    "version": "0.1.7",
    "maintainer": "Campo Bavo D.C. <bavo.decock@kuleuven.be>",
    "author": "Campo Bavo D.C. [aut, cre]",
    "url": "https://bavodc.github.io/websiteactuaRE/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=actuaRE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "actuaRE Handling Hierarchically Structured Risk Factors using Random\nEffects Models Using this package, you can fit a random effects model using either the hierarchical credibility model, a combination of the hierarchical credibility model with a generalized linear model or a Tweedie generalized linear mixed model. See Campo, B.D.C. and Antonio, K. (2023) <doi:10.1080/03461238.2022.2161413>.  "
  },
  {
    "id": 8234,
    "package_name": "actuar",
    "title": "Actuarial Functions and Heavy Tailed Distributions",
    "description": "Functions and data sets for actuarial science:\n  modeling of loss distributions; risk theory and ruin theory;\n  simulation of compound models, discrete mixtures and compound\n  hierarchical models; credibility theory. Support for many additional\n  probability distributions to model insurance loss size and\n  frequency: 23 continuous heavy tailed distributions; the\n  Poisson-inverse Gaussian discrete distribution; zero-truncated and\n  zero-modified extensions of the standard discrete distributions.\n  Support for phase-type distributions commonly used to compute ruin\n  probabilities. Main reference: <doi:10.18637/jss.v025.i07>.\n  Implementation of the Feller-Pareto family of distributions:\n  <doi:10.18637/jss.v103.i06>.",
    "version": "3.3-6",
    "maintainer": "Vincent Goulet <vincent.goulet@act.ulaval.ca>",
    "author": "Vincent Goulet [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9315-5719>),\n  S\u00e9bastien Auclair [ctb],\n  Christophe Dutang [aut] (ORCID:\n    <https://orcid.org/0000-0001-6732-1501>),\n  Walter Garcia-Fontes [ctb],\n  Nicholas Langevin [ctb],\n  Xavier Milhaud [ctb],\n  Tommy Ouellet [ctb],\n  Alexandre Parent [ctb],\n  Mathieu Pigeon [aut],\n  Louis-Philippe Pouliot [ctb],\n  Jeffrey A. Ryan [aut] (Package API),\n  Robert Gentleman [aut] (Parts of the R to C interface),\n  Ross Ihaka [aut] (Parts of the R to C interface),\n  R Core Team [aut] (Parts of the R to C interface),\n  R Foundation [aut] (Parts of the R to C interface)",
    "url": "https://gitlab.com/vigou3/actuar",
    "bug_reports": "https://gitlab.com/vigou3/actuar/-/issues",
    "repository": "https://cran.r-project.org/package=actuar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "actuar Actuarial Functions and Heavy Tailed Distributions Functions and data sets for actuarial science:\n  modeling of loss distributions; risk theory and ruin theory;\n  simulation of compound models, discrete mixtures and compound\n  hierarchical models; credibility theory. Support for many additional\n  probability distributions to model insurance loss size and\n  frequency: 23 continuous heavy tailed distributions; the\n  Poisson-inverse Gaussian discrete distribution; zero-truncated and\n  zero-modified extensions of the standard discrete distributions.\n  Support for phase-type distributions commonly used to compute ruin\n  probabilities. Main reference: <doi:10.18637/jss.v025.i07>.\n  Implementation of the Feller-Pareto family of distributions:\n  <doi:10.18637/jss.v103.i06>.  "
  },
  {
    "id": 8236,
    "package_name": "actxps",
    "title": "Create Actuarial Experience Studies: Prepare Data, Summarize\nResults, and Create Reports",
    "description": "Experience studies are used by actuaries to explore historical\n    experience across blocks of business and to inform assumption setting\n    activities. This package provides functions for preparing data, creating \n    studies, visualizing results, and beginning assumption development. \n    Experience study methods, including exposure calculations, are described in:\n    Atkinson & McGarry (2016) \"Experience Study Calculations\" \n    <https://www.soa.org/49378a/globalassets/assets/files/research/experience-study-calculations.pdf>.\n    The limited fluctuation credibility method used by the 'exp_stats()'\n    function is described in: Herzog (1999, ISBN:1-56698-374-6) \n    \"Introduction to Credibility Theory\".",
    "version": "1.6.1",
    "maintainer": "Matt Heaphy <mattrmattrs@gmail.com>",
    "author": "Matt Heaphy [aut, cre]",
    "url": "https://github.com/mattheaphy/actxps/,\nhttps://mattheaphy.github.io/actxps/",
    "bug_reports": "https://github.com/mattheaphy/actxps/issues",
    "repository": "https://cran.r-project.org/package=actxps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "actxps Create Actuarial Experience Studies: Prepare Data, Summarize\nResults, and Create Reports Experience studies are used by actuaries to explore historical\n    experience across blocks of business and to inform assumption setting\n    activities. This package provides functions for preparing data, creating \n    studies, visualizing results, and beginning assumption development. \n    Experience study methods, including exposure calculations, are described in:\n    Atkinson & McGarry (2016) \"Experience Study Calculations\" \n    <https://www.soa.org/49378a/globalassets/assets/files/research/experience-study-calculations.pdf>.\n    The limited fluctuation credibility method used by the 'exp_stats()'\n    function is described in: Herzog (1999, ISBN:1-56698-374-6) \n    \"Introduction to Credibility Theory\".  "
  },
  {
    "id": 8266,
    "package_name": "adcontabil",
    "title": "Accounting Analysis",
    "description": "Provides methods for processing corporate balance sheets with a focus on the Brazilian reporting format. Includes data standardization, classification by accounting categories, and aggregation of values. Supports accounting and financial analyses of companies, improving efficiency and ensuring reproducibility of empirical studies.",
    "version": "1.1.8",
    "maintainer": "Lissandro Costa de Sousa <lisandrosousa54@gmail.com>",
    "author": "Lissandro Costa de Sousa [cre, aut],\n  Francisco Gildemir Ferreira da Silva [ths, aut]",
    "url": "https://github.com/LissandroSousa/adcontabil.R",
    "bug_reports": "https://github.com/LissandroSousa/adcontabil.R/issues",
    "repository": "https://cran.r-project.org/package=adcontabil",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adcontabil Accounting Analysis Provides methods for processing corporate balance sheets with a focus on the Brazilian reporting format. Includes data standardization, classification by accounting categories, and aggregation of values. Supports accounting and financial analyses of companies, improving efficiency and ensuring reproducibility of empirical studies.  "
  },
  {
    "id": 8299,
    "package_name": "adjustedCurves",
    "title": "Confounder-Adjusted Survival Curves and Cumulative Incidence\nFunctions",
    "description": "Estimate and plot confounder-adjusted survival curves using\n    either 'Direct Adjustment', 'Direct Adjustment with Pseudo-Values',\n    various forms of 'Inverse Probability of Treatment Weighting', two\n    forms of 'Augmented Inverse Probability of Treatment Weighting',\n    'Empirical Likelihood Estimation' or 'Targeted Maximum Likelihood Estimation'.\n\tAlso includes a significance test for the difference\n    between two adjusted survival curves and the calculation of adjusted\n    restricted mean survival times.  Additionally enables the user to\n    estimate and plot cause-specific confounder-adjusted cumulative\n    incidence functions in the competing risks setting using the same\n    methods (with some exceptions).\n\tFor details, see Denz et. al (2023) <doi:10.1002/sim.9681>.",
    "version": "0.11.3",
    "maintainer": "Robin Denz <robin.denz@rub.de>",
    "author": "Robin Denz [aut, cre]",
    "url": "https://github.com/RobinDenz1/adjustedCurves,\nhttps://robindenz1.github.io/adjustedCurves/",
    "bug_reports": "https://github.com/RobinDenz1/adjustedCurves/issues",
    "repository": "https://cran.r-project.org/package=adjustedCurves",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adjustedCurves Confounder-Adjusted Survival Curves and Cumulative Incidence\nFunctions Estimate and plot confounder-adjusted survival curves using\n    either 'Direct Adjustment', 'Direct Adjustment with Pseudo-Values',\n    various forms of 'Inverse Probability of Treatment Weighting', two\n    forms of 'Augmented Inverse Probability of Treatment Weighting',\n    'Empirical Likelihood Estimation' or 'Targeted Maximum Likelihood Estimation'.\n\tAlso includes a significance test for the difference\n    between two adjusted survival curves and the calculation of adjusted\n    restricted mean survival times.  Additionally enables the user to\n    estimate and plot cause-specific confounder-adjusted cumulative\n    incidence functions in the competing risks setting using the same\n    methods (with some exceptions).\n\tFor details, see Denz et. al (2023) <doi:10.1002/sim.9681>.  "
  },
  {
    "id": 8303,
    "package_name": "admiral",
    "title": "ADaM in R Asset Library",
    "description": "A toolbox for programming Clinical Data Interchange Standards\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>).",
    "version": "1.3.1",
    "maintainer": "Ben Straub <ben.x.straub@gsk.com>",
    "author": "Ben Straub [aut, cre],\n  Stefan Bundfuss [aut] (ORCID: <https://orcid.org/0009-0005-0027-1198>),\n  Arianna Cascone [aut] (ORCID: <https://orcid.org/0000-0001-5948-2831>),\n  Jeffrey Dickinson [aut],\n  Ross Farrugia [aut],\n  Fanny Gautier [aut],\n  G Gayatri [aut],\n  Solveig Holmgaard [aut],\n  Dinakar Kulkarni [aut],\n  Edoardo Mancini [aut] (ORCID: <https://orcid.org/0009-0006-4899-8641>),\n  Gordon Miller [aut],\n  Jim Rothstein [aut] (ORCID: <https://orcid.org/0009-0009-8659-6071>),\n  Daniel Sjoberg [aut] (ORCID: <https://orcid.org/0000-0003-0862-2018>),\n  Stefan Thoma [aut] (ORCID: <https://orcid.org/0000-0002-5553-9252>),\n  Junze Zhang [aut],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiral/,\nhttps://github.com/pharmaverse/admiral",
    "bug_reports": "https://github.com/pharmaverse/admiral/issues",
    "repository": "https://cran.r-project.org/package=admiral",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiral ADaM in R Asset Library A toolbox for programming Clinical Data Interchange Standards\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>).  "
  },
  {
    "id": 8306,
    "package_name": "admiralmetabolic",
    "title": "Metabolism Extension Package for ADaM in 'R' Asset Library",
    "description": "A toolbox for programming Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>).  The package is\n    an extension package of the 'admiral' package focusing on the\n    metabolism therapeutic area.",
    "version": "0.2.0",
    "maintainer": "Anders Askeland <iakd@novonordisk.com>",
    "author": "Anders Askeland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2996-129X>),\n  Andrii Yurovskyi [aut] (ORCID: <https://orcid.org/0009-0006-8606-0929>),\n  Kathrin Flunkert [aut],\n  Edoardo Mancini [aut] (ORCID: <https://orcid.org/0009-0006-4899-8641>),\n  Shunsuke Goto [aut],\n  Siddhesh Pujari [aut] (ORCID: <https://orcid.org/0009-0003-4971-1217>),\n  Sonali Das [aut],\n  Olga Starostecka [aut],\n  Vang Le-Quy [aut] (ORCID: <https://orcid.org/0000-0003-2035-8619>),\n  Keita Takahashi [aut] (ORCID: <https://orcid.org/0009-0004-9847-5115>)",
    "url": "https://pharmaverse.github.io/admiralmetabolic/,\nhttps://github.com/pharmaverse/admiralmetabolic",
    "bug_reports": "https://github.com/pharmaverse/admiralmetabolic/issues",
    "repository": "https://cran.r-project.org/package=admiralmetabolic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiralmetabolic Metabolism Extension Package for ADaM in 'R' Asset Library A toolbox for programming Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>).  The package is\n    an extension package of the 'admiral' package focusing on the\n    metabolism therapeutic area.  "
  },
  {
    "id": 8307,
    "package_name": "admiralneuro",
    "title": "Neuroscience Extension Package for ADaM in 'R' Asset Library",
    "description": "Programming neuroscience Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>). This package extends \n    the 'admiral' package.",
    "version": "0.1.0",
    "maintainer": "Jian Wang <wang_jian_wj@lilly.com>",
    "author": "Jian Wang [aut, cre] (ORCID: <https://orcid.org/0009-0002-4677-3781>),\n  Miles Almond [aut] (ORCID: <https://orcid.org/0009-0007-1784-0355>),\n  Xiao Chen [aut] (ORCID: <https://orcid.org/0009-0000-6959-5151>),\n  Fanny Gautier [aut] (ORCID: <https://orcid.org/0009-0004-3581-0131>),\n  Gayatri G. [aut],\n  Meilin Jiang [aut] (ORCID: <https://orcid.org/0000-0003-4515-4567>),\n  Leena Khatri [aut] (ORCID: <https://orcid.org/0000-0002-2268-4023>),\n  Edoardo Mancini [aut] (ORCID: <https://orcid.org/0009-0006-4899-8641>),\n  Eric Nantz [aut],\n  Lina Patil [aut],\n  Chris Pelentrides [aut],\n  Eli Lilly and Company [cph, fnd],\n  Cytel Inc. [cph, fnd],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiralneuro/,\nhttps://github.com/pharmaverse/admiralneuro",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=admiralneuro",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiralneuro Neuroscience Extension Package for ADaM in 'R' Asset Library Programming neuroscience Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>). This package extends \n    the 'admiral' package.  "
  },
  {
    "id": 8308,
    "package_name": "admiralonco",
    "title": "Oncology Extension Package for ADaM in 'R' Asset Library",
    "description": "Programming oncology specific Clinical Data Interchange Standards Consortium\n    (CDISC) compliant Analysis Data Model (ADaM) datasets in 'R'. ADaM datasets are a\n    mandatory part of any New Drug or Biologics License Application submitted to the\n    United States Food and Drug Administration (FDA). Analysis derivations are\n    implemented in accordance with the \"Analysis Data Model Implementation Guide\"\n    (CDISC Analysis Data Model Team (2021), <https://www.cdisc.org/standards/foundational/adam>).\n    The package is an extension package of the 'admiral' package.",
    "version": "1.3.0",
    "maintainer": "Stefan Bundfuss <stefan.bundfuss@roche.com>",
    "author": "Stefan Bundfuss [aut, cre],\n  Amit Jain [aut],\n  Vinh Nguyen [aut],\n  Olga Starostecka [aut],\n  Kiran Peddamudium [aut],\n  Tomoyuki Namai [aut],\n  Ross Farrugia [aut],\n  Yirong Cao [ctb],\n  Ashwini Weber [ctb],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd],\n  Amgen Inc. [cph, fnd],\n  Bristol Myers Squibb [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiralonco/,\nhttps://github.com/pharmaverse/admiralonco",
    "bug_reports": "https://github.com/pharmaverse/admiralonco/issues",
    "repository": "https://cran.r-project.org/package=admiralonco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiralonco Oncology Extension Package for ADaM in 'R' Asset Library Programming oncology specific Clinical Data Interchange Standards Consortium\n    (CDISC) compliant Analysis Data Model (ADaM) datasets in 'R'. ADaM datasets are a\n    mandatory part of any New Drug or Biologics License Application submitted to the\n    United States Food and Drug Administration (FDA). Analysis derivations are\n    implemented in accordance with the \"Analysis Data Model Implementation Guide\"\n    (CDISC Analysis Data Model Team (2021), <https://www.cdisc.org/standards/foundational/adam>).\n    The package is an extension package of the 'admiral' package.  "
  },
  {
    "id": 8309,
    "package_name": "admiralophtha",
    "title": "ADaM in R Asset Library - Ophthalmology",
    "description": "Aids the programming of Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Ophthalmology Analysis Data Model (ADaM)\n    datasets in R. ADaM datasets are a mandatory part of any New Drug or\n    Biologics License Application submitted to the United States Food and\n    Drug Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam/adamig-v1-3-release-package>).",
    "version": "1.3.0",
    "maintainer": "Edoardo Mancini <edoardo.mancini@roche.com>",
    "author": "Edoardo Mancini [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-4899-8641>),\n  Ritika Aggarwal [aut],\n  Jane Gao [aut],\n  William Holmes [aut],\n  Josie Jackson [aut],\n  Sonali Jain [aut],\n  Yuki Matsunaga [aut],\n  Gordon Miller [aut],\n  Rachel Linacre [aut],\n  Lucy Palmen [aut],\n  Nandini R Thampi [aut],\n  Aldrich Salva [aut],\n  Steven Ting [aut]",
    "url": "https://pharmaverse.github.io/admiralophtha/,\nhttps://github.com/pharmaverse/admiralophtha/",
    "bug_reports": "https://github.com/pharmaverse/admiralophtha/issues/",
    "repository": "https://cran.r-project.org/package=admiralophtha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiralophtha ADaM in R Asset Library - Ophthalmology Aids the programming of Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Ophthalmology Analysis Data Model (ADaM)\n    datasets in R. ADaM datasets are a mandatory part of any New Drug or\n    Biologics License Application submitted to the United States Food and\n    Drug Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam/adamig-v1-3-release-package>).  "
  },
  {
    "id": 8310,
    "package_name": "admiralpeds",
    "title": "Pediatrics Extension Package for ADaM in 'R' Asset Library",
    "description": "A toolbox for programming Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>). The package is\n    an extension package of the 'admiral' package for pediatric clinical\n    trials.",
    "version": "0.2.1",
    "maintainer": "Fanny Gautier <fanny.gautier@cytel.com>",
    "author": "Fanny Gautier [aut, cre] (ORCID:\n    <https://orcid.org/0009-0004-3581-0131>),\n  Ross Farrugia [aut],\n  Zelos Zhu [aut],\n  Sukalpo Saha [aut],\n  Lina Patil [aut],\n  Samia Kabi [aut],\n  Laura Liao [ctb],\n  Remigiusz Kudlacz [ctb],\n  Pierre Wallet [ctb],\n  Amin Sherzad [ctb],\n  David Freedman [ctb],\n  Mahmoud Hamza [ctb],\n  Cytel Inc. [cph, fnd],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  Pfizer Inc. [cph, fnd],\n  Atorus Research [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiralpeds/,\nhttps://github.com/pharmaverse/admiralpeds",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=admiralpeds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiralpeds Pediatrics Extension Package for ADaM in 'R' Asset Library A toolbox for programming Clinical Data Standards Interchange\n    Consortium (CDISC) compliant Analysis Data Model (ADaM) datasets in R.\n    ADaM datasets are a mandatory part of any New Drug or Biologics\n    License Application submitted to the United States Food and Drug\n    Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team, 2021,\n    <https://www.cdisc.org/standards/foundational/adam>). The package is\n    an extension package of the 'admiral' package for pediatric clinical\n    trials.  "
  },
  {
    "id": 8311,
    "package_name": "admiralvaccine",
    "title": "Vaccine Extension Package for ADaM in 'R' Asset Library",
    "description": "Programming vaccine specific Clinical Data Interchange\n    Standards Consortium (CDISC) compliant Analysis Data Model (ADaM)\n    datasets in 'R'. Flat model is followed as per Center for Biologics\n    Evaluation and Research (CBER) guidelines for creating vaccine\n    specific domains. ADaM datasets are a mandatory part of any New Drug\n    or Biologics License Application submitted to the United States Food\n    and Drug Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team (2021),\n    <https://www.cdisc.org/standards/foundational/adam/adamig-v1-3-release-package>).\n    The package is an extension package of the 'admiral' package.",
    "version": "0.5.0",
    "maintainer": "Arjun Rubalingam <arjun.rubalingam@pfizer.com>",
    "author": "Arjun Rubalingam [aut, cre],\n  Sukalpo Saha [aut, ctb],\n  Ben Straub [aut],\n  Vikram S [aut],\n  Dhivya Kanagaraj [aut],\n  Federico Baratin [aut],\n  Yamini Purna Bollu [aut],\n  Ilse Augustyns [aut],\n  Kalyani Bodicherla [aut],\n  Hilde Delanghe [aut],\n  Lee Armishaw [aut],\n  Neetu Sangari [ctb],\n  Abdul Khayat [ctb],\n  Ankur Jindal [ctb],\n  Jayashree V [ctb],\n  Jagadish Katam [ctb],\n  Andrea Pammolli [ctb],\n  Daniele Bottigliengo [ctb],\n  Ranya Ben Hsain [ctb],\n  Marleen Nijs [ctb],\n  Mandy Peng [ctb],\n  Tina Zhai [ctb],\n  Ross Farrugia [ctb],\n  Stefan Bundfuss [ctb],\n  Edoardo Mancini [ctb],\n  Pfizer Inc. [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd],\n  Johnson & Johnson [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiralvaccine/,\nhttps://github.com/pharmaverse/admiralvaccine/",
    "bug_reports": "https://github.com/pharmaverse/admiralvaccine/issues/",
    "repository": "https://cran.r-project.org/package=admiralvaccine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiralvaccine Vaccine Extension Package for ADaM in 'R' Asset Library Programming vaccine specific Clinical Data Interchange\n    Standards Consortium (CDISC) compliant Analysis Data Model (ADaM)\n    datasets in 'R'. Flat model is followed as per Center for Biologics\n    Evaluation and Research (CBER) guidelines for creating vaccine\n    specific domains. ADaM datasets are a mandatory part of any New Drug\n    or Biologics License Application submitted to the United States Food\n    and Drug Administration (FDA). Analysis derivations are implemented in\n    accordance with the \"Analysis Data Model Implementation Guide\" (CDISC\n    Analysis Data Model Team (2021),\n    <https://www.cdisc.org/standards/foundational/adam/adamig-v1-3-release-package>).\n    The package is an extension package of the 'admiral' package.  "
  },
  {
    "id": 8407,
    "package_name": "airt",
    "title": "Evaluation of Algorithm Collections Using Item Response Theory",
    "description": "An evaluation framework for algorithm portfolios using Item Response\n    Theory (IRT). We use continuous and polytomous IRT models to evaluate algorithms and introduce\n    algorithm characteristics such as stability, effectiveness and anomalousness \n    (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>.",
    "version": "0.2.2",
    "maintainer": "Sevvandi Kandanaarachchi <sevvandik@gmail.com>",
    "author": "Sevvandi Kandanaarachchi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0337-0395>)",
    "url": "https://sevvandi.github.io/airt/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=airt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "airt Evaluation of Algorithm Collections Using Item Response Theory An evaluation framework for algorithm portfolios using Item Response\n    Theory (IRT). We use continuous and polytomous IRT models to evaluate algorithms and introduce\n    algorithm characteristics such as stability, effectiveness and anomalousness \n    (Kandanaarachchi, Smith-Miles 2020) <doi:10.13140/RG.2.2.11363.09760>.  "
  },
  {
    "id": 8434,
    "package_name": "allestimates",
    "title": "Effect Estimates from All Models",
    "description": "Estimates and plots effect estimates from models with all possible \n    combinations of a list of variables. It can be used for assessing treatment \n    effects in clinical trials or risk factors in bio-medical and epidemiological \n    research. Like Stata command 'confall' (Wang Z (2007) <doi:10.1177/1536867X0700700203> ), \n    'allestimates' calculates and stores all effect estimates, and plots them against p values or \n    Akaike information criterion (AIC) values. It currently has functions for linear \n    regression: all_lm(), logistic and Poisson regression: all_glm(), \n    and Cox proportional hazards regression: all_cox(). ",
    "version": "0.2.3",
    "maintainer": "Zhiqiang Wang <menzies.uq@gmail.com>",
    "author": "Zhiqiang Wang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=allestimates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "allestimates Effect Estimates from All Models Estimates and plots effect estimates from models with all possible \n    combinations of a list of variables. It can be used for assessing treatment \n    effects in clinical trials or risk factors in bio-medical and epidemiological \n    research. Like Stata command 'confall' (Wang Z (2007) <doi:10.1177/1536867X0700700203> ), \n    'allestimates' calculates and stores all effect estimates, and plots them against p values or \n    Akaike information criterion (AIC) values. It currently has functions for linear \n    regression: all_lm(), logistic and Poisson regression: all_glm(), \n    and Cox proportional hazards regression: all_cox().   "
  },
  {
    "id": 8461,
    "package_name": "altmeta",
    "title": "Alternative Meta-Analysis Methods",
    "description": "Provides alternative statistical methods for meta-analysis, including:\n - bivariate generalized linear mixed models for synthesizing odds ratios, relative risks,\n   and risk differences\n   (Chu et al., 2012 <doi:10.1177/0962280210393712>)\n - tests and measures for between-study heterogeneity\n   (Lin et al., 2017 <doi:10.1111/biom.12543>;\n    Wang et al., 2022 <doi:10.1002/sim.9261>);\n - measures, tests, and visualization tools for publication bias or small-study effects\n   (Lin and Chu, 2018 <doi:10.1111/biom.12817>;\n    Lin, 2019 <doi:10.1002/jrsm.1340>;\n    Lin, 2020 <doi:10.1177/0962280220910172>;\n    Shi et al., 2020 <doi:10.1002/jrsm.1415>);\n - meta-analysis of combining standardized mean differences and odds ratios\n   (Jing et al., 2023 <doi:10.1080/10543406.2022.2105345>);\n - meta-analysis of diagnostic tests for synthesizing sensitivities, specificities, etc.\n   (Reitsma et al., 2005 <doi:10.1016/j.jclinepi.2005.02.022>;\n    Chu and Cole, 2006 <doi:10.1016/j.jclinepi.2006.06.011>);\n - meta-analysis methods for synthesizing proportions\n   (Lin and Chu, 2020 <doi:10.1097/ede.0000000000001232>);\n - models for multivariate meta-analysis, measures of inconsistency degrees of freedom\n   in Bayesian network meta-analysis, and predictive P-score\n   (Lin and Chu, 2018 <doi:10.1002/jrsm.1293>;\n    Lin, 2020 <doi:10.1080/10543406.2020.1852247>;\n    Rosenberger et al., 2021 <doi:10.1186/s12874-021-01397-5>).",
    "version": "4.3",
    "maintainer": "Lifeng Lin <lifenglin@arizona.edu>",
    "author": "Lifeng Lin [aut, cre] (ORCID: <https://orcid.org/0000-0002-3562-9816>),\n  Yaqi Jing [ctb],\n  Kristine J. Rosenberger [ctb],\n  Linyu Shi [ctb],\n  Yipeng Wang [ctb],\n  Xing Xing [ctb] (ORCID: <https://orcid.org/0000-0001-9394-8957>),\n  Zhiyuan Yu [ctb],\n  Haitao Chu [aut] (ORCID: <https://orcid.org/0000-0003-0932-598X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=altmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "altmeta Alternative Meta-Analysis Methods Provides alternative statistical methods for meta-analysis, including:\n - bivariate generalized linear mixed models for synthesizing odds ratios, relative risks,\n   and risk differences\n   (Chu et al., 2012 <doi:10.1177/0962280210393712>)\n - tests and measures for between-study heterogeneity\n   (Lin et al., 2017 <doi:10.1111/biom.12543>;\n    Wang et al., 2022 <doi:10.1002/sim.9261>);\n - measures, tests, and visualization tools for publication bias or small-study effects\n   (Lin and Chu, 2018 <doi:10.1111/biom.12817>;\n    Lin, 2019 <doi:10.1002/jrsm.1340>;\n    Lin, 2020 <doi:10.1177/0962280220910172>;\n    Shi et al., 2020 <doi:10.1002/jrsm.1415>);\n - meta-analysis of combining standardized mean differences and odds ratios\n   (Jing et al., 2023 <doi:10.1080/10543406.2022.2105345>);\n - meta-analysis of diagnostic tests for synthesizing sensitivities, specificities, etc.\n   (Reitsma et al., 2005 <doi:10.1016/j.jclinepi.2005.02.022>;\n    Chu and Cole, 2006 <doi:10.1016/j.jclinepi.2006.06.011>);\n - meta-analysis methods for synthesizing proportions\n   (Lin and Chu, 2020 <doi:10.1097/ede.0000000000001232>);\n - models for multivariate meta-analysis, measures of inconsistency degrees of freedom\n   in Bayesian network meta-analysis, and predictive P-score\n   (Lin and Chu, 2018 <doi:10.1002/jrsm.1293>;\n    Lin, 2020 <doi:10.1080/10543406.2020.1852247>;\n    Rosenberger et al., 2021 <doi:10.1186/s12874-021-01397-5>).  "
  },
  {
    "id": 8487,
    "package_name": "amscorer",
    "title": "Clinical Scores Calculator for Healthcare",
    "description": "Provides functions to compute various clinical scores used in healthcare.\n            These include the Charlson Comorbidity Index (CCI), predicting 10-year survival in patients with multiple comorbidities; \n            the EPICES score, an individual indicator of precariousness considering its multidimensional nature;\n            the MELD score for chronic liver disease severity; the Alternative Fistula Risk Score (a-FRS) for postoperative pancreatic fistula risk;\n            and the Distal Pancreatectomy Fistula Risk Score (D-FRS) for risk following distal pancreatectomy.\n            For detailed methodology, refer to Charlson et al. (1987) <doi:10.1016/0021-9681(87)90171-8> ,\n            Sass et al. (2006) <doi:10.1007/s10332-006-0131-5>, \n            Kamath et al. (2001) <doi:10.1053/jhep.2001.22172>,\n            Kim et al. (2008) <doi:10.1056/NEJMoa0801209>\n            Kim et al. (2021) <doi:10.1053/j.gastro.2021.08.050>,\n            Mungroop et al. (2019) <doi:10.1097/SLA.0000000000002620>,\n            and de Pastena et al. (2023) <doi:10.1097/SLA.0000000000005497>..",
    "version": "0.1.0",
    "maintainer": "Amadou Khalilou Sow <kkhalilou.sow@gmail.com>",
    "author": "Amadou Khalilou Sow [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8073-9128>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=amscorer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amscorer Clinical Scores Calculator for Healthcare Provides functions to compute various clinical scores used in healthcare.\n            These include the Charlson Comorbidity Index (CCI), predicting 10-year survival in patients with multiple comorbidities; \n            the EPICES score, an individual indicator of precariousness considering its multidimensional nature;\n            the MELD score for chronic liver disease severity; the Alternative Fistula Risk Score (a-FRS) for postoperative pancreatic fistula risk;\n            and the Distal Pancreatectomy Fistula Risk Score (D-FRS) for risk following distal pancreatectomy.\n            For detailed methodology, refer to Charlson et al. (1987) <doi:10.1016/0021-9681(87)90171-8> ,\n            Sass et al. (2006) <doi:10.1007/s10332-006-0131-5>, \n            Kamath et al. (2001) <doi:10.1053/jhep.2001.22172>,\n            Kim et al. (2008) <doi:10.1056/NEJMoa0801209>\n            Kim et al. (2021) <doi:10.1053/j.gastro.2021.08.050>,\n            Mungroop et al. (2019) <doi:10.1097/SLA.0000000000002620>,\n            and de Pastena et al. (2023) <doi:10.1097/SLA.0000000000005497>..  "
  },
  {
    "id": 8555,
    "package_name": "apisensr",
    "title": "Interface to 'episensr' for Sensitivity Analysis of\nEpidemiological Results",
    "description": "API for using 'episensr', Basic sensitivity analysis of the\n    observed relative risks adjusting for unmeasured confounding and\n    misclassification of the exposure/outcome, or both. See\n    <https://cran.r-project.org/package=episensr>.",
    "version": "2.0.0",
    "maintainer": "Denis Haine <cheval@zaclys.net>",
    "author": "Denis Haine [aut, cre] (ORCID: <https://orcid.org/0000-0002-6691-7335>)",
    "url": "https://codeberg.org/dhaine/apisensr",
    "bug_reports": "https://codeberg.org/dhaine/apisensr/issues",
    "repository": "https://cran.r-project.org/package=apisensr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apisensr Interface to 'episensr' for Sensitivity Analysis of\nEpidemiological Results API for using 'episensr', Basic sensitivity analysis of the\n    observed relative risks adjusting for unmeasured confounding and\n    misclassification of the exposure/outcome, or both. See\n    <https://cran.r-project.org/package=episensr>.  "
  },
  {
    "id": 8620,
    "package_name": "argminCS",
    "title": "Argmin Inference over a Discrete Candidate Set",
    "description": "Provides methods to construct frequentist confidence sets with valid marginal \n    coverage for identifying the population-level argmin or argmax based on IID data. \n    For instance, given an n by p loss matrix\u2014where n is the sample size and p is the \n    number of models\u2014the CS.argmin() method produces a discrete confidence set that contains \n    the model with the minimal (best) expected risk with desired probability. The argmin.HT() \n    method helps check if a specific model should be included in such a confidence set. The main\n    implemented method is proposed by Tianyu Zhang, Hao Lee and Jing Lei (2024) \n    \"Winners with confidence: Discrete argmin inference with an application to model selection\".",
    "version": "1.1.0",
    "maintainer": "Hao Lee <haolee@andrew.cmu.edu>",
    "author": "Tianyu Zhang [aut],\n  Hao Lee [aut, cre, cph],\n  Jing Lei [aut]",
    "url": "https://github.com/xu3cl4/argminCS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=argminCS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "argminCS Argmin Inference over a Discrete Candidate Set Provides methods to construct frequentist confidence sets with valid marginal \n    coverage for identifying the population-level argmin or argmax based on IID data. \n    For instance, given an n by p loss matrix\u2014where n is the sample size and p is the \n    number of models\u2014the CS.argmin() method produces a discrete confidence set that contains \n    the model with the minimal (best) expected risk with desired probability. The argmin.HT() \n    method helps check if a specific model should be included in such a confidence set. The main\n    implemented method is proposed by Tianyu Zhang, Hao Lee and Jing Lei (2024) \n    \"Winners with confidence: Discrete argmin inference with an application to model selection\".  "
  },
  {
    "id": 8635,
    "package_name": "armaOptions",
    "title": "ARMA Models to Value Stock Options",
    "description": "Providing ways to estimate the value of European stock options \n    given historical stock price data. It includes functions for calculating option values \n    based on autoregressive\u2013moving-average (ARMA) models and generates information about these models. \n    This package is made to be easy to understand and for financial analysis capabilities. ",
    "version": "1.0.1",
    "maintainer": "Brian MacCarvill <brianmaccarvills@gmail.com>",
    "author": "Brian MacCarvill [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=armaOptions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "armaOptions ARMA Models to Value Stock Options Providing ways to estimate the value of European stock options \n    given historical stock price data. It includes functions for calculating option values \n    based on autoregressive\u2013moving-average (ARMA) models and generates information about these models. \n    This package is made to be easy to understand and for financial analysis capabilities.   "
  },
  {
    "id": 8675,
    "package_name": "asm",
    "title": "Optimal Convex M-Estimation for Linear Regression via Antitonic\nScore Matching",
    "description": "Performs linear regression with respect to a data-driven convex loss function that is chosen to minimize the asymptotic covariance of the resulting M-estimator. The convex loss function is estimated in 5 steps: (1) form an initial OLS (ordinary least squares) or LAD (least absolute deviation) estimate of the regression coefficients; (2) use the resulting residuals to obtain a kernel estimator of the error density; (3) estimate the score function of the errors by differentiating the logarithm of the kernel density estimate; (4) compute the L2 projection of the estimated score function onto the set of decreasing functions; (5) take a negative antiderivative of the projected score function estimate. Newton's method (with Hessian modification) is then used to minimize the convex empirical risk function. Further details of the method are given in Feng et al. (2024) <doi:10.48550/arXiv.2403.16688>.",
    "version": "0.2.4",
    "maintainer": "Min Xu <min.cut@gmail.com>",
    "author": "Yu-Chun Kao [aut],\n  Oliver Y. Feng [aut],\n  Lucy Xia [aut],\n  Yang Feng [aut],\n  Min Xu [aut, cre],\n  Richard J. Samworth [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=asm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asm Optimal Convex M-Estimation for Linear Regression via Antitonic\nScore Matching Performs linear regression with respect to a data-driven convex loss function that is chosen to minimize the asymptotic covariance of the resulting M-estimator. The convex loss function is estimated in 5 steps: (1) form an initial OLS (ordinary least squares) or LAD (least absolute deviation) estimate of the regression coefficients; (2) use the resulting residuals to obtain a kernel estimator of the error density; (3) estimate the score function of the errors by differentiating the logarithm of the kernel density estimate; (4) compute the L2 projection of the estimated score function onto the set of decreasing functions; (5) take a negative antiderivative of the projected score function estimate. Newton's method (with Hessian modification) is then used to minimize the convex empirical risk function. Further details of the method are given in Feng et al. (2024) <doi:10.48550/arXiv.2403.16688>.  "
  },
  {
    "id": 8693,
    "package_name": "asteRisk",
    "title": "Computation of Satellite Position",
    "description": "Provides basic functionalities to calculate the position of\n    satellites given a known state vector. The package includes implementations\n    of the SGP4 and SDP4 simplified perturbation models to propagate orbital\n    state vectors, as well as utilities to read TLE files and convert coordinates\n    between different frames of reference. Several of the functionalities of the\n    package (including the high-precision numerical orbit propagator) require\n    the coefficients and data included in the 'asteRiskData' package, available\n    in a 'drat' repository. To install this data package, run \n    'install.packages(\"asteRiskData\", repos=\"https://rafael-ayala.github.io/drat/\")'.\n    Felix R. Hoots, Ronald L. Roehrich and T.S. Kelso (1988) <https://celestrak.org/NORAD/documentation/spacetrk.pdf>.\n    David Vallado, Paul Crawford, Richard Hujsak and T.S. Kelso (2012) <doi:10.2514/6.2006-6753>.\n    Felix R. Hoots, Paul W. Schumacher Jr. and Robert A. Glover (2014) <doi:10.2514/1.9161>.",
    "version": "1.4.5",
    "maintainer": "Rafael Ayala <rafaelayalahernandez@gmail.com>",
    "author": "Rafael Ayala [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9332-4623>),\n  Daniel Ayala [aut] (ORCID: <https://orcid.org/0000-0003-2095-1009>),\n  David Ruiz [aut] (ORCID: <https://orcid.org/0000-0003-4460-5493>),\n  Pablo Hernandez [aut] (ORCID: <https://orcid.org/0009-0000-9279-6744>),\n  Lara Selles Vidal [aut] (ORCID:\n    <https://orcid.org/0000-0003-2537-6824>)",
    "url": "",
    "bug_reports": "https://github.com/Rafael-Ayala/asteRisk/issues",
    "repository": "https://cran.r-project.org/package=asteRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asteRisk Computation of Satellite Position Provides basic functionalities to calculate the position of\n    satellites given a known state vector. The package includes implementations\n    of the SGP4 and SDP4 simplified perturbation models to propagate orbital\n    state vectors, as well as utilities to read TLE files and convert coordinates\n    between different frames of reference. Several of the functionalities of the\n    package (including the high-precision numerical orbit propagator) require\n    the coefficients and data included in the 'asteRiskData' package, available\n    in a 'drat' repository. To install this data package, run \n    'install.packages(\"asteRiskData\", repos=\"https://rafael-ayala.github.io/drat/\")'.\n    Felix R. Hoots, Ronald L. Roehrich and T.S. Kelso (1988) <https://celestrak.org/NORAD/documentation/spacetrk.pdf>.\n    David Vallado, Paul Crawford, Richard Hujsak and T.S. Kelso (2012) <doi:10.2514/6.2006-6753>.\n    Felix R. Hoots, Paul W. Schumacher Jr. and Robert A. Glover (2014) <doi:10.2514/1.9161>.  "
  },
  {
    "id": 8707,
    "package_name": "atRisk",
    "title": "At-Risk",
    "description": "The at-Risk (aR) approach is based on a two-step parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the aR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al. (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.",
    "version": "0.2.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Quentin Lajaunie [aut, cre],\n  Guillaume Flament [aut, ctb],\n  Christophe Hurlin [aut],\n  Souzan Kazemi [rev]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=atRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "atRisk At-Risk The at-Risk (aR) approach is based on a two-step parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the aR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al. (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.  "
  },
  {
    "id": 8708,
    "package_name": "ata",
    "title": "Automated Test Assembly",
    "description": "Provides a collection of psychometric methods to process item metadata\n and use target assessment and measurement blueprint constraints to assemble a test form. Currently two automatic\n test assembly (ata) approaches are enabled. For example, the weighted (positive) deviations method, wdm(), proposed\n by Swanson and Stocking (1993) <doi:10.1177/014662169301700205> was implemented in its full specification allowing\n for both item selection as well as test form refinement. The linear constraint programming approach, atalp(), uses the \n linear equation solver by Berkelaar et. al (2014) <http://lpsolve.sourceforge.net/5.5/>\n to enable a variety of approaches to select items.",
    "version": "1.1.1",
    "maintainer": "Michael Chajewski <mchajewski@hotmail.com>",
    "author": "Gulsah Gurkan [aut],\n  Michael Chajewski [aut, cre],\n  Sam Buttrey [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ata Automated Test Assembly Provides a collection of psychometric methods to process item metadata\n and use target assessment and measurement blueprint constraints to assemble a test form. Currently two automatic\n test assembly (ata) approaches are enabled. For example, the weighted (positive) deviations method, wdm(), proposed\n by Swanson and Stocking (1993) <doi:10.1177/014662169301700205> was implemented in its full specification allowing\n for both item selection as well as test form refinement. The linear constraint programming approach, atalp(), uses the \n linear equation solver by Berkelaar et. al (2014) <http://lpsolve.sourceforge.net/5.5/>\n to enable a variety of approaches to select items.  "
  },
  {
    "id": 8722,
    "package_name": "attrib",
    "title": "Attributable Burden of Disease",
    "description": "Provides functions for estimating the attributable burden of disease due to risk factors. The posterior simulation is performed using arm::sim as described in Gelman, Hill (2012) <doi:10.1017/CBO9780511790942> and the attributable burden method is based on Nielsen, Krause, Molbak <doi:10.1111/irv.12564>.",
    "version": "2021.1.2",
    "maintainer": "Richard Aubrey White <hello@rwhite.no>",
    "author": "Richard Aubrey White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6747-1726>),\n  Aurora Hofman [aut] (ORCID: <https://orcid.org/0000-0001-6543-5134>),\n  Folkehelseinstituttet [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=attrib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "attrib Attributable Burden of Disease Provides functions for estimating the attributable burden of disease due to risk factors. The posterior simulation is performed using arm::sim as described in Gelman, Hill (2012) <doi:10.1017/CBO9780511790942> and the attributable burden method is based on Nielsen, Krause, Molbak <doi:10.1111/irv.12564>.  "
  },
  {
    "id": 8727,
    "package_name": "audit",
    "title": "Bounds for Accounting Populations",
    "description": "Find an upper bound for the total amount of overstatement\n    of assets in a set of accounts, or estimate the amount of sales tax\n    owed on a collection of transactions (Meeden and Sargent, 2007,\n    <doi:10.1080/03610920701386802>).",
    "version": "0.1-2",
    "maintainer": "Glen Meeden <glen@stat.umn.edu>",
    "author": "Glen Meeden <glen@stat.umn.edu>.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=audit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "audit Bounds for Accounting Populations Find an upper bound for the total amount of overstatement\n    of assets in a set of accounts, or estimate the amount of sales tax\n    owed on a collection of transactions (Meeden and Sargent, 2007,\n    <doi:10.1080/03610920701386802>).  "
  },
  {
    "id": 8764,
    "package_name": "autoslider.core",
    "title": "Slide Automation for Tables, Listings and Figures",
    "description": "The normal process of creating clinical study slides is that\n    a statistician manually type in the numbers from outputs and a\n    separate statistician to double check the typed in numbers. This\n    process is time consuming, resource intensive, and error prone.\n    Automatic slide generation is a solution to address these issues. It\n    reduces the amount of work and the required time when creating slides,\n    and reduces the risk of errors from manually typing or copying numbers\n    from the output to slides. It also helps users to avoid unnecessary\n    stress when creating large amounts of slide decks in a short time\n    window.",
    "version": "0.3.1",
    "maintainer": "Joe Zhu <joe.zhu@roche.com>",
    "author": "Joe Zhu [cre, aut] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  Heng Wang [aut],\n  Yinqi Zhao [aut],\n  Bo Ci [aut],\n  Liming Li [aut],\n  Laura Wang [ctb],\n  Xiaoli Duan [aut],\n  Stefan Pascal Thoma [aut],\n  Thomas Neitmann [ctb],\n  Miles Almond [aut],\n  Mahdi About [ctb],\n  Kai Lim [ctb],\n  Nolan Steed [ctb],\n  Daoling Pang [ctb],\n  Elisabeth Deutschmann [ctb],\n  Chenkai Lv [aut],\n  Nina Qi [ctb],\n  Jasmina Uzunovic [aut],\n  Yolanda Zhou [aut]",
    "url": "https://github.com/insightsengineering/autoslider.core",
    "bug_reports": "https://github.com/insightsengineering/autoslider.core/issues",
    "repository": "https://cran.r-project.org/package=autoslider.core",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "autoslider.core Slide Automation for Tables, Listings and Figures The normal process of creating clinical study slides is that\n    a statistician manually type in the numbers from outputs and a\n    separate statistician to double check the typed in numbers. This\n    process is time consuming, resource intensive, and error prone.\n    Automatic slide generation is a solution to address these issues. It\n    reduces the amount of work and the required time when creating slides,\n    and reduces the risk of errors from manually typing or copying numbers\n    from the output to slides. It also helps users to avoid unnecessary\n    stress when creating large amounts of slide decks in a short time\n    window.  "
  },
  {
    "id": 8773,
    "package_name": "averisk",
    "title": "Calculation of Average Population Attributable Fractions and\nConfidence Intervals",
    "description": "Average population attributable fractions are calculated for a set\n    of risk factors (either binary or ordinal valued) for both prospective and case-\n    control designs. Confidence intervals are found by Monte Carlo simulation. The\n    method can be applied to either prospective or case control designs, provided an\n    estimate of disease prevalence is provided. In addition to an exact calculation\n    of AF, an approximate calculation, based on randomly sampling permutations has\n    been implemented to ensure the calculation is computationally tractable when the\n    number of risk factors is large.",
    "version": "1.0.3",
    "maintainer": "John Ferguson <john.ferguson@nuigalway.ie>",
    "author": "John Ferguson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=averisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "averisk Calculation of Average Population Attributable Fractions and\nConfidence Intervals Average population attributable fractions are calculated for a set\n    of risk factors (either binary or ordinal valued) for both prospective and case-\n    control designs. Confidence intervals are found by Monte Carlo simulation. The\n    method can be applied to either prospective or case control designs, provided an\n    estimate of disease prevalence is provided. In addition to an exact calculation\n    of AF, an approximate calculation, based on randomly sampling permutations has\n    been implemented to ensure the calculation is computationally tractable when the\n    number of risk factors is large.  "
  },
  {
    "id": 8819,
    "package_name": "backtest",
    "title": "Exploring Portfolio-Based Conjectures About Financial\nInstruments",
    "description": "The backtest package provides facilities for exploring\n        portfolio-based conjectures about financial instruments\n        (stocks, bonds, swaps, options, et cetera).",
    "version": "0.3-4",
    "maintainer": "Daniel Gerlanc <dgerlanc@enplusadvisors.com>",
    "author": "Jeff Enos <jeff@kanecap.com> and David Kane <dave@kanecap.com>,\n        with contributions from Kyle Campbell\n        <kyle.w.campbell@williams.edu>, Daniel Gerlanc\n        <daniel@gerlanc.com>, Aaron Schwartz\n        <Aaron.J.Schwartz@williams.edu>, Daniel Suo\n        <danielsuo@gmail.com>, Alexei Colin <acolin@fas.harvard.edu>,\n        and Luyi Zhao <luyizhao@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=backtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "backtest Exploring Portfolio-Based Conjectures About Financial\nInstruments The backtest package provides facilities for exploring\n        portfolio-based conjectures about financial instruments\n        (stocks, bonds, swaps, options, et cetera).  "
  },
  {
    "id": 8820,
    "package_name": "backtestGraphics",
    "title": "Interactive Graphics for Portfolio Data",
    "description": "Creates an interactive graphics \n             interface to visualize backtest results of different financial \n             instruments, such as equities, futures, and credit default swaps.\n             The package does not run backtests on the given data set but \n             displays a graphical explanation of the backtest results. Users can\n             look at backtest graphics for different instruments, investment \n             strategies, and portfolios. Summary statistics of different \n             portfolio holdings are shown in the left panel, and interactive \n             plots of profit and loss (P&L), net market value (NMV) and \n             gross market value (GMV) are displayed in the right panel. ",
    "version": "0.1.8",
    "maintainer": "Yanrong Song <yrsong129@gmail.com>",
    "author": "Yanrong Song [aut, cre],\n  Zijie Zhu [aut],\n  David Kane [aut],\n  Ziqi Lu [aut],\n  Karan Tibrewal [aut],\n  Fan Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=backtestGraphics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "backtestGraphics Interactive Graphics for Portfolio Data Creates an interactive graphics \n             interface to visualize backtest results of different financial \n             instruments, such as equities, futures, and credit default swaps.\n             The package does not run backtests on the given data set but \n             displays a graphical explanation of the backtest results. Users can\n             look at backtest graphics for different instruments, investment \n             strategies, and portfolios. Summary statistics of different \n             portfolio holdings are shown in the left panel, and interactive \n             plots of profit and loss (P&L), net market value (NMV) and \n             gross market value (GMV) are displayed in the right panel.   "
  },
  {
    "id": 8833,
    "package_name": "bahc",
    "title": "Filter Covariance and Correlation Matrices with\nBootstrapped-Averaged Hierarchical Ansatz",
    "description": "A method to filter correlation and covariance matrices by averaging\n     bootstrapped filtered hierarchical clustering and boosting. See Ch. Bongiorno and D. Challet,\n     Covariance matrix filtering with bootstrapped hierarchies (2020) <arXiv:2003.05807> and\n     Ch. Bongiorno and D. Challet, Reactive Global Minimum Variance Portfolios with k-BAHC covariance cleaning\n     (2020) <arXiv:2005.08703>.",
    "version": "0.3.0",
    "maintainer": "Damien Challet <damien.challet@gmail.com>",
    "author": "Christian Bongiorno and Damien Challet",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bahc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bahc Filter Covariance and Correlation Matrices with\nBootstrapped-Averaged Hierarchical Ansatz A method to filter correlation and covariance matrices by averaging\n     bootstrapped filtered hierarchical clustering and boosting. See Ch. Bongiorno and D. Challet,\n     Covariance matrix filtering with bootstrapped hierarchies (2020) <arXiv:2003.05807> and\n     Ch. Bongiorno and D. Challet, Reactive Global Minimum Variance Portfolios with k-BAHC covariance cleaning\n     (2020) <arXiv:2005.08703>.  "
  },
  {
    "id": 8966,
    "package_name": "bbdetection",
    "title": "Identification of Bull and Bear States of the Market",
    "description": "Implements two algorithms of detecting Bull and Bear markets in stock prices: the algorithm of Pagan and Sossounov (2002, <doi:10.1002/jae.664>) and the algorithm of Lunde and Timmermann (2004, <doi:10.1198/073500104000000136>).  \n       The package also contains functions for printing out the dating of the Bull and Bear states of the market, the descriptive statistics of the states, and functions for plotting the results. \n       For the sake of convenience, the package includes the monthly and daily data on the prices (not adjusted for dividends) of the S&P 500 stock market index.",
    "version": "1.0",
    "maintainer": "Valeriy Zakamulin <valeriz@uia.no>",
    "author": "Valeriy Zakamulin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bbdetection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bbdetection Identification of Bull and Bear States of the Market Implements two algorithms of detecting Bull and Bear markets in stock prices: the algorithm of Pagan and Sossounov (2002, <doi:10.1002/jae.664>) and the algorithm of Lunde and Timmermann (2004, <doi:10.1198/073500104000000136>).  \n       The package also contains functions for printing out the dating of the Bull and Bear states of the market, the descriptive statistics of the states, and functions for plotting the results. \n       For the sake of convenience, the package includes the monthly and daily data on the prices (not adjusted for dividends) of the S&P 500 stock market index.  "
  },
  {
    "id": 8995,
    "package_name": "bddkR",
    "title": "Gathering Monthly Banking Sector Data from BDDK of Turkey",
    "description": "Fetches monthly financial tables and banking sector data published on the official website of the Banking Regulation and Supervision Agency of Turkey and also enables you to save it as an Excel file. It is a R implementation of the Python package <https://pypi.org/project/bddkdata/>.    ",
    "version": "0.1.1",
    "maintainer": "Ozancan Ozdemir <ozancanozdemir@gmail.com>",
    "author": "Ozancan Ozdemir [aut, cre]",
    "url": "https://github.com/ozancanozdemir/bddkR",
    "bug_reports": "https://github.com/ozancanozdemir/bddkR/issues",
    "repository": "https://cran.r-project.org/package=bddkR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bddkR Gathering Monthly Banking Sector Data from BDDK of Turkey Fetches monthly financial tables and banking sector data published on the official website of the Banking Regulation and Supervision Agency of Turkey and also enables you to save it as an Excel file. It is a R implementation of the Python package <https://pypi.org/project/bddkdata/>.      "
  },
  {
    "id": 9005,
    "package_name": "bdribs",
    "title": "Bayesian Detection of Potential Risk Using Inference on Blinded\nSafety Data",
    "description": "Implements Bayesian inference to detect signal from blinded \n    clinical trial when total number of adverse events of special \n    concerns and total risk exposures from all patients are available in the study.  \n    For more details see the article by Mukhopadhyay et. al. (2018) \n    titled 'Bayesian Detection of Potential Risk Using Inference on Blinded Safety Data', \n    in Pharmaceutical Statistics (to appear). ",
    "version": "1.0.4",
    "maintainer": "Saurabh Mukhopadhyay <saurabh.mukhopadhyay@abbvie.com>",
    "author": "Saurabh Mukhopadhyay [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bdribs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bdribs Bayesian Detection of Potential Risk Using Inference on Blinded\nSafety Data Implements Bayesian inference to detect signal from blinded \n    clinical trial when total number of adverse events of special \n    concerns and total risk exposures from all patients are available in the study.  \n    For more details see the article by Mukhopadhyay et. al. (2018) \n    titled 'Bayesian Detection of Potential Risk Using Inference on Blinded Safety Data', \n    in Pharmaceutical Statistics (to appear).   "
  },
  {
    "id": 9011,
    "package_name": "beadplexr",
    "title": "Analysis of Multiplex Cytometric Bead Assays",
    "description": "Reproducible and automated analysis of multiplex bead assays such\n    as CBA (Morgan et al. 2004; <doi: 10.1016/j.clim.2003.11.017>), LEGENDplex\n    (Yu et al. 2015; <doi: 10.1084/jem.20142318>), and MACSPlex (Miltenyi\n    Biotec 2014; Application note: Data acquisition and analysis without the\n    MACSQuant analyzer;\n    <https://www.miltenyibiotec.com/upload/assets/IM0021608.PDF>). The\n    package provides functions for streamlined reading of fcs files, and\n    identification of bead clusters and analyte expression. The package eases\n    the calculation of standard curves and the subsequent calculation of the\n    analyte concentration.",
    "version": "0.5.0",
    "maintainer": "Ulrik Stervbo <ulrik.stervbo@gmail.com>",
    "author": "Ulrik Stervbo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2831-8868>)",
    "url": "https://gitlab.com/ustervbo/beadplexr",
    "bug_reports": "https://gitlab.com/ustervbo/beadplexr/-/issues",
    "repository": "https://cran.r-project.org/package=beadplexr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "beadplexr Analysis of Multiplex Cytometric Bead Assays Reproducible and automated analysis of multiplex bead assays such\n    as CBA (Morgan et al. 2004; <doi: 10.1016/j.clim.2003.11.017>), LEGENDplex\n    (Yu et al. 2015; <doi: 10.1084/jem.20142318>), and MACSPlex (Miltenyi\n    Biotec 2014; Application note: Data acquisition and analysis without the\n    MACSQuant analyzer;\n    <https://www.miltenyibiotec.com/upload/assets/IM0021608.PDF>). The\n    package provides functions for streamlined reading of fcs files, and\n    identification of bead clusters and analyte expression. The package eases\n    the calculation of standard curves and the subsequent calculation of the\n    analyte concentration.  "
  },
  {
    "id": 9016,
    "package_name": "bearishTrader",
    "title": "Trading Strategies for Bearish Outlook",
    "description": "Stock, Options and Futures Trading Strategies for Traders and Investors with Bearish Outlook. The indicators, strategies, calculations, functions and all other discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Juan A. Serur, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    Chartered Financial Analyst Institute (\"Chartered Financial Analyst Program Curriculum 2020 Level I Volumes 1-6. (Vol. 5, pp. 385-453)\", 2019, ISBN: 9781119593577).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).",
    "version": "1.0.2",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bearishTrader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bearishTrader Trading Strategies for Bearish Outlook Stock, Options and Futures Trading Strategies for Traders and Investors with Bearish Outlook. The indicators, strategies, calculations, functions and all other discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Juan A. Serur, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    Chartered Financial Analyst Institute (\"Chartered Financial Analyst Program Curriculum 2020 Level I Volumes 1-6. (Vol. 5, pp. 385-453)\", 2019, ISBN: 9781119593577).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).  "
  },
  {
    "id": 9029,
    "package_name": "belex",
    "title": "Download Historical Data from the Belgrade Stock Exchange",
    "description": "Tools for downloading historical financial data from the www.belex.rs.",
    "version": "0.1.0",
    "maintainer": "Milos Vilotic <milos.vilotic@gmail.com>",
    "author": "Milos Vilotic [aut, cre],\n  Zlatko Kovacic [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=belex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "belex Download Historical Data from the Belgrade Stock Exchange Tools for downloading historical financial data from the www.belex.rs.  "
  },
  {
    "id": 9087,
    "package_name": "bhpm",
    "title": "Bayesian Hierarchical Poisson Models for Multiple Grouped\nOutcomes with Clustering",
    "description": "Bayesian hierarchical methods for the detection of differences in rates of related outcomes for multiple treatments for clustered observations (Carragher et al. (2020) <doi:10.1002/sim.8563>). This software was developed for the Precision Drug Theraputics: Risk Prediction in Pharmacoepidemiology project as part of a Rutherford Fund Fellowship at Health Data Research (UK), Medical Research Council (UK) award reference MR/S003967/1 (<https://gtr.ukri.org/>). Principal Investigator: Raymond Carragher.",
    "version": "1.8.1",
    "maintainer": "Raymond Carragher <rcarragh@gmail.com>",
    "author": "Raymond Carragher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0120-625X>)",
    "url": "https://github.com/rcarragh/bhpm",
    "bug_reports": "https://github.com/rcarragh/bhpm/issues",
    "repository": "https://cran.r-project.org/package=bhpm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bhpm Bayesian Hierarchical Poisson Models for Multiple Grouped\nOutcomes with Clustering Bayesian hierarchical methods for the detection of differences in rates of related outcomes for multiple treatments for clustered observations (Carragher et al. (2020) <doi:10.1002/sim.8563>). This software was developed for the Precision Drug Theraputics: Risk Prediction in Pharmacoepidemiology project as part of a Rutherford Fund Fellowship at Health Data Research (UK), Medical Research Council (UK) award reference MR/S003967/1 (<https://gtr.ukri.org/>). Principal Investigator: Raymond Carragher.  "
  },
  {
    "id": 9136,
    "package_name": "bigtcr",
    "title": "Nonparametric Analysis of Bivariate Gap Time with Competing\nRisks",
    "description": "For studying recurrent disease and death with competing\n    risks, comparisons based on the well-known cumulative incidence function\n    can be confounded by different prevalence rates of the competing events.\n    Alternatively, comparisons of the conditional distribution of the survival\n    time given the failure event type are more relevant for investigating the\n    prognosis of different patterns of recurrence disease. This package implements\n    a nonparametric estimator for the conditional cumulative incidence function\n    and a nonparametric conditional bivariate cumulative incidence function for the\n    bivariate gap times proposed in Huang et al. (2016) <doi:10.1111/biom.12494>.",
    "version": "1.1",
    "maintainer": "Chenguang Wang <cwang68@jhmi.edu>",
    "author": "Chenguang Wang [aut, cre],\n    Chiung-Yu Huang [aut],\n    Mei-Cheng Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bigtcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigtcr Nonparametric Analysis of Bivariate Gap Time with Competing\nRisks For studying recurrent disease and death with competing\n    risks, comparisons based on the well-known cumulative incidence function\n    can be confounded by different prevalence rates of the competing events.\n    Alternatively, comparisons of the conditional distribution of the survival\n    time given the failure event type are more relevant for investigating the\n    prognosis of different patterns of recurrence disease. This package implements\n    a nonparametric estimator for the conditional cumulative incidence function\n    and a nonparametric conditional bivariate cumulative incidence function for the\n    bivariate gap times proposed in Huang et al. (2016) <doi:10.1111/biom.12494>.  "
  },
  {
    "id": 9148,
    "package_name": "binancer",
    "title": "API Client to 'Binance'",
    "description": "R client to the 'Binance' Public Rest API for data collection on cryptocurrencies, portfolio management and trading: <https://github.com/binance/binance-spot-api-docs/blob/master/rest-api.md>.",
    "version": "1.2.0",
    "maintainer": "Gergely Dar\u00f3czi <daroczig@rapporter.net>",
    "author": "Gergely Dar\u00f3czi [aut, cre],\n  David Andel [aut]",
    "url": "https://daroczig.github.io/binancer/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=binancer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binancer API Client to 'Binance' R client to the 'Binance' Public Rest API for data collection on cryptocurrencies, portfolio management and trading: <https://github.com/binance/binance-spot-api-docs/blob/master/rest-api.md>.  "
  },
  {
    "id": 9181,
    "package_name": "biobricks",
    "title": "Access Data Dependencies Installed Through 'Biobricks.ai'",
    "description": "Provides an integrated data management solution for assets installed via the 'Biobricks.ai' platform. Streamlines the process of loading and interacting with diverse datasets in a consistent manner. A list of bricks is available at <https://status.biobricks.ai>. Documentation for 'Biobricks.ai' is available at <https://docs.biobricks.ai>.",
    "version": "0.2.2",
    "maintainer": "Thomas Luechtefeld <tom@insilica.co>",
    "author": "Thomas Luechtefeld [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biobricks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biobricks Access Data Dependencies Installed Through 'Biobricks.ai' Provides an integrated data management solution for assets installed via the 'Biobricks.ai' platform. Streamlines the process of loading and interacting with diverse datasets in a consistent manner. A list of bricks is available at <https://status.biobricks.ai>. Documentation for 'Biobricks.ai' is available at <https://docs.biobricks.ai>.  "
  },
  {
    "id": 9255,
    "package_name": "blm",
    "title": "Binomial Linear Regression",
    "description": "Implements regression models for binary data on the absolute risk scale. These models are applicable to cohort and population-based case-control data.",
    "version": "2022.0.0.1",
    "maintainer": "S.Kovalchik <s.a.kovalchik@gmail.com>",
    "author": "S. Kovalchik",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=blm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blm Binomial Linear Regression Implements regression models for binary data on the absolute risk scale. These models are applicable to cohort and population-based case-control data.  "
  },
  {
    "id": 9299,
    "package_name": "bnma",
    "title": "Bayesian Network Meta-Analysis using 'JAGS'",
    "description": "Network meta-analyses using Bayesian framework following Dias et al. (2013) <DOI:10.1177/0272989X12458724>. Based on the data input, creates prior, model file, and initial values needed to run models in 'rjags'. Able to handle binomial, normal and multinomial arm-level data. Can handle multi-arm trials and includes methods to incorporate covariate and baseline risk effects. Includes standard diagnostics and visualization tools to evaluate the results.",
    "version": "1.6.1",
    "maintainer": "Michael Seo <swj8874@gmail.com>",
    "author": "Michael Seo [aut, cre],\n  Christopher Schmid [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bnma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bnma Bayesian Network Meta-Analysis using 'JAGS' Network meta-analyses using Bayesian framework following Dias et al. (2013) <DOI:10.1177/0272989X12458724>. Based on the data input, creates prior, model file, and initial values needed to run models in 'rjags'. Able to handle binomial, normal and multinomial arm-level data. Can handle multi-arm trials and includes methods to incorporate covariate and baseline risk effects. Includes standard diagnostics and visualization tools to evaluate the results.  "
  },
  {
    "id": 9302,
    "package_name": "bnns",
    "title": "Bayesian Neural Network with 'Stan'",
    "description": "Offers a flexible formula-based interface for building and training Bayesian Neural Networks powered by 'Stan'. The package supports modeling complex relationships while providing rigorous uncertainty quantification via posterior distributions. With features like user chosen priors, clear predictions, and support for regression, binary, and multi-class classification, it is well-suited for applications in clinical trials, finance, and other fields requiring robust Bayesian inference and decision-making. References: Neal(1996) <doi:10.1007/978-1-4612-0745-0>.",
    "version": "0.1.2",
    "maintainer": "Swarnendu Chatterjee <swarnendu.stat@gmail.com>",
    "author": "Swarnendu Chatterjee [aut, cre, cph]",
    "url": "https://github.com/swarnendu-stat/bnns,\nhttps://swarnendu-stat.github.io/bnns/",
    "bug_reports": "https://github.com/swarnendu-stat/bnns/issues",
    "repository": "https://cran.r-project.org/package=bnns",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bnns Bayesian Neural Network with 'Stan' Offers a flexible formula-based interface for building and training Bayesian Neural Networks powered by 'Stan'. The package supports modeling complex relationships while providing rigorous uncertainty quantification via posterior distributions. With features like user chosen priors, clear predictions, and support for regression, binary, and multi-class classification, it is well-suited for applications in clinical trials, finance, and other fields requiring robust Bayesian inference and decision-making. References: Neal(1996) <doi:10.1007/978-1-4612-0745-0>.  "
  },
  {
    "id": 9320,
    "package_name": "bondAnalyst",
    "title": "Methods for Fixed-Income Valuation, Risk and Return",
    "description": "Bond Pricing and Fixed-Income Valuation of Selected Securities included here serve as a quick reference of Quantitative Methods for undergraduate courses on Fixed-Income and CFA Level I Readings on Fixed-Income Valuation, Risk and Return.\n    CFA Institute (\"CFA Program Curriculum 2020 Level I Volumes 1-6. (Vol. 5, pp. 107-151, pp. 237-299)\", 2019, ISBN: 9781119593577).\n    Barbara S. Petitt (\"Fixed Income Analysis\", 2019, ISBN: 9781119628132).\n    Frank J. Fabozzi (\"Handbook of Finance: Financial Markets and Instruments\", 2008, ISBN: 9780470078143).\n    Frank J. Fabozzi (\"Fixed Income Analysis\", 2007, ISBN: 9780470052211).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre],\n  MaheshP Kumar [aut],\n  MaheshP Kumar [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bondAnalyst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bondAnalyst Methods for Fixed-Income Valuation, Risk and Return Bond Pricing and Fixed-Income Valuation of Selected Securities included here serve as a quick reference of Quantitative Methods for undergraduate courses on Fixed-Income and CFA Level I Readings on Fixed-Income Valuation, Risk and Return.\n    CFA Institute (\"CFA Program Curriculum 2020 Level I Volumes 1-6. (Vol. 5, pp. 107-151, pp. 237-299)\", 2019, ISBN: 9781119593577).\n    Barbara S. Petitt (\"Fixed Income Analysis\", 2019, ISBN: 9781119628132).\n    Frank J. Fabozzi (\"Handbook of Finance: Financial Markets and Instruments\", 2008, ISBN: 9780470078143).\n    Frank J. Fabozzi (\"Fixed Income Analysis\", 2007, ISBN: 9780470052211).  "
  },
  {
    "id": 9334,
    "package_name": "bootComb",
    "title": "Combine Parameter Estimates via Parametric Bootstrap",
    "description": "Propagate uncertainty from several estimates when combining these estimates via a function.\n    This is done by using the parametric bootstrap to simulate values from the distribution of each estimate to build up an empirical distribution of the combined parameter.\n    Finally either the percentile method is used or the highest density interval is chosen to derive a confidence interval for the combined parameter with the desired coverage.\n    Gaussian copulas are used for when parameters are assumed to be dependent / correlated.\n    References: Davison and Hinkley (1997,ISBN:0-521-57471-4) for the parametric bootstrap and percentile method, Gelman et al. (2014,ISBN:978-1-4398-4095-5) for the highest density interval, Stockdale et al. (2020)<doi:10.1016/j.jhep.2020.04.008> for an example of combining conditional prevalences.",
    "version": "1.1.2",
    "maintainer": "Marc Henrion <mhenrion@mlw.mw>",
    "author": "Marc Henrion [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1242-839X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bootComb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootComb Combine Parameter Estimates via Parametric Bootstrap Propagate uncertainty from several estimates when combining these estimates via a function.\n    This is done by using the parametric bootstrap to simulate values from the distribution of each estimate to build up an empirical distribution of the combined parameter.\n    Finally either the percentile method is used or the highest density interval is chosen to derive a confidence interval for the combined parameter with the desired coverage.\n    Gaussian copulas are used for when parameters are assumed to be dependent / correlated.\n    References: Davison and Hinkley (1997,ISBN:0-521-57471-4) for the parametric bootstrap and percentile method, Gelman et al. (2014,ISBN:978-1-4398-4095-5) for the highest density interval, Stockdale et al. (2020)<doi:10.1016/j.jhep.2020.04.008> for an example of combining conditional prevalences.  "
  },
  {
    "id": 9347,
    "package_name": "bootruin",
    "title": "A Bootstrap Test for the Probability of Ruin in the Classical\nRisk Process",
    "description": "We provide a framework for testing the probability of ruin in the classical (compound Poisson) risk process. It also includes some procedures for assessing and comparing the performance between the bootstrap test and the test using asymptotic normality.",
    "version": "1.2-4",
    "maintainer": "Benjamin Baumgartner <benjamin@baumgrt.com>",
    "author": "Benjamin Baumgartner <benjamin@baumgrt.com>, Riccardo Gatto <gatto@stat.unibe.ch>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bootruin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootruin A Bootstrap Test for the Probability of Ruin in the Classical\nRisk Process We provide a framework for testing the probability of ruin in the classical (compound Poisson) risk process. It also includes some procedures for assessing and comparing the performance between the bootstrap test and the test using asymptotic normality.  "
  },
  {
    "id": 9385,
    "package_name": "bqror",
    "title": "Bayesian Quantile Regression for Ordinal Models",
    "description": "Package provides functions for estimation and inference in Bayesian quantile regression with ordinal outcomes. An ordinal model with 3 or more outcomes (labeled OR1 model) is estimated by a combination of Gibbs sampling and Metropolis-Hastings (MH) algorithm. Whereas an ordinal model with exactly 3 outcomes (labeled OR2 model) is estimated using a Gibbs sampling algorithm. The summary output presents the posterior mean, posterior standard deviation, 95% credible intervals, and the inefficiency factors along with the two model comparison measures \u2013 logarithm of marginal likelihood and the deviance information criterion (DIC). The package also provides functions for computing the covariate effects and other functions that aids either the estimation or inference in quantile ordinal models.\n    Rahman, M. A. (2016).\u201cBayesian Quantile Regression for Ordinal Models.\u201d Bayesian Analysis, 11(1): 1-24 <doi: 10.1214/15-BA939>.\n    Yu, K., and Moyeed, R. A. (2001). \u201cBayesian Quantile Regression.\u201d Statistics and Probability Letters, 54(4): 437\u2013447 <doi: 10.1016/S0167-7152(01)00124-9>.\n    Koenker, R., and Bassett, G. (1978).\u201cRegression Quantiles.\u201d Econometrica, 46(1): 33-50 <doi: 10.2307/1913643>.\n    Chib, S. (1995). \u201cMarginal likelihood from the Gibbs output.\u201d Journal of the American Statistical Association, 90(432):1313\u20131321, 1995. <doi: 10.1080/01621459.1995.10476635>.\n    Chib, S., and Jeliazkov, I. (2001). \u201cMarginal likelihood from the Metropolis-Hastings output.\u201d Journal of the American Statistical Association, 96(453):270\u2013281, 2001. <doi: 10.1198/016214501750332848>.",
    "version": "1.7.1",
    "maintainer": "Prajual Maheshwari <prajual1391@gmail.com>",
    "author": "Mohammad Arshad Rahman Developer [aut],\n  Prajual Maheshwari [cre]",
    "url": "https://github.com/prajual/bqror",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bqror",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bqror Bayesian Quantile Regression for Ordinal Models Package provides functions for estimation and inference in Bayesian quantile regression with ordinal outcomes. An ordinal model with 3 or more outcomes (labeled OR1 model) is estimated by a combination of Gibbs sampling and Metropolis-Hastings (MH) algorithm. Whereas an ordinal model with exactly 3 outcomes (labeled OR2 model) is estimated using a Gibbs sampling algorithm. The summary output presents the posterior mean, posterior standard deviation, 95% credible intervals, and the inefficiency factors along with the two model comparison measures \u2013 logarithm of marginal likelihood and the deviance information criterion (DIC). The package also provides functions for computing the covariate effects and other functions that aids either the estimation or inference in quantile ordinal models.\n    Rahman, M. A. (2016).\u201cBayesian Quantile Regression for Ordinal Models.\u201d Bayesian Analysis, 11(1): 1-24 <doi: 10.1214/15-BA939>.\n    Yu, K., and Moyeed, R. A. (2001). \u201cBayesian Quantile Regression.\u201d Statistics and Probability Letters, 54(4): 437\u2013447 <doi: 10.1016/S0167-7152(01)00124-9>.\n    Koenker, R., and Bassett, G. (1978).\u201cRegression Quantiles.\u201d Econometrica, 46(1): 33-50 <doi: 10.2307/1913643>.\n    Chib, S. (1995). \u201cMarginal likelihood from the Gibbs output.\u201d Journal of the American Statistical Association, 90(432):1313\u20131321, 1995. <doi: 10.1080/01621459.1995.10476635>.\n    Chib, S., and Jeliazkov, I. (2001). \u201cMarginal likelihood from the Metropolis-Hastings output.\u201d Journal of the American Statistical Association, 96(453):270\u2013281, 2001. <doi: 10.1198/016214501750332848>.  "
  },
  {
    "id": 9396,
    "package_name": "branchingprocess",
    "title": "Calculate Outbreak Probabilities for a Branching Process Model",
    "description": "Quantify outbreak risk posed by individual importers of a transmissible\n    pathogen. Input parameters of negative binomial offspring distributions for the\n    number of transmissions from each infected individual and initial number of\n    infected. Calculate probabilities of final outbreak size and generations of\n    transmission, as described in Toth et al. (2015) <doi:10.3201/eid2108.150170> \n    and Toth et al. (2016) <doi:10.1016/j.epidem.2016.04.002>.",
    "version": "0.1.0",
    "maintainer": "Damon Toth <damon.toth@hsc.utah.edu>",
    "author": "Damon Toth [aut, cre] (ORCID: <https://orcid.org/0000-0001-7393-4814>),\n  Erin Clancey [ctb] (ORCID: <https://orcid.org/0000-0003-4728-4023>),\n  Centers for Disease Control and Prevention's Center for Forecasting and\n    Outbreak Analytics [fnd] (Cooperative agreement CDC-RFA-FT-23-0069)",
    "url": "https://github.com/EpiForeSITE/branchingprocess,\nhttps://epiforesite.github.io/branchingprocess/",
    "bug_reports": "https://github.com/EpiForeSITE/branchingprocess/issues",
    "repository": "https://cran.r-project.org/package=branchingprocess",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "branchingprocess Calculate Outbreak Probabilities for a Branching Process Model Quantify outbreak risk posed by individual importers of a transmissible\n    pathogen. Input parameters of negative binomial offspring distributions for the\n    number of transmissions from each infected individual and initial number of\n    infected. Calculate probabilities of final outbreak size and generations of\n    transmission, as described in Toth et al. (2015) <doi:10.3201/eid2108.150170> \n    and Toth et al. (2016) <doi:10.1016/j.epidem.2016.04.002>.  "
  },
  {
    "id": 9405,
    "package_name": "brea",
    "title": "Bayesian Recurrent Events Analysis",
    "description": "Functions to produce MCMC samples for posterior inference in semiparametric Bayesian discrete time competing risks recurrent events models and multistate models.",
    "version": "0.4.2",
    "maintainer": "Adam J King <king@cpp.edu>",
    "author": "Adam J King [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=brea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brea Bayesian Recurrent Events Analysis Functions to produce MCMC samples for posterior inference in semiparametric Bayesian discrete time competing risks recurrent events models and multistate models.  "
  },
  {
    "id": 9413,
    "package_name": "brfinance",
    "title": "Simplified Access to Brazilian Financial and Macroeconomic Data",
    "description": "It offers simplified access to Brazilian macroeconomic and financial indicators selected from official sources, such as the 'IBGE' (Brazilian Institute of Geography and Statistics) via the 'SIDRA' API and the 'Central Bank of Brazil' via the 'SGS' API. It allows users to quickly retrieve and visualize data series such as the unemployment rate and the Selic interest rate. This package was developed for data access and visualization purposes, without generating forecasts or statistical results. For more information, see the official APIs: <https://sidra.ibge.gov.br/> and <https://dadosabertos.bcb.gov.br/dataset/>.",
    "version": "0.2.2",
    "maintainer": "Jo\u00e3o Paulo dos Santos Pereira Barbosa <joao.31582129@gmail.com>",
    "author": "Jo\u00e3o Paulo dos Santos Pereira Barbosa [aut, cre]",
    "url": "https://github.com/efram2/brfinance",
    "bug_reports": "https://github.com/efram2/brfinance/issues",
    "repository": "https://cran.r-project.org/package=brfinance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brfinance Simplified Access to Brazilian Financial and Macroeconomic Data It offers simplified access to Brazilian macroeconomic and financial indicators selected from official sources, such as the 'IBGE' (Brazilian Institute of Geography and Statistics) via the 'SIDRA' API and the 'Central Bank of Brazil' via the 'SGS' API. It allows users to quickly retrieve and visualize data series such as the unemployment rate and the Selic interest rate. This package was developed for data access and visualization purposes, without generating forecasts or statistical results. For more information, see the official APIs: <https://sidra.ibge.gov.br/> and <https://dadosabertos.bcb.gov.br/dataset/>.  "
  },
  {
    "id": 9425,
    "package_name": "brisk",
    "title": "Bayesian Benefit Risk Analysis",
    "description": "Quantitative methods for benefit-risk analysis help to condense\n    complex decisions into a univariate metric describing the overall benefit\n    relative to risk.  One approach is to use the multi-criteria decision\n    analysis framework (MCDA), as in Mussen, Salek, and Walker\n    (2007) <doi:10.1002/pds.1435>.  Bayesian benefit-risk\n    analysis incorporates uncertainty through posterior distributions which are\n    inputs to the benefit-risk framework.  The brisk package provides functions\n    to assist with Bayesian benefit-risk analyses, such as MCDA.\n    Users input posterior samples, utility functions, weights, and the package\n    outputs quantitative benefit-risk scores.  The posterior of the benefit-risk\n    scores for each group can be compared.  Some plotting capabilities are also\n    included.",
    "version": "0.1.0",
    "maintainer": "Richard Payne <paynestatistics@gmail.com>",
    "author": "Richard Payne [aut, cre],\n  Sai Dharmarajan [rev],\n  Eli Lilly and Company [cph]",
    "url": "https://rich-payne.github.io/brisk/",
    "bug_reports": "https://github.com/rich-payne/brisk/issues",
    "repository": "https://cran.r-project.org/package=brisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brisk Bayesian Benefit Risk Analysis Quantitative methods for benefit-risk analysis help to condense\n    complex decisions into a univariate metric describing the overall benefit\n    relative to risk.  One approach is to use the multi-criteria decision\n    analysis framework (MCDA), as in Mussen, Salek, and Walker\n    (2007) <doi:10.1002/pds.1435>.  Bayesian benefit-risk\n    analysis incorporates uncertainty through posterior distributions which are\n    inputs to the benefit-risk framework.  The brisk package provides functions\n    to assist with Bayesian benefit-risk analyses, such as MCDA.\n    Users input posterior samples, utility functions, weights, and the package\n    outputs quantitative benefit-risk scores.  The posterior of the benefit-risk\n    scores for each group can be compared.  Some plotting capabilities are also\n    included.  "
  },
  {
    "id": 9427,
    "package_name": "brm",
    "title": "Binary Regression Model",
    "description": "Fits novel models for the conditional relative risk, risk difference and odds ratio <doi:10.1080/01621459.2016.1192546>.",
    "version": "1.1.1",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "author": "Linbo Wang, Mark Clements, Thomas Richardson",
    "url": "http://github.com/mclements/brm",
    "bug_reports": "http://github.com/mclements/brm/issues",
    "repository": "https://cran.r-project.org/package=brm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brm Binary Regression Model Fits novel models for the conditional relative risk, risk difference and odds ratio <doi:10.1080/01621459.2016.1192546>.  "
  },
  {
    "id": 9484,
    "package_name": "bullishTrader",
    "title": "Bullish Trading Strategies Through Graphs",
    "description": "Stock, Options and Futures Trading Strategies for Traders and Investors with Bullish Outlook are represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bullishTrader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bullishTrader Bullish Trading Strategies Through Graphs Stock, Options and Futures Trading Strategies for Traders and Investors with Bullish Outlook are represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).  "
  },
  {
    "id": 9500,
    "package_name": "busdater",
    "title": "Standard Date Calculations for Business",
    "description": "Get a current financial year, start of current\n    month, End of current month, start of financial year and end of it.\n    Allow for offset from the date.",
    "version": "0.2.0",
    "maintainer": "Mick Mioduszewski <mick@mioduszewski.net>",
    "author": "Mick Mioduszewski [aut, cre]",
    "url": "https://mickmioduszewski.github.io/busdater/,\nhttps://github.com/mickmioduszewski/busdater/",
    "bug_reports": "https://github.com/mickmioduszewski/busdater/issues",
    "repository": "https://cran.r-project.org/package=busdater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "busdater Standard Date Calculations for Business Get a current financial year, start of current\n    month, End of current month, start of financial year and end of it.\n    Allow for offset from the date.  "
  },
  {
    "id": 9502,
    "package_name": "businessPlanR",
    "title": "Simple Modelling Tools for Business Plans",
    "description": "A collection of S4 classes, methods and functions to create\n        and visualize business plans. Different types of cash flows can be\n        defined, which can then be used and tabulated to create profit and\n        loss statements, cash flow plans, investment and depreciation\n        schedules, loan amortization schedules, etc. The methods are\n        designed to produce handsome tables in both PDF and HTML using\n        'RMarkdown' or 'Shiny'.",
    "version": "0.1-0",
    "maintainer": "Meik Michalke <meik.michalke@c3s.cc>",
    "author": "Meik Michalke [aut, cre]",
    "url": "https://www.c3s.cc",
    "bug_reports": "https://github.com/C3S/businessPlanR/issues",
    "repository": "https://cran.r-project.org/package=businessPlanR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "businessPlanR Simple Modelling Tools for Business Plans A collection of S4 classes, methods and functions to create\n        and visualize business plans. Different types of cash flows can be\n        defined, which can then be used and tabulated to create profit and\n        loss statements, cash flow plans, investment and depreciation\n        schedules, loan amortization schedules, etc. The methods are\n        designed to produce handsome tables in both PDF and HTML using\n        'RMarkdown' or 'Shiny'.  "
  },
  {
    "id": 9503,
    "package_name": "butterflyOptions",
    "title": "Trading Butterfly Options Strategies",
    "description": "Trading of Butterfly Options Strategies is represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=butterflyOptions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "butterflyOptions Trading Butterfly Options Strategies Trading of Butterfly Options Strategies is represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).  "
  },
  {
    "id": 9551,
    "package_name": "calibmsm",
    "title": "Calibration Plots for the Transition Probabilities from\nMultistate Models",
    "description": "Assess the calibration of an existing (i.e. previously developed) multistate\n  model through calibration plots. \n  Calibration is assessed using one of three methods. 1) Calibration methods for \n  binary logistic regression models applied at a fixed time point in conjunction \n  with inverse probability of censoring weights. 2) Calibration methods for \n  multinomial logistic regression models applied at a fixed time point in conjunction \n  with inverse probability of censoring weights. 3) Pseudo-values estimated using \n  the Aalen-Johansen estimator of observed risk. All methods are applied in conjunction\n  with landmarking when required. These calibration plots evaluate the calibration \n  (in a validation cohort of interest) of the transition probabilities estimated from an \n  existing multistate model. While package development has focused on multistate \n  models, calibration plots can be produced for any model which utilises information \n  post baseline to update predictions (e.g. dynamic models); competing risks models; \n  or standard single outcome survival models, where predictions can be made at \n  any landmark time. Please see Pate et al. (2024) <doi:10.1002/sim.10094>\n  and Pate et al. (2024) <https://alexpate30.github.io/calibmsm/articles/Overview.html>.",
    "version": "1.1.3",
    "maintainer": "Alexander Pate <alexander.pate@manchester.ac.uk>",
    "author": "Alexander Pate [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0849-3458>),\n  Glen P Martin [fnd, rev] (ORCID:\n    <https://orcid.org/0000-0002-3410-9472>)",
    "url": "https://github.com/alexpate30/calibmsm,\nhttps://alexpate30.github.io/calibmsm/",
    "bug_reports": "https://github.com/alexpate30/calibmsm/issues",
    "repository": "https://cran.r-project.org/package=calibmsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "calibmsm Calibration Plots for the Transition Probabilities from\nMultistate Models Assess the calibration of an existing (i.e. previously developed) multistate\n  model through calibration plots. \n  Calibration is assessed using one of three methods. 1) Calibration methods for \n  binary logistic regression models applied at a fixed time point in conjunction \n  with inverse probability of censoring weights. 2) Calibration methods for \n  multinomial logistic regression models applied at a fixed time point in conjunction \n  with inverse probability of censoring weights. 3) Pseudo-values estimated using \n  the Aalen-Johansen estimator of observed risk. All methods are applied in conjunction\n  with landmarking when required. These calibration plots evaluate the calibration \n  (in a validation cohort of interest) of the transition probabilities estimated from an \n  existing multistate model. While package development has focused on multistate \n  models, calibration plots can be produced for any model which utilises information \n  post baseline to update predictions (e.g. dynamic models); competing risks models; \n  or standard single outcome survival models, where predictions can be made at \n  any landmark time. Please see Pate et al. (2024) <doi:10.1002/sim.10094>\n  and Pate et al. (2024) <https://alexpate30.github.io/calibmsm/articles/Overview.html>.  "
  },
  {
    "id": 9568,
    "package_name": "campfin",
    "title": "Wrangle Campaign Finance Data",
    "description": "Explore and normalize American campaign finance\n    data. Created by the Investigative Reporting Workshop to facilitate\n    work on The Accountability Project, an effort to collect public data\n    into a central, standard database that is more easily searched:\n    <https://publicaccountability.org/>.",
    "version": "1.0.11",
    "maintainer": "Kiernan Nicholls <kiernann@protonmail.com>",
    "author": "Kiernan Nicholls [aut, cre, cph],\n  Investigative Reporting Workshop [cph],\n  Yanqi Xu [aut],\n  Schuyler Erle [cph]",
    "url": "https://github.com/irworkshop/campfin,\nhttps://irworkshop.github.io/campfin/",
    "bug_reports": "https://github.com/irworkshop/campfin/issues",
    "repository": "https://cran.r-project.org/package=campfin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "campfin Wrangle Campaign Finance Data Explore and normalize American campaign finance\n    data. Created by the Investigative Reporting Workshop to facilitate\n    work on The Accountability Project, an effort to collect public data\n    into a central, standard database that is more easily searched:\n    <https://publicaccountability.org/>.  "
  },
  {
    "id": 9578,
    "package_name": "cancerradarr",
    "title": "Cancer RADAR Project Tool",
    "description": "Cancer RADAR is a project which aim is to develop an\n    infrastructure that allows quantifying the risk of cancer by migration\n    background across Europe.  This package contains a set of functions\n    cancer registries partners should use to reshape 5 year-age group\n    cancer incidence data into a set of summary statistics (see Boyle &\n    Parkin (1991, ISBN:978-92-832-1195-2)) in lines with Cancer RADAR data\n    protections rules.",
    "version": "2.1.0",
    "maintainer": "Damien Georges <georgesd@iarc.who.int>",
    "author": "Nienke Alberts [aut],\n  Damien Georges [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2425-7591>),\n  Stefano Rosso [aut],\n  Iacopo Baussano [aut] (ORCID: <https://orcid.org/0000-0002-7322-1862>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cancerradarr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cancerradarr Cancer RADAR Project Tool Cancer RADAR is a project which aim is to develop an\n    infrastructure that allows quantifying the risk of cancer by migration\n    background across Europe.  This package contains a set of functions\n    cancer registries partners should use to reshape 5 year-age group\n    cancer incidence data into a set of summary statistics (see Boyle &\n    Parkin (1991, ISBN:978-92-832-1195-2)) in lines with Cancer RADAR data\n    protections rules.  "
  },
  {
    "id": 9591,
    "package_name": "capn",
    "title": "Capital Asset Pricing for Nature",
    "description": "Implements approximation methods for natural capital asset prices suggested by Fenichel and Abbott (2014) <doi:10.1086/676034> in Journal of the Associations of Environmental and Resource Economists (JAERE), Fenichel et al. (2016) <doi:10.1073/pnas.1513779113> in Proceedings of the National Academy of Sciences (PNAS), and Yun et al. (2017) in PNAS (accepted), and their extensions: creating Chebyshev polynomial nodes and grids, calculating basis of Chebyshev polynomials, approximation and their simulations for: V-approximation (single and multiple stocks, PNAS), P-approximation (single stock, PNAS), and Pdot-approximation (single stock, JAERE). Development of this package was generously supported by the Knobloch Family Foundation.",
    "version": "1.0.0",
    "maintainer": "Seong Do Yun <seongdo.yun@yale.edu>",
    "author": "Seong Do Yun [aut, cre],\n  Eli P. Fenichel [aut, ctb],\n  Joshua K. Abbott [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=capn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "capn Capital Asset Pricing for Nature Implements approximation methods for natural capital asset prices suggested by Fenichel and Abbott (2014) <doi:10.1086/676034> in Journal of the Associations of Environmental and Resource Economists (JAERE), Fenichel et al. (2016) <doi:10.1073/pnas.1513779113> in Proceedings of the National Academy of Sciences (PNAS), and Yun et al. (2017) in PNAS (accepted), and their extensions: creating Chebyshev polynomial nodes and grids, calculating basis of Chebyshev polynomials, approximation and their simulations for: V-approximation (single and multiple stocks, PNAS), P-approximation (single stock, PNAS), and Pdot-approximation (single stock, JAERE). Development of this package was generously supported by the Knobloch Family Foundation.  "
  },
  {
    "id": 9609,
    "package_name": "care4cmodel",
    "title": "Carbon-Related Assessment of Silvicultural Concepts",
    "description": "A simulation model and accompanying functions that support \n    assessing silvicultural concepts on the forest estate level with a focus on \n    the CO2 uptake by wood growth and CO2 emissions by forest operations. For \n    achieving this, a virtual forest estate area is split into the areas covered\n    by typical phases of the silvicultural concept of interest. Given initial \n    area shares of these phases, the dynamics of these areas is simulated. The \n    typical carbon stocks and flows which are known for all phases are \n    attributed post-hoc to the areas and upscaled to the estate level. CO2 \n    emissions by forest operations are estimated based on the amounts and \n    dimensions of the harvested timber. Probabilities of damage events are taken\n    into account.",
    "version": "1.0.3",
    "maintainer": "Peter Biber <p.biber@tum.de>",
    "author": "Peter Biber [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9700-8708>),\n  Stefano Grigolato [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2089-3892>),\n  Julia Schmucker [ctb] (ORCID: <https://orcid.org/0000-0001-9996-4851>),\n  Enno Uhl [ctb] (ORCID: <https://orcid.org/0000-0002-7847-923X>),\n  Hans Pretzsch [ctb] (ORCID: <https://orcid.org/0000-0002-4958-1868>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=care4cmodel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "care4cmodel Carbon-Related Assessment of Silvicultural Concepts A simulation model and accompanying functions that support \n    assessing silvicultural concepts on the forest estate level with a focus on \n    the CO2 uptake by wood growth and CO2 emissions by forest operations. For \n    achieving this, a virtual forest estate area is split into the areas covered\n    by typical phases of the silvicultural concept of interest. Given initial \n    area shares of these phases, the dynamics of these areas is simulated. The \n    typical carbon stocks and flows which are known for all phases are \n    attributed post-hoc to the areas and upscaled to the estate level. CO2 \n    emissions by forest operations are estimated based on the amounts and \n    dimensions of the harvested timber. Probabilities of damage events are taken\n    into account.  "
  },
  {
    "id": 9632,
    "package_name": "casebase",
    "title": "Fitting Flexible Smooth-in-Time Hazards and Risk Functions via\nLogistic and Multinomial Regression",
    "description": "Fit flexible and fully parametric hazard regression models to survival data with single event type or multiple \n    competing causes via logistic and multinomial regression. Our formulation allows for arbitrary functional forms \n    of time and its interactions with other predictors for time-dependent hazards and hazard ratios. From the \n    fitted hazard model, we provide functions to readily calculate and plot cumulative incidence and survival \n    curves for a given covariate profile. This approach accommodates any log-linear hazard function of \n    prognostic time, treatment, and covariates, and readily allows for non-proportionality. We also provide \n    a plot method for visualizing incidence density via population time plots. Based on the case-base sampling \n    approach of Hanley and Miettinen (2009) <DOI:10.2202/1557-4679.1125>, Saarela and Arjas (2015) <DOI:10.1111/sjos.12125>, \n    and Saarela (2015) <DOI:10.1007/s10985-015-9352-x>.",
    "version": "0.10.6",
    "maintainer": "Sahir Bhatnagar <sahir.bhatnagar@gmail.com>",
    "author": "Sahir Bhatnagar [aut, cre] (https://sahirbhatnagar.com/),\n  Maxime Turgeon [aut] (https://www.maxturgeon.ca/),\n  Jesse Islam [aut] (https://www.jesseislam.com/),\n  Olli Saarela [aut]\n    (https://www.dlsph.utoronto.ca/faculty-profile/saarela-olli/),\n  James Hanley [aut] (https://jhanley.biostat.mcgill.ca/)",
    "url": "https://sahirbhatnagar.com/casebase/",
    "bug_reports": "https://github.com/sahirbhatnagar/casebase/issues",
    "repository": "https://cran.r-project.org/package=casebase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "casebase Fitting Flexible Smooth-in-Time Hazards and Risk Functions via\nLogistic and Multinomial Regression Fit flexible and fully parametric hazard regression models to survival data with single event type or multiple \n    competing causes via logistic and multinomial regression. Our formulation allows for arbitrary functional forms \n    of time and its interactions with other predictors for time-dependent hazards and hazard ratios. From the \n    fitted hazard model, we provide functions to readily calculate and plot cumulative incidence and survival \n    curves for a given covariate profile. This approach accommodates any log-linear hazard function of \n    prognostic time, treatment, and covariates, and readily allows for non-proportionality. We also provide \n    a plot method for visualizing incidence density via population time plots. Based on the case-base sampling \n    approach of Hanley and Miettinen (2009) <DOI:10.2202/1557-4679.1125>, Saarela and Arjas (2015) <DOI:10.1111/sjos.12125>, \n    and Saarela (2015) <DOI:10.1007/s10985-015-9352-x>.  "
  },
  {
    "id": 9652,
    "package_name": "catfun",
    "title": "Categorical Data Analysis",
    "description": "Includes wrapper functions around existing functions for the analysis of categorical data and introduces functions for calculating risk differences and matched odds ratios. R currently supports a wide variety of tools for the analysis of categorical data. However, many functions are spread across a variety of packages with differing syntax and poor compatibility with each another. prop_test() combines the functions binom.test(), prop.test() and BinomCI() into one output. prop_power() allows for power and sample size calculations for both balanced and unbalanced designs. riskdiff() is used for calculating risk differences and matched_or() is used for calculating matched odds ratios. For further information on methods used that are not documented in other packages see Nathan Mantel and William Haenszel (1959) <doi:10.1093/jnci/22.4.719> and Alan Agresti (2002) <ISBN:0-471-36093-7>. ",
    "version": "0.1.4",
    "maintainer": "Nick Williams <ntwilliams.personal@gmail.com>",
    "author": "Nick Williams",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catfun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catfun Categorical Data Analysis Includes wrapper functions around existing functions for the analysis of categorical data and introduces functions for calculating risk differences and matched odds ratios. R currently supports a wide variety of tools for the analysis of categorical data. However, many functions are spread across a variety of packages with differing syntax and poor compatibility with each another. prop_test() combines the functions binom.test(), prop.test() and BinomCI() into one output. prop_power() allows for power and sample size calculations for both balanced and unbalanced designs. riskdiff() is used for calculating risk differences and matched_or() is used for calculating matched odds ratios. For further information on methods used that are not documented in other packages see Nathan Mantel and William Haenszel (1959) <doi:10.1093/jnci/22.4.719> and Alan Agresti (2002) <ISBN:0-471-36093-7>.   "
  },
  {
    "id": 9668,
    "package_name": "causalCmprsk",
    "title": "Nonparametric and Cox-Based Estimation of Average Treatment\nEffects in Competing Risks",
    "description": "Estimation of average treatment effects (ATE) of point interventions on time-to-event outcomes with K competing risks (K can be 1). The method uses propensity scores and inverse probability weighting for emulation of baseline randomization, which is described in Charpignon et al. (2022) <doi:10.1038/s41467-022-35157-w>.",
    "version": "2.0.0",
    "maintainer": "Bella Vakulenko-Lagun <blagun@stat.haifa.ac.il>",
    "author": "Bella Vakulenko-Lagun [aut, cre],\n  Colin Magdamo [aut],\n  Marie-Laure Charpignon [aut],\n  Bang Zheng [aut],\n  Mark Albers [aut],\n  Sudeshna Das [aut]",
    "url": "https://github.com/Bella2001/causalCmprsk",
    "bug_reports": "https://github.com/Bella2001/causalCmprsk/issues",
    "repository": "https://cran.r-project.org/package=causalCmprsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "causalCmprsk Nonparametric and Cox-Based Estimation of Average Treatment\nEffects in Competing Risks Estimation of average treatment effects (ATE) of point interventions on time-to-event outcomes with K competing risks (K can be 1). The method uses propensity scores and inverse probability weighting for emulation of baseline randomization, which is described in Charpignon et al. (2022) <doi:10.1038/s41467-022-35157-w>.  "
  },
  {
    "id": 9738,
    "package_name": "ceRtainty",
    "title": "Certainty Equivalent",
    "description": "Compute the certainty equivalents and premium risks\n              as tools for risk-efficiency analysis. For more technical information, please refer to: Hardaker, Richardson, Lien, & Schumann (2004) <doi:10.1111/j.1467-8489.2004.00239.x>, and Richardson, & Outlaw (2008) <doi:10.2495/RISK080231>.",
    "version": "1.0.0",
    "maintainer": "Ariel Soto-Caro <arielsotocaro@gmail.com>",
    "author": "Ariel Soto-Caro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7008-4009>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ceRtainty",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ceRtainty Certainty Equivalent Compute the certainty equivalents and premium risks\n              as tools for risk-efficiency analysis. For more technical information, please refer to: Hardaker, Richardson, Lien, & Schumann (2004) <doi:10.1111/j.1467-8489.2004.00239.x>, and Richardson, & Outlaw (2008) <doi:10.2495/RISK080231>.  "
  },
  {
    "id": 9796,
    "package_name": "cgmquantify",
    "title": "Analyzing Glucose and Glucose Variability",
    "description": "Continuous glucose monitoring (CGM) systems provide real-time, \n  dynamic glucose information by tracking interstitial glucose values \n  throughout the day. Glycemic variability, also known as glucose variability, \n  is an established risk factor for hypoglycemia (Kovatchev) and \n  has been shown to be a risk factor in diabetes complications. \n  Over 20 metrics of glycemic variability have been identified. \n  Here, we provide functions to calculate glucose summary metrics, \n  glucose variability metrics (as defined in clinical publications), \n  and visualizations to visualize trends in CGM data.\n  Cho P, Bent B, Wittmann A, et al. (2020) <https://diabetes.diabetesjournals.org/content/69/Supplement_1/73-LB.abstract>\n  American Diabetes Association (2020) <https://professional.diabetes.org/diapro/glucose_calc>\n  Kovatchev B (2019) <doi:10.1177/1932296819826111>\n  Kovdeatchev BP (2017) <doi:10.1038/nrendo.2017.3>\n  Tamborlane W V., Beck RW, Bode BW, et al. (2008) <doi:10.1056/NEJMoa0805017>\n  Umpierrez GE, P. Kovatchev B (2018) <doi:10.1016/j.amjms.2018.09.010>.",
    "version": "0.1.0",
    "maintainer": "Maria Henriquez <marhenriq@gmail.com>",
    "author": "Maria Henriquez [aut, com, cph, cre, trl],\n  Brinnae Bent [aut, cph, dtc]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cgmquantify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cgmquantify Analyzing Glucose and Glucose Variability Continuous glucose monitoring (CGM) systems provide real-time, \n  dynamic glucose information by tracking interstitial glucose values \n  throughout the day. Glycemic variability, also known as glucose variability, \n  is an established risk factor for hypoglycemia (Kovatchev) and \n  has been shown to be a risk factor in diabetes complications. \n  Over 20 metrics of glycemic variability have been identified. \n  Here, we provide functions to calculate glucose summary metrics, \n  glucose variability metrics (as defined in clinical publications), \n  and visualizations to visualize trends in CGM data.\n  Cho P, Bent B, Wittmann A, et al. (2020) <https://diabetes.diabetesjournals.org/content/69/Supplement_1/73-LB.abstract>\n  American Diabetes Association (2020) <https://professional.diabetes.org/diapro/glucose_calc>\n  Kovatchev B (2019) <doi:10.1177/1932296819826111>\n  Kovdeatchev BP (2017) <doi:10.1038/nrendo.2017.3>\n  Tamborlane W V., Beck RW, Bode BW, et al. (2008) <doi:10.1056/NEJMoa0805017>\n  Umpierrez GE, P. Kovatchev B (2018) <doi:10.1016/j.amjms.2018.09.010>.  "
  },
  {
    "id": 9832,
    "package_name": "checkhelper",
    "title": "Deal with Check Outputs",
    "description": "Deal with packages 'check' outputs and reduce the risk of\n    rejection by 'CRAN' by following policies.",
    "version": "0.1.1",
    "maintainer": "Sebastien Rochette <sebastien@thinkr.fr>",
    "author": "Sebastien Rochette [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1565-9313>),\n  Vincent Guyader [aut] (ORCID: <https://orcid.org/0000-0003-0671-9270>),\n  Arthur Br\u00e9ant [aut] (ORCID: <https://orcid.org/0000-0003-1668-0963>),\n  Murielle Delmotte [aut] (ORCID:\n    <https://orcid.org/0000-0002-1339-2424>),\n  ThinkR [cph]",
    "url": "https://thinkr-open.github.io/checkhelper/,\nhttps://github.com/ThinkR-open/checkhelper",
    "bug_reports": "https://github.com/ThinkR-open/checkhelper/issues",
    "repository": "https://cran.r-project.org/package=checkhelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "checkhelper Deal with Check Outputs Deal with packages 'check' outputs and reduce the risk of\n    rejection by 'CRAN' by following policies.  "
  },
  {
    "id": 9981,
    "package_name": "climextRemes",
    "title": "Tools for Analyzing Climate Extremes",
    "description": "Functions for fitting GEV and POT (via point process fitting)\n    models for extremes in climate data, providing return values, return\n    probabilities, and return periods for stationary and nonstationary models.\n    Also provides differences in return values and differences in log return\n    probabilities for contrasts of covariate values. Functions for estimating risk\n    ratios for event attribution analyses, including uncertainty. Under the hood,\n    many of the functions use functions from 'extRemes', including for fitting the\n    statistical models. Details are given in Paciorek, Stone, and Wehner (2018)\n    <doi:10.1016/j.wace.2018.01.002>.",
    "version": "0.3.1",
    "maintainer": "Christopher Paciorek <paciorek@stat.berkeley.edu>",
    "author": "Christopher Paciorek [aut, cre],\n  Harinarayan Krishnan [ctb]",
    "url": "https://bitbucket.org/lbl-cascade/climextremes-dev",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=climextRemes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "climextRemes Tools for Analyzing Climate Extremes Functions for fitting GEV and POT (via point process fitting)\n    models for extremes in climate data, providing return values, return\n    probabilities, and return periods for stationary and nonstationary models.\n    Also provides differences in return values and differences in log return\n    probabilities for contrasts of covariate values. Functions for estimating risk\n    ratios for event attribution analyses, including uncertainty. Under the hood,\n    many of the functions use functions from 'extRemes', including for fitting the\n    statistical models. Details are given in Paciorek, Stone, and Wehner (2018)\n    <doi:10.1016/j.wace.2018.01.002>.  "
  },
  {
    "id": 9999,
    "package_name": "cliot",
    "title": "Clinical Indices and Outcomes Tools",
    "description": "Collection of indices and tools relating to clinical research that aid epidemiological cohort or retrospective chart review with big data. All indices and tools take commonly used lab values, patient demographics, and clinical measurements to compute various risk and predictive values for survival or further classification/stratification. References to original literature and validation contained in each function documentation. Includes all commonly available calculators available online. ",
    "version": "1.0.0",
    "maintainer": "Neel Agarwal <neel.agarwal.216@gmail.com>",
    "author": "Neel Agarwal [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cliot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cliot Clinical Indices and Outcomes Tools Collection of indices and tools relating to clinical research that aid epidemiological cohort or retrospective chart review with big data. All indices and tools take commonly used lab values, patient demographics, and clinical measurements to compute various risk and predictive values for survival or further classification/stratification. References to original literature and validation contained in each function documentation. Includes all commonly available calculators available online.   "
  },
  {
    "id": 10080,
    "package_name": "cmpp",
    "title": "Direct Parametric Inference for the Cumulative Incidence\nFunction in Competing Risks",
    "description": "Implements parametric (Direct) regression methods for modeling cumulative incidence functions (CIFs) in \n                the presence of competing risks. Methods include the direct Gompertz-based approach \n                and generalized regression models as described in Jeong and Fine (2006) <doi:10.1111/j.1467-9876.2006.00532.x> \n                and Jeong and Fine (2007) <doi:10.1093/biostatistics/kxj040>.\n                The package facilitates maximum likelihood estimation, variance computation, \n                with applications to clinical trials and survival analysis.",
    "version": "0.0.2",
    "maintainer": "Habib Ezzatabadipour <habibezati@outlook.com>",
    "author": "Habib Ezzatabadipour [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cmpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmpp Direct Parametric Inference for the Cumulative Incidence\nFunction in Competing Risks Implements parametric (Direct) regression methods for modeling cumulative incidence functions (CIFs) in \n                the presence of competing risks. Methods include the direct Gompertz-based approach \n                and generalized regression models as described in Jeong and Fine (2006) <doi:10.1111/j.1467-9876.2006.00532.x> \n                and Jeong and Fine (2007) <doi:10.1093/biostatistics/kxj040>.\n                The package facilitates maximum likelihood estimation, variance computation, \n                with applications to clinical trials and survival analysis.  "
  },
  {
    "id": 10081,
    "package_name": "cmprsk",
    "title": "Subdistribution Analysis of Competing Risks",
    "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks, as described in Gray\n (1988), A class of K-sample tests for comparing the cumulative\n incidence of a competing risk, Ann. Stat. 16:1141-1154\n <DOI:10.1214/aos/1176350951>, and Fine JP and\n Gray RJ (1999), A proportional hazards model for the subdistribution\n of a competing risk, JASA, 94:496-509, <DOI:10.1080/01621459.1999.10474144>.",
    "version": "2.2-12",
    "maintainer": "Bob Gray <gray@jimmy.harvard.edu>",
    "author": "Bob Gray <gray@jimmy.harvard.edu>",
    "url": "https://www.R-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cmprsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmprsk Subdistribution Analysis of Competing Risks Estimation, testing and regression modeling of\n subdistribution functions in competing risks, as described in Gray\n (1988), A class of K-sample tests for comparing the cumulative\n incidence of a competing risk, Ann. Stat. 16:1141-1154\n <DOI:10.1214/aos/1176350951>, and Fine JP and\n Gray RJ (1999), A proportional hazards model for the subdistribution\n of a competing risk, JASA, 94:496-509, <DOI:10.1080/01621459.1999.10474144>.  "
  },
  {
    "id": 10082,
    "package_name": "cmprskQR",
    "title": "Analysis of Competing Risks Using Quantile Regressions",
    "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks using quantile regressions,\n as described in Peng and Fine (2009) <DOI:10.1198/jasa.2009.tm08228>.",
    "version": "0.9.2",
    "maintainer": "Stephan Dlugosz <stephan.dlugosz@googlemail.com>",
    "author": "Stephan Dlugosz [aut, cre], \n        Limin Peng [aut],\n        Ruosha Li [aut],\n        Shuolin Shi [ctb]",
    "url": "https://bitbucket.org/sdlugosz/cmprskqr",
    "bug_reports": "https://bitbucket.org/sdlugosz/cmprskqr/issues",
    "repository": "https://cran.r-project.org/package=cmprskQR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmprskQR Analysis of Competing Risks Using Quantile Regressions Estimation, testing and regression modeling of\n subdistribution functions in competing risks using quantile regressions,\n as described in Peng and Fine (2009) <DOI:10.1198/jasa.2009.tm08228>.  "
  },
  {
    "id": 10083,
    "package_name": "cmprskcoxmsm",
    "title": "Use IPW to Estimate Treatment Effect under Competing Risks",
    "description": "Uses inverse probability weighting methods to estimate treatment effect under marginal structure model for the cause-specific hazard of competing risk events. Estimates also the cumulative incidence function (i.e. risk) of the potential outcomes, and provides inference on risk difference and risk ratio. Reference: Kalbfleisch & Prentice (2002)<doi:10.1002/9781118032985>; Hernan et al (2001)<doi:10.1198/016214501753168154>.",
    "version": "0.2.1",
    "maintainer": "Yiran Zhang <yiz038@health.ucsd.edu>",
    "author": "Yiran Zhang, Ronghui Xu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cmprskcoxmsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmprskcoxmsm Use IPW to Estimate Treatment Effect under Competing Risks Uses inverse probability weighting methods to estimate treatment effect under marginal structure model for the cause-specific hazard of competing risk events. Estimates also the cumulative incidence function (i.e. risk) of the potential outcomes, and provides inference on risk difference and risk ratio. Reference: Kalbfleisch & Prentice (2002)<doi:10.1002/9781118032985>; Hernan et al (2001)<doi:10.1198/016214501753168154>.  "
  },
  {
    "id": 10162,
    "package_name": "cointReg",
    "title": "Parameter Estimation and Inference in a Cointegrating Regression",
    "description": "Cointegration methods are widely used in empirical macroeconomics\n    and empirical finance. It is well known that in a cointegrating\n    regression the ordinary least squares (OLS) estimator of the\n    parameters is super-consistent, i.e. converges at rate equal to the\n    sample size T. When the regressors are endogenous, the limiting\n    distribution of the OLS estimator is contaminated by so-called second\n    order bias terms, see e.g. Phillips and Hansen (1990) <DOI:10.2307/2297545>.\n    The presence of these bias terms renders inference difficult. Consequently,\n    several modifications to OLS that lead to zero mean Gaussian mixture\n    limiting distributions have been proposed, which in turn make\n    standard asymptotic inference feasible. These methods include\n    the fully modified OLS (FM-OLS) approach of Phillips and Hansen\n    (1990) <DOI:10.2307/2297545>, the dynamic OLS (D-OLS) approach of Phillips\n    and Loretan (1991) <DOI:10.2307/2298004>, Saikkonen (1991)\n    <DOI:10.1017/S0266466600004217> and Stock and Watson (1993)\n    <DOI:10.2307/2951763> and the new estimation approach called integrated\n    modified OLS (IM-OLS) of Vogelsang and Wagner (2014)\n    <DOI:10.1016/j.jeconom.2013.10.015>. The latter is based on an augmented\n    partial sum (integration) transformation of the regression model. IM-OLS is\n    similar in spirit to the FM- and D-OLS approaches, with the key difference\n    that it does not require estimation of long run variance matrices and avoids\n    the need to choose tuning parameters (kernels, bandwidths, lags). However,\n    inference does require that a long run variance be scaled out.\n    This package provides functions for the parameter estimation and inference\n    with all three modified OLS approaches. That includes the automatic\n    bandwidth selection approaches of Andrews (1991) <DOI:10.2307/2938229> and\n    of Newey and West (1994) <DOI:10.2307/2297912> as well as the calculation of\n    the long run variance.",
    "version": "0.2.0",
    "maintainer": "Philipp Aschersleben <aschersleben@statistik.tu-dortmund.de>",
    "author": "Philipp Aschersleben [aut, cre],\n  Martin Wagner [aut] (Author of underlying MATLAB code.)",
    "url": "https://github.com/aschersleben/cointReg",
    "bug_reports": "https://github.com/aschersleben/cointReg/issues",
    "repository": "https://cran.r-project.org/package=cointReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cointReg Parameter Estimation and Inference in a Cointegrating Regression Cointegration methods are widely used in empirical macroeconomics\n    and empirical finance. It is well known that in a cointegrating\n    regression the ordinary least squares (OLS) estimator of the\n    parameters is super-consistent, i.e. converges at rate equal to the\n    sample size T. When the regressors are endogenous, the limiting\n    distribution of the OLS estimator is contaminated by so-called second\n    order bias terms, see e.g. Phillips and Hansen (1990) <DOI:10.2307/2297545>.\n    The presence of these bias terms renders inference difficult. Consequently,\n    several modifications to OLS that lead to zero mean Gaussian mixture\n    limiting distributions have been proposed, which in turn make\n    standard asymptotic inference feasible. These methods include\n    the fully modified OLS (FM-OLS) approach of Phillips and Hansen\n    (1990) <DOI:10.2307/2297545>, the dynamic OLS (D-OLS) approach of Phillips\n    and Loretan (1991) <DOI:10.2307/2298004>, Saikkonen (1991)\n    <DOI:10.1017/S0266466600004217> and Stock and Watson (1993)\n    <DOI:10.2307/2951763> and the new estimation approach called integrated\n    modified OLS (IM-OLS) of Vogelsang and Wagner (2014)\n    <DOI:10.1016/j.jeconom.2013.10.015>. The latter is based on an augmented\n    partial sum (integration) transformation of the regression model. IM-OLS is\n    similar in spirit to the FM- and D-OLS approaches, with the key difference\n    that it does not require estimation of long run variance matrices and avoids\n    the need to choose tuning parameters (kernels, bandwidths, lags). However,\n    inference does require that a long run variance be scaled out.\n    This package provides functions for the parameter estimation and inference\n    with all three modified OLS approaches. That includes the automatic\n    bandwidth selection approaches of Andrews (1991) <DOI:10.2307/2938229> and\n    of Newey and West (1994) <DOI:10.2307/2297912> as well as the calculation of\n    the long run variance.  "
  },
  {
    "id": 10289,
    "package_name": "condorOptions",
    "title": "Trading Condor Options Strategies",
    "description": "Trading of Condor Options Strategies is represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=condorOptions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "condorOptions Trading Condor Options Strategies Trading of Condor Options Strategies is represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).  "
  },
  {
    "id": 10365,
    "package_name": "convergenceDFM",
    "title": "Convergence and Dynamic Factor Models",
    "description": "Tests convergence in macro-financial panels combining\n    Dynamic Factor Models (DFM) and mean-reverting Ornstein-Uhlenbeck (OU)\n    processes. Provides: (i) static/approximate DFMs for large panels with\n    VAR/VECM stability checks, Portmanteau tests and rolling out-of-sample R^2,\n    following Stock and Watson (2002) <doi:10.1198/073500102317351921> and the\n    Generalized Dynamic Factor Model of Forni, Hallin, Lippi and Reichlin (2000)\n    <doi:10.1162/003465300559037>; (ii) cointegration analysis \u00e0 la Johansen\n    (1988) <doi:10.1016/0165-1889(88)90041-3>; (iii) OU-based convergence and\n    half-life summaries grounded in Uhlenbeck and Ornstein (1930)\n    <doi:10.1103/PhysRev.36.823> and Vasicek (1977) <doi:10.1016/0304-405X(77)90016-2>;\n    (iv) robust inference via 'sandwich' HC/HAC estimators (Zeileis (2004)\n    <doi:10.18637/jss.v011.i10>) and regression diagnostics ('lmtest'); and\n    (v) optional PLS-based factor preselection (Mevik and Wehrens (2007)\n    <doi:10.18637/jss.v018.i02>). Functions emphasize reproducibility and clear,\n    publication-ready summaries.",
    "version": "0.1.4",
    "maintainer": "Jos\u00e9 Mauricio G\u00f3mez Juli\u00e1n <isadorenabi@pm.me>",
    "author": "Jos\u00e9 Mauricio G\u00f3mez Juli\u00e1n [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=convergenceDFM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "convergenceDFM Convergence and Dynamic Factor Models Tests convergence in macro-financial panels combining\n    Dynamic Factor Models (DFM) and mean-reverting Ornstein-Uhlenbeck (OU)\n    processes. Provides: (i) static/approximate DFMs for large panels with\n    VAR/VECM stability checks, Portmanteau tests and rolling out-of-sample R^2,\n    following Stock and Watson (2002) <doi:10.1198/073500102317351921> and the\n    Generalized Dynamic Factor Model of Forni, Hallin, Lippi and Reichlin (2000)\n    <doi:10.1162/003465300559037>; (ii) cointegration analysis \u00e0 la Johansen\n    (1988) <doi:10.1016/0165-1889(88)90041-3>; (iii) OU-based convergence and\n    half-life summaries grounded in Uhlenbeck and Ornstein (1930)\n    <doi:10.1103/PhysRev.36.823> and Vasicek (1977) <doi:10.1016/0304-405X(77)90016-2>;\n    (iv) robust inference via 'sandwich' HC/HAC estimators (Zeileis (2004)\n    <doi:10.18637/jss.v011.i10>) and regression diagnostics ('lmtest'); and\n    (v) optional PLS-based factor preselection (Mevik and Wehrens (2007)\n    <doi:10.18637/jss.v018.i02>). Functions emphasize reproducibility and clear,\n    publication-ready summaries.  "
  },
  {
    "id": 10381,
    "package_name": "copcor",
    "title": "Correlates of Protection and Correlates of Risk Functions",
    "description": "Correlates of protection (CoP) and correlates of risk (CoR) study the immune biomarkers associated with an infectious disease outcome, e.g. COVID or HIV-1 infection. This package contains shared functions for analyzing CoP and CoR, including bootstrapping procedures, competing risk estimation, and bootstrapping marginalized risks.",
    "version": "2024.7-31",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "author": "Youyi Fong [cre],\n  Yiwen He [aut],\n  Chenchen Yu [aut],\n  Bhavesh Borate [aut],\n  Peter Gilbert [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=copcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "copcor Correlates of Protection and Correlates of Risk Functions Correlates of protection (CoP) and correlates of risk (CoR) study the immune biomarkers associated with an infectious disease outcome, e.g. COVID or HIV-1 infection. This package contains shared functions for analyzing CoP and CoR, including bootstrapping procedures, competing risk estimation, and bootstrapping marginalized risks.  "
  },
  {
    "id": 10413,
    "package_name": "corpmetrics",
    "title": "Tools for Valuation, Financial Metrics and Modeling in Corporate\nFinance",
    "description": "Balance sheet and income statement metrics, investment analysis methods, valuation methods, loan amortization schedules, and Capital Asset Pricing Model.",
    "version": "1.0",
    "maintainer": "Pavlos Pantatosakis <pantatosakisp@yahoo.com>",
    "author": "Pavlos Pantatosakis [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corpmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corpmetrics Tools for Valuation, Financial Metrics and Modeling in Corporate\nFinance Balance sheet and income statement metrics, investment analysis methods, valuation methods, loan amortization schedules, and Capital Asset Pricing Model.  "
  },
  {
    "id": 10490,
    "package_name": "covidsymptom",
    "title": "COVID Symptom Study Sweden Open Dataset",
    "description": "The COVID Symptom Study is a non-commercial project that uses\n    a free mobile app to facilitate real-time data collection of symptoms,\n    exposures, and risk factors related to COVID19. The package allows\n    easy access to summary statistics data from COVID Symptom Study\n    Sweden.",
    "version": "1.0.0",
    "maintainer": "Hugo Fitipaldi <hugofitipaldi@gmail.com>",
    "author": "Hugo Fitipaldi [aut, cre]",
    "url": "https://github.com/hugofitipaldi/covidsymptom",
    "bug_reports": "https://github.com/hugofitipaldi/covidsymptom/issues",
    "repository": "https://cran.r-project.org/package=covidsymptom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covidsymptom COVID Symptom Study Sweden Open Dataset The COVID Symptom Study is a non-commercial project that uses\n    a free mobile app to facilitate real-time data collection of symptoms,\n    exposures, and risk factors related to COVID19. The package allows\n    easy access to summary statistics data from COVID Symptom Study\n    Sweden.  "
  },
  {
    "id": 10495,
    "package_name": "cowfootR",
    "title": "Dairy Farm Carbon Footprint Assessment",
    "description": "Calculates the carbon footprint of dairy farms based on methodologies of the International Dairy Federation and the Intergovernmental Panel on Climate Change. Includes tools for single-farm and batch analysis, report generation, and visualization. Methods follow International Dairy Federation (2022) \"The IDF global Carbon Footprint standard for the dairy sector\" (Bulletin of the IDF n\u00b0 520/2022) <doi:10.56169/FKRK7166> and IPCC (2019) \"2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories, Chapter 10: Emissions from Livestock and Manure Management\" <https://www.ipcc-nggip.iges.or.jp/public/2019rf/pdf/4_Volume4/19R_V4_Ch10_Livestock.pdf> guidelines.",
    "version": "0.1.2",
    "maintainer": "Juan Moreno <juanmarcosmoreno@gmail.com>",
    "author": "Juan Moreno [aut, cre]",
    "url": "https://github.com/juanmarcosmoreno-arch/cowfootR,\nhttps://juanmarcosmoreno-arch.github.io/cowfootR/",
    "bug_reports": "https://github.com/juanmarcosmoreno-arch/cowfootR/issues",
    "repository": "https://cran.r-project.org/package=cowfootR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cowfootR Dairy Farm Carbon Footprint Assessment Calculates the carbon footprint of dairy farms based on methodologies of the International Dairy Federation and the Intergovernmental Panel on Climate Change. Includes tools for single-farm and batch analysis, report generation, and visualization. Methods follow International Dairy Federation (2022) \"The IDF global Carbon Footprint standard for the dairy sector\" (Bulletin of the IDF n\u00b0 520/2022) <doi:10.56169/FKRK7166> and IPCC (2019) \"2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories, Chapter 10: Emissions from Livestock and Manure Management\" <https://www.ipcc-nggip.iges.or.jp/public/2019rf/pdf/4_Volume4/19R_V4_Ch10_Livestock.pdf> guidelines.  "
  },
  {
    "id": 10540,
    "package_name": "cragg",
    "title": "Tests for Weak Instruments in R",
    "description": "Implements Cragg-Donald (1993) <doi:10.1017/S0266466600007519> and Stock and Yogo (2005) <doi:10.1017/CBO9780511614491.006> tests for weak instruments in R.",
    "version": "0.0.1",
    "maintainer": "Beniamino Green <ben@greendalba.com>",
    "author": "Beniamino Green [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cragg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cragg Tests for Weak Instruments in R Implements Cragg-Donald (1993) <doi:10.1017/S0266466600007519> and Stock and Yogo (2005) <doi:10.1017/CBO9780511614491.006> tests for weak instruments in R.  "
  },
  {
    "id": 10553,
    "package_name": "creditmodel",
    "title": "Toolkit for Credit Modeling, Analysis and Visualization",
    "description": "\n  Provides a highly efficient R tool suite for Credit Modeling, Analysis and Visualization.Contains infrastructure functionalities such as data exploration and preparation, missing values treatment, outliers treatment, variable derivation, variable selection, dimensionality reduction, grid search for hyper parameters, data mining and visualization, model evaluation, strategy analysis etc. This package is designed to make the development of binary classification models (machine learning based models as well as credit scorecard) simpler and faster. The references including: 1 Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: Development and Implementation Using SAS; 2 Bezdek, James C.FCM: The fuzzy c-means clustering algorithm. Computers & Geosciences (0098-3004),<DOI:10.1016/0098-3004(84)90020-7>.",
    "version": "1.3.1",
    "maintainer": "Dongping Fan <fdp@pku.edu.cn>",
    "author": "Dongping Fan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=creditmodel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "creditmodel Toolkit for Credit Modeling, Analysis and Visualization \n  Provides a highly efficient R tool suite for Credit Modeling, Analysis and Visualization.Contains infrastructure functionalities such as data exploration and preparation, missing values treatment, outliers treatment, variable derivation, variable selection, dimensionality reduction, grid search for hyper parameters, data mining and visualization, model evaluation, strategy analysis etc. This package is designed to make the development of binary classification models (machine learning based models as well as credit scorecard) simpler and faster. The references including: 1 Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: Development and Implementation Using SAS; 2 Bezdek, James C.FCM: The fuzzy c-means clustering algorithm. Computers & Geosciences (0098-3004),<DOI:10.1016/0098-3004(84)90020-7>.  "
  },
  {
    "id": 10604,
    "package_name": "crrSC",
    "title": "Competing Risks Regression for Stratified and Clustered Data",
    "description": "Extension of 'cmprsk' to Stratified and Clustered data.    \n      A goodness of fit test for Fine-Gray model is also provided.        \n      Methods are detailed in the following articles: Zhou et al. (2011) <doi:10.1111/j.1541-0420.2010.01493.x>,\n      Zhou et al. (2012) <doi:10.1093/biostatistics/kxr032>, \n      Zhou et al. (2013) <doi: 10.1002/sim.5815>.",
    "version": "1.1.2",
    "maintainer": "Aurelien Latouche <aurelien.latouche@cnam.fr>",
    "author": "Bingqing Zhou and Aurelien Latouche",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crrSC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crrSC Competing Risks Regression for Stratified and Clustered Data Extension of 'cmprsk' to Stratified and Clustered data.    \n      A goodness of fit test for Fine-Gray model is also provided.        \n      Methods are detailed in the following articles: Zhou et al. (2011) <doi:10.1111/j.1541-0420.2010.01493.x>,\n      Zhou et al. (2012) <doi:10.1093/biostatistics/kxr032>, \n      Zhou et al. (2013) <doi: 10.1002/sim.5815>.  "
  },
  {
    "id": 10605,
    "package_name": "crrcbcv",
    "title": "Bias-Corrected Variance for Competing Risks Regression with\nClustered Data",
    "description": "\n  A user friendly function 'crrcbcv' to compute bias-corrected variances for competing risks regression models using proportional subdistribution hazards with small-sample clustered data. Four types of bias correction are included: the MD-type bias correction by Mancl and DeRouen (2001) <doi:10.1111/j.0006-341X.2001.00126.x>, the KC-type bias correction by Kauermann and Carroll (2001) <doi:10.1198/016214501753382309>, the FG-type bias correction by Fay and Graubard (2001) <doi:10.1111/j.0006-341X.2001.01198.x>, and the MBN-type bias correction by Morel, Bokossa, and Neerchal (2003) <doi:10.1002/bimj.200390021>.",
    "version": "1.0",
    "maintainer": "Xinyuan Chen <xchen@math.msstate.edu>",
    "author": "Xinyuan Chen [aut, cre, cph],\n  Fan Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crrcbcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crrcbcv Bias-Corrected Variance for Competing Risks Regression with\nClustered Data \n  A user friendly function 'crrcbcv' to compute bias-corrected variances for competing risks regression models using proportional subdistribution hazards with small-sample clustered data. Four types of bias correction are included: the MD-type bias correction by Mancl and DeRouen (2001) <doi:10.1111/j.0006-341X.2001.00126.x>, the KC-type bias correction by Kauermann and Carroll (2001) <doi:10.1198/016214501753382309>, the FG-type bias correction by Fay and Graubard (2001) <doi:10.1111/j.0006-341X.2001.01198.x>, and the MBN-type bias correction by Morel, Bokossa, and Neerchal (2003) <doi:10.1002/bimj.200390021>.  "
  },
  {
    "id": 10606,
    "package_name": "crrstep",
    "title": "Stepwise Covariate Selection for the Fine & Gray Competing Risks\nRegression Model",
    "description": "Performs forward and backwards stepwise regression for the Proportional subdistribution hazards model in competing risks (Fine & Gray 1999). Procedure uses AIC, BIC and BICcr as selection criteria. BICcr has a penalty of k = log(n*), where n* is the number of primary events.",
    "version": "2024.1.1",
    "maintainer": "Ravi Varadhan <ravi.varadhan@jhu.edu>",
    "author": "Ravi Varadhan [aut, cre],\n  Deborah Kuk [aut],\n  Leon Wang [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crrstep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crrstep Stepwise Covariate Selection for the Fine & Gray Competing Risks\nRegression Model Performs forward and backwards stepwise regression for the Proportional subdistribution hazards model in competing risks (Fine & Gray 1999). Procedure uses AIC, BIC and BICcr as selection criteria. BICcr has a penalty of k = log(n*), where n* is the number of primary events.  "
  },
  {
    "id": 10608,
    "package_name": "crseEventStudy",
    "title": "A Robust and Powerful Test of Abnormal Stock Returns in\nLong-Horizon Event Studies",
    "description": "Based on Dutta et al. (2018) <doi:10.1016/j.jempfin.2018.02.004>, this package provides their standardized test for abnormal returns in long-horizon event studies. The methods used improve the major weaknesses of size, power, and robustness of long-run statistical tests described in Kothari/Warner (2007) <doi:10.1016/B978-0-444-53265-7.50015-9>. Abnormal returns are weighted by their statistical precision (i.e., standard deviation), resulting in abnormal standardized returns. This procedure efficiently captures the heteroskedasticity problem. Clustering techniques following Cameron et al. (2011) <doi:10.1198/jbes.2010.07136> are adopted for computing cross-sectional correlation robust standard errors. The statistical tests in this package therefore accounts for potential biases arising from returns' cross-sectional correlation, autocorrelation, and volatility clustering without power loss.",
    "version": "1.2.2",
    "maintainer": "Siegfried K\u00f6stlmeier <siegfried.koestlmeier@gmail.com>",
    "author": "Siegfried K\u00f6stlmeier [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7221-6981>),\n  Seppo Pynnonen [aut]",
    "url": "https://github.com/skoestlmeier/crseEventStudy",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crseEventStudy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crseEventStudy A Robust and Powerful Test of Abnormal Stock Returns in\nLong-Horizon Event Studies Based on Dutta et al. (2018) <doi:10.1016/j.jempfin.2018.02.004>, this package provides their standardized test for abnormal returns in long-horizon event studies. The methods used improve the major weaknesses of size, power, and robustness of long-run statistical tests described in Kothari/Warner (2007) <doi:10.1016/B978-0-444-53265-7.50015-9>. Abnormal returns are weighted by their statistical precision (i.e., standard deviation), resulting in abnormal standardized returns. This procedure efficiently captures the heteroskedasticity problem. Clustering techniques following Cameron et al. (2011) <doi:10.1198/jbes.2010.07136> are adopted for computing cross-sectional correlation robust standard errors. The statistical tests in this package therefore accounts for potential biases arising from returns' cross-sectional correlation, autocorrelation, and volatility clustering without power loss.  "
  },
  {
    "id": 10620,
    "package_name": "cryptoQuotes",
    "title": "Open Access to Cryptocurrency Market Data, Sentiment Indicators\nand Interactive Charts",
    "description": "\n  This high-level API client provides open access to cryptocurrency market data, sentiment indicators, and interactive charting tools. \n  The data is sourced from major cryptocurrency exchanges via 'curl' and returned in 'xts'-format. The data comes in open, high, low, and close (OHLC) format with flexible granularity, ranging from seconds to months. \n  This flexibility makes it ideal for developing and backtesting trading strategies or conducting detailed market analysis.",
    "version": "1.3.3",
    "maintainer": "Serkan Korkmaz <serkor1@duck.com>",
    "author": "Serkan Korkmaz [cre, aut, ctb, cph] (ORCID:\n    <https://orcid.org/0000-0002-5052-0982>),\n  Jonas Cuzulan Hirani [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9512-1993>)",
    "url": "https://serkor1.github.io/cryptoQuotes/,\nhttps://github.com/serkor1/cryptoQuotes",
    "bug_reports": "https://github.com/serkor1/cryptoQuotes/issues",
    "repository": "https://cran.r-project.org/package=cryptoQuotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cryptoQuotes Open Access to Cryptocurrency Market Data, Sentiment Indicators\nand Interactive Charts \n  This high-level API client provides open access to cryptocurrency market data, sentiment indicators, and interactive charting tools. \n  The data is sourced from major cryptocurrency exchanges via 'curl' and returned in 'xts'-format. The data comes in open, high, low, and close (OHLC) format with flexible granularity, ranging from seconds to months. \n  This flexibility makes it ideal for developing and backtesting trading strategies or conducting detailed market analysis.  "
  },
  {
    "id": 10623,
    "package_name": "cryptotrackr",
    "title": "An Interface to Crypto Data Sources",
    "description": "Allows you to connect to data sources across the \n        crypto ecosystem. This data can enable a range of activity such as \n        portfolio tracking, programmatic trading, or industry analysis. The \n        package is described in French (2024) <https://github.com/TrevorFrench/cryptotrackr/wiki>.",
    "version": "1.3.3",
    "maintainer": "Trevor French <FrenchTrevor@outlook.com>",
    "author": "Trevor French [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6246-2249>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cryptotrackr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cryptotrackr An Interface to Crypto Data Sources Allows you to connect to data sources across the \n        crypto ecosystem. This data can enable a range of activity such as \n        portfolio tracking, programmatic trading, or industry analysis. The \n        package is described in French (2024) <https://github.com/TrevorFrench/cryptotrackr/wiki>.  "
  },
  {
    "id": 10691,
    "package_name": "cumulcalib",
    "title": "Cumulative Calibration Assessment for Prediction Models",
    "description": "Tools for visualization of, and inference on, the calibration of prediction models on the cumulative domain. This provides a method for evaluating calibration of risk prediction models without having to group the data or use tuning parameters (e.g., loess bandwidth). This package implements the methodology described in Sadatsafavi and Patkau (2024) <doi:10.1002/sim.10138>. The core of the package is cumulcalib(), which takes in vectors of binary responses and predicted risks. The plot() and summary() methods are implemented for the results returned by cumulcalib().",
    "version": "0.0.1",
    "maintainer": "Mohsen Sadatsafavi <mohsen.sadatsafavi@ubc.ca>",
    "author": "Mohsen Sadatsafavi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0419-7862>)",
    "url": "https://github.com/resplab/cumulcalib",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cumulcalib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cumulcalib Cumulative Calibration Assessment for Prediction Models Tools for visualization of, and inference on, the calibration of prediction models on the cumulative domain. This provides a method for evaluating calibration of risk prediction models without having to group the data or use tuning parameters (e.g., loess bandwidth). This package implements the methodology described in Sadatsafavi and Patkau (2024) <doi:10.1002/sim.10138>. The core of the package is cumulcalib(), which takes in vectors of binary responses and predicted risks. The plot() and summary() methods are implemented for the results returned by cumulcalib().  "
  },
  {
    "id": 10765,
    "package_name": "dMrs",
    "title": "Competing Risk in Dependent Net Survival Analysis",
    "description": "Provides statistical tools for \n\tanalyzing net and relative survival, with a key feature \n\tof relaxing the assumption of independent censoring and \n\tincorporating the effect of dependent competing risks. \n\tIt employs a copula-based methodology, specifically the \n\tArchimedean copula, to simulate data, conduct survival \n\tanalysis, and offer comparisons with other methods. \n\tThis approach is detailed in the work of \n\tAdatorwovor et al. (2022) <doi:10.1515/ijb-2021-0016>.",
    "version": "1.0.0",
    "maintainer": "Paul Little <pllittle321@gmail.com>",
    "author": "Reuben Adatorwovor [aut],\n  Paul Little [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dMrs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dMrs Competing Risk in Dependent Net Survival Analysis Provides statistical tools for \n\tanalyzing net and relative survival, with a key feature \n\tof relaxing the assumption of independent censoring and \n\tincorporating the effect of dependent competing risks. \n\tIt employs a copula-based methodology, specifically the \n\tArchimedean copula, to simulate data, conduct survival \n\tanalysis, and offer comparisons with other methods. \n\tThis approach is detailed in the work of \n\tAdatorwovor et al. (2022) <doi:10.1515/ijb-2021-0016>.  "
  },
  {
    "id": 10794,
    "package_name": "damAOI",
    "title": "Create an 'Area of Interest' Around a Constructed Dam for\nComparative Impact Evaluations",
    "description": "Define a spatial 'Area of Interest' (AOI) around a constructed dam using hydrology data.\n  Dams have environmental and social impacts, both positive and negative.\n  Current analyses of dams have no consistent way to specify at what spatial extent we should evaluate these impacts.\n  'damAOI' implements methods to adjust reservoir polygons to match satellite-observed surface water areas, plot upstream and downstream rivers using elevation data and accumulated river flow, and draw buffers clipped by river basins around reservoirs and relevant rivers. \n  This helps to consistently determine the areas which could be impacted by dam construction, facilitating comparative analysis and informed infrastructure investments.",
    "version": "0.1",
    "maintainer": "Chris Littleboy <chris.littleboy@stir.ac.uk>",
    "author": "Chris Littleboy [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3293-7987>),\n  Isabel Jones [ctb, fnd]",
    "url": "https://github.com/chrislittleboy/damaoi",
    "bug_reports": "https://github.com/chrislittleboy/damaoi/issues",
    "repository": "https://cran.r-project.org/package=damAOI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "damAOI Create an 'Area of Interest' Around a Constructed Dam for\nComparative Impact Evaluations Define a spatial 'Area of Interest' (AOI) around a constructed dam using hydrology data.\n  Dams have environmental and social impacts, both positive and negative.\n  Current analyses of dams have no consistent way to specify at what spatial extent we should evaluate these impacts.\n  'damAOI' implements methods to adjust reservoir polygons to match satellite-observed surface water areas, plot upstream and downstream rivers using elevation data and accumulated river flow, and draw buffers clipped by river basins around reservoirs and relevant rivers. \n  This helps to consistently determine the areas which could be impacted by dam construction, facilitating comparative analysis and informed infrastructure investments.  "
  },
  {
    "id": 10798,
    "package_name": "dani",
    "title": "Design and Analysis of Non-Inferiority Trials",
    "description": "Provides tools to help the design and analysis of resilient non-inferiority trials. These include functions for sample size \n calculations and analyses of trials, with either a risk difference, risk ratio or arc-sine difference margin, and a function to run simulations to \n design a trial with the methods described in Quartagno et al. (2019) <arXiv:1905.00241>.  ",
    "version": "0.1-1",
    "maintainer": "Matteo Quartagno <m.quartagno@ucl.ac.uk>",
    "author": "Matteo Quartagno",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dani",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dani Design and Analysis of Non-Inferiority Trials Provides tools to help the design and analysis of resilient non-inferiority trials. These include functions for sample size \n calculations and analyses of trials, with either a risk difference, risk ratio or arc-sine difference margin, and a function to run simulations to \n design a trial with the methods described in Quartagno et al. (2019) <arXiv:1905.00241>.    "
  },
  {
    "id": 10816,
    "package_name": "data.tree",
    "title": "General Purpose Hierarchical Data Structure",
    "description": "Create tree structures from hierarchical data, and traverse the\n    tree in various orders. Aggregate, cumulate, print, plot, convert to and from\n    data.frame and more. Useful for decision trees, machine learning, finance,\n    conversion from and to JSON, and many other applications.",
    "version": "1.2.0",
    "maintainer": "Christoph Glur <christoph.glur@powerpartners.pro>",
    "author": "Russ Hyde [ctb] (improve dependencies),\n  Chris Hammill [ctb] (improve getting),\n  Facundo Munoz [ctb] (improve list conversion),\n  Markus Wamser [ctb] (fixed some typos),\n  Pierre Formont [ctb] (additional features),\n  Kent Russel [ctb] (documentation),\n  Noam Ross [ctb] (fixes),\n  Duncan Garmonsway [ctb] (fixes),\n  Christoph Glur [aut, cre] (R interface)",
    "url": "https://github.com/gluc/data.tree",
    "bug_reports": "https://github.com/gluc/data.tree/issues",
    "repository": "https://cran.r-project.org/package=data.tree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "data.tree General Purpose Hierarchical Data Structure Create tree structures from hierarchical data, and traverse the\n    tree in various orders. Aggregate, cumulate, print, plot, convert to and from\n    data.frame and more. Useful for decision trees, machine learning, finance,\n    conversion from and to JSON, and many other applications.  "
  },
  {
    "id": 10839,
    "package_name": "dataonderivatives",
    "title": "Easily Source Publicly Available Data on Derivatives",
    "description": "Post Global Financial Crisis derivatives reforms have lifted\n    the veil off over-the-counter (OTC) derivative markets. Swap Execution\n    Facilities (SEFs) and Swap Data Repositories (SDRs) now publish data\n    on swaps that are traded on or reported to those facilities\n    (respectively). This package provides you the ability to get this data\n    from supported sources.",
    "version": "0.4.0",
    "maintainer": "Imanuel Costigan <i.costigan@me.com>",
    "author": "Imanuel Costigan [aut, cre]",
    "url": "https://github.com/imanuelcostigan/dataonderivatives,\nhttp://imanuelcostigan.github.io/dataonderivatives/",
    "bug_reports": "https://github.com/imanuelcostigan/dataonderivatives/issues",
    "repository": "https://cran.r-project.org/package=dataonderivatives",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataonderivatives Easily Source Publicly Available Data on Derivatives Post Global Financial Crisis derivatives reforms have lifted\n    the veil off over-the-counter (OTC) derivative markets. Swap Execution\n    Facilities (SEFs) and Swap Data Repositories (SDRs) now publish data\n    on swaps that are traded on or reported to those facilities\n    (respectively). This package provides you the ability to get this data\n    from supported sources.  "
  },
  {
    "id": 10900,
    "package_name": "dccmidas",
    "title": "DCC Models with GARCH and GARCH-MIDAS Specifications in the\nUnivariate Step, RiskMetrics, Moving Covariance and Scalar and\nDiagonal BEKK Models",
    "description": "Estimates a variety of Dynamic Conditional Correlation (DCC) models. More in detail, the 'dccmidas' package allows the estimation of the corrected DCC (cDCC) of Aielli (2013) <doi:10.1080/07350015.2013.771027>, the DCC-MIDAS of Colacito et al. (2011) <doi:10.1016/j.jeconom.2011.02.013>, the Asymmetric DCC of Cappiello et al. <doi:10.1093/jjfinec/nbl005>, and the Dynamic Equicorrelation (DECO) of Engle and Kelly (2012) <doi:10.1080/07350015.2011.652048>. 'dccmidas' offers the possibility of including standard GARCH <doi:10.1016/0304-4076(86)90063-1>, GARCH-MIDAS <doi:10.1162/REST_a_00300> and Double Asymmetric GARCH-MIDAS <doi:10.1016/j.econmod.2018.07.025> models in the univariate estimation. Moreover, also the scalar and diagonal BEKK <doi:10.1017/S0266466600009063> models can be estimated. Finally, the package calculates also the var-cov matrix under two non-parametric models: the Moving Covariance and the RiskMetrics specifications.",
    "version": "0.1.2",
    "maintainer": "Vincenzo Candila <vcandila@unisa.it>",
    "author": "Vincenzo Candila [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dccmidas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dccmidas DCC Models with GARCH and GARCH-MIDAS Specifications in the\nUnivariate Step, RiskMetrics, Moving Covariance and Scalar and\nDiagonal BEKK Models Estimates a variety of Dynamic Conditional Correlation (DCC) models. More in detail, the 'dccmidas' package allows the estimation of the corrected DCC (cDCC) of Aielli (2013) <doi:10.1080/07350015.2013.771027>, the DCC-MIDAS of Colacito et al. (2011) <doi:10.1016/j.jeconom.2011.02.013>, the Asymmetric DCC of Cappiello et al. <doi:10.1093/jjfinec/nbl005>, and the Dynamic Equicorrelation (DECO) of Engle and Kelly (2012) <doi:10.1080/07350015.2011.652048>. 'dccmidas' offers the possibility of including standard GARCH <doi:10.1016/0304-4076(86)90063-1>, GARCH-MIDAS <doi:10.1162/REST_a_00300> and Double Asymmetric GARCH-MIDAS <doi:10.1016/j.econmod.2018.07.025> models in the univariate estimation. Moreover, also the scalar and diagonal BEKK <doi:10.1017/S0266466600009063> models can be estimated. Finally, the package calculates also the var-cov matrix under two non-parametric models: the Moving Covariance and the RiskMetrics specifications.  "
  },
  {
    "id": 10939,
    "package_name": "debiasedTrialEmulation",
    "title": "Pipeline for Debiased Target Trial Emulation",
    "description": "Supports propensity score-based methods\u2014including matching, stratification, and weighting\u2014for estimating causal treatment effects. It also implements calibration using negative control outcomes to enhance robustness. 'debiasedTrialEmulation' facilitates effect estimation for both binary and time-to-event outcomes, supporting risk ratio (RR), odds ratio (OR), and hazard ratio (HR) as effect measures. It integrates statistical modeling and visualization tools to assess covariate balance, equipoise, and bias calibration. Additional methods\u2014including approaches to address immortal time bias, information bias, selection bias, and informative censoring\u2014are under development. Users interested in these extended features are encouraged to contact the package authors.",
    "version": "0.1.0",
    "maintainer": "Bingyu Zhang <bingyuz7@sas.upenn.edu>",
    "author": "Bingyu Zhang [aut, cre],\n  Yiwen Lu [aut],\n  Dazheng Zhang [aut],\n  Yuqing Lei [aut],\n  Tingyin Wang [aut],\n  Siqi Chen [aut],\n  Yong Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=debiasedTrialEmulation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "debiasedTrialEmulation Pipeline for Debiased Target Trial Emulation Supports propensity score-based methods\u2014including matching, stratification, and weighting\u2014for estimating causal treatment effects. It also implements calibration using negative control outcomes to enhance robustness. 'debiasedTrialEmulation' facilitates effect estimation for both binary and time-to-event outcomes, supporting risk ratio (RR), odds ratio (OR), and hazard ratio (HR) as effect measures. It integrates statistical modeling and visualization tools to assess covariate balance, equipoise, and bias calibration. Additional methods\u2014including approaches to address immortal time bias, information bias, selection bias, and informative censoring\u2014are under development. Users interested in these extended features are encouraged to contact the package authors.  "
  },
  {
    "id": 11037,
    "package_name": "derivmkts",
    "title": "Functions and R Code to Accompany Derivatives Markets",
    "description": "A set of pricing and expository functions that should\n    be useful in teaching a course on financial derivatives.",
    "version": "0.2.5",
    "maintainer": "Robert McDonald <rmcd1024@gmail.com>",
    "author": "Robert McDonald [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=derivmkts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "derivmkts Functions and R Code to Accompany Derivatives Markets A set of pricing and expository functions that should\n    be useful in teaching a course on financial derivatives.  "
  },
  {
    "id": 11201,
    "package_name": "diseasemapping",
    "title": "Modelling Spatial Variation in Disease Risk for Areal Data",
    "description": "Formatting of population and case data, calculation of Standardized\n    Incidence Ratios, and fitting the BYM model using 'INLA'. For details see Brown (2015) <doi:10.18637/jss.v063.i12>. ",
    "version": "2.0.6",
    "maintainer": "Patrick Brown <patrick.brown@utoronto.ca>",
    "author": "Patrick Brown [aut, cre, cph],\n  Lutong Zhou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=diseasemapping",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diseasemapping Modelling Spatial Variation in Disease Risk for Areal Data Formatting of population and case data, calculation of Standardized\n    Incidence Ratios, and fitting the BYM model using 'INLA'. For details see Brown (2015) <doi:10.18637/jss.v063.i12>.   "
  },
  {
    "id": 11212,
    "package_name": "dispositionEffect",
    "title": "Analysis of Disposition Effect on Financial Portfolios",
    "description": "Evaluate the presence of disposition effect and others irrational\n    investor's behaviors based solely on investor's transactions and financial\n    market data. Experimental data can also be used to perform the analysis.  \n    Four different methodologies are implemented to account for the different\n    nature of human behaviors on financial markets.  \n    Novel analyses such as portfolio driven and time series disposition effect\n    are also allowed.",
    "version": "1.0.1",
    "maintainer": "Marco Zanotti <zanottimarco17@gmail.com>",
    "author": "Lorenzo Mazzucchelli [aut],\n  Marco Zanotti [aut, cre]",
    "url": "https://marcozanotti.github.io/dispositionEffect/,\nhttps://github.com/marcozanotti/dispositionEffect",
    "bug_reports": "https://github.com/marcozanotti/dispositionEffect/issues",
    "repository": "https://cran.r-project.org/package=dispositionEffect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dispositionEffect Analysis of Disposition Effect on Financial Portfolios Evaluate the presence of disposition effect and others irrational\n    investor's behaviors based solely on investor's transactions and financial\n    market data. Experimental data can also be used to perform the analysis.  \n    Four different methodologies are implemented to account for the different\n    nature of human behaviors on financial markets.  \n    Novel analyses such as portfolio driven and time series disposition effect\n    are also allowed.  "
  },
  {
    "id": 11221,
    "package_name": "distantia",
    "title": "Advanced Toolset for Efficient Time Series Dissimilarity\nAnalysis",
    "description": "Fast C++ implementation of Dynamic Time Warping for time series dissimilarity analysis, with applications in environmental monitoring and sensor data analysis, climate science, signal processing and pattern recognition, and financial data analysis. Built upon the ideas presented in Benito and Birks (2020) <doi:10.1111/ecog.04895>, provides tools for analyzing time series of varying lengths and structures, including irregular multivariate time series. Key features include individual variable contribution analysis, restricted permutation tests for statistical significance, and imputation of missing data via GAMs. Additionally, the package provides an ample set of tools to prepare and manage time series data.",
    "version": "2.0.2",
    "maintainer": "Blas M. Benito <blasbenito@gmail.com>",
    "author": "Blas M. Benito [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5105-7232>)",
    "url": "https://blasbenito.github.io/distantia/",
    "bug_reports": "https://github.com/BlasBenito/distantia/issues",
    "repository": "https://cran.r-project.org/package=distantia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distantia Advanced Toolset for Efficient Time Series Dissimilarity\nAnalysis Fast C++ implementation of Dynamic Time Warping for time series dissimilarity analysis, with applications in environmental monitoring and sensor data analysis, climate science, signal processing and pattern recognition, and financial data analysis. Built upon the ideas presented in Benito and Birks (2020) <doi:10.1111/ecog.04895>, provides tools for analyzing time series of varying lengths and structures, including irregular multivariate time series. Key features include individual variable contribution analysis, restricted permutation tests for statistical significance, and imputation of missing data via GAMs. Additionally, the package provides an ample set of tools to prepare and manage time series data.  "
  },
  {
    "id": 11224,
    "package_name": "distdichoR",
    "title": "Distributional Method for the Dichotomisation of Continuous\nOutcomes",
    "description": "Contains a range of functions covering the present\n  development of the distributional method for the dichotomisation of continuous outcomes. \n  The method provides estimates with standard error of a comparison of proportions \n  (difference, odds ratio and risk ratio) derived, with similar precision, from a comparison of means. \n  See the URL below or <arXiv:1809.03279> for more information.",
    "version": "0.1-1",
    "maintainer": "Odile Sauzet <odile.sauzet@uni-bielefeld.de>",
    "author": "Odile Sauzet",
    "url": "https://arxiv.org/abs/1809.03279",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=distdichoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distdichoR Distributional Method for the Dichotomisation of Continuous\nOutcomes Contains a range of functions covering the present\n  development of the distributional method for the dichotomisation of continuous outcomes. \n  The method provides estimates with standard error of a comparison of proportions \n  (difference, odds ratio and risk ratio) derived, with similar precision, from a comparison of means. \n  See the URL below or <arXiv:1809.03279> for more information.  "
  },
  {
    "id": 11242,
    "package_name": "div",
    "title": "Report on Diversity and Inclusion in a Corporate Setting",
    "description": "Facilitate the analysis of teams in a corporate setting:\n    assess the diversity per grade and job, present the results,\n    search for bias (in hiring and/or promoting processes).\n    It also provides methods to simulate the effect of bias, random team-data, etc.\n    White paper: 'Philippe J.S. De Brouwer' (2021) <http://www.de-brouwer.com/assets/div/div-white-paper.pdf>.\n    Book (chapter 36): 'Philippe J.S. De Brouwer' (2020, ISBN:978-1-119-63272-6) and 'Philippe J.S. De Brouwer' (2020) <doi:10.1002/9781119632757>.",
    "version": "0.3.1",
    "maintainer": "Philippe J.S. De Brouwer <philippe@de-brouwer.com>",
    "author": "Philippe J.S. De Brouwer [aut, cre]",
    "url": "http://www.de-brouwer.com/div/",
    "bug_reports": "https://github.com/DrPhilippeDB/div/issues/",
    "repository": "https://cran.r-project.org/package=div",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "div Report on Diversity and Inclusion in a Corporate Setting Facilitate the analysis of teams in a corporate setting:\n    assess the diversity per grade and job, present the results,\n    search for bias (in hiring and/or promoting processes).\n    It also provides methods to simulate the effect of bias, random team-data, etc.\n    White paper: 'Philippe J.S. De Brouwer' (2021) <http://www.de-brouwer.com/assets/div/div-white-paper.pdf>.\n    Book (chapter 36): 'Philippe J.S. De Brouwer' (2020, ISBN:978-1-119-63272-6) and 'Philippe J.S. De Brouwer' (2020) <doi:10.1002/9781119632757>.  "
  },
  {
    "id": 11327,
    "package_name": "door",
    "title": "Analysis of Clinical Trials with the Desirability of Outcome\nRanking Methodology",
    "description": "Statistical methods and related graphical representations for the Desirability of Outcome Ranking (DOOR) methodology. The DOOR is a paradigm for the design, analysis, interpretation of clinical trials and other research studies based on the patient centric benefit risk evaluation. The package provides functions for generating summary statistics from individual level/summary level datasets, conduct DOOR probability-based inference, and visualization of the results. For more details of DOOR methodology, see Hamasaki and Evans (2025) <doi:10.1201/9781003390855>. For more explanation of the statistical methods and the graphics, see the technical document and user manual of the DOOR 'Shiny' apps at <https://methods.bsc.gwu.edu>. ",
    "version": "0.0.3",
    "maintainer": "Yijie He <yih148@gwu.edu>",
    "author": "Yijie He [aut, cre],\n  Qihang Wu [ctb],\n  Toshimitsu Hamasaki [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=door",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "door Analysis of Clinical Trials with the Desirability of Outcome\nRanking Methodology Statistical methods and related graphical representations for the Desirability of Outcome Ranking (DOOR) methodology. The DOOR is a paradigm for the design, analysis, interpretation of clinical trials and other research studies based on the patient centric benefit risk evaluation. The package provides functions for generating summary statistics from individual level/summary level datasets, conduct DOOR probability-based inference, and visualization of the results. For more details of DOOR methodology, see Hamasaki and Evans (2025) <doi:10.1201/9781003390855>. For more explanation of the statistical methods and the graphics, see the technical document and user manual of the DOOR 'Shiny' apps at <https://methods.bsc.gwu.edu>.   "
  },
  {
    "id": 11343,
    "package_name": "doublIn",
    "title": "Estimate Incubation or Latency Time using Doubly Interval\nCensored Observations",
    "description": "Visualize contact tracing data using a 'shiny' app and estimate the incubation or latency time of an infectious disease respecting the following characteristics in the analysis; (i) doubly interval censoring with (partly) overlapping or distinct windows; (ii) an infection risk corresponding to exponential growth; (iii) right truncation allowing for individual truncation times; (iv) different choices concerning the family of the distribution. For our earlier work, we refer to Arntzen et al. (2023) <doi:10.1002/sim.9726>. A paper describing our approach in detail will follow.\t\t",
    "version": "0.2.0",
    "maintainer": "Vera Arntzen <v.h.arntzen@math.leidenuniv.nl>",
    "author": "Vera Arntzen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=doublIn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doublIn Estimate Incubation or Latency Time using Doubly Interval\nCensored Observations Visualize contact tracing data using a 'shiny' app and estimate the incubation or latency time of an infectious disease respecting the following characteristics in the analysis; (i) doubly interval censoring with (partly) overlapping or distinct windows; (ii) an infection risk corresponding to exponential growth; (iii) right truncation allowing for individual truncation times; (iv) different choices concerning the family of the distribution. For our earlier work, we refer to Arntzen et al. (2023) <doi:10.1002/sim.9726>. A paper describing our approach in detail will follow.\t\t  "
  },
  {
    "id": 11381,
    "package_name": "dream",
    "title": "Dynamic Relational Event Analysis and Modeling",
    "description": "A set of tools for relational and event analysis, including two- and one-mode network brokerage and structural measures, and helper functions optimized for relational event analysis with large datasets, including creating relational risk sets, computing network statistics, estimating relational event models, and simulating relational event sequences. For more information on relational event models, see Butts (2008) <doi:10.1111/j.1467-9531.2008.00203.x>, Lerner and Lomi (2020) <doi:10.1017/nws.2019.57>, Bianchi et al. (2024) <doi:10.1146/annurev-statistics-040722-060248>, and Butts et al. (2023) <doi:10.1017/nws.2023.9>. In terms of the structural measures in this package, see Leal (2025) <doi:10.1177/00491241251322517>, Burchard and Cornwell (2018) <doi:10.1016/j.socnet.2018.04.001>, and Fujimoto et al. (2018) <doi:10.1017/nws.2018.11>. This package was developed with support from the National Science Foundation\u2019s (NSF) Human Networks and Data Science Program (HNDS) under award number 2241536 (PI: Diego F. Leal). Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.",
    "version": "0.1.0",
    "maintainer": "Kevin A. Carson <kacarson@arizona.edu>",
    "author": "Kevin A. Carson [aut, cre] (ORCID:\n    <https://orcid.org/0009-0001-5762-2586>),\n  Diego F. Leal [aut] (ORCID: <https://orcid.org/0000-0001-9973-4626>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dream",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dream Dynamic Relational Event Analysis and Modeling A set of tools for relational and event analysis, including two- and one-mode network brokerage and structural measures, and helper functions optimized for relational event analysis with large datasets, including creating relational risk sets, computing network statistics, estimating relational event models, and simulating relational event sequences. For more information on relational event models, see Butts (2008) <doi:10.1111/j.1467-9531.2008.00203.x>, Lerner and Lomi (2020) <doi:10.1017/nws.2019.57>, Bianchi et al. (2024) <doi:10.1146/annurev-statistics-040722-060248>, and Butts et al. (2023) <doi:10.1017/nws.2023.9>. In terms of the structural measures in this package, see Leal (2025) <doi:10.1177/00491241251322517>, Burchard and Cornwell (2018) <doi:10.1016/j.socnet.2018.04.001>, and Fujimoto et al. (2018) <doi:10.1017/nws.2018.11>. This package was developed with support from the National Science Foundation\u2019s (NSF) Human Networks and Data Science Program (HNDS) under award number 2241536 (PI: Diego F. Leal). Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.  "
  },
  {
    "id": 11394,
    "package_name": "dropR",
    "title": "Dropout Analysis by Condition",
    "description": "Analysis and visualization of dropout between conditions in surveys and (online) experiments. Features include computation of dropout statistics, comparing dropout between conditions (e.g. Chi square), analyzing survival (e.g. Kaplan-Meier estimation), comparing conditions with the most different rates of dropout (Kolmogorov-Smirnov) and visualizing the result of each in designated plotting functions. Sources: Andrea Frick, Marie-Terese Baechtiger & Ulf-Dietrich Reips (2001) <https://www.researchgate.net/publication/223956222_Financial_incentives_personal_information_and_drop-out_in_online_studies>; Ulf-Dietrich Reips (2002) \"Standards for Internet-Based Experimenting\" <doi:10.1027//1618-3169.49.4.243>.",
    "version": "1.0.3",
    "maintainer": "Annika Tave Overlander <annika-tave.overlander@uni.kn>",
    "author": "Annika Tave Overlander [aut, cre],\n  Matthias Bannert [aut],\n  Ulf-Dietrich Reips [ctb]",
    "url": "https://iscience-kn.github.io/dropR/,\nhttps://github.com/iscience-kn/dropR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dropR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dropR Dropout Analysis by Condition Analysis and visualization of dropout between conditions in surveys and (online) experiments. Features include computation of dropout statistics, comparing dropout between conditions (e.g. Chi square), analyzing survival (e.g. Kaplan-Meier estimation), comparing conditions with the most different rates of dropout (Kolmogorov-Smirnov) and visualizing the result of each in designated plotting functions. Sources: Andrea Frick, Marie-Terese Baechtiger & Ulf-Dietrich Reips (2001) <https://www.researchgate.net/publication/223956222_Financial_incentives_personal_information_and_drop-out_in_online_studies>; Ulf-Dietrich Reips (2002) \"Standards for Internet-Based Experimenting\" <doi:10.1027//1618-3169.49.4.243>.  "
  },
  {
    "id": 11398,
    "package_name": "drought",
    "title": "Statistical Modeling and Assessment of Drought",
    "description": "Provide tools for drought monitoring based on univariate and multivariate drought indicators.Statistical drought prediction based on Ensemble Streamflow Prediction (ESP), drought risk assessments, and drought propagation are also provided. Please see Hao Zengchao et al. (2017) <doi:10.1016/j.envsoft.2017.02.008>.",
    "version": "1.2",
    "maintainer": "Zengchao Hao <z.hao4univ@gmail.com>",
    "author": "Zengchao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=drought",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drought Statistical Modeling and Assessment of Drought Provide tools for drought monitoring based on univariate and multivariate drought indicators.Statistical drought prediction based on Ensemble Streamflow Prediction (ESP), drought risk assessments, and drought propagation are also provided. Please see Hao Zengchao et al. (2017) <doi:10.1016/j.envsoft.2017.02.008>.  "
  },
  {
    "id": 11421,
    "package_name": "dsmmR",
    "title": "Estimation and Simulation of Drifting Semi-Markov Models",
    "description": "Performs parametric and non-parametric estimation and simulation of \n    drifting semi-Markov processes. The definition of parametric and non-parametric\n    model specifications is also possible. Furthermore, three different types of\n    drifting semi-Markov models are considered. These models differ in the number\n    of transition matrices and sojourn time distributions used for the computation\n    of a number of semi-Markov kernels, which in turn characterize the drifting \n    semi-Markov kernel. For the parametric model estimation and specification, \n    several discrete distributions are considered for the sojourn times: Uniform,\n    Poisson, Geometric, Discrete Weibull and Negative Binomial. The non-parametric\n    model specification makes no assumptions about the shape of the sojourn time\n    distributions. Semi-Markov models are described in:\n    Barbu, V.S., Limnios, N. (2008) <doi:10.1007/978-0-387-73173-5>.\n    Drifting Markov models are described in:\n    Vergne, N. (2008) <doi:10.2202/1544-6115.1326>.\n    Reliability indicators of Drifting Markov models are described in:\n    Barbu, V. S., Vergne, N. (2019) <doi:10.1007/s11009-018-9682-8>. \n    We acknowledge the DATALAB Project\n    <https://lmrs-num.math.cnrs.fr/projet-datalab.html> (financed by the\n    European Union with the European Regional Development fund (ERDF) and by\n    the Normandy Region) and the HSMM-INCA Project (financed by the French\n    Agence Nationale de la Recherche (ANR) under grant ANR-21-CE40-0005).",
    "version": "1.0.7",
    "maintainer": "Ioannis Mavrogiannis <mavrogiannis.ioa@gmail.com>",
    "author": "Vlad Stefan Barbu [aut] (ORCID:\n    <https://orcid.org/0000-0002-0840-016X>),\n  Ioannis Mavrogiannis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2948-0648>),\n  Nicolas Vergne [aut]",
    "url": "https://github.com/Mavrogiannis-Ioannis/dsmmR",
    "bug_reports": "https://github.com/Mavrogiannis-Ioannis/dsmmR/issues",
    "repository": "https://cran.r-project.org/package=dsmmR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dsmmR Estimation and Simulation of Drifting Semi-Markov Models Performs parametric and non-parametric estimation and simulation of \n    drifting semi-Markov processes. The definition of parametric and non-parametric\n    model specifications is also possible. Furthermore, three different types of\n    drifting semi-Markov models are considered. These models differ in the number\n    of transition matrices and sojourn time distributions used for the computation\n    of a number of semi-Markov kernels, which in turn characterize the drifting \n    semi-Markov kernel. For the parametric model estimation and specification, \n    several discrete distributions are considered for the sojourn times: Uniform,\n    Poisson, Geometric, Discrete Weibull and Negative Binomial. The non-parametric\n    model specification makes no assumptions about the shape of the sojourn time\n    distributions. Semi-Markov models are described in:\n    Barbu, V.S., Limnios, N. (2008) <doi:10.1007/978-0-387-73173-5>.\n    Drifting Markov models are described in:\n    Vergne, N. (2008) <doi:10.2202/1544-6115.1326>.\n    Reliability indicators of Drifting Markov models are described in:\n    Barbu, V. S., Vergne, N. (2019) <doi:10.1007/s11009-018-9682-8>. \n    We acknowledge the DATALAB Project\n    <https://lmrs-num.math.cnrs.fr/projet-datalab.html> (financed by the\n    European Union with the European Regional Development fund (ERDF) and by\n    the Normandy Region) and the HSMM-INCA Project (financed by the French\n    Agence Nationale de la Recherche (ANR) under grant ANR-21-CE40-0005).  "
  },
  {
    "id": 11624,
    "package_name": "econtools",
    "title": "Enrich and Analyze Sovereign-Level Economic Data",
    "description": "Provides a consistent set of functions for enriching and analyzing \n    sovereign-level economic data. Economists, data scientists, and financial \n    professionals can use the package to add standardized identifiers, \n    demographic and macroeconomic indicators, and derived metrics such as \n    gross domestic product per capita or government expenditure shares.",
    "version": "0.1.0",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://github.com/tidy-intelligence/r-econtools,\nhttps://tidy-intelligence.github.io/r-econtools/",
    "bug_reports": "https://github.com/tidy-intelligence/r-econtools/issues",
    "repository": "https://cran.r-project.org/package=econtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "econtools Enrich and Analyze Sovereign-Level Economic Data Provides a consistent set of functions for enriching and analyzing \n    sovereign-level economic data. Economists, data scientists, and financial \n    professionals can use the package to add standardized identifiers, \n    demographic and macroeconomic indicators, and derived metrics such as \n    gross domestic product per capita or government expenditure shares.  "
  },
  {
    "id": 11630,
    "package_name": "ecorisk",
    "title": "Risk Assessments for Ecosystems or Ecosystem Components",
    "description": "Implementation of a modular framework for ecosystem risk assessments, \n    combining existing risk assessment approaches tailored to semi-quantitative and\n    quantitative analyses. ",
    "version": "0.2.1",
    "maintainer": "Helene Gutte <helenegutte96@gmail.com>",
    "author": "Helene Gutte [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0006-0681-0444>),\n  Saskia A. Otto [aut] (ORCID: <https://orcid.org/0000-0001-7780-1322>)",
    "url": "https://github.com/HeleneGutte/ecorisk",
    "bug_reports": "https://github.com/HeleneGutte/ecorisk/issues",
    "repository": "https://cran.r-project.org/package=ecorisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ecorisk Risk Assessments for Ecosystems or Ecosystem Components Implementation of a modular framework for ecosystem risk assessments, \n    combining existing risk assessment approaches tailored to semi-quantitative and\n    quantitative analyses.   "
  },
  {
    "id": 11659,
    "package_name": "edfinr",
    "title": "Access Tidy Education Finance Data",
    "description": "Provides easy access to tidy education finance data using Bellwether's methodology \n    to combine NCES F-33 Survey, Census Bureau Small Area Income Poverty Estimates (SAIPE),\n    and community data from the ACS 5-Year Estimates. The package simplifies\n    downloading, caching, and filtering education finance data by year and state,\n    enabling researchers and analysts to explore K-12 education funding patterns,\n    revenue sources, expenditure categories, and demographic factors across \n    U.S. school districts.",
    "version": "0.1.1",
    "maintainer": "Alex Spurrier <alex.spurrier@bellwether.org>",
    "author": "Alex Spurrier [aut, cre],\n  Krista Kaput [aut],\n  Michael Chrzan [ctb],\n  Bellwether [cph]",
    "url": "https://github.com/bellwetherorg/edfinr,\nhttps://bellwetherorg.github.io/edfinr/",
    "bug_reports": "https://github.com/bellwetherorg/edfinr/issues",
    "repository": "https://cran.r-project.org/package=edfinr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edfinr Access Tidy Education Finance Data Provides easy access to tidy education finance data using Bellwether's methodology \n    to combine NCES F-33 Survey, Census Bureau Small Area Income Poverty Estimates (SAIPE),\n    and community data from the ACS 5-Year Estimates. The package simplifies\n    downloading, caching, and filtering education finance data by year and state,\n    enabling researchers and analysts to explore K-12 education funding patterns,\n    revenue sources, expenditure categories, and demographic factors across \n    U.S. school districts.  "
  },
  {
    "id": 11666,
    "package_name": "edgedata",
    "title": "Datasets that Support the EDGE Server DIY Logic",
    "description": "Datasets from most recent CCIIO DIY entry\n  in a tidy format. These support the Centers for Medicare and Medicaid\n  Services' (CMS) risk adjustment Do-It-Yourself (DIY) process, which allows\n  health insurance issuers to calculate member risk profiles under the Health\n  and Human Services-Hierarchical Condition Categories (HHS-HCC) regression\n  model. This regression model is used to calculate risk adjustment transfers.\n  Risk adjustment is a selection mitigation program implemented under the\n  Patient Protection and Affordable Care Act (ACA or Obamacare) in the USA.\n  Under the ACA, health insurance issuers submit claims data to CMS\tin order\n  for CMS to calculate a risk score under the HHS-HCC regression model.\n  However, CMS does not inform issuers of their average risk score until after\n  the data submission deadline. These data sets can be used by issuers to\n  calculate their average risk score mid-year. More information about risk\n  adjustment and the HHS-HCC model can be found here:\n  <https://www.cms.gov/mmrr/Articles/A2014/MMRR2014_004_03_a03.html>.",
    "version": "0.2.0",
    "maintainer": "Ethan Brockmann <ethanbrockmann@gmail.com>",
    "author": "Ethan Brockmann [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=edgedata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edgedata Datasets that Support the EDGE Server DIY Logic Datasets from most recent CCIIO DIY entry\n  in a tidy format. These support the Centers for Medicare and Medicaid\n  Services' (CMS) risk adjustment Do-It-Yourself (DIY) process, which allows\n  health insurance issuers to calculate member risk profiles under the Health\n  and Human Services-Hierarchical Condition Categories (HHS-HCC) regression\n  model. This regression model is used to calculate risk adjustment transfers.\n  Risk adjustment is a selection mitigation program implemented under the\n  Patient Protection and Affordable Care Act (ACA or Obamacare) in the USA.\n  Under the ACA, health insurance issuers submit claims data to CMS\tin order\n  for CMS to calculate a risk score under the HHS-HCC regression model.\n  However, CMS does not inform issuers of their average risk score until after\n  the data submission deadline. These data sets can be used by issuers to\n  calculate their average risk score mid-year. More information about risk\n  adjustment and the HHS-HCC model can be found here:\n  <https://www.cms.gov/mmrr/Articles/A2014/MMRR2014_004_03_a03.html>.  "
  },
  {
    "id": 11702,
    "package_name": "eha",
    "title": "Event History Analysis",
    "description": "Parametric proportional hazards fitting with left truncation and\n        right censoring for common families of distributions, piecewise constant \n        hazards, and discrete models. Parametric accelerated failure time models\n        for left truncated and right censored data. Proportional hazards\n        models for tabular and register data. Sampling of risk sets in Cox \n        regression, selections in the Lexis diagram, bootstrapping. \n        Brostr\u00f6m (2022) <doi:10.1201/9780429503764>.",
    "version": "2.11.5",
    "maintainer": "G\u00f6ran Brostr\u00f6m <goran.brostrom@umu.se>",
    "author": "G\u00f6ran Brostr\u00f6m [aut, cre],\n  Jianming Jin [ctb]",
    "url": "https://ehar.se/r/eha/",
    "bug_reports": "https://github.com/goranbrostrom/eha/issues",
    "repository": "https://cran.r-project.org/package=eha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eha Event History Analysis Parametric proportional hazards fitting with left truncation and\n        right censoring for common families of distributions, piecewise constant \n        hazards, and discrete models. Parametric accelerated failure time models\n        for left truncated and right censored data. Proportional hazards\n        models for tabular and register data. Sampling of risk sets in Cox \n        regression, selections in the Lexis diagram, bootstrapping. \n        Brostr\u00f6m (2022) <doi:10.1201/9780429503764>.  "
  },
  {
    "id": 11719,
    "package_name": "eiopaR",
    "title": "Access to RFR (Risk-Free Rate) Curves Produced by the EIOPA",
    "description": "Provides EIOPA (European Insurance And Occupational Pensions Authority) risk-free rates. Please note that the author of this package is not affiliated with EIOPA. The data is accessed through a REST API available at <https://mehdiechchelh.com/api/>.",
    "version": "0.1.1",
    "maintainer": "Mehdi Echchelh <mehdi.echel@gmail.com>",
    "author": "Mehdi Echchelh [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eiopaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eiopaR Access to RFR (Risk-Free Rate) Curves Produced by the EIOPA Provides EIOPA (European Insurance And Occupational Pensions Authority) risk-free rates. Please note that the author of this package is not affiliated with EIOPA. The data is accessed through a REST API available at <https://mehdiechchelh.com/api/>.  "
  },
  {
    "id": 11795,
    "package_name": "energyr",
    "title": "Data Published by the United States Federal Energy Regulatory\nCommission",
    "description": "Data published by the United States Federal Energy Regulatory Commission including\n    electric company financial data, natural gas company financial data, \n    hydropower plant data, liquified natural gas plant data, oil company financial data\n    natural gas company financial data, and natural gas storage field data.",
    "version": "0.2",
    "maintainer": "Paul Govan <paul.govan2@gmail.com>",
    "author": "Paul Govan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1821-8492>)",
    "url": "https://github.com/paulgovan/energyr,\nhttp://paulgovan.github.io/energyr/",
    "bug_reports": "https://github.com/paulgovan/energyr/issues",
    "repository": "https://cran.r-project.org/package=energyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "energyr Data Published by the United States Federal Energy Regulatory\nCommission Data published by the United States Federal Energy Regulatory Commission including\n    electric company financial data, natural gas company financial data, \n    hydropower plant data, liquified natural gas plant data, oil company financial data\n    natural gas company financial data, and natural gas storage field data.  "
  },
  {
    "id": 11817,
    "package_name": "envi",
    "title": "Environmental Interpolation using Spatial Kernel Density\nEstimation",
    "description": "Estimates an ecological niche using occurrence data, covariates, and kernel\n        density-based estimation methods. For a single species with presence and absence data,\n        the 'envi' package uses the spatial relative risk function that is estimated using the\n        'sparr' package. Details about the 'sparr' package methods can be found in the tutorial:\n        Davies et al. (2018) <doi:10.1002/sim.7577>. Details about kernel density estimation can\n        be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>. More information about\n        relative risk functions using kernel density estimation can be found in J. F. Bithell\n        (1991) <doi:10.1002/sim.4780101112>.",
    "version": "1.0.1",
    "maintainer": "Ian D. Buller <ian.buller@alumni.emory.edu>",
    "author": "Ian D. Buller [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9477-8582>),\n  Lance A. Waller [ctb, ths] (ORCID:\n    <https://orcid.org/0000-0001-5002-8886>),\n  Emory University [cph]",
    "url": "https://github.com/lance-waller-lab/envi",
    "bug_reports": "https://github.com/lance-waller-lab/envi/issues",
    "repository": "https://cran.r-project.org/package=envi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "envi Environmental Interpolation using Spatial Kernel Density\nEstimation Estimates an ecological niche using occurrence data, covariates, and kernel\n        density-based estimation methods. For a single species with presence and absence data,\n        the 'envi' package uses the spatial relative risk function that is estimated using the\n        'sparr' package. Details about the 'sparr' package methods can be found in the tutorial:\n        Davies et al. (2018) <doi:10.1002/sim.7577>. Details about kernel density estimation can\n        be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>. More information about\n        relative risk functions using kernel density estimation can be found in J. F. Bithell\n        (1991) <doi:10.1002/sim.4780101112>.  "
  },
  {
    "id": 11829,
    "package_name": "eodhdR2",
    "title": "Official R API for Fetching Data from 'EODHD'",
    "description": "Second and backward-incompatible version of R package 'eodhd' <https://eodhd.com/>, extended with a cache and quota system, \n also offering functions for cleaning and aggregating the financial data. ",
    "version": "0.5.2",
    "maintainer": "Marcelo S. Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo S. Perlin [aut, cre, ctr],\n  Unicorn Data Services [cph]",
    "url": "https://github.com/EodHistoricalData/R-Library-for-financial-data-2024",
    "bug_reports": "https://github.com/EodHistoricalData/R-Library-for-financial-data-2024/issues",
    "repository": "https://cran.r-project.org/package=eodhdR2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eodhdR2 Official R API for Fetching Data from 'EODHD' Second and backward-incompatible version of R package 'eodhd' <https://eodhd.com/>, extended with a cache and quota system, \n also offering functions for cleaning and aggregating the financial data.   "
  },
  {
    "id": 11836,
    "package_name": "epe4md",
    "title": "EPE's 4MD Model to Forecast the Adoption of Distributed\nGeneration",
    "description": "EPE's (Empresa de Pesquisa Energ\u00e9tica) 4MD (Modelo de Mercado da Micro e Minigera\u00e7\u00e3o Distribu\u00edda - Micro and Mini Distributed Generation Market Model) model to forecast the adoption of Distributed Generation. Given the user's assumptions, it is possible to estimate how many consumer units will have distributed generation in Brazil over the next 10 years, for example. In addition, it is possible to estimate the installed capacity, the amount of investments that will be made in the country and the monthly energy contribution of this type of generation. <https://www.epe.gov.br/sites-pt/publicacoes-dados-abertos/publicacoes/PublicacoesArquivos/publicacao-689/topico-639/NT_Metodologia_4MD_PDE_2032_VF.pdf>.",
    "version": "0.1.4",
    "maintainer": "Gabriel Konzen <gabriel.konzen@epe.gov.br>",
    "author": "Gabriel Konzen [aut, cre],\n  Bruno Crotman [aut],\n  Jo\u00e3o Santos [aut],\n  Leticia Minini [aut],\n  Empresa de Pesquisa Energ\u00e9tica [cph, fnd]",
    "url": "https://epe-gov-br.github.io/epe4md/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=epe4md",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epe4md EPE's 4MD Model to Forecast the Adoption of Distributed\nGeneration EPE's (Empresa de Pesquisa Energ\u00e9tica) 4MD (Modelo de Mercado da Micro e Minigera\u00e7\u00e3o Distribu\u00edda - Micro and Mini Distributed Generation Market Model) model to forecast the adoption of Distributed Generation. Given the user's assumptions, it is possible to estimate how many consumer units will have distributed generation in Brazil over the next 10 years, for example. In addition, it is possible to estimate the installed capacity, the amount of investments that will be made in the country and the monthly energy contribution of this type of generation. <https://www.epe.gov.br/sites-pt/publicacoes-dados-abertos/publicacoes/PublicacoesArquivos/publicacao-689/topico-639/NT_Metodologia_4MD_PDE_2032_VF.pdf>.  "
  },
  {
    "id": 11843,
    "package_name": "epidict",
    "title": "Epidemiology Data Dictionaries and Random Data Generators",
    "description": "The 'R4EPIs' project <https://r4epi.github.io/sitrep/> seeks\n    to provide a set of standardized tools for analysis of outbreak and\n    survey data in humanitarian aid settings. This package currently\n    provides standardized data dictionaries from Medecins Sans Frontieres\n    Operational Centre Amsterdam for outbreak scenarios (Acute Jaundice\n    Syndrome, Cholera, Diphtheria, Measles, Meningitis) and surveys\n    (Retrospective mortality and access to care, Malnutrition, Vaccination\n    coverage and Event Based Surveillance) - as described in the following\n    <https://scienceportal.msf.org/assets/standardised-mortality-surveys?utm_source=chatgpt.com>.\n    In addition, a data generator from these dictionaries is provided. It\n    is also possible to read in any Open Data Kit format data dictionary.",
    "version": "0.1.0",
    "maintainer": "Alexander Spina <aspina@appliedepi.org>",
    "author": "Alexander Spina [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8425-1867>),\n  Zhian N. Kamvar [aut] (ORCID: <https://orcid.org/0000-0003-1458-7108>),\n  Lukas Richter [aut],\n  Patrick Keating [aut],\n  Annick Lenglet [ctb],\n  Applied Epi Incorporated [cph],\n  Medecins Sans Frontieres Operational Centre Amsterdam [fnd]",
    "url": "https://github.com/R4EPI/epidict/,\nhttps://r4epi.github.io/epidict/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=epidict",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epidict Epidemiology Data Dictionaries and Random Data Generators The 'R4EPIs' project <https://r4epi.github.io/sitrep/> seeks\n    to provide a set of standardized tools for analysis of outbreak and\n    survey data in humanitarian aid settings. This package currently\n    provides standardized data dictionaries from Medecins Sans Frontieres\n    Operational Centre Amsterdam for outbreak scenarios (Acute Jaundice\n    Syndrome, Cholera, Diphtheria, Measles, Meningitis) and surveys\n    (Retrospective mortality and access to care, Malnutrition, Vaccination\n    coverage and Event Based Surveillance) - as described in the following\n    <https://scienceportal.msf.org/assets/standardised-mortality-surveys?utm_source=chatgpt.com>.\n    In addition, a data generator from these dictionaries is provided. It\n    is also possible to read in any Open Data Kit format data dictionary.  "
  },
  {
    "id": 11855,
    "package_name": "episensr",
    "title": "Basic Sensitivity Analysis of Epidemiological Results",
    "description": "Basic sensitivity analysis of the observed relative risks\n    adjusting for unmeasured confounding and misclassification of the\n    exposure/outcome, or both. It follows the bias analysis methods and\n    examples from the book by Fox M.P., MacLehose R.F., and Lash T.L. \"Applying\n    Quantitative Bias Analysis to Epidemiologic Data, second ed.\", ('Springer', 2021).",
    "version": "2.1.0",
    "maintainer": "Denis Haine <cheval@zaclys.net>",
    "author": "Denis Haine [aut, cre] (ORCID: <https://orcid.org/0000-0002-6691-7335>)",
    "url": "https://codeberg.org/dhaine/episensr,\nhttps://dhaine.codeberg.page/episensr/",
    "bug_reports": "https://codeberg.org/dhaine/episensr/issues",
    "repository": "https://cran.r-project.org/package=episensr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "episensr Basic Sensitivity Analysis of Epidemiological Results Basic sensitivity analysis of the observed relative risks\n    adjusting for unmeasured confounding and misclassification of the\n    exposure/outcome, or both. It follows the bias analysis methods and\n    examples from the book by Fox M.P., MacLehose R.F., and Lash T.L. \"Applying\n    Quantitative Bias Analysis to Epidemiologic Data, second ed.\", ('Springer', 2021).  "
  },
  {
    "id": 11857,
    "package_name": "epitabulate",
    "title": "Tables for Epidemiological Analysis",
    "description": "Produces tables for descriptive epidemiological analysis.\n    These tables include attack rates, case fatality ratios, and mortality\n    rates (with appropriate confidence intervals), with additional\n    functionality to calculate Mantel-Haenszel odds, risk, and incidence\n    rate ratios. The methods implemented follow standard epidemiological\n    approaches described in Rothman et al. (2008, ISBN:978-0-19-513554-2).\n    This package is part of the 'R4EPIs' project <https://R4EPI.github.io/sitrep/>.",
    "version": "0.1.0",
    "maintainer": "Alexander Spina <aspina@appliedepi.org>",
    "author": "Alexander Spina [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8425-1867>),\n  Zhian N. Kamvar [aut] (ORCID: <https://orcid.org/0000-0003-1458-7108>),\n  Amy Gimma [aut],\n  Kate Doyle [ctb],\n  Applied Epi Incorporated [cph],\n  Medecins Sans Frontieres Operational Centre Amsterdam [fnd]",
    "url": "https://R4EPI.github.io/epitabulate/,\nhttps://github.com/R4EPI/epitabulate/",
    "bug_reports": "https://github.com/R4EPI/epitabulate/issues",
    "repository": "https://cran.r-project.org/package=epitabulate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epitabulate Tables for Epidemiological Analysis Produces tables for descriptive epidemiological analysis.\n    These tables include attack rates, case fatality ratios, and mortality\n    rates (with appropriate confidence intervals), with additional\n    functionality to calculate Mantel-Haenszel odds, risk, and incidence\n    rate ratios. The methods implemented follow standard epidemiological\n    approaches described in Rothman et al. (2008, ISBN:978-0-19-513554-2).\n    This package is part of the 'R4EPIs' project <https://R4EPI.github.io/sitrep/>.  "
  },
  {
    "id": 11869,
    "package_name": "epo",
    "title": "Enhanced Portfolio Optimization (EPO)",
    "description": "Implements the Enhanced Portfolio Optimization (EPO) method as \n    described in Pedersen, Babu and Levine (2021)\n    <doi:10.2139/ssrn.3530390>.",
    "version": "0.1.0",
    "maintainer": "Bernardo Reckziegel <bernardo_cse@hotmail.com>",
    "author": "Bernardo Reckziegel [aut, cre, cph]",
    "url": "https://github.com/Reckziegel/epo,\nhttps://reckziegel.github.io/epo/",
    "bug_reports": "https://github.com/Reckziegel/epo/issues",
    "repository": "https://cran.r-project.org/package=epo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epo Enhanced Portfolio Optimization (EPO) Implements the Enhanced Portfolio Optimization (EPO) method as \n    described in Pedersen, Babu and Levine (2021)\n    <doi:10.2139/ssrn.3530390>.  "
  },
  {
    "id": 11898,
    "package_name": "erer",
    "title": "Empirical Research in Economics with R",
    "description": "Several functions, datasets, and sample codes related to empirical research in economics are included. They cover the marginal effects for binary or ordered choice models, static and dynamic Almost Ideal Demand System (AIDS) models, and a typical event analysis in finance.",
    "version": "4.0",
    "maintainer": "Changyou Sun <edwinsun258@gmail.com>",
    "author": "Changyou Sun [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=erer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "erer Empirical Research in Economics with R Several functions, datasets, and sample codes related to empirical research in economics are included. They cover the marginal effects for binary or ordered choice models, static and dynamic Almost Ideal Demand System (AIDS) models, and a typical event analysis in finance.  "
  },
  {
    "id": 11919,
    "package_name": "esback",
    "title": "Expected Shortfall Backtesting",
    "description": "Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.",
    "version": "0.3.1",
    "maintainer": "Sebastian Bayer <sebastian.bayer@uni-konstanz.de>",
    "author": "Sebastian Bayer [aut, cre],\n  Timo Dimitriadis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=esback",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "esback Expected Shortfall Backtesting Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.  "
  },
  {
    "id": 11943,
    "package_name": "estimraw",
    "title": "Estimation of Four-Fold Table Cell Frequencies (Raw Data) from\nEffect Size Measures",
    "description": "Estimation of four-fold table cell frequencies (raw data) from risk ratios (relative risks), risk differences and odds ratios. While raw data can be useful for doing meta-analysis, such data is often not provided by primary studies (with summary statistics being solely presented). Therefore, based on summary statistics (namely, risk ratios, risk differences and odds ratios), this package estimates the value of each cell in a 2x2 table according to the equations described in Di Pietrantonj C (2006) <doi:10.1002/sim.2287>.",
    "version": "1.0.0",
    "maintainer": "Bernardo Sousa-Pinto <bernardo@med.up.pt>",
    "author": "Bernardo Sousa-Pinto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1277-3401>),\n  Carlo Di Pietrantonj [aut] (ORCID:\n    <https://orcid.org/0000-0002-7302-5714>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=estimraw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "estimraw Estimation of Four-Fold Table Cell Frequencies (Raw Data) from\nEffect Size Measures Estimation of four-fold table cell frequencies (raw data) from risk ratios (relative risks), risk differences and odds ratios. While raw data can be useful for doing meta-analysis, such data is often not provided by primary studies (with summary statistics being solely presented). Therefore, based on summary statistics (namely, risk ratios, risk differences and odds ratios), this package estimates the value of each cell in a 2x2 table according to the equations described in Di Pietrantonj C (2006) <doi:10.1002/sim.2287>.  "
  },
  {
    "id": 11954,
    "package_name": "etrader",
    "title": "'ETRADE' API Interface for R",
    "description": "Use R to interface with the 'ETRADE' API <https://developer.etrade.com/home>.\n    Functions include authentication, trading, quote requests, account information, and option \n    chains. A user will need an ETRADE brokerage account and 'ETRADE' API approval. See README \n    for authentication process and examples.",
    "version": "0.1.5",
    "maintainer": "Anthony Balentine <exploringfinance1@gmail.com>",
    "author": "Anthony Balentine [aut, cre]",
    "url": "https://exploringfinance.github.io/etrader/",
    "bug_reports": "https://github.com/exploringfinance/etrader/issues",
    "repository": "https://cran.r-project.org/package=etrader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "etrader 'ETRADE' API Interface for R Use R to interface with the 'ETRADE' API <https://developer.etrade.com/home>.\n    Functions include authentication, trading, quote requests, account information, and option \n    chains. A user will need an ETRADE brokerage account and 'ETRADE' API approval. See README \n    for authentication process and examples.  "
  },
  {
    "id": 11956,
    "package_name": "etrm",
    "title": "Energy Trading and Risk Management",
    "description": "Provides a collection of functions to perform core tasks within\n    Energy Trading and Risk Management (ETRM). Calculation of maximum smoothness \n    forward price curves for electricity and natural gas contracts with flow delivery, as presented in\n    F. E. Benth, S. Koekebakker, and F. Ollmar (2007) <doi:10.3905/jod.2007.694791>\n    and F. E. Benth,  J. S. Benth,  and S. Koekebakker (2008) <doi:10.1142/6811>.\n    Portfolio insurance trading strategies for price risk management in the forward market, see\n    F. Black (1976) <doi:10.1016/0304-405X(76)90024-6>, \n    T. Bjork (2009) <https://EconPapers.repec.org/RePEc:oxp:obooks:9780199574742>,  \n    F. Black and R. W. Jones (1987) <doi:10.3905/jpm.1987.409131> and\n    H. E. Leland (1980) <http://www.jstor.org/stable/2327419>.",
    "version": "1.0.1",
    "maintainer": "Anders D. Sleire <sleire@gmail.com>",
    "author": "Anders D. Sleire",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=etrm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "etrm Energy Trading and Risk Management Provides a collection of functions to perform core tasks within\n    Energy Trading and Risk Management (ETRM). Calculation of maximum smoothness \n    forward price curves for electricity and natural gas contracts with flow delivery, as presented in\n    F. E. Benth, S. Koekebakker, and F. Ollmar (2007) <doi:10.3905/jod.2007.694791>\n    and F. E. Benth,  J. S. Benth,  and S. Koekebakker (2008) <doi:10.1142/6811>.\n    Portfolio insurance trading strategies for price risk management in the forward market, see\n    F. Black (1976) <doi:10.1016/0304-405X(76)90024-6>, \n    T. Bjork (2009) <https://EconPapers.repec.org/RePEc:oxp:obooks:9780199574742>,  \n    F. Black and R. W. Jones (1987) <doi:10.3905/jpm.1987.409131> and\n    H. E. Leland (1980) <http://www.jstor.org/stable/2327419>.  "
  },
  {
    "id": 12013,
    "package_name": "evt0",
    "title": "Mean of Order P, Peaks over Random Threshold Hill and High\nQuantile Estimates",
    "description": "The R package proposes extreme value index estimators for heavy tailed models \n             by mean of order p <DOI:10.1016/j.csda.2012.07.019>, peaks over random threshold\n             <DOI:10.57805/revstat.v4i3.37> and a bias-reduced estimator \n             <DOI:10.1080/00949655.2010.547196>.\n\t     The package also computes moment, generalised Hill <DOI:10.2307/3318416> \n\t     and mixed moment estimates for the extreme value index.\n\t     High quantiles and value at risk estimators based on these estimators are implemented.",
    "version": "1.1.5",
    "maintainer": "Leo Belzile <belzilel@gmail.com>",
    "author": "Leo Belzile [cre] (ORCID: <https://orcid.org/0000-0002-9135-014X>),\n  B. G. Manjunath [aut] (ORCID: <https://orcid.org/0000-0003-2687-0138>),\n  Frederico Caeiro [aut] (ORCID: <https://orcid.org/0000-0001-8628-7281>),\n  Maria Ivette. Gomes [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2903-6993>),\n  Maria Isabel Fraga Alves [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3824-2403>)",
    "url": "",
    "bug_reports": "https://github.com/lbelzile/evt0/issues/",
    "repository": "https://cran.r-project.org/package=evt0",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evt0 Mean of Order P, Peaks over Random Threshold Hill and High\nQuantile Estimates The R package proposes extreme value index estimators for heavy tailed models \n             by mean of order p <DOI:10.1016/j.csda.2012.07.019>, peaks over random threshold\n             <DOI:10.57805/revstat.v4i3.37> and a bias-reduced estimator \n             <DOI:10.1080/00949655.2010.547196>.\n\t     The package also computes moment, generalised Hill <DOI:10.2307/3318416> \n\t     and mixed moment estimates for the extreme value index.\n\t     High quantiles and value at risk estimators based on these estimators are implemented.  "
  },
  {
    "id": 12018,
    "package_name": "exCon",
    "title": "Interactive Exploration of Contour Data",
    "description": "Interactive tools to explore topographic-like data\n    sets.  Such data sets take the form of a matrix in which the rows and\n    columns provide location/frequency information, and the matrix elements\n    contain altitude/response information.  Such data is found in cartography,\n    2D spectroscopy and chemometrics.  The functions in this package create\n    interactive web pages showing the contoured data, possibly with\n    slices from the original matrix parallel to each dimension. The interactive\n    behavior is created using the 'D3.js' 'JavaScript' library by Mike Bostock.",
    "version": "0.2.5",
    "maintainer": "Bryan A. Hanson <hanson@depauw.edu>",
    "author": "Bryan A. Hanson [aut, cre],\n  Kristina R. Mulry [ctb],\n  Mike Bostock [cph, ctb] (author of the d3.js library, http://d3js.org)",
    "url": "https://github.com/bryanhanson/exCon",
    "bug_reports": "https://github.com/bryanhanson/exCon/issues",
    "repository": "https://cran.r-project.org/package=exCon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exCon Interactive Exploration of Contour Data Interactive tools to explore topographic-like data\n    sets.  Such data sets take the form of a matrix in which the rows and\n    columns provide location/frequency information, and the matrix elements\n    contain altitude/response information.  Such data is found in cartography,\n    2D spectroscopy and chemometrics.  The functions in this package create\n    interactive web pages showing the contoured data, possibly with\n    slices from the original matrix parallel to each dimension. The interactive\n    behavior is created using the 'D3.js' 'JavaScript' library by Mike Bostock.  "
  },
  {
    "id": 12076,
    "package_name": "extr",
    "title": "Extinction Risk Estimation",
    "description": "Estimates extinction risk from population time series under a\n    drifted Wiener process using the w-z method for accurate confidence\n    intervals.",
    "version": "1.0.0",
    "maintainer": "Hiroshi Hakoyama <hiroshi.hakoyama@gmail.com>",
    "author": "Hiroshi Hakoyama [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7464-0754>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=extr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extr Extinction Risk Estimation Estimates extinction risk from population time series under a\n    drifted Wiener process using the w-z method for accurate confidence\n    intervals.  "
  },
  {
    "id": 12116,
    "package_name": "fBasics",
    "title": "Rmetrics - Markets and Basic Statistics",
    "description": "Provides a collection of functions to \n    explore and to investigate basic properties of financial returns \n    and related quantities.\n    The covered fields include techniques of explorative data analysis\n    and the investigation of distributional properties, including\n    parameter estimation and hypothesis testing. Even more there are\n    several utility functions for data handling and management.",
    "version": "4052.98",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\n  Tobias Setz [aut],\n  Yohan Chalabi [aut],\n  Martin Maechler [ctb] (ORCID: <https://orcid.org/0000-0002-8685-9910>),\n  CRAN Team [ctb],\n  Georgi N. Boshnakov [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/fBasicsDoc/ (doc),\nhttps://r-forge.r-project.org/scm/viewvc.php/pkg/fBasics/?root=rmetrics\n(devel), https://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/projects/rmetrics",
    "repository": "https://cran.r-project.org/package=fBasics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fBasics Rmetrics - Markets and Basic Statistics Provides a collection of functions to \n    explore and to investigate basic properties of financial returns \n    and related quantities.\n    The covered fields include techniques of explorative data analysis\n    and the investigation of distributional properties, including\n    parameter estimation and hypothesis testing. Even more there are\n    several utility functions for data handling and management.  "
  },
  {
    "id": 12119,
    "package_name": "fCopulae",
    "title": "Rmetrics - Bivariate Dependence Structures with Copulae",
    "description": "Provides a  collection of functions to \n\tmanage, to investigate and to analyze bivariate financial returns by  \n\tCopulae. Included are the families of Archemedean, Elliptical, \n\tExtreme Value, and Empirical Copulae.",
    "version": "4022.85",
    "maintainer": "Paul Smith <paul@waternumbers.co.uk>",
    "author": "Diethelm Wuertz [aut],\n\tTobias Setz [aut],\n\tYohan Chalabi [ctb],\n\tPaul Smith [cre]",
    "url": "https://www.rmetrics.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fCopulae",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fCopulae Rmetrics - Bivariate Dependence Structures with Copulae Provides a  collection of functions to \n\tmanage, to investigate and to analyze bivariate financial returns by  \n\tCopulae. Included are the families of Archemedean, Elliptical, \n\tExtreme Value, and Empirical Copulae.  "
  },
  {
    "id": 12120,
    "package_name": "fDMA",
    "title": "Dynamic Model Averaging and Dynamic Model Selection for\nContinuous Outcomes",
    "description": "Allows to estimate dynamic model averaging, dynamic model selection and median probability model. The original methods are implemented, as well as, selected further modifications of these methods. In particular the user might choose between recursive moment estimation and exponentially moving average for variance updating. Inclusion probabilities might be modified in a way using 'Google Trends'. The code is written in a way which minimises the computational burden (which is quite an obstacle for dynamic model averaging if many variables are used). For example, this package allows for parallel computations and Occam's window approach. The package is designed in a way that is hoped to be especially useful in economics and finance. Main reference: Raftery, A.E., Karny, M., Ettler, P. (2010) <doi:10.1198/TECH.2009.08104>. ",
    "version": "2.2.9",
    "maintainer": "Krzysztof Drachal <kdrachal@wne.uw.edu.pl>",
    "author": "Krzysztof Drachal [aut, cre] (Faculty of Economic Sciences, University\n    of Warsaw, Poland)",
    "url": "https://CRAN.R-project.org/package=fDMA",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fDMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fDMA Dynamic Model Averaging and Dynamic Model Selection for\nContinuous Outcomes Allows to estimate dynamic model averaging, dynamic model selection and median probability model. The original methods are implemented, as well as, selected further modifications of these methods. In particular the user might choose between recursive moment estimation and exponentially moving average for variance updating. Inclusion probabilities might be modified in a way using 'Google Trends'. The code is written in a way which minimises the computational burden (which is quite an obstacle for dynamic model averaging if many variables are used). For example, this package allows for parallel computations and Occam's window approach. The package is designed in a way that is hoped to be especially useful in economics and finance. Main reference: Raftery, A.E., Karny, M., Ettler, P. (2010) <doi:10.1198/TECH.2009.08104>.   "
  },
  {
    "id": 12121,
    "package_name": "fEGarch",
    "title": "SM/LM EGARCH & GARCH, VaR/ES Backtesting & Dual LM Extensions",
    "description": "Implement and fit a variety of short-memory (SM) and long-memory\n  (LM) models from a very broad family of exponential generalized autoregressive\n  conditional heteroskedasticity (EGARCH) models, such as a MEGARCH (modified\n  EGARCH), FIEGARCH (fractionally integrated EGARCH), FIMLog-GARCH (fractionally\n  integrated modulus Log-GARCH), and more. The FIMLog-GARCH as part of the\n  EGARCH family is discussed in Feng et al. (2023)\n  <https://econpapers.repec.org/paper/pdnciepap/156.htm>. For convenience and\n  the purpose of comparison, a variety of other popular SM and LM GARCH-type\n  models, like an APARCH model, a fractionally integrated\n  APARCH (FIAPARCH) model, standard GARCH and fractionally integrated GARCH\n  (FIGARCH) models, GJR-GARCH and FIGJR-GARCH models, TGARCH and FITGARCH\n  models, are implemented as well as dual models with simultaneous modelling of\n  the mean, including dual long-memory models with a fractionally integrated\n  autoregressive moving average (FARIMA) model in the mean and a long-memory\n  model in the variance, and semiparametric volatility model extensions.\n  Parametric models and parametric model parts are fitted through\n  quasi-maximum-likelihood estimation.\n  Furthermore, common forecasting and backtesting functions for value-at-risk\n  (VaR) and expected shortfall (ES) based on the package's models are provided.",
    "version": "1.0.3",
    "maintainer": "Dominik Schulz <dominik.schulz@uni-paderborn.de>",
    "author": "Dominik Schulz [aut, cre] (Paderborn University, Germany),\n  Yuanhua Feng [aut] (Paderborn University, Germany),\n  Christian Peitz [aut] (Financial Intelligence Unit (German Government)),\n  Oliver Kojo Ayensu [aut] (Paderborn University, Germany),\n  Thomas Gries [ctb] (Paderborn University, Germany),\n  Sikandar Siddiqui [ctb] (Deloitte Audit Analytics GmbH, Frankfurt,\n    Germany),\n  Shujie Li [ctb] (Paderborn University, Germany)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fEGarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fEGarch SM/LM EGARCH & GARCH, VaR/ES Backtesting & Dual LM Extensions Implement and fit a variety of short-memory (SM) and long-memory\n  (LM) models from a very broad family of exponential generalized autoregressive\n  conditional heteroskedasticity (EGARCH) models, such as a MEGARCH (modified\n  EGARCH), FIEGARCH (fractionally integrated EGARCH), FIMLog-GARCH (fractionally\n  integrated modulus Log-GARCH), and more. The FIMLog-GARCH as part of the\n  EGARCH family is discussed in Feng et al. (2023)\n  <https://econpapers.repec.org/paper/pdnciepap/156.htm>. For convenience and\n  the purpose of comparison, a variety of other popular SM and LM GARCH-type\n  models, like an APARCH model, a fractionally integrated\n  APARCH (FIAPARCH) model, standard GARCH and fractionally integrated GARCH\n  (FIGARCH) models, GJR-GARCH and FIGJR-GARCH models, TGARCH and FITGARCH\n  models, are implemented as well as dual models with simultaneous modelling of\n  the mean, including dual long-memory models with a fractionally integrated\n  autoregressive moving average (FARIMA) model in the mean and a long-memory\n  model in the variance, and semiparametric volatility model extensions.\n  Parametric models and parametric model parts are fitted through\n  quasi-maximum-likelihood estimation.\n  Furthermore, common forecasting and backtesting functions for value-at-risk\n  (VaR) and expected shortfall (ES) based on the package's models are provided.  "
  },
  {
    "id": 12122,
    "package_name": "fHMM",
    "title": "Fitting Hidden Markov Models to Financial Data",
    "description": "Fitting (hierarchical) hidden Markov models to financial data\n    via maximum likelihood estimation. See Oelschl\u00e4ger, L. and Adam, T.\n    \"Detecting Bearish and Bullish Markets in Financial Time Series Using \n    Hierarchical Hidden Markov Models\" (2021, Statistical Modelling) \n    <doi:10.1177/1471082X211034048> for a reference on the method. A user guide \n    is provided by the accompanying software paper \"fHMM: Hidden Markov Models \n    for Financial Time Series in R\", Oelschl\u00e4ger, L., Adam, T., and Michels, R.\n    (2024, Journal of Statistical Software)  <doi:10.18637/jss.v109.i09>.",
    "version": "1.4.2",
    "maintainer": "Lennart Oelschl\u00e4ger <oelschlaeger.lennart@gmail.com>",
    "author": "Lennart Oelschl\u00e4ger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5421-9313>),\n  Timo Adam [aut] (ORCID: <https://orcid.org/0000-0001-9079-3259>),\n  Rouven Michels [aut] (ORCID: <https://orcid.org/0000-0002-5433-6197>)",
    "url": "https://loelschlaeger.de/fHMM/",
    "bug_reports": "https://github.com/loelschlaeger/fHMM/issues",
    "repository": "https://cran.r-project.org/package=fHMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fHMM Fitting Hidden Markov Models to Financial Data Fitting (hierarchical) hidden Markov models to financial data\n    via maximum likelihood estimation. See Oelschl\u00e4ger, L. and Adam, T.\n    \"Detecting Bearish and Bullish Markets in Financial Time Series Using \n    Hierarchical Hidden Markov Models\" (2021, Statistical Modelling) \n    <doi:10.1177/1471082X211034048> for a reference on the method. A user guide \n    is provided by the accompanying software paper \"fHMM: Hidden Markov Models \n    for Financial Time Series in R\", Oelschl\u00e4ger, L., Adam, T., and Michels, R.\n    (2024, Journal of Statistical Software)  <doi:10.18637/jss.v109.i09>.  "
  },
  {
    "id": 12126,
    "package_name": "fMultivar",
    "title": "Rmetrics - Modeling of Multivariate Financial Return\nDistributions",
    "description": "A collection of functions inspired by Venables and Ripley (2002) <doi:10.1007/978-0-387-21706-2>\n             and Azzalini and Capitanio (1999) <arXiv:0911.2093> to manage, investigate and analyze\n             bivariate and multivariate data sets of financial returns.",
    "version": "4031.84",
    "maintainer": "Stefan Theussl <Stefan.Theussl@R-Project.org>",
    "author": "Diethelm Wuertz [aut],\n  Tobias Setz [aut],\n  Stefan Theussl [aut, cre],\n  Yohan Chalabi [ctb],\n  Martin Maechler [ctb],\n  CRAN team [ctb]",
    "url": "https://www.rmetrics.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fMultivar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fMultivar Rmetrics - Modeling of Multivariate Financial Return\nDistributions A collection of functions inspired by Venables and Ripley (2002) <doi:10.1007/978-0-387-21706-2>\n             and Azzalini and Capitanio (1999) <arXiv:0911.2093> to manage, investigate and analyze\n             bivariate and multivariate data sets of financial returns.  "
  },
  {
    "id": 12128,
    "package_name": "fPASS",
    "title": "Power and Sample Size for Projection Test under Repeated\nMeasures",
    "description": "Computes the power and sample size (PASS) required to test for the\n    difference in the mean function between two groups under a repeatedly measured longitudinal \n    or sparse functional design. See the manuscript by Koner and Luo (2023) <https://salilkoner.github.io/assets/PASS_manuscript.pdf> \n    for details of the PASS formula and computational details. The details of the testing\n    procedure for univariate and multivariate response are presented in\n    Wang (2021) <doi:10.1214/21-EJS1802> and Koner and Luo (2023) \n    <arXiv:2302.05612> respectively. ",
    "version": "1.0.0",
    "maintainer": "Salil Koner <salil.koner@duke.edu>",
    "author": "Salil Koner [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1952-4210>),\n  Sheng Luo [ctb, fnd]",
    "url": "https://github.com/SalilKoner/fPASS",
    "bug_reports": "https://github.com/SalilKoner/fPASS/issues",
    "repository": "https://cran.r-project.org/package=fPASS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fPASS Power and Sample Size for Projection Test under Repeated\nMeasures Computes the power and sample size (PASS) required to test for the\n    difference in the mean function between two groups under a repeatedly measured longitudinal \n    or sparse functional design. See the manuscript by Koner and Luo (2023) <https://salilkoner.github.io/assets/PASS_manuscript.pdf> \n    for details of the PASS formula and computational details. The details of the testing\n    procedure for univariate and multivariate response are presented in\n    Wang (2021) <doi:10.1214/21-EJS1802> and Koner and Luo (2023) \n    <arXiv:2302.05612> respectively.   "
  },
  {
    "id": 12131,
    "package_name": "fTrading",
    "title": "Rmetrics - Trading and Rebalancing Financial Instruments",
    "description": "A collection of functions for trading and rebalancing financial\n\tinstruments. It implements various technical indicators to analyse time series such\n\tas moving averages or stochastic oscillators.",
    "version": "3042.79",
    "maintainer": "Tobias Setz <tobias.setz@live.com>",
    "author": "Diethelm Wuertz [aut],\n\tTobias Setz [cre],\n\tYohan Chalabi [ctb]",
    "url": "http://www.rmetrics.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fTrading",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fTrading Rmetrics - Trading and Rebalancing Financial Instruments A collection of functions for trading and rebalancing financial\n\tinstruments. It implements various technical indicators to analyse time series such\n\tas moving averages or stochastic oscillators.  "
  },
  {
    "id": 12151,
    "package_name": "facmodCS",
    "title": "Cross-Section Factor Models",
    "description": "Linear cross-section factor model fitting with least-squares \n    and robust fitting the 'lmrobdetMM()' function from 'RobStatTM'; related\n    volatility, Value at Risk and Expected Shortfall risk and performance \n    attribution (factor-contributed vs idiosyncratic returns);\n    tabular displays of risk and performance reports;\n    factor model Monte Carlo. The package authors would like to thank Chicago\n    Research on Security Prices,LLC for the cross-section of about 300\n    CRSP stocks data (in the data.table object 'stocksCRSP', and S&P GLOBAL MARKET\n    INTELLIGENCE for contributing 14 factor scores (a.k.a \"alpha factors\".and\n    \"factor exposures\") fundamental data on the 300 companies in the data.table\n    object 'factorSPGMI'.  The 'stocksCRSP' and 'factorsSPGMI' data are not covered by\n    the GPL-2 license, are not provided as open source of any kind, and they are\n    not to be redistributed in any form.",
    "version": "1.0",
    "maintainer": "Mido Shammaa <midoshammaa@yahoo.com>",
    "author": "Mido Shammaa [aut, cre],\n  Doug Martin [ctb, aut],\n  Kirk Li [aut, ctb],\n  Avinash Acharya [ctb],\n  Lingjie Yi [ctb]",
    "url": "https://github.com/robustport/facmodCS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=facmodCS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "facmodCS Cross-Section Factor Models Linear cross-section factor model fitting with least-squares \n    and robust fitting the 'lmrobdetMM()' function from 'RobStatTM'; related\n    volatility, Value at Risk and Expected Shortfall risk and performance \n    attribution (factor-contributed vs idiosyncratic returns);\n    tabular displays of risk and performance reports;\n    factor model Monte Carlo. The package authors would like to thank Chicago\n    Research on Security Prices,LLC for the cross-section of about 300\n    CRSP stocks data (in the data.table object 'stocksCRSP', and S&P GLOBAL MARKET\n    INTELLIGENCE for contributing 14 factor scores (a.k.a \"alpha factors\".and\n    \"factor exposures\") fundamental data on the 300 companies in the data.table\n    object 'factorSPGMI'.  The 'stocksCRSP' and 'factorsSPGMI' data are not covered by\n    the GPL-2 license, are not provided as open source of any kind, and they are\n    not to be redistributed in any form.  "
  },
  {
    "id": 12152,
    "package_name": "facmodTS",
    "title": "Time Series Factor Models for Asset Returns",
    "description": "Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.",
    "version": "1.0",
    "maintainer": "Doug Martin <martinrd3d@gmail.com>",
    "author": "Doug Martin [cre, aut],\n  Eric Zivot [aut],\n  Sangeetha Srinivasan [aut],\n  Avinash Acharya [ctb],\n  Yi-An Chen [ctb],\n  Kirk Li [ctb],\n  Lingjie Yi [ctb],\n  Justin Shea [ctb],\n  Mido Shammaa [ctb],\n  Jon Spinney [ctb]",
    "url": "https://github.com/robustport/facmodTS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=facmodTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "facmodTS Time Series Factor Models for Asset Returns Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.  "
  },
  {
    "id": 12184,
    "package_name": "fam.recrisk",
    "title": "Familial Recurrence Risk",
    "description": "Given vectors of family sizes and number of affecteds per family, calculates the risk of disease recurrence in an unaffected person, conditional on a family having at least k affected members. Methods also model heterogeneity of disease risk across families by fitting a mixture model, allowing for high and low risk families.",
    "version": "0.1",
    "maintainer": "Jason Sinnwell <sinnwell.jason@mayo.edu>",
    "author": "Schaid Daniel [aut],\n  Jason Sinnwell [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fam.recrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fam.recrisk Familial Recurrence Risk Given vectors of family sizes and number of affecteds per family, calculates the risk of disease recurrence in an unaffected person, conditional on a family having at least k affected members. Methods also model heterogeneity of disease risk across families by fitting a mixture model, allowing for high and low risk families.  "
  },
  {
    "id": 12187,
    "package_name": "familiar",
    "title": "End-to-End Automated Machine Learning and Model Evaluation",
    "description": "Single unified interface for end-to-end modelling of regression, \n    categorical and time-to-event (survival) outcomes. Models created using\n    familiar are self-containing, and their use does not require additional\n    information such as baseline survival, feature clustering, or feature\n    transformation and normalisation parameters. Model performance,\n    calibration, risk group stratification, (permutation) variable importance,\n    individual conditional expectation, partial dependence, and more, are\n    assessed automatically as part of the evaluation process and exported in\n    tabular format and plotted, and may also be computed manually using export\n    and plot functions. Where possible, metrics and values obtained during the\n    evaluation process come with confidence intervals.",
    "version": "1.5.0",
    "maintainer": "Alex Zwanenburg <alexander.zwanenburg@nct-dresden.de>",
    "author": "Alex Zwanenburg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0342-9545>),\n  Steffen L\u00f6ck [aut],\n  Stefan Leger [ctb],\n  Iram Shahzadi [ctb],\n  Asier Rabasco Meneghetti [ctb],\n  Sebastian Starke [ctb],\n  Technische Universit\u00e4t Dresden [cph],\n  German Cancer Research Center (DKFZ) [cph]",
    "url": "https://github.com/alexzwanenburg/familiar",
    "bug_reports": "https://github.com/alexzwanenburg/familiar/issues",
    "repository": "https://cran.r-project.org/package=familiar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "familiar End-to-End Automated Machine Learning and Model Evaluation Single unified interface for end-to-end modelling of regression, \n    categorical and time-to-event (survival) outcomes. Models created using\n    familiar are self-containing, and their use does not require additional\n    information such as baseline survival, feature clustering, or feature\n    transformation and normalisation parameters. Model performance,\n    calibration, risk group stratification, (permutation) variable importance,\n    individual conditional expectation, partial dependence, and more, are\n    assessed automatically as part of the evaluation process and exported in\n    tabular format and plotted, and may also be computed manually using export\n    and plot functions. Where possible, metrics and values obtained during the\n    evaluation process come with confidence intervals.  "
  },
  {
    "id": 12235,
    "package_name": "fastcmprsk",
    "title": "Fine-Gray Regression via Forward-Backward Scan",
    "description": "In competing risks regression, the proportional subdistribution hazards (PSH) model is popular for its direct assessment of covariate effects on the cumulative incidence function. This package allows for both penalized and unpenalized PSH regression in linear time using a novel forward-backward scan. Penalties include Ridge, Lease Absolute Shrinkage and Selection Operator (LASSO), Smoothly Clipped Absolute Deviation (SCAD), Minimax Concave Plus (MCP), and elastic net <doi: 10.32614/RJ-2021-010>.",
    "version": "1.24.10",
    "maintainer": "Eric S. Kawaguchi <ekawaguc@usc.edu>",
    "author": "Eric S. Kawaguchi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fastcmprsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastcmprsk Fine-Gray Regression via Forward-Backward Scan In competing risks regression, the proportional subdistribution hazards (PSH) model is popular for its direct assessment of covariate effects on the cumulative incidence function. This package allows for both penalized and unpenalized PSH regression in linear time using a novel forward-backward scan. Penalties include Ridge, Lease Absolute Shrinkage and Selection Operator (LASSO), Smoothly Clipped Absolute Deviation (SCAD), Minimax Concave Plus (MCP), and elastic net <doi: 10.32614/RJ-2021-010>.  "
  },
  {
    "id": 12287,
    "package_name": "fcall",
    "title": "Parse Farm Credit Administration Call Report Data into Tidy Data\nFrames",
    "description": "Parses financial condition and performance data (Call Reports) for\n    institutions in the United States Farm Credit System. Contains functions for\n    downloading files from the Farm Credit Administration (FCA) Call Report\n    archive website and reading the files into tidy data frame format.\n    The archive website can be found at\n    <https://www.fca.gov/bank-oversight/call-report-data-for-download>.",
    "version": "0.1.6",
    "maintainer": "Michael Thomas <mthomas@ketchbrookanalytics.com>",
    "author": "Michael Thomas [aut, cre],\n  Ivan Millanes [aut],\n  Ketchbrook Analytics [cph, fnd]",
    "url": "https://ketchbrookanalytics.github.io/fcall/,\nhttps://github.com/ketchbrookanalytics/fcall",
    "bug_reports": "https://github.com/ketchbrookanalytics/fcall/issues",
    "repository": "https://cran.r-project.org/package=fcall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fcall Parse Farm Credit Administration Call Report Data into Tidy Data\nFrames Parses financial condition and performance data (Call Reports) for\n    institutions in the United States Farm Credit System. Contains functions for\n    downloading files from the Farm Credit Administration (FCA) Call Report\n    archive website and reading the files into tidy data frame format.\n    The archive website can be found at\n    <https://www.fca.gov/bank-oversight/call-report-data-for-download>.  "
  },
  {
    "id": 12291,
    "package_name": "fcl",
    "title": "A Financial Calculator",
    "description": "A financial calculator that provides very fast implementations\n    of common financial indicators using 'Rust' code. It includes functions for\n    bond-related indicators, such as yield to maturity ('YTM'), modified duration,\n    and Macaulay duration, as well as functions for calculating time-weighted\n    and money-weighted rates of return (using 'Modified Dietz' method) for multiple portfolios,\n    given their market values and profit and loss ('PnL') data. 'fcl' is designed\n    to be efficient and accurate for financial analysis and computation. The methods\n    used in this package are based on the following references:\n    <https://en.wikipedia.org/wiki/Modified_Dietz_method>,\n    <https://en.wikipedia.org/wiki/Time-weighted_return>.",
    "version": "0.1.4",
    "maintainer": "Xianying Tan <shrektan@126.com>",
    "author": "Xianying Tan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6072-3521>),\n  Raymon Mina [ctb] (find_root.rs, xirr.rs),\n  The authors of the dependency Rust crates [ctb] (see inst/AUTHORS file\n    for details)",
    "url": "https://github.com/shrektan/fcl, https://shrektan.github.io/fcl/",
    "bug_reports": "https://github.com/shrektan/fcl/issues",
    "repository": "https://cran.r-project.org/package=fcl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fcl A Financial Calculator A financial calculator that provides very fast implementations\n    of common financial indicators using 'Rust' code. It includes functions for\n    bond-related indicators, such as yield to maturity ('YTM'), modified duration,\n    and Macaulay duration, as well as functions for calculating time-weighted\n    and money-weighted rates of return (using 'Modified Dietz' method) for multiple portfolios,\n    given their market values and profit and loss ('PnL') data. 'fcl' is designed\n    to be efficient and accurate for financial analysis and computation. The methods\n    used in this package are based on the following references:\n    <https://en.wikipedia.org/wiki/Modified_Dietz_method>,\n    <https://en.wikipedia.org/wiki/Time-weighted_return>.  "
  },
  {
    "id": 12327,
    "package_name": "featForge",
    "title": "Automated Feature Engineering for Credit Scoring",
    "description": "Automated feature engineering functions tailored for credit scoring. It includes utilities for extracting structured features from timestamps, IP addresses, and email addresses, enabling enhanced predictive modeling for financial risk assessment.",
    "version": "0.1.2",
    "maintainer": "Rudolfs Kregers <rudolfs.kregers@gmail.com>",
    "author": "Rudolfs Kregers [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=featForge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "featForge Automated Feature Engineering for Credit Scoring Automated feature engineering functions tailored for credit scoring. It includes utilities for extracting structured features from timestamps, IP addresses, and email addresses, enabling enhanced predictive modeling for financial risk assessment.  "
  },
  {
    "id": 12335,
    "package_name": "fec16",
    "title": "Data Package for the 2016 United States Federal Elections",
    "description": "Easily analyze relational data from the United States 2016 federal \n    election cycle as reported by the Federal Election Commission.\n    This package contains data about candidates, committees, and a\n    variety of different financial expenditures. Data is from <https://www.fec.gov/data/browse-data/?tab=bulk-data>. ",
    "version": "0.1.6",
    "maintainer": "Marium Tapal <mariumtapal@gmail.com>",
    "author": "Marium Tapal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5093-6462>),\n  Rana Gahwagy [aut],\n  Irene Ryan [aut],\n  Benjamin S. Baumer [aut] (ORCID:\n    <https://orcid.org/0000-0002-3279-0516>)",
    "url": "https://github.com/baumer-lab/fec16",
    "bug_reports": "https://github.com/baumer-lab/fec16/issues",
    "repository": "https://cran.r-project.org/package=fec16",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fec16 Data Package for the 2016 United States Federal Elections Easily analyze relational data from the United States 2016 federal \n    election cycle as reported by the Federal Election Commission.\n    This package contains data about candidates, committees, and a\n    variety of different financial expenditures. Data is from <https://www.fec.gov/data/browse-data/?tab=bulk-data>.   "
  },
  {
    "id": 12341,
    "package_name": "fedz1",
    "title": "An Easier Access to Financial Accounts of the United States(Z.1)",
    "description": "Flow of funds are financial accounts that are provided by Federal Reserve quarterly. The package contains all\n    datasets <https://www.federalreserve.gov/datadownload/Choose.aspx?rel=z1>, tables <https://www.federalreserve.gov/apps/fof/FOFTables.aspx> \n    and descriptions <https://www.federalreserve.gov/apps/fof/Guide/z1_tables_description.pdf> with functions to understand series <https://www.federalreserve.gov/apps/fof/SeriesStructure.aspx> and explore them. ",
    "version": "0.1.0",
    "maintainer": "Hamid Shafiezadeh <shafiezadehhamid@gmail.com>",
    "author": "Hamid Shafiezadeh [aut, cre]",
    "url": "https://github.com/shaf1430/fedz1",
    "bug_reports": "https://github.com/shaf1430/fedz1/issues",
    "repository": "https://cran.r-project.org/package=fedz1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fedz1 An Easier Access to Financial Accounts of the United States(Z.1) Flow of funds are financial accounts that are provided by Federal Reserve quarterly. The package contains all\n    datasets <https://www.federalreserve.gov/datadownload/Choose.aspx?rel=z1>, tables <https://www.federalreserve.gov/apps/fof/FOFTables.aspx> \n    and descriptions <https://www.federalreserve.gov/apps/fof/Guide/z1_tables_description.pdf> with functions to understand series <https://www.federalreserve.gov/apps/fof/SeriesStructure.aspx> and explore them.   "
  },
  {
    "id": 12356,
    "package_name": "ffiec",
    "title": "R Interface to 'FFIEC Central Data Repository REST API' Service",
    "description": "Provides a simplified interface to the Central Data Repository 'REST\n    API' service made available by the United States Federal Financial\n    Institutions Examination Council ('FFIEC'). Contains functions to retrieve\n    reports of Condition and Income (Call Reports) and Uniform Bank Performance\n    Reports ('UBPR') in list or tidy data frame format for most 'FDIC' insured\n    institutions. See\n    <https://cdr.ffiec.gov/public/Files/SIS611_-_Retrieve_Public_Data_via_Web_Service.pdf>\n    for the official 'REST API' documentation published by the 'FFIEC'.",
    "version": "0.1.3",
    "maintainer": "Michael Thomas <mthomas@ketchbrookanalytics.com>",
    "author": "Michael Thomas [aut, cre],\n  Ketchbrook Analytics [cph, fnd]",
    "url": "https://github.com/ketchbrookanalytics/ffiec,\nhttps://ketchbrookanalytics.github.io/ffiec/",
    "bug_reports": "https://github.com/ketchbrookanalytics/ffiec/issues",
    "repository": "https://cran.r-project.org/package=ffiec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffiec R Interface to 'FFIEC Central Data Repository REST API' Service Provides a simplified interface to the Central Data Repository 'REST\n    API' service made available by the United States Federal Financial\n    Institutions Examination Council ('FFIEC'). Contains functions to retrieve\n    reports of Condition and Income (Call Reports) and Uniform Bank Performance\n    Reports ('UBPR') in list or tidy data frame format for most 'FDIC' insured\n    institutions. See\n    <https://cdr.ffiec.gov/public/Files/SIS611_-_Retrieve_Public_Data_via_Web_Service.pdf>\n    for the official 'REST API' documentation published by the 'FFIEC'.  "
  },
  {
    "id": 12359,
    "package_name": "ffp",
    "title": "Fully Flexible Probabilities for Stress Testing and Portfolio\nConstruction",
    "description": "Implements numerical entropy-pooling for portfolio construction and \n    scenario analysis as described in Meucci, Attilio (2008) and Meucci, Attilio (2010) \n    <doi:10.2139/ssrn.1696802>.",
    "version": "0.2.2",
    "maintainer": "Bernardo Reckziegel <bernardo_cse@hotmail.com>",
    "author": "Bernardo Reckziegel [aut, cre]",
    "url": "https://github.com/Reckziegel/FFP",
    "bug_reports": "https://github.com/Reckziegel/FFP/issues",
    "repository": "https://cran.r-project.org/package=ffp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffp Fully Flexible Probabilities for Stress Testing and Portfolio\nConstruction Implements numerical entropy-pooling for portfolio construction and \n    scenario analysis as described in Meucci, Attilio (2008) and Meucci, Attilio (2010) \n    <doi:10.2139/ssrn.1696802>.  "
  },
  {
    "id": 12381,
    "package_name": "figir",
    "title": "Check Validity of FIGI, CUSIP, ISIN, SEDOL",
    "description": "With the functions in this package you can check the\n  validity of the following financial instrument identifiers:\n  FIGI (Financial Instrument Global Identifier\n  <https://www.openfigi.com/about/figi>),\n  CUSIP (Committee on Uniform Security Identification Procedures\n  <https://www.cusip.com/identifiers.html#/CUSIP>),\n  ISIN (International Securities Identification Number\n  <https://www.cusip.com/identifiers.html#/ISIN>),\n  SEDOL (Stock Exchange Daily Official List\n  <https://www2.lseg.com/SEDOL-masterfile-service-tech-guide-v8.6>).\n  You can also calculate the FIGI checksum of 11-character strings,\n  which can be useful if you want to create your own FIGI identifiers.",
    "version": "0.1.7.0",
    "maintainer": "Panagiotis Cheilaris <philaris@gmail.com>",
    "author": "Panagiotis Cheilaris [aut, cre]",
    "url": "https://github.com/philaris/figir",
    "bug_reports": "https://github.com/philaris/figir/issues",
    "repository": "https://cran.r-project.org/package=figir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "figir Check Validity of FIGI, CUSIP, ISIN, SEDOL With the functions in this package you can check the\n  validity of the following financial instrument identifiers:\n  FIGI (Financial Instrument Global Identifier\n  <https://www.openfigi.com/about/figi>),\n  CUSIP (Committee on Uniform Security Identification Procedures\n  <https://www.cusip.com/identifiers.html#/CUSIP>),\n  ISIN (International Securities Identification Number\n  <https://www.cusip.com/identifiers.html#/ISIN>),\n  SEDOL (Stock Exchange Daily Official List\n  <https://www2.lseg.com/SEDOL-masterfile-service-tech-guide-v8.6>).\n  You can also calculate the FIGI checksum of 11-character strings,\n  which can be useful if you want to create your own FIGI identifiers.  "
  },
  {
    "id": 12413,
    "package_name": "finiteruinprob",
    "title": "Computation of the Probability of Ruin Within a Finite Time\nHorizon",
    "description": "In the Cram\u00e9r\u2013Lundberg risk process perturbed by a Wiener\n    process, this packages provides approximations to the probability of\n    ruin within a finite time horizon.  Currently, there are three methods\n    implemented: The first one uses saddlepoint approximation (two\n    variants are provided), the second one uses importance sampling and\n    the third one is based on the simulation of a dual process.  This last\n    method is not very accurate and only given here for completeness.",
    "version": "0.6",
    "maintainer": "Benjamin Baumgartner <benjamin@baumgrt.com>",
    "author": "Benjamin Baumgartner [aut, cre],\n  Riccardo Gatto [ctb, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=finiteruinprob",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "finiteruinprob Computation of the Probability of Ruin Within a Finite Time\nHorizon In the Cram\u00e9r\u2013Lundberg risk process perturbed by a Wiener\n    process, this packages provides approximations to the probability of\n    ruin within a finite time horizon.  Currently, there are three methods\n    implemented: The first one uses saddlepoint approximation (two\n    variants are provided), the second one uses importance sampling and\n    the third one is based on the simulation of a dual process.  This last\n    method is not very accurate and only given here for completeness.  "
  },
  {
    "id": 12418,
    "package_name": "finnts",
    "title": "Microsoft Finance Time Series Forecasting Framework",
    "description": "Automated time series forecasting developed by Microsoft Finance. The Microsoft Finance Time \n    Series Forecasting Framework, aka Finn, can be used to forecast any component of the income \n    statement, balance sheet, or any other area of interest by finance. Any numerical quantity over time, \n    Finn can be used to forecast it. While it can be applied outside of the finance domain, Finn was built \n    to meet the needs of financial analysts to better forecast their businesses within a company, and has \n    a lot of built in features that are specific to the needs of financial forecasters. Happy forecasting!",
    "version": "0.6.0",
    "maintainer": "Mike Tokic <mftokic@gmail.com>",
    "author": "Mike Tokic [aut, cre] (ORCID: <https://orcid.org/0000-0002-7630-7055>),\n  Aadharsh Kannan [aut] (ORCID: <https://orcid.org/0000-0002-6475-8211>)",
    "url": "https://microsoft.github.io/finnts/,\nhttps://github.com/microsoft/finnts",
    "bug_reports": "https://github.com/microsoft/finnts/issues",
    "repository": "https://cran.r-project.org/package=finnts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "finnts Microsoft Finance Time Series Forecasting Framework Automated time series forecasting developed by Microsoft Finance. The Microsoft Finance Time \n    Series Forecasting Framework, aka Finn, can be used to forecast any component of the income \n    statement, balance sheet, or any other area of interest by finance. Any numerical quantity over time, \n    Finn can be used to forecast it. While it can be applied outside of the finance domain, Finn was built \n    to meet the needs of financial analysts to better forecast their businesses within a company, and has \n    a lot of built in features that are specific to the needs of financial forecasters. Happy forecasting!  "
  },
  {
    "id": 12433,
    "package_name": "fishboot",
    "title": "Bootstrap-Based Methods for the Study of Fish Stocks and Aquatic\nPopulations",
    "description": "A suite of bootstrap-based models and tools for analyzing fish \n  stocks and aquatic populations. Designed for ecologists and fisheries \n  scientists, it supports data from length-frequency distributions, \n  tag-and-recapture studies, and hard structure readings (e.g., otoliths). \n  See Schwamborn et al., 2019 <doi:10.1016/j.ecolmodel.2018.12.001> \n  for background. The package includes functions for bootstrapped fitting of \n  growth curves and plotting. ",
    "version": "1.0.2",
    "maintainer": "Ralf Schwamborn <ralf.schwamborn@ufpe.br>",
    "author": "Ralf Schwamborn [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9150-8720>),\n  Tobias K. Mildenberger [aut],\n  Marc H. Taylor [aut],\n  Margit Wilhelm [aut] (ORCID: <https://orcid.org/0000-0001-9271-2109>),\n  Wencheng Lau-Medrano [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7979-9710>)",
    "url": "https://github.com/rschwamborn/fishboot",
    "bug_reports": "https://github.com/rschwamborn/fishboot/issues",
    "repository": "https://cran.r-project.org/package=fishboot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fishboot Bootstrap-Based Methods for the Study of Fish Stocks and Aquatic\nPopulations A suite of bootstrap-based models and tools for analyzing fish \n  stocks and aquatic populations. Designed for ecologists and fisheries \n  scientists, it supports data from length-frequency distributions, \n  tag-and-recapture studies, and hard structure readings (e.g., otoliths). \n  See Schwamborn et al., 2019 <doi:10.1016/j.ecolmodel.2018.12.001> \n  for background. The package includes functions for bootstrapped fitting of \n  growth curves and plotting.   "
  },
  {
    "id": 12438,
    "package_name": "fishmethods",
    "title": "Fishery Science Methods and Models",
    "description": "Functions for applying a wide range of fisheries stock assessment methods.",
    "version": "1.13-1",
    "maintainer": "Gary A. Nelson <gary.nelson@mass.gov>",
    "author": "Gary A. Nelson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fishmethods",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fishmethods Fishery Science Methods and Models Functions for applying a wide range of fisheries stock assessment methods.  "
  },
  {
    "id": 12452,
    "package_name": "fitdistcp",
    "title": "Distribution Fitting with Calibrating Priors for Commonly Used\nDistributions",
    "description": "Generates predictive distributions based on calibrating priors\n    for various commonly used statistical models, including models with \n    predictors. Routines for densities, probabilities, quantiles, random\n    deviates and the parameter posterior are provided. The predictions are\n    generated from the Bayesian prediction integral, with priors chosen to\n    give good reliability (also known as calibration). For homogeneous models,\n    the prior is set to the right Haar prior, giving predictions which are\n    exactly reliable. As a result, in repeated testing, the frequencies of\n    out-of-sample outcomes and the probabilities from the predictions agree.\n    For other models, the prior is chosen to give good reliability. Where\n    possible, the Bayesian prediction integral is solved exactly. Where exact\n    solutions are not possible, the Bayesian prediction integral is solved\n    using the Datta-Mukerjee-Ghosh-Sweeting (DMGS) asymptotic expansion.\n    Optionally, the prediction integral can also be solved using posterior\n    samples generated using Paul Northrop's ratio of uniforms sampling package\n    ('rust'). Results are also generated based on maximum likelihood, for\n    comparison purposes. Various model selection diagnostics and testing\n    routines are included. Based on \"Reducing reliability bias in assessments\n    of extreme weather risk using calibrating priors\", Jewson, S., Sweeting, T.\n    and Jewson, L. (2024); <doi:10.5194/ascmo-11-1-2025>.",
    "version": "0.2.3",
    "maintainer": "Stephen Jewson <stephen.jewson@gmail.com>",
    "author": "Stephen Jewson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6011-6262>)",
    "url": "https://www.fitdistcp.info",
    "bug_reports": "https://github.com/stephenjewson/fitdistcp/issues",
    "repository": "https://cran.r-project.org/package=fitdistcp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fitdistcp Distribution Fitting with Calibrating Priors for Commonly Used\nDistributions Generates predictive distributions based on calibrating priors\n    for various commonly used statistical models, including models with \n    predictors. Routines for densities, probabilities, quantiles, random\n    deviates and the parameter posterior are provided. The predictions are\n    generated from the Bayesian prediction integral, with priors chosen to\n    give good reliability (also known as calibration). For homogeneous models,\n    the prior is set to the right Haar prior, giving predictions which are\n    exactly reliable. As a result, in repeated testing, the frequencies of\n    out-of-sample outcomes and the probabilities from the predictions agree.\n    For other models, the prior is chosen to give good reliability. Where\n    possible, the Bayesian prediction integral is solved exactly. Where exact\n    solutions are not possible, the Bayesian prediction integral is solved\n    using the Datta-Mukerjee-Ghosh-Sweeting (DMGS) asymptotic expansion.\n    Optionally, the prediction integral can also be solved using posterior\n    samples generated using Paul Northrop's ratio of uniforms sampling package\n    ('rust'). Results are also generated based on maximum likelihood, for\n    comparison purposes. Various model selection diagnostics and testing\n    routines are included. Based on \"Reducing reliability bias in assessments\n    of extreme weather risk using calibrating priors\", Jewson, S., Sweeting, T.\n    and Jewson, L. (2024); <doi:10.5194/ascmo-11-1-2025>.  "
  },
  {
    "id": 12552,
    "package_name": "fmpapi",
    "title": "Flexible Client for the 'Financial Modeling Prep' API",
    "description": "Provides a flexible interface to the 'Financial Modeling Prep' API \n    <https://site.financialmodelingprep.com/developer/docs>. The package \n    supports all available endpoints and parameters, enabling R users \n    to interact with a wide range of financial data.",
    "version": "1.0.0",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://github.com/tidy-finance/r-fmpapi,\nhttps://tidy-finance.github.io/r-fmpapi/",
    "bug_reports": "https://github.com/tidy-finance/r-fmpapi/issues",
    "repository": "https://cran.r-project.org/package=fmpapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fmpapi Flexible Client for the 'Financial Modeling Prep' API Provides a flexible interface to the 'Financial Modeling Prep' API \n    <https://site.financialmodelingprep.com/developer/docs>. The package \n    supports all available endpoints and parameters, enabling R users \n    to interact with a wide range of financial data.  "
  },
  {
    "id": 12553,
    "package_name": "fmpcloudr",
    "title": "R Access to the 'FMP Cloud' and 'Financial Modeling Prep' API",
    "description": "Use R to access to the 'FMP Cloud' API <https://fmpcloud.io/> and \n    'Financial Modeling Prep' API <https://financialmodelingprep.com/developer/docs/>.\n    Data available includes stock prices, market indexes, company fundamentals,\n    13F holdings data, and much more. A valid API token must be set to enable\n    functions. ",
    "version": "0.1.5",
    "maintainer": "Anthony Balentine <exploringfinance1@gmail.com>",
    "author": "Anthony Balentine [aut, cre]",
    "url": "https://exploringfinance.github.io/fmpcloudr/",
    "bug_reports": "https://github.com/exploringfinance/fmpcloudr/issues",
    "repository": "https://cran.r-project.org/package=fmpcloudr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fmpcloudr R Access to the 'FMP Cloud' and 'Financial Modeling Prep' API Use R to access to the 'FMP Cloud' API <https://fmpcloud.io/> and \n    'Financial Modeling Prep' API <https://financialmodelingprep.com/developer/docs/>.\n    Data available includes stock prices, market indexes, company fundamentals,\n    13F holdings data, and much more. A valid API token must be set to enable\n    functions.   "
  },
  {
    "id": 12671,
    "package_name": "frailtyHL",
    "title": "Frailty Models via Hierarchical Likelihood",
    "description": "Implements the h-likelihood estimation procedures for general frailty models including competing-risk models and joint models.",
    "version": "2.3",
    "maintainer": "Maengseok Noh <msnoh@pknu.ac.kr>",
    "author": "Il Do Ha, Maengseok Noh, Jiwoong Kim, Youngjo Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=frailtyHL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "frailtyHL Frailty Models via Hierarchical Likelihood Implements the h-likelihood estimation procedures for general frailty models including competing-risk models and joint models.  "
  },
  {
    "id": 12675,
    "package_name": "frailtypack",
    "title": "Shared, Joint (Generalized) Frailty Models; Surrogate Endpoints",
    "description": "The following several classes of frailty models using a\n    penalized likelihood estimation on the hazard function but also a\n    parametric estimation can be fit using this R package: 1) A shared\n    frailty model (with gamma or log-normal frailty distribution) and Cox\n    proportional hazard model. Clustered and recurrent survival times can\n    be studied.  2) Additive frailty models for proportional hazard models\n    with two correlated random effects (intercept random effect with\n    random slope).  3) Nested frailty models for hierarchically clustered\n    data (with 2 levels of clustering) by including two iid gamma random\n    effects.  4) Joint frailty models in the context of the joint\n    modelling for recurrent events with terminal event for clustered data\n    or not. A joint frailty model for two semi-competing risks and\n    clustered data is also proposed.  5) Joint general frailty models in\n    the context of the joint modelling for recurrent events with terminal\n    event data with two independent frailty terms.  6) Joint Nested\n    frailty models in the context of the joint modelling for recurrent\n    events with terminal event, for hierarchically clustered data (with\n    two levels of clustering) by including two iid gamma random effects.\n    7) Multivariate joint frailty models for two types of recurrent events\n    and a terminal event.  8) Joint models for longitudinal data and a\n    terminal event.  9) Trivariate joint models for longitudinal data,\n    recurrent events and a terminal event.  10) Joint frailty models for\n    the validation of surrogate endpoints in multiple randomized clinical\n    trials with failure-time and/or longitudinal endpoints with the\n    possibility to use a mediation analysis model.  11) Conditional and\n    Marginal two-part joint models for longitudinal semicontinuous data\n    and a terminal event.  12) Joint frailty-copula models for the\n    validation of surrogate endpoints in multiple randomized clinical\n    trials with failure-time endpoints.  13) Generalized shared and joint\n    frailty models for recurrent and terminal events. Proportional hazards\n    (PH), additive hazard (AH), proportional odds (PO) and probit models\n    are available in a fully parametric framework. For PH and AH models,\n    it is possible to consider type-varying coefficients and flexible\n    semiparametric hazard function.  Prediction values are available (for\n    a terminal event or for a new recurrent event). Left-truncated (not\n    for Joint model), right-censored data, interval-censored data (only\n    for Cox proportional hazard and shared frailty model) and strata are\n    allowed. In each model, the random effects have the gamma or normal\n    distribution. Now, you can also consider time-varying covariates\n    effects in Cox, shared and joint frailty models (1-5). The package\n    includes concordance measures for Cox proportional hazards models and\n    for shared frailty models.  14) Competing Joint Frailty Model: A\n    single type of recurrent event and two terminal events.  15) functions \n    to compute power and sample size for four Gamma-frailty-based designs: \n    Shared Frailty Models, Nested Frailty Models, Joint Frailty Models, and \n    General Joint Frailty Models. Each design includes two primary functions: a \n    power function, which computes power given a specified sample size; \n    and a sample size function, which computes the required sample size to achieve \n    a specified power. 16) Weibull Illness-Death model with or without shared frailty\n    between transitions. Left-truncated and right-censored data are allowed. \n    17) Weibull Competing risks model with or without shared frailty between the \n    transitions. Left-truncated and right-censored data are allowed. Moreover, the package can be used with its shiny\n    application, in a local mode or by following the link below.",
    "version": "3.8.0",
    "maintainer": "Virginie Rondeau <virginie.rondeau@u-bordeaux.fr>",
    "author": "Virginie Rondeau [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7109-4831>),\n  Juan R. Gonzalez [aut],\n  Yassin Mazroui [aut],\n  Audrey Mauguen [aut],\n  Amadou Diakite [aut],\n  Alexandre Laurent [aut],\n  Myriam Lopez [aut],\n  Agnieszka Krol [aut],\n  Casimir L. Sofeu [aut],\n  Julien Dumerc [aut],\n  Denis Rustand [aut],\n  Jocelyn Chauvet [aut],\n  Quentin Le Coent [aut],\n  Romain Pierlot [aut],\n  Lacey Etzkorn [aut],\n  Derek Dinart [aut],\n  Adrien Oru\u00e9 [aut],\n  Ayoub Bifenzi [aut],\n  Viviane Philipps [aut],\n  David Hill [cph],\n  John Burkardt [cph],\n  Alan Genz [cph],\n  Ashwith J. Rego [cph]",
    "url": "https://virginie1rondeau.wixsite.com/virginierondeau/software-frailtypack\nhttps://frailtypack-pkg.shinyapps.io/shiny_frailtypack/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=frailtypack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "frailtypack Shared, Joint (Generalized) Frailty Models; Surrogate Endpoints The following several classes of frailty models using a\n    penalized likelihood estimation on the hazard function but also a\n    parametric estimation can be fit using this R package: 1) A shared\n    frailty model (with gamma or log-normal frailty distribution) and Cox\n    proportional hazard model. Clustered and recurrent survival times can\n    be studied.  2) Additive frailty models for proportional hazard models\n    with two correlated random effects (intercept random effect with\n    random slope).  3) Nested frailty models for hierarchically clustered\n    data (with 2 levels of clustering) by including two iid gamma random\n    effects.  4) Joint frailty models in the context of the joint\n    modelling for recurrent events with terminal event for clustered data\n    or not. A joint frailty model for two semi-competing risks and\n    clustered data is also proposed.  5) Joint general frailty models in\n    the context of the joint modelling for recurrent events with terminal\n    event data with two independent frailty terms.  6) Joint Nested\n    frailty models in the context of the joint modelling for recurrent\n    events with terminal event, for hierarchically clustered data (with\n    two levels of clustering) by including two iid gamma random effects.\n    7) Multivariate joint frailty models for two types of recurrent events\n    and a terminal event.  8) Joint models for longitudinal data and a\n    terminal event.  9) Trivariate joint models for longitudinal data,\n    recurrent events and a terminal event.  10) Joint frailty models for\n    the validation of surrogate endpoints in multiple randomized clinical\n    trials with failure-time and/or longitudinal endpoints with the\n    possibility to use a mediation analysis model.  11) Conditional and\n    Marginal two-part joint models for longitudinal semicontinuous data\n    and a terminal event.  12) Joint frailty-copula models for the\n    validation of surrogate endpoints in multiple randomized clinical\n    trials with failure-time endpoints.  13) Generalized shared and joint\n    frailty models for recurrent and terminal events. Proportional hazards\n    (PH), additive hazard (AH), proportional odds (PO) and probit models\n    are available in a fully parametric framework. For PH and AH models,\n    it is possible to consider type-varying coefficients and flexible\n    semiparametric hazard function.  Prediction values are available (for\n    a terminal event or for a new recurrent event). Left-truncated (not\n    for Joint model), right-censored data, interval-censored data (only\n    for Cox proportional hazard and shared frailty model) and strata are\n    allowed. In each model, the random effects have the gamma or normal\n    distribution. Now, you can also consider time-varying covariates\n    effects in Cox, shared and joint frailty models (1-5). The package\n    includes concordance measures for Cox proportional hazards models and\n    for shared frailty models.  14) Competing Joint Frailty Model: A\n    single type of recurrent event and two terminal events.  15) functions \n    to compute power and sample size for four Gamma-frailty-based designs: \n    Shared Frailty Models, Nested Frailty Models, Joint Frailty Models, and \n    General Joint Frailty Models. Each design includes two primary functions: a \n    power function, which computes power given a specified sample size; \n    and a sample size function, which computes the required sample size to achieve \n    a specified power. 16) Weibull Illness-Death model with or without shared frailty\n    between transitions. Left-truncated and right-censored data are allowed. \n    17) Weibull Competing risks model with or without shared frailty between the \n    transitions. Left-truncated and right-censored data are allowed. Moreover, the package can be used with its shiny\n    application, in a local mode or by following the link below.  "
  },
  {
    "id": 12694,
    "package_name": "frenchdata",
    "title": "Download Data Sets from Kenneth's French Finance Data Library\nSite",
    "description": "Download data sets from Kenneth's French finance data library site <http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html>, reads all the data subsets from the file. Allows R users to collect the data as\n    'tidyverse'-ready data frames.",
    "version": "0.2.0",
    "maintainer": "Nelson Areal <nareal@gmail.com>",
    "author": "Nelson Areal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1157-0178>)",
    "url": "https://nareal.github.io/frenchdata/,\nhttps://github.com/nareal/frenchdata",
    "bug_reports": "https://github.com/nareal/frenchdata/issues",
    "repository": "https://cran.r-project.org/package=frenchdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "frenchdata Download Data Sets from Kenneth's French Finance Data Library\nSite Download data sets from Kenneth's French finance data library site <http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html>, reads all the data subsets from the file. Allows R users to collect the data as\n    'tidyverse'-ready data frames.  "
  },
  {
    "id": 12718,
    "package_name": "fruclimadapt",
    "title": "Evaluation Tools for Assessing Climate Adaptation of Fruit Tree\nSpecies",
    "description": "Climate is a critical component limiting growing range of plant species, which\n    also determines cultivar adaptation to a region. The evaluation of climate influence on\n    fruit production is critical for decision-making in the design stage of orchards and \n    vineyards and in the evaluation of the potential consequences of future climate. Bio-\n    climatic indices and plant phenology are commonly used to describe the suitability of \n    climate for growing quality fruit and to provide temporal and spatial information about \n    regarding ongoing and future changes. 'fruclimadapt' streamlines the assessment of \n    climate adaptation and the identification of potential risks for grapevines and fruit \n    trees. Procedures in the package allow to i) downscale daily meteorological variables\n    to hourly values (Forster et al (2016) <doi:10.5194/gmd-9-2315-2016>),\n    ii) estimate chilling and forcing heat accumulation (Miranda et al (2019)\n    <https://ec.europa.eu/eip/agriculture/sites/default/files/fg30_mp5_phenology_critical_temperatures.pdf>),\n    iii) estimate plant phenology (Schwartz (2012) <doi:10.1007/978-94-007-6925-0>), iv) \n    calculate bioclimatic indices to evaluate fruit tree and grapevine adaptation (e.g. Badr \n    et al (2017) <doi:10.3354/cr01532>), v) estimate the incidence of weather-related disorders \n    in fruits (e.g. Snyder and de Melo-Abreu (2005, ISBN:92-5-105328-6) and vi)\n    estimate plant water requirements (Allen et al (1998, ISBN:92-5-104219-5)). ",
    "version": "0.4.5",
    "maintainer": "Carlos Miranda <carlos.miranda@unavarra.es>",
    "author": "Carlos Miranda",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fruclimadapt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fruclimadapt Evaluation Tools for Assessing Climate Adaptation of Fruit Tree\nSpecies Climate is a critical component limiting growing range of plant species, which\n    also determines cultivar adaptation to a region. The evaluation of climate influence on\n    fruit production is critical for decision-making in the design stage of orchards and \n    vineyards and in the evaluation of the potential consequences of future climate. Bio-\n    climatic indices and plant phenology are commonly used to describe the suitability of \n    climate for growing quality fruit and to provide temporal and spatial information about \n    regarding ongoing and future changes. 'fruclimadapt' streamlines the assessment of \n    climate adaptation and the identification of potential risks for grapevines and fruit \n    trees. Procedures in the package allow to i) downscale daily meteorological variables\n    to hourly values (Forster et al (2016) <doi:10.5194/gmd-9-2315-2016>),\n    ii) estimate chilling and forcing heat accumulation (Miranda et al (2019)\n    <https://ec.europa.eu/eip/agriculture/sites/default/files/fg30_mp5_phenology_critical_temperatures.pdf>),\n    iii) estimate plant phenology (Schwartz (2012) <doi:10.1007/978-94-007-6925-0>), iv) \n    calculate bioclimatic indices to evaluate fruit tree and grapevine adaptation (e.g. Badr \n    et al (2017) <doi:10.3354/cr01532>), v) estimate the incidence of weather-related disorders \n    in fruits (e.g. Snyder and de Melo-Abreu (2005, ISBN:92-5-105328-6) and vi)\n    estimate plant water requirements (Allen et al (1998, ISBN:92-5-104219-5)).   "
  },
  {
    "id": 12790,
    "package_name": "futility",
    "title": "Interim Analysis of Operational Futility in Randomized Trials\nwith Time-to-Event Endpoints and Fixed Follow-Up",
    "description": "Randomized clinical trials commonly follow participants for a time-to-event efficacy endpoint for a fixed period of time. Consequently, at the time when the last enrolled participant completes their follow-up, the number of observed endpoints is a random variable. Assuming data collected through an interim timepoint, simulation-based estimation and inferential procedures in the standard right-censored failure time analysis framework are conducted for the distribution of the number of endpoints--in total as well as by treatment arm--at the end of the follow-up period. The future (i.e., yet unobserved) enrollment, endpoint, and dropout times are generated according to mechanisms specified in the simTrial() function in the 'seqDesign' package. A Bayesian model for the endpoint rate, offering the option to specify a robust mixture prior distribution, is used for generating future data (see the vignette for details). Inference can be restricted to participants who received treatment according to the protocol and are observed to be at risk for the endpoint at a specified timepoint. Plotting functions are provided for graphical display of results.",
    "version": "0.4",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "author": "Yingying Zhuang [aut],\n  Michal Juraska [aut, cre],\n  Doug Grove [ctb],\n  Peter Gilbert [ctb],\n  Alexander Luedtke [ctb],\n  Sanne Roels [ctb],\n  An Vandebosch [ctb]",
    "url": "https://github.com/mjuraska/futility",
    "bug_reports": "https://github.com/mjuraska/futility/issues",
    "repository": "https://cran.r-project.org/package=futility",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "futility Interim Analysis of Operational Futility in Randomized Trials\nwith Time-to-Event Endpoints and Fixed Follow-Up Randomized clinical trials commonly follow participants for a time-to-event efficacy endpoint for a fixed period of time. Consequently, at the time when the last enrolled participant completes their follow-up, the number of observed endpoints is a random variable. Assuming data collected through an interim timepoint, simulation-based estimation and inferential procedures in the standard right-censored failure time analysis framework are conducted for the distribution of the number of endpoints--in total as well as by treatment arm--at the end of the follow-up period. The future (i.e., yet unobserved) enrollment, endpoint, and dropout times are generated according to mechanisms specified in the simTrial() function in the 'seqDesign' package. A Bayesian model for the endpoint rate, offering the option to specify a robust mixture prior distribution, is used for generating future data (see the vignette for details). Inference can be restricted to participants who received treatment according to the protocol and are observed to be at risk for the endpoint at a specified timepoint. Plotting functions are provided for graphical display of results.  "
  },
  {
    "id": 12807,
    "package_name": "fxTWAPLS",
    "title": "An Improved Version of WA-PLS",
    "description": "The goal of this package is to provide an improved version of \n    WA-PLS (Weighted Averaging Partial Least Squares) by including the \n    tolerances of taxa and the frequency of the sampled climate variable. \n    This package also provides a way of leave-out cross-validation that \n    removes both the test site and sites that are both geographically \n    close and climatically close for each cycle, to avoid the risk of \n    pseudo-replication.",
    "version": "0.1.3",
    "maintainer": "Roberto Villegas-Diaz <r.villegas-diaz@outlook.com>",
    "author": "Mengmeng Liu [aut] (ORCID: <https://orcid.org/0000-0001-6250-0148>),\n  Iain Colin Prentice [aut] (ORCID:\n    <https://orcid.org/0000-0002-1296-6764>),\n  Cajo J. F. ter Braak [aut] (ORCID:\n    <https://orcid.org/0000-0002-0414-8745>),\n  Sandy P. Harrison [aut] (ORCID:\n    <https://orcid.org/0000-0001-5687-1903>),\n  Roberto Villegas-Diaz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5036-8661>),\n  SPECIAL Research Group @ University of Reading [cph]",
    "url": "https://github.com/special-uor/fxTWAPLS/,\nhttps://special-uor.github.io/fxTWAPLS/,\nhttps://research.reading.ac.uk/palaeoclimate/",
    "bug_reports": "https://github.com/special-uor/fxTWAPLS/issues/",
    "repository": "https://cran.r-project.org/package=fxTWAPLS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fxTWAPLS An Improved Version of WA-PLS The goal of this package is to provide an improved version of \n    WA-PLS (Weighted Averaging Partial Least Squares) by including the \n    tolerances of taxa and the frequency of the sampled climate variable. \n    This package also provides a way of leave-out cross-validation that \n    removes both the test site and sites that are both geographically \n    close and climatically close for each cycle, to avoid the risk of \n    pseudo-replication.  "
  },
  {
    "id": 12810,
    "package_name": "fy",
    "title": "Utilities for Financial Years",
    "description": "In Australia, a financial year (or fiscal year) is the period from 1 July to 30 June\n     of the following calendar year. As such, many databases need to represent and \n     validate financial years efficiently. While the use of integer years with a convention that  \n     they represent the year ending is common, it may lead to ambiguity with calendar years.\n     On the other hand, string representations may be too inefficient and do not easily admit\n     arithmetic operations. This package tries to make validation of financial years quicker while\n     retaining clarity.",
    "version": "0.4.2",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fy Utilities for Financial Years In Australia, a financial year (or fiscal year) is the period from 1 July to 30 June\n     of the following calendar year. As such, many databases need to represent and \n     validate financial years efficiently. While the use of integer years with a convention that  \n     they represent the year ending is common, it may lead to ambiguity with calendar years.\n     On the other hand, string representations may be too inefficient and do not easily admit\n     arithmetic operations. This package tries to make validation of financial years quicker while\n     retaining clarity.  "
  },
  {
    "id": 12858,
    "package_name": "gamRR",
    "title": "Calculate the RR for the GAM",
    "description": "To calculate the relative risk (RR) for the generalized additive model.",
    "version": "0.7.0",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Wangjian Zhang, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gamRR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gamRR Calculate the RR for the GAM To calculate the relative risk (RR) for the generalized additive model.  "
  },
  {
    "id": 12906,
    "package_name": "gateR",
    "title": "Flow/Mass Cytometry Gating via Spatial Kernel Density Estimation",
    "description": "Estimates statistically significant marker combination values within\n        which one immunologically distinctive group (i.e., disease case) is more associated than\n        another group (i.e., healthy control), successively, using various combinations (i.e.,\n        \"gates\") of markers to examine features of cells that may be different between\n        groups. For a two-group comparison, the 'gateR' package uses the spatial relative risk\n        function estimated using the 'sparr' package. Details about the 'sparr' package\n        methods can be found in the tutorial: Davies et al. (2018) <doi:10.1002/sim.7577>. Details\n        about kernel density estimation can be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>.\n        More information about relative risk functions using kernel density estimation can be\n        found in J. F. Bithell (1991) <doi:10.1002/sim.4780101112>.",
    "version": "0.1.16",
    "maintainer": "Ian D. Buller <ian.buller@alumni.emory.edu>",
    "author": "Ian D. Buller [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9477-8582>),\n  Elena Hsieh [ctb] (ORCID: <https://orcid.org/0000-0003-3969-6597>),\n  Debashis Ghosh [ctb] (ORCID: <https://orcid.org/0000-0001-6618-1316>),\n  Lance A. Waller [ctb] (ORCID: <https://orcid.org/0000-0001-5002-8886>),\n  NCI [cph]",
    "url": "https://github.com/lance-waller-lab/gateR",
    "bug_reports": "https://github.com/lance-waller-lab/gateR/issues",
    "repository": "https://cran.r-project.org/package=gateR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gateR Flow/Mass Cytometry Gating via Spatial Kernel Density Estimation Estimates statistically significant marker combination values within\n        which one immunologically distinctive group (i.e., disease case) is more associated than\n        another group (i.e., healthy control), successively, using various combinations (i.e.,\n        \"gates\") of markers to examine features of cells that may be different between\n        groups. For a two-group comparison, the 'gateR' package uses the spatial relative risk\n        function estimated using the 'sparr' package. Details about the 'sparr' package\n        methods can be found in the tutorial: Davies et al. (2018) <doi:10.1002/sim.7577>. Details\n        about kernel density estimation can be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>.\n        More information about relative risk functions using kernel density estimation can be\n        found in J. F. Bithell (1991) <doi:10.1002/sim.4780101112>.  "
  },
  {
    "id": 12919,
    "package_name": "gbeta",
    "title": "Generalized Beta and Beta Prime Distributions",
    "description": "Density, distribution function, quantile function, and random generation for the generalized Beta and Beta prime distributions. The family of generalized Beta distributions is conjugate for the Bayesian binomial model, and the generalized Beta prime distribution is the posterior distribution of the relative risk in the Bayesian 'two Poisson samples' model when a Gamma prior is assigned to the Poisson rate of the reference group and a Beta prime prior is assigned to the relative risk. References: Laurent (2012) <doi:10.1214/11-BJPS139>, Hamza & Vallois (2016) <doi:10.1016/j.spl.2016.03.014>, Chen & Novick (1984) <doi:10.3102/10769986009002163>.",
    "version": "0.1.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent",
    "url": "https://github.com/stla/gbeta",
    "bug_reports": "https://github.com/stla/gbeta/issues",
    "repository": "https://cran.r-project.org/package=gbeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gbeta Generalized Beta and Beta Prime Distributions Density, distribution function, quantile function, and random generation for the generalized Beta and Beta prime distributions. The family of generalized Beta distributions is conjugate for the Bayesian binomial model, and the generalized Beta prime distribution is the posterior distribution of the relative risk in the Bayesian 'two Poisson samples' model when a Gamma prior is assigned to the Poisson rate of the reference group and a Beta prime prior is assigned to the relative risk. References: Laurent (2012) <doi:10.1214/11-BJPS139>, Hamza & Vallois (2016) <doi:10.1016/j.spl.2016.03.014>, Chen & Novick (1984) <doi:10.3102/10769986009002163>.  "
  },
  {
    "id": 12933,
    "package_name": "gcerisk",
    "title": "Generalized Competing Event Model",
    "description": "Generalized competing event model based on Cox PH model and Fine-Gray model.\n    This function is designed to develop optimized risk-stratification methods for competing\n    risks data, such as described in:\n    1. Carmona R, Gulaya S, Murphy JD, Rose BS, Wu J, Noticewala S,McHale MT, Yashar CM, Vaida F,\n    and Mell LK (2014) <DOI:10.1016/j.ijrobp.2014.03.047>.\n    2. Carmona R, Zakeri K, Green G, Hwang L, Gulaya S, Xu B, Verma R, Williamson CW, Triplett DP, Rose\n    BS, Shen H, Vaida F, Murphy JD, and Mell LK (2016) <DOI:10.1200/JCO.2015.65.0739>.\n    3. Lunn, Mary, and Don McNeil (1995) <DOI:10.2307/2532940>.",
    "version": "19.05.24",
    "maintainer": "Hanjie Shen <shenhanjie0418@gmail.com>",
    "author": "Hanjie Shen <shenhanjie0418@gmail.com>, Ruben Carmona\n    <ruben.carmona13@gmail.com>, Loren Mell <lmell@ucsd.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gcerisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gcerisk Generalized Competing Event Model Generalized competing event model based on Cox PH model and Fine-Gray model.\n    This function is designed to develop optimized risk-stratification methods for competing\n    risks data, such as described in:\n    1. Carmona R, Gulaya S, Murphy JD, Rose BS, Wu J, Noticewala S,McHale MT, Yashar CM, Vaida F,\n    and Mell LK (2014) <DOI:10.1016/j.ijrobp.2014.03.047>.\n    2. Carmona R, Zakeri K, Green G, Hwang L, Gulaya S, Xu B, Verma R, Williamson CW, Triplett DP, Rose\n    BS, Shen H, Vaida F, Murphy JD, and Mell LK (2016) <DOI:10.1200/JCO.2015.65.0739>.\n    3. Lunn, Mary, and Don McNeil (1995) <DOI:10.2307/2532940>.  "
  },
  {
    "id": 13009,
    "package_name": "generalCorr",
    "title": "Generalized Correlations, Causal Paths and Portfolio Selection",
    "description": "Function gmcmtx0() computes a more reliable (general) \n    correlation matrix. Since causal paths from data are important for all sciences, the\n    package provides many sophisticated functions. causeSummBlk() and causeSum2Blk()\n    give easy-to-interpret causal paths.  Let Z denote control variables and compare \n    two flipped kernel regressions: X=f(Y, Z)+e1 and Y=g(X, Z)+e2. Our criterion Cr1 \n    says that if |e1*Y|>|e2*X| then variation in X is more \"exogenous or independent\"\n    than in Y, and the causal path is X to Y. Criterion Cr2 requires |e2|<|e1|. These\n    inequalities between many absolute values are quantified by four orders of \n    stochastic dominance. Our third criterion Cr3, for the causal path X to Y,\n    requires new generalized partial correlations to satisfy |r*(x|y,z)|< |r*(y|x,z)|.\n    The function parcorVec() reports generalized partials between the first\n    variable and all others.  The package provides several R functions including\n    get0outliers() for outlier detection, bigfp() for numerical integration by the\n    trapezoidal rule, stochdom2() for stochastic dominance, pillar3D() for 3D charts,\n    canonRho() for generalized canonical correlations, depMeas() measures nonlinear\n    dependence, and causeSummary(mtx) reports summary of causal paths among matrix \n    columns. Portfolio selection: decileVote(), momentVote(), dif4mtx(), exactSdMtx()\n    can rank several stocks. Functions whose names begin with 'boot' provide bootstrap\n    statistical inference, including a new bootGcRsq() test for \"Granger-causality\" \n    allowing nonlinear relations. A new tool for evaluation of out-of-sample\n    portfolio performance is outOFsamp(). Panel data implementation is now included.\n    See eight vignettes of the package for theory, examples, and\n    usage tips. See Vinod (2019) \\doi{10.1080/03610918.2015.1122048}.",
    "version": "1.2.6",
    "maintainer": "H. D. Vinod <vinod@fordham.edu>",
    "author": "Prof. H. D. Vinod, Fordham University, NY.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=generalCorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "generalCorr Generalized Correlations, Causal Paths and Portfolio Selection Function gmcmtx0() computes a more reliable (general) \n    correlation matrix. Since causal paths from data are important for all sciences, the\n    package provides many sophisticated functions. causeSummBlk() and causeSum2Blk()\n    give easy-to-interpret causal paths.  Let Z denote control variables and compare \n    two flipped kernel regressions: X=f(Y, Z)+e1 and Y=g(X, Z)+e2. Our criterion Cr1 \n    says that if |e1*Y|>|e2*X| then variation in X is more \"exogenous or independent\"\n    than in Y, and the causal path is X to Y. Criterion Cr2 requires |e2|<|e1|. These\n    inequalities between many absolute values are quantified by four orders of \n    stochastic dominance. Our third criterion Cr3, for the causal path X to Y,\n    requires new generalized partial correlations to satisfy |r*(x|y,z)|< |r*(y|x,z)|.\n    The function parcorVec() reports generalized partials between the first\n    variable and all others.  The package provides several R functions including\n    get0outliers() for outlier detection, bigfp() for numerical integration by the\n    trapezoidal rule, stochdom2() for stochastic dominance, pillar3D() for 3D charts,\n    canonRho() for generalized canonical correlations, depMeas() measures nonlinear\n    dependence, and causeSummary(mtx) reports summary of causal paths among matrix \n    columns. Portfolio selection: decileVote(), momentVote(), dif4mtx(), exactSdMtx()\n    can rank several stocks. Functions whose names begin with 'boot' provide bootstrap\n    statistical inference, including a new bootGcRsq() test for \"Granger-causality\" \n    allowing nonlinear relations. A new tool for evaluation of out-of-sample\n    portfolio performance is outOFsamp(). Panel data implementation is now included.\n    See eight vignettes of the package for theory, examples, and\n    usage tips. See Vinod (2019) \\doi{10.1080/03610918.2015.1122048}.  "
  },
  {
    "id": 13069,
    "package_name": "geohabnet",
    "title": "Geographical Risk Analysis Based on Habitat Connectivity",
    "description": "\n    The 'geohabnet' package is designed to perform a geographically or spatially explicit risk analysis of habitat connectivity. Xing et al (2021) <doi:10.1093/biosci/biaa067> proposed the concept of cropland connectivity as a risk factor for plant pathogen or pest invasions. As the functions in 'geohabnet' were initially developed thinking on cropland connectivity, users are recommended to first be familiar with the concept by looking at the Xing et al paper. In a nutshell, a habitat connectivity analysis combines information from maps of host density, estimates the relative likelihood of pathogen movement between habitat locations in the area of interest, and applies network analysis to calculate the connectivity of habitat locations.\n    The functions of 'geohabnet' are built to conduct a habitat connectivity analysis relying on geographic parameters (spatial resolution and spatial extent), dispersal parameters (in two commonly used dispersal kernels: inverse power law and negative exponential models), and network parameters (link weight thresholds and network metrics).\n    The functionality and main extensions provided by the functions in 'geohabnet' to habitat connectivity analysis are \n    a) Capability to easily calculate the connectivity of locations in a landscape using a single function, such as sensitivity_analysis() or msean().\n    b) As backbone datasets, the 'geohabnet' package supports the use of two publicly available global datasets to calculate cropland density. The backbone datasets in the 'geohabnet' package include crop distribution maps from Monfreda, C., N. Ramankutty, and J. A. Foley (2008) <doi:10.1029/2007gb002947> \"Farming the planet: 2. Geographic distribution of crop areas, yields, physiological types, and net primary production in the year 2000, Global Biogeochem. Cycles, 22, GB1022\" and International Food Policy Research Institute (2019) <doi:10.7910/DVN/PRFF8V> \"Global Spatially-Disaggregated Crop Production Statistics Data for 2010 Version 2.0, Harvard Dataverse, V4\". Users can also provide any other geographic dataset that represents host density.\n    c) Because the 'geohabnet' package allows R users to provide maps of host density (as originally in Xing et al (2021)), host landscape density (representing the geographic distribution of either crops or wild species), or habitat distribution (such as host landscape density adjusted by climate suitability) as inputs, we propose the term habitat connectivity.\n    d) The 'geohabnet' package allows R users to customize parameter values in the habitat connectivity analysis, facilitating context-specific (pathogen- or pest-specific) analyses.\n    e) The 'geohabnet' package allows users to automatically visualize maps of the habitat connectivity of locations resulting from a sensitivity analysis across all customized parameter combinations.\n    The primary functions are msean() and sensitivity analysis().\n    Most functions in 'geohabnet' provide three main outcomes: i) A map of mean habitat connectivity across parameters selected by the user, ii) a map of variance of habitat connectivity across the selected parameters, and iii) a map of the difference between the ranks of habitat connectivity and habitat density.\n    Each function can be used to generate these maps as 'final' outcomes. \n    Each function can also provide intermediate outcomes, such as the adjacency matrices built to perform the analysis, which can be used in other network analysis.\n    Refer to article at <https://garrettlab.github.io/HabitatConnectivity/articles/analysis.html> to see examples of each function and how to access each of these outcome types.\n    To change parameter values, the file called 'parameters.yaml' stores the parameters and their values, can be accessed using 'get_parameters()' and set new parameter values with 'set_parameters()'.\n    Users can modify up to ten parameters.",
    "version": "2.2",
    "maintainer": "Krishna Keshav <krishnakeshav.pes@gmail.com>",
    "author": "Krishna Keshav [aut, cre],\n  Aaron Plex [aut] (ORCID: <https://orcid.org/0000-0001-7317-3090>),\n  Garrett Lab [ctb] (https://garrettlab.com),\n  Karen Garrett [aut] (ORCID: <https://orcid.org/0000-0002-6578-1616>),\n  University of Florida [cph, fnd] (https://www.ufl.edu)",
    "url": "https://garrettlab.github.io/HabitatConnectivity/,\nhttps://CRAN.R-project.org/package=geohabnet/,\nhttps://github.com/GarrettLab/HabitatConnectivity/tree/main/geohabnet/,\nhttps://www.garrettlab.com/",
    "bug_reports": "https://github.com/GarrettLab/HabitatConnectivity/issues",
    "repository": "https://cran.r-project.org/package=geohabnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geohabnet Geographical Risk Analysis Based on Habitat Connectivity \n    The 'geohabnet' package is designed to perform a geographically or spatially explicit risk analysis of habitat connectivity. Xing et al (2021) <doi:10.1093/biosci/biaa067> proposed the concept of cropland connectivity as a risk factor for plant pathogen or pest invasions. As the functions in 'geohabnet' were initially developed thinking on cropland connectivity, users are recommended to first be familiar with the concept by looking at the Xing et al paper. In a nutshell, a habitat connectivity analysis combines information from maps of host density, estimates the relative likelihood of pathogen movement between habitat locations in the area of interest, and applies network analysis to calculate the connectivity of habitat locations.\n    The functions of 'geohabnet' are built to conduct a habitat connectivity analysis relying on geographic parameters (spatial resolution and spatial extent), dispersal parameters (in two commonly used dispersal kernels: inverse power law and negative exponential models), and network parameters (link weight thresholds and network metrics).\n    The functionality and main extensions provided by the functions in 'geohabnet' to habitat connectivity analysis are \n    a) Capability to easily calculate the connectivity of locations in a landscape using a single function, such as sensitivity_analysis() or msean().\n    b) As backbone datasets, the 'geohabnet' package supports the use of two publicly available global datasets to calculate cropland density. The backbone datasets in the 'geohabnet' package include crop distribution maps from Monfreda, C., N. Ramankutty, and J. A. Foley (2008) <doi:10.1029/2007gb002947> \"Farming the planet: 2. Geographic distribution of crop areas, yields, physiological types, and net primary production in the year 2000, Global Biogeochem. Cycles, 22, GB1022\" and International Food Policy Research Institute (2019) <doi:10.7910/DVN/PRFF8V> \"Global Spatially-Disaggregated Crop Production Statistics Data for 2010 Version 2.0, Harvard Dataverse, V4\". Users can also provide any other geographic dataset that represents host density.\n    c) Because the 'geohabnet' package allows R users to provide maps of host density (as originally in Xing et al (2021)), host landscape density (representing the geographic distribution of either crops or wild species), or habitat distribution (such as host landscape density adjusted by climate suitability) as inputs, we propose the term habitat connectivity.\n    d) The 'geohabnet' package allows R users to customize parameter values in the habitat connectivity analysis, facilitating context-specific (pathogen- or pest-specific) analyses.\n    e) The 'geohabnet' package allows users to automatically visualize maps of the habitat connectivity of locations resulting from a sensitivity analysis across all customized parameter combinations.\n    The primary functions are msean() and sensitivity analysis().\n    Most functions in 'geohabnet' provide three main outcomes: i) A map of mean habitat connectivity across parameters selected by the user, ii) a map of variance of habitat connectivity across the selected parameters, and iii) a map of the difference between the ranks of habitat connectivity and habitat density.\n    Each function can be used to generate these maps as 'final' outcomes. \n    Each function can also provide intermediate outcomes, such as the adjacency matrices built to perform the analysis, which can be used in other network analysis.\n    Refer to article at <https://garrettlab.github.io/HabitatConnectivity/articles/analysis.html> to see examples of each function and how to access each of these outcome types.\n    To change parameter values, the file called 'parameters.yaml' stores the parameters and their values, can be accessed using 'get_parameters()' and set new parameter values with 'set_parameters()'.\n    Users can modify up to ten parameters.  "
  },
  {
    "id": 13097,
    "package_name": "geospatialsuite",
    "title": "Comprehensive Geospatiotemporal Analysis and Multimodal\nIntegration Toolkit",
    "description": "A comprehensive toolkit for geospatiotemporal analysis\n    featuring 60+ vegetation indices, advanced raster visualization,\n    universal spatial mapping, water quality analysis, CDL crop analysis,\n    spatial interpolation, temporal analysis, and terrain analysis.\n    Designed for agricultural research, environmental monitoring, remote\n    sensing applications, and publication-quality mapping with support for\n    any geographic region and robust error handling. Methods include\n    vegetation indices calculations (Rouse et al. 1974), NDVI and enhanced\n    vegetation indices (Huete et al. 1997)\n    <doi:10.1016/S0034-4257(97)00104-1>, (Akanbi et al. 2024) \n    <doi:10.1007/s41651-023-00164-y>, spatial interpolation techniques\n    (Cressie 1993, ISBN:9780471002556), water quality indices (McFeeters\n    1996) <doi:10.1080/01431169608948714>, and crop data layer analysis\n    (USDA NASS 2024)\n    <https://www.nass.usda.gov/Research_and_Science/Cropland/>.  Funding:\n    This material is based upon financial support by the National Science\n    Foundation, EEC Division of Engineering Education and Centers, NSF\n    Engineering Research Center for Advancing Sustainable and Distributed\n    Fertilizer production (CASFER), NSF 20-553 Gen-4 Engineering Research\n    Centers award 2133576.",
    "version": "0.1.1",
    "maintainer": "Olatunde D. Akanbi <olatunde.akanbi@case.edu>",
    "author": "Olatunde D. Akanbi [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7719-2619>),\n  Vibha Mandayam [aut] (ORCID: <https://orcid.org/0009-0008-8628-9904>),\n  Yinghui Wu [aut] (ORCID: <https://orcid.org/0000-0003-3991-5155>),\n  Jeffrey Yarus [aut] (ORCID: <https://orcid.org/0000-0002-9331-9568>),\n  Erika I. Barcelos [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-9273-8488>),\n  Roger H. French [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6162-0532>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geospatialsuite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geospatialsuite Comprehensive Geospatiotemporal Analysis and Multimodal\nIntegration Toolkit A comprehensive toolkit for geospatiotemporal analysis\n    featuring 60+ vegetation indices, advanced raster visualization,\n    universal spatial mapping, water quality analysis, CDL crop analysis,\n    spatial interpolation, temporal analysis, and terrain analysis.\n    Designed for agricultural research, environmental monitoring, remote\n    sensing applications, and publication-quality mapping with support for\n    any geographic region and robust error handling. Methods include\n    vegetation indices calculations (Rouse et al. 1974), NDVI and enhanced\n    vegetation indices (Huete et al. 1997)\n    <doi:10.1016/S0034-4257(97)00104-1>, (Akanbi et al. 2024) \n    <doi:10.1007/s41651-023-00164-y>, spatial interpolation techniques\n    (Cressie 1993, ISBN:9780471002556), water quality indices (McFeeters\n    1996) <doi:10.1080/01431169608948714>, and crop data layer analysis\n    (USDA NASS 2024)\n    <https://www.nass.usda.gov/Research_and_Science/Cropland/>.  Funding:\n    This material is based upon financial support by the National Science\n    Foundation, EEC Division of Engineering Education and Centers, NSF\n    Engineering Research Center for Advancing Sustainable and Distributed\n    Fertilizer production (CASFER), NSF 20-553 Gen-4 Engineering Research\n    Centers award 2133576.  "
  },
  {
    "id": 13136,
    "package_name": "gfer",
    "title": "Green Finance and Environmental Risk",
    "description": "Focuses on data collecting, analyzing and visualization in green finance and environmental \n  risk research and analysis. Main function includes environmental data collecting from \n  official websites such as MEP (Ministry of Environmental Protection of China, <https://www.mee.gov.cn>), water \n  related projects identification and environmental data visualization.",
    "version": "0.1.12",
    "maintainer": "Yuanchao Xu <xuyuanchao37@gmail.com>",
    "author": "Yuanchao Xu [aut, cre]",
    "url": "https://yuanchao-xu.github.io/gfer/",
    "bug_reports": "https://github.com/Yuanchao-Xu/gfer/issues",
    "repository": "https://cran.r-project.org/package=gfer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gfer Green Finance and Environmental Risk Focuses on data collecting, analyzing and visualization in green finance and environmental \n  risk research and analysis. Main function includes environmental data collecting from \n  official websites such as MEP (Ministry of Environmental Protection of China, <https://www.mee.gov.cn>), water \n  related projects identification and environmental data visualization.  "
  },
  {
    "id": 13139,
    "package_name": "gfoRmula",
    "title": "Parametric G-Formula",
    "description": "Implements the non-iterative conditional expectation (NICE) \n    algorithm of the g-formula algorithm (Robins (1986) \n    <doi:10.1016/0270-0255(86)90088-6>, Hern\u00e1n and Robins (2024, ISBN:9781420076165)). \n    The g-formula can estimate an outcome's counterfactual mean or risk under \n    hypothetical treatment strategies (interventions) when there is sufficient \n    information on time-varying treatments and confounders. \n    This package can be used for discrete or continuous time-varying treatments \n    and for failure time outcomes or continuous/binary end of follow-up \n    outcomes. The package can handle a random measurement/visit process and a \n    priori knowledge of the data structure, as well as censoring (e.g., by loss \n    to follow-up) and two options for handling competing events for failure time\n    outcomes. Interventions can be flexibly specified, both as interventions on \n    a single treatment or as joint interventions on multiple treatments.\n    See McGrath et al. (2020) <doi:10.1016/j.patter.2020.100008> for a guide on \n    how to use the package.",
    "version": "1.1.1",
    "maintainer": "Sean McGrath <sean.mcgrath514@gmail.com>",
    "author": "Victoria Lin [aut] (V. Lin and S. McGrath made equal contributions),\n  Sean McGrath [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7281-3516>, V. Lin and S. McGrath made\n    equal contributions),\n  Zilu Zhang [aut],\n  Roger W. Logan [aut],\n  Lucia C. Petito [aut],\n  Jing Li [aut],\n  McGee Emma [aut] (ORCID: <https://orcid.org/0000-0002-7456-6408>),\n  Cheng Carrie [aut],\n  Jessica G. Young [aut] (ORCID: <https://orcid.org/0000-0002-2758-6932>,\n    M.A. Hern\u00e1n and J.G. Young made equal contributions),\n  Miguel A. Hern\u00e1n [aut] (M.A. Hern\u00e1n and J.G. Young made equal\n    contributions),\n  2019 The President and Fellows of Harvard College [cph]",
    "url": "https://github.com/CausalInference/gfoRmula,\nhttps://doi.org/10.1016/j.patter.2020.100008",
    "bug_reports": "https://github.com/CausalInference/gfoRmula/issues",
    "repository": "https://cran.r-project.org/package=gfoRmula",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gfoRmula Parametric G-Formula Implements the non-iterative conditional expectation (NICE) \n    algorithm of the g-formula algorithm (Robins (1986) \n    <doi:10.1016/0270-0255(86)90088-6>, Hern\u00e1n and Robins (2024, ISBN:9781420076165)). \n    The g-formula can estimate an outcome's counterfactual mean or risk under \n    hypothetical treatment strategies (interventions) when there is sufficient \n    information on time-varying treatments and confounders. \n    This package can be used for discrete or continuous time-varying treatments \n    and for failure time outcomes or continuous/binary end of follow-up \n    outcomes. The package can handle a random measurement/visit process and a \n    priori knowledge of the data structure, as well as censoring (e.g., by loss \n    to follow-up) and two options for handling competing events for failure time\n    outcomes. Interventions can be flexibly specified, both as interventions on \n    a single treatment or as joint interventions on multiple treatments.\n    See McGrath et al. (2020) <doi:10.1016/j.patter.2020.100008> for a guide on \n    how to use the package.  "
  },
  {
    "id": 13189,
    "package_name": "ggdemetra",
    "title": "'ggplot2' Extension for Seasonal and Trading Day Adjustment with\n'RJDemetra'",
    "description": "Provides 'ggplot2' functions to return the results of seasonal and trading day adjustment \n    made by 'RJDemetra'. 'RJDemetra' is an 'R' interface around 'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>),\n    the seasonal adjustment software officially recommended to the members of the European Statistical System and\n    the European System of Central Banks.",
    "version": "0.2.9",
    "maintainer": "Alain Quartier-la-Tente <alain.quartier@yahoo.fr>",
    "author": "Alain Quartier-la-Tente [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7890-3857>)",
    "url": "https://aqlt.github.io/ggdemetra/,\nhttps://github.com/AQLT/ggdemetra",
    "bug_reports": "https://github.com/AQLT/ggdemetra/issues",
    "repository": "https://cran.r-project.org/package=ggdemetra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggdemetra 'ggplot2' Extension for Seasonal and Trading Day Adjustment with\n'RJDemetra' Provides 'ggplot2' functions to return the results of seasonal and trading day adjustment \n    made by 'RJDemetra'. 'RJDemetra' is an 'R' interface around 'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>),\n    the seasonal adjustment software officially recommended to the members of the European Statistical System and\n    the European System of Central Banks.  "
  },
  {
    "id": 13306,
    "package_name": "ggrisk",
    "title": "Risk Score Plot for Cox Regression",
    "description": "The risk plot may be one of the most commonly used figures in \n    tumor genetic data analysis. We can conclude the following two points: \n    Comparing the prediction results of the model with the real survival situation \n    to see whether the survival rate of the high-risk group is lower than that of the \n    low-level group, and whether the survival time of the high-risk group is \n    shorter than that of the low-risk group. The other is to compare the heat \n    map and scatter plot to see the correlation between the predictors and the \n    outcome.",
    "version": "1.3",
    "maintainer": "Jing Zhang <zj391120@163.com>",
    "author": "Jing Zhang [aut, cre],\n  Zhi Jin [aut]",
    "url": "https://github.com/yikeshu0611/ggrisk",
    "bug_reports": "https://github.com/yikeshu0611/ggrisk/issues",
    "repository": "https://cran.r-project.org/package=ggrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggrisk Risk Score Plot for Cox Regression The risk plot may be one of the most commonly used figures in \n    tumor genetic data analysis. We can conclude the following two points: \n    Comparing the prediction results of the model with the real survival situation \n    to see whether the survival rate of the high-risk group is lower than that of the \n    low-level group, and whether the survival time of the high-risk group is \n    shorter than that of the low-risk group. The other is to compare the heat \n    map and scatter plot to see the correlation between the predictors and the \n    outcome.  "
  },
  {
    "id": 13321,
    "package_name": "ggsolvencyii",
    "title": "A 'ggplot2'-Plot of Composition of Solvency II SCR: SF and IM",
    "description": "An implementation of 'ggplot2'-methods to present the composition of Solvency II Solvency Capital Requirement (SCR) as a series of concentric circle-parts. \n Solvency II (Solvency 2) is European insurance legislation, coming in force by the delegated acts of October 10, 2014.\n <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ%3AL%3A2015%3A012%3ATOC>. \n Additional files, defining the structure of the Standard Formula (SF) method of the SCR-calculation are provided.\n The structure files can be adopted for localization or for insurance companies who use Internal Models (IM).\n Options are available for combining smaller components, horizontal and vertical scaling, rotation, and plotting only some circle-parts.\n With outlines and connectors several SCR-compositions can be compared, for example in ORSA-scenarios (Own Risk and Solvency Assessment).",
    "version": "0.1.2",
    "maintainer": "Marco van Zanden <git@vanzanden.nl>",
    "author": "Marco van Zanden [aut, cre]",
    "url": "https://github.com/vanzanden/ggsolvencyii",
    "bug_reports": "https://github.com/vanzanden/ggsolvencyii/issues",
    "repository": "https://cran.r-project.org/package=ggsolvencyii",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggsolvencyii A 'ggplot2'-Plot of Composition of Solvency II SCR: SF and IM An implementation of 'ggplot2'-methods to present the composition of Solvency II Solvency Capital Requirement (SCR) as a series of concentric circle-parts. \n Solvency II (Solvency 2) is European insurance legislation, coming in force by the delegated acts of October 10, 2014.\n <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ%3AL%3A2015%3A012%3ATOC>. \n Additional files, defining the structure of the Standard Formula (SF) method of the SCR-calculation are provided.\n The structure files can be adopted for localization or for insurance companies who use Internal Models (IM).\n Options are available for combining smaller components, horizontal and vertical scaling, rotation, and plotting only some circle-parts.\n With outlines and connectors several SCR-compositions can be compared, for example in ORSA-scenarios (Own Risk and Solvency Assessment).  "
  },
  {
    "id": 13363,
    "package_name": "ghyp",
    "title": "Generalized Hyperbolic Distribution and Its Special Cases",
    "description": "Detailed functionality for working\n        with the univariate and multivariate Generalized Hyperbolic\n        distribution and its special cases (Hyperbolic (hyp), Normal\n        Inverse Gaussian (NIG), Variance Gamma (VG), skewed Student-t\n        and Gaussian distribution). Especially, it contains fitting\n        procedures, an AIC-based model selection routine, and functions\n        for the computation of density, quantile, probability, random\n        variates, expected shortfall and some portfolio optimization\n        and plotting routines as well as the likelihood ratio test. In\n        addition, it contains the Generalized Inverse Gaussian\n        distribution. See Chapter 3 of A. J. McNeil, R. Frey, and P. Embrechts. \n        Quantitative risk management: Concepts, techniques and tools. \n        Princeton University Press, Princeton (2005).",
    "version": "1.6.5",
    "maintainer": "Marc Weibel <marc.weibel@quantsulting.ch>",
    "author": "Marc Weibel [aut, cre],\n  David Luethi [aut],\n  Henriette-Elise Breymann [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ghyp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ghyp Generalized Hyperbolic Distribution and Its Special Cases Detailed functionality for working\n        with the univariate and multivariate Generalized Hyperbolic\n        distribution and its special cases (Hyperbolic (hyp), Normal\n        Inverse Gaussian (NIG), Variance Gamma (VG), skewed Student-t\n        and Gaussian distribution). Especially, it contains fitting\n        procedures, an AIC-based model selection routine, and functions\n        for the computation of density, quantile, probability, random\n        variates, expected shortfall and some portfolio optimization\n        and plotting routines as well as the likelihood ratio test. In\n        addition, it contains the Generalized Inverse Gaussian\n        distribution. See Chapter 3 of A. J. McNeil, R. Frey, and P. Embrechts. \n        Quantitative risk management: Concepts, techniques and tools. \n        Princeton University Press, Princeton (2005).  "
  },
  {
    "id": 13391,
    "package_name": "gitlink",
    "title": "Add 'Git' Links to Your Web Based Assets",
    "description": "Provides helpers to add 'Git' links to 'shiny'\n    applications, 'rmarkdown' documents, and other 'HTML' based resources.\n    This is most commonly used for 'GitHub' ribbons.",
    "version": "0.1.3",
    "maintainer": "Cole Arendt <cole@rstudio.com>",
    "author": "Cole Arendt [aut, cre],\n  RStudio [cph, fnd]",
    "url": "https://github.com/colearendt/gitlink",
    "bug_reports": "https://github.com/colearendt/gitlink/issues",
    "repository": "https://cran.r-project.org/package=gitlink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gitlink Add 'Git' Links to Your Web Based Assets Provides helpers to add 'Git' links to 'shiny'\n    applications, 'rmarkdown' documents, and other 'HTML' based resources.\n    This is most commonly used for 'GitHub' ribbons.  "
  },
  {
    "id": 13418,
    "package_name": "glmMisrep",
    "title": "Generalized Linear Models Adjusting for Misrepresentation",
    "description": "Fit Generalized Linear Models to continuous and count outcomes, as well as estimate the prevalence of misrepresentation of an important binary predictor. Misrepresentation typically arises when there is an incentive for the binary factor to be misclassified in one direction (e.g., in insurance settings where policy holders may purposely deny a risk status in order to lower the insurance premium). This is accomplished by treating a subset of the response variable as resulting from a mixture distribution. Model parameters are estimated via the Expectation Maximization algorithm and standard errors of the estimates are obtained from closed forms of the Observed Fisher Information. For an introduction to the models and the misrepresentation framework, see Xia et. al., (2023) <https://variancejournal.org/article/73151-maximum-likelihood-approaches-to-misrepresentation-models-in-glm-ratemaking-model-comparisons>.",
    "version": "0.1.1",
    "maintainer": "Patrick Rafael <pbr2608@vt.edu>",
    "author": "Patrick Rafael [cre, aut],\n  Xia Michelle [aut],\n  Rexford Akakpo [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glmMisrep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glmMisrep Generalized Linear Models Adjusting for Misrepresentation Fit Generalized Linear Models to continuous and count outcomes, as well as estimate the prevalence of misrepresentation of an important binary predictor. Misrepresentation typically arises when there is an incentive for the binary factor to be misclassified in one direction (e.g., in insurance settings where policy holders may purposely deny a risk status in order to lower the insurance premium). This is accomplished by treating a subset of the response variable as resulting from a mixture distribution. Model parameters are estimated via the Expectation Maximization algorithm and standard errors of the estimates are obtained from closed forms of the Observed Fisher Information. For an introduction to the models and the misrepresentation framework, see Xia et. al., (2023) <https://variancejournal.org/article/73151-maximum-likelihood-approaches-to-misrepresentation-models-in-glm-ratemaking-model-comparisons>.  "
  },
  {
    "id": 13471,
    "package_name": "gluvarpro",
    "title": "Glucose Variability Measures from Continuous Glucose Monitoring\nData",
    "description": "Calculate different glucose variability measures, \n including average measures of glycemia, measures of glycemic variability and\n measures of glycemic risk, from continuous glucose monitoring data.\n Boris P. Kovatchev, Erik Otto, Daniel Cox, Linda Gonder-Frederick, and William Clarke (2006) <doi:10.2337/dc06-1085>.\n Jean-Pierre Le Floch, Philippe Escuyer, Eric Baudin, Dominique Baudon, and Leon Perlemuter (1990) <doi:10.2337/diacare.13.2.172>.\n C.M. McDonnell, S.M. Donath, S.I. Vidmar, G.A. Werther, and F.J. Cameron (2005) <doi:10.1089/dia.2005.7.253>.\n Everitt, Brian (1998) <doi:10.1111/j.1751-5823.2011.00149_2.x>.\n Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) <doi:10.2307/2234167>.\n Dougherty, R. L., Edelman, A. and Hyman, J. M. (1989) <doi:10.1090/S0025-5718-1989-0962209-1>.\n Tukey, J. W. (1977) <doi:10.1016/0377-2217(86)90209-2>.\n F. John Service (2013) <doi:10.2337/db12-1396>.\n Edmond A. Ryan, Tami Shandro, Kristy Green, Breay W. Paty, Peter A. Senior, David Bigam, A.M. James Shapiro, and Marie-Christine Vantyghem (2004) <doi:10.2337/diabetes.53.4.955>.\n F. John Service, George D. Molnar, John W. Rosevear, Eugene Ackerman, Leal C. Gatewood, William F. Taylor (1970) <doi:10.2337/diab.19.9.644>.\n Sarah E. Siegelaar, Frits Holleman, Joost B. L. Hoekstra, and J. Hans DeVries (2010) <doi:10.1210/er.2009-0021>.\n Gabor Marics, Zsofia Lendvai, Csaba Lodi, Levente Koncz, David Zakarias, Gyorgy Schuster, Borbala Mikos, Csaba Hermann, Attila J. Szabo, and Peter Toth-Heyn (2015) <doi:10.1186/s12938-015-0035-3>.\n Thomas Danne, Revital Nimri, Tadej Battelino, Richard M. Bergenstal, Kelly L. Close, J. Hans DeVries, SatishGarg, Lutz  Heinemann, Irl Hirsch, Stephanie A. Amiel, Roy Beck,  Emanuele Bosi, Bruce Buckingham, \n ClaudioCobelli, Eyal Dassau, Francis J. Doyle, Simon Heller, Roman Hovorka, Weiping Jia, Tim Jones, Olga Kordonouri,Boris Kovatchev, Aaron Kowalski, Lori Laffel, David Maahs, Helen R. Murphy, Kirsten N\u00f8rgaard, \n Christopher G.Parkin, Eric Renard, Banshi Saboo, Mauro Scharf, William V. Tamborlane, Stuart A. Weinzimer, and Moshe Phillip.International consensus on use of continuous glucose monitoring.Diabetes Care, 2017 <doi:10.2337/dc17-1600>.",
    "version": "7.0",
    "maintainer": "Sergio Contador <scontador@ucm.es>",
    "author": "Sergio Contador",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gluvarpro",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gluvarpro Glucose Variability Measures from Continuous Glucose Monitoring\nData Calculate different glucose variability measures, \n including average measures of glycemia, measures of glycemic variability and\n measures of glycemic risk, from continuous glucose monitoring data.\n Boris P. Kovatchev, Erik Otto, Daniel Cox, Linda Gonder-Frederick, and William Clarke (2006) <doi:10.2337/dc06-1085>.\n Jean-Pierre Le Floch, Philippe Escuyer, Eric Baudin, Dominique Baudon, and Leon Perlemuter (1990) <doi:10.2337/diacare.13.2.172>.\n C.M. McDonnell, S.M. Donath, S.I. Vidmar, G.A. Werther, and F.J. Cameron (2005) <doi:10.1089/dia.2005.7.253>.\n Everitt, Brian (1998) <doi:10.1111/j.1751-5823.2011.00149_2.x>.\n Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) <doi:10.2307/2234167>.\n Dougherty, R. L., Edelman, A. and Hyman, J. M. (1989) <doi:10.1090/S0025-5718-1989-0962209-1>.\n Tukey, J. W. (1977) <doi:10.1016/0377-2217(86)90209-2>.\n F. John Service (2013) <doi:10.2337/db12-1396>.\n Edmond A. Ryan, Tami Shandro, Kristy Green, Breay W. Paty, Peter A. Senior, David Bigam, A.M. James Shapiro, and Marie-Christine Vantyghem (2004) <doi:10.2337/diabetes.53.4.955>.\n F. John Service, George D. Molnar, John W. Rosevear, Eugene Ackerman, Leal C. Gatewood, William F. Taylor (1970) <doi:10.2337/diab.19.9.644>.\n Sarah E. Siegelaar, Frits Holleman, Joost B. L. Hoekstra, and J. Hans DeVries (2010) <doi:10.1210/er.2009-0021>.\n Gabor Marics, Zsofia Lendvai, Csaba Lodi, Levente Koncz, David Zakarias, Gyorgy Schuster, Borbala Mikos, Csaba Hermann, Attila J. Szabo, and Peter Toth-Heyn (2015) <doi:10.1186/s12938-015-0035-3>.\n Thomas Danne, Revital Nimri, Tadej Battelino, Richard M. Bergenstal, Kelly L. Close, J. Hans DeVries, SatishGarg, Lutz  Heinemann, Irl Hirsch, Stephanie A. Amiel, Roy Beck,  Emanuele Bosi, Bruce Buckingham, \n ClaudioCobelli, Eyal Dassau, Francis J. Doyle, Simon Heller, Roman Hovorka, Weiping Jia, Tim Jones, Olga Kordonouri,Boris Kovatchev, Aaron Kowalski, Lori Laffel, David Maahs, Helen R. Murphy, Kirsten N\u00f8rgaard, \n Christopher G.Parkin, Eric Renard, Banshi Saboo, Mauro Scharf, William V. Tamborlane, Stuart A. Weinzimer, and Moshe Phillip.International consensus on use of continuous glucose monitoring.Diabetes Care, 2017 <doi:10.2337/dc17-1600>.  "
  },
  {
    "id": 13497,
    "package_name": "gnFit",
    "title": "Goodness of Fit Test for Continuous Distribution Functions",
    "description": "Computes the test statistic and p-value of the Cramer-von Mises and Anderson-Darling test for some continuous distribution functions proposed by Chen and Balakrishnan (1995) <http://asq.org/qic/display-item/index.html?item=11407>. In addition to our classic distribution functions here, we  calculate the Goodness of Fit (GoF) test to dataset which follows the extreme value distribution function, without remembering the formula of distribution/density functions. Calculates the Value at Risk (VaR) and Average VaR are another important risk factors which are estimated by using well-known distribution functions. Pflug and Romisch (2007, ISBN: 9812707409) is a good reference to study the properties of risk measures.",
    "version": "0.2.0",
    "maintainer": "Ali Saeb <ali.saeb@gmail.com>",
    "author": "Ali Saeb",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gnFit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gnFit Goodness of Fit Test for Continuous Distribution Functions Computes the test statistic and p-value of the Cramer-von Mises and Anderson-Darling test for some continuous distribution functions proposed by Chen and Balakrishnan (1995) <http://asq.org/qic/display-item/index.html?item=11407>. In addition to our classic distribution functions here, we  calculate the Goodness of Fit (GoF) test to dataset which follows the extreme value distribution function, without remembering the formula of distribution/density functions. Calculates the Value at Risk (VaR) and Average VaR are another important risk factors which are estimated by using well-known distribution functions. Pflug and Romisch (2007, ISBN: 9812707409) is a good reference to study the properties of risk measures.  "
  },
  {
    "id": 13509,
    "package_name": "godley",
    "title": "Stock-Flow-Consistent Model Simulator",
    "description": "Define, simulate, and validate stock-flow consistent (SFC) macroeconomic models. The godley R package offers tools to dynamically define model structures by adding variables and specifying governing systems of equations. With it, users can analyze how different macroeconomic structures affect key variables, perform parameter sensitivity analyses, introduce policy shocks, and visualize resulting economic scenarios. The accounting structure of SFC models follows the approach outlined in the seminal study by Godley and Lavoie (2007, ISBN:978-1-137-08599-3), ensuring a comprehensive integration of all economic flows and stocks. The algorithms implemented to solve the models are based on methodologies from Kinsella and O'Shea (2010) <doi:10.2139/ssrn.1729205>, Peressini and Sullivan (1988, ISBN:0-387-96614-5), and contributions by Joao Macalos.",
    "version": "0.2.2",
    "maintainer": "El\u017cbieta Jowik <jowik.elzbieta@gmail.com>",
    "author": "Micha\u0142 Gamrot [aut, cph],\n  Iwo Augusty\u0144ski [ctb],\n  Julian Kacprzak [ctb],\n  El\u017cbieta Jowik [cre, ctb]",
    "url": "https://gamrot.github.io/godley/",
    "bug_reports": "https://github.com/gamrot/godley/issues",
    "repository": "https://cran.r-project.org/package=godley",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "godley Stock-Flow-Consistent Model Simulator Define, simulate, and validate stock-flow consistent (SFC) macroeconomic models. The godley R package offers tools to dynamically define model structures by adding variables and specifying governing systems of equations. With it, users can analyze how different macroeconomic structures affect key variables, perform parameter sensitivity analyses, introduce policy shocks, and visualize resulting economic scenarios. The accounting structure of SFC models follows the approach outlined in the seminal study by Godley and Lavoie (2007, ISBN:978-1-137-08599-3), ensuring a comprehensive integration of all economic flows and stocks. The algorithms implemented to solve the models are based on methodologies from Kinsella and O'Shea (2010) <doi:10.2139/ssrn.1729205>, Peressini and Sullivan (1988, ISBN:0-387-96614-5), and contributions by Joao Macalos.  "
  },
  {
    "id": 13627,
    "package_name": "greeks",
    "title": "Sensitivities of Prices of Financial Options and Implied\nVolatilities",
    "description": "Methods to calculate sensitivities of financial option prices for\n European, geometric and arithmetic Asian, and American options, with various\n payoff functions in the Black Scholes model, and in more general jump diffusion\n models. A shiny app to interactively plot the results is included. Furthermore,\n methods to compute implied volatilities are provided for a wide range of option\n types and  custom payoff functions. Classical formulas are implemented for\n European options in the Black Scholes Model, as is presented in Hull, J. C.\n (2017), Options, Futures, and Other Derivatives.\n In the case of Asian options, Malliavin Monte Carlo Greeks are implemented, see\n Hudde, A. & R\u00fcschendorf, L. (2023). European and Asian Greeks for exponential\n L\u00e9vy processes. <doi:10.1007/s11009-023-10014-5>. For American\n options, the Binomial Tree  Method is implemented, as is presented in Hull,\n J. C. (2017). ",
    "version": "1.5.1",
    "maintainer": "Anselm Hudde <anselmhudde@gmx.de>",
    "author": "Anselm Hudde [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5652-2815>)",
    "url": "https://github.com/ahudde/greeks",
    "bug_reports": "https://github.com/ahudde/greeks/issues",
    "repository": "https://cran.r-project.org/package=greeks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "greeks Sensitivities of Prices of Financial Options and Implied\nVolatilities Methods to calculate sensitivities of financial option prices for\n European, geometric and arithmetic Asian, and American options, with various\n payoff functions in the Black Scholes model, and in more general jump diffusion\n models. A shiny app to interactively plot the results is included. Furthermore,\n methods to compute implied volatilities are provided for a wide range of option\n types and  custom payoff functions. Classical formulas are implemented for\n European options in the Black Scholes Model, as is presented in Hull, J. C.\n (2017), Options, Futures, and Other Derivatives.\n In the case of Asian options, Malliavin Monte Carlo Greeks are implemented, see\n Hudde, A. & R\u00fcschendorf, L. (2023). European and Asian Greeks for exponential\n L\u00e9vy processes. <doi:10.1007/s11009-023-10014-5>. For American\n options, the Binomial Tree  Method is implemented, as is presented in Hull,\n J. C. (2017).   "
  },
  {
    "id": 13804,
    "package_name": "haplo.ccs",
    "title": "Estimate Haplotype Relative Risks in Case-Control Data",
    "description": "Haplotype and covariate relative risks in case-control data are estimated by weighted logistic regression. Diplotype probabilities, which are estimated by EM computation with progressive insertion of loci, are utilized as weights. French et al. (2006) <doi:10.1002/gepi.20161>.",
    "version": "1.3.3",
    "maintainer": "Benjamin French <b.french@vumc.org>",
    "author": "Benjamin French [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-9265-5378>),\n  Shawn Garbett [ctb] (ORCID: <https://orcid.org/0000-0003-4079-5621>),\n  Thomas Lumley [aut]",
    "url": "https://github.com/vubiostat/haplo.ccs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=haplo.ccs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "haplo.ccs Estimate Haplotype Relative Risks in Case-Control Data Haplotype and covariate relative risks in case-control data are estimated by weighted logistic regression. Diplotype probabilities, which are estimated by EM computation with progressive insertion of loci, are utilized as weights. French et al. (2006) <doi:10.1002/gepi.20161>.  "
  },
  {
    "id": 13828,
    "package_name": "hbbr",
    "title": "Hierarchical Bayesian Benefit-Risk Assessment Using Discrete\nChoice Experiment",
    "description": "Implements assessment of benefit-risk balance using Bayesian Discrete Choice Experiment. For more details see the article by Mukhopadhyay et al. (2019) <DOI:10.1080/19466315.2018.1527248>. ",
    "version": "1.1.2",
    "maintainer": "Saurabh Mukhopadhyay <stat.mukherjee@gmail.com>",
    "author": "Saurabh Mukhopadhyay [aut] (AbbVie Inc.),\n  Saurabh Mukhopadhyay [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hbbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hbbr Hierarchical Bayesian Benefit-Risk Assessment Using Discrete\nChoice Experiment Implements assessment of benefit-risk balance using Bayesian Discrete Choice Experiment. For more details see the article by Mukhopadhyay et al. (2019) <DOI:10.1080/19466315.2018.1527248>.   "
  },
  {
    "id": 13829,
    "package_name": "hbim",
    "title": "Hill/Bliss Independence Model for Combination Vaccines",
    "description": "Calculate expected relative risk and proportion protected assuming normally distributed log10 transformed antibody dose for a several component vaccine. Uses Hill models for each component which are combined under Bliss independence. See Saul and Fay, 2007 <DOI:10.1371/journal.pone.0000850>. ",
    "version": "1.1.2",
    "maintainer": "Michael P. Fay <mfay@niaid.nih.gov>",
    "author": "Michael P. Fay",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hbim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hbim Hill/Bliss Independence Model for Combination Vaccines Calculate expected relative risk and proportion protected assuming normally distributed log10 transformed antibody dose for a several component vaccine. Uses Hill models for each component which are combined under Bliss independence. See Saul and Fay, 2007 <DOI:10.1371/journal.pone.0000850>.   "
  },
  {
    "id": 13876,
    "package_name": "healthfinance",
    "title": "Financial Projections and Planning for Health Care Practices",
    "description": "Provides a shiny interface for a free, open-source managerial\n    accounting-like system for health care practices. This package allows\n    health care administrators to project revenue with monthly adjustments\n    and procedure-specific boosts up to a 3-year period. Granular data\n    (patient-level) to aggregated data (department- or hospital-level) can\n    all be used as valid inputs provided historical volume and revenue\n    data is available. For more details on managerial accounting\n    techniques, see Brewer et al. (2015, ISBN:9780078025792).",
    "version": "0.1.0",
    "maintainer": "Raoul Wadhwa <raoulwadhwa@gmail.com>",
    "author": "Raoul Wadhwa [aut, cre],\n  Vigneshwar Subramanian [aut],\n  Milind Desai [aut]",
    "url": "https://rrrlw.github.io/healthfinance/",
    "bug_reports": "https://github.com/rrrlw/healthfinance/issues",
    "repository": "https://cran.r-project.org/package=healthfinance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "healthfinance Financial Projections and Planning for Health Care Practices Provides a shiny interface for a free, open-source managerial\n    accounting-like system for health care practices. This package allows\n    health care administrators to project revenue with monthly adjustments\n    and procedure-specific boosts up to a 3-year period. Granular data\n    (patient-level) to aggregated data (department- or hospital-level) can\n    all be used as valid inputs provided historical volume and revenue\n    data is available. For more details on managerial accounting\n    techniques, see Brewer et al. (2015, ISBN:9780078025792).  "
  },
  {
    "id": 13877,
    "package_name": "healthiar",
    "title": "Quantify and Monetize the Burden of Disease Attributable to\nExposure",
    "description": "This R package has been developed with a focus on air pollution and noise but can applied to other exposures. The initial development has been funded by the European Union project BEST-COST. Disclaimer: It is work in progress and the developers are not liable for any calculation errors or inaccuracies resulting from the use of this package.\n References (in chronological order): \n WHO (2003a) \"Assessing the environmental burden of disease at national and local levels\" <https://www.who.int/publications/i/item/9241546204> (accessed October 2025);\n WHO (2003b) \"Comparative quantification of health risks: Conceptual framework and methodological issues\" <doi:10.1186/1478-7954-1-1> (accessed October 2025);\n Miller & Hurley (2003) \"Life table methods for quantitative impact assessments in chronic mortality\" <doi:10.1136/jech.57.3.200> (accessed October 2025);\n Steenland & Armstrong (2006) \"An Overview of Methods for Calculating the Burden of Disease Due to Specific Risk Factors\" <doi:10.1097/01.ede.0000229155.05644.43> (accessed October 2025);\n Miller (2010) \"Report on estimation of mortality impacts of particulate air pollution in London\" <https://cleanair.london/app/uploads/CAL-098-Mayors-health-study-report-June-2010-1.pdf> (accessed October 2025);\n WHO (2011) \"Burden of disease from environmental noise\" <https://iris.who.int/items/723ab97c-5c33-4e3b-8df1-744aa5bc1c27> (accessed October 2025);\n Jerrett et al. (2013) \"Spatial Analysis of Air Pollution and Mortality in California\" <doi:10.1164/rccm.201303-0609OC> (accessed October 2025);\n GBD 2019 Risk Factors Collaborators (2020) \"Global burden of 87 risk factors in 204 countries and territories, 1990\u20132019\" <doi:10.1016/S0140-6736(20)30752-2> (accessed October 2025);\n VanderWeele (2019) \"Optimal Approximate Conversions of Odds Ratios and Hazard Ratios to Risk Ratios\" <doi: 10.1111/biom.13197> (accessed October 2025);\n WHO (2020) \"Health impact assessment of air pollution: AirQ+ life table manual\" <https://iris.who.int/bitstream/handle/10665/337683/WHO-EURO-2020-1559-41310-56212-eng.pdf?sequence=1> (accessed October 2025);\n ETC HE (2022) \"Health risk assessment of air pollution and the impact of the new WHO guidelines\" <https://www.eionet.europa.eu/etcs/all-etc-reports> (accessed October 2025);\n Kim et al. (2022) \"DALY Estimation Approaches: Understanding and Using the Incidence-based Approach and the Prevalence-based Approach\" <doi:10.3961/jpmph.21.597> (accessed October 2025);\n Pozzer et al. (2022) \"Mortality Attributable to Ambient Air Pollution: A Review of Global Estimates\" <doi:10.1029/2022GH000711> (accessed October 2025);\n Teaching group in EBM (2022) \"Evidence-based medicine research helper\" <https://ebm-helper.cn/en/Conv/HR_RR.html> (accessed October 2025).",
    "version": "0.2.1",
    "maintainer": "Alberto Castro <alberto.castrofernandez@swisstph.ch>",
    "author": "Alberto Castro [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4665-3299>),\n  Axel Luyten [aut] (ORCID: <https://orcid.org/0000-0002-7005-5889>),\n  Arno Pauwels [ctb] (ORCID: <https://orcid.org/0000-0001-7519-8080>),\n  Liliana Vazquez Fernandez [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3778-9415>),\n  Vanessa Gorasso [ctb] (ORCID: <https://orcid.org/0000-0001-6884-9316>),\n  Carl Michael Baravelli [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7772-5315>),\n  Susanne Breitner [ctb] (ORCID: <https://orcid.org/0000-0002-0956-6911>),\n  Maria Lepnurm [ctb] (ORCID: <https://orcid.org/0009-0009-4372-6227>),\n  Maria Jose Rueda Lopez [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2443-1038>),\n  Iracy Pimenta [ctb] (ORCID: <https://orcid.org/0000-0003-0032-1536>),\n  Andreia Novais [ctb] (ORCID: <https://orcid.org/0009-0007-7775-108X>),\n  Ana Barbosa [ctb] (ORCID: <https://orcid.org/0000-0002-9623-9002>),\n  Joao Vasco Santos [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4696-1002>),\n  Anette Kocbach Bolling [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4209-7448>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=healthiar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "healthiar Quantify and Monetize the Burden of Disease Attributable to\nExposure This R package has been developed with a focus on air pollution and noise but can applied to other exposures. The initial development has been funded by the European Union project BEST-COST. Disclaimer: It is work in progress and the developers are not liable for any calculation errors or inaccuracies resulting from the use of this package.\n References (in chronological order): \n WHO (2003a) \"Assessing the environmental burden of disease at national and local levels\" <https://www.who.int/publications/i/item/9241546204> (accessed October 2025);\n WHO (2003b) \"Comparative quantification of health risks: Conceptual framework and methodological issues\" <doi:10.1186/1478-7954-1-1> (accessed October 2025);\n Miller & Hurley (2003) \"Life table methods for quantitative impact assessments in chronic mortality\" <doi:10.1136/jech.57.3.200> (accessed October 2025);\n Steenland & Armstrong (2006) \"An Overview of Methods for Calculating the Burden of Disease Due to Specific Risk Factors\" <doi:10.1097/01.ede.0000229155.05644.43> (accessed October 2025);\n Miller (2010) \"Report on estimation of mortality impacts of particulate air pollution in London\" <https://cleanair.london/app/uploads/CAL-098-Mayors-health-study-report-June-2010-1.pdf> (accessed October 2025);\n WHO (2011) \"Burden of disease from environmental noise\" <https://iris.who.int/items/723ab97c-5c33-4e3b-8df1-744aa5bc1c27> (accessed October 2025);\n Jerrett et al. (2013) \"Spatial Analysis of Air Pollution and Mortality in California\" <doi:10.1164/rccm.201303-0609OC> (accessed October 2025);\n GBD 2019 Risk Factors Collaborators (2020) \"Global burden of 87 risk factors in 204 countries and territories, 1990\u20132019\" <doi:10.1016/S0140-6736(20)30752-2> (accessed October 2025);\n VanderWeele (2019) \"Optimal Approximate Conversions of Odds Ratios and Hazard Ratios to Risk Ratios\" <doi: 10.1111/biom.13197> (accessed October 2025);\n WHO (2020) \"Health impact assessment of air pollution: AirQ+ life table manual\" <https://iris.who.int/bitstream/handle/10665/337683/WHO-EURO-2020-1559-41310-56212-eng.pdf?sequence=1> (accessed October 2025);\n ETC HE (2022) \"Health risk assessment of air pollution and the impact of the new WHO guidelines\" <https://www.eionet.europa.eu/etcs/all-etc-reports> (accessed October 2025);\n Kim et al. (2022) \"DALY Estimation Approaches: Understanding and Using the Incidence-based Approach and the Prevalence-based Approach\" <doi:10.3961/jpmph.21.597> (accessed October 2025);\n Pozzer et al. (2022) \"Mortality Attributable to Ambient Air Pollution: A Review of Global Estimates\" <doi:10.1029/2022GH000711> (accessed October 2025);\n Teaching group in EBM (2022) \"Evidence-based medicine research helper\" <https://ebm-helper.cn/en/Conv/HR_RR.html> (accessed October 2025).  "
  },
  {
    "id": 13966,
    "package_name": "highOrderPortfolios",
    "title": "Design of High-Order Portfolios Including Skewness and Kurtosis",
    "description": "The classical Markowitz's mean-variance portfolio formulation ignores \n    heavy tails and skewness. High-order portfolios use higher order moments to\n    better characterize the return distribution. Different formulations and fast \n    algorithms are proposed for high-order portfolios based on the mean, variance, \n    skewness, and kurtosis.\n    The package is based on the papers:\n    R. Zhou and D. P. Palomar (2021). \"Solving High-Order Portfolios via \n    Successive Convex Approximation Algorithms.\" <arXiv:2008.00863>.\n    X. Wang, R. Zhou, J. Ying, and D. P. Palomar (2022). \"Efficient and Scalable \n    High-Order Portfolios Design via Parametric Skew-t Distribution.\" <arXiv:2206.02412>.",
    "version": "0.1.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Daniel P. Palomar [cre, aut],\n  Rui Zhou [aut],\n  Xiwen Wang [aut]",
    "url": "https://github.com/dppalomar/highOrderPortfolios,\nhttps://www.danielppalomar.com",
    "bug_reports": "https://github.com/dppalomar/highOrderPortfolios/issues",
    "repository": "https://cran.r-project.org/package=highOrderPortfolios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highOrderPortfolios Design of High-Order Portfolios Including Skewness and Kurtosis The classical Markowitz's mean-variance portfolio formulation ignores \n    heavy tails and skewness. High-order portfolios use higher order moments to\n    better characterize the return distribution. Different formulations and fast \n    algorithms are proposed for high-order portfolios based on the mean, variance, \n    skewness, and kurtosis.\n    The package is based on the papers:\n    R. Zhou and D. P. Palomar (2021). \"Solving High-Order Portfolios via \n    Successive Convex Approximation Algorithms.\" <arXiv:2008.00863>.\n    X. Wang, R. Zhou, J. Ying, and D. P. Palomar (2022). \"Efficient and Scalable \n    High-Order Portfolios Design via Parametric Skew-t Distribution.\" <arXiv:2206.02412>.  "
  },
  {
    "id": 13970,
    "package_name": "highfrequency",
    "title": "Tools for Highfrequency Data Analysis",
    "description": "Provide functionality to manage, clean and match highfrequency\n    trades and quotes data, calculate various liquidity measures, estimate and\n    forecast volatility, detect price jumps and investigate microstructure noise and intraday\n    periodicity. A detailed vignette can be found in the open-access paper \n    \"Analyzing Intraday Financial Data in R: The highfrequency Package\" \n    by Boudt, Kleen, and Sjoerup (2022, <doi:10.18637/jss.v104.i08>). ",
    "version": "1.0.2",
    "maintainer": "Kris Boudt <kris.boudt@ugent.be>",
    "author": "Kris Boudt [aut, cre] (ORCID: <https://orcid.org/0000-0002-1000-5142>),\n  Jonathan Cornelissen [aut],\n  Scott Payseur [aut],\n  Giang Nguyen [ctb],\n  Onno Kleen [aut] (ORCID: <https://orcid.org/0000-0003-4731-4640>),\n  Emil Sjoerup [aut]",
    "url": "https://github.com/jonathancornelissen/highfrequency",
    "bug_reports": "https://github.com/jonathancornelissen/highfrequency/issues",
    "repository": "https://cran.r-project.org/package=highfrequency",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highfrequency Tools for Highfrequency Data Analysis Provide functionality to manage, clean and match highfrequency\n    trades and quotes data, calculate various liquidity measures, estimate and\n    forecast volatility, detect price jumps and investigate microstructure noise and intraday\n    periodicity. A detailed vignette can be found in the open-access paper \n    \"Analyzing Intraday Financial Data in R: The highfrequency Package\" \n    by Boudt, Kleen, and Sjoerup (2022, <doi:10.18637/jss.v104.i08>).   "
  },
  {
    "id": 13977,
    "package_name": "highriskzone",
    "title": "Determining and Evaluating High-Risk Zones",
    "description": "Functions for determining and evaluating high-risk zones and\n    simulating and thinning point process data, as described in 'Determining\n    high risk zones using point process methodology - Realization by building\n    an R package' Seibold (2012) <http://highriskzone.r-forge.r-project.org/Bachelorarbeit.pdf> \n    and 'Determining high-risk zones for unexploded World War II bombs by using point \n    process methodology', Mahling et al. (2013) <doi:10.1111/j.1467-9876.2012.01055.x>.",
    "version": "1.4.9",
    "maintainer": "Rickmer Schulte <R.Schulte@campus.lmu.de>",
    "author": "Heidi Seibold <Heidi.Seibold@uzh.ch>, Monia Mahling\n    <monia.mahling@stat.uni-muenchen.de>, Sebastian Linne\n    <Sebastian.Linne@campus.lmu.de>, Felix Guenther\n    <felix.guenther@stat.uni-muenchen.de>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=highriskzone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highriskzone Determining and Evaluating High-Risk Zones Functions for determining and evaluating high-risk zones and\n    simulating and thinning point process data, as described in 'Determining\n    high risk zones using point process methodology - Realization by building\n    an R package' Seibold (2012) <http://highriskzone.r-forge.r-project.org/Bachelorarbeit.pdf> \n    and 'Determining high-risk zones for unexploded World War II bombs by using point \n    process methodology', Mahling et al. (2013) <doi:10.1111/j.1467-9876.2012.01055.x>.  "
  },
  {
    "id": 14060,
    "package_name": "hrcomprisk",
    "title": "Nonparametric Assessment Between Competing Risks Hazard Ratios",
    "description": "Nonparametric cumulative-incidence based estimation of the ratios of sub-hazard ratios to cause-specific hazard ratios using the approach from Ng et al. (2020).",
    "version": "0.1.1",
    "maintainer": "Daniel Antiporta <dantiporta@jhu.edu>",
    "author": "Daniel Antiporta <dantiporta@jhu.edu>; \n        Matthew Matheson <mmathes4@jhu.edu>;\n        Derek Ng <dng@jhu.edu>;\n        Alvaro Munoz <amunoz@jhu.edu>",
    "url": "https://github.com/AntiportaD/hrcomprisk",
    "bug_reports": "https://github.com/AntiportaD/hrcomprisk/issues",
    "repository": "https://cran.r-project.org/package=hrcomprisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hrcomprisk Nonparametric Assessment Between Competing Risks Hazard Ratios Nonparametric cumulative-incidence based estimation of the ratios of sub-hazard ratios to cause-specific hazard ratios using the approach from Ng et al. (2020).  "
  },
  {
    "id": 14065,
    "package_name": "hscovar",
    "title": "Calculation of Covariance Between Markers for Half-Sib Families",
    "description": "The theoretical covariance between pairs of markers is calculated\n    from either paternal haplotypes and maternal linkage disequilibrium (LD) or \n    vise versa. A genetic map is required. Grouping of markers is based on the \n    correlation matrix and a representative marker is suggested for each group.\n    Employing the correlation matrix, optimal sample size can be derived for \n    association studies based on a SNP-BLUP approach.\n    The implementation relies on paternal half-sib families and biallelic \n    markers. If maternal half-sib families are used, the roles of sire/dam are \n    swapped. Multiple families can be considered.\n    Wittenburg, Bonk, Doschoris, Reyer (2020) \"Design of Experiments for \n    Fine-Mapping Quantitative Trait Loci in Livestock Populations\" \n    <doi:10.1186/s12863-020-00871-1>.\n    Carlson, Eberle, Rieder, Yi, Kruglyak, Nickerson (2004) \"Selecting a \n    maximally informative set of single-nucleotide polymorphisms for association\n    analyses using linkage disequilibrium\" <doi:10.1086/381000>.",
    "version": "0.4.2",
    "maintainer": "D\u00f6rte Wittenburg <wittenburg@fbn-dummerstorf.de>",
    "author": "D\u00f6rte Wittenburg [aut, cre],\n  Michael Doschoris [aut],\n  Jan Klosa [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hscovar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hscovar Calculation of Covariance Between Markers for Half-Sib Families The theoretical covariance between pairs of markers is calculated\n    from either paternal haplotypes and maternal linkage disequilibrium (LD) or \n    vise versa. A genetic map is required. Grouping of markers is based on the \n    correlation matrix and a representative marker is suggested for each group.\n    Employing the correlation matrix, optimal sample size can be derived for \n    association studies based on a SNP-BLUP approach.\n    The implementation relies on paternal half-sib families and biallelic \n    markers. If maternal half-sib families are used, the roles of sire/dam are \n    swapped. Multiple families can be considered.\n    Wittenburg, Bonk, Doschoris, Reyer (2020) \"Design of Experiments for \n    Fine-Mapping Quantitative Trait Loci in Livestock Populations\" \n    <doi:10.1186/s12863-020-00871-1>.\n    Carlson, Eberle, Rieder, Yi, Kruglyak, Nickerson (2004) \"Selecting a \n    maximally informative set of single-nucleotide polymorphisms for association\n    analyses using linkage disequilibrium\" <doi:10.1086/381000>.  "
  },
  {
    "id": 14173,
    "package_name": "iClick",
    "title": "A Button-Based GUI for Financial and Economic Data Analysis",
    "description": "A GUI designed to support the analysis of financial-economic time\n    series data.",
    "version": "1.6",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "author": "Ho Tsung-wu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iClick",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iClick A Button-Based GUI for Financial and Economic Data Analysis A GUI designed to support the analysis of financial-economic time\n    series data.  "
  },
  {
    "id": 14195,
    "package_name": "iPRSue",
    "title": "Individual Polygenic Risk Score Uncertainty Estimation",
    "description": "Provides tools for estimating uncertainty in individual \n    polygenic risk scores (PRSs) using both sampling-based and analytical \n    methods, as well as the Best Linear Unbiased Estimator (BLUE). \n    These methods quantify variability in PRS estimates for both binary \n    and quantitative traits. \n    See Henderson (1975) <doi:10.2307/2529430> for more details.",
    "version": "1.0.0",
    "maintainer": "Dovini Jayasinghe <dovini.jayasinghe@mymail.unisa.edu.au>",
    "author": "Dovini Jayasinghe [aut, cre, cph],\n  Hong Lee [aut, cph]",
    "url": "https://github.com/DoviniJ/iPRSue",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iPRSue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iPRSue Individual Polygenic Risk Score Uncertainty Estimation Provides tools for estimating uncertainty in individual \n    polygenic risk scores (PRSs) using both sampling-based and analytical \n    methods, as well as the Best Linear Unbiased Estimator (BLUE). \n    These methods quantify variability in PRS estimates for both binary \n    and quantitative traits. \n    See Henderson (1975) <doi:10.2307/2529430> for more details.  "
  },
  {
    "id": 14255,
    "package_name": "icesSAG",
    "title": "Stock Assessment Graphs Database Web Services",
    "description": "R interface to access the web services of the ICES Stock Assessment\n             Graphs database <https://sg.ices.dk>.",
    "version": "1.6.2",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre],\n  Scott Large [aut],\n  Arni Magnusson [aut],\n  Carlos Pinto [aut],\n  Laura Andreea Petre [aut]",
    "url": "https://sg.ices.dk, https://github.com/ices-tools-prod/icesSAG",
    "bug_reports": "https://github.com/ices-tools-prod/icesSAG/issues",
    "repository": "https://cran.r-project.org/package=icesSAG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesSAG Stock Assessment Graphs Database Web Services R interface to access the web services of the ICES Stock Assessment\n             Graphs database <https://sg.ices.dk>.  "
  },
  {
    "id": 14256,
    "package_name": "icesSD",
    "title": "Stock Database Web Services",
    "description": "R interface to access the web services of the ICES Stock Database <https://sd.ices.dk>.",
    "version": "2.1.0",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre],\n  Scott Large [aut],\n  Arni Magnusson [aut]",
    "url": "https://sd.ices.dk, https://github.com/ices-tools-prod/icesSD",
    "bug_reports": "https://github.com/ices-tools-prod/icesSD/issues",
    "repository": "https://cran.r-project.org/package=icesSD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesSD Stock Database Web Services R interface to access the web services of the ICES Stock Database <https://sd.ices.dk>.  "
  },
  {
    "id": 14304,
    "package_name": "igfetchr",
    "title": "Access the 'IG Trading REST API'",
    "description": "Provides functions to fetch market data, search historical prices, execute trades, and get account details from the 'IG Trading REST API' <https://labs.ig.com>. Returns tidy tibbles for easy analysis. Trading contracts for difference (CFDs), options and spread bets carries a high risk of losing money. This package is not financial or trading advice.",
    "version": "0.1.0",
    "maintainer": "Saw Simeon <saw.s@ku.th>",
    "author": "Saw Simeon [aut, cre]",
    "url": "https://github.com/sawsimeon/igfetchr,\nhttps://sawsimeon.github.io/igfetchr/",
    "bug_reports": "https://github.com/sawsimeon/igfetchr/issues",
    "repository": "https://cran.r-project.org/package=igfetchr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "igfetchr Access the 'IG Trading REST API' Provides functions to fetch market data, search historical prices, execute trades, and get account details from the 'IG Trading REST API' <https://labs.ig.com>. Returns tidy tibbles for easy analysis. Trading contracts for difference (CFDs), options and spread bets carries a high risk of losing money. This package is not financial or trading advice.  "
  },
  {
    "id": 14338,
    "package_name": "imaginator",
    "title": "Simulate General Insurance Policies and Losses",
    "description": "Simulate general insurance policies, losses and loss emergence. The functions contemplate \n  deterministic and stochastic policy retention and growth scenarios. Retention and growth rates are percentages relative\n  to the expiring portfolio. Claims are simulated for each policy. This is accomplished either be assuming a frequency\n  distribution per development lag or by generating random wait times until claim emergence and settlement. Loss simulation \n  uses standard loss distributions for claim amounts.",
    "version": "1.0.0",
    "maintainer": "Brian Fannin <bfannin@casact.org>",
    "author": "Brian Fannin [aut, cre]",
    "url": "https://github.com/casact/imaginator",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=imaginator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imaginator Simulate General Insurance Policies and Losses Simulate general insurance policies, losses and loss emergence. The functions contemplate \n  deterministic and stochastic policy retention and growth scenarios. Retention and growth rates are percentages relative\n  to the expiring portfolio. Claims are simulated for each policy. This is accomplished either be assuming a frequency\n  distribution per development lag or by generating random wait times until claim emergence and settlement. Loss simulation \n  uses standard loss distributions for claim amounts.  "
  },
  {
    "id": 14371,
    "package_name": "imputeFin",
    "title": "Imputation of Financial Time Series with Missing Values and/or\nOutliers",
    "description": "Missing values often occur in financial data due to a variety \n    of reasons (errors in the collection process or in the processing stage, \n    lack of asset liquidity, lack of reporting of funds, etc.). However, \n    most data analysis methods expect complete data and cannot be employed \n    with missing values. One convenient way to deal with this issue without \n    having to redesign the data analysis method is to impute the missing \n    values. This package provides an efficient way to impute the missing \n    values based on modeling the time series with a random walk or an \n    autoregressive (AR) model, convenient to model log-prices and log-volumes \n    in financial data. In the current version, the imputation is \n    univariate-based (so no asset correlation is used). In addition,\n    outliers can be detected and removed.\n    The package is based on the paper:\n    J. Liu, S. Kumar, and D. P. Palomar (2019). Parameter Estimation of \n    Heavy-Tailed AR Model With Missing Data Via Stochastic EM. IEEE Trans. on \n    Signal Processing, vol. 67, no. 8, pp. 2159-2172. <doi:10.1109/TSP.2019.2899816>.",
    "version": "0.1.2",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Daniel P. Palomar [cre, aut],\n  Junyan Liu [aut],\n  Rui Zhou [aut]",
    "url": "https://CRAN.R-project.org/package=imputeFin,\nhttps://github.com/dppalomar/imputeFin,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2019.2899816,\nhttps://doi.org/10.1109/TSP.2020.3033378",
    "bug_reports": "https://github.com/dppalomar/imputeFin/issues",
    "repository": "https://cran.r-project.org/package=imputeFin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imputeFin Imputation of Financial Time Series with Missing Values and/or\nOutliers Missing values often occur in financial data due to a variety \n    of reasons (errors in the collection process or in the processing stage, \n    lack of asset liquidity, lack of reporting of funds, etc.). However, \n    most data analysis methods expect complete data and cannot be employed \n    with missing values. One convenient way to deal with this issue without \n    having to redesign the data analysis method is to impute the missing \n    values. This package provides an efficient way to impute the missing \n    values based on modeling the time series with a random walk or an \n    autoregressive (AR) model, convenient to model log-prices and log-volumes \n    in financial data. In the current version, the imputation is \n    univariate-based (so no asset correlation is used). In addition,\n    outliers can be detected and removed.\n    The package is based on the paper:\n    J. Liu, S. Kumar, and D. P. Palomar (2019). Parameter Estimation of \n    Heavy-Tailed AR Model With Missing Data Via Stochastic EM. IEEE Trans. on \n    Signal Processing, vol. 67, no. 8, pp. 2159-2172. <doi:10.1109/TSP.2019.2899816>.  "
  },
  {
    "id": 14440,
    "package_name": "injurytools",
    "title": "A Toolkit for Sports Injury and Illness Data Analysis",
    "description": "Sports Injury Data analysis aims to identify and describe the\n    magnitude of the injury problem, and to gain more insights (e.g.\n    determine potential risk factors) by statistical modelling approaches.\n    The 'injurytools' package provides standardized routines and utilities\n    that simplify such analyses. It offers functions for data preparation,\n    informative visualizations and descriptive and model-based analyses.",
    "version": "2.0.0",
    "maintainer": "Lore Zumeta Olaskoaga <lorezumeta@gmail.com>",
    "author": "Lore Zumeta Olaskoaga [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6141-1469>),\n  Dae-Jin Lee [ctb] (ORCID: <https://orcid.org/0000-0002-8995-8535>)",
    "url": "https://github.com/lzumeta/injurytools,\nhttps://lzumeta.github.io/injurytools/",
    "bug_reports": "https://github.com/lzumeta/injurytools/issues",
    "repository": "https://cran.r-project.org/package=injurytools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "injurytools A Toolkit for Sports Injury and Illness Data Analysis Sports Injury Data analysis aims to identify and describe the\n    magnitude of the injury problem, and to gain more insights (e.g.\n    determine potential risk factors) by statistical modelling approaches.\n    The 'injurytools' package provides standardized routines and utilities\n    that simplify such analyses. It offers functions for data preparation,\n    informative visualizations and descriptive and model-based analyses.  "
  },
  {
    "id": 14465,
    "package_name": "insuranceData",
    "title": "A Collection of Insurance Datasets Useful in Risk Classification\nin Non-life Insurance",
    "description": "Insurance datasets, which are often used in claims severity and claims frequency modelling. It helps testing new regression models in those problems, such as GLM, GLMM, HGLM, non-linear mixed models etc. Most of the data sets are applied in the project \"Mixed models in ratemaking\" supported by grant NN 111461540 from Polish National Science Center.    ",
    "version": "1.0",
    "maintainer": "Alicja Wolny--Dominiak <alicja.wolny-dominiak@ue.katowice.pl>",
    "author": "Alicja Wolny--Dominiak and Michal Trzesiok",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=insuranceData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "insuranceData A Collection of Insurance Datasets Useful in Risk Classification\nin Non-life Insurance Insurance datasets, which are often used in claims severity and claims frequency modelling. It helps testing new regression models in those problems, such as GLM, GLMM, HGLM, non-linear mixed models etc. Most of the data sets are applied in the project \"Mixed models in ratemaking\" supported by grant NN 111461540 from Polish National Science Center.      "
  },
  {
    "id": 14466,
    "package_name": "insurancerating",
    "title": "Analytic Insurance Rating Techniques",
    "description": "Functions to build, evaluate, and visualize insurance rating \n    models. It simplifies the process of modeling premiums, and allows to \n    analyze insurance risk factors effectively. The package employs a \n    data-driven strategy for constructing insurance tariff classes, drawing on \n    the work of Antonio and Valdez (2012) <doi:10.1007/s10182-011-0152-7>.",
    "version": "0.7.5",
    "maintainer": "Martin Haringa <mtharinga@gmail.com>",
    "author": "Martin Haringa [aut, cre]",
    "url": "https://mharinga.github.io/insurancerating/,\nhttps://github.com/MHaringa/insurancerating",
    "bug_reports": "https://github.com/MHaringa/insurancerating/issues",
    "repository": "https://cran.r-project.org/package=insurancerating",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "insurancerating Analytic Insurance Rating Techniques Functions to build, evaluate, and visualize insurance rating \n    models. It simplifies the process of modeling premiums, and allows to \n    analyze insurance risk factors effectively. The package employs a \n    data-driven strategy for constructing insurance tariff classes, drawing on \n    the work of Antonio and Valdez (2012) <doi:10.1007/s10182-011-0152-7>.  "
  },
  {
    "id": 14472,
    "package_name": "intccr",
    "title": "Semiparametric Competing Risks Regression under Interval\nCensoring",
    "description": "Semiparametric regression models on the cumulative incidence function for interval-censored competing risks data as described in Bakoyannis, Yu, & Yiannoutsos (2017) /doi{10.1002/sim.7350} and the models with missing event types as described in Park, Bakoyannis, Zhang, & Yiannoutsos (2021) \\doi{10.1093/biostatistics/kxaa052}. The proportional subdistribution hazards model (Fine-Gray model), the proportional odds model, and other models that belong to the class of semiparametric generalized odds rate transformation models.",
    "version": "3.0.4",
    "maintainer": "Jun Park <jun.park@alumni.iu.edu>",
    "author": "Giorgos Bakoyannis <gbakogia@iu.edu>, Jun Park <jun.park@alumni.iu.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intccr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intccr Semiparametric Competing Risks Regression under Interval\nCensoring Semiparametric regression models on the cumulative incidence function for interval-censored competing risks data as described in Bakoyannis, Yu, & Yiannoutsos (2017) /doi{10.1002/sim.7350} and the models with missing event types as described in Park, Bakoyannis, Zhang, & Yiannoutsos (2021) \\doi{10.1093/biostatistics/kxaa052}. The proportional subdistribution hazards model (Fine-Gray model), the proportional odds model, and other models that belong to the class of semiparametric generalized odds rate transformation models.  "
  },
  {
    "id": 14511,
    "package_name": "intradayModel",
    "title": "Modeling and Forecasting Financial Intraday Signals",
    "description": "Models, analyzes, and forecasts financial intraday signals. This package\n    currently supports a univariate state-space model for intraday trading volume provided\n    by Chen (2016) <doi:10.2139/ssrn.3101695>.",
    "version": "0.0.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Shengjie Xiu [aut],\n  Yifan Yu [aut],\n  Daniel P. Palomar [cre, aut, cph]",
    "url": "https://github.com/convexfi/intradayModel,\nhttps://www.danielppalomar.com,\nhttps://dx.doi.org/10.2139/ssrn.3101695",
    "bug_reports": "https://github.com/convexfi/intradayModel/issues",
    "repository": "https://cran.r-project.org/package=intradayModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intradayModel Modeling and Forecasting Financial Intraday Signals Models, analyzes, and forecasts financial intraday signals. This package\n    currently supports a univariate state-space model for intraday trading volume provided\n    by Chen (2016) <doi:10.2139/ssrn.3101695>.  "
  },
  {
    "id": 14513,
    "package_name": "intrinsicFRP",
    "title": "An R Package for Factor Model Asset Pricing",
    "description": "Functions for evaluating and testing asset pricing models, including\n    estimation and testing of factor risk premia, selection of \"strong\" risk \n    factors (factors having nonzero population correlation with test asset\n    returns), heteroskedasticity and autocorrelation robust covariance matrix\n    estimation and testing for model misspecification and identification. \n    The functions for estimating and testing factor risk \n    premia implement the Fama-MachBeth (1973) <doi:10.1086/260061> two-pass \n    approach, the misspecification-robust approaches of Kan-Robotti-Shanken (2013) \n    <doi:10.1111/jofi.12035>, and the approaches based on tradable factor risk\n    premia of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683>. The \n    functions for selecting the \"strong\" risk factors are based on the Oracle\n    estimator of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683> and the \n    factor screening procedure of Gospodinov-Kan-Robotti (2014) <doi:10.2139/ssrn.2579821>. \n    The functions for evaluating model misspecification implement the HJ\n    model misspecification distance of Kan-Robotti (2008) <doi:10.1016/j.jempfin.2008.03.003>,\n    which is a modification of the prominent Hansen-Jagannathan (1997)\n    <doi:10.1111/j.1540-6261.1997.tb04813.x> distance.\n    The functions for testing model identification \n    specialize the Kleibergen-Paap (2006) <doi:10.1016/j.jeconom.2005.02.011> \n    and the Chen-Fang (2019) <doi:10.1111/j.1540-6261.1997.tb04813.x> rank test \n    to the regression coefficient matrix of test asset returns on risk factors.\n    Finally, the function for heteroskedasticity and autocorrelation robust \n    covariance estimation implements the Newey-West (1994) <doi:10.2307/2297912>\n    covariance estimator.",
    "version": "2.1.0",
    "maintainer": "Alberto Quaini <alberto91quaini@gmail.com>",
    "author": "Alberto Quaini [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1251-0599>)",
    "url": "https://github.com/a91quaini/intrinsicFRP",
    "bug_reports": "https://github.com/a91quaini/intrinsicFRP/issues",
    "repository": "https://cran.r-project.org/package=intrinsicFRP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intrinsicFRP An R Package for Factor Model Asset Pricing Functions for evaluating and testing asset pricing models, including\n    estimation and testing of factor risk premia, selection of \"strong\" risk \n    factors (factors having nonzero population correlation with test asset\n    returns), heteroskedasticity and autocorrelation robust covariance matrix\n    estimation and testing for model misspecification and identification. \n    The functions for estimating and testing factor risk \n    premia implement the Fama-MachBeth (1973) <doi:10.1086/260061> two-pass \n    approach, the misspecification-robust approaches of Kan-Robotti-Shanken (2013) \n    <doi:10.1111/jofi.12035>, and the approaches based on tradable factor risk\n    premia of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683>. The \n    functions for selecting the \"strong\" risk factors are based on the Oracle\n    estimator of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683> and the \n    factor screening procedure of Gospodinov-Kan-Robotti (2014) <doi:10.2139/ssrn.2579821>. \n    The functions for evaluating model misspecification implement the HJ\n    model misspecification distance of Kan-Robotti (2008) <doi:10.1016/j.jempfin.2008.03.003>,\n    which is a modification of the prominent Hansen-Jagannathan (1997)\n    <doi:10.1111/j.1540-6261.1997.tb04813.x> distance.\n    The functions for testing model identification \n    specialize the Kleibergen-Paap (2006) <doi:10.1016/j.jeconom.2005.02.011> \n    and the Chen-Fang (2019) <doi:10.1111/j.1540-6261.1997.tb04813.x> rank test \n    to the regression coefficient matrix of test asset returns on risk factors.\n    Finally, the function for heteroskedasticity and autocorrelation robust \n    covariance estimation implements the Newey-West (1994) <doi:10.2307/2297912>\n    covariance estimator.  "
  },
  {
    "id": 14522,
    "package_name": "inventorize",
    "title": "Inventory Analytics, Pricing and Markdowns",
    "description": "Simulate inventory policies with and without forecasting, facilitate inventory analysis calculations such as  stock levels and re-order points,pricing and promotions calculations. \n  The package includes calculations of inventory metrics, stock-out calculations and ABC analysis calculations.\n    The package includes revenue management techniques such as Multi-product optimization,logit and polynomial model optimization.\n  The functions are referenced from :\n  1-Harris, Ford W. (1913). \"How many parts to make at once\". Factory, The Magazine of Management. \n  2- Nahmias, S. Production and Operations Analysis. McGraw-Hill International Edition.\n  3-Silver, E.A., Pyke, D.F., Peterson, R. Inventory Management and Production Planning and Scheduling. \n  4-Ballou, R.H. Business Logistics Management. \n  5-MIT Micromasters Program. \n  6- Columbia University  course for supply and demand analysis.\n  8- Price Elasticity of Demand MATH 104,Mark Mac Lean (with assistance from Patrick Chan) 2011W\n  For further details or correspondence :<www.linkedin.com/in/haythamomar>, <www.rescaleanalytics.com>.",
    "version": "1.1.2",
    "maintainer": "Haytham Omar <haytham@rescaleanalytics.com>",
    "author": "Haytham Omar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=inventorize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inventorize Inventory Analytics, Pricing and Markdowns Simulate inventory policies with and without forecasting, facilitate inventory analysis calculations such as  stock levels and re-order points,pricing and promotions calculations. \n  The package includes calculations of inventory metrics, stock-out calculations and ABC analysis calculations.\n    The package includes revenue management techniques such as Multi-product optimization,logit and polynomial model optimization.\n  The functions are referenced from :\n  1-Harris, Ford W. (1913). \"How many parts to make at once\". Factory, The Magazine of Management. \n  2- Nahmias, S. Production and Operations Analysis. McGraw-Hill International Edition.\n  3-Silver, E.A., Pyke, D.F., Peterson, R. Inventory Management and Production Planning and Scheduling. \n  4-Ballou, R.H. Business Logistics Management. \n  5-MIT Micromasters Program. \n  6- Columbia University  course for supply and demand analysis.\n  8- Price Elasticity of Demand MATH 104,Mark Mac Lean (with assistance from Patrick Chan) 2011W\n  For further details or correspondence :<www.linkedin.com/in/haythamomar>, <www.rescaleanalytics.com>.  "
  },
  {
    "id": 14553,
    "package_name": "ipeadatar",
    "title": "API Wrapper for 'Ipeadata'",
    "description": "Allows direct access to the macroeconomic, \n             financial and regional database maintained by \n             Brazilian Institute for Applied Economic Research ('Ipea').\n             This R package uses the 'Ipeadata' API. For more information, \n             see <http://www.ipeadata.gov.br/>.",
    "version": "0.1.6",
    "maintainer": "Luiz Eduardo S. Gomes <gomes.leduardo@gmail.com>",
    "author": "Luiz Eduardo S. Gomes [aut, cre],\n  Jessyka A. P. Goltara [ctb]",
    "url": "https://github.com/gomesleduardo/ipeadatar",
    "bug_reports": "https://github.com/gomesleduardo/ipeadatar/issues",
    "repository": "https://cran.r-project.org/package=ipeadatar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ipeadatar API Wrapper for 'Ipeadata' Allows direct access to the macroeconomic, \n             financial and regional database maintained by \n             Brazilian Institute for Applied Economic Research ('Ipea').\n             This R package uses the 'Ipeadata' API. For more information, \n             see <http://www.ipeadata.gov.br/>.  "
  },
  {
    "id": 14555,
    "package_name": "iperform",
    "title": "Time Series Performance",
    "description": "A tool to calculate the performance of a time series in a specific date or period. It is more intended for data analysis in the fields of finance, banking, telecommunications or operational marketing.",
    "version": "0.0.3",
    "maintainer": "Patrick Ilunga <patrick.ilunga@unikin.ac.cd>",
    "author": "Patrick Ilunga [aut, cre],\n  Ilunga Buabua Patrick [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iperform",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iperform Time Series Performance A tool to calculate the performance of a time series in a specific date or period. It is more intended for data analysis in the fields of finance, banking, telecommunications or operational marketing.  "
  },
  {
    "id": 14593,
    "package_name": "irtoys",
    "title": "A Collection of Functions Related to Item Response Theory (IRT)",
    "description": "A collection of functions useful in learning and practicing IRT,\n    which can be combined into larger programs. Provides basic CTT analysis,\n    a simple common interface to the estimation of item\n    parameters in IRT models for binary responses with three different programs\n    (ICL, BILOG-MG, and ltm), ability estimation (MLE, BME, EAP, WLE, plausible \n    values), item and person fit statistics, scaling methods (MM, MS, Stocking-Lord,\n    and the complete Hebaera method), and a rich array of parametric and \n    non-parametric (kernel) plots. Estimates and plots Haberman's interaction model\n    when all items are dichotomously scored.",
    "version": "0.2.2",
    "maintainer": "Ivailo Partchev <partchev@gmail.com>",
    "author": "Ivailo Partchev [aut, cre],\n  Gunter Maris [aut],\n  Tamaki Hattori [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=irtoys",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "irtoys A Collection of Functions Related to Item Response Theory (IRT) A collection of functions useful in learning and practicing IRT,\n    which can be combined into larger programs. Provides basic CTT analysis,\n    a simple common interface to the estimation of item\n    parameters in IRT models for binary responses with three different programs\n    (ICL, BILOG-MG, and ltm), ability estimation (MLE, BME, EAP, WLE, plausible \n    values), item and person fit statistics, scaling methods (MM, MS, Stocking-Lord,\n    and the complete Hebaera method), and a rich array of parametric and \n    non-parametric (kernel) plots. Estimates and plots Haberman's interaction model\n    when all items are dichotomously scored.  "
  },
  {
    "id": 14653,
    "package_name": "iucnr",
    "title": "IUCN Red List Data",
    "description": "Facilitates access to the International Union for Conservation of Nature (IUCN) Red List of Threatened Species, a comprehensive global inventory of species at risk of extinction. This package streamlines the process of determining conservation status by matching species names with Red List data, providing tools to easily query and retrieve conservation statuses. Designed to support biodiversity research and conservation planning, this package relies on data from the 'iucnrdata' package, available on GitHub <https://github.com/PaulESantos/iucnrdata>. To install the data package, use pak::pak('PaulESantos/iucnrdata').",
    "version": "0.0.0.1",
    "maintainer": "Paul Efren Santos Andrade <paulefrens@gmail.com>",
    "author": "Paul Efren Santos Andrade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6635-0375>)",
    "url": "https://github.com/PaulESantos/iucnr,\nhttps://paulesantos.github.io/iucnr/",
    "bug_reports": "https://github.com/PaulESantos/iucnr/issues",
    "repository": "https://cran.r-project.org/package=iucnr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iucnr IUCN Red List Data Facilitates access to the International Union for Conservation of Nature (IUCN) Red List of Threatened Species, a comprehensive global inventory of species at risk of extinction. This package streamlines the process of determining conservation status by matching species names with Red List data, providing tools to easily query and retrieve conservation statuses. Designed to support biodiversity research and conservation planning, this package relies on data from the 'iucnrdata' package, available on GitHub <https://github.com/PaulESantos/iucnrdata>. To install the data package, use pak::pak('PaulESantos/iucnrdata').  "
  },
  {
    "id": 14661,
    "package_name": "ivitr",
    "title": "Estimate IV-Optimal Individualized Treatment Rules",
    "description": "A method that estimates\n    an IV-optimal individualized treatment rule. An individualized\n    treatment rule is said to be IV-optimal if it minimizes the \n    maximum risk with respect to the putative IV and the set of\n    IV identification assumptions. Please refer to \n    <arXiv:2002.02579> for more details on the methodology and \n    some theory underpinning the method. Function IV-PILE() uses\n    functions in the package 'locClass'. Package 'locClass' can be\n    accessed and installed from the 'R-Forge' repository via the following link: \n    <https://r-forge.r-project.org/projects/locclass/>.\n    Alternatively, one can install the package by entering the following in R:\n    'install.packages(\"locClass\", repos=\"<http://R-Forge.R-project.org>\")'.",
    "version": "0.1.0",
    "maintainer": "Bo Zhang <bozhan@wharton.upenn.edu>",
    "author": "Bo Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ivitr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ivitr Estimate IV-Optimal Individualized Treatment Rules A method that estimates\n    an IV-optimal individualized treatment rule. An individualized\n    treatment rule is said to be IV-optimal if it minimizes the \n    maximum risk with respect to the putative IV and the set of\n    IV identification assumptions. Please refer to \n    <arXiv:2002.02579> for more details on the methodology and \n    some theory underpinning the method. Function IV-PILE() uses\n    functions in the package 'locClass'. Package 'locClass' can be\n    accessed and installed from the 'R-Forge' repository via the following link: \n    <https://r-forge.r-project.org/projects/locclass/>.\n    Alternatively, one can install the package by entering the following in R:\n    'install.packages(\"locClass\", repos=\"<http://R-Forge.R-project.org>\")'.  "
  },
  {
    "id": 14686,
    "package_name": "jadeLizardOptions",
    "title": "Trading Jade Lizard Option Strategies",
    "description": "Jade Lizard and Reverse Jade Lizard Option Strategies are presented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Russell A. Stultz (\u201cThe option strategy desk reference: an essential reference for option traders (First edition.)\u201d, 2019, ISBN: 9781949443912).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jadeLizardOptions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jadeLizardOptions Trading Jade Lizard Option Strategies Jade Lizard and Reverse Jade Lizard Option Strategies are presented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Russell A. Stultz (\u201cThe option strategy desk reference: an essential reference for option traders (First edition.)\u201d, 2019, ISBN: 9781949443912).  "
  },
  {
    "id": 14734,
    "package_name": "joineR",
    "title": "Joint Modelling of Repeated Measurements and Time-to-Event Data",
    "description": "Analysis of repeated measurements and time-to-event data via random\n    effects joint models. Fits the joint models proposed by Henderson and colleagues\n    <doi:10.1093/biostatistics/1.4.465> (single event time) and by Williamson and\n    colleagues (2008) <doi:10.1002/sim.3451> (competing risks events time) to a\n    single continuous repeated measure. The time-to-event data is modelled using a \n    (cause-specific) Cox proportional hazards regression model with time-varying \n    covariates. The longitudinal outcome is modelled using a linear mixed effects\n    model. The association is captured by a latent Gaussian process. The model is \n    estimated using am Expectation Maximization algorithm. Some plotting functions \n    and the variogram are also included. This project is funded by the Medical \n    Research Council (Grant numbers G0400615 and MR/M013227/1).",
    "version": "1.2.8",
    "maintainer": "Graeme L. Hickey <graemeleehickey@gmail.com>",
    "author": "Pete Philipson [aut] (ORCID: <https://orcid.org/0000-0001-7846-0208>),\n  Ines Sousa [aut] (ORCID: <https://orcid.org/0000-0002-2712-1713>),\n  Peter J. Diggle [aut] (ORCID: <https://orcid.org/0000-0003-3521-5020>),\n  Paula Williamson [aut] (ORCID: <https://orcid.org/0000-0001-9802-6636>),\n  Ruwanthi Kolamunnage-Dona [aut] (ORCID:\n    <https://orcid.org/0000-0003-3886-6208>),\n  Robin Henderson [aut],\n  Graeme L. Hickey [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4989-0054>),\n  Maria Sudell [ctb],\n  Medical Research Council [fnd] (Grant numbers: G0400615 and\n    MR/M013227/1)",
    "url": "https://github.com/graemeleehickey/joineR/",
    "bug_reports": "https://github.com/graemeleehickey/joineR/issues",
    "repository": "https://cran.r-project.org/package=joineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "joineR Joint Modelling of Repeated Measurements and Time-to-Event Data Analysis of repeated measurements and time-to-event data via random\n    effects joint models. Fits the joint models proposed by Henderson and colleagues\n    <doi:10.1093/biostatistics/1.4.465> (single event time) and by Williamson and\n    colleagues (2008) <doi:10.1002/sim.3451> (competing risks events time) to a\n    single continuous repeated measure. The time-to-event data is modelled using a \n    (cause-specific) Cox proportional hazards regression model with time-varying \n    covariates. The longitudinal outcome is modelled using a linear mixed effects\n    model. The association is captured by a latent Gaussian process. The model is \n    estimated using am Expectation Maximization algorithm. Some plotting functions \n    and the variogram are also included. This project is funded by the Medical \n    Research Council (Grant numbers G0400615 and MR/M013227/1).  "
  },
  {
    "id": 14739,
    "package_name": "jointCompRisk",
    "title": "Joint Inference for Competing Risks Data Using Multiple\nEndpoints",
    "description": "Tools for competing risks trials that allow simultaneous inference on\n    recovery and mortality endpoints. Provides data preparation helpers, standard\n    cumulative incidence estimators (restricted mean time gained/lost), and severity\n    weighted extensions that integrate longitudinal ordinal outcomes to summarise\n    treatment benefit. Methods follow Wen, Hu, and Wang (2023) Biometrics 79(3):1635-1645\n    <doi:10.1111/biom.13752>.",
    "version": "0.1.1",
    "maintainer": "Wenqing Zhang <wzhan115@jhu.edu>",
    "author": "Wenqing Zhang [aut, cre],\n  Jiyang Wen [aut],\n  Chen Hu [aut],\n  Meicheng Wang [aut]",
    "url": "https://github.com/cathyzzzhang/jointCompRisk",
    "bug_reports": "https://github.com/cathyzzzhang/jointCompRisk/issues",
    "repository": "https://cran.r-project.org/package=jointCompRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jointCompRisk Joint Inference for Competing Risks Data Using Multiple\nEndpoints Tools for competing risks trials that allow simultaneous inference on\n    recovery and mortality endpoints. Provides data preparation helpers, standard\n    cumulative incidence estimators (restricted mean time gained/lost), and severity\n    weighted extensions that integrate longitudinal ordinal outcomes to summarise\n    treatment benefit. Methods follow Wen, Hu, and Wang (2023) Biometrics 79(3):1635-1645\n    <doi:10.1111/biom.13752>.  "
  },
  {
    "id": 14743,
    "package_name": "jointPm",
    "title": "Risk Estimation Using the Joint Probability Method",
    "description": "Estimate risk caused by two extreme and dependent forcing variables using bivariate extreme value models as described in Zheng, Westra, and Sisson (2013) <doi:10.1016/j.jhydrol.2013.09.054>; Zheng, Westra and Leonard (2014) <doi:10.1002/2013WR014616>; Zheng, Leonard and Westra (2015) <doi:10.2166/hydro.2015.052>.",
    "version": "2.3.2",
    "maintainer": "Leo Belzile <belzilel@gmail.com>",
    "author": "Leo Belzile [cre] (ORCID: <https://orcid.org/0000-0002-9135-014X>),\n  Feifei Zheng [aut] (ORCID: <https://orcid.org/0000-0003-3048-7086>),\n  Michael Leonard [aut] (ORCID: <https://orcid.org/0000-0002-9519-3188>),\n  Seth Westra [aut] (ORCID: <https://orcid.org/0000-0003-4023-6061>)",
    "url": "",
    "bug_reports": "https://github.com/lbelzile/jointPm/issues/",
    "repository": "https://cran.r-project.org/package=jointPm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jointPm Risk Estimation Using the Joint Probability Method Estimate risk caused by two extreme and dependent forcing variables using bivariate extreme value models as described in Zheng, Westra, and Sisson (2013) <doi:10.1016/j.jhydrol.2013.09.054>; Zheng, Westra and Leonard (2014) <doi:10.1002/2013WR014616>; Zheng, Leonard and Westra (2015) <doi:10.2166/hydro.2015.052>.  "
  },
  {
    "id": 14766,
    "package_name": "jrvFinance",
    "title": "Basic Finance; NPV/IRR/Annuities/Bond-Pricing; Black Scholes",
    "description": "Implements the basic financial analysis\n    functions similar to (but not identical to) what\n    is available in most spreadsheet software. This\n    includes finding the IRR and NPV of regularly\n    spaced cash flows and annuities. Bond pricing and\n    YTM calculations are included. In addition, Black\n    Scholes option pricing and Greeks are also\n    provided.",
    "version": "1.4.3",
    "maintainer": "Jayanth Varma <jrvarma@iima.ac.in>",
    "author": "Jayanth Varma [aut, cre]",
    "url": "https://github.com/jrvarma/jrvFinance",
    "bug_reports": "https://github.com/jrvarma/jrvFinance/issues",
    "repository": "https://cran.r-project.org/package=jrvFinance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jrvFinance Basic Finance; NPV/IRR/Annuities/Bond-Pricing; Black Scholes Implements the basic financial analysis\n    functions similar to (but not identical to) what\n    is available in most spreadsheet software. This\n    includes finding the IRR and NPV of regularly\n    spaced cash flows and annuities. Bond pricing and\n    YTM calculations are included. In addition, Black\n    Scholes option pricing and Greeks are also\n    provided.  "
  },
  {
    "id": 14771,
    "package_name": "jskm",
    "title": "Kaplan-Meier Plot with 'ggplot2'",
    "description": "The function 'jskm()' creates publication quality Kaplan-Meier plot with at risk tables below. 'svyjskm()' provides plot for weighted Kaplan-Meier estimator. ",
    "version": "0.5.22",
    "maintainer": "Jinseob Kim <jinseob2kim@gmail.com>",
    "author": "Jinseob Kim [aut, cre] (ORCID: <https://orcid.org/0000-0002-9403-605X>),\n  yoonkyoung Chun [aut],\n  Zarathu [cph, fnd],\n  sungho Choi [aut],\n  Mingu Jee [aut],\n  Wonbin Hahn [aut]",
    "url": "https://github.com/jinseob2kim/jskm,\nhttps://jinseob2kim.github.io/jskm/",
    "bug_reports": "https://github.com/jinseob2kim/jstable/issues",
    "repository": "https://cran.r-project.org/package=jskm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jskm Kaplan-Meier Plot with 'ggplot2' The function 'jskm()' creates publication quality Kaplan-Meier plot with at risk tables below. 'svyjskm()' provides plot for weighted Kaplan-Meier estimator.   "
  },
  {
    "id": 14784,
    "package_name": "jubilee",
    "title": "Forecasting Long-Term Growth of the U.S. Stock Market and\nBusiness Cycles",
    "description": "A long-term forecast model called \"Jubilee-Tectonic model\" is implemented to forecast future returns of the U.S. stock market, Treasury yield, and gold price. The five-factor model forecasts the 10-year and 20-year future equity returns with high R-squared above 80 percent. It is based on linear growth and mean reversion characteristics in the U.S. stock market. This model also enhances the CAPE model by introducing the hypothesis that there are fault lines in the historical CAPE, which can be calibrated and corrected through statistical learning. In addition, it contains a module for business cycles, optimal interest rate, and recession forecasts.",
    "version": "0.3.3",
    "maintainer": "Stephen H-T. Lihn <stevelihn@gmail.com>",
    "author": "Stephen H-T. Lihn [aut, cre]",
    "url": "https://ssrn.com/abstract=3156574\nhttps://ssrn.com/abstract=3422278\nhttps://ssrn.com/abstract=3435667",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jubilee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jubilee Forecasting Long-Term Growth of the U.S. Stock Market and\nBusiness Cycles A long-term forecast model called \"Jubilee-Tectonic model\" is implemented to forecast future returns of the U.S. stock market, Treasury yield, and gold price. The five-factor model forecasts the 10-year and 20-year future equity returns with high R-squared above 80 percent. It is based on linear growth and mean reversion characteristics in the U.S. stock market. This model also enhances the CAPE model by introducing the hypothesis that there are fault lines in the historical CAPE, which can be calibrated and corrected through statistical learning. In addition, it contains a module for business cycles, optimal interest rate, and recession forecasts.  "
  },
  {
    "id": 14851,
    "package_name": "kernscr",
    "title": "Kernel Machine Score Test for Semi-Competing Risks",
    "description": "Kernel Machine Score Test for Pathway Analysis in the Presence of \n    Semi-Competing Risks. Method is detailed in: Neykov, Hejblum & Sinnott (2018) \n    <doi: 10.1177/0962280216653427>.",
    "version": "1.0.7",
    "maintainer": "Boris P Hejblum <boris.hejblum@u-bordeaux.fr>",
    "author": "Matey Neykov [aut],\n  Boris P Hejblum [aut, cre],\n  Jennifer A Sinnot [aut]",
    "url": "http://borishejblum.github.io/kernscr/",
    "bug_reports": "https://github.com/borishejblum/kernscr/issues",
    "repository": "https://cran.r-project.org/package=kernscr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kernscr Kernel Machine Score Test for Semi-Competing Risks Kernel Machine Score Test for Pathway Analysis in the Presence of \n    Semi-Competing Risks. Method is detailed in: Neykov, Hejblum & Sinnott (2018) \n    <doi: 10.1177/0962280216653427>.  "
  },
  {
    "id": 14868,
    "package_name": "kfre",
    "title": "Kidney Failure Risk Equation (KFRE) Tools",
    "description": "Implements the Kidney Failure Risk Equation\n  (KFRE; Tangri and colleagues (2011) <doi:10.1001/jama.2011.451>;\n  Tangri and colleagues (2016) <doi:10.1001/jama.2015.18202>) to compute\n  2- and 5-year kidney failure risk using 4-, 6-, and 8-variable models.\n  Includes helpers to append risk columns to data frames, classify\n  chronic kidney disease (CKD) stages and end-stage renal disease (ESRD)\n  outcomes, and evaluate and plot model performance.",
    "version": "0.0.2",
    "maintainer": "Leonid Shpaner <lshpaner@ucla.edu>",
    "author": "Leonid Shpaner [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-5311-8095>, URL:\n    https://www.leonshpaner.com)",
    "url": "https://github.com/lshpaner/kfre_r,\nhttps://lshpaner.github.io/kfre_r/",
    "bug_reports": "https://github.com/lshpaner/kfre_r/issues",
    "repository": "https://cran.r-project.org/package=kfre",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kfre Kidney Failure Risk Equation (KFRE) Tools Implements the Kidney Failure Risk Equation\n  (KFRE; Tangri and colleagues (2011) <doi:10.1001/jama.2011.451>;\n  Tangri and colleagues (2016) <doi:10.1001/jama.2015.18202>) to compute\n  2- and 5-year kidney failure risk using 4-, 6-, and 8-variable models.\n  Includes helpers to append risk columns to data frames, classify\n  chronic kidney disease (CKD) stages and end-stage renal disease (ESRD)\n  outcomes, and evaluate and plot model performance.  "
  },
  {
    "id": 14877,
    "package_name": "kidney.epi",
    "title": "Kidney-Related Functions for Clinical and Epidemiological\nResearch",
    "description": "Contains kidney care oriented functions.\n\t\t\t Current version contains functions for calculation of:\n\t\t\t - Estimated glomerular filtration rate by CKD-EPI (2021 and 2009), MDRD, CKiD, FAS, EKFC, etc.\n\t\t\t - Kidney Donor Risk Index and Kidney Donor Profile Index for kidney transplant donors.\n\t\t\t - Citation: Bikbov B. kidney.epi: Kidney-Related Functions for Clinical and Epidemiological Research. Scientific-Tools.Org, <https://Scientific-Tools.Org>.  <doi:10.32614/CRAN.package.kidney.epi>.",
    "version": "1.4.0",
    "maintainer": "Boris Bikbov <boris.bikbov@scientific-tools.org>",
    "author": "Boris Bikbov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1925-7506>)",
    "url": "https://Scientific-Tools.Org/,\nhttps://www.linkedin.com/in/boris-bikbov/,\nhttps://kidney.Scientific-Tools.Org/r/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kidney.epi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kidney.epi Kidney-Related Functions for Clinical and Epidemiological\nResearch Contains kidney care oriented functions.\n\t\t\t Current version contains functions for calculation of:\n\t\t\t - Estimated glomerular filtration rate by CKD-EPI (2021 and 2009), MDRD, CKiD, FAS, EKFC, etc.\n\t\t\t - Kidney Donor Risk Index and Kidney Donor Profile Index for kidney transplant donors.\n\t\t\t - Citation: Bikbov B. kidney.epi: Kidney-Related Functions for Clinical and Epidemiological Research. Scientific-Tools.Org, <https://Scientific-Tools.Org>.  <doi:10.32614/CRAN.package.kidney.epi>.  "
  },
  {
    "id": 14882,
    "package_name": "kin.cohort",
    "title": "Analysis of Kin-Cohort Studies",
    "description": "Analysis of kin-cohort studies. kin.cohort provides estimates of age-specific \n cumulative risk of a disease for carriers and noncarriers of a mutation. The cohorts are\n retrospectively built from relatives of probands for whom the genotype is known. Currently \n the method of moments and marginal maximum likelihood are implemented. Confidence intervals \n are calculated from bootstrap samples.\n Most of the code is a translation from previous 'MATLAB' code by N. Chatterjee.",
    "version": "0.7",
    "maintainer": "Victor Moreno <v.moreno@iconcologia.net>",
    "author": "Victor Moreno, Nilanjan Chatterjee, Bhramar Mukherjee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kin.cohort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kin.cohort Analysis of Kin-Cohort Studies Analysis of kin-cohort studies. kin.cohort provides estimates of age-specific \n cumulative risk of a disease for carriers and noncarriers of a mutation. The cohorts are\n retrospectively built from relatives of probands for whom the genotype is known. Currently \n the method of moments and marginal maximum likelihood are implemented. Confidence intervals \n are calculated from bootstrap samples.\n Most of the code is a translation from previous 'MATLAB' code by N. Chatterjee.  "
  },
  {
    "id": 14890,
    "package_name": "kisopenapi",
    "title": "Korea Investment & Securities (KIS) Open Trading API",
    "description": "API Wrapper to use Korea Investment & Securities (KIS) trading \n             system that provides various financial services like stock price \n             check, orders and balance check \n             <https://apiportal.koreainvestment.com/>.",
    "version": "0.0.2",
    "maintainer": "Seokhoon Joo <seokhoonj@gmail.com>",
    "author": "Seokhoon Joo [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kisopenapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kisopenapi Korea Investment & Securities (KIS) Open Trading API API Wrapper to use Korea Investment & Securities (KIS) trading \n             system that provides various financial services like stock price \n             check, orders and balance check \n             <https://apiportal.koreainvestment.com/>.  "
  },
  {
    "id": 14903,
    "package_name": "klovan",
    "title": "Geostatistics Methods and Klovan Data",
    "description": "A comprehensive set of geostatistical, visual,\n    and analytical methods, in conjunction with the expanded version of the\n    acclaimed J.E. Klovan's mining dataset, are included in 'klovan'. This makes the\n    package an excellent learning resource for Principal Component Analysis (PCA),\n    Factor Analysis (FA), kriging, and other geostatistical techniques. Originally\n    published in the 1976 book 'Geological Factor Analysis', the included mining\n    dataset was assembled by Professor J. E. Klovan of the University of Calgary.\n    Being one of the first applications of FA in the geosciences, this dataset has\n    significant historical importance. As a well-regarded and published dataset, it\n    is an excellent resource for demonstrating the capabilities of PCA, FA, kriging,\n    and other geostatistical techniques in geosciences. For those interested in\n    these methods, the 'klovan' datasets provide a valuable and illustrative resource.\n    Note that some methods require the 'RGeostats' package. Please refer to the\n    README or Additional_repositories for installation instructions. This material is\n    based upon research in the Materials Data Science for Stockpile Stewardship Center\n    of Excellence (MDS3-COE), and supported by the Department of Energy's National\n    Nuclear Security Administration under Award Number DE-NA0004104.",
    "version": "0.1.0",
    "maintainer": "Roger H French <rxf131@case.edu>",
    "author": "Jonathan E Gordon [aut] (ORCID:\n    <https://orcid.org/0009-0007-5958-7386>),\n  Eric K Helfer [aut] (ORCID: <https://orcid.org/0000-0002-8958-7690>),\n  Hope E Omodolor [aut] (ORCID: <https://orcid.org/0009-0005-7842-406X>),\n  Jeffery M Yarus [aut] (ORCID: <https://orcid.org/0000-0002-9331-9568>),\n  Roger H French [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6162-0532>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=klovan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "klovan Geostatistics Methods and Klovan Data A comprehensive set of geostatistical, visual,\n    and analytical methods, in conjunction with the expanded version of the\n    acclaimed J.E. Klovan's mining dataset, are included in 'klovan'. This makes the\n    package an excellent learning resource for Principal Component Analysis (PCA),\n    Factor Analysis (FA), kriging, and other geostatistical techniques. Originally\n    published in the 1976 book 'Geological Factor Analysis', the included mining\n    dataset was assembled by Professor J. E. Klovan of the University of Calgary.\n    Being one of the first applications of FA in the geosciences, this dataset has\n    significant historical importance. As a well-regarded and published dataset, it\n    is an excellent resource for demonstrating the capabilities of PCA, FA, kriging,\n    and other geostatistical techniques in geosciences. For those interested in\n    these methods, the 'klovan' datasets provide a valuable and illustrative resource.\n    Note that some methods require the 'RGeostats' package. Please refer to the\n    README or Additional_repositories for installation instructions. This material is\n    based upon research in the Materials Data Science for Stockpile Stewardship Center\n    of Excellence (MDS3-COE), and supported by the Department of Energy's National\n    Nuclear Security Administration under Award Number DE-NA0004104.  "
  },
  {
    "id": 14907,
    "package_name": "kmBlock",
    "title": "k-Means Like Blockmodeling of One-Mode and Linked Networks",
    "description": "Implements k-means like blockmodeling of one-mode and linked networks as presented in \u017diberna (2020) <doi:10.1016/j.socnet.2019.10.006>. The development of this package is financially supported by the Slovenian Research Agency (<https://www.arrs.si/>) within the research programs P5-0168 and the research projects J7-8279 (Blockmodeling multilevel and temporal networks) and J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).",
    "version": "0.1.4",
    "maintainer": "Ale\u0161 \u017diberna <ales.ziberna@fdv.uni-lj.si>",
    "author": "Ale\u0161 \u017diberna [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kmBlock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kmBlock k-Means Like Blockmodeling of One-Mode and Linked Networks Implements k-means like blockmodeling of one-mode and linked networks as presented in \u017diberna (2020) <doi:10.1016/j.socnet.2019.10.006>. The development of this package is financially supported by the Slovenian Research Agency (<https://www.arrs.si/>) within the research programs P5-0168 and the research projects J7-8279 (Blockmodeling multilevel and temporal networks) and J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).  "
  },
  {
    "id": 14912,
    "package_name": "kmi",
    "title": "Kaplan-Meier Multiple Imputation for the Analysis of Cumulative\nIncidence Functions in the Competing Risks Setting",
    "description": "Performs a Kaplan-Meier multiple imputation to recover the missing potential censoring information from competing risks events, so that standard right-censored methods could be applied to the imputed data sets to perform analyses of the cumulative incidence functions (Allignol and Beyersmann, 2010 <doi:10.1093/biostatistics/kxq018>).",
    "version": "0.5.5",
    "maintainer": "Arthur Allignol <arthur.allignol@gmail.com>",
    "author": "Arthur Allignol <arthur.allignol@gmail.com>",
    "url": "https://github.com/aallignol/kmi",
    "bug_reports": "https://github.com/aallignol/kmi/issues",
    "repository": "https://cran.r-project.org/package=kmi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kmi Kaplan-Meier Multiple Imputation for the Analysis of Cumulative\nIncidence Functions in the Competing Risks Setting Performs a Kaplan-Meier multiple imputation to recover the missing potential censoring information from competing risks events, so that standard right-censored methods could be applied to the imputed data sets to perform analyses of the cumulative incidence functions (Allignol and Beyersmann, 2010 <doi:10.1093/biostatistics/kxq018>).  "
  },
  {
    "id": 14927,
    "package_name": "knobi",
    "title": "Known-Biomass Production Model (KBPM)",
    "description": "Application of a Known Biomass Production Model (KBPM): (1) the fitting of KBPM to each stock; (2) the estimation of the effects of environmental variability; (3) the retrospective analysis to identify regime shifts; (4) the estimation of forecasts. For more details see Schaefer (1954) <https://www.iattc.org/GetAttachment/62d510ee-13d0-40f2-847b-0fde415476b8/Vol-1-No-2-1954-SCHAEFER,-MILNER-B-_Some-aspects-of-the-dynamics-of-populations-important-to-the-management-of-the-commercial-marine-fisheries.pdf>, Pella and Tomlinson (1969) <https://www.iattc.org/GetAttachment/9865079c-6ee7-40e2-9e30-c4523ff81ddf/Vol-13-No-3-1969-PELLA,-JEROME-J-,-and-PATRICK-K-TOMLINSON_A-generalized-stock-production-model.pdf> and MacCall (2002) <doi:10.1577/1548-8675(2002)022%3C0272:UOKBPM%3E2.0.CO;2>.",
    "version": "0.1.0",
    "maintainer": "Anxo Paz <anxo.paz@hotmail.com>",
    "author": "Anxo Paz [aut, cre],\n  Marta Cousido Rocha [aut, ths],\n  Santiago Cervino Lopez [aut, ths],\n  Maria Grazia Peninno [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=knobi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knobi Known-Biomass Production Model (KBPM) Application of a Known Biomass Production Model (KBPM): (1) the fitting of KBPM to each stock; (2) the estimation of the effects of environmental variability; (3) the retrospective analysis to identify regime shifts; (4) the estimation of forecasts. For more details see Schaefer (1954) <https://www.iattc.org/GetAttachment/62d510ee-13d0-40f2-847b-0fde415476b8/Vol-1-No-2-1954-SCHAEFER,-MILNER-B-_Some-aspects-of-the-dynamics-of-populations-important-to-the-management-of-the-commercial-marine-fisheries.pdf>, Pella and Tomlinson (1969) <https://www.iattc.org/GetAttachment/9865079c-6ee7-40e2-9e30-c4523ff81ddf/Vol-13-No-3-1969-PELLA,-JEROME-J-,-and-PATRICK-K-TOMLINSON_A-generalized-stock-production-model.pdf> and MacCall (2002) <doi:10.1577/1548-8675(2002)022%3C0272:UOKBPM%3E2.0.CO;2>.  "
  },
  {
    "id": 15071,
    "package_name": "lazytrade",
    "title": "Learn Computer and Data Science using Algorithmic Trading",
    "description": "Provide sets of functions and methods to learn and practice data science using idea of algorithmic trading.\n    Main goal is to process information within \"Decision Support System\" to come up with analysis or predictions.\n    There are several utilities such as dynamic and adaptive risk management using reinforcement learning\n    and even functions to generate predictions of price changes using pattern recognition deep regression learning.\n    Summary of Methods used: Awesome H2O tutorials: <https://github.com/h2oai/awesome-h2o>, \n    Market Type research of Van Tharp Institute: <https://vantharp.com/>,\n    Reinforcement Learning R package: <https://CRAN.R-project.org/package=ReinforcementLearning>.",
    "version": "0.5.4",
    "maintainer": "Vladimir Zhbanko <vladimir.zhbanko@gmail.com>",
    "author": "Vladimir Zhbanko",
    "url": "https://vladdsm.github.io/myblog_attempt/topics/lazy%20trading/,\nhttps://github.com/vzhomeexperiments/lazytrade",
    "bug_reports": "https://github.com/vzhomeexperiments/lazytrade/issues",
    "repository": "https://cran.r-project.org/package=lazytrade",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lazytrade Learn Computer and Data Science using Algorithmic Trading Provide sets of functions and methods to learn and practice data science using idea of algorithmic trading.\n    Main goal is to process information within \"Decision Support System\" to come up with analysis or predictions.\n    There are several utilities such as dynamic and adaptive risk management using reinforcement learning\n    and even functions to generate predictions of price changes using pattern recognition deep regression learning.\n    Summary of Methods used: Awesome H2O tutorials: <https://github.com/h2oai/awesome-h2o>, \n    Market Type research of Van Tharp Institute: <https://vantharp.com/>,\n    Reinforcement Learning R package: <https://CRAN.R-project.org/package=ReinforcementLearning>.  "
  },
  {
    "id": 15082,
    "package_name": "lchemix",
    "title": "A Bayesian Multi-Dimensional Couple-Based Latent Risk Model",
    "description": "A joint latent class model where a hierarchical structure exists, with an interaction between female and male partners of a couple. A Bayesian perspective to inference and Markov chain Monte Carlo algorithms to obtain posterior estimates of model parameters. The reference paper is: Beom Seuk Hwang, Zhen Chen, Germaine M.Buck Louis, Paul S. Albert, (2018) \"A Bayesian multi-dimensional couple-based latent risk model with an application to infertility\". Biometrics, 75, 315-325. <doi:10.1111/biom.12972>.",
    "version": "0.1.0",
    "maintainer": "Weimin Zhang <zhangwm@hotmail.com>",
    "author": "Beom Seuk Hwang [aut],\n  Zhen Chen [aut],\n  Germaine M. Buck Louis [aut],\n  Paul S. Albert [aut],\n  Weimin Zhang [cre]",
    "url": "http://github.com/wzhang17/lchemix.git,\nhttps://doi.org/10.1111/biom.12972",
    "bug_reports": "http://github.com/wzhang17/lchemix/issues",
    "repository": "https://cran.r-project.org/package=lchemix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lchemix A Bayesian Multi-Dimensional Couple-Based Latent Risk Model A joint latent class model where a hierarchical structure exists, with an interaction between female and male partners of a couple. A Bayesian perspective to inference and Markov chain Monte Carlo algorithms to obtain posterior estimates of model parameters. The reference paper is: Beom Seuk Hwang, Zhen Chen, Germaine M.Buck Louis, Paul S. Albert, (2018) \"A Bayesian multi-dimensional couple-based latent risk model with an application to infertility\". Biometrics, 75, 315-325. <doi:10.1111/biom.12972>.  "
  },
  {
    "id": 15089,
    "package_name": "lcyanalysis",
    "title": "Stock Data Analysis Functions",
    "description": "Analysis of stock data ups and downs trend, the stock technical analysis indicators function have trend line, reversal pattern and market trend.",
    "version": "1.0.4",
    "maintainer": "Chun-Yu Liu <john401528@gmail.com>",
    "author": "Chun-Yu Liu [aut,cph],\n        Shu-Nung Yao [rev,ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lcyanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lcyanalysis Stock Data Analysis Functions Analysis of stock data ups and downs trend, the stock technical analysis indicators function have trend line, reversal pattern and market trend.  "
  },
  {
    "id": 15096,
    "package_name": "ldhmm",
    "title": "Hidden Markov Model for Financial Time-Series Based on Lambda\nDistribution",
    "description": "Hidden Markov Model (HMM) based on symmetric lambda distribution\n    framework is implemented for the study of return time-series in the financial\n    market. Major features in the S&P500 index, such as regime identification,\n    volatility clustering, and anti-correlation between return and volatility,\n    can be extracted from HMM cleanly. Univariate symmetric lambda distribution\n    is essentially a location-scale family of exponential power distribution.\n    Such distribution is suitable for describing highly leptokurtic time series\n    obtained from the financial market. It provides a theoretically solid foundation\n    to explore such data where the normal distribution is not adequate. The HMM\n    implementation follows closely the book: \"Hidden Markov Models for Time Series\",\n    by Zucchini, MacDonald, Langrock (2016).",
    "version": "0.6.1",
    "maintainer": "Stephen H-T. Lihn <stevelihn@gmail.com>",
    "author": "Stephen H-T. Lihn [aut, cre]",
    "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2979516\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3435667",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ldhmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ldhmm Hidden Markov Model for Financial Time-Series Based on Lambda\nDistribution Hidden Markov Model (HMM) based on symmetric lambda distribution\n    framework is implemented for the study of return time-series in the financial\n    market. Major features in the S&P500 index, such as regime identification,\n    volatility clustering, and anti-correlation between return and volatility,\n    can be extracted from HMM cleanly. Univariate symmetric lambda distribution\n    is essentially a location-scale family of exponential power distribution.\n    Such distribution is suitable for describing highly leptokurtic time series\n    obtained from the financial market. It provides a theoretically solid foundation\n    to explore such data where the normal distribution is not adequate. The HMM\n    implementation follows closely the book: \"Hidden Markov Models for Time Series\",\n    by Zucchini, MacDonald, Langrock (2016).  "
  },
  {
    "id": 15162,
    "package_name": "lg",
    "title": "Locally Gaussian Distributions: Estimation and Methods",
    "description": "An implementation of locally Gaussian distributions. It provides methods for\n    implementing locally Gaussian multivariate density estimation, conditional density \n    estimation, various independence tests for iid and time series data, a test for conditional \n    independence and a test for financial contagion.",
    "version": "0.4.1",
    "maintainer": "H\u00e5kon Otneim <hakon.otneim@nhh.no>",
    "author": "H\u00e5kon Otneim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lg Locally Gaussian Distributions: Estimation and Methods An implementation of locally Gaussian distributions. It provides methods for\n    implementing locally Gaussian multivariate density estimation, conditional density \n    estimation, various independence tests for iid and time series data, a test for conditional \n    independence and a test for financial contagion.  "
  },
  {
    "id": 15189,
    "package_name": "lifecontingencies",
    "title": "Financial and Actuarial Mathematics for Life Contingencies",
    "description": "Classes and methods that allow the user to manage life table,\n    actuarial tables (also multiple decrements tables). Moreover, functions to easily\n    perform demographic, financial and actuarial mathematics on life contingencies\n    insurances calculations are contained therein. See Spedicato (2013)\t<doi:10.18637/jss.v055.i10>.",
    "version": "1.4.4",
    "maintainer": "Giorgio Alfredo Spedicato <spedicato_giorgio@yahoo.it>",
    "author": "Giorgio Alfredo Spedicato [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0315-8888>),\n  Christophe Dutang [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6732-1501>),\n  Reinhold Kainhofer [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7895-1311>),\n  Kevin J Owens [ctb],\n  Ernesto Schirmacher [ctb],\n  Gian Paolo Clemente [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6795-4595>),\n  Ivan Williams [ctb]",
    "url": "https://github.com/spedygiorgio/lifecontingencies",
    "bug_reports": "https://github.com/spedygiorgio/lifecontingencies/issues",
    "repository": "https://cran.r-project.org/package=lifecontingencies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lifecontingencies Financial and Actuarial Mathematics for Life Contingencies Classes and methods that allow the user to manage life table,\n    actuarial tables (also multiple decrements tables). Moreover, functions to easily\n    perform demographic, financial and actuarial mathematics on life contingencies\n    insurances calculations are contained therein. See Spedicato (2013)\t<doi:10.18637/jss.v055.i10>.  "
  },
  {
    "id": 15191,
    "package_name": "lifepack",
    "title": "Insurance Reserve Calculations",
    "description": "Calculates insurance reserves and equivalence premiums using advanced numerical methods, including the Runge-Kutta algorithm and product integrals for transition probabilities. This package is useful for actuarial analyses and life insurance modeling, facilitating accurate financial projections.",
    "version": "0.1.0",
    "maintainer": "Oskar Allerslev <Oskar.m1660@gmail.com>",
    "author": "Oskar Allerslev [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lifepack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lifepack Insurance Reserve Calculations Calculates insurance reserves and equivalence premiums using advanced numerical methods, including the Runge-Kutta algorithm and product integrals for transition probabilities. This package is useful for actuarial analyses and life insurance modeling, facilitating accurate financial projections.  "
  },
  {
    "id": 15208,
    "package_name": "limexhub",
    "title": "Quantitative Trade Signals",
    "description": "Provides an interface to the financial data platform <https://datahub.limex.com/>., enabling users to retrieve real-time and historical financial data. Functions within the package allow access to instruments, candlestick charts, fundamentals, news, events, models, and trading signals. Authentication is managed through user-specific API tokens, which are securely handled via environment variables.",
    "version": "0.1.5",
    "maintainer": "Vyacheslav Arbuzov <arbuzov1989@gmail.com>",
    "author": "Vyacheslav Arbuzov [aut, cre, cph]",
    "url": "https://datahub.limex.com",
    "bug_reports": "https://github.com/Limex-com/limexhub-r/issues",
    "repository": "https://cran.r-project.org/package=limexhub",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "limexhub Quantitative Trade Signals Provides an interface to the financial data platform <https://datahub.limex.com/>., enabling users to retrieve real-time and historical financial data. Functions within the package allow access to instruments, candlestick charts, fundamentals, news, events, models, and trading signals. Authentication is managed through user-specific API tokens, which are securely handled via environment variables.  "
  },
  {
    "id": 15213,
    "package_name": "linERR",
    "title": "Linear Excess Relative Risk Model",
    "description": "Fits a linear excess relative risk model by maximum likelihood, possibly including several variables and allowing for lagged exposures.",
    "version": "1.0",
    "maintainer": "David Mori\u00f1a Soler <david.morina@uab.cat>",
    "author": "David Mori\u00f1a (ISGlobal, Centre for Research in Environmental Epidemiology)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=linERR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "linERR Linear Excess Relative Risk Model Fits a linear excess relative risk model by maximum likelihood, possibly including several variables and allowing for lagged exposures.  "
  },
  {
    "id": 15253,
    "package_name": "listviewer",
    "title": "'htmlwidget' for Interactive Views of R Lists",
    "description": "R lists, especially nested lists, can be very difficult to\n    visualize or represent. Sometimes 'str()' is not enough, so this suite of\n    htmlwidgets is designed to help see, understand, and maybe even modify your R\n    lists.  The function 'reactjson()' requires a package\n    'reactR' that can be installed from CRAN or <https://github.com/timelyportfolio/reactR>.",
    "version": "4.0.0",
    "maintainer": "Kent Russell <kent.russell@timelyportfolio.com>",
    "author": "Jos de Jong [aut, cph] (jsoneditor.js library in\n    htmlwidgets/jsoneditor, http://github.com/josdejong/jsoneditor/),\n  Mac Gainer [aut, cph] (react-json-view library in\n    htmlwidgets/react-json, https://github.com/mac-s-g/react-json-view),\n  Kent Russell [aut, cre] (R interface)",
    "url": "https://github.com/timelyportfolio/listviewer",
    "bug_reports": "https://github.com/timelyportfolio/listviewer/issues",
    "repository": "https://cran.r-project.org/package=listviewer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "listviewer 'htmlwidget' for Interactive Views of R Lists R lists, especially nested lists, can be very difficult to\n    visualize or represent. Sometimes 'str()' is not enough, so this suite of\n    htmlwidgets is designed to help see, understand, and maybe even modify your R\n    lists.  The function 'reactjson()' requires a package\n    'reactR' that can be installed from CRAN or <https://github.com/timelyportfolio/reactR>.  "
  },
  {
    "id": 15266,
    "package_name": "llama",
    "title": "Leveraging Learning to Automatically Manage Algorithms",
    "description": "Provides functionality to train and evaluate algorithm selection models for portfolios.",
    "version": "0.10.1",
    "maintainer": "Lars Kotthoff <larsko@uwyo.edu>",
    "author": "Lars Kotthoff [aut,cre],\n    Bernd Bischl [aut],\n    Barry Hurley [ctb],\n    Talal Rahwan [ctb],\n    Damir Pulatov [ctb]",
    "url": "https://bitbucket.org/lkotthoff/llama",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=llama",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "llama Leveraging Learning to Automatically Manage Algorithms Provides functionality to train and evaluate algorithm selection models for portfolios.  "
  },
  {
    "id": 15303,
    "package_name": "lmtp",
    "title": "Non-Parametric Causal Effects of Feasible Interventions Based on\nModified Treatment Policies",
    "description": "Non-parametric estimators for casual effects based on longitudinal modified treatment \n  policies as described in Diaz, Williams, Hoffman, and Schenck <doi:10.1080/01621459.2021.1955691>, traditional point treatment, \n  and traditional longitudinal effects. Continuous, binary, categorical treatments, and multivariate treatments are allowed as well are \n  censored outcomes. The treatment mechanism is estimated via a density ratio classification procedure \n  irrespective of treatment variable type. For both continuous and binary outcomes, additive treatment effects \n  can be calculated and relative risks and odds ratios may be calculated for binary outcomes. \n  Supports survival outcomes with competing risks (Diaz, Hoffman, and Hejazi; <doi:10.1007/s10985-023-09606-7>).",
    "version": "1.5.3",
    "maintainer": "Nicholas Williams <ntwilliams.personal@gmail.com>",
    "author": "Nicholas Williams [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1378-4831>),\n  Iv\u00e1n D\u00edaz [aut, cph] (ORCID: <https://orcid.org/0000-0001-9056-2047>),\n  Beaudan Campbell-Brown [ctb] (ORCID:\n    <https://orcid.org/0009-0006-6967-9295>)",
    "url": "https://beyondtheate.com/, https://github.com/nt-williams/lmtp",
    "bug_reports": "https://github.com/nt-williams/lmtp/issues",
    "repository": "https://cran.r-project.org/package=lmtp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmtp Non-Parametric Causal Effects of Feasible Interventions Based on\nModified Treatment Policies Non-parametric estimators for casual effects based on longitudinal modified treatment \n  policies as described in Diaz, Williams, Hoffman, and Schenck <doi:10.1080/01621459.2021.1955691>, traditional point treatment, \n  and traditional longitudinal effects. Continuous, binary, categorical treatments, and multivariate treatments are allowed as well are \n  censored outcomes. The treatment mechanism is estimated via a density ratio classification procedure \n  irrespective of treatment variable type. For both continuous and binary outcomes, additive treatment effects \n  can be calculated and relative risks and odds ratios may be calculated for binary outcomes. \n  Supports survival outcomes with competing risks (Diaz, Hoffman, and Hejazi; <doi:10.1007/s10985-023-09606-7>).  "
  },
  {
    "id": 15341,
    "package_name": "logbin",
    "title": "Relative Risk Regression Using the Log-Binomial Model",
    "description": "Methods for fitting log-link GLMs and GAMs to binomial data,\n    including EM-type algorithms with more stable convergence properties than standard methods.",
    "version": "2.0.6",
    "maintainer": "Mark W. Donoghoe <markdonoghoe@gmail.com>",
    "author": "Mark W. Donoghoe [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0212-6443>),\n  Ian C. Marschner [ths] (ORCID: <https://orcid.org/0000-0002-6225-1572>),\n  Alexandra C. Gillett [ctb] (wrote an initial version of the nplbin\n    function, ORCID: <https://orcid.org/0000-0002-5069-3197>),\n  The R Core Team [cph] (summary.logbin, anova.logbin and others are\n    based on stats::glm-related functions)",
    "url": "https://github.com/mdonoghoe/logbin",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=logbin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "logbin Relative Risk Regression Using the Log-Binomial Model Methods for fitting log-link GLMs and GAMs to binomial data,\n    including EM-type algorithms with more stable convergence properties than standard methods.  "
  },
  {
    "id": 15357,
    "package_name": "logisticRR",
    "title": "Adjusted Relative Risk from Logistic Regression",
    "description": "Adjusted odds ratio conditional on potential confounders can be directly obtained from logistic regression. However, those adjusted odds ratios have been widely incorrectly interpreted as a relative risk. As relative risk is often of interest in public health, we provide a simple code to return adjusted relative risks from logistic regression model under potential confounders. ",
    "version": "0.3.0",
    "maintainer": "Youjin Lee <youjin.lee@pennmedicine.upenn.edu>",
    "author": "Youjin Lee",
    "url": "https://github.com/youjin1207/logisticRR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=logisticRR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "logisticRR Adjusted Relative Risk from Logistic Regression Adjusted odds ratio conditional on potential confounders can be directly obtained from logistic regression. However, those adjusted odds ratios have been widely incorrectly interpreted as a relative risk. As relative risk is often of interest in public health, we provide a simple code to return adjusted relative risks from logistic regression model under potential confounders.   "
  },
  {
    "id": 15431,
    "package_name": "lsbs",
    "title": "Bandwidth Selection for Level Sets and HDR Estimation",
    "description": "Bandwidth selection for kernel density estimators of 2-d\n    level sets and highest density regions. It applies a plug-in\n    strategy to estimate the asymptotic risk function and minimize to\n    get the optimal bandwidth matrix. See Doss and Weng (2018)\n    <arXiv:1806.00731> for more detail.",
    "version": "0.1",
    "maintainer": "Guangwei Weng <wengx076@umn.edu>",
    "author": "Guangwei Weng [aut, cre]",
    "url": "http://arxiv.org/abs/1806.00731",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lsbs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lsbs Bandwidth Selection for Level Sets and HDR Estimation Bandwidth selection for kernel density estimators of 2-d\n    level sets and highest density regions. It applies a plug-in\n    strategy to estimate the asymptotic risk function and minimize to\n    get the optimal bandwidth matrix. See Doss and Weng (2018)\n    <arXiv:1806.00731> for more detail.  "
  },
  {
    "id": 15487,
    "package_name": "mFilter",
    "title": "Miscellaneous Time Series Filters",
    "description": "The mFilter package implements several time series filters useful\n        for smoothing and extracting trend and cyclical components of a\n        time series. The routines are commonly used in economics and\n        finance, however they should also be interest to other areas.\n        Currently, Christiano-Fitzgerald, Baxter-King,\n        Hodrick-Prescott, Butterworth, and trigonometric regression\n        filters are included in the package.",
    "version": "0.1-5",
    "maintainer": "Mehmet Balcilar <mehmet@mbalcilar.net>",
    "author": "Mehmet Balcilar <mehmet@mbalcilar.net>",
    "url": "http://www.mbalcilar.net",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mFilter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mFilter Miscellaneous Time Series Filters The mFilter package implements several time series filters useful\n        for smoothing and extracting trend and cyclical components of a\n        time series. The routines are commonly used in economics and\n        finance, however they should also be interest to other areas.\n        Currently, Christiano-Fitzgerald, Baxter-King,\n        Hodrick-Prescott, Butterworth, and trigonometric regression\n        filters are included in the package.  "
  },
  {
    "id": 15510,
    "package_name": "macrocol",
    "title": "Colombian Macro-Financial Time Series Generator",
    "description": "This repository aims to contribute to the econometric models' production\n\twith Colombian data, by providing a set of web-scrapping functions \n\tof some of the main macro-financial indicators. All the sources are public and\n\tfree, but the advantage of these functions is that they directly download \n\tand harmonize the information in R's environment. No need to import or download\n\tadditional files. You only need an internet connection!",
    "version": "0.1.0",
    "maintainer": "Pedro Alejandro Cabra-Acela <jando2797@gmail.com>",
    "author": "Pedro Alejandro Cabra-Acela [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4788-5910>)",
    "url": "<https://github.com/pedroCabraAcela/Scrapping-Colombian-Macrodata>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=macrocol",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "macrocol Colombian Macro-Financial Time Series Generator This repository aims to contribute to the econometric models' production\n\twith Colombian data, by providing a set of web-scrapping functions \n\tof some of the main macro-financial indicators. All the sources are public and\n\tfree, but the advantage of these functions is that they directly download \n\tand harmonize the information in R's environment. No need to import or download\n\tadditional files. You only need an internet connection!  "
  },
  {
    "id": 15592,
    "package_name": "mapme.biodiversity",
    "title": "Efficient Monitoring of Global Biodiversity Portfolios",
    "description": "Biodiversity areas, especially primary forest, serve a\n    multitude of functions for local economy, regional functionality of\n    the ecosystems as well as the global health of our planet. Recently,\n    adverse changes in human land use practices and climatic responses to\n    increased greenhouse gas emissions, put these biodiversity areas under\n    a variety of different threats. The present package helps to analyse a\n    number of biodiversity indicators based on freely available\n    geographical datasets. It supports computational efficient routines\n    that allow the analysis of potentially global biodiversity portfolios.\n    The primary use case of the package is to support evidence based\n    reporting of an organization's effort to protect biodiversity areas\n    under threat and to identify regions were intervention is most duly\n    needed.",
    "version": "0.9.5",
    "maintainer": "Sven Bergtold <sven.bergtold@gmail.com>",
    "author": "Darius A. G\u00f6rgen [aut] (ORCID: <https://orcid.org/0009-0008-5503-7704>),\n  Om Prakash Bhandari [aut],\n  Andreas Petutschnig [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5029-2425>),\n  Sven Bergtold [ctb, cre],\n  Zivan Karaman [ctb] (ORCID: <https://orcid.org/0000-0002-8933-4589>),\n  MAPME-Initiative [cph, fnd]",
    "url": "https://mapme-initiative.github.io/mapme.biodiversity/,\nhttps://github.com/mapme-initiative/mapme.biodiversity/",
    "bug_reports": "https://github.com/mapme-initiative/mapme.biodiversity/issues",
    "repository": "https://cran.r-project.org/package=mapme.biodiversity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapme.biodiversity Efficient Monitoring of Global Biodiversity Portfolios Biodiversity areas, especially primary forest, serve a\n    multitude of functions for local economy, regional functionality of\n    the ecosystems as well as the global health of our planet. Recently,\n    adverse changes in human land use practices and climatic responses to\n    increased greenhouse gas emissions, put these biodiversity areas under\n    a variety of different threats. The present package helps to analyse a\n    number of biodiversity indicators based on freely available\n    geographical datasets. It supports computational efficient routines\n    that allow the analysis of potentially global biodiversity portfolios.\n    The primary use case of the package is to support evidence based\n    reporting of an organization's effort to protect biodiversity areas\n    under threat and to identify regions were intervention is most duly\n    needed.  "
  },
  {
    "id": 15596,
    "package_name": "mappestRisk",
    "title": "Create Maps Forecasting Risk of Pest Occurrence",
    "description": "There are three different modules: (1) model fitting and selection \n    using a set of the most commonly used equations describing developmental \n    responses to temperature helped by already existing R packages ('rTPC') \n    and nonlinear regression model functions from 'nls.multstart' \n    (Padfield et al. 2021, <doi:10.1111/2041-210X.13585>), with visualization \n    of model predictions to guide ecological criteria for model selection; \n    (2) calculation of suitability thermal limits, which consist on a \n    temperature interval delimiting the optimal performance zone or suitability; \n    and (3) climatic data extraction and visualization inspired on previous \n    research (Taylor et al. 2019, <doi:10.1111/1365-2664.13455>), with either \n    exportable rasters, static map images or html, interactive maps.",
    "version": "0.1.2",
    "maintainer": "Dar\u00edo San-Segundo Molina <dario.ssm2@gmail.com>",
    "author": "Dar\u00edo San-Segundo Molina [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7831-9623>),\n  A. M\u00e1rcia Barbosa [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-8972-7713>),\n  Antonio Jes\u00fas P\u00e9rez-Luque [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-1747-0469>),\n  Francisco Rodr\u00edguez-S\u00e1nchez [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-7981-1599>)",
    "url": "https://github.com/EcologyR/mappestRisk,\nhttps://ecologyr.github.io/mappestRisk/",
    "bug_reports": "https://github.com/EcologyR/mappestRisk/issues",
    "repository": "https://cran.r-project.org/package=mappestRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mappestRisk Create Maps Forecasting Risk of Pest Occurrence There are three different modules: (1) model fitting and selection \n    using a set of the most commonly used equations describing developmental \n    responses to temperature helped by already existing R packages ('rTPC') \n    and nonlinear regression model functions from 'nls.multstart' \n    (Padfield et al. 2021, <doi:10.1111/2041-210X.13585>), with visualization \n    of model predictions to guide ecological criteria for model selection; \n    (2) calculation of suitability thermal limits, which consist on a \n    temperature interval delimiting the optimal performance zone or suitability; \n    and (3) climatic data extraction and visualization inspired on previous \n    research (Taylor et al. 2019, <doi:10.1111/1365-2664.13455>), with either \n    exportable rasters, static map images or html, interactive maps.  "
  },
  {
    "id": 15619,
    "package_name": "marginalizedRisk",
    "title": "Estimating Marginalized Risk",
    "description": "Estimates risk as a function of a marker by integrating over other covariates in a conditional risk model.",
    "version": "2024.5-17",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "author": "Youyi Fong [cre],\n  Peter Gilbert [aut],\n  Marco Carone [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=marginalizedRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "marginalizedRisk Estimating Marginalized Risk Estimates risk as a function of a marker by integrating over other covariates in a conditional risk model.  "
  },
  {
    "id": 15620,
    "package_name": "marginme",
    "title": "Estimation of Relative Risks, Risk Differences, and Marginal\nEffects from Mixed Models Using Marginal Standardization",
    "description": "Functionality to estimate relative risks, risk differences, and \n  partial effects from mixed model. Marginalisation over random effect terms is accomplished using Markov Chain Monte Carlo.",
    "version": "0.1.0",
    "maintainer": "Sam Watson <S.I.Watson@bham.ac.uk>",
    "author": "Sam Watson [aut, cre]",
    "url": "https://github.com/samuel-watson/marginme",
    "bug_reports": "https://github.com/samuel-watson/marginme/issues",
    "repository": "https://cran.r-project.org/package=marginme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "marginme Estimation of Relative Risks, Risk Differences, and Marginal\nEffects from Mixed Models Using Marginal Standardization Functionality to estimate relative risks, risk differences, and \n  partial effects from mixed model. Marginalisation over random effect terms is accomplished using Markov Chain Monte Carlo.  "
  },
  {
    "id": 15633,
    "package_name": "markowitz",
    "title": "Markowitz Criterion",
    "description": "The Markowitz criterion is a multicriteria decision-making method that stands out in risk and uncertainty analysis in contexts where probabilities are known. This approach represents an evolution of Pascal's criterion by incorporating the dimension of variability. In this framework, the expected value reflects the anticipated return, while the standard deviation serves as a measure of risk. The 'markowitz' package provides a practical and accessible tool for implementing this method, enabling researchers and professionals to perform analyses without complex calculations. Thus, the package facilitates the application of the Markowitz criterion. More details on the method can be found in Octave Jokung-Ngu\u00e9na (2001, ISBN 2100055372).",
    "version": "0.1.0",
    "maintainer": "Luana Oliveira <luana.azevedo.oliveira.2022@gmail.com>",
    "author": "Luana Oliveira [aut, cre],\n  Marcos Santos [ctb] (ORCID: <https://orcid.org/0000-0003-1533-5535>)",
    "url": "https://github.com/luana1909/Markowitiz",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=markowitz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "markowitz Markowitz Criterion The Markowitz criterion is a multicriteria decision-making method that stands out in risk and uncertainty analysis in contexts where probabilities are known. This approach represents an evolution of Pascal's criterion by incorporating the dimension of variability. In this framework, the expected value reflects the anticipated return, while the standard deviation serves as a measure of risk. The 'markowitz' package provides a practical and accessible tool for implementing this method, enabling researchers and professionals to perform analyses without complex calculations. Thus, the package facilitates the application of the Markowitz criterion. More details on the method can be found in Octave Jokung-Ngu\u00e9na (2001, ISBN 2100055372).  "
  },
  {
    "id": 15653,
    "package_name": "matchedcc",
    "title": "'Stata'-Like Matched Case-Control Analysis",
    "description": "Calculate multiple statistics with confidence intervals for matched\n    case-control data including risk difference, risk ratio, relative\n    difference, and the odds ratio. Results are equivalent to those from\n    'Stata', and you can choose how to format your input data. Methods used are\n    those described on page 56 the 'Stata' documentation for \"Epitab - Tables\n    for Epidemologists\" <https://www.stata.com/manuals/repitab.pdf>.",
    "version": "0.1.1",
    "maintainer": "Simon R Parker <simon.parker.24@ucl.ac.uk>",
    "author": "Simon R Parker [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0003-8214-4496>)",
    "url": "https://github.com/simpar1471/matchedcc/,\nhttps://simpar1471.github.io/matchedcc/",
    "bug_reports": "https://github.com/simpar1471/matchedcc/issues",
    "repository": "https://cran.r-project.org/package=matchedcc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matchedcc 'Stata'-Like Matched Case-Control Analysis Calculate multiple statistics with confidence intervals for matched\n    case-control data including risk difference, risk ratio, relative\n    difference, and the odds ratio. Results are equivalent to those from\n    'Stata', and you can choose how to format your input data. Methods used are\n    those described on page 56 the 'Stata' documentation for \"Epitab - Tables\n    for Epidemologists\" <https://www.stata.com/manuals/repitab.pdf>.  "
  },
  {
    "id": 15655,
    "package_name": "matchingR",
    "title": "Matching Algorithms in R and C++",
    "description": "Computes matching algorithms quickly using Rcpp.\n    Implements the Gale-Shapley Algorithm to compute the stable\n    matching for two-sided markets, such as the stable marriage\n    problem and the college-admissions problem. Implements Irving's\n    Algorithm for the stable roommate problem. Implements the top\n    trading cycle algorithm for the indivisible goods trading problem.",
    "version": "2.0.0",
    "maintainer": "Jan Tilly <jantilly@gmail.com>",
    "author": "Jan Tilly [aut, cre],\n  Nick Janetos [aut]",
    "url": "https://github.com/jtilly/matchingR/",
    "bug_reports": "https://github.com/jtilly/matchingR/issues/",
    "repository": "https://cran.r-project.org/package=matchingR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matchingR Matching Algorithms in R and C++ Computes matching algorithms quickly using Rcpp.\n    Implements the Gale-Shapley Algorithm to compute the stable\n    matching for two-sided markets, such as the stable marriage\n    problem and the college-admissions problem. Implements Irving's\n    Algorithm for the stable roommate problem. Implements the top\n    trading cycle algorithm for the indivisible goods trading problem.  "
  },
  {
    "id": 15673,
    "package_name": "matrisk",
    "title": "Macroeconomic-at-Risk",
    "description": "The Macroeconomics-at-Risk (MaR) approach is based on a two-step semi-parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the MaR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.",
    "version": "0.1.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Quentin Lajaunie [aut, cre],\n  Guillaume Flament [aut],\n  Christophe Hurlin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrisk Macroeconomic-at-Risk The Macroeconomics-at-Risk (MaR) approach is based on a two-step semi-parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the MaR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.  "
  },
  {
    "id": 15681,
    "package_name": "matrixcalc",
    "title": "Collection of Functions for Matrix Calculations",
    "description": "A collection of functions to support matrix calculations\n        for probability, econometric and numerical analysis. There are\n        additional functions that are comparable to APL functions which\n        are useful for actuarial models such as pension mathematics.\n        This package is used for teaching and research purposes at the\n        Department of Finance and Risk Engineering, New York\n        University, Polytechnic Institute, Brooklyn, NY 11201.\n        Horn, R.A. (1990) Matrix Analysis. ISBN 978-0521386326.\n        Lancaster, P. (1969) Theory of Matrices. ISBN 978-0124355507.\n        Lay, D.C. (1995) Linear Algebra: And Its Applications. ISBN 978-0201845563.",
    "version": "1.0-6",
    "maintainer": "S. Thomas Kelly <tomkellygenetics@gmail.com>",
    "author": "Frederick Novomestky <fnovomes@poly.edu>",
    "url": "",
    "bug_reports": "https://github.com/TomKellyGenetics/matrixcalc/issues",
    "repository": "https://cran.r-project.org/package=matrixcalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrixcalc Collection of Functions for Matrix Calculations A collection of functions to support matrix calculations\n        for probability, econometric and numerical analysis. There are\n        additional functions that are comparable to APL functions which\n        are useful for actuarial models such as pension mathematics.\n        This package is used for teaching and research purposes at the\n        Department of Finance and Risk Engineering, New York\n        University, Polytechnic Institute, Brooklyn, NY 11201.\n        Horn, R.A. (1990) Matrix Analysis. ISBN 978-0521386326.\n        Lancaster, P. (1969) Theory of Matrices. ISBN 978-0124355507.\n        Lay, D.C. (1995) Linear Algebra: And Its Applications. ISBN 978-0201845563.  "
  },
  {
    "id": 15692,
    "package_name": "mau",
    "title": "Decision Models with Multi Attribute Utility Theory",
    "description": "Provides functions for the creation, evaluation and test of decision models based in\n    Multi Attribute Utility Theory (MAUT). Can process and evaluate local risk aversion utilities\n    for a set of indexes, compute utilities and weights for the whole decision tree defining the\n    decision model and simulate weights employing Dirichlet distributions under addition constraints \n    in weights.",
    "version": "0.1.2",
    "maintainer": "Pedro Guarderas <pedro.felipe.guarderas@gmail.com>",
    "author": "Felipe Aguirre [ctb],\n  Julio Andrade [ctb],\n  Pedro Guarderas [aut, cre],\n  Daniel Lagos [ctb],\n  Andr\u00e9s Lopez [ctb],\n  Nelson Recalde [ctb],\n  Edison Salazar [ctb]",
    "url": "https://github.com/pedroguarderas/mau",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mau",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mau Decision Models with Multi Attribute Utility Theory Provides functions for the creation, evaluation and test of decision models based in\n    Multi Attribute Utility Theory (MAUT). Can process and evaluate local risk aversion utilities\n    for a set of indexes, compute utilities and weights for the whole decision tree defining the\n    decision model and simulate weights employing Dirichlet distributions under addition constraints \n    in weights.  "
  },
  {
    "id": 15720,
    "package_name": "mboost",
    "title": "Model-Based Boosting",
    "description": "Functional gradient descent algorithm\n  (boosting) for optimizing general risk functions utilizing\n  component-wise (penalised) least squares estimates or regression\n  trees as base-learners for fitting generalized linear, additive\n  and interaction models to potentially high-dimensional data.\n  Models and algorithms are described in <doi:10.1214/07-STS242>,\n  a hands-on tutorial is available from <doi:10.1007/s00180-012-0382-5>.\n  The package allows user-specified loss functions and base-learners.",
    "version": "2.9-11",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "author": "Torsten Hothorn [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-8301-0471>),\n  Peter Buehlmann [aut] (ORCID: <https://orcid.org/0000-0002-1782-6015>),\n  Thomas Kneib [aut] (ORCID: <https://orcid.org/0000-0003-3390-0972>),\n  Matthias Schmid [aut] (ORCID: <https://orcid.org/0000-0002-0788-0317>),\n  Benjamin Hofner [aut] (ORCID: <https://orcid.org/0000-0003-2810-3186>),\n  Fabian Otto-Sobotka [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9874-1311>),\n  Fabian Scheipl [ctb] (ORCID: <https://orcid.org/0000-0001-8172-3603>),\n  Andreas Mayr [ctb] (ORCID: <https://orcid.org/0000-0001-7106-9732>)",
    "url": "https://github.com/boost-R/mboost",
    "bug_reports": "https://github.com/boost-R/mboost/issues",
    "repository": "https://cran.r-project.org/package=mboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mboost Model-Based Boosting Functional gradient descent algorithm\n  (boosting) for optimizing general risk functions utilizing\n  component-wise (penalised) least squares estimates or regression\n  trees as base-learners for fitting generalized linear, additive\n  and interaction models to potentially high-dimensional data.\n  Models and algorithms are described in <doi:10.1214/07-STS242>,\n  a hands-on tutorial is available from <doi:10.1007/s00180-012-0382-5>.\n  The package allows user-specified loss functions and base-learners.  "
  },
  {
    "id": 15740,
    "package_name": "mccount",
    "title": "Estimate Recurrent Event Burden with Competing Risks",
    "description": "Calculates mean cumulative count (MCC) to estimate the expected cumulative number of recurrent events per person over time in the presence of competing risks and censoring. Implements both the Dong-Yasui equation method and sum of cumulative incidence method described in Dong, et al. (2015) <doi:10.1093/aje/kwu289>. Supports inverse probability weighting for causal inference as outlined in Gaber, et al. (2023) <doi:10.1093/aje/kwad031>. Provides S3 methods for printing, summarizing, plotting, and extracting results. Handles grouped analyses and integrates with 'ggplot2' <https://ggplot2.tidyverse.org/> for visualization.",
    "version": "0.1.1",
    "maintainer": "Kenneth A. Taylor <kenneth.taylor.dpt@gmail.com>",
    "author": "Kenneth A. Taylor [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3205-9280>)",
    "url": "https://github.com/KennethATaylor/mccount,\nhttps://kennethataylor.github.io/mccount/",
    "bug_reports": "https://github.com/KennethATaylor/mccount/issues",
    "repository": "https://cran.r-project.org/package=mccount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mccount Estimate Recurrent Event Burden with Competing Risks Calculates mean cumulative count (MCC) to estimate the expected cumulative number of recurrent events per person over time in the presence of competing risks and censoring. Implements both the Dong-Yasui equation method and sum of cumulative incidence method described in Dong, et al. (2015) <doi:10.1093/aje/kwu289>. Supports inverse probability weighting for causal inference as outlined in Gaber, et al. (2023) <doi:10.1093/aje/kwad031>. Provides S3 methods for printing, summarizing, plotting, and extracting results. Handles grouped analyses and integrates with 'ggplot2' <https://ggplot2.tidyverse.org/> for visualization.  "
  },
  {
    "id": 15750,
    "package_name": "mclustAddons",
    "title": "Addons for the 'mclust' Package",
    "description": "Extend the functionality of the 'mclust' package for Gaussian\n    finite mixture modeling by including: density estimation for data with\n    bounded support (Scrucca, 2019 <doi:10.1002/bimj.201800174>); modal\n    clustering using MEM (Modal EM) algorithm for Gaussian mixtures\n    (Scrucca, 2021 <doi:10.1002/sam.11527>); entropy estimation via\n    Gaussian mixture modeling (Robin & Scrucca, 2023\n    <doi:10.1016/j.csda.2022.107582>); Gaussian mixtures modeling of \n    financial log-returns (Scrucca, 2024 <doi:10.3390/e26110907>).",
    "version": "0.10",
    "maintainer": "Luca Scrucca <luca.scrucca@unibo.it>",
    "author": "Luca Scrucca [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3826-0484>)",
    "url": "https://mclust-org.github.io/mclustAddons/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mclustAddons",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mclustAddons Addons for the 'mclust' Package Extend the functionality of the 'mclust' package for Gaussian\n    finite mixture modeling by including: density estimation for data with\n    bounded support (Scrucca, 2019 <doi:10.1002/bimj.201800174>); modal\n    clustering using MEM (Modal EM) algorithm for Gaussian mixtures\n    (Scrucca, 2021 <doi:10.1002/sam.11527>); entropy estimation via\n    Gaussian mixture modeling (Robin & Scrucca, 2023\n    <doi:10.1016/j.csda.2022.107582>); Gaussian mixtures modeling of \n    financial log-returns (Scrucca, 2024 <doi:10.3390/e26110907>).  "
  },
  {
    "id": 15752,
    "package_name": "mcmapper",
    "title": "Mapping First Moment and C-Statistic to the Parameters of\nDistributions for Risk",
    "description": "Provides a series of numerical methods for extracting parameters of distributions for risks based on knowing the expected value and c-statistics (e.g., from a published report on the performance of a risk prediction model). This package implements the methodology described in Sadatsafavi et al (2024) <doi:10.48550/arXiv.2409.09178>. The core of the package is mcmap(), which takes a pair of (mean, c-statistic) and the distribution type requested. This function provides a generic interface to more customized functions (mcmap_beta(), mcmap_logitnorm(), mcmap_probitnorm()) for specific distributions. ",
    "version": "0.0.11",
    "maintainer": "Mohsen Sadatsafavi <mohsen.sadatsafavi@ubc.ca>",
    "author": "Mohsen Sadatsafavi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0419-7862>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcmapper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcmapper Mapping First Moment and C-Statistic to the Parameters of\nDistributions for Risk Provides a series of numerical methods for extracting parameters of distributions for risks based on knowing the expected value and c-statistics (e.g., from a published report on the performance of a risk prediction model). This package implements the methodology described in Sadatsafavi et al (2024) <doi:10.48550/arXiv.2409.09178>. The core of the package is mcmap(), which takes a pair of (mean, c-statistic) and the distribution type requested. This function provides a generic interface to more customized functions (mcmap_beta(), mcmap_logitnorm(), mcmap_probitnorm()) for specific distributions.   "
  },
  {
    "id": 15759,
    "package_name": "mcmodule",
    "title": "Modular Monte Carlo Risk Analysis",
    "description": "Framework for building modular Monte Carlo risk analysis models. It extends the capabilities of 'mc2d' to facilitate working with multiple risk pathways, variates and scenarios. It provides tools to organize risk analysis in independent flexible modules, perform multivariate Monte Carlo node operations, automate the creation of Monte Carlo nodes and visualize risk analysis models. For more details see Ciria (2025) <https://nataliaciria.github.io/mcmodule/articles/mcmodule>.",
    "version": "1.1.1",
    "maintainer": "Natalia Ciria <nataliaciria@hotmail.com>",
    "author": "Natalia Ciria [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0006-2669-6634>),\n  Alberto Allepuz [ths] (ORCID: <https://orcid.org/0000-0003-3518-1991>),\n  Giovanna Ciaravino [ths] (ORCID:\n    <https://orcid.org/0000-0002-5796-8093>)",
    "url": "https://nataliaciria.github.io/mcmodule/,\nhttps://github.com/NataliaCiria/mcmodule",
    "bug_reports": "https://github.com/NataliaCiria/mcmodule/issues",
    "repository": "https://cran.r-project.org/package=mcmodule",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcmodule Modular Monte Carlo Risk Analysis Framework for building modular Monte Carlo risk analysis models. It extends the capabilities of 'mc2d' to facilitate working with multiple risk pathways, variates and scenarios. It provides tools to organize risk analysis in independent flexible modules, perform multivariate Monte Carlo node operations, automate the creation of Monte Carlo nodes and visualize risk analysis models. For more details see Ciria (2025) <https://nataliaciria.github.io/mcmodule/articles/mcmodule>.  "
  },
  {
    "id": 15882,
    "package_name": "metaSurvival",
    "title": "Meta-Analysis of a Single Survival Curve",
    "description": "To assess a summary survival curve from survival probabilities and number of at-risk patients collected at various points in time in various studies, and to test the between-strata heterogeneity.",
    "version": "0.1.0",
    "maintainer": "Shubhram Pandey <shubhram1992@gmail.com>",
    "author": "Shubhram Pandey [aut, cre]",
    "url": "https://github.com/shubhrampandey/metaSurvival",
    "bug_reports": "https://github.com/shubhrampandey/metaSurvival/issues",
    "repository": "https://cran.r-project.org/package=metaSurvival",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaSurvival Meta-Analysis of a Single Survival Curve To assess a summary survival curve from survival probabilities and number of at-risk patients collected at various points in time in various studies, and to test the between-strata heterogeneity.  "
  },
  {
    "id": 15886,
    "package_name": "metabolic",
    "title": "Datasets and Functions for Reproducing Meta-Analyses",
    "description": "Dataset and functions from the meta-analysis published in Medicine & Science in Sports & Exercise. \n    It contains all the data and functions to reproduce the analysis.\n    \"Effectiveness of HIIE versus MICT in Improving Cardiometabolic Risk Factors in Health and Disease: A Meta-analysis\".\n    Felipe Mattioni Maturana, Peter Martus, Stephan Zipfel, Andreas M Nie\u00df (2020) <doi:10.1249/MSS.0000000000002506>.",
    "version": "0.1.2",
    "maintainer": "Felipe Mattioni Maturana <felipe.mattioni@med.uni-tuebingen.de>",
    "author": "Felipe Mattioni Maturana [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4221-6104>)",
    "url": "https://github.com/fmmattioni/metabolic",
    "bug_reports": "https://github.com/fmmattioni/metabolic/issues",
    "repository": "https://cran.r-project.org/package=metabolic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metabolic Datasets and Functions for Reproducing Meta-Analyses Dataset and functions from the meta-analysis published in Medicine & Science in Sports & Exercise. \n    It contains all the data and functions to reproduce the analysis.\n    \"Effectiveness of HIIE versus MICT in Improving Cardiometabolic Risk Factors in Health and Disease: A Meta-analysis\".\n    Felipe Mattioni Maturana, Peter Martus, Stephan Zipfel, Andreas M Nie\u00df (2020) <doi:10.1249/MSS.0000000000002506>.  "
  },
  {
    "id": 15897,
    "package_name": "metafolio",
    "title": "Metapopulation Simulations for Conserving Salmon Through\nPortfolio Optimization",
    "description": "A tool to simulate salmon metapopulations and apply financial\n    portfolio optimization concepts. The package accompanies the paper \n    Anderson et al. (2015) <doi:10.1101/2022.03.24.485545>.",
    "version": "0.1.2",
    "maintainer": "Sean C. Anderson <sean@seananderson.ca>",
    "author": "Sean C. Anderson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9563-1937>),\n  Jonathan W. Moore [ctb],\n  Michelle M. McClure [ctb],\n  Nicholas K. Dulvy [ctb],\n  Andrew B. Cooper [ctb]",
    "url": "https://github.com/seananderson/metafolio",
    "bug_reports": "https://github.com/seananderson/metafolio/issues",
    "repository": "https://cran.r-project.org/package=metafolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metafolio Metapopulation Simulations for Conserving Salmon Through\nPortfolio Optimization A tool to simulate salmon metapopulations and apply financial\n    portfolio optimization concepts. The package accompanies the paper \n    Anderson et al. (2015) <doi:10.1101/2022.03.24.485545>.  "
  },
  {
    "id": 15936,
    "package_name": "metavcov",
    "title": "Computing Variances and Covariances, Visualization and Missing\nData Solution for Multivariate Meta-Analysis",
    "description": "Collection of functions to compute within-study covariances for different effect sizes, data visualization, and single and multiple imputations for missing data. Effect sizes include correlation (r), mean difference (MD), standardized mean difference (SMD), log odds ratio (logOR), log risk ratio (logRR), and risk difference (RD).",
    "version": "2.1.5",
    "maintainer": "Min Lu <m.lu6@umiami.edu>",
    "author": "Min Lu <m.lu6@umiami.edu>",
    "url": "https://github.com/luminwin/metavcov",
    "bug_reports": "https://github.com/luminwin/metavcov/issues/new",
    "repository": "https://cran.r-project.org/package=metavcov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metavcov Computing Variances and Covariances, Visualization and Missing\nData Solution for Multivariate Meta-Analysis Collection of functions to compute within-study covariances for different effect sizes, data visualization, and single and multiple imputations for missing data. Effect sizes include correlation (r), mean difference (MD), standardized mean difference (SMD), log odds ratio (logOR), log risk ratio (logRR), and risk difference (RD).  "
  },
  {
    "id": 15959,
    "package_name": "mfGARCH",
    "title": "Mixed-Frequency GARCH Models",
    "description": "Estimating GARCH-MIDAS (MIxed-DAta-Sampling) models (Engle, Ghysels, Sohn, 2013, <doi:10.1162/REST_a_00300>) and related statistical inference, accompanying the paper \"Two are better than one: Volatility forecasting using multiplicative component GARCH models\" by Conrad and Kleen (2020, <doi:10.1002/jae.2742>). The GARCH-MIDAS model decomposes the conditional variance of (daily) stock returns into a short- and long-term component, where the latter may depend on an exogenous covariate sampled at a lower frequency. ",
    "version": "0.2.1",
    "maintainer": "Onno Kleen <r@onnokleen.de>",
    "author": "Onno Kleen [aut, cre] (ORCID: <https://orcid.org/0000-0003-4731-4640>)",
    "url": "https://github.com/onnokleen/mfGARCH/",
    "bug_reports": "https://github.com/onnokleen/mfGARCH/issues",
    "repository": "https://cran.r-project.org/package=mfGARCH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mfGARCH Mixed-Frequency GARCH Models Estimating GARCH-MIDAS (MIxed-DAta-Sampling) models (Engle, Ghysels, Sohn, 2013, <doi:10.1162/REST_a_00300>) and related statistical inference, accompanying the paper \"Two are better than one: Volatility forecasting using multiplicative component GARCH models\" by Conrad and Kleen (2020, <doi:10.1002/jae.2742>). The GARCH-MIDAS model decomposes the conditional variance of (daily) stock returns into a short- and long-term component, where the latter may depend on an exogenous covariate sampled at a lower frequency.   "
  },
  {
    "id": 15964,
    "package_name": "mfpp",
    "title": "'Matrix-Based Flexible Project Planning'",
    "description": "Matrix-Based Flexible Project Planning. This package models, plans, and schedules flexible, such as agile, extreme, and hybrid project plans. The package contains project planning, scheduling, and risk assessment functions. Kosztyan (2022) <doi:10.1016/j.softx.2022.100973>.",
    "version": "0.0.8",
    "maintainer": "Zsolt T. Kosztyan <kosztyan.zsolt@gtk.uni-pannon.hu>",
    "author": "Zsolt T. Kosztyan [aut, cre],\n  Aamir Sagir [aut]",
    "url": "https://github.com/kzst/mfpp",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mfpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mfpp 'Matrix-Based Flexible Project Planning' Matrix-Based Flexible Project Planning. This package models, plans, and schedules flexible, such as agile, extreme, and hybrid project plans. The package contains project planning, scheduling, and risk assessment functions. Kosztyan (2022) <doi:10.1016/j.softx.2022.100973>.  "
  },
  {
    "id": 16006,
    "package_name": "miceafter",
    "title": "Data and Statistical Analyses after Multiple Imputation",
    "description": "\n   Statistical Analyses and Pooling after Multiple Imputation. A large variety \n   of repeated statistical analysis can be performed and finally pooled. Statistical analysis \n   that are available are, among others, Levene's test, Odds and Risk Ratios, One sample \n   proportions, difference between proportions and linear and logistic regression models. \n   Functions can also be used in combination with the Pipe operator. \n   More and more statistical analyses and pooling functions will be added over time.\n   Heymans (2007) <doi:10.1186/1471-2288-7-33>.\n   Eekhout (2017) <doi:10.1186/s12874-017-0404-7>.\n\t Wiel (2009) <doi:10.1093/biostatistics/kxp011>.\n\t Marshall (2009) <doi:10.1186/1471-2288-9-57>.\n\t Sidi (2021) <doi:10.1080/00031305.2021.1898468>.\n\t Lott (2018) <doi:10.1080/00031305.2018.1473796>.\n\t Grund (2021) <doi:10.31234/osf.io/d459g>.",
    "version": "0.5.0",
    "maintainer": "Martijn Heymans <mw.heymans@amsterdamumc.nl>",
    "author": "Martijn Heymans [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-3889-0921>),\n  Jaap Brand [ctb]",
    "url": "https://mwheymans.github.io/miceafter/",
    "bug_reports": "https://github.com/mwheymans/miceafter/issues",
    "repository": "https://cran.r-project.org/package=miceafter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "miceafter Data and Statistical Analyses after Multiple Imputation \n   Statistical Analyses and Pooling after Multiple Imputation. A large variety \n   of repeated statistical analysis can be performed and finally pooled. Statistical analysis \n   that are available are, among others, Levene's test, Odds and Risk Ratios, One sample \n   proportions, difference between proportions and linear and logistic regression models. \n   Functions can also be used in combination with the Pipe operator. \n   More and more statistical analyses and pooling functions will be added over time.\n   Heymans (2007) <doi:10.1186/1471-2288-7-33>.\n   Eekhout (2017) <doi:10.1186/s12874-017-0404-7>.\n\t Wiel (2009) <doi:10.1093/biostatistics/kxp011>.\n\t Marshall (2009) <doi:10.1186/1471-2288-9-57>.\n\t Sidi (2021) <doi:10.1080/00031305.2021.1898468>.\n\t Lott (2018) <doi:10.1080/00031305.2018.1473796>.\n\t Grund (2021) <doi:10.31234/osf.io/d459g>.  "
  },
  {
    "id": 16050,
    "package_name": "migrate",
    "title": "Create Credit State Migration (Transition) Matrices",
    "description": "Tools to help convert credit risk data at two timepoints\n    into traditional credit state migration (aka, \"transition\") matrices.\n    At a higher level, 'migrate' is intended to help an analyst understand\n    how risk moved in their credit portfolio over a time interval.\n    References to this methodology include:\n    1. Schuermann, T. (2008) <doi:10.1002/9780470061596.risk0409>.\n    2. Perederiy, V. (2017) <doi:10.48550/arXiv.1708.00062>.",
    "version": "0.5.0",
    "maintainer": "Michael Thomas <mthomas@ketchbrookanalytics.com>",
    "author": "Michael Thomas [aut, cre],\n  Brad Lindblad [ctb],\n  Ivan Millanes [ctb]",
    "url": "https://github.com/ketchbrookanalytics/migrate,\nhttps://ketchbrookanalytics.github.io/migrate/",
    "bug_reports": "https://github.com/ketchbrookanalytics/migrate/issues",
    "repository": "https://cran.r-project.org/package=migrate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "migrate Create Credit State Migration (Transition) Matrices Tools to help convert credit risk data at two timepoints\n    into traditional credit state migration (aka, \"transition\") matrices.\n    At a higher level, 'migrate' is intended to help an analyst understand\n    how risk moved in their credit portfolio over a time interval.\n    References to this methodology include:\n    1. Schuermann, T. (2008) <doi:10.1002/9780470061596.risk0409>.\n    2. Perederiy, V. (2017) <doi:10.48550/arXiv.1708.00062>.  "
  },
  {
    "id": 16110,
    "package_name": "miscFuncs",
    "title": "Miscellaneous Useful Functions Including LaTeX Tables, Kalman\nFiltering, QQplots with Simulation-Based Confidence Intervals,\nLinear Regression Diagnostics and Development Tools",
    "description": "Implementing various things including functions for LaTeX tables,\n    the Kalman filter, QQ-plots with simulation-based confidence intervals, linear regression diagnostics, web scraping, development tools, relative risk and odds\n    rati, GARCH(1,1) Forecasting.",
    "version": "1.5-10",
    "maintainer": "Benjamin M. Taylor <benjamin.taylor.software@gmail.com>",
    "author": "Benjamin M. Taylor [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=miscFuncs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "miscFuncs Miscellaneous Useful Functions Including LaTeX Tables, Kalman\nFiltering, QQplots with Simulation-Based Confidence Intervals,\nLinear Regression Diagnostics and Development Tools Implementing various things including functions for LaTeX tables,\n    the Kalman filter, QQ-plots with simulation-based confidence intervals, linear regression diagnostics, web scraping, development tools, relative risk and odds\n    rati, GARCH(1,1) Forecasting.  "
  },
  {
    "id": 16119,
    "package_name": "missDeaths",
    "title": "Simulating and Analyzing Time to Event Data in the Presence of\nPopulation Mortality",
    "description": "Implements two methods: a nonparametric risk adjustment and a\n    data imputation method that use general population mortality tables to allow a\n    correct analysis of time to disease recurrence. Also includes a powerful set of\n    object oriented survival data simulation functions.",
    "version": "2.8",
    "maintainer": "Tomaz Stupnik <tomaz.stupnik@guest.arnes.si>",
    "author": "Tomaz Stupnik [aut, cre],\n  Maja Pohar Perme [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=missDeaths",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "missDeaths Simulating and Analyzing Time to Event Data in the Presence of\nPopulation Mortality Implements two methods: a nonparametric risk adjustment and a\n    data imputation method that use general population mortality tables to allow a\n    correct analysis of time to disease recurrence. Also includes a powerful set of\n    object oriented survival data simulation functions.  "
  },
  {
    "id": 16253,
    "package_name": "mlspatial",
    "title": "Machine Learning and Mapping for Spatial Epidemiology",
    "description": "Provides tools for the integration, visualisation, and modelling of spatial epidemiological data using the method described in Azeez, A., & Noel, C. (2025). 'Predictive Modelling and Spatial Distribution of Pancreatic Cancer in Africa Using Machine Learning-Based Spatial Model' <doi:10.5281/zenodo.16529986> and <doi:10.5281/zenodo.16529016>. It facilitates the analysis of geographic health data by combining modern spatial mapping tools with advanced machine learning (ML) algorithms. 'mlspatial' enables users to import and pre-process shapefile and associated demographic or disease incidence data, generate richly annotated thematic maps, and apply predictive models, including Random Forest, 'XGBoost', and Support Vector Regression, to identify spatial patterns and risk factors. It is suited for spatial epidemiologists, public health researchers, and GIS analysts aiming to uncover hidden geographic patterns in health-related outcomes and inform evidence-based interventions.",
    "version": "0.1.0",
    "maintainer": "Adeboye Azeez <azizadeboye@gmail.com>",
    "author": "Adeboye Azeez [aut, cre],\n  Colin Noel [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlspatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlspatial Machine Learning and Mapping for Spatial Epidemiology Provides tools for the integration, visualisation, and modelling of spatial epidemiological data using the method described in Azeez, A., & Noel, C. (2025). 'Predictive Modelling and Spatial Distribution of Pancreatic Cancer in Africa Using Machine Learning-Based Spatial Model' <doi:10.5281/zenodo.16529986> and <doi:10.5281/zenodo.16529016>. It facilitates the analysis of geographic health data by combining modern spatial mapping tools with advanced machine learning (ML) algorithms. 'mlspatial' enables users to import and pre-process shapefile and associated demographic or disease incidence data, generate richly annotated thematic maps, and apply predictive models, including Random Forest, 'XGBoost', and Support Vector Regression, to identify spatial patterns and risk factors. It is suited for spatial epidemiologists, public health researchers, and GIS analysts aiming to uncover hidden geographic patterns in health-related outcomes and inform evidence-based interventions.  "
  },
  {
    "id": 16275,
    "package_name": "mmeta",
    "title": "Multivariate Meta-Analysis",
    "description": "Multiple 2 by 2 tables often arise in meta-analysis which combines statistical evidence from multiple studies. Two risks within the same study are possibly correlated because they share some common factors such as environment and population structure. This package implements a set of novel Bayesian approaches for multivariate meta analysis when the risks within the same study are independent or correlated. The exact posterior inference of odds ratio, relative risk, and risk difference given either a single 2 by 2 table or multiple 2 by 2 tables is provided. Luo, Chen, Su, Chu, (2014) <doi:10.18637/jss.v056.i11>, Chen, Luo, (2011) <doi:10.1002/sim.4248>, Chen, Chu, Luo, Nie, Chen, (2015) <doi:10.1177/0962280211430889>, Chen, Luo, Chu, Su, Nie, (2014) <doi:10.1080/03610926.2012.700379>, Chen, Luo, Chu, Wei, (2013) <doi:10.1080/19466315.2013.791483>.",
    "version": "3.0.2",
    "maintainer": "Bingyu Zhang <bingyuz7@sas.upenn.edu>",
    "author": "Sheng Luo [aut],\n  Yong Chen [aut],\n  Xiao Su [aut],\n  Haitao Chu [aut],\n  Bingyu Zhang [cre],\n  Wally Gilks [ctb],\n  Giovanni Petris [ctb],\n  Luca Tardella [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmeta Multivariate Meta-Analysis Multiple 2 by 2 tables often arise in meta-analysis which combines statistical evidence from multiple studies. Two risks within the same study are possibly correlated because they share some common factors such as environment and population structure. This package implements a set of novel Bayesian approaches for multivariate meta analysis when the risks within the same study are independent or correlated. The exact posterior inference of odds ratio, relative risk, and risk difference given either a single 2 by 2 table or multiple 2 by 2 tables is provided. Luo, Chen, Su, Chu, (2014) <doi:10.18637/jss.v056.i11>, Chen, Luo, (2011) <doi:10.1002/sim.4248>, Chen, Chu, Luo, Nie, Chen, (2015) <doi:10.1177/0962280211430889>, Chen, Luo, Chu, Su, Nie, (2014) <doi:10.1080/03610926.2012.700379>, Chen, Luo, Chu, Wei, (2013) <doi:10.1080/19466315.2013.791483>.  "
  },
  {
    "id": 16315,
    "package_name": "modelDown",
    "title": "Make Static HTML Website for Predictive Models",
    "description": "Website generator with HTML summaries for predictive models.\n    This package uses 'DALEX' explainers to describe global model behavior. \n    We can see how well models behave (tabs: Model Performance, Auditor),\n    how much each variable contributes to predictions (tabs: Variable Response) \n    and which variables are the most important for a given model (tabs: Variable Importance).\n    We can also compare Concept Drift for pairs of models (tabs: Drifter).\n    Additionally, data available on the website can be easily recreated in current R session.\n    Work on this package was financially supported by the NCN Opus grant 2017/27/B/ST6/01307 \n    at Warsaw University of Technology, Faculty of Mathematics and Information Science.",
    "version": "1.1",
    "maintainer": "Kamil Romaszko <kamil.romaszko@gmail.com>",
    "author": "Przemys\u0142aw Biecek [aut],\n  Magda Tatarynowicz [aut],\n  Kamil Romaszko [aut, cre],\n  Mateusz Urba\u0144ski [aut]",
    "url": "https://github.com/ModelOriented/modelDown",
    "bug_reports": "https://github.com/ModelOriented/modelDown/issues",
    "repository": "https://cran.r-project.org/package=modelDown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modelDown Make Static HTML Website for Predictive Models Website generator with HTML summaries for predictive models.\n    This package uses 'DALEX' explainers to describe global model behavior. \n    We can see how well models behave (tabs: Model Performance, Auditor),\n    how much each variable contributes to predictions (tabs: Variable Response) \n    and which variables are the most important for a given model (tabs: Variable Importance).\n    We can also compare Concept Drift for pairs of models (tabs: Drifter).\n    Additionally, data available on the website can be easily recreated in current R session.\n    Work on this package was financially supported by the NCN Opus grant 2017/27/B/ST6/01307 \n    at Warsaw University of Technology, Faculty of Mathematics and Information Science.  "
  },
  {
    "id": 16323,
    "package_name": "modelimpact",
    "title": "Functions to Assess the Business Impact of Churn Prediction\nModels",
    "description": "Calculate the financial impact of using a churn model in terms of cost, revenue, profit and return on investment.",
    "version": "1.0.0",
    "maintainer": "Peer Christensen <hr.pchristensen@gmail.com>",
    "author": "Peer Christensen",
    "url": "https://github.com/PeerChristensen/modelimpact",
    "bug_reports": "https://github.com/PeerChristensen/modelimpact/issues",
    "repository": "https://cran.r-project.org/package=modelimpact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modelimpact Functions to Assess the Business Impact of Churn Prediction\nModels Calculate the financial impact of using a churn model in terms of cost, revenue, profit and return on investment.  "
  },
  {
    "id": 16361,
    "package_name": "monitOS",
    "title": "Monitoring Overall Survival in Pivotal Trials in Indolent\nCancers",
    "description": "These guidelines are meant to provide a pragmatic, yet rigorous, help to drug developers and decision makers, since they are shaped by three fundamental ingredients: the clinically determined margin of detriment on OS that is unacceptably high (delta null); the benefit on OS that is plausible given the mechanism of action of the novel intervention (delta alt); and the quantity of information (i.e. survival events) it is feasible to accrue given the clinical and drug development setting. The proposed guidelines facilitate transparent discussions between stakeholders focusing on the risks of erroneous decisions and what might be an acceptable trade-off between power and the false positive error rate.",
    "version": "0.1.6",
    "maintainer": "Thibaud Coroller <thibaud.coroller@novartis.com>",
    "author": "Thomas Fleming [ctb],\n  Lisa Hampson [aut],\n  Bharani Bharani-Dharan [ctb],\n  Frank Bretz [ctb],\n  Arunava Chakravartty [ctb],\n  Thibaud Coroller [aut, cre],\n  Evanthia Koukouli [aut],\n  Janet Wittes [ctb],\n  Nigel Yateman [ctb],\n  Emmanuel Zuber [ctb],\n  Novartis Pharma AG [cph]",
    "url": "https://opensource.nibr.com/monitOS/,\nhttps://github.com/Novartis/monitOS",
    "bug_reports": "https://github.com/Novartis/monitOS/issues",
    "repository": "https://cran.r-project.org/package=monitOS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monitOS Monitoring Overall Survival in Pivotal Trials in Indolent\nCancers These guidelines are meant to provide a pragmatic, yet rigorous, help to drug developers and decision makers, since they are shaped by three fundamental ingredients: the clinically determined margin of detriment on OS that is unacceptably high (delta null); the benefit on OS that is plausible given the mechanism of action of the novel intervention (delta alt); and the quantity of information (i.e. survival events) it is feasible to accrue given the clinical and drug development setting. The proposed guidelines facilitate transparent discussions between stakeholders focusing on the risks of erroneous decisions and what might be an acceptable trade-off between power and the false positive error rate.  "
  },
  {
    "id": 16365,
    "package_name": "monobin",
    "title": "Monotonic Binning for Credit Rating Models",
    "description": "Performs monotonic binning of numeric risk factor in credit rating models (PD, LGD, EAD) \n\tdevelopment. All functions handle both binary and continuous target variable. \n\tFunctions that use isotonic regression in the first stage of binning process have an additional \n\tfeature for correction of minimum percentage of observations and minimum target rate per bin. \t\n\tAdditionally, monotonic trend can be identified based on raw data or, if known in advance,\n\tforced by functions' argument. Missing values and other possible special values are treated \n\tseparately from so-called complete cases.",
    "version": "0.2.4",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/monobin",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=monobin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monobin Monotonic Binning for Credit Rating Models Performs monotonic binning of numeric risk factor in credit rating models (PD, LGD, EAD) \n\tdevelopment. All functions handle both binary and continuous target variable. \n\tFunctions that use isotonic regression in the first stage of binning process have an additional \n\tfeature for correction of minimum percentage of observations and minimum target rate per bin. \t\n\tAdditionally, monotonic trend can be identified based on raw data or, if known in advance,\n\tforced by functions' argument. Missing values and other possible special values are treated \n\tseparately from so-called complete cases.  "
  },
  {
    "id": 16366,
    "package_name": "monobinShiny",
    "title": "Shiny User Interface for 'monobin' Package",
    "description": "This is an add-on package to the 'monobin' package that simplifies its use. It provides shiny-based user interface (UI) \n\t     that is especially handy for less experienced 'R' users as well as for those who intend to perform quick scanning \n\t     of numeric risk factors when building credit rating models. The additional functions implemented in \n\t     'monobinShiny' that do no exist in 'monobin' package are: descriptive statistics, special case and outliers imputation. \n\t     The function descriptive statistics is exported and can be used in 'R' sessions independently from the user interface, \n\t     while special case and outlier imputation functions are written to be used with shiny UI.",
    "version": "0.1.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/monobinShiny",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=monobinShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monobinShiny Shiny User Interface for 'monobin' Package This is an add-on package to the 'monobin' package that simplifies its use. It provides shiny-based user interface (UI) \n\t     that is especially handy for less experienced 'R' users as well as for those who intend to perform quick scanning \n\t     of numeric risk factors when building credit rating models. The additional functions implemented in \n\t     'monobinShiny' that do no exist in 'monobin' package are: descriptive statistics, special case and outliers imputation. \n\t     The function descriptive statistics is exported and can be used in 'R' sessions independently from the user interface, \n\t     while special case and outlier imputation functions are written to be used with shiny UI.  "
  },
  {
    "id": 16371,
    "package_name": "monoreg",
    "title": "Bayesian Monotonic Regression Using a Marked Point Process\nConstruction",
    "description": "An extended version of the nonparametric Bayesian monotonic regression procedure described in Saarela & Arjas (2011) <DOI:10.1111/j.1467-9469.2010.00716.x>, allowing for multiple additive monotonic components in the linear predictor, and time-to-event outcomes through case-base sampling. The extension and its applications, including estimation of absolute risks, are described in Saarela & Arjas (2015) <DOI:10.1111/sjos.12125>. The package also implements the nonparametric ordinal regression model described in Saarela, Rohrbeck & Arjas <DOI:10.1214/22-BA1310>.",
    "version": "2.1",
    "maintainer": "Olli Saarela <olli.saarela@utoronto.ca>",
    "author": "Olli Saarela, Christian Rohrbeck",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=monoreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monoreg Bayesian Monotonic Regression Using a Marked Point Process\nConstruction An extended version of the nonparametric Bayesian monotonic regression procedure described in Saarela & Arjas (2011) <DOI:10.1111/j.1467-9469.2010.00716.x>, allowing for multiple additive monotonic components in the linear predictor, and time-to-event outcomes through case-base sampling. The extension and its applications, including estimation of absolute risks, are described in Saarela & Arjas (2015) <DOI:10.1111/sjos.12125>. The package also implements the nonparametric ordinal regression model described in Saarela, Rohrbeck & Arjas <DOI:10.1214/22-BA1310>.  "
  },
  {
    "id": 16373,
    "package_name": "monotonicity",
    "title": "Test for Monotonicity in Expected Asset Returns, Sorted by\nPortfolios",
    "description": "Test for monotonicity in financial variables sorted by portfolios. It is conventional practice in empirical research to form portfolios of assets ranked by a certain sort variable. A t-test is then used to consider the mean return spread between the portfolios with the highest and lowest values of the sort variable. Yet comparing only the average returns on the top and bottom portfolios does not provide a sufficient way to test for a monotonic relation between expected returns and the sort variable. This package provides nonparametric tests for the full set of monotonic patterns by Patton, A. and Timmermann, A. (2010) <doi:10.1016/j.jfineco.2010.06.006> and compares the proposed results with extant alternatives such as t-tests, Bonferroni bounds, and multivariate inequality tests through empirical applications and simulations.",
    "version": "1.3.1",
    "maintainer": "Siegfried K\u00f6stlmeier <siegfried.koestlmeier@gmail.com>",
    "author": "Siegfried K\u00f6stlmeier [aut, cre, trl] (ORCID:\n    <https://orcid.org/0000-0002-7221-6981>)",
    "url": "https://github.com/skoestlmeier/monotonicity",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=monotonicity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monotonicity Test for Monotonicity in Expected Asset Returns, Sorted by\nPortfolios Test for monotonicity in financial variables sorted by portfolios. It is conventional practice in empirical research to form portfolios of assets ranked by a certain sort variable. A t-test is then used to consider the mean return spread between the portfolios with the highest and lowest values of the sort variable. Yet comparing only the average returns on the top and bottom portfolios does not provide a sufficient way to test for a monotonic relation between expected returns and the sort variable. This package provides nonparametric tests for the full set of monotonic patterns by Patton, A. and Timmermann, A. (2010) <doi:10.1016/j.jfineco.2010.06.006> and compares the proposed results with extant alternatives such as t-tests, Bonferroni bounds, and multivariate inequality tests through empirical applications and simulations.  "
  },
  {
    "id": 16393,
    "package_name": "morse",
    "title": "Modelling Reproduction and Survival Data in Ecotoxicology",
    "description": "Advanced methods for a valuable quantitative environmental risk \n   assessment using Bayesian inference of survival and reproduction Data. Among\n   others, it facilitates Bayesian inference of the general unified\n   threshold model of survival (GUTS). See our companion paper \n   Baudrot and Charles (2021) <doi:10.21105/joss.03200>,\n   as well as complementary details in Baudrot et al. (2018)\n   <doi:10.1021/acs.est.7b05464> and Delignette-Muller et al.\n   (2017) <doi:10.1021/acs.est.6b05326>.",
    "version": "3.3.4",
    "maintainer": "Virgile Baudrot <virgile.baudrot@qonfluens.com>",
    "author": "Virgile Baudrot [aut, cre],\n  Sandrine Charles [aut],\n  Marie Laure Delignette-Muller [aut],\n  Wandrille Duchemin [ctb],\n  Benoit Goussen [ctb],\n  Nils Kehrein [ctb],\n  Guillaume Kon-Kam-King [ctb],\n  Christelle Lopes [ctb],\n  Philippe Ruiz [ctb],\n  Alexander Singer [ctb],\n  Philippe Veber [aut]",
    "url": "https://gitlab.in2p3.fr/mosaic-software/morse",
    "bug_reports": "https://gitlab.in2p3.fr/mosaic-software/morse/-/issues",
    "repository": "https://cran.r-project.org/package=morse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morse Modelling Reproduction and Survival Data in Ecotoxicology Advanced methods for a valuable quantitative environmental risk \n   assessment using Bayesian inference of survival and reproduction Data. Among\n   others, it facilitates Bayesian inference of the general unified\n   threshold model of survival (GUTS). See our companion paper \n   Baudrot and Charles (2021) <doi:10.21105/joss.03200>,\n   as well as complementary details in Baudrot et al. (2018)\n   <doi:10.1021/acs.est.7b05464> and Delignette-Muller et al.\n   (2017) <doi:10.1021/acs.est.6b05326>.  "
  },
  {
    "id": 16394,
    "package_name": "morseDR",
    "title": "Bayesian Inference of Binary, Count and Continuous Data in\nToxicology",
    "description": "Advanced methods for a valuable quantitative environmental\n    risk assessment using Bayesian inference of several type of\n    toxicological data. 'binary' (e.g., survival, mobility), 'count'\n    (e.g., reproduction) and 'continuous' (e.g., growth as length,\n    weight).  Estimation procedures can be used without a deep knowledge\n    of their underlying probabilistic model or inference methods. Rather,\n    they were designed to behave as well as possible without requiring a\n    user to provide values for some obscure parameters. That said, models\n    can also be used as a first step to tailor new models for more\n    specific situations.",
    "version": "0.1.2",
    "maintainer": "Virgile Baudrot <virgile.baudrot@qonfluens.com>",
    "author": "Virgile Baudrot [aut, cre],\n  Sandrine Charles [aut],\n  Marie Laure Delignette-Muller [aut],\n  Nils Kehrein [ctb],\n  Guillaume Kon-Kam-King [ctb],\n  Christelle Lopes [ctb],\n  Philippe Veber [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=morseDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morseDR Bayesian Inference of Binary, Count and Continuous Data in\nToxicology Advanced methods for a valuable quantitative environmental\n    risk assessment using Bayesian inference of several type of\n    toxicological data. 'binary' (e.g., survival, mobility), 'count'\n    (e.g., reproduction) and 'continuous' (e.g., growth as length,\n    weight).  Estimation procedures can be used without a deep knowledge\n    of their underlying probabilistic model or inference methods. Rather,\n    they were designed to behave as well as possible without requiring a\n    user to provide values for some obscure parameters. That said, models\n    can also be used as a first step to tailor new models for more\n    specific situations.  "
  },
  {
    "id": 16395,
    "package_name": "morseTKTD",
    "title": "Bayesian Inference of TKTD Models",
    "description": "Advanced methods for a valuable quantitative environmental risk \n   assessment using Bayesian inference of survival Data with toxicokinetics\n   toxicodynamics (TKTD) models. Among others, it facilitates Bayesian inference of \n   the general unified threshold model of survival (GUTS). See models description\n   in Jager et al. (2011) <doi:10.1021/es103092a> and implementation\n   using Bayesian inference in Baudrot and Charles (2019)\n   <doi:10.1038/s41598-019-47698-0>.",
    "version": "0.1.3",
    "maintainer": "Virgile Baudrot <virgile.baudrot@qonfluens.com>",
    "author": "Virgile Baudrot [aut, cre],\n  Sandrine Charles [aut],\n  Marie Laure Delignette-Muller [aut],\n  Benoit Goussen [ctb],\n  Nils Kehrein [ctb],\n  Guillaume Kon-Kam-King [ctb],\n  Christelle Lopes [ctb],\n  Alexander Singer [ctb],\n  Philippe Veber [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=morseTKTD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morseTKTD Bayesian Inference of TKTD Models Advanced methods for a valuable quantitative environmental risk \n   assessment using Bayesian inference of survival Data with toxicokinetics\n   toxicodynamics (TKTD) models. Among others, it facilitates Bayesian inference of \n   the general unified threshold model of survival (GUTS). See models description\n   in Jager et al. (2011) <doi:10.1021/es103092a> and implementation\n   using Bayesian inference in Baudrot and Charles (2019)\n   <doi:10.1038/s41598-019-47698-0>.  "
  },
  {
    "id": 16426,
    "package_name": "movieROC",
    "title": "Visualizing the Decision Rules Underlying Binary Classification",
    "description": "Visualization of decision rules for binary classification and Receiver Operating Characteristic (ROC) curve estimation under different generalizations proposed in the literature:\n  - making the classification subsets flexible to cover those scenarios where both extremes of the\n  marker are associated with a higher risk of being positive, considering two thresholds \n  (gROC() function);\n  - transforming the marker by a proper function trying to improve the classification performance \n  (hROC() function);\n  - when dealing with multivariate markers, considering a proper transformation to univariate space \n  trying to maximize the resulting AUC of the TPR for each FPR (multiROC() function).\n  The classification regions behind each point of the ROC curve are displayed in both static \n  graphics (plot_buildROC(), plot_regions() or plot_funregions() function) or \n  videos (movieROC() function).",
    "version": "0.1.2",
    "maintainer": "Sonia Perez-Fernandez <perezsonia@uniovi.es>",
    "author": "Sonia Perez-Fernandez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2767-6399>)",
    "url": "",
    "bug_reports": "https://github.com/perezsonia/movieROC/issues",
    "repository": "https://cran.r-project.org/package=movieROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "movieROC Visualizing the Decision Rules Underlying Binary Classification Visualization of decision rules for binary classification and Receiver Operating Characteristic (ROC) curve estimation under different generalizations proposed in the literature:\n  - making the classification subsets flexible to cover those scenarios where both extremes of the\n  marker are associated with a higher risk of being positive, considering two thresholds \n  (gROC() function);\n  - transforming the marker by a proper function trying to improve the classification performance \n  (hROC() function);\n  - when dealing with multivariate markers, considering a proper transformation to univariate space \n  trying to maximize the resulting AUC of the TPR for each FPR (multiROC() function).\n  The classification regions behind each point of the ROC curve are displayed in both static \n  graphics (plot_buildROC(), plot_regions() or plot_funregions() function) or \n  videos (movieROC() function).  "
  },
  {
    "id": 16498,
    "package_name": "mstate",
    "title": "Data Preparation, Estimation and Prediction in Multi-State\nModels",
    "description": "Contains functions for data preparation, descriptives, hazard estimation and prediction with Aalen-Johansen or simulation in competing risks and multi-state models, see Putter, Fiocco, Geskus (2007) <doi:10.1002/sim.2712>.",
    "version": "0.3.3",
    "maintainer": "Hein Putter <H.Putter@lumc.nl>",
    "author": "Hein Putter [aut, cre],\n  Liesbeth C. de Wreede [aut],\n  Marta Fiocco [aut],\n  Ronald B. Geskus [ctb],\n  Edouard F. Bonneville [aut],\n  Damjan Manevski [ctb]",
    "url": "https://github.com/hputter/mstate",
    "bug_reports": "https://github.com/hputter/mstate/issues",
    "repository": "https://cran.r-project.org/package=mstate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mstate Data Preparation, Estimation and Prediction in Multi-State\nModels Contains functions for data preparation, descriptives, hazard estimation and prediction with Aalen-Johansen or simulation in competing risks and multi-state models, see Putter, Fiocco, Geskus (2007) <doi:10.1002/sim.2712>.  "
  },
  {
    "id": 16520,
    "package_name": "mueRelativeRisk",
    "title": "Relative Risk Based on the Ratio of Median Unbiased Estimates",
    "description": "Implements an estimator for relative risk based on the median unbiased estimator. The relative risk estimator is well defined and performs satisfactorily for a wide range of data configurations. The details of the method are available in Carter et al (2010) <doi:10.1111/j.1467-9876.2010.00711.x>.",
    "version": "0.1.1",
    "maintainer": "Rickey Carter <carter.rickey@mayo.edu>",
    "author": "Rickey Carter [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mueRelativeRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mueRelativeRisk Relative Risk Based on the Ratio of Median Unbiased Estimates Implements an estimator for relative risk based on the median unbiased estimator. The relative risk estimator is well defined and performs satisfactorily for a wide range of data configurations. The details of the method are available in Carter et al (2010) <doi:10.1111/j.1467-9876.2010.00711.x>.  "
  },
  {
    "id": 16538,
    "package_name": "multiAssetOptions",
    "title": "Finite Difference Method for Multi-Asset Option Valuation",
    "description": "Efficient finite difference method for valuing European and American multi-asset options.",
    "version": "0.1-2",
    "maintainer": "Michael Eichenberger <mike.eichenberger@gmail.com>",
    "author": "Michael Eichenberger and Carlo Rosa",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multiAssetOptions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multiAssetOptions Finite Difference Method for Multi-Asset Option Valuation Efficient finite difference method for valuing European and American multi-asset options.  "
  },
  {
    "id": 16554,
    "package_name": "multibreakeR",
    "title": "Tests for a Structural Change in Multivariate Time Series",
    "description": "Flexible implementation of a structural change point detection algorithm for multivariate time series.\n    It authorizes inclusion of trends, exogenous variables, and break test on the intercept or on the full vector autoregression system.\n    Bai, Lumsdaine, and Stock (1998) <doi:10.1111/1467-937X.00051>.",
    "version": "0.1.0",
    "maintainer": "Loic Marechal <loic.marechal@unil.ch>",
    "author": "Loic Marechal [cre, aut]",
    "url": "https://github.com/loicym/multibreakeR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multibreakeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multibreakeR Tests for a Structural Change in Multivariate Time Series Flexible implementation of a structural change point detection algorithm for multivariate time series.\n    It authorizes inclusion of trends, exogenous variables, and break test on the intercept or on the full vector autoregression system.\n    Bai, Lumsdaine, and Stock (1998) <doi:10.1111/1467-937X.00051>.  "
  },
  {
    "id": 16699,
    "package_name": "mycaas",
    "title": "My Computerized Adaptive Assessment",
    "description": "Implementation of adaptive assessment procedures based\n    on Knowledge Space Theory (KST, Doignon & Falmagne, 1999 <ISBN:9783540645016>) and Formal Psychological Assessment\n    (FPA, Spoto, Stefanutti & Vidotto, 2010 <doi:10.3758/BRM.42.1.342>) frameworks. An adaptive assessment is a type of evaluation that\n    adjusts the difficulty and nature of subsequent questions based on the\n    test taker's responses to previous ones. The package contains functions\n    to perform and simulate an adaptive assessment. Moreover, it is\n    integrated with two 'Shiny' interfaces, making it both accessible and\n    user-friendly.  The package has been partially funded by the European Union -\n    NextGenerationEU and by the Ministry of University and Research (MUR),\n    National Recovery and Resilience Plan (NRRP), Mission 4, Component 2,\n    Investment 1.5, project \u201cRAISE - Robotics and AI for Socio-economic\n    Empowerment\u201d (ECS00000035).",
    "version": "0.0.1",
    "maintainer": "Andrea Brancaccio <andreabrancaccio01@gmail.com>",
    "author": "Andrea Brancaccio [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-5919-6990>),\n  Umberto Granziol [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-6286-6569>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mycaas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mycaas My Computerized Adaptive Assessment Implementation of adaptive assessment procedures based\n    on Knowledge Space Theory (KST, Doignon & Falmagne, 1999 <ISBN:9783540645016>) and Formal Psychological Assessment\n    (FPA, Spoto, Stefanutti & Vidotto, 2010 <doi:10.3758/BRM.42.1.342>) frameworks. An adaptive assessment is a type of evaluation that\n    adjusts the difficulty and nature of subsequent questions based on the\n    test taker's responses to previous ones. The package contains functions\n    to perform and simulate an adaptive assessment. Moreover, it is\n    integrated with two 'Shiny' interfaces, making it both accessible and\n    user-friendly.  The package has been partially funded by the European Union -\n    NextGenerationEU and by the Ministry of University and Research (MUR),\n    National Recovery and Resilience Plan (NRRP), Mission 4, Component 2,\n    Investment 1.5, project \u201cRAISE - Robotics and AI for Socio-economic\n    Empowerment\u201d (ECS00000035).  "
  },
  {
    "id": 16735,
    "package_name": "nasadata",
    "title": "Interface to Various NASA API's",
    "description": "Provides functions to access NASA's Earth Imagery and Assets API\n    and the Earth Observatory Natural Event Tracker (EONET) webservice.",
    "version": "0.9.0",
    "maintainer": "Eduardo Flores <eduardo@enelmargen.org>",
    "author": "Eduardo Flores",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nasadata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nasadata Interface to Various NASA API's Provides functions to access NASA's Earth Imagery and Assets API\n    and the Earth Observatory Natural Event Tracker (EONET) webservice.  "
  },
  {
    "id": 16793,
    "package_name": "nemBM",
    "title": "Using Network Evolution Models to Generate Networks with\nSelected Blockmodel Type",
    "description": "To study network evolution models and different blockmodeling approaches. Various functions enable generating (temporal) networks with a selected blockmodel type, taking into account selected local network mechanisms. The development of this package is financially supported the Slovenian Research Agency (www.arrs.gov.si) within the research program P5<96>0168 and the research project J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).",
    "version": "1.00.01",
    "maintainer": "Marjan Cugmas <marjan.cugmas@fdv.uni-lj.si>",
    "author": "Marjan Cugmas [aut, cre],\n  Ale\u0161 \u017diberna [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nemBM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nemBM Using Network Evolution Models to Generate Networks with\nSelected Blockmodel Type To study network evolution models and different blockmodeling approaches. Various functions enable generating (temporal) networks with a selected blockmodel type, taking into account selected local network mechanisms. The development of this package is financially supported the Slovenian Research Agency (www.arrs.gov.si) within the research program P5<96>0168 and the research project J5-2557 (Comparison and evaluation of different approaches to blockmodeling dynamic networks by simulations with application to Slovenian co-authorship networks).  "
  },
  {
    "id": 16839,
    "package_name": "netstat",
    "title": "Retrieve Network Statistics Including Available TCP Ports",
    "description": "R interface for the 'netstat' command line utility used to retrieve and parse \n    commonly used network statistics, including available and in-use \n    transmission control protocol (TCP) ports. Primers offering technical background information \n    on the 'netstat' command line utility are available in the \"Linux System Administrator's Manual\" \n    by Michael Kerrisk (2014) \n    <https://man7.org/linux/man-pages/man8/netstat.8.html>, and on the Microsoft website (2017) \n    <https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/netstat>.",
    "version": "0.1.2",
    "maintainer": "Steve Condylios <steve.condylios@gmail.com>",
    "author": "Steve Condylios [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0599-844X>),\n  Bob Rudis [aut] (ORCID: <https://orcid.org/0000-0001-5670-2640>),\n  Patrice Kiener [aut] (ORCID: <https://orcid.org/0000-0002-0505-9920>)",
    "url": "https://github.com/stevecondylios/netstat",
    "bug_reports": "https://github.com/stevecondylios/netstat/issues",
    "repository": "https://cran.r-project.org/package=netstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "netstat Retrieve Network Statistics Including Available TCP Ports R interface for the 'netstat' command line utility used to retrieve and parse \n    commonly used network statistics, including available and in-use \n    transmission control protocol (TCP) ports. Primers offering technical background information \n    on the 'netstat' command line utility are available in the \"Linux System Administrator's Manual\" \n    by Michael Kerrisk (2014) \n    <https://man7.org/linux/man-pages/man8/netstat.8.html>, and on the Microsoft website (2017) \n    <https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/netstat>.  "
  },
  {
    "id": 16855,
    "package_name": "neuralGAM",
    "title": "Interpretable Neural Network Based on Generalized Additive\nModels",
    "description": "Neural Additive Model framework based on Generalized Additive Models from Hastie & Tibshirani (1990, ISBN:9780412343902), which trains a different neural network to estimate the contribution of each feature to the response variable. The networks are trained independently leveraging the local scoring and backfitting algorithms to ensure that the Generalized Additive Model converges and it is additive. The resultant Neural Network is a highly accurate and interpretable deep learning model, which can be used for high-risk AI practices where decision-making should be based on accountable and interpretable algorithms. ",
    "version": "2.0.1",
    "maintainer": "Ines Ortega-Fernandez <iortega@gradiant.org>",
    "author": "Ines Ortega-Fernandez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8041-6860>),\n  Marta Sestelo [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4284-6509>)",
    "url": "https://inesortega.github.io/neuralGAM/,\nhttps://github.com/inesortega/neuralGAM",
    "bug_reports": "https://github.com/inesortega/neuralGAM/issues",
    "repository": "https://cran.r-project.org/package=neuralGAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neuralGAM Interpretable Neural Network Based on Generalized Additive\nModels Neural Additive Model framework based on Generalized Additive Models from Hastie & Tibshirani (1990, ISBN:9780412343902), which trains a different neural network to estimate the contribution of each feature to the response variable. The networks are trained independently leveraging the local scoring and backfitting algorithms to ensure that the Generalized Additive Model converges and it is additive. The resultant Neural Network is a highly accurate and interpretable deep learning model, which can be used for high-risk AI practices where decision-making should be based on accountable and interpretable algorithms.   "
  },
  {
    "id": 16910,
    "package_name": "nichevol",
    "title": "Tools for Ecological Niche Evolution Assessment Considering\nUncertainty",
    "description": "A collection of tools that allow users to perform critical steps \n    in the process of assessing ecological niche evolution over phylogenies, with\n    uncertainty incorporated explicitly in reconstructions. The method proposed\n    here for ancestral reconstruction of ecological niches characterizes species'\n    niches using a bin-based approach that incorporates uncertainty in estimations.\n    Compared to other existing methods, the approaches presented here reduce risk\n    of overestimation of amounts and rates of ecological niche evolution. The\n    main analyses include: initial exploration of environmental data in occurrence\n    records and accessible areas, preparation of data for phylogenetic analyses,\n    executing comparative phylogenetic analyses of ecological niches, and plotting\n    for interpretations. Details on the theoretical background and methods used \n    can be found in: Owens et al. (2020) <doi:10.1002/ece3.6359>,\n    Peterson et al. (1999) <doi:10.1126/science.285.5431.1265>,\n    Sober\u00f3n and Peterson (2005) <doi:10.17161/bi.v2i0.4>,\n    Peterson (2011) <doi:10.1111/j.1365-2699.2010.02456.x>,\n    Barve et al. (2011) <doi:10.1111/ecog.02671>, \n    Machado-Stredel et al. (2021) <doi:10.21425/F5FBG48814>,\n    Owens et al. (2013) <doi:10.1016/j.ecolmodel.2013.04.011>, \n    Saupe et al. (2018) <doi:10.1093/sysbio/syx084>, and \n    Cobos et al. (2021) <doi:10.1111/jav.02868>.",
    "version": "0.1.20",
    "maintainer": "Marlon E. Cobos <manubio13@gmail.com>",
    "author": "Marlon E. Cobos [aut, cre],\n  Hannah L. Owens [aut],\n  A. Townsend Peterson [aut]",
    "url": "https://github.com/marlonecobos/nichevol",
    "bug_reports": "https://github.com/marlonecobos/nichevol/issues",
    "repository": "https://cran.r-project.org/package=nichevol",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nichevol Tools for Ecological Niche Evolution Assessment Considering\nUncertainty A collection of tools that allow users to perform critical steps \n    in the process of assessing ecological niche evolution over phylogenies, with\n    uncertainty incorporated explicitly in reconstructions. The method proposed\n    here for ancestral reconstruction of ecological niches characterizes species'\n    niches using a bin-based approach that incorporates uncertainty in estimations.\n    Compared to other existing methods, the approaches presented here reduce risk\n    of overestimation of amounts and rates of ecological niche evolution. The\n    main analyses include: initial exploration of environmental data in occurrence\n    records and accessible areas, preparation of data for phylogenetic analyses,\n    executing comparative phylogenetic analyses of ecological niches, and plotting\n    for interpretations. Details on the theoretical background and methods used \n    can be found in: Owens et al. (2020) <doi:10.1002/ece3.6359>,\n    Peterson et al. (1999) <doi:10.1126/science.285.5431.1265>,\n    Sober\u00f3n and Peterson (2005) <doi:10.17161/bi.v2i0.4>,\n    Peterson (2011) <doi:10.1111/j.1365-2699.2010.02456.x>,\n    Barve et al. (2011) <doi:10.1111/ecog.02671>, \n    Machado-Stredel et al. (2021) <doi:10.21425/F5FBG48814>,\n    Owens et al. (2013) <doi:10.1016/j.ecolmodel.2013.04.011>, \n    Saupe et al. (2018) <doi:10.1093/sysbio/syx084>, and \n    Cobos et al. (2021) <doi:10.1111/jav.02868>.  "
  },
  {
    "id": 16955,
    "package_name": "nlpred",
    "title": "Estimators of Non-Linear Cross-Validated Risks Optimized for\nSmall Samples",
    "description": "Methods for obtaining improved estimates of non-linear cross-validated risks are obtained using targeted minimum loss-based estimation, estimating equations, and one-step estimation (Benkeser, Petersen, van der Laan (2019), <doi:10.1080/01621459.2019.1668794>). Cross-validated area under the receiver operating characteristics curve (LeDell, Petersen, van der Laan (2015), <doi:10.1214/15-EJS1035>) and other metrics are included.",
    "version": "1.0.1",
    "maintainer": "David Benkeser <benkeser@emory.edu>",
    "author": "David Benkeser [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nlpred",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlpred Estimators of Non-Linear Cross-Validated Risks Optimized for\nSmall Samples Methods for obtaining improved estimates of non-linear cross-validated risks are obtained using targeted minimum loss-based estimation, estimating equations, and one-step estimation (Benkeser, Petersen, van der Laan (2019), <doi:10.1080/01621459.2019.1668794>). Cross-validated area under the receiver operating characteristics curve (LeDell, Petersen, van der Laan (2015), <doi:10.1214/15-EJS1035>) and other metrics are included.  "
  },
  {
    "id": 16959,
    "package_name": "nlrr",
    "title": "Non-Linear Relative Risk Estimation and Plotting",
    "description": "Estimate the non-linear odds ratio and plot it against a continuous exposure.",
    "version": "0.1",
    "maintainer": "Yiqiang Zhan <zhanyiqiang@gmail.com>",
    "author": "Yiqiang Zhan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nlrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlrr Non-Linear Relative Risk Estimation and Plotting Estimate the non-linear odds ratio and plot it against a continuous exposure.  "
  },
  {
    "id": 16974,
    "package_name": "nmaplateplot",
    "title": "The Plate Plot for Network Meta-Analysis Results",
    "description": "A graphical display of results from network meta-analysis (NMA). \n  It is suitable for outcomes like odds ratio (OR), risk ratio (RR), \n  risk difference (RD) and standardized mean difference (SMD). \n  It also has an option to visually display and compare \n  the surface under the cumulative ranking (SUCRA) of different treatments.",
    "version": "1.0.3",
    "maintainer": "Zhenxun Wang <wang6795@alumni.umn.edu>",
    "author": "Zhenxun Wang [aut, cre],\n  Lifeng Lin [ctb],\n  Shanshan Zhao [ctb],\n  Haitao Chu [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nmaplateplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nmaplateplot The Plate Plot for Network Meta-Analysis Results A graphical display of results from network meta-analysis (NMA). \n  It is suitable for outcomes like odds ratio (OR), risk ratio (RR), \n  risk difference (RD) and standardized mean difference (SMD). \n  It also has an option to visually display and compare \n  the surface under the cumulative ranking (SUCRA) of different treatments.  "
  },
  {
    "id": 17055,
    "package_name": "nose",
    "title": "Classification of Sparseness in 2-by-2 Categorical Data",
    "description": "Provides functions for classifying sparseness in 2 x 2 categorical data where one or more cells have zero counts. The classification uses three widely applied summary measures: Risk Difference (RD), Relative Risk (RR), and Odds Ratio (OR). Helps in selecting suitable continuity corrections for zero cells in multi-centre or meta-analysis studies. Also supports sensitivity analysis and can detect phenomena such as Simpson's paradox. The methodology is based on Subbiah and Srinivasan (2008) <doi:10.1016/j.spl.2008.06.023>.",
    "version": "1.0.5",
    "maintainer": "Subbiah M <sisufive@gmail.com>",
    "author": "Subbiah M [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nose",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nose Classification of Sparseness in 2-by-2 Categorical Data Provides functions for classifying sparseness in 2 x 2 categorical data where one or more cells have zero counts. The classification uses three widely applied summary measures: Risk Difference (RD), Relative Risk (RR), and Odds Ratio (OR). Helps in selecting suitable continuity corrections for zero cells in multi-centre or meta-analysis studies. Also supports sensitivity analysis and can detect phenomena such as Simpson's paradox. The methodology is based on Subbiah and Srinivasan (2008) <doi:10.1016/j.spl.2008.06.023>.  "
  },
  {
    "id": 17113,
    "package_name": "nricens",
    "title": "NRI for Risk Prediction Models with Time to Event and Binary\nResponse Data",
    "description": "Calculating the net reclassification improvement (NRI) for risk prediction models with time to event and binary data.",
    "version": "1.6",
    "maintainer": "Eisuke Inoue <eisuke.inoue@marianna-u.ac.jp>",
    "author": "Eisuke Inoue",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nricens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nricens NRI for Risk Prediction Models with Time to Event and Binary\nResponse Data Calculating the net reclassification improvement (NRI) for risk prediction models with time to event and binary data.  "
  },
  {
    "id": 17136,
    "package_name": "numKM",
    "title": "Create a Kaplan-Meier Plot with Numbers at Risk",
    "description": "To add the table of numbers at risk below the Kaplan-Meier plot.",
    "version": "0.2.0",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=numKM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "numKM Create a Kaplan-Meier Plot with Numbers at Risk To add the table of numbers at risk below the Kaplan-Meier plot.  "
  },
  {
    "id": 17163,
    "package_name": "obcost",
    "title": "Obesity Cost Database",
    "description": "This database contains necessary data relevant to medical costs on obesity throughout the United States. This database, in form of an R package, could output necessary data frames relevant to obesity costs, where the clients could easily manipulate the output using difference parameters, e.g. relative risks for each illnesses. This package contributes to parts of our published journal named \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\". Please use the following citation for the journal: Woods Thomas, Tatjana Miljkovic (2022) \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\" <doi:10.3390/risks10100197>. The database is composed of the following main tables: 1. Relative_Risks: (constant) Relative risks for a given disease group with a risk factor of obesity; 2. Disease_Cost: (obesity_cost_disease) Supplementary output with all variables related to individual disease groups in a given state and year; 3. Full_Cost: (obesity_cost_full) Complete output with all variables used to make cost calculations, as well as cost calculations in a given state and year; 4. National_Summary: (obesity_cost_national_summary) National summary cost calculations in a given year. Three functions are included to assist users in calling and adjusting the mentioned tables and they are data_load(), data_produce(), and rel_risk_fun(). ",
    "version": "0.1.0",
    "maintainer": "Tianyue Zang <zangt2@miamioh.edu>",
    "author": "Tianyue Zang [aut, cre, cph],\n  Thomas Woods [aut],\n  Tatjana Miljkovic [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=obcost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "obcost Obesity Cost Database This database contains necessary data relevant to medical costs on obesity throughout the United States. This database, in form of an R package, could output necessary data frames relevant to obesity costs, where the clients could easily manipulate the output using difference parameters, e.g. relative risks for each illnesses. This package contributes to parts of our published journal named \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\". Please use the following citation for the journal: Woods Thomas, Tatjana Miljkovic (2022) \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\" <doi:10.3390/risks10100197>. The database is composed of the following main tables: 1. Relative_Risks: (constant) Relative risks for a given disease group with a risk factor of obesity; 2. Disease_Cost: (obesity_cost_disease) Supplementary output with all variables related to individual disease groups in a given state and year; 3. Full_Cost: (obesity_cost_full) Complete output with all variables used to make cost calculations, as well as cost calculations in a given state and year; 4. National_Summary: (obesity_cost_national_summary) National summary cost calculations in a given year. Three functions are included to assist users in calling and adjusting the mentioned tables and they are data_load(), data_produce(), and rel_risk_fun().   "
  },
  {
    "id": 17291,
    "package_name": "openMSE",
    "title": "Easily Install and Load the 'openMSE' Packages",
    "description": "The 'openMSE' package is designed for building operating models, \n    doing simulation modelling and management strategy evaluation for fisheries.\n    'openMSE' is an umbrella package for the 'MSEtool' (Management Strategy Evaluation\n    toolkit), 'DLMtool' (Data-Limited Methods toolkit), and \n    SAMtool (Stock Assessment Methods toolkit) packages. By loading and installing\n    'openMSE', users have access to the full functionality contained within\n    these packages. Learn more about 'openMSE' at <https://openmse.com/>.",
    "version": "1.0.1",
    "maintainer": "Adrian Hordyk <adrian@bluematterscience.com>",
    "author": "Adrian Hordyk [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5620-3446>),\n  Quang Huynh [aut],\n  Tom Carruthers [aut]",
    "url": "https://openmse.com/, https://github.com/Blue-Matter/openMSE,\nhttps://openMSE.openMSE.com",
    "bug_reports": "https://github.com/Blue-Matter/openMSE/issues",
    "repository": "https://cran.r-project.org/package=openMSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openMSE Easily Install and Load the 'openMSE' Packages The 'openMSE' package is designed for building operating models, \n    doing simulation modelling and management strategy evaluation for fisheries.\n    'openMSE' is an umbrella package for the 'MSEtool' (Management Strategy Evaluation\n    toolkit), 'DLMtool' (Data-Limited Methods toolkit), and \n    SAMtool (Stock Assessment Methods toolkit) packages. By loading and installing\n    'openMSE', users have access to the full functionality contained within\n    these packages. Learn more about 'openMSE' at <https://openmse.com/>.  "
  },
  {
    "id": 17344,
    "package_name": "optimLanduse",
    "title": "Robust Land-Use Optimization",
    "description": "Robust multi-criteria land-allocation optimization that explicitly accounts for the uncertainty of the indicators in the objective function. Solves the problem of allocating scarce land to various land-use options with regard to multiple, coequal indicators. The method aims to find the land allocation that represents the indicator composition with the best possible trade-off under uncertainty. optimLanduse includes the actual optimization procedure as described by Knoke et al. (2016) <doi:10.1038/ncomms11877> and the post-hoc calculation of the portfolio performance as presented by Gosling et al. (2020) <doi:10.1016/j.jenvman.2020.110248>.",
    "version": "1.2.1",
    "maintainer": "Kai Husmann <kai.husmann@uni-goettingen.de>",
    "author": "Kai Husmann [aut, cre] (ORCID: <https://orcid.org/0000-0003-2970-4709>),\n  Volker von Gro\u00df [aut] (ORCID: <https://orcid.org/0000-0001-7372-0066>),\n  Jasper Fuchs [aut] (ORCID: <https://orcid.org/0000-0001-5951-7897>),\n  Kai B\u00f6deker [aut] (ORCID: <https://orcid.org/0000-0002-5307-5108>),\n  Carola Paul [aut] (ORCID: <https://orcid.org/0000-0002-6257-2026>),\n  Thomas Knoke [aut] (ORCID: <https://orcid.org/0000-0003-0535-5946>),\n  Goettingen University - Forest Economics and Sustainable Land-use\n    Planning [cph, fnd],\n  TUM School of Life Sciences - Forest Management [cph, fnd]",
    "url": "https://github.com/Forest-Economics-Goettingen/optimLanduse/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=optimLanduse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "optimLanduse Robust Land-Use Optimization Robust multi-criteria land-allocation optimization that explicitly accounts for the uncertainty of the indicators in the objective function. Solves the problem of allocating scarce land to various land-use options with regard to multiple, coequal indicators. The method aims to find the land allocation that represents the indicator composition with the best possible trade-off under uncertainty. optimLanduse includes the actual optimization procedure as described by Knoke et al. (2016) <doi:10.1038/ncomms11877> and the post-hoc calculation of the portfolio performance as presented by Gosling et al. (2020) <doi:10.1016/j.jenvman.2020.110248>.  "
  },
  {
    "id": 17361,
    "package_name": "optistock",
    "title": "Determine Optimum Stocking Times Used in Fishery Enhancements",
    "description": "A collection of functions that aid in calculating the optimum\n  time to stock hatchery reared fish into a body of water given the growth, \n  mortality and cost of raising a particular number of individuals to a certain \n  length.",
    "version": "0.0.2",
    "maintainer": "Paul Frater <paul.frater@wisconsin.gov>",
    "author": "Paul Frater [aut, cre] (ORCID: <https://orcid.org/0000-0002-7237-6563>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=optistock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "optistock Determine Optimum Stocking Times Used in Fishery Enhancements A collection of functions that aid in calculating the optimum\n  time to stock hatchery reared fish into a body of water given the growth, \n  mortality and cost of raising a particular number of individuals to a certain \n  length.  "
  },
  {
    "id": 17411,
    "package_name": "orsk",
    "title": "Converting Odds Ratio to Relative Risk in Cohort Studies with\nPartial Data Information",
    "description": "Convert odds ratio to relative risk in cohort studies with partial data information (Wang (2013) <doi:10.18637/jss.v055.i05>).",
    "version": "1.0-8",
    "maintainer": "Zhu Wang <zwang145@uthsc.edu>",
    "author": "Zhu Wang <https://orcid.org/0000-0002-0773-0052>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=orsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "orsk Converting Odds Ratio to Relative Risk in Cohort Studies with\nPartial Data Information Convert odds ratio to relative risk in cohort studies with partial data information (Wang (2013) <doi:10.18637/jss.v055.i05>).  "
  },
  {
    "id": 17435,
    "package_name": "otsfeatures",
    "title": "Ordinal Time Series Analysis",
    "description": "An implementation of several functions for feature extraction in \n    ordinal time series datasets. Specifically, some of the features proposed by\n    Weiss (2019) <doi:10.1080/01621459.2019.1604370> can be computed.  \n    These features can be used to perform inferential tasks or to feed machine\n    learning algorithms for ordinal time series, among others. The package also includes some\n    interesting datasets containing financial time series. Practitioners from a \n    broad variety of fields could benefit from the general framework provided \n    by 'otsfeatures'.",
    "version": "1.0.0",
    "maintainer": "Angel Lopez-Oriona <oriona38@hotmail.com>",
    "author": "Angel Lopez-Oriona [aut, cre],\n  Jose A. Vilar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=otsfeatures",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "otsfeatures Ordinal Time Series Analysis An implementation of several functions for feature extraction in \n    ordinal time series datasets. Specifically, some of the features proposed by\n    Weiss (2019) <doi:10.1080/01621459.2019.1604370> can be computed.  \n    These features can be used to perform inferential tasks or to feed machine\n    learning algorithms for ordinal time series, among others. The package also includes some\n    interesting datasets containing financial time series. Practitioners from a \n    broad variety of fields could benefit from the general framework provided \n    by 'otsfeatures'.  "
  },
  {
    "id": 17460,
    "package_name": "owidapi",
    "title": "Access the Our World in Data Chart API",
    "description": "Retrieve data from the Our World in Data (OWID) Chart API\n    <https://docs.owid.io/projects/etl/api/>. OWID provides public access to \n    more than 5,000 charts focusing on global problems such as poverty, \n    disease, hunger, climate change, war, existential risks, and inequality.",
    "version": "0.1.1",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://github.com/tidy-intelligence/r-owidapi,\nhttps://tidy-intelligence.github.io/r-owidapi/",
    "bug_reports": "https://github.com/tidy-intelligence/r-owidapi/issues",
    "repository": "https://cran.r-project.org/package=owidapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "owidapi Access the Our World in Data Chart API Retrieve data from the Our World in Data (OWID) Chart API\n    <https://docs.owid.io/projects/etl/api/>. OWID provides public access to \n    more than 5,000 charts focusing on global problems such as poverty, \n    disease, hunger, climate change, war, existential risks, and inequality.  "
  },
  {
    "id": 17487,
    "package_name": "pa",
    "title": "Performance Attribution for Equity Portfolios",
    "description": "It provides tools for conducting performance attribution for equity portfolios. The package uses two methods: the Brinson method and a regression-based analysis.",
    "version": "1.2-4",
    "maintainer": "Yang Lu <yang.lu2014@gmail.com>",
    "author": "Yang Lu [aut, cre],\n  David Kane [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pa Performance Attribution for Equity Portfolios It provides tools for conducting performance attribution for equity portfolios. The package uses two methods: the Brinson method and a regression-based analysis.  "
  },
  {
    "id": 17490,
    "package_name": "packHV",
    "title": "A few Useful Functions for Statisticians",
    "description": "Various useful functions for statisticians: describe data, plot Kaplan-Meier curves with numbers of subjects at risk, compare data sets, display spaghetti-plot, build multi-contingency tables...",
    "version": "2.4",
    "maintainer": "Hugo Varet <varethugo@gmail.com>",
    "author": "Hugo Varet [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=packHV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "packHV A few Useful Functions for Statisticians Various useful functions for statisticians: describe data, plot Kaplan-Meier curves with numbers of subjects at risk, compare data sets, display spaghetti-plot, build multi-contingency tables...  "
  },
  {
    "id": 17503,
    "package_name": "pacta.loanbook",
    "title": "Easily Install and Load PACTA for Banks Packages",
    "description": "PACTA (Paris Agreement Capital Transition Assessment) for Banks is\n  a tool that allows banks to calculate the climate alignment of their corporate\n  lending portfolios. This package is designed to make it easy to install and\n  load multiple PACTA for Banks packages in a single step. It also provides\n  thorough documentation - the PACTA for Banks cookbook at\n  <https://rmi-pacta.github.io/pacta.loanbook/articles/cookbook_overview.html> -\n  on how to run a PACTA for Banks analysis. This covers prerequisites for the\n  analysis, the separate steps of running the analysis, the interpretation of\n  PACTA for Banks results, and advanced use cases.",
    "version": "0.1.1",
    "maintainer": "Jacob Kastl <jacob.kastl@gmail.com>",
    "author": "Jacob Kastl [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0000-8281-8129>),\n  Jackson Hoffart [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-8600-5042>),\n  CJ Yetman [aut, ctr] (ORCID: <https://orcid.org/0000-0001-5099-9500>),\n  RMI [cph, fnd] (ROR: <https://ror.org/03anfar33>)",
    "url": "https://rmi-pacta.github.io/pacta.loanbook/,\nhttps://github.com/RMI-PACTA/pacta.loanbook",
    "bug_reports": "https://github.com/RMI-PACTA/pacta.loanbook/issues",
    "repository": "https://cran.r-project.org/package=pacta.loanbook",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pacta.loanbook Easily Install and Load PACTA for Banks Packages PACTA (Paris Agreement Capital Transition Assessment) for Banks is\n  a tool that allows banks to calculate the climate alignment of their corporate\n  lending portfolios. This package is designed to make it easy to install and\n  load multiple PACTA for Banks packages in a single step. It also provides\n  thorough documentation - the PACTA for Banks cookbook at\n  <https://rmi-pacta.github.io/pacta.loanbook/articles/cookbook_overview.html> -\n  on how to run a PACTA for Banks analysis. This covers prerequisites for the\n  analysis, the separate steps of running the analysis, the interpretation of\n  PACTA for Banks results, and advanced use cases.  "
  },
  {
    "id": 17509,
    "package_name": "pafdR",
    "title": "Book Companion for Processing and Analyzing Financial Data with\nR",
    "description": "Provides access to material from the book \"Processing and Analyzing Financial Data with R\" by Marcelo Perlin (2017) available at <https://sites.google.com/view/pafdr/home>.",
    "version": "1.0",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pafdR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pafdR Book Companion for Processing and Analyzing Financial Data with\nR Provides access to material from the book \"Processing and Analyzing Financial Data with R\" by Marcelo Perlin (2017) available at <https://sites.google.com/view/pafdr/home>.  "
  },
  {
    "id": 17546,
    "package_name": "pammtools",
    "title": "Piece-Wise Exponential Additive Mixed Modeling Tools for\nSurvival Analysis",
    "description": "The Piece-wise exponential (Additive Mixed) Model\n    (PAMM; Bender and others (2018) <doi: 10.1177/1471082X17748083>) is a\n    powerful model class for the analysis of survival (or time-to-event) data,\n    based on Generalized Additive (Mixed) Models (GA(M)Ms).\n    It offers intuitive specification and robust estimation of complex survival\n    models with stratified baseline hazards, random effects, time-varying effects,\n    time-dependent covariates and cumulative effects (Bender and others (2019)),\n    as well as support for left-truncated data as well as competing risks,\n    recurrent events and multi-state settings.\n    pammtools provides tidy workflow for survival analysis with PAMMs,\n    including data simulation, transformation and other functions for data\n    preprocessing and model post-processing as well as visualization.",
    "version": "0.7.3",
    "maintainer": "Andreas Bender <andreas.bender@stat.uni-muenchen.de>",
    "author": "Andreas Bender [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5628-8611>),\n  Fabian Scheipl [aut] (ORCID: <https://orcid.org/0000-0001-8172-3603>),\n  Johannes Piller [aut] (ORCID: <https://orcid.org/0009-0008-3010-9556>),\n  Philipp Kopper [aut] (ORCID: <https://orcid.org/0000-0002-5037-7135>),\n  Lukas Burk [ctb] (ORCID: <https://orcid.org/0000-0001-7528-3795>)",
    "url": "https://adibender.github.io/pammtools/",
    "bug_reports": "https://github.com/adibender/pammtools/issues",
    "repository": "https://cran.r-project.org/package=pammtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pammtools Piece-Wise Exponential Additive Mixed Modeling Tools for\nSurvival Analysis The Piece-wise exponential (Additive Mixed) Model\n    (PAMM; Bender and others (2018) <doi: 10.1177/1471082X17748083>) is a\n    powerful model class for the analysis of survival (or time-to-event) data,\n    based on Generalized Additive (Mixed) Models (GA(M)Ms).\n    It offers intuitive specification and robust estimation of complex survival\n    models with stratified baseline hazards, random effects, time-varying effects,\n    time-dependent covariates and cumulative effects (Bender and others (2019)),\n    as well as support for left-truncated data as well as competing risks,\n    recurrent events and multi-state settings.\n    pammtools provides tidy workflow for survival analysis with PAMMs,\n    including data simulation, transformation and other functions for data\n    preprocessing and model post-processing as well as visualization.  "
  },
  {
    "id": 17591,
    "package_name": "pargasite",
    "title": "Pollution-Associated Risk Geospatial Analysis Site",
    "description": "Offers tools to estimate and visualize levels of major pollutants\n             (CO, NO2, SO2, Ozone, PM2.5 and PM10) across the conterminous\n             United States for user-defined time ranges. Provides functions to\n             retrieve pollutant data from the U.S. Environmental Protection\n             Agency\u2019s 'Air Quality System' (AQS) API service\n             <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             interactive visualization through a 'shiny' application, allowing\n             users to explore pollutant levels for a given location over time\n             relative to the National Ambient Air Quality Standards (NAAQS).",
    "version": "2.1.1",
    "maintainer": "Jaehyun Joo <jaehyunjoo@outlook.com>",
    "author": "Jaehyun Joo [aut, cre],\n  Rebecca Greenblatt [aut],\n  Avantika Diwadkar [aut],\n  Nisha Narayanan [aut],\n  Blanca Himes [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pargasite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pargasite Pollution-Associated Risk Geospatial Analysis Site Offers tools to estimate and visualize levels of major pollutants\n             (CO, NO2, SO2, Ozone, PM2.5 and PM10) across the conterminous\n             United States for user-defined time ranges. Provides functions to\n             retrieve pollutant data from the U.S. Environmental Protection\n             Agency\u2019s 'Air Quality System' (AQS) API service\n             <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             interactive visualization through a 'shiny' application, allowing\n             users to explore pollutant levels for a given location over time\n             relative to the National Ambient Air Quality Standards (NAAQS).  "
  },
  {
    "id": 17593,
    "package_name": "parma",
    "title": "Portfolio Allocation and Risk Management Applications",
    "description": "Provision of a set of models and methods for use in the allocation and management of capital in financial portfolios.",
    "version": "1.7",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "author": "Alexios Galanos [aut, cre],\n  Bernhard Pfaff [ctb],\n  Miguel Sousa Lobo [ctb] (SOCP),\n  Lieven Vandenberghe [ctb] (SOCP),\n  Stephen Boyd [ctb] (SOCP),\n  Herve Lebret [ctb] (SOCP)",
    "url": "https://github.com/alexiosg/parma",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=parma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parma Portfolio Allocation and Risk Management Applications Provision of a set of models and methods for use in the allocation and management of capital in financial portfolios.  "
  },
  {
    "id": 17721,
    "package_name": "pdfcombiner",
    "title": "Graphical User Interface for Manipulating PDF and Image Files",
    "description": "A 'shiny' app that supports merging of PDF and/or image files with page selection, removal, or rotation options.\n    It is a fast, free, and secure alternative to commercial software or various online websites which require users to sign-up, and it avoids any potential risks associated with uploading files elsewhere.",
    "version": "1.9.8",
    "maintainer": "Steve Choy <steve.choy@outlook.com>",
    "author": "Steve Choy [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0002-5385-4096>)",
    "url": "https://github.com/stevechoy/pdfcombiner,\nhttps://lagom.shinyapps.io/pdfcombiner/",
    "bug_reports": "https://github.com/stevechoy/pdfcombiner/issues",
    "repository": "https://cran.r-project.org/package=pdfcombiner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdfcombiner Graphical User Interface for Manipulating PDF and Image Files A 'shiny' app that supports merging of PDF and/or image files with page selection, removal, or rotation options.\n    It is a fast, free, and secure alternative to commercial software or various online websites which require users to sign-up, and it avoids any potential risks associated with uploading files elsewhere.  "
  },
  {
    "id": 17722,
    "package_name": "pdfetch",
    "title": "Fetch Economic and Financial Time Series Data from Public\nSources",
    "description": "Download economic and financial time series from public sources, \n  including the St Louis Fed's FRED system, Yahoo Finance, the US Bureau of Labor Statistics, \n  the US Energy Information Administration, the World Bank, Eurostat, the European Central Bank,\n  the Bank of England, the UK's Office of National Statistics, Deutsche Bundesbank, and INSEE.",
    "version": "0.3.3",
    "maintainer": "Abiel Reinhart <abielr@gmail.com>",
    "author": "Abiel Reinhart [aut, cre]",
    "url": "https://github.com/abielr/pdfetch",
    "bug_reports": "https://github.com/abielr/pdfetch/issues",
    "repository": "https://cran.r-project.org/package=pdfetch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdfetch Fetch Economic and Financial Time Series Data from Public\nSources Download economic and financial time series from public sources, \n  including the St Louis Fed's FRED system, Yahoo Finance, the US Bureau of Labor Statistics, \n  the US Energy Information Administration, the World Bank, Eurostat, the European Central Bank,\n  the Bank of England, the UK's Office of National Statistics, Deutsche Bundesbank, and INSEE.  "
  },
  {
    "id": 17737,
    "package_name": "pec",
    "title": "Prediction Error Curves for Risk Prediction Models in Survival\nAnalysis",
    "description": "Validation of risk predictions obtained from survival models and\n    competing risk models based on censored data using inverse weighting and\n    cross-validation. Most of the 'pec' functionality has been moved to 'riskRegression'.",
    "version": "2025.06.24",
    "maintainer": "Thomas A. Gerds <tag@biostat.ku.dk>",
    "author": "Thomas A. Gerds [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pec Prediction Error Curves for Risk Prediction Models in Survival\nAnalysis Validation of risk predictions obtained from survival models and\n    competing risk models based on censored data using inverse weighting and\n    cross-validation. Most of the 'pec' functionality has been moved to 'riskRegression'.  "
  },
  {
    "id": 17738,
    "package_name": "pecan",
    "title": "Portfolio for Economic Complexity Analysis and Navigation",
    "description": "A portfolio of tools for economic complexity analysis and\n    industrial upgrading navigation. The package implements essential measures\n    in international trade and development economics, including the relative\n    comparative advantage (RCA), economic complexity index (ECI) and product\n    complexity index (PCI). It enables users to analyze export structures,\n    explore product relatedness, and identify potential upgrading paths grounded\n    in economic theory, following the framework in Hausmann et al. (2014)\n    <doi:10.7551/mitpress/9647.001.0001>.",
    "version": "0.1.0",
    "maintainer": "Shiying Xiao <shiying.xiao@outlook.com>",
    "author": "Shiying Xiao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8846-3258>)",
    "url": "https://github.com/Carol-seven/pecan",
    "bug_reports": "https://github.com/Carol-seven/pecan/issues",
    "repository": "https://cran.r-project.org/package=pecan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pecan Portfolio for Economic Complexity Analysis and Navigation A portfolio of tools for economic complexity analysis and\n    industrial upgrading navigation. The package implements essential measures\n    in international trade and development economics, including the relative\n    comparative advantage (RCA), economic complexity index (ECI) and product\n    complexity index (PCI). It enables users to analyze export structures,\n    explore product relatedness, and identify potential upgrading paths grounded\n    in economic theory, following the framework in Hausmann et al. (2014)\n    <doi:10.7551/mitpress/9647.001.0001>.  "
  },
  {
    "id": 17753,
    "package_name": "pedquant",
    "title": "Public Economic Data and Quantitative Analysis",
    "description": "\n    Provides an interface to access public economic and financial data for \n    economic research and quantitative analysis. The data sources including \n    NBS, FRED, Sina, Eastmoney and etc. It also provides quantitative \n    functions for trading strategies based on the 'data.table', 'TTR', \n    'PerformanceAnalytics' and etc packages.",
    "version": "0.2.6",
    "maintainer": "Shichen Xie <xie@shichen.name>",
    "author": "Shichen Xie [aut, cre]",
    "url": "https://github.com/ShichenXie/pedquant, https://pedquant.com/",
    "bug_reports": "https://github.com/ShichenXie/pedquant/issues",
    "repository": "https://cran.r-project.org/package=pedquant",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pedquant Public Economic Data and Quantitative Analysis \n    Provides an interface to access public economic and financial data for \n    economic research and quantitative analysis. The data sources including \n    NBS, FRED, Sina, Eastmoney and etc. It also provides quantitative \n    functions for trading strategies based on the 'data.table', 'TTR', \n    'PerformanceAnalytics' and etc packages.  "
  },
  {
    "id": 17762,
    "package_name": "pemultinom",
    "title": "L1-Penalized Multinomial Regression with Statistical Inference",
    "description": "We aim for fitting a multinomial regression model with Lasso penalty and doing statistical inference (calculating confidence intervals of coefficients and p-values for individual variables). It implements 1) the coordinate descent algorithm to fit an l1-penalized multinomial regression model (parameterized with a reference level); 2) the debiasing approach to obtain the inference results, which is described in \"Tian, Y., Rusinek, H., Masurkar, A. V., & Feng, Y. (2024). L1\u2010Penalized Multinomial Regression: Estimation, Inference, and Prediction, With an Application to Risk Factor Identification for Different Dementia Subtypes. Statistics in Medicine, 43(30), 5711-5747.\"",
    "version": "0.1.1",
    "maintainer": "Ye Tian <ye.t@columbia.edu>",
    "author": "Ye Tian [aut, cre],\n  Henry Rusinek [aut],\n  Arjun V. Masurkar [aut],\n  Yang Feng [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pemultinom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pemultinom L1-Penalized Multinomial Regression with Statistical Inference We aim for fitting a multinomial regression model with Lasso penalty and doing statistical inference (calculating confidence intervals of coefficients and p-values for individual variables). It implements 1) the coordinate descent algorithm to fit an l1-penalized multinomial regression model (parameterized with a reference level); 2) the debiasing approach to obtain the inference results, which is described in \"Tian, Y., Rusinek, H., Masurkar, A. V., & Feng, Y. (2024). L1\u2010Penalized Multinomial Regression: Estimation, Inference, and Prediction, With an Application to Risk Factor Identification for Different Dementia Subtypes. Statistics in Medicine, 43(30), 5711-5747.\"  "
  },
  {
    "id": 17844,
    "package_name": "ph2mult",
    "title": "Phase II Clinical Trial Design for Multinomial Endpoints",
    "description": "Provide multinomial design methods under intersection-union test (IUT) and union-intersection test (UIT) scheme for Phase II trial. The design types include : Minimax (minimize the maximum sample size), Optimal (minimize the expected sample size), Admissible (minimize the Bayesian risk) and Maxpower (maximize the exact power level).",
    "version": "0.1.1",
    "maintainer": "Yalin Zhu <yalin.zhu@outlook.com>",
    "author": "Yalin Zhu, Rui Qin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ph2mult",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ph2mult Phase II Clinical Trial Design for Multinomial Endpoints Provide multinomial design methods under intersection-union test (IUT) and union-intersection test (UIT) scheme for Phase II trial. The design types include : Minimax (minimize the maximum sample size), Optimal (minimize the expected sample size), Admissible (minimize the Bayesian risk) and Maxpower (maximize the exact power level).  "
  },
  {
    "id": 17876,
    "package_name": "phers",
    "title": "Calculate Phenotype Risk Scores",
    "description": "Use phenotype risk scores based on linked clinical and genetic data\n  to study Mendelian disease and rare genetic variants. See Bastarache et al.\n  2018 <doi:10.1126/science.aal4043>.",
    "version": "1.0.2",
    "maintainer": "Jake Hughey <jakejhughey@gmail.com>",
    "author": "Jake Hughey [aut, cre],\n  Layla Aref [aut]",
    "url": "https://phers.hugheylab.org, https://github.com/hugheylab/phers",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phers Calculate Phenotype Risk Scores Use phenotype risk scores based on linked clinical and genetic data\n  to study Mendelian disease and rare genetic variants. See Bastarache et al.\n  2018 <doi:10.1126/science.aal4043>.  "
  },
  {
    "id": 18023,
    "package_name": "plink",
    "title": "IRT Separate Calibration Linking Methods",
    "description": "Item response theory based methods are used to compute\n        linking constants and conduct chain linking of unidimensional\n        or multidimensional tests for multiple groups under a common\n        item design.  The unidimensional methods include the Mean/Mean,\n        Mean/Sigma, Haebara, and Stocking-Lord methods for dichotomous\n        (1PL, 2PL and 3PL) and/or polytomous (graded response, partial\n        credit/generalized partial credit, nominal, and multiple-choice\n        model) items.  The multidimensional methods include the least\n        squares method and extensions of the Haebara and Stocking-Lord\n        method using single or multiple dilation parameters for\n        multidimensional extensions of all the unidimensional\n        dichotomous and polytomous item response models.  The package\n        also includes functions for importing item and/or ability\n        parameters from common IRT software, conducting IRT true score\n        and observed score equating, and plotting item response\n        curves/surfaces, vector plots, information plots, and comparison \n        plots for examining parameter drift.",
    "version": "1.5-1",
    "maintainer": "Jonathan P. Weeks <weeksjp@gmail.com>",
    "author": "Jonathan P. Weeks <weeksjp@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=plink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plink IRT Separate Calibration Linking Methods Item response theory based methods are used to compute\n        linking constants and conduct chain linking of unidimensional\n        or multidimensional tests for multiple groups under a common\n        item design.  The unidimensional methods include the Mean/Mean,\n        Mean/Sigma, Haebara, and Stocking-Lord methods for dichotomous\n        (1PL, 2PL and 3PL) and/or polytomous (graded response, partial\n        credit/generalized partial credit, nominal, and multiple-choice\n        model) items.  The multidimensional methods include the least\n        squares method and extensions of the Haebara and Stocking-Lord\n        method using single or multiple dilation parameters for\n        multidimensional extensions of all the unidimensional\n        dichotomous and polytomous item response models.  The package\n        also includes functions for importing item and/or ability\n        parameters from common IRT software, conducting IRT true score\n        and observed score equating, and plotting item response\n        curves/surfaces, vector plots, information plots, and comparison \n        plots for examining parameter drift.  "
  },
  {
    "id": 18203,
    "package_name": "portfolio",
    "title": "Analysing Equity Portfolios",
    "description": "Classes for analysing and implementing equity portfolios,\n    including routines for generating tradelists and calculating\n    exposures to user-specified risk factors.",
    "version": "0.5-3",
    "maintainer": "Daniel Gerlanc <dan@gerlanc.com>",
    "author": "Jeff Enos [aut],\n  David Kane [aut],\n  Daniel Gerlanc [aut, cre],\n  Kyle Campbell [ctb]",
    "url": "https://github.com/dgerlanc/portfolio",
    "bug_reports": "https://github.com/dgerlanc/portfolio/issues",
    "repository": "https://cran.r-project.org/package=portfolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portfolio Analysing Equity Portfolios Classes for analysing and implementing equity portfolios,\n    including routines for generating tradelists and calculating\n    exposures to user-specified risk factors.  "
  },
  {
    "id": 18204,
    "package_name": "portfolio.optimization",
    "title": "Contemporary Portfolio Optimization",
    "description": "Simplify your portfolio optimization process by applying a contemporary modeling way to model and solve your portfolio problems. While most approaches and packages are rather complicated this one tries to simplify things and is agnostic regarding risk measures as well as optimization solvers. Some of the methods implemented are described by Konno and Yamazaki (1991) <doi:10.1287/mnsc.37.5.519>, Rockafellar and Uryasev (2001) <doi:10.21314/JOR.2000.038> and Markowitz (1952) <doi:10.1111/j.1540-6261.1952.tb01525.x>.",
    "version": "1.0-0",
    "maintainer": "Ronald Hochreiter <ron@hochreiter.net>",
    "author": "Ronald Hochreiter [aut, cre]",
    "url": "http://www.finance-r.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=portfolio.optimization",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portfolio.optimization Contemporary Portfolio Optimization Simplify your portfolio optimization process by applying a contemporary modeling way to model and solve your portfolio problems. While most approaches and packages are rather complicated this one tries to simplify things and is agnostic regarding risk measures as well as optimization solvers. Some of the methods implemented are described by Konno and Yamazaki (1991) <doi:10.1287/mnsc.37.5.519>, Rockafellar and Uryasev (2001) <doi:10.21314/JOR.2000.038> and Markowitz (1952) <doi:10.1111/j.1540-6261.1952.tb01525.x>.  "
  },
  {
    "id": 18205,
    "package_name": "portfolioBacktest",
    "title": "Automated Backtesting of Portfolios over Multiple Datasets",
    "description": "Automated backtesting of multiple portfolios over multiple \n    datasets of stock prices in a rolling-window fashion. Intended for \n    researchers and practitioners to backtest a set of different portfolios, \n    as well as by a course instructor to assess the students in their portfolio \n    design in a fully automated and convenient manner, with results conveniently \n    formatted in tables and plots. Each portfolio design is easily defined as a\n    function that takes as input a window of the stock prices and outputs the \n    portfolio weights. Multiple portfolios can be easily specified as a list \n    of functions or as files in a folder. Multiple datasets can be conveniently \n    extracted randomly from different markets, different time periods, and \n    different subsets of the stock universe. The results can be later assessed \n    and ranked with tables based on a number of performance criteria (e.g., \n    expected return, volatility, Sharpe ratio, drawdown, turnover rate, return \n    on investment, computational time, etc.), as well as plotted in a number of \n    ways with nice barplots and boxplots.",
    "version": "0.4.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Daniel P. Palomar [cre, aut],\n  Rui Zhou [aut]",
    "url": "https://CRAN.R-project.org/package=portfolioBacktest,\nhttps://github.com/dppalomar/portfolioBacktest",
    "bug_reports": "https://github.com/dppalomar/portfolioBacktest/issues",
    "repository": "https://cran.r-project.org/package=portfolioBacktest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portfolioBacktest Automated Backtesting of Portfolios over Multiple Datasets Automated backtesting of multiple portfolios over multiple \n    datasets of stock prices in a rolling-window fashion. Intended for \n    researchers and practitioners to backtest a set of different portfolios, \n    as well as by a course instructor to assess the students in their portfolio \n    design in a fully automated and convenient manner, with results conveniently \n    formatted in tables and plots. Each portfolio design is easily defined as a\n    function that takes as input a window of the stock prices and outputs the \n    portfolio weights. Multiple portfolios can be easily specified as a list \n    of functions or as files in a folder. Multiple datasets can be conveniently \n    extracted randomly from different markets, different time periods, and \n    different subsets of the stock universe. The results can be later assessed \n    and ranked with tables based on a number of performance criteria (e.g., \n    expected return, volatility, Sharpe ratio, drawdown, turnover rate, return \n    on investment, computational time, etc.), as well as plotted in a number of \n    ways with nice barplots and boxplots.  "
  },
  {
    "id": 18207,
    "package_name": "portn",
    "title": "Portfolio Analysis for Nature",
    "description": "The functions are designed to find the efficient mean-variance frontier or \n portfolio weights for static portfolio (called  Markowitz portfolio) analysis in resource \n economics or nature conservation. Using the nonlinear programming solver ('Rsolnp'), \n this package deals with the quadratic minimization of the variance-covariances without \n shorting (i.e., non-negative portfolio weights) studied in Ando and Mallory (2012) \n <doi:10.1073/pnas.1114653109>. See the examples, testing versions, and more details from: \n <https://github.com/ysd2004/portn>.",
    "version": "1.0.0",
    "maintainer": "Seong Yun <seong.yun@msstate.edu>",
    "author": "Seong Yun [aut, cre]",
    "url": "https://github.com/ysd2004/portn",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=portn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portn Portfolio Analysis for Nature The functions are designed to find the efficient mean-variance frontier or \n portfolio weights for static portfolio (called  Markowitz portfolio) analysis in resource \n economics or nature conservation. Using the nonlinear programming solver ('Rsolnp'), \n this package deals with the quadratic minimization of the variance-covariances without \n shorting (i.e., non-negative portfolio weights) studied in Ando and Mallory (2012) \n <doi:10.1073/pnas.1114653109>. See the examples, testing versions, and more details from: \n <https://github.com/ysd2004/portn>.  "
  },
  {
    "id": 18208,
    "package_name": "portvine",
    "title": "Vine Based (Un)Conditional Portfolio Risk Measure Estimation",
    "description": "Following Sommer (2022) <https://mediatum.ub.tum.de/1658240>\n    portfolio level risk estimates (e.g. Value at Risk, Expected\n    Shortfall) are estimated by modeling each asset univariately by an\n    ARMA-GARCH model and then their cross dependence via a Vine Copula\n    model in a rolling window fashion. One can even condition on\n    variables/time series at certain quantile levels to stress test the\n    risk measure estimates.",
    "version": "1.0.3",
    "maintainer": "Emanuel Sommer <emanuel_sommer@gmx.de>",
    "author": "Emanuel Sommer [cre, aut]",
    "url": "https://github.com/EmanuelSommer/portvine,\nhttps://emanuelsommer.github.io/portvine/",
    "bug_reports": "https://github.com/EmanuelSommer/portvine/issues",
    "repository": "https://cran.r-project.org/package=portvine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portvine Vine Based (Un)Conditional Portfolio Risk Measure Estimation Following Sommer (2022) <https://mediatum.ub.tum.de/1658240>\n    portfolio level risk estimates (e.g. Value at Risk, Expected\n    Shortfall) are estimated by modeling each asset univariately by an\n    ARMA-GARCH model and then their cross dependence via a Vine Copula\n    model in a rolling window fashion. One can even condition on\n    variables/time series at certain quantile levels to stress test the\n    risk measure estimates.  "
  },
  {
    "id": 18234,
    "package_name": "powerCompRisk",
    "title": "Power Analysis Tool for Joint Testing Hazards with Competing\nRisks Data",
    "description": "A power analysis tool for jointly testing the cause-1 cause-specific hazard and the any-cause hazard with competing risks data.",
    "version": "1.0.1",
    "maintainer": "Eric Kawaguchi <erickawaguchi@ucla.edu>",
    "author": "Qing Yang[aut], Wing K. Fung[aut], Eric Kawaguchi[ctb], Gang Li[aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerCompRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerCompRisk Power Analysis Tool for Joint Testing Hazards with Competing\nRisks Data A power analysis tool for jointly testing the cause-1 cause-specific hazard and the any-cause hazard with competing risks data.  "
  },
  {
    "id": 18242,
    "package_name": "powerSurvEpi",
    "title": "Power and Sample Size Calculation for Survival Analysis of\nEpidemiological Studies",
    "description": "Functions to calculate power and\n                sample size for testing main effect or interaction effect in\n                the survival analysis of epidemiological studies\n                (non-randomized studies), taking into account the \n                correlation between the covariate of the\n                interest and other covariates. Some calculations also take \n                into account the competing risks and stratified analysis. \n                This package also includes\n                a set of functions to calculate power and sample size\n                for testing main effect in the survival analysis of \n                randomized clinical trials and conditional logistic regression for nested case-control study.",
    "version": "0.1.5",
    "maintainer": "Weiliang Qiu <weiliang.qiu@gmail.com>",
    "author": "Weiliang Qiu [aut, cre],\n  Jorge Chavarro [aut],\n  Ross Lazarus [aut],\n  Bernard Rosner [aut],\n  Jing Ma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerSurvEpi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerSurvEpi Power and Sample Size Calculation for Survival Analysis of\nEpidemiological Studies Functions to calculate power and\n                sample size for testing main effect or interaction effect in\n                the survival analysis of epidemiological studies\n                (non-randomized studies), taking into account the \n                correlation between the covariate of the\n                interest and other covariates. Some calculations also take \n                into account the competing risks and stratified analysis. \n                This package also includes\n                a set of functions to calculate power and sample size\n                for testing main effect in the survival analysis of \n                randomized clinical trials and conditional logistic regression for nested case-control study.  "
  },
  {
    "id": 18250,
    "package_name": "powerpkg",
    "title": "Power Analyses for the Affected Sib Pair and the TDT Design",
    "description": "There are two main functions: (1) To estimate the power of testing for linkage using an\n        affected sib pair design, as a function of the recurrence risk\n        ratios. We will use analytical power formulae as implemented in\n        R. These are based on a Mathematica notebook created by Martin\n        Farrall. (2) To examine how the power of the transmission\n        disequilibrium test (TDT) depends on the disease allele\n        frequency, the marker allele frequency, the strength of the\n        linkage disequilibrium, and the magnitude of the genetic\n        effect. We will use an R program that implements the power\n        formulae of Abel and Muller-Myhsok (1998). These formulae allow\n        one to quickly compute power of the TDT approach under a\n        variety of different conditions. This R program was modeled on\n        Martin Farrall's Mathematica notebook.",
    "version": "1.6",
    "maintainer": "Daniel E. Weeks <weeks@pitt.edu>",
    "author": "Daniel E. Weeks",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerpkg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerpkg Power Analyses for the Affected Sib Pair and the TDT Design There are two main functions: (1) To estimate the power of testing for linkage using an\n        affected sib pair design, as a function of the recurrence risk\n        ratios. We will use analytical power formulae as implemented in\n        R. These are based on a Mathematica notebook created by Martin\n        Farrall. (2) To examine how the power of the transmission\n        disequilibrium test (TDT) depends on the disease allele\n        frequency, the marker allele frequency, the strength of the\n        linkage disequilibrium, and the magnitude of the genetic\n        effect. We will use an R program that implements the power\n        formulae of Abel and Muller-Myhsok (1998). These formulae allow\n        one to quickly compute power of the TDT approach under a\n        variety of different conditions. This R program was modeled on\n        Martin Farrall's Mathematica notebook.  "
  },
  {
    "id": 18264,
    "package_name": "ppitables",
    "title": "Lookup Tables to Generate Poverty Likelihoods and Rates using\nthe Poverty Probability Index (PPI)",
    "description": "The Poverty Probability Index (PPI) is a poverty measurement tool \n    for organizations and businesses with a mission to serve the poor. The PPI \n    is statistically-sound, yet simple to use: the answers to 10 questions about \n    a household's characteristics and asset ownership are scored to compute the \n    likelihood that the household is living below the poverty line - or above by \n    only a narrow margin. This package contains country-specific lookup data\n    tables used as reference to determine the poverty likelihood of a household\n    based on their score from the country-specific PPI questionnaire. These\n    lookup tables have been extracted from documentation of the PPI found at \n    <https://www.povertyindex.org> and managed by Innovations for Poverty Action \n    <https://poverty-action.org/>.",
    "version": "0.6.0",
    "maintainer": "Ernest Guevarra <ernestgmd@gmail.com>",
    "author": "Ernest Guevarra [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4887-4415>)",
    "url": "https://github.com/katilingban/ppitables,\nhttps://katilingban.io/ppitables/",
    "bug_reports": "https://github.com/katilingban/ppitables/issues",
    "repository": "https://cran.r-project.org/package=ppitables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppitables Lookup Tables to Generate Poverty Likelihoods and Rates using\nthe Poverty Probability Index (PPI) The Poverty Probability Index (PPI) is a poverty measurement tool \n    for organizations and businesses with a mission to serve the poor. The PPI \n    is statistically-sound, yet simple to use: the answers to 10 questions about \n    a household's characteristics and asset ownership are scored to compute the \n    likelihood that the household is living below the poverty line - or above by \n    only a narrow margin. This package contains country-specific lookup data\n    tables used as reference to determine the poverty likelihood of a household\n    based on their score from the country-specific PPI questionnaire. These\n    lookup tables have been extracted from documentation of the PPI found at \n    <https://www.povertyindex.org> and managed by Innovations for Poverty Action \n    <https://poverty-action.org/>.  "
  },
  {
    "id": 18268,
    "package_name": "ppmHR",
    "title": "Privacy-Protecting Hazard Ratio Estimation in Distributed Data\nNetworks",
    "description": "An implementation of the one-step privacy-protecting method for estimating the overall and site-specific hazard ratios using inverse probability weighted Cox models in distributed data network studies, as proposed by Shu, Yoshida, Fireman, and Toh (2019) <doi: 10.1177/0962280219869742>. This method only requires sharing of summary-level riskset tables instead of individual-level data. Both the conventional inverse probability weights and the stabilized weights are implemented. ",
    "version": "1.0",
    "maintainer": "Di Shu <shudi1991@gmail.com>",
    "author": "Di Shu <shudi1991@gmail.com>, Sengwee Toh <darren_toh@harvardpilgrim.org>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ppmHR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppmHR Privacy-Protecting Hazard Ratio Estimation in Distributed Data\nNetworks An implementation of the one-step privacy-protecting method for estimating the overall and site-specific hazard ratios using inverse probability weighted Cox models in distributed data network studies, as proposed by Shu, Yoshida, Fireman, and Toh (2019) <doi: 10.1177/0962280219869742>. This method only requires sharing of summary-level riskset tables instead of individual-level data. Both the conventional inverse probability weights and the stabilized weights are implemented.   "
  },
  {
    "id": 18339,
    "package_name": "pretestcad",
    "title": "Pretest Probability for Coronary Artery Disease",
    "description": "An application to calculate a patient's pretest probability\n    (PTP) for obstructive Coronary Artery Disease (CAD) from a collection\n    of guidelines or studies. Guidelines usually comes from the American\n    Heart Association (AHA), American College of Cardiology (ACC) or\n    European Society of Cardiology (ESC). Examples of PTP scores that\n    comes from studies are the 2020 Winther et al. basic, Risk\n    Factor-weighted Clinical Likelihood (RF-CL) and Coronary Artery\n    Calcium Score-weighted Clinical Likelihood (CACS-CL) models\n    <doi:10.1016/j.jacc.2020.09.585>, 2019 Reeh et al. basic and clinical\n    models <doi:10.1093/eurheartj/ehy806> and 2017 Fordyce et al. PROMISE\n    Minimal-Risk Tool <doi:10.1001/jamacardio.2016.5501>.  As diagnosis of\n    CAD involves a costly and invasive coronary angiography procedure for\n    patients, having a reliable PTP for CAD helps doctors to make better\n    decisions during patient management.  This ensures high risk patients\n    can be diagnosed and treated early for CAD while avoiding unnecessary\n    testing for low risk patients.",
    "version": "1.1.0",
    "maintainer": "Jeremy Selva <jeremy1189.jjs@gmail.com>",
    "author": "Jeremy Selva [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4498-2662>)",
    "url": "https://github.com/JauntyJJS/pretestcad,\nhttps://jauntyjjs.github.io/pretestcad/",
    "bug_reports": "https://github.com/JauntyJJS/pretestcad/issues",
    "repository": "https://cran.r-project.org/package=pretestcad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pretestcad Pretest Probability for Coronary Artery Disease An application to calculate a patient's pretest probability\n    (PTP) for obstructive Coronary Artery Disease (CAD) from a collection\n    of guidelines or studies. Guidelines usually comes from the American\n    Heart Association (AHA), American College of Cardiology (ACC) or\n    European Society of Cardiology (ESC). Examples of PTP scores that\n    comes from studies are the 2020 Winther et al. basic, Risk\n    Factor-weighted Clinical Likelihood (RF-CL) and Coronary Artery\n    Calcium Score-weighted Clinical Likelihood (CACS-CL) models\n    <doi:10.1016/j.jacc.2020.09.585>, 2019 Reeh et al. basic and clinical\n    models <doi:10.1093/eurheartj/ehy806> and 2017 Fordyce et al. PROMISE\n    Minimal-Risk Tool <doi:10.1001/jamacardio.2016.5501>.  As diagnosis of\n    CAD involves a costly and invasive coronary angiography procedure for\n    patients, having a reliable PTP for CAD helps doctors to make better\n    decisions during patient management.  This ensures high risk patients\n    can be diagnosed and treated early for CAD while avoiding unnecessary\n    testing for low risk patients.  "
  },
  {
    "id": 18345,
    "package_name": "prevR",
    "title": "Estimating Regional Trends of a Prevalence from a DHS and\nSimilar Surveys",
    "description": "Spatial estimation of a prevalence surface\n    or a relative risks surface, using data from a Demographic and Health\n    Survey (DHS) or an analog survey, see Larmarange et al. (2011)\n    <doi:10.4000/cybergeo.24606>.",
    "version": "5.0.0",
    "maintainer": "Joseph Larmarange <joseph.larmarange@ird.fr>",
    "author": "Joseph Larmarange [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7097-700X>)",
    "url": "https://github.com/larmarange/prevR/",
    "bug_reports": "https://github.com/larmarange/prevR/issues",
    "repository": "https://cran.r-project.org/package=prevR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prevR Estimating Regional Trends of a Prevalence from a DHS and\nSimilar Surveys Spatial estimation of a prevalence surface\n    or a relative risks surface, using data from a Demographic and Health\n    Survey (DHS) or an analog survey, see Larmarange et al. (2011)\n    <doi:10.4000/cybergeo.24606>.  "
  },
  {
    "id": 18348,
    "package_name": "preventr",
    "title": "An Implementation of the PREVENT and Pooled Cohort Equations",
    "description": "Implements the American Heart Association Predicting\n    Risk of cardiovascular disease EVENTs (PREVENT) equations from Khan\n    SS, Matsushita K, Sang Y, and colleagues (2023)\n    <doi:10.1161/CIRCULATIONAHA.123.067626>, with optional comparison \n    with their de facto predecessor, the Pooled Cohort Equations from the \n\t  American Heart Association and American College of Cardiology (2013)\n\t  <doi:10.1161/01.cir.0000437741.48606.98> and the revision to the Pooled \n\t  Cohort Equations from Yadlowsky and colleagues (2018) \n\t  <doi:10.7326/M17-3011>.",
    "version": "0.11.0",
    "maintainer": "Martin Mayer <mmayer@ebsco.com>",
    "author": "Martin Mayer [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3210-2274>)",
    "url": "https://martingmayer.com/preventr,\nhttps://github.com/martingmayer/preventr,\nhttps://martingmayer.shinyapps.io/prevent-equations/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=preventr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "preventr An Implementation of the PREVENT and Pooled Cohort Equations Implements the American Heart Association Predicting\n    Risk of cardiovascular disease EVENTs (PREVENT) equations from Khan\n    SS, Matsushita K, Sang Y, and colleagues (2023)\n    <doi:10.1161/CIRCULATIONAHA.123.067626>, with optional comparison \n    with their de facto predecessor, the Pooled Cohort Equations from the \n\t  American Heart Association and American College of Cardiology (2013)\n\t  <doi:10.1161/01.cir.0000437741.48606.98> and the revision to the Pooled \n\t  Cohort Equations from Yadlowsky and colleagues (2018) \n\t  <doi:10.7326/M17-3011>.  "
  },
  {
    "id": 18367,
    "package_name": "prioGene",
    "title": "Candidate Gene Prioritization for Non-Communicable Diseases\nBased on Functional Information",
    "description": "In gene sequencing methods, the topological features of protein-protein interaction (PPI) networks are \n    often used, such as ToppNet <https://toppgene.cchmc.org>. In this study, a candidate gene prioritization method\n    was proposed for non-communicable diseases considering disease risks transferred between genes in weighted disease \n    PPI networks with weights for nodes and edges based on functional information.",
    "version": "1.0.1",
    "maintainer": "Erqiang Hu <13766876214@163.com>",
    "author": "Erqiang Hu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=prioGene",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prioGene Candidate Gene Prioritization for Non-Communicable Diseases\nBased on Functional Information In gene sequencing methods, the topological features of protein-protein interaction (PPI) networks are \n    often used, such as ToppNet <https://toppgene.cchmc.org>. In this study, a candidate gene prioritization method\n    was proposed for non-communicable diseases considering disease risks transferred between genes in weighted disease \n    PPI networks with weights for nodes and edges based on functional information.  "
  },
  {
    "id": 18432,
    "package_name": "prop.comb.RR",
    "title": "Analyzing Combination of Proportions and Relative Risk",
    "description": "Carrying out inferences about any linear combination of proportions and the ratio of two proportions.",
    "version": "1.2",
    "maintainer": "Maria Alvarez <maria.alvarez@uvigo.es>",
    "author": "Maria Alvarez and Javier Roca-Pardinas",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=prop.comb.RR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prop.comb.RR Analyzing Combination of Proportions and Relative Risk Carrying out inferences about any linear combination of proportions and the ratio of two proportions.  "
  },
  {
    "id": 18480,
    "package_name": "pscore",
    "title": "Standardizing Physiological Composite Risk Endpoints",
    "description": "Provides a number of functions to\n  simplify and automate the scoring, comparison, and evaluation of\n  different ways of creating composites of data.  It is particularly\n  aimed at facilitating the creation of physiological composites of\n  metabolic syndrome symptom score (MetSSS) and allostatic load (AL).\n  Provides a wrapper to calculate the MetSSS on new data using the\n  Healthy Hearts formula. ",
    "version": "0.4.1",
    "maintainer": "Joshua F. Wiley <jwiley.psych@gmail.com>",
    "author": "Joshua F. Wiley [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0271-6702>)",
    "url": "https://score-project.org, https://github.com/JWiley/score-project",
    "bug_reports": "https://github.com/JWiley/score-project/issues",
    "repository": "https://cran.r-project.org/package=pscore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pscore Standardizing Physiological Composite Risk Endpoints Provides a number of functions to\n  simplify and automate the scoring, comparison, and evaluation of\n  different ways of creating composites of data.  It is particularly\n  aimed at facilitating the creation of physiological composites of\n  metabolic syndrome symptom score (MetSSS) and allostatic load (AL).\n  Provides a wrapper to calculate the MetSSS on new data using the\n  Healthy Hearts formula.   "
  },
  {
    "id": 18483,
    "package_name": "pseudo",
    "title": "Computes Pseudo-Observations for Modeling",
    "description": "Various functions for computing pseudo-observations for censored data regression. Computes pseudo-observations for modeling: competing risks based on the cumulative incidence function, survival function based on the restricted mean,  survival function based on the Kaplan-Meier estimator see Klein et al. (2008) <doi:10.1016/j.cmpb.2007.11.017>. ",
    "version": "1.4.3",
    "maintainer": "Kevin Rodrigues <kevin.asr@outlook.com>",
    "author": "Maja Pohar Perme [aut],\n  Mette Gerster [aut],\n  Kevin Rodrigues [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pseudo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pseudo Computes Pseudo-Observations for Modeling Various functions for computing pseudo-observations for censored data regression. Computes pseudo-observations for modeling: competing risks based on the cumulative incidence function, survival function based on the restricted mean,  survival function based on the Kaplan-Meier estimator see Klein et al. (2008) <doi:10.1016/j.cmpb.2007.11.017>.   "
  },
  {
    "id": 18488,
    "package_name": "pseval",
    "title": "Methods for Evaluating Principal Surrogates of Treatment\nResponse",
    "description": "Contains the core methods for the evaluation of principal\n    surrogates in a single clinical trial. Provides a flexible interface for\n    defining models for the risk given treatment and the surrogate, the models\n    for integration over the missing counterfactual surrogate responses, and the\n    estimation methods. Estimated maximum likelihood and pseudo-score can be used\n    for estimation, and the bootstrap for inference. A variety of post-estimation\n    summary methods are provided, including print, summary, plot, and testing.",
    "version": "1.3.3",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "author": "Michael C Sachs [aut, cre],\n  Erin E Gabriel [aut]",
    "url": "https://sachsmc.github.io/pseval/",
    "bug_reports": "https://github.com/sachsmc/pseval/issues/",
    "repository": "https://cran.r-project.org/package=pseval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pseval Methods for Evaluating Principal Surrogates of Treatment\nResponse Contains the core methods for the evaluation of principal\n    surrogates in a single clinical trial. Provides a flexible interface for\n    defining models for the risk given treatment and the surrogate, the models\n    for integration over the missing counterfactual surrogate responses, and the\n    estimation methods. Estimated maximum likelihood and pseudo-score can be used\n    for estimation, and the bootstrap for inference. A variety of post-estimation\n    summary methods are provided, including print, summary, plot, and testing.  "
  },
  {
    "id": 18505,
    "package_name": "pssmooth",
    "title": "Flexible and Efficient Evaluation of Principal\nSurrogates/Treatment Effect Modifiers",
    "description": "Implements estimation and testing procedures for evaluating an intermediate biomarker response as a principal surrogate of a clinical response to treatment (i.e., principal stratification effect modification analysis), as described in Juraska M, Huang Y, and Gilbert PB (2020), Inference on treatment effect modification by biomarker response in a three-phase sampling design, Biostatistics, 21(3): 545-560 <doi:10.1093/biostatistics/kxy074>. The methods avoid the restrictive 'placebo structural risk' modeling assumption common to past methods and further improve robustness by the use of nonparametric kernel smoothing for biomarker density estimation. A randomized controlled two-group clinical efficacy trial is assumed with an ordered categorical or continuous univariate biomarker response measured at a fixed timepoint post-randomization and with a univariate baseline surrogate measure allowed to be observed in only a subset of trial participants with an observed biomarker response (see the flexible three-phase sampling design in the paper for details). Bootstrap-based procedures are available for pointwise and simultaneous confidence intervals and testing of four relevant hypotheses. Summary and plotting functions are provided for estimation results.",
    "version": "1.0.3",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "author": "Michal Juraska [aut, cre]",
    "url": "https://github.com/mjuraska/pssmooth",
    "bug_reports": "https://github.com/mjuraska/pssmooth/issues",
    "repository": "https://cran.r-project.org/package=pssmooth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pssmooth Flexible and Efficient Evaluation of Principal\nSurrogates/Treatment Effect Modifiers Implements estimation and testing procedures for evaluating an intermediate biomarker response as a principal surrogate of a clinical response to treatment (i.e., principal stratification effect modification analysis), as described in Juraska M, Huang Y, and Gilbert PB (2020), Inference on treatment effect modification by biomarker response in a three-phase sampling design, Biostatistics, 21(3): 545-560 <doi:10.1093/biostatistics/kxy074>. The methods avoid the restrictive 'placebo structural risk' modeling assumption common to past methods and further improve robustness by the use of nonparametric kernel smoothing for biomarker density estimation. A randomized controlled two-group clinical efficacy trial is assumed with an ordered categorical or continuous univariate biomarker response measured at a fixed timepoint post-randomization and with a univariate baseline surrogate measure allowed to be observed in only a subset of trial participants with an observed biomarker response (see the flexible three-phase sampling design in the paper for details). Bootstrap-based procedures are available for pointwise and simultaneous confidence intervals and testing of four relevant hypotheses. Summary and plotting functions are provided for estimation results.  "
  },
  {
    "id": 18604,
    "package_name": "qPRAentry",
    "title": "Quantitative Pest Risk Assessment at the Entry Step",
    "description": "Supports risk assessors in performing the entry step of the\n    quantitative Pest Risk Assessment. It allows the estimation of the\n    amount of a plant pest entering a risk assessment area (in terms of\n    founder populations) through the calculation of the imported\n    commodities that could be potential pathways of pest entry, and the\n    development of a pathway model. Two 'Shiny' apps based on the\n    functionalities of the package are included, that simplify the process\n    of assessing the risk of entry of plant pests. The approach is based\n    on the work of the European Food Safety Authority (EFSA PLH Panel et\n    al., 2018) <doi:10.2903/j.efsa.2018.5350>.",
    "version": "0.1.1",
    "maintainer": "Martina Cendoya <cendoya_marmar@gva.es>",
    "author": "Martina Cendoya [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0134-7781>),\n  Maria Chiara Rosace [aut] (ORCID:\n    <https://orcid.org/0000-0002-7263-8228>)",
    "url": "https://github.com/mcendoya/qPRAentry",
    "bug_reports": "https://github.com/mcendoya/qPRAentry/issues",
    "repository": "https://cran.r-project.org/package=qPRAentry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qPRAentry Quantitative Pest Risk Assessment at the Entry Step Supports risk assessors in performing the entry step of the\n    quantitative Pest Risk Assessment. It allows the estimation of the\n    amount of a plant pest entering a risk assessment area (in terms of\n    founder populations) through the calculation of the imported\n    commodities that could be potential pathways of pest entry, and the\n    development of a pathway model. Two 'Shiny' apps based on the\n    functionalities of the package are included, that simplify the process\n    of assessing the risk of entry of plant pests. The approach is based\n    on the work of the European Food Safety Authority (EFSA PLH Panel et\n    al., 2018) <doi:10.2903/j.efsa.2018.5350>.  "
  },
  {
    "id": 18638,
    "package_name": "qgg",
    "title": "Statistical Tools for Quantitative Genetic Analyses",
    "description": "Provides an infrastructure for efficient processing of large-scale genetic and phenotypic data including core functions for: 1) fitting linear mixed models, 2) constructing marker-based genomic relationship matrices, 3) estimating genetic parameters (heritability and correlation), 4) performing genomic prediction and genetic risk profiling, and 5) single or multi-marker association analyses.\n    Rohde et al. (2019) <doi:10.1101/503631>.",
    "version": "1.1.6",
    "maintainer": "Peter Soerensen <peter.sorensen@r-qgg.org>",
    "author": "Peter Soerensen [aut, cre],\n  Palle Duun Rohde [aut],\n  Izel Fourie Soerensen [aut]",
    "url": "https://github.com/psoerensen/qgg",
    "bug_reports": "https://github.com/psoerensen/qgg/issues",
    "repository": "https://cran.r-project.org/package=qgg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qgg Statistical Tools for Quantitative Genetic Analyses Provides an infrastructure for efficient processing of large-scale genetic and phenotypic data including core functions for: 1) fitting linear mixed models, 2) constructing marker-based genomic relationship matrices, 3) estimating genetic parameters (heritability and correlation), 4) performing genomic prediction and genetic risk profiling, and 5) single or multi-marker association analyses.\n    Rohde et al. (2019) <doi:10.1101/503631>.  "
  },
  {
    "id": 18653,
    "package_name": "qmj",
    "title": "Quality Scores for the Russell 3000",
    "description": "Produces quality scores for each of the US companies from \n    the Russell 3000, following the approach described in \n    \"Quality Minus Junk\" (Asness, Frazzini, & Pedersen, 2013) \n    <http://www.aqr.com/library/working-papers/quality-minus-junk>. \n    The package includes datasets for users who wish to view the most recently \n    uploaded quality scores. It also provides tools to automatically gather \n    relevant financials and stock price information, allowing users to update \n    their data and customize their universe for further analysis.",
    "version": "0.2.1",
    "maintainer": "Yanrong Song <yrsong129@gmail.com>",
    "author": "Anthoney Tsou [aut],\n  Eugene Choe [aut],\n  David Kane [aut],\n  Ryan Kwon [aut],\n  Yanrong Song [aut, cre],\n  Zijie Zhu [aut]",
    "url": "",
    "bug_reports": "https://github.com/anttsou/qmj/issues",
    "repository": "https://cran.r-project.org/package=qmj",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qmj Quality Scores for the Russell 3000 Produces quality scores for each of the US companies from \n    the Russell 3000, following the approach described in \n    \"Quality Minus Junk\" (Asness, Frazzini, & Pedersen, 2013) \n    <http://www.aqr.com/library/working-papers/quality-minus-junk>. \n    The package includes datasets for users who wish to view the most recently \n    uploaded quality scores. It also provides tools to automatically gather \n    relevant financials and stock price information, allowing users to update \n    their data and customize their universe for further analysis.  "
  },
  {
    "id": 18677,
    "package_name": "qrmdata",
    "title": "Data Sets for Quantitative Risk Management Practice",
    "description": "Various data sets (stocks, stock indices, constituent data, FX,\n zero-coupon bond yield curves, volatility, commodities) for Quantitative\n Risk Management practice.",
    "version": "2025-07-24-3",
    "maintainer": "Marius Hofert <mhofert@hku.hk>",
    "author": "Marius Hofert [aut, cre],\n  Kurt Hornik [aut],\n  Alexander J. McNeil [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qrmdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qrmdata Data Sets for Quantitative Risk Management Practice Various data sets (stocks, stock indices, constituent data, FX,\n zero-coupon bond yield curves, volatility, commodities) for Quantitative\n Risk Management practice.  "
  },
  {
    "id": 18679,
    "package_name": "qrmtools",
    "title": "Tools for Quantitative Risk Management",
    "description": "Functions and data sets for reproducing selected results from\n  the book \"Quantitative Risk Management: Concepts, Techniques and Tools\".\n  Furthermore, new developments and auxiliary functions for Quantitative\n  Risk Management practice.",
    "version": "0.0-19",
    "maintainer": "Marius Hofert <mhofert@hku.hk>",
    "author": "Marius Hofert [aut, cre],\n  Kurt Hornik [aut],\n  Alexander J. McNeil [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qrmtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qrmtools Tools for Quantitative Risk Management Functions and data sets for reproducing selected results from\n  the book \"Quantitative Risk Management: Concepts, Techniques and Tools\".\n  Furthermore, new developments and auxiliary functions for Quantitative\n  Risk Management practice.  "
  },
  {
    "id": 18723,
    "package_name": "quantdates",
    "title": "Manipulate Dates for Finance",
    "description": "Functions to manipulate dates and count days for quantitative finance analysis. The 'quantdates' package considers leap, holidays and business days for relevant calendars in a financial context to simplify quantitative finance calculations, consistent with International Swaps and Derivatives Association (ISDA) (2006) <https://www.isda.org/book/2006-isda-definitions/> regulations.",
    "version": "2.0.4",
    "maintainer": "Juan Pablo Bermudez <juan.bermudez@quantil.com.co>",
    "author": "Julian Chitiva [aut],\n  Diego Jara [aut],\n  Erick Translateur [com],\n  Juan Pablo Bermudez [aut, cre],\n  Quantil S.A.S [aut, cph]",
    "url": "https://github.com/quantilma/quantdates",
    "bug_reports": "https://github.com/quantilma/quantdates/issues",
    "repository": "https://cran.r-project.org/package=quantdates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantdates Manipulate Dates for Finance Functions to manipulate dates and count days for quantitative finance analysis. The 'quantdates' package considers leap, holidays and business days for relevant calendars in a financial context to simplify quantitative finance calculations, consistent with International Swaps and Derivatives Association (ISDA) (2006) <https://www.isda.org/book/2006-isda-definitions/> regulations.  "
  },
  {
    "id": 18734,
    "package_name": "quantmod",
    "title": "Quantitative Financial Modelling Framework",
    "description": "Specify, build, trade, and analyse quantitative financial trading strategies.",
    "version": "0.4.28",
    "maintainer": "Joshua M. Ulrich <josh.m.ulrich@gmail.com>",
    "author": "Jeffrey A. Ryan [aut, cph],\n  Joshua M. Ulrich [cre, aut],\n  Ethan B. Smith [ctb],\n  Wouter Thielen [ctb],\n  Paul Teetor [ctb],\n  Steve Bronder [ctb]",
    "url": "https://www.quantmod.com/,\nhttps://github.com/joshuaulrich/quantmod",
    "bug_reports": "https://github.com/joshuaulrich/quantmod/issues",
    "repository": "https://cran.r-project.org/package=quantmod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantmod Quantitative Financial Modelling Framework Specify, build, trade, and analyse quantitative financial trading strategies.  "
  },
  {
    "id": 18736,
    "package_name": "quantreg",
    "title": "Quantile Regression",
    "description": "Estimation and inference methods for models for conditional quantile functions: \n  Linear and nonlinear parametric and non-parametric (total variation penalized) models \n  for conditional quantiles of a univariate response and several methods for handling\n  censored survival data.  Portfolio selection methods based on expected shortfall\n  risk are also now included. See Koenker, R. (2005) Quantile Regression, Cambridge U. Press,\n  <doi:10.1017/CBO9780511754098> and Koenker, R. et al. (2017) Handbook of Quantile Regression, \n  CRC Press, <doi:10.1201/9781315120256>. ",
    "version": "6.1",
    "maintainer": "Roger Koenker <rkoenker@illinois.edu>",
    "author": "Roger Koenker [cre, aut],\n  Stephen Portnoy [ctb] (Contributions to Censored QR code),\n  Pin Tian Ng [ctb] (Contributions to Sparse QR code),\n  Blaise Melly [ctb] (Contributions to preprocessing code),\n  Achim Zeileis [ctb] (Contributions to dynrq code essentially identical\n    to his dynlm code),\n  Philip Grosjean [ctb] (Contributions to nlrq code),\n  Cleve Moler [ctb] (author of several linpack routines),\n  Yousef Saad [ctb] (author of sparskit2),\n  Victor Chernozhukov [ctb] (contributions to extreme value inference\n    code),\n  Ivan Fernandez-Val [ctb] (contributions to extreme value inference\n    code),\n  Martin Maechler [ctb] (tweaks (src/chlfct.f, 'tiny','Large'), ORCID:\n    <https://orcid.org/0000-0002-8685-9910>),\n  Brian D Ripley [trl, ctb] (Initial (2001) R port from S (to my\n    everlasting shame -- how could I have been so slow to adopt R!) and\n    for numerous other suggestions and useful advice)",
    "url": "https://www.r-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quantreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantreg Quantile Regression Estimation and inference methods for models for conditional quantile functions: \n  Linear and nonlinear parametric and non-parametric (total variation penalized) models \n  for conditional quantiles of a univariate response and several methods for handling\n  censored survival data.  Portfolio selection methods based on expected shortfall\n  risk are also now included. See Koenker, R. (2005) Quantile Regression, Cambridge U. Press,\n  <doi:10.1017/CBO9780511754098> and Koenker, R. et al. (2017) Handbook of Quantile Regression, \n  CRC Press, <doi:10.1201/9781315120256>.   "
  },
  {
    "id": 18743,
    "package_name": "quarks",
    "title": "Simple Methods for Calculating and Backtesting Value at Risk and\nExpected Shortfall",
    "description": "Enables the user to calculate Value at Risk (VaR)\n    and Expected Shortfall (ES) by means of various types of historical\n    simulation. Currently plain-, age-, volatility-weighted- and filtered\n    historical simulation are implemented in this package. Volatility weighting\n    can be carried out via an exponentially weighted moving average model\n    (EWMA) or other GARCH-type models. The performance can be assessed via\n    Traffic Light Test, Coverage Tests and Loss Functions. The methods of the\n    package are described in Gurrola-Perez, P. and Murphy, D. (2015)\n    <https://EconPapers.repec.org/RePEc:boe:boeewp:0525> as well as McNeil, J.,\n    Frey, R., and Embrechts, P. (2015) <https://ideas.repec.org/b/pup/pbooks/10496.html>.",
    "version": "1.1.5",
    "maintainer": "Sebastian Letmathe <sebastian.let@yahoo.com>",
    "author": "Sebastian Letmathe [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quarks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quarks Simple Methods for Calculating and Backtesting Value at Risk and\nExpected Shortfall Enables the user to calculate Value at Risk (VaR)\n    and Expected Shortfall (ES) by means of various types of historical\n    simulation. Currently plain-, age-, volatility-weighted- and filtered\n    historical simulation are implemented in this package. Volatility weighting\n    can be carried out via an exponentially weighted moving average model\n    (EWMA) or other GARCH-type models. The performance can be assessed via\n    Traffic Light Test, Coverage Tests and Loss Functions. The methods of the\n    package are described in Gurrola-Perez, P. and Murphy, D. (2015)\n    <https://EconPapers.repec.org/RePEc:boe:boeewp:0525> as well as McNeil, J.,\n    Frey, R., and Embrechts, P. (2015) <https://ideas.repec.org/b/pup/pbooks/10496.html>.  "
  },
  {
    "id": 18786,
    "package_name": "r2dii.analysis",
    "title": "Measure Climate Scenario Alignment of Corporate Loans",
    "description": "These tools help you to assess if a corporate lending\n    portfolio aligns with climate goals. They summarize key climate\n    indicators attributed to the portfolio (e.g. production, emission\n    factors), and calculate alignment targets based on climate scenarios.\n    They implement in R the last step of the free software 'PACTA' (Paris\n    Agreement Capital Transition Assessment;\n    <https://www.transitionmonitor.com/>). Financial institutions use 'PACTA'\n    to study how their capital allocation decisions align with climate\n    change mitigation goals.",
    "version": "0.5.2",
    "maintainer": "Jacob Kastl <jacob.kastl@gmail.com>",
    "author": "Jacob Kastl [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0000-8281-8129>),\n  Alex Axthelm [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0001-8579-8565>),\n  Jackson Hoffart [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-8600-5042>),\n  Mauro Lepore [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>),\n  Klaus Hogedorn [aut],\n  Nicky Halterman [aut],\n  RMI [cph, fnd]",
    "url": "https://rmi-pacta.github.io/r2dii.analysis/,\nhttps://github.com/RMI-PACTA/r2dii.analysis",
    "bug_reports": "https://github.com/RMI-PACTA/r2dii.analysis/issues",
    "repository": "https://cran.r-project.org/package=r2dii.analysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dii.analysis Measure Climate Scenario Alignment of Corporate Loans These tools help you to assess if a corporate lending\n    portfolio aligns with climate goals. They summarize key climate\n    indicators attributed to the portfolio (e.g. production, emission\n    factors), and calculate alignment targets based on climate scenarios.\n    They implement in R the last step of the free software 'PACTA' (Paris\n    Agreement Capital Transition Assessment;\n    <https://www.transitionmonitor.com/>). Financial institutions use 'PACTA'\n    to study how their capital allocation decisions align with climate\n    change mitigation goals.  "
  },
  {
    "id": 18787,
    "package_name": "r2dii.data",
    "title": "Datasets to Measure the Alignment of Corporate Loan Books with\nClimate Goals",
    "description": "These datasets support the implementation in R of the\n    software 'PACTA' (Paris Agreement Capital Transition Assessment),\n    which is a free tool that calculates the alignment between corporate\n    lending portfolios and climate scenarios\n    (<https://www.transitionmonitor.com/>). Financial institutions use\n    'PACTA' to study how their capital allocation decisions align with\n    climate change mitigation goals. Because both financial institutions\n    and market data providers keep their data private, this package\n    provides fake, public data to enable the development and use of\n    'PACTA' in R.",
    "version": "0.6.1",
    "maintainer": "Jacob Kastl <jacob.kastl@gmail.com>",
    "author": "Jacob Kastl [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0000-8281-8129>),\n  Alex Axthelm [aut, ctr, dtc] (ORCID:\n    <https://orcid.org/0000-0001-8579-8565>),\n  Jackson Hoffart [aut, ctr, dtc] (ORCID:\n    <https://orcid.org/0000-0002-8600-5042>),\n  Mauro Lepore [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>),\n  RMI [cph, fnd]",
    "url": "https://rmi-pacta.github.io/r2dii.data/,\nhttps://github.com/RMI-PACTA/r2dii.data",
    "bug_reports": "https://github.com/RMI-PACTA/r2dii.data/issues",
    "repository": "https://cran.r-project.org/package=r2dii.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dii.data Datasets to Measure the Alignment of Corporate Loan Books with\nClimate Goals These datasets support the implementation in R of the\n    software 'PACTA' (Paris Agreement Capital Transition Assessment),\n    which is a free tool that calculates the alignment between corporate\n    lending portfolios and climate scenarios\n    (<https://www.transitionmonitor.com/>). Financial institutions use\n    'PACTA' to study how their capital allocation decisions align with\n    climate change mitigation goals. Because both financial institutions\n    and market data providers keep their data private, this package\n    provides fake, public data to enable the development and use of\n    'PACTA' in R.  "
  },
  {
    "id": 18788,
    "package_name": "r2dii.match",
    "title": "Tools to Match Corporate Lending Portfolios with Climate Data",
    "description": "These tools implement in R a fundamental part of the software\n    'PACTA' (Paris Agreement Capital Transition Assessment), which is a\n    free tool that calculates the alignment between financial portfolios\n    and climate scenarios (<https://www.transitionmonitor.com/>). Financial\n    institutions use 'PACTA' to study how their capital allocation\n    decisions align with climate change mitigation goals. This package\n    matches data from corporate lending portfolios to asset level data\n    from market-intelligence databases (e.g. power plant capacities,\n    emission factors, etc.). This is the first step to assess if a\n    financial portfolio aligns with climate goals.",
    "version": "0.4.1",
    "maintainer": "Jacob Kastl <jacob.kastl@gmail.com>",
    "author": "Jacob Kastl [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0000-8281-8129>),\n  Alex Axthelm [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0001-8579-8565>),\n  Jackson Hoffart [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-8600-5042>),\n  Mauro Lepore [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>),\n  Klaus Hagedorn [aut],\n  Florence Palandri [aut],\n  Evgeny Petrovsky [aut],\n  RMI [cph, fnd]",
    "url": "https://rmi-pacta.github.io/r2dii.match/,\nhttps://github.com/RMI-PACTA/r2dii.match",
    "bug_reports": "https://github.com/RMI-PACTA/r2dii.match/issues",
    "repository": "https://cran.r-project.org/package=r2dii.match",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dii.match Tools to Match Corporate Lending Portfolios with Climate Data These tools implement in R a fundamental part of the software\n    'PACTA' (Paris Agreement Capital Transition Assessment), which is a\n    free tool that calculates the alignment between financial portfolios\n    and climate scenarios (<https://www.transitionmonitor.com/>). Financial\n    institutions use 'PACTA' to study how their capital allocation\n    decisions align with climate change mitigation goals. This package\n    matches data from corporate lending portfolios to asset level data\n    from market-intelligence databases (e.g. power plant capacities,\n    emission factors, etc.). This is the first step to assess if a\n    financial portfolio aligns with climate goals.  "
  },
  {
    "id": 18789,
    "package_name": "r2dii.plot",
    "title": "Visualize the Climate Scenario Alignment of a Financial\nPortfolio",
    "description": "Create plots to visualize the alignment of a corporate\n    lending financial portfolio to climate change scenarios based on\n    climate indicators (production and emission intensities) across key\n    climate relevant sectors of the 'PACTA' methodology (Paris Agreement Capital \n    Transition Assessment; <https://www.transitionmonitor.com/>).\n    Financial institutions use 'PACTA' to study how their capital\n    allocation decisions align with climate change mitigation goals.",
    "version": "0.5.2",
    "maintainer": "Monika Furdyna <monika.furdyna@gmail.com>",
    "author": "Monika Furdyna [aut, ctr, cre] (ORCID:\n    <https://orcid.org/0000-0002-3728-0646>),\n  Mauro Lepore [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>),\n  Alex Axthelm [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0001-8579-8565>),\n  RMI [cph, fnd]",
    "url": "https://rmi-pacta.github.io/r2dii.plot/,\nhttps://github.com/RMI-PACTA/r2dii.plot",
    "bug_reports": "https://github.com/RMI-PACTA/r2dii.plot/issues",
    "repository": "https://cran.r-project.org/package=r2dii.plot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dii.plot Visualize the Climate Scenario Alignment of a Financial\nPortfolio Create plots to visualize the alignment of a corporate\n    lending financial portfolio to climate change scenarios based on\n    climate indicators (production and emission intensities) across key\n    climate relevant sectors of the 'PACTA' methodology (Paris Agreement Capital \n    Transition Assessment; <https://www.transitionmonitor.com/>).\n    Financial institutions use 'PACTA' to study how their capital\n    allocation decisions align with climate change mitigation goals.  "
  },
  {
    "id": 18813,
    "package_name": "r4ss",
    "title": "R Code for Stock Synthesis",
    "description": "A collection of R functions for use with Stock Synthesis, a\n    fisheries stock assessment modeling platform written in ADMB by Dr. Richard\n    D. Methot at the NOAA Northwest Fisheries Science Center. The functions\n    include tools for summarizing and plotting results, manipulating files,\n    visualizing model parameterizations, and various other common stock\n    assessment tasks.\n    This version of '{r4ss}' is compatible with Stock Synthesis versions\n    3.24 through 3.30 (specifically version 3.30.19.01, from April\n    2022).",
    "version": "1.44.0",
    "maintainer": "Ian G. Taylor <Ian.Taylor@noaa.gov>",
    "author": "Ian G. Taylor [aut, cre],\n  Ian J. Stewart [aut],\n  Allan C. Hicks [aut],\n  Tommy M. Garrison [aut],\n  Andre E. Punt [aut],\n  John R. Wallace [aut],\n  Chantel R. Wetzel [aut],\n  James T. Thorson [aut],\n  Yukio Takeuchi [aut],\n  Kotaro Ono [aut],\n  Cole C. Monnahan [aut],\n  Christine C. Stawitz [aut],\n  Z. Teresa A'mar [aut],\n  Athol R. Whitten [aut],\n  Kelli F. Johnson [aut],\n  Robbie L. Emmet [aut],\n  Sean C. Anderson [aut],\n  Gwladys I. Lambert [aut],\n  Megan M. Stachura [aut],\n  Andrew B. Cooper [aut],\n  Andi Stephens [aut],\n  Neil L. Klaer [aut],\n  Carey R. McGilliard [aut],\n  Iago Mosqueira [aut],\n  Watal M. Iwasaki [aut],\n  Kathryn L. Doering [aut],\n  Andrea M. Havron [aut],\n  Nathan Vaughan [aut],\n  LaTreese S. Denson [aut],\n  Ashleigh J. Novak [aut],\n  Henning Winker [aut],\n  Lee Qi [aut],\n  Megumi Oshima [aut],\n  Eric Fletcher [aut]",
    "url": "https://github.com/r4ss/r4ss",
    "bug_reports": "https://github.com/r4ss/r4ss/issues",
    "repository": "https://cran.r-project.org/package=r4ss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r4ss R Code for Stock Synthesis A collection of R functions for use with Stock Synthesis, a\n    fisheries stock assessment modeling platform written in ADMB by Dr. Richard\n    D. Methot at the NOAA Northwest Fisheries Science Center. The functions\n    include tools for summarizing and plotting results, manipulating files,\n    visualizing model parameterizations, and various other common stock\n    assessment tasks.\n    This version of '{r4ss}' is compatible with Stock Synthesis versions\n    3.24 through 3.30 (specifically version 3.30.19.01, from April\n    2022).  "
  },
  {
    "id": 18836,
    "package_name": "rCoinbase",
    "title": "'Coinbase Advance Trade API Interface'",
    "description": "The 'Coinbase Advanced Trade API' <https://docs.cdp.coinbase.com/api-reference/advanced-trade-api/rest-api/introduction> lets you manage orders, portfolios, products, and fees with the new v3 endpoints.",
    "version": "1.0.0",
    "maintainer": "Jason Guevara <Jason.guevara.yt@gmail.com>",
    "author": "Jason Guevara [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rCoinbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rCoinbase 'Coinbase Advance Trade API Interface' The 'Coinbase Advanced Trade API' <https://docs.cdp.coinbase.com/api-reference/advanced-trade-api/rest-api/introduction> lets you manage orders, portfolios, products, and fees with the new v3 endpoints.  "
  },
  {
    "id": 18840,
    "package_name": "rDecode",
    "title": "Descent-Based Calibrated Optimal Direct Estimation",
    "description": "Algorithms for solving a self-calibrated l1-regularized quadratic programming problem without parameter tuning. The algorithm, called DECODE, can handle high-dimensional data without cross-validation. It is found useful in high dimensional portfolio selection (see Pun (2018) <https://ssrn.com/abstract=3179569>) and large precision matrix estimation and sparse linear discriminant analysis (see Pun and Hadimaja (2019) <https://ssrn.com/abstract=3422590>).",
    "version": "0.1.0",
    "maintainer": "Chi Seng Pun <cspun@ntu.edu.sg>",
    "author": "Chi Seng Pun, Matthew Zakharia Hadimaja",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rDecode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rDecode Descent-Based Calibrated Optimal Direct Estimation Algorithms for solving a self-calibrated l1-regularized quadratic programming problem without parameter tuning. The algorithm, called DECODE, can handle high-dimensional data without cross-validation. It is found useful in high dimensional portfolio selection (see Pun (2018) <https://ssrn.com/abstract=3179569>) and large precision matrix estimation and sparse linear discriminant analysis (see Pun and Hadimaja (2019) <https://ssrn.com/abstract=3422590>).  "
  },
  {
    "id": 18853,
    "package_name": "rIACI",
    "title": "Iberian Actuarial Climate Index Calculations",
    "description": "Calculates the Iberian Actuarial Climate Index and its components\u2014including temperature, precipitation, wind power, and sea level data\u2014to support climate change analysis and risk assessment. See \"Zhou et al.\" (2023) <doi:10.26360/2023_3> for further details.",
    "version": "1.0.0",
    "maintainer": "Nan Zhou <zhounan@ucm.es>",
    "author": "Nan Zhou [aut, cre] (ORCID: <https://orcid.org/0009-0002-5166-6753>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rIACI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rIACI Iberian Actuarial Climate Index Calculations Calculates the Iberian Actuarial Climate Index and its components\u2014including temperature, precipitation, wind power, and sea level data\u2014to support climate change analysis and risk assessment. See \"Zhou et al.\" (2023) <doi:10.26360/2023_3> for further details.  "
  },
  {
    "id": 18885,
    "package_name": "rPublic",
    "title": "'Public Trading API'",
    "description": "The 'Public Trading API' <https://public.com/api/docs> allows clients to access their brokerage accounts,  request market data, and place stock/etf/option orders. ",
    "version": "1.0.0",
    "maintainer": "Jason Guevara <Jason.guevara.yt@gmail.com>",
    "author": "Jason Guevara [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rPublic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rPublic 'Public Trading API' The 'Public Trading API' <https://public.com/api/docs> allows clients to access their brokerage accounts,  request market data, and place stock/etf/option orders.   "
  },
  {
    "id": 18934,
    "package_name": "ragtop",
    "title": "Pricing Equity Derivatives with Extensions of Black-Scholes",
    "description": "Algorithms to price American and European\n    equity options, convertible bonds and a\n    variety of other financial derivatives. It uses an\n    extension of the usual Black-Scholes model in which\n    jump to default may occur at a probability specified\n    by a power-law link between stock price and hazard\n    rate as found in the paper by Takahashi, Kobayashi,\n    and Nakagawa (2001) <doi:10.3905/jfi.2001.319302>.  We\n    use ideas and techniques from Andersen and\n    Buffum (2002) <doi:10.2139/ssrn.355308> and\n    Linetsky (2006) <doi:10.1111/j.1467-9965.2006.00271.x>.",
    "version": "1.2.0",
    "maintainer": "Brian K. Boonstra <ragtop@boonstra.org>",
    "author": "Brian K. Boonstra [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ragtop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ragtop Pricing Equity Derivatives with Extensions of Black-Scholes Algorithms to price American and European\n    equity options, convertible bonds and a\n    variety of other financial derivatives. It uses an\n    extension of the usual Black-Scholes model in which\n    jump to default may occur at a probability specified\n    by a power-law link between stock price and hazard\n    rate as found in the paper by Takahashi, Kobayashi,\n    and Nakagawa (2001) <doi:10.3905/jfi.2001.319302>.  We\n    use ideas and techniques from Andersen and\n    Buffum (2002) <doi:10.2139/ssrn.355308> and\n    Linetsky (2006) <doi:10.1111/j.1467-9965.2006.00271.x>.  "
  },
  {
    "id": 18945,
    "package_name": "rameritrade",
    "title": "'TD Ameritrade' API Interface for R",
    "description": "Use R to interface with the 'TD Ameritrade' API <https://developer.tdameritrade.com/>.\n    Functions include authentication, trading, price requests, account information, and option \n    chains. A user will need a TD brokerage account and TD Ameritrade developer app. See README \n    for authentication process and examples.",
    "version": "0.1.5",
    "maintainer": "Anthony Balentine <exploringfinance1@gmail.com>",
    "author": "Anthony Balentine [aut, cre]",
    "url": "https://exploringfinance.github.io/rameritrade/",
    "bug_reports": "https://github.com/exploringfinance/rameritrade/issues",
    "repository": "https://cran.r-project.org/package=rameritrade",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rameritrade 'TD Ameritrade' API Interface for R Use R to interface with the 'TD Ameritrade' API <https://developer.tdameritrade.com/>.\n    Functions include authentication, trading, price requests, account information, and option \n    chains. A user will need a TD brokerage account and TD Ameritrade developer app. See README \n    for authentication process and examples.  "
  },
  {
    "id": 18961,
    "package_name": "randomForestSRC",
    "title": "Fast Unified Random Forests for Survival, Regression, and\nClassification (RF-SRC)",
    "description": "Fast OpenMP parallel computing of Breiman's random forests for univariate, multivariate, unsupervised, survival, competing risks, class imbalanced classification and quantile regression. New Mahalanobis splitting for correlated outcomes.  Extreme random forests and randomized splitting.  Suite of imputation methods for missing data.  Fast random forests using subsampling. Confidence regions and standard errors for variable importance. New improved holdout importance. Case-specific importance.  Minimal depth variable importance. Visualize trees on your Safari or Google Chrome browser. Anonymous random forests for data privacy.",
    "version": "3.4.5",
    "maintainer": "Udaya B. Kogalur <ubk@kogalur.com>",
    "author": "Hemant Ishwaran [aut],\n  Udaya B. Kogalur [aut, cre]",
    "url": "https://www.randomforestsrc.org/ https://ishwaran.org/",
    "bug_reports": "https://github.com/kogalur/randomForestSRC/issues/",
    "repository": "https://cran.r-project.org/package=randomForestSRC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randomForestSRC Fast Unified Random Forests for Survival, Regression, and\nClassification (RF-SRC) Fast OpenMP parallel computing of Breiman's random forests for univariate, multivariate, unsupervised, survival, competing risks, class imbalanced classification and quantile regression. New Mahalanobis splitting for correlated outcomes.  Extreme random forests and randomized splitting.  Suite of imputation methods for missing data.  Fast random forests using subsampling. Confidence regions and standard errors for variable importance. New improved holdout importance. Case-specific importance.  Minimal depth variable importance. Visualize trees on your Safari or Google Chrome browser. Anonymous random forests for data privacy.  "
  },
  {
    "id": 18995,
    "package_name": "rapidoc",
    "title": "Generates 'RapiDoc' Documentation from an 'OpenAPI'\nSpecification",
    "description": "A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'RapiDoc' documentation from an 'OpenAPI' Specification:\n   <https://mrin9.github.io/RapiDoc/>.",
    "version": "9.3.4",
    "maintainer": "Bruno Tremblay <openr@neoxone.com>",
    "author": "Bruno Tremblay [aut, cre],\n  Barret Schloerke [ctb] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\n  Mrinmoy Majumdar [cph]",
    "url": "https://github.com/meztez/rapidoc",
    "bug_reports": "https://github.com/meztez/rapidoc/issues",
    "repository": "https://cran.r-project.org/package=rapidoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rapidoc Generates 'RapiDoc' Documentation from an 'OpenAPI'\nSpecification A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'RapiDoc' documentation from an 'OpenAPI' Specification:\n   <https://mrin9.github.io/RapiDoc/>.  "
  },
  {
    "id": 19006,
    "package_name": "raptools",
    "title": "Risk Assessment Plot and Reclassification Metrics",
    "description": "Assessing the comparative performance of two logistic regression models or results of such models or classification models. Discrimination metrics include Integrated Discrimination Improvement (IDI), Net Reclassification Improvement (NRI), and difference in Area Under the Curves (AUCs), Brier scores and Brier skill. Plots include Risk Assessment Plots, Decision curves and Calibration plots. Methods are described in Pickering and Endre (2012) <doi:10.1373/clinchem.2011.167965> and Pencina et al. (2008) <doi:10.1002/sim.2929>.  ",
    "version": "1.23.0",
    "maintainer": "Daniel Perez Vicencio <dvicencio947@gmail.com>",
    "author": "John W Pickering [aut],\n  Dimitrios Doudesis [aut],\n  Daniel Perez Vicencio [cre]",
    "url": "https://github.com/Researchverse/raptools,\nhttps://researchverse.github.io/raptools/",
    "bug_reports": "https://github.com/Researchverse/raptools/issues",
    "repository": "https://cran.r-project.org/package=raptools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raptools Risk Assessment Plot and Reclassification Metrics Assessing the comparative performance of two logistic regression models or results of such models or classification models. Discrimination metrics include Integrated Discrimination Improvement (IDI), Net Reclassification Improvement (NRI), and difference in Area Under the Curves (AUCs), Brier scores and Brier skill. Plots include Risk Assessment Plots, Decision curves and Calibration plots. Methods are described in Pickering and Endre (2012) <doi:10.1373/clinchem.2011.167965> and Pencina et al. (2008) <doi:10.1002/sim.2929>.    "
  },
  {
    "id": 19009,
    "package_name": "rar",
    "title": "Risk-Adjusted Regression",
    "description": "Perform risk-adjusted regression and sensitivity analysis as\n    developed in \"Mitigating Omitted- and Included-Variable Bias in Estimates of\n    Disparate Impact\" Jung et al. (2024) <arXiv:1809.05651>.",
    "version": "0.0.3",
    "maintainer": "Johann Gaebler <me@jgaeb.com>",
    "author": "Johann Gaebler [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3340-9542>)",
    "url": "https://rar.jgaeb.com, https://github.com/jgaeb/rar",
    "bug_reports": "https://github.com/jgaeb/rar/issues",
    "repository": "https://cran.r-project.org/package=rar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rar Risk-Adjusted Regression Perform risk-adjusted regression and sensitivity analysis as\n    developed in \"Mitigating Omitted- and Included-Variable Bias in Estimates of\n    Disparate Impact\" Jung et al. (2024) <arXiv:1809.05651>.  "
  },
  {
    "id": 19033,
    "package_name": "ratesci",
    "title": "Confidence Intervals and Tests for Comparisons of Binomial\nProportions or Poisson Rates",
    "description": "Computes confidence intervals for binomial or Poisson rates and\n    their differences or ratios. Including the rate (or risk)\n    difference ('RD') or rate ratio (or relative risk, 'RR') for binomial\n    proportions or Poisson rates, and odds ratio ('OR', binomial only).\n    Also confidence intervals for RD, RR or OR for paired binomial data, \n    and estimation of a proportion from clustered binomial data.  \n    Includes skewness-corrected asymptotic score ('SCAS') methods, \n    which have been developed in Laud (2017) <doi:10.1002/pst.1813> \n    from Miettinen and Nurminen (1985) <doi:10.1002/sim.4780040211> \n    and Gart and Nam (1988) <doi:10.2307/2531848>, and in \n    Laud (2025, under review) for paired proportions. The same score produces \n    hypothesis tests that are improved versions of the non-inferiority test \n    for binomial RD and RR by Farrington and Manning (1990) \n    <doi:10.1002/sim.4780091208>, or a generalisation of the McNemar test for \n    paired data. The package also includes MOVER methods (Method Of Variance\n    Estimates Recovery) for all contrasts, derived from the Newcombe\n    method but with options to use equal-tailed intervals in place of the\n    Wilson score method, and generalised for Bayesian applications\n    incorporating prior information. So-called 'exact' methods for\n    strictly conservative coverage are approximated using continuity\n    adjustments, and the amount of adjustment can be selected to avoid\n    over-conservative coverage.  Also includes methods for stratified\n    calculations (e.g. meta-analysis), either with fixed effect assumption\n    (matching the CMH test) or incorporating stratum heterogeneity.",
    "version": "1.0.0",
    "maintainer": "Pete Laud <p.j.laud@sheffield.ac.uk>",
    "author": "Pete Laud [aut, cre] (ORCID: <https://orcid.org/0000-0002-3766-7090>)",
    "url": "https://github.com/petelaud/ratesci,\nhttps://petelaud.github.io/ratesci/",
    "bug_reports": "https://github.com/petelaud/ratesci/issues",
    "repository": "https://cran.r-project.org/package=ratesci",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ratesci Confidence Intervals and Tests for Comparisons of Binomial\nProportions or Poisson Rates Computes confidence intervals for binomial or Poisson rates and\n    their differences or ratios. Including the rate (or risk)\n    difference ('RD') or rate ratio (or relative risk, 'RR') for binomial\n    proportions or Poisson rates, and odds ratio ('OR', binomial only).\n    Also confidence intervals for RD, RR or OR for paired binomial data, \n    and estimation of a proportion from clustered binomial data.  \n    Includes skewness-corrected asymptotic score ('SCAS') methods, \n    which have been developed in Laud (2017) <doi:10.1002/pst.1813> \n    from Miettinen and Nurminen (1985) <doi:10.1002/sim.4780040211> \n    and Gart and Nam (1988) <doi:10.2307/2531848>, and in \n    Laud (2025, under review) for paired proportions. The same score produces \n    hypothesis tests that are improved versions of the non-inferiority test \n    for binomial RD and RR by Farrington and Manning (1990) \n    <doi:10.1002/sim.4780091208>, or a generalisation of the McNemar test for \n    paired data. The package also includes MOVER methods (Method Of Variance\n    Estimates Recovery) for all contrasts, derived from the Newcombe\n    method but with options to use equal-tailed intervals in place of the\n    Wilson score method, and generalised for Bayesian applications\n    incorporating prior information. So-called 'exact' methods for\n    strictly conservative coverage are approximated using continuity\n    adjustments, and the amount of adjustment can be selected to avoid\n    over-conservative coverage.  Also includes methods for stratified\n    calculations (e.g. meta-analysis), either with fixed effect assumption\n    (matching the CMH test) or incorporating stratum heterogeneity.  "
  },
  {
    "id": 19044,
    "package_name": "rawKS",
    "title": "Easily Get True-Positive Rate and False-Positive Rate and KS\nStatistic",
    "description": "The Kolmogorov-Smirnov (K-S) statistic is a standard method to measure the model strength for \n    credit risk scoring models. This package calculates the K\u2013S statistic and \n    plots the true-positive rate and false-positive rate to measure the model strength. \n    This package was written with the credit marketer, who uses risk models in \n    conjunction with his campaigns. The users could read more details from \n    Thrasher (1992) <doi:10.1002/dir.4000060408> and 'pyks' <https://pypi.org/project/pyks/>.",
    "version": "0.1.0",
    "maintainer": "Jiaxiang Li <alex.lijiaxiang@foxmail.com>",
    "author": "Jiaxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0003-3196-6492>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rawKS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rawKS Easily Get True-Positive Rate and False-Positive Rate and KS\nStatistic The Kolmogorov-Smirnov (K-S) statistic is a standard method to measure the model strength for \n    credit risk scoring models. This package calculates the K\u2013S statistic and \n    plots the true-positive rate and false-positive rate to measure the model strength. \n    This package was written with the credit marketer, who uses risk models in \n    conjunction with his campaigns. The users could read more details from \n    Thrasher (1992) <doi:10.1002/dir.4000060408> and 'pyks' <https://pypi.org/project/pyks/>.  "
  },
  {
    "id": 19056,
    "package_name": "rbcb",
    "title": "R Interface to Brazilian Central Bank Web Services",
    "description": "The Brazilian Central Bank API delivers many datasets which regard economic\n  activity, regional economy, international economy, public finances, credit\n  indicators and many more. For more information please see <http://dadosabertos.bcb.gov.br/>.\n  These datasets can be accessed through 'rbcb' functions and can be obtained in\n  different data structures common to R ('tibble', 'data.frame', 'xts', ...).",
    "version": "0.1.14",
    "maintainer": "Wilson Freitas <wilson.freitas@gmail.com>",
    "author": "Wilson Freitas <wilson.freitas@gmail.com>",
    "url": "https://github.com/wilsonfreitas/rbcb,\nhttps://wilsonfreitas.github.io/rbcb/",
    "bug_reports": "https://github.com/wilsonfreitas/rbcb/issues",
    "repository": "https://cran.r-project.org/package=rbcb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbcb R Interface to Brazilian Central Bank Web Services The Brazilian Central Bank API delivers many datasets which regard economic\n  activity, regional economy, international economy, public finances, credit\n  indicators and many more. For more information please see <http://dadosabertos.bcb.gov.br/>.\n  These datasets can be accessed through 'rbcb' functions and can be obtained in\n  different data structures common to R ('tibble', 'data.frame', 'xts', ...).  "
  },
  {
    "id": 19057,
    "package_name": "rbcc",
    "title": "Risk-Based Control Charts",
    "description": "Univariate and multivariate versions of risk-based control charts. Univariate versions of control charts, such as the risk-based version of X-bar, Moving Average (MA), Exponentially Weighted Moving Average Control Charts (EWMA), and Cumulative Sum Control Charts (CUSUM) charts. The risk-based version of the multivariate T2 control chart. Plot and summary functions. Kosztyan et. al. (2016) <doi:10.1016/j.eswa.2016.06.019>.",
    "version": "0.1.5",
    "maintainer": "Zsolt Tibor Kosztyan <kosztyan.zsolt@gtk.uni-pannon.hu>",
    "author": "Aamir Saghir [aut],\n  Attila Imre Katona [aut],\n  Zsolt Tibor Kosztyan [aut, cre]",
    "url": "https://github.com/kzst/rbcc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rbcc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbcc Risk-Based Control Charts Univariate and multivariate versions of risk-based control charts. Univariate versions of control charts, such as the risk-based version of X-bar, Moving Average (MA), Exponentially Weighted Moving Average Control Charts (EWMA), and Cumulative Sum Control Charts (CUSUM) charts. The risk-based version of the multivariate T2 control chart. Plot and summary functions. Kosztyan et. al. (2016) <doi:10.1016/j.eswa.2016.06.019>.  "
  },
  {
    "id": 19074,
    "package_name": "rbranding",
    "title": "Manage Branding and Accessibility of R Projects",
    "description": "A tool for building projects that are visually consistent,\n    accessible, and easy to maintain. It provides functions for managing\n    branding assets, applying organization-wide themes using 'brand.yml',\n    and setting up new projects with accessibility features and correct\n    branding. It supports 'quarto', 'shiny', and 'rmarkdown' projects, and\n    integrates with 'ggplot2'. The accessibility features are based\n    on the Web Content Accessibility Guidelines\n    <https://www.w3.org/WAI/WCAG22/quickref/?versions=2.1>\n    and Accessible Rich Internet Applications (ARIA) specifications\n    <https://www.w3.org/WAI/ARIA/apg/>. The branding framework implements\n    the 'brand.yml' specification <https://posit-dev.github.io/brand-yml/>.",
    "version": "0.1.1",
    "maintainer": "Willy Ray <william.ray@hsc.utah.edu>",
    "author": "Willy Ray [aut, cre],\n  Andrew Pulsipher [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-0773-3210>),\n  Centers for Disease Control and Prevention's Center for Forecasting and\n    Outbreak Analytics [fnd] (Cooperative agreement CDC-RFA-FT-23-0069)",
    "url": "https://epiforesite.github.io/rbranding/,\nhttps://github.com/EpiForeSITE/rbranding",
    "bug_reports": "https://github.com/EpiForeSITE/rbranding/issues",
    "repository": "https://cran.r-project.org/package=rbranding",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbranding Manage Branding and Accessibility of R Projects A tool for building projects that are visually consistent,\n    accessible, and easy to maintain. It provides functions for managing\n    branding assets, applying organization-wide themes using 'brand.yml',\n    and setting up new projects with accessibility features and correct\n    branding. It supports 'quarto', 'shiny', and 'rmarkdown' projects, and\n    integrates with 'ggplot2'. The accessibility features are based\n    on the Web Content Accessibility Guidelines\n    <https://www.w3.org/WAI/WCAG22/quickref/?versions=2.1>\n    and Accessible Rich Internet Applications (ARIA) specifications\n    <https://www.w3.org/WAI/ARIA/apg/>. The branding framework implements\n    the 'brand.yml' specification <https://posit-dev.github.io/brand-yml/>.  "
  },
  {
    "id": 19166,
    "package_name": "reactablefmtr",
    "title": "Streamlined Table Styling and Formatting for Reactable",
    "description": "Provides various features to streamline and enhance the styling of interactive \n    reactable tables with easy-to-use and highly-customizable functions and themes. \n    Apply conditional formatting to cells with data bars, color scales, color tiles,\n    and icon sets. Utilize custom table themes inspired by popular websites such \n    and bootstrap themes. Apply sparkline line & bar charts \n    (note this feature requires the 'dataui' package which can be downloaded from\n    <https://github.com/timelyportfolio/dataui>).\n    Increase the portability and reproducibility of reactable tables by embedding images \n    from the web directly into cells. Save the final table output as a static image or \n    interactive file.",
    "version": "2.0.0",
    "maintainer": "Kyle Cuilla <kyle.cuilla@gmail.com>",
    "author": "Kyle Cuilla [aut, cre, cph],\n  Greg Lin [ctb],\n  June Choe [ctb],\n  Kent Russell [ctb]",
    "url": "https://kcuilla.github.io/reactablefmtr/,\nhttps://github.com/kcuilla/reactablefmtr",
    "bug_reports": "https://github.com/kcuilla/reactablefmtr/issues",
    "repository": "https://cran.r-project.org/package=reactablefmtr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reactablefmtr Streamlined Table Styling and Formatting for Reactable Provides various features to streamline and enhance the styling of interactive \n    reactable tables with easy-to-use and highly-customizable functions and themes. \n    Apply conditional formatting to cells with data bars, color scales, color tiles,\n    and icon sets. Utilize custom table themes inspired by popular websites such \n    and bootstrap themes. Apply sparkline line & bar charts \n    (note this feature requires the 'dataui' package which can be downloaded from\n    <https://github.com/timelyportfolio/dataui>).\n    Increase the portability and reproducibility of reactable tables by embedding images \n    from the web directly into cells. Save the final table output as a static image or \n    interactive file.  "
  },
  {
    "id": 19189,
    "package_name": "readrba",
    "title": "Download and Tidy Data from the Reserve Bank of Australia",
    "description": "Download up-to-date data from the Reserve Bank of Australia \n    in a tidy data frame. Package includes functions to download current and \n    historical statistical tables \n    (<https://www.rba.gov.au/statistics/tables/>) and forecasts \n    (<https://www.rba.gov.au/publications/smp/forecasts-archive.html>). Data\n    includes a broad range of Australian macroeconomic and financial time\n    series.",
    "version": "0.1.12",
    "maintainer": "Matt Cowgill <mattcowgill@gmail.com>",
    "author": "Matt Cowgill [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0422-3300>),\n  Angus Moore [ctb]",
    "url": "https://mattcowgill.github.io/readrba/index.html",
    "bug_reports": "https://github.com/MattCowgill/readrba/issues",
    "repository": "https://cran.r-project.org/package=readrba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "readrba Download and Tidy Data from the Reserve Bank of Australia Download up-to-date data from the Reserve Bank of Australia \n    in a tidy data frame. Package includes functions to download current and \n    historical statistical tables \n    (<https://www.rba.gov.au/statistics/tables/>) and forecasts \n    (<https://www.rba.gov.au/publications/smp/forecasts-archive.html>). Data\n    includes a broad range of Australian macroeconomic and financial time\n    series.  "
  },
  {
    "id": 19240,
    "package_name": "red",
    "title": "IUCN Redlisting Tools",
    "description": "Includes algorithms to facilitate the assessment of extinction risk of species according to the IUCN (International Union for Conservation of Nature, see <https://iucn.org/> for more information) red list criteria.",
    "version": "1.6.3",
    "maintainer": "Vasco V. Branco <vasco.branco@helsinki.fi>",
    "author": "Pedro Cardoso [aut] (ORCID: <https://orcid.org/0000-0001-8119-9960>),\n  Vasco V. Branco [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-7797-3183>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=red",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "red IUCN Redlisting Tools Includes algorithms to facilitate the assessment of extinction risk of species according to the IUCN (International Union for Conservation of Nature, see <https://iucn.org/> for more information) red list criteria.  "
  },
  {
    "id": 19252,
    "package_name": "redoc",
    "title": "Generates 'Redoc' Documentation from an 'OpenAPI' Specification",
    "description": "A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'Redoc' documentation from an 'OpenAPI' Specification:\n   <https://redocly.com/redoc/>.",
    "version": "2.0.0.75",
    "maintainer": "Bruno Tremblay <openr@neoxone.com>",
    "author": "Bruno Tremblay [aut, cre],\n  Barret Schloerke [ctb] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\n  Rebilly [aut, cph]",
    "url": "https://github.com/meztez/redoc",
    "bug_reports": "https://github.com/meztez/redoc/issues",
    "repository": "https://cran.r-project.org/package=redoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redoc Generates 'Redoc' Documentation from an 'OpenAPI' Specification A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'Redoc' documentation from an 'OpenAPI' Specification:\n   <https://redocly.com/redoc/>.  "
  },
  {
    "id": 19307,
    "package_name": "reinsureR",
    "title": "Reinsurance Treaties Application",
    "description": "Application of reinsurance treaties to claims portfolios. \n            The package creates a class Claims whose objective is to \n            store claims and premiums, on which different treaties can be applied.\n            A statistical analysis can then be applied to measure the impact of\n            reinsurance, producing a table or graphical output. This package can\n            be used for estimating the impact of reinsurance on several portfolios\n            or for pricing treaties through statistical analysis. Documentation\n            for the implemented methods can be found in \"Reinsurance: Actuarial\n            and Statistical Aspects\" by Hansj\u00f6erg Albrecher, Jan Beirlant,\n            Jozef L. Teugels (2017, ISBN: 978-0-470-77268-3) and \n            \"REINSURANCE: A Basic Guide to Facultative and Treaty Reinsurance\"\n            by Munich Re (2010) <https://www.munichre.com/site/mram/get/documents_E96160999/mram/assetpool.mr_america/PDFs/3_Publications/reinsurance_basic_guide.pdf>.",
    "version": "0.1.0",
    "maintainer": "Arnaud Buzzi <arnaud.buzzi@sia-partners.com>",
    "author": "Arnaud Buzzi",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reinsureR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reinsureR Reinsurance Treaties Application Application of reinsurance treaties to claims portfolios. \n            The package creates a class Claims whose objective is to \n            store claims and premiums, on which different treaties can be applied.\n            A statistical analysis can then be applied to measure the impact of\n            reinsurance, producing a table or graphical output. This package can\n            be used for estimating the impact of reinsurance on several portfolios\n            or for pricing treaties through statistical analysis. Documentation\n            for the implemented methods can be found in \"Reinsurance: Actuarial\n            and Statistical Aspects\" by Hansj\u00f6erg Albrecher, Jan Beirlant,\n            Jozef L. Teugels (2017, ISBN: 978-0-470-77268-3) and \n            \"REINSURANCE: A Basic Guide to Facultative and Treaty Reinsurance\"\n            by Munich Re (2010) <https://www.munichre.com/site/mram/get/documents_E96160999/mram/assetpool.mr_america/PDFs/3_Publications/reinsurance_basic_guide.pdf>.  "
  },
  {
    "id": 19324,
    "package_name": "rema",
    "title": "Rare Event Meta Analysis",
    "description": "The rema package implements a permutation-based approach for binary \n\tmeta-analyses of 2x2 tables, founded on conditional logistic regression, \n\tthat provides more reliable statistical tests when heterogeneity is \n\tobserved in rare event data (Zabriskie et al. 2021 <doi:10.1002/sim.9142>). \n\tTo adjust for the effect of heterogeneity, this method conditions on the \n\tsufficient statistic of a proxy for the heterogeneity effect as opposed to\n\testimating the heterogeneity variance. While this results in the model not\n\tstrictly falling under the random-effects framework, it is akin to a \n\trandom-effects approach in that it assumes differences in variability due \n\tto treatment. Further, this method does not rely on large-sample \n\tapproximations or continuity corrections for rare event data. This method\n\tuses the permutational distribution of the test statistic instead of\n\tasymptotic approximations for inference. The number of observed events \n\tdrives the computation complexity for creating this permutational \n\tdistribution. Accordingly, for this method to be computationally feasible,\n\tit should only be applied to meta-analyses with a relatively low number of\n\tobserved events. To create this permutational distribution, a network \n\talgorithm, based on the work of Mehta et al. (1992) <doi:10.2307/1390598> \n\tand Corcoran et al. (2001) <doi:10.1111/j.0006-341x.2001.00941.x>, is \n\temployed using C++ and integrated into the package.",
    "version": "0.0.1",
    "maintainer": "Brinley N. Zabriskie <zabriskie@stat.byu.edu>",
    "author": "Brinley N. Zabriskie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2621-7000>),\n  Benjamin Kinard [aut],\n  Chris Sypherd [aut],\n  Ryan Whetten [aut],\n  Madeleine Hays [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rema",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rema Rare Event Meta Analysis The rema package implements a permutation-based approach for binary \n\tmeta-analyses of 2x2 tables, founded on conditional logistic regression, \n\tthat provides more reliable statistical tests when heterogeneity is \n\tobserved in rare event data (Zabriskie et al. 2021 <doi:10.1002/sim.9142>). \n\tTo adjust for the effect of heterogeneity, this method conditions on the \n\tsufficient statistic of a proxy for the heterogeneity effect as opposed to\n\testimating the heterogeneity variance. While this results in the model not\n\tstrictly falling under the random-effects framework, it is akin to a \n\trandom-effects approach in that it assumes differences in variability due \n\tto treatment. Further, this method does not rely on large-sample \n\tapproximations or continuity corrections for rare event data. This method\n\tuses the permutational distribution of the test statistic instead of\n\tasymptotic approximations for inference. The number of observed events \n\tdrives the computation complexity for creating this permutational \n\tdistribution. Accordingly, for this method to be computationally feasible,\n\tit should only be applied to meta-analyses with a relatively low number of\n\tobserved events. To create this permutational distribution, a network \n\talgorithm, based on the work of Mehta et al. (1992) <doi:10.2307/1390598> \n\tand Corcoran et al. (2001) <doi:10.1111/j.0006-341x.2001.00941.x>, is \n\temployed using C++ and integrated into the package.  "
  },
  {
    "id": 19350,
    "package_name": "replicateBE",
    "title": "Average Bioequivalence with Expanding Limits (ABEL)",
    "description": "Performs comparative bioavailability calculations for Average\n    Bioequivalence with Expanding Limits (ABEL). Implemented are 'Method A' /\n    'Method B' and the detection of outliers. If the design allows, assessment\n    of the empiric Type I Error and iteratively adjusting alpha to control the\n    consumer risk. Average Bioequivalence - optionally with a tighter (narrow\n    therapeutic index drugs) or wider acceptance range (South Africa: Cmax) -\n    is implemented as well.",
    "version": "1.1.3",
    "maintainer": "Helmut Sch\u00fctz <helmut.schuetz@bebac.at>",
    "author": "Helmut Sch\u00fctz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1167-7880>),\n  Michael Tomashevskiy [ctb],\n  Detlew Labes [ctb] (ORCID: <https://orcid.org/0000-0003-2169-426X>)",
    "url": "https://github.com/Helmut01/replicateBE",
    "bug_reports": "https://github.com/Helmut01/replicateBE/issues",
    "repository": "https://cran.r-project.org/package=replicateBE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "replicateBE Average Bioequivalence with Expanding Limits (ABEL) Performs comparative bioavailability calculations for Average\n    Bioequivalence with Expanding Limits (ABEL). Implemented are 'Method A' /\n    'Method B' and the detection of outliers. If the design allows, assessment\n    of the empiric Type I Error and iteratively adjusting alpha to control the\n    consumer risk. Average Bioequivalence - optionally with a tighter (narrow\n    therapeutic index drugs) or wider acceptance range (South Africa: Cmax) -\n    is implemented as well.  "
  },
  {
    "id": 19358,
    "package_name": "reportReg",
    "title": "An Easy Way to Report Regression Analysis",
    "description": "Provides an easy way to report the results of regression analysis, including:\n    1. Proportional hazards regression from function 'coxph' of package 'survival';\n    2. Conditional logistic regression from function 'clogit' of package 'survival';\n    3. Ordered logistic regression from function 'polr' of package 'MASS';\n    4. Binary logistic regression from function 'glm' of package 'stats';\n    5. Linear regression from function 'lm' of package 'stats';\n    6. Risk regression model for survival analysis with competing risks from function 'FGR' of package 'riskRegression';\n    7. Multilevel model from function 'lme' of package 'nlme'.",
    "version": "0.3.0",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reportReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reportReg An Easy Way to Report Regression Analysis Provides an easy way to report the results of regression analysis, including:\n    1. Proportional hazards regression from function 'coxph' of package 'survival';\n    2. Conditional logistic regression from function 'clogit' of package 'survival';\n    3. Ordered logistic regression from function 'polr' of package 'MASS';\n    4. Binary logistic regression from function 'glm' of package 'stats';\n    5. Linear regression from function 'lm' of package 'stats';\n    6. Risk regression model for survival analysis with competing risks from function 'FGR' of package 'riskRegression';\n    7. Multilevel model from function 'lme' of package 'nlme'.  "
  },
  {
    "id": 19388,
    "package_name": "reservoirnet",
    "title": "Reservoir Computing and Echo State Networks",
    "description": "A simple user-friendly library based on the 'python' module 'reservoirpy'.\n             It provides a flexible interface to implement efficient Reservoir\n             Computing (RC) architectures with a particular focus on Echo State Networks\n             (ESN). Some of its features are: offline and online training, parallel implementation, \n             sparse matrix computation, fast spectral initialization, advanced learning \n             rules (e.g. Intrinsic Plasticity) etc. It also makes possible to easily create \n             complex architectures with multiple reservoirs (e.g. deep reservoirs), readouts, \n             and complex feedback loops. Moreover, graphical tools are included to easily \n             explore hyperparameters. Finally, it includes several tutorials exploring\n             time series forecasting, classification and hyperparameter tuning. For more information\n             about 'reservoirpy', please see Trouvain et al. (2020) <doi:10.1007/978-3-030-61616-8_40>.\n             This package was developed in the framework of the University of Bordeaux\u2019s IdEx\n             \"Investments for the Future\" program / RRI PHDS.",
    "version": "0.3.0",
    "maintainer": "Thomas Ferte <thomas.ferte@u-bordeaux.fr>",
    "author": "Thomas Ferte [aut, cre, trl],\n  Kalidou Ba [aut, trl],\n  Nathan Trouvain [aut],\n  Rodolphe Thiebaut [aut],\n  Xavier Hinaut [aut],\n  Boris Hejblum [aut, trl]",
    "url": "https://github.com/reservoirpy/reservoirR",
    "bug_reports": "https://github.com/reservoirpy/reservoirR/issues",
    "repository": "https://cran.r-project.org/package=reservoirnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reservoirnet Reservoir Computing and Echo State Networks A simple user-friendly library based on the 'python' module 'reservoirpy'.\n             It provides a flexible interface to implement efficient Reservoir\n             Computing (RC) architectures with a particular focus on Echo State Networks\n             (ESN). Some of its features are: offline and online training, parallel implementation, \n             sparse matrix computation, fast spectral initialization, advanced learning \n             rules (e.g. Intrinsic Plasticity) etc. It also makes possible to easily create \n             complex architectures with multiple reservoirs (e.g. deep reservoirs), readouts, \n             and complex feedback loops. Moreover, graphical tools are included to easily \n             explore hyperparameters. Finally, it includes several tutorials exploring\n             time series forecasting, classification and hyperparameter tuning. For more information\n             about 'reservoirpy', please see Trouvain et al. (2020) <doi:10.1007/978-3-030-61616-8_40>.\n             This package was developed in the framework of the University of Bordeaux\u2019s IdEx\n             \"Investments for the Future\" program / RRI PHDS.  "
  },
  {
    "id": 19389,
    "package_name": "reservr",
    "title": "Fit Distributions and Neural Networks to Censored and Truncated\nData",
    "description": "Define distribution families and fit them to\n    interval-censored and interval-truncated data, where the truncation\n    bounds may depend on the individual observation. The defined\n    distributions feature density, probability, sampling and fitting\n    methods as well as efficient implementations of the log-density log\n    f(x) and log-probability log P(x0 <= X <= x1) for use in 'TensorFlow'\n    neural networks via the 'tensorflow' package. Allows training\n    parametric neural networks on interval-censored and interval-truncated\n    data with flexible parameterization. Applications include Claims\n    Development in Non-Life Insurance, e.g. modelling reporting delay\n    distributions from incomplete data, see B\u00fccher, Rosenstock (2022)\n    <doi:10.1007/s13385-022-00314-4>.",
    "version": "0.0.3",
    "maintainer": "Alexander Rosenstock <alexander.rosenstock@web.de>",
    "author": "Alexander Rosenstock [aut, cre, cph]",
    "url": "https://ashesitr.github.io/reservr/,\nhttps://github.com/AshesITR/reservr",
    "bug_reports": "https://github.com/AshesITR/reservr/issues",
    "repository": "https://cran.r-project.org/package=reservr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reservr Fit Distributions and Neural Networks to Censored and Truncated\nData Define distribution families and fit them to\n    interval-censored and interval-truncated data, where the truncation\n    bounds may depend on the individual observation. The defined\n    distributions feature density, probability, sampling and fitting\n    methods as well as efficient implementations of the log-density log\n    f(x) and log-probability log P(x0 <= X <= x1) for use in 'TensorFlow'\n    neural networks via the 'tensorflow' package. Allows training\n    parametric neural networks on interval-censored and interval-truncated\n    data with flexible parameterization. Applications include Claims\n    Development in Non-Life Insurance, e.g. modelling reporting delay\n    distributions from incomplete data, see B\u00fccher, Rosenstock (2022)\n    <doi:10.1007/s13385-022-00314-4>.  "
  },
  {
    "id": 19407,
    "package_name": "restimizeapi",
    "title": "Functions for Working with the 'www.estimize.com' Web Services",
    "description": "Provides the user with functions to develop their trading strategy,\n    uncover actionable trading ideas, and monitor consensus shifts with\n    crowdsourced earnings and economic estimate data directly from\n    <www.estimize.com>. Further information regarding the web services this\n    package invokes can be found at <www.estimize.com/api>.",
    "version": "1.0.0",
    "maintainer": "Thomas P. Fuller <thomas.fuller@coherentlogic.com>",
    "author": "Thomas P. Fuller <thomas.fuller@coherentlogic.com>",
    "url": "http://www.r-project.org,\nhttp://coherentlogic.com/middleware-development/r-package-for-the-estimize-com-api?source=cran,\nhttps://www.estimize.com?source=cran",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=restimizeapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "restimizeapi Functions for Working with the 'www.estimize.com' Web Services Provides the user with functions to develop their trading strategy,\n    uncover actionable trading ideas, and monitor consensus shifts with\n    crowdsourced earnings and economic estimate data directly from\n    <www.estimize.com>. Further information regarding the web services this\n    package invokes can be found at <www.estimize.com/api>.  "
  },
  {
    "id": 19410,
    "package_name": "result",
    "title": "Result Type for Safely Handling Operations that can Succeed or\nFail",
    "description": "Allows wrapping values in success() and failure() types to capture \n    the result of operations, along with any status codes. Risky expressions can be \n    wrapped in as_result() and functions wrapped in result() to catch errors\n    and assign the relevant result types. Monadic functions can be bound together \n    as pipelines or transaction scripts using then_try(), to gracefully handle \n    errors at any step.",
    "version": "0.1.0",
    "maintainer": "Soumya Ray <soumya.ray@gmail.com>",
    "author": "Soumya Ray [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7497-3281>)",
    "url": "https://github.com/soumyaray/result",
    "bug_reports": "https://github.com/soumyaray/result/issues",
    "repository": "https://cran.r-project.org/package=result",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "result Result Type for Safely Handling Operations that can Succeed or\nFail Allows wrapping values in success() and failure() types to capture \n    the result of operations, along with any status codes. Risky expressions can be \n    wrapped in as_result() and functions wrapped in result() to catch errors\n    and assign the relevant result types. Monadic functions can be bound together \n    as pipelines or transaction scripts using then_try(), to gracefully handle \n    errors at any step.  "
  },
  {
    "id": 19529,
    "package_name": "risk.assessr",
    "title": "Assessing Package Risk Metrics",
    "description": "A reliable and validated tool that captures detailed risk metrics \n    such as R 'CMD' check, test coverage, traceability matrix, documentation, dependencies, \n    reverse dependencies, suggested dependency analysis, repository data, \n    and enhanced reporting for R packages that are local or stored \n    on remote repositories such as GitHub, CRAN, and Bioconductor.",
    "version": "3.0.1",
    "maintainer": "Edward Gillian <edward.gillian-ext@sanofi.com>",
    "author": "Edward Gillian [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2732-5107>),\n  Hugo Bottois [aut] (ORCID: <https://orcid.org/0000-0003-4674-0875>),\n  Paulin Charliquart [aut],\n  Andre Couturier [aut],\n  Sanofi [cph, fnd]",
    "url": "https://sanofi-public.github.io/risk.assessr/",
    "bug_reports": "https://github.com/Sanofi-Public/risk.assessr/issues",
    "repository": "https://cran.r-project.org/package=risk.assessr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "risk.assessr Assessing Package Risk Metrics A reliable and validated tool that captures detailed risk metrics \n    such as R 'CMD' check, test coverage, traceability matrix, documentation, dependencies, \n    reverse dependencies, suggested dependency analysis, repository data, \n    and enhanced reporting for R packages that are local or stored \n    on remote repositories such as GitHub, CRAN, and Bioconductor.  "
  },
  {
    "id": 19531,
    "package_name": "riskParityPortfolio",
    "title": "Design of Risk Parity Portfolios",
    "description": "Fast design of risk parity portfolios for financial investment.\n    The goal of the risk parity portfolio formulation is to equalize or distribute\n    the risk contributions of the different assets, which is missing if we simply\n    consider the overall volatility of the portfolio as in the mean-variance\n    Markowitz portfolio. In addition to the vanilla formulation, where the risk\n    contributions are perfectly equalized subject to no shortselling and budget\n    constraints, many other formulations are considered that allow for box\n    constraints and shortselling, as well as the inclusion of additional\n    objectives like the expected return and overall variance. See vignette for\n    a detailed documentation and comparison, with several illustrative examples.\n    The package is based on the papers:\n    Y. Feng, and D. P. Palomar (2015). SCRIP: Successive Convex Optimization Methods\n    for Risk Parity Portfolio Design. IEEE Trans. on Signal Processing, vol. 63,\n    no. 19, pp. 5285-5300. <doi:10.1109/TSP.2015.2452219>.\n    F. Spinu (2013), An Algorithm for Computing Risk Parity Weights.\n    <doi:10.2139/ssrn.2297383>.\n    T. Griveau-Billion, J. Richard, and T. Roncalli (2013). A fast algorithm for computing\n    High-dimensional risk parity portfolios. <arXiv:1311.4057>.",
    "version": "0.2.2",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Ze Vinicius [aut],\n  Daniel P. Palomar [cre, aut]",
    "url": "https://CRAN.R-project.org/package=riskParityPortfolio,\nhttps://github.com/dppalomar/riskParityPortfolio,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2015.2452219",
    "bug_reports": "https://github.com/dppalomar/riskParityPortfolio/issues",
    "repository": "https://cran.r-project.org/package=riskParityPortfolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskParityPortfolio Design of Risk Parity Portfolios Fast design of risk parity portfolios for financial investment.\n    The goal of the risk parity portfolio formulation is to equalize or distribute\n    the risk contributions of the different assets, which is missing if we simply\n    consider the overall volatility of the portfolio as in the mean-variance\n    Markowitz portfolio. In addition to the vanilla formulation, where the risk\n    contributions are perfectly equalized subject to no shortselling and budget\n    constraints, many other formulations are considered that allow for box\n    constraints and shortselling, as well as the inclusion of additional\n    objectives like the expected return and overall variance. See vignette for\n    a detailed documentation and comparison, with several illustrative examples.\n    The package is based on the papers:\n    Y. Feng, and D. P. Palomar (2015). SCRIP: Successive Convex Optimization Methods\n    for Risk Parity Portfolio Design. IEEE Trans. on Signal Processing, vol. 63,\n    no. 19, pp. 5285-5300. <doi:10.1109/TSP.2015.2452219>.\n    F. Spinu (2013), An Algorithm for Computing Risk Parity Weights.\n    <doi:10.2139/ssrn.2297383>.\n    T. Griveau-Billion, J. Richard, and T. Roncalli (2013). A fast algorithm for computing\n    High-dimensional risk parity portfolios. <arXiv:1311.4057>.  "
  },
  {
    "id": 19532,
    "package_name": "riskPredictClustData",
    "title": "Assessing Risk Predictions for Clustered Data",
    "description": "Assessing and comparing risk prediction rules for clustered data. The method is based on the paper: Rosner B, Qiu W, and Lee MLT.(2013) <doi: 10.1007/s10985-012-9240-6>.",
    "version": "0.2.6",
    "maintainer": "Weiliang Qiu <Weiliang.Qiu@gmail.com>",
    "author": "Bernard Rosner [aut, ctb],\n  Weiliang Qiu [aut, cre],\n  Meiling T. Lee [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=riskPredictClustData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskPredictClustData Assessing Risk Predictions for Clustered Data Assessing and comparing risk prediction rules for clustered data. The method is based on the paper: Rosner B, Qiu W, and Lee MLT.(2013) <doi: 10.1007/s10985-012-9240-6>.  "
  },
  {
    "id": 19533,
    "package_name": "riskRegression",
    "title": "Risk Regression Models and Prediction Scores for Survival\nAnalysis with Competing Risks",
    "description": "Implementation of the following methods for event history analysis.\n    Risk regression models for survival endpoints also in the presence of competing\n    risks are fitted using binomial regression based on a time sequence of binary\n    event status variables. A formula interface for the Fine-Gray regression model\n    and an interface for the combination of cause-specific Cox regression models.\n    A toolbox for assessing and comparing performance of risk predictions (risk\n    markers and risk prediction models). Prediction performance is measured by the\n    Brier score and the area under the ROC curve for binary possibly time-dependent\n    outcome. Inverse probability of censoring weighting and pseudo values are used\n    to deal with right censored data. Lists of risk markers and lists of risk models\n    are assessed simultaneously. Cross-validation repeatedly splits the data, trains\n    the risk prediction models on one part of each split and then summarizes and\n    compares the performance across splits.",
    "version": "2025.09.17",
    "maintainer": "Thomas Alexander Gerds <tag@biostat.ku.dk>",
    "author": "Thomas Alexander Gerds [aut, cre],\n  Johan Sebastian Ohlendorff [aut],\n  Paul Blanche [ctb],\n  Rikke Mortensen [ctb],\n  Marvin Wright [ctb],\n  Nikolaj Tollenaar [ctb],\n  John Muschelli [ctb],\n  Ulla Brasch Mogensen [ctb],\n  Asbj\u00f8rn Risom [ctb],\n  Brice Ozenne [aut]",
    "url": "https://github.com/tagteam/riskRegression",
    "bug_reports": "https://github.com/tagteam/riskRegression/issues",
    "repository": "https://cran.r-project.org/package=riskRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskRegression Risk Regression Models and Prediction Scores for Survival\nAnalysis with Competing Risks Implementation of the following methods for event history analysis.\n    Risk regression models for survival endpoints also in the presence of competing\n    risks are fitted using binomial regression based on a time sequence of binary\n    event status variables. A formula interface for the Fine-Gray regression model\n    and an interface for the combination of cause-specific Cox regression models.\n    A toolbox for assessing and comparing performance of risk predictions (risk\n    markers and risk prediction models). Prediction performance is measured by the\n    Brier score and the area under the ROC curve for binary possibly time-dependent\n    outcome. Inverse probability of censoring weighting and pseudo values are used\n    to deal with right censored data. Lists of risk markers and lists of risk models\n    are assessed simultaneously. Cross-validation repeatedly splits the data, trains\n    the risk prediction models on one part of each split and then summarizes and\n    compares the performance across splits.  "
  },
  {
    "id": 19534,
    "package_name": "riskSimul",
    "title": "Risk Quantification for Stock Portfolios under the T-Copula\nModel",
    "description": "Implements efficient simulation procedures to estimate tail loss probabilities and conditional excess for a stock portfolio. The log-returns are assumed to follow a t-copula model with generalized hyperbolic or t marginals. ",
    "version": "0.1.2",
    "maintainer": "Wolfgang Hormann <hormanngw@yahoo.com>",
    "author": "Wolfgang Hormann [aut, cre],\n  Ismail Basoglu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=riskSimul",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskSimul Risk Quantification for Stock Portfolios under the T-Copula\nModel Implements efficient simulation procedures to estimate tail loss probabilities and conditional excess for a stock portfolio. The log-returns are assumed to follow a t-copula model with generalized hyperbolic or t marginals.   "
  },
  {
    "id": 19535,
    "package_name": "riskclustr",
    "title": "Functions to Study Etiologic Heterogeneity",
    "description": "A collection of functions related to the study of etiologic heterogeneity both across disease subtypes and across individual disease markers. The included functions allow one to quantify the extent of etiologic heterogeneity in the context of a case-control study, and provide p-values to test for etiologic heterogeneity across individual risk factors. Begg CB, Zabor EC, Bernstein JL, Bernstein L, Press MF, Seshan VE (2013) <doi:10.1002/sim.5902>.",
    "version": "0.4.1",
    "maintainer": "Emily C. Zabor <zabore2@ccf.org>",
    "author": "Emily C. Zabor [aut, cre]",
    "url": "https://www.emilyzabor.com/riskclustr/,\nhttps://github.com/zabore/riskclustr",
    "bug_reports": "https://github.com/zabore/riskclustr/issues",
    "repository": "https://cran.r-project.org/package=riskclustr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskclustr Functions to Study Etiologic Heterogeneity A collection of functions related to the study of etiologic heterogeneity both across disease subtypes and across individual disease markers. The included functions allow one to quantify the extent of etiologic heterogeneity in the context of a case-control study, and provide p-values to test for etiologic heterogeneity across individual risk factors. Begg CB, Zabor EC, Bernstein JL, Bernstein L, Press MF, Seshan VE (2013) <doi:10.1002/sim.5902>.  "
  },
  {
    "id": 19536,
    "package_name": "riskdiff",
    "title": "Risk Difference Estimation with Multiple Link Functions and\nInverse Probability of Treatment Weighting",
    "description": "Calculates risk differences (or prevalence differences for \n    cross-sectional data) using generalized linear models with automatic \n    link function selection. Provides robust model fitting with fallback \n    methods, support for stratification and adjustment variables, inverse\n    probability of treatment weighting (IPTW) for causal inference, and \n    publication-ready output formatting. Handles model convergence issues\n    gracefully and provides confidence intervals using multiple approaches.\n    Methods are based on approaches described in Mark W. Donoghoe and \n    Ian C. Marschner (2018) \"logbin: An R Package for Relative Risk \n    Regression Using the Log-Binomial Model\" <doi:10.18637/jss.v086.i09> \n    for robust GLM fitting, Peter C. Austin (2011) \"An Introduction to \n    Propensity Score Methods for Reducing the Effects of Confounding in \n    Observational Studies\" <doi:10.1080/00273171.2011.568786> for IPTW methods,\n    and standard epidemiological methods for risk difference estimation as \n    described in Kenneth J. Rothman, Sander Greenland and Timothy L. Lash \n    (2008, ISBN:9780781755641) \"Modern Epidemiology\".",
    "version": "0.2.1",
    "maintainer": "John D. Murphy <jackmurphy2351@gmail.com>",
    "author": "John D. Murphy [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7714-9976>, MPH, PhD)",
    "url": "https://github.com/jackmurphy2351/riskdiff",
    "bug_reports": "https://github.com/jackmurphy2351/riskdiff/issues",
    "repository": "https://cran.r-project.org/package=riskdiff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskdiff Risk Difference Estimation with Multiple Link Functions and\nInverse Probability of Treatment Weighting Calculates risk differences (or prevalence differences for \n    cross-sectional data) using generalized linear models with automatic \n    link function selection. Provides robust model fitting with fallback \n    methods, support for stratification and adjustment variables, inverse\n    probability of treatment weighting (IPTW) for causal inference, and \n    publication-ready output formatting. Handles model convergence issues\n    gracefully and provides confidence intervals using multiple approaches.\n    Methods are based on approaches described in Mark W. Donoghoe and \n    Ian C. Marschner (2018) \"logbin: An R Package for Relative Risk \n    Regression Using the Log-Binomial Model\" <doi:10.18637/jss.v086.i09> \n    for robust GLM fitting, Peter C. Austin (2011) \"An Introduction to \n    Propensity Score Methods for Reducing the Effects of Confounding in \n    Observational Studies\" <doi:10.1080/00273171.2011.568786> for IPTW methods,\n    and standard epidemiological methods for risk difference estimation as \n    described in Kenneth J. Rothman, Sander Greenland and Timothy L. Lash \n    (2008, ISBN:9780781755641) \"Modern Epidemiology\".  "
  },
  {
    "id": 19537,
    "package_name": "riskmetric",
    "title": "Risk Metrics to Evaluating R Packages",
    "description": "Facilities for assessing R packages against a number of metrics to \n    help quantify their robustness.",
    "version": "0.2.5",
    "maintainer": "Eli Miller <eli.miller@atorusresearch.com>",
    "author": "R Validation Hub [aut],\n  Doug Kelkhoff [aut],\n  Marly Gotti [aut],\n  Eli Miller [cre, aut],\n  Kevin K [aut],\n  Yilong Zhang [aut],\n  Eric Milliman [aut],\n  Juliane Manitz [aut],\n  Mark Padgham [ctb],\n  PSI special interest group Application and Implementation of\n    Methodologies in Statistics [cph]",
    "url": "https://pharmar.github.io/riskmetric/,\nhttps://github.com/pharmaR/riskmetric",
    "bug_reports": "https://github.com/pharmaR/riskmetric/issues",
    "repository": "https://cran.r-project.org/package=riskmetric",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskmetric Risk Metrics to Evaluating R Packages Facilities for assessing R packages against a number of metrics to \n    help quantify their robustness.  "
  },
  {
    "id": 19538,
    "package_name": "risks",
    "title": "Estimate Risk Ratios and Risk Differences using Regression",
    "description": "Risk ratios and risk differences are estimated using regression \n  models that allow for binary, categorical, and continuous exposures and \n  confounders. Implemented are marginal standardization after fitting logistic \n  models (g-computation) with delta-method and bootstrap standard errors, \n  Miettinen's case-duplication approach (Schouten et al. 1993, \n  <doi:10.1002/sim.4780121808>), log-binomial (Poisson) models with empirical \n  variance (Zou 2004, <doi:10.1093/aje/kwh090>), binomial models with starting \n  values from Poisson models (Spiegelman and Hertzmark 2005, \n  <doi:10.1093/aje/kwi188>), and others.",
    "version": "0.4.3",
    "maintainer": "Konrad Stopsack <stopsack@post.harvard.edu>",
    "author": "Konrad Stopsack [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0722-1311>),\n  Travis Gerke [aut] (ORCID: <https://orcid.org/0000-0002-9500-8907>)",
    "url": "https://stopsack.github.io/risks/",
    "bug_reports": "https://github.com/stopsack/risks/issues",
    "repository": "https://cran.r-project.org/package=risks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "risks Estimate Risk Ratios and Risk Differences using Regression Risk ratios and risk differences are estimated using regression \n  models that allow for binary, categorical, and continuous exposures and \n  confounders. Implemented are marginal standardization after fitting logistic \n  models (g-computation) with delta-method and bootstrap standard errors, \n  Miettinen's case-duplication approach (Schouten et al. 1993, \n  <doi:10.1002/sim.4780121808>), log-binomial (Poisson) models with empirical \n  variance (Zou 2004, <doi:10.1093/aje/kwh090>), binomial models with starting \n  values from Poisson models (Spiegelman and Hertzmark 2005, \n  <doi:10.1093/aje/kwi188>), and others.  "
  },
  {
    "id": 19539,
    "package_name": "riskscores",
    "title": "Optimized Integer Risk Score Models",
    "description": "Implements an optimized approach to learning risk score models, where sparsity and integer constraints are integrated into the model-fitting process.",
    "version": "1.2.3",
    "maintainer": "Hannah Eglinton <eglintonh@gmail.com>",
    "author": "Hannah Eglinton [aut, cre],\n  Seehanah Tang [aut, aut],\n  Alice Paul [aut, cph],\n  Oscar Yan [aut],\n  R Core Team [ctb, cph] (Copyright holder of Rinternals.h, R.h, lm.c,\n    Applic.h, statsR.h, glm package),\n  Robert Gentleman [ctb, cph] (Author and copyright holder of\n    Rinternals.h),\n  Ross Ihaka [ctb, cph] (Author and copyright holder of Rinternals.h),\n  Simon Davies [ctb] (Author of glm.fit function (modified in\n    cv_risk_mod.R)),\n  Thomas Lumley [ctb] (Author of glm.fit function (modified in\n    cv_risk_mod.R))",
    "url": "https://github.com/hjeglinton/riskscores",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=riskscores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskscores Optimized Integer Risk Score Models Implements an optimized approach to learning risk score models, where sparsity and integer constraints are integrated into the model-fitting process.  "
  },
  {
    "id": 19540,
    "package_name": "risksetROC",
    "title": "Riskset ROC Curve Estimation from Censored Survival Data",
    "description": "Compute time-dependent Incident/dynamic accuracy measures\n        (ROC curve, AUC, integrated AUC )from censored survival data\n        under proportional or non-proportional hazard assumption of\n        Heagerty & Zheng (Biometrics, Vol 61 No 1, 2005, PP 92-105).",
    "version": "1.0.4.1",
    "maintainer": "Paramita Saha-Chaudhuri\n<paramita.sahachaudhuri.work@gmail.com>",
    "author": "Patrick J. Heagerty <heagerty@u.washington.edu>, packaging by\n        Paramita Saha-Chaudhuri <paramita.sahachaudhuri.work@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=risksetROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "risksetROC Riskset ROC Curve Estimation from Censored Survival Data Compute time-dependent Incident/dynamic accuracy measures\n        (ROC curve, AUC, integrated AUC )from censored survival data\n        under proportional or non-proportional hazard assumption of\n        Heagerty & Zheng (Biometrics, Vol 61 No 1, 2005, PP 92-105).  "
  },
  {
    "id": 19541,
    "package_name": "riskyr",
    "title": "Rendering Risk Literacy more Transparent",
    "description": "Risk-related information (like the prevalence of conditions, the sensitivity and specificity of diagnostic tests, or the effectiveness of interventions or treatments) can be expressed in terms of frequencies or probabilities. By providing a toolbox of corresponding metrics and representations, 'riskyr' computes, translates, and visualizes risk-related information in a variety of ways. Adopting multiple complementary perspectives provides insights into the interplay between key parameters and renders teaching and training programs on risk literacy more transparent (see <doi:10.3389/fpsyg.2020.567817>, for details).  ",
    "version": "0.5.0",
    "maintainer": "Hansjoerg Neth <h.neth@uni.kn>",
    "author": "Hansjoerg Neth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5427-3141>),\n  Felix Gaisbauer [aut] (ORCID: <https://orcid.org/0000-0002-1285-1246>),\n  Nico Gradwohl [aut] (ORCID: <https://orcid.org/0000-0002-8703-905X>),\n  Wolfgang Gaissmaier [aut] (ORCID:\n    <https://orcid.org/0000-0001-6273-178X>)",
    "url": "https://riskyr.org/, https://CRAN.R-project.org/package=riskyr,\nhttps://github.com/hneth/riskyr/,\nhttps://hneth.github.io/riskyr/",
    "bug_reports": "https://github.com/hneth/riskyr/issues/",
    "repository": "https://cran.r-project.org/package=riskyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskyr Rendering Risk Literacy more Transparent Risk-related information (like the prevalence of conditions, the sensitivity and specificity of diagnostic tests, or the effectiveness of interventions or treatments) can be expressed in terms of frequencies or probabilities. By providing a toolbox of corresponding metrics and representations, 'riskyr' computes, translates, and visualizes risk-related information in a variety of ways. Adopting multiple complementary perspectives provides insights into the interplay between key parameters and renders teaching and training programs on risk literacy more transparent (see <doi:10.3389/fpsyg.2020.567817>, for details).    "
  },
  {
    "id": 19552,
    "package_name": "rjdmarkdown",
    "title": "'rmarkdown' Extension for Formatted 'RJDemetra' Outputs",
    "description": "Functions to have nice 'rmarkdown' outputs of the \n  seasonal and trading day adjustment models made with 'RJDemetra'.",
    "version": "0.2.2",
    "maintainer": "Alain Quartier-la-Tente <alain.quartier@yahoo.fr>",
    "author": "Alain Quartier-la-Tente [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7890-3857>)",
    "url": "https://github.com/AQLT/rjdmarkdown",
    "bug_reports": "https://github.com/AQLT/rjdmarkdown/issues",
    "repository": "https://cran.r-project.org/package=rjdmarkdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjdmarkdown 'rmarkdown' Extension for Formatted 'RJDemetra' Outputs Functions to have nice 'rmarkdown' outputs of the \n  seasonal and trading day adjustment models made with 'RJDemetra'.  "
  },
  {
    "id": 19600,
    "package_name": "rmda",
    "title": "Risk Model Decision Analysis",
    "description": "Provides tools to evaluate the value of using a risk prediction instrument to decide treatment or intervention (versus no treatment or intervention).  Given one or more risk prediction instruments (risk models) that estimate the probability of a binary outcome, rmda provides functions to estimate and display decision curves and other figures that help assess the population impact of using a risk model for clinical decision making.   Here, \"population\" refers to the relevant patient population. Decision curves display estimates of the (standardized) net benefit over a range of probability thresholds used to categorize observations as 'high risk'. The curves help evaluate a treatment policy that recommends treatment for patients who are estimated to be 'high risk' by comparing the population impact of a risk-based policy to \"treat all\" and \"treat none\" intervention policies.  Curves can be estimated using data from a prospective cohort.  In addition, rmda can estimate decision curves using data from a case-control study if an estimate of the population outcome prevalence is available.  Version 1.4 of the package provides an alternative framing of the decision problem for situations where treatment is the standard-of-care and a risk model might be used to recommend that low-risk patients (i.e., patients below some risk threshold) opt out of treatment. Confidence intervals calculated using the bootstrap can be computed and displayed. A wrapper function to calculate cross-validated curves using k-fold cross-validation is also provided. ",
    "version": "1.6",
    "maintainer": "Marshall Brown <mdbrown@fredhutch.org>",
    "author": "Marshall Brown",
    "url": "http://mdbrown.github.io/rmda/, https://github.com/mdbrown/rmda",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rmda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmda Risk Model Decision Analysis Provides tools to evaluate the value of using a risk prediction instrument to decide treatment or intervention (versus no treatment or intervention).  Given one or more risk prediction instruments (risk models) that estimate the probability of a binary outcome, rmda provides functions to estimate and display decision curves and other figures that help assess the population impact of using a risk model for clinical decision making.   Here, \"population\" refers to the relevant patient population. Decision curves display estimates of the (standardized) net benefit over a range of probability thresholds used to categorize observations as 'high risk'. The curves help evaluate a treatment policy that recommends treatment for patients who are estimated to be 'high risk' by comparing the population impact of a risk-based policy to \"treat all\" and \"treat none\" intervention policies.  Curves can be estimated using data from a prospective cohort.  In addition, rmda can estimate decision curves using data from a case-control study if an estimate of the population outcome prevalence is available.  Version 1.4 of the package provides an alternative framing of the decision problem for situations where treatment is the standard-of-care and a risk model might be used to recommend that low-risk patients (i.e., patients below some risk threshold) opt out of treatment. Confidence intervals calculated using the bootstrap can be computed and displayed. A wrapper function to calculate cross-validated curves using k-fold cross-validation is also provided.   "
  },
  {
    "id": 19701,
    "package_name": "robvis",
    "title": "Visualize the Results of Risk-of-Bias (ROB) Assessments",
    "description": "Helps users in quickly visualizing risk-of-bias \n    assessments performed as part of a systematic review. It allows users to \n    create weighted bar-plots of the distribution of risk-of-bias judgments \n    within each bias domain, in addition to traffic-light plots of the \n    specific domain-level judgments for each study. The resulting figures are \n    of publication quality and are formatted according the risk-of-bias \n    assessment tool use to perform the assessments. Currently, the supported \n    tools are ROB2.0 (for randomized controlled trials; Sterne et al (2019)  \n    <doi:10.1136/bmj.l4898>), ROBINS-I (for non-randomised studies of \n    interventions; Sterne et al (2016) <doi:10.1136/bmj.i4919>), and QUADAS-2 \n    (for diagnostic accuracy studies; Whiting et al (2011) \n    <doi:10.7326/0003-4819-155-8-201110180-00009>).",
    "version": "0.3.0",
    "maintainer": "Luke McGuinness <luke.mcguinness@bristol.ac.uk>",
    "author": "Luke McGuinness [aut, cre],\n  Emily Kothe [ctb]",
    "url": "https://github.com/mcguinlu/robvis",
    "bug_reports": "https://github.com/mcguinlu/robvis",
    "repository": "https://cran.r-project.org/package=robvis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robvis Visualize the Results of Risk-of-Bias (ROB) Assessments Helps users in quickly visualizing risk-of-bias \n    assessments performed as part of a systematic review. It allows users to \n    create weighted bar-plots of the distribution of risk-of-bias judgments \n    within each bias domain, in addition to traffic-light plots of the \n    specific domain-level judgments for each study. The resulting figures are \n    of publication quality and are formatted according the risk-of-bias \n    assessment tool use to perform the assessments. Currently, the supported \n    tools are ROB2.0 (for randomized controlled trials; Sterne et al (2019)  \n    <doi:10.1136/bmj.l4898>), ROBINS-I (for non-randomised studies of \n    interventions; Sterne et al (2016) <doi:10.1136/bmj.i4919>), and QUADAS-2 \n    (for diagnostic accuracy studies; Whiting et al (2011) \n    <doi:10.7326/0003-4819-155-8-201110180-00009>).  "
  },
  {
    "id": 19790,
    "package_name": "rpmodel",
    "title": "P-Model",
    "description": "Implements the P-model \n  (Stocker et al., 2020 <doi:10.5194/gmd-13-1545-2020>),\n  predicting acclimated parameters of the enzyme kinetics of C3 photosynthesis,\n  assimilation, and dark respiration rates as a function of the environment\n  (temperature, CO2, vapour pressure deficit, light, atmospheric pressure).",
    "version": "1.2.3",
    "maintainer": "Benjamin Stocker <benjamin.stocker@gmail.com>",
    "author": "Benjamin Stocker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2697-9096>),\n  Koen Hufkens [ctb] (ORCID: <https://orcid.org/0000-0002-5070-8109>)",
    "url": "https://github.com/geco-bern/rpmodel",
    "bug_reports": "https://github.com/geco-bern/rpmodel/issues",
    "repository": "https://cran.r-project.org/package=rpmodel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpmodel P-Model Implements the P-model \n  (Stocker et al., 2020 <doi:10.5194/gmd-13-1545-2020>),\n  predicting acclimated parameters of the enzyme kinetics of C3 photosynthesis,\n  assimilation, and dark respiration rates as a function of the environment\n  (temperature, CO2, vapour pressure deficit, light, atmospheric pressure).  "
  },
  {
    "id": 19793,
    "package_name": "rportfolio",
    "title": "Portfolio Theory",
    "description": "Collection of tools to calculate portfolio performance metrics. Portfolio performance is a key measure for investors. These metrics are important to analyse how effectively their money has been invested. This package uses portfolio theories to give investor tools to evaluate their portfolio performance. For more information see,  Markowitz, H.M. (1952), <doi:10.2307/2975974>. Analysis of Investments & Management of Portfolios [2012, ISBN:978-8131518748]. ",
    "version": "0.0.3",
    "maintainer": "Anurag Agrawal <agrawalanurag1999@gmail.com>",
    "author": "Anurag Agrawal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2272-8273>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rportfolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rportfolio Portfolio Theory Collection of tools to calculate portfolio performance metrics. Portfolio performance is a key measure for investors. These metrics are important to analyse how effectively their money has been invested. This package uses portfolio theories to give investor tools to evaluate their portfolio performance. For more information see,  Markowitz, H.M. (1952), <doi:10.2307/2975974>. Analysis of Investments & Management of Portfolios [2012, ISBN:978-8131518748].   "
  },
  {
    "id": 19826,
    "package_name": "rriskDistributions",
    "title": "Fitting Distributions to Given Data or Known Quantiles",
    "description": "Collection of functions for fitting distributions to given data or\n    by known quantiles. Two main functions fit.perc() and fit.cont() provide\n    users a GUI that allows to choose a most appropriate distribution without\n    any knowledge of the R syntax. Note, this package is a part of the 'rrisk'\n    project.",
    "version": "2.1.2",
    "maintainer": "Matthias Greiner <matthias.greiner@bfr.bund.de>",
    "author": "Natalia Belgorodski [aut] (STAT-UP Statistical Consulting),\n  Matthias Greiner [aut, cre] (Federal Institute for Risk Assessment,\n    Germany),\n  Kristin Tolksdorf [aut] (Federal Institute for Risk Assessment,\n    Germany),\n  Katharina Schueller [aut] (STAT-UP Statistical Consulting),\n  Matthias Flor [ctb] (Federal Institute for Risk Assessment, Germany),\n  Lutz G\u00f6hring [ctb] (Lutz G\u00f6hring Consulting)",
    "url": "http://www.bfr.bund.de/cd/52158",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rriskDistributions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rriskDistributions Fitting Distributions to Given Data or Known Quantiles Collection of functions for fitting distributions to given data or\n    by known quantiles. Two main functions fit.perc() and fit.cont() provide\n    users a GUI that allows to choose a most appropriate distribution without\n    any knowledge of the R syntax. Note, this package is a part of the 'rrisk'\n    project.  "
  },
  {
    "id": 19837,
    "package_name": "rsatscan",
    "title": "Tools, Classes, and Methods for Interfacing with 'SaTScan'\nStand-Alone Software",
    "description": "'SaTScan'(TM) <https://www.satscan.org> is software for finding regions in \n    Time, Space, or Time-Space that have excess risk, based on scan statistics, and \n\tuses Monte Carlo hypothesis testing to generate P-values for these regions.  The \n\t'rsatscan' package provides functions for writing R data frames in \n\t'SaTScan'-readable formats, for setting 'SaTScan' parameters, for running 'SaTScan' in \n\tthe OS, and for reading the files that 'SaTScan' creates.  ",
    "version": "1.0.9",
    "maintainer": "Scott Hostovich <HostovichS@imsweb.com>",
    "author": "Ken Kleinman [aut],\n  Scott Hostovich [cre],\n  Amer Moosa [ctb]",
    "url": "https://www.satscan.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rsatscan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsatscan Tools, Classes, and Methods for Interfacing with 'SaTScan'\nStand-Alone Software 'SaTScan'(TM) <https://www.satscan.org> is software for finding regions in \n    Time, Space, or Time-Space that have excess risk, based on scan statistics, and \n\tuses Monte Carlo hypothesis testing to generate P-values for these regions.  The \n\t'rsatscan' package provides functions for writing R data frames in \n\t'SaTScan'-readable formats, for setting 'SaTScan' parameters, for running 'SaTScan' in \n\tthe OS, and for reading the files that 'SaTScan' creates.    "
  },
  {
    "id": 19855,
    "package_name": "rsmatch",
    "title": "Matching Methods for Time-Varying Observational Studies",
    "description": "Implements popular methods for matching in time-varying\n    observational studies. Matching is difficult in this scenario because\n    participants can be treated at different times which may have an\n    influence on the outcomes. The core methods include: \"Balanced Risk\n    Set Matching\" from Li, Propert, and Rosenbaum (2011)\n    <doi:10.1198/016214501753208573> and \"Propensity Score Matching with\n    Time-Dependent Covariates\" from Lu (2005)\n    <doi:10.1111/j.1541-0420.2005.00356.x>. Some functions use the\n    'Gurobi' optimization back-end to improve the optimization problem\n    speed; the 'gurobi' R package and associated software can be\n    downloaded from <https://www.gurobi.com> after obtaining a license.",
    "version": "0.2.1",
    "maintainer": "Sean Kent <skent259@gmail.com>",
    "author": "Sean Kent [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8697-9069>),\n  Mitchell Paukner [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-3839-5311>)",
    "url": "https://skent259.github.io/rsmatch/,\nhttps://github.com/skent259/rsmatch",
    "bug_reports": "https://github.com/skent259/rsmatch/issues",
    "repository": "https://cran.r-project.org/package=rsmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsmatch Matching Methods for Time-Varying Observational Studies Implements popular methods for matching in time-varying\n    observational studies. Matching is difficult in this scenario because\n    participants can be treated at different times which may have an\n    influence on the outcomes. The core methods include: \"Balanced Risk\n    Set Matching\" from Li, Propert, and Rosenbaum (2011)\n    <doi:10.1198/016214501753208573> and \"Propensity Score Matching with\n    Time-Dependent Covariates\" from Lu (2005)\n    <doi:10.1111/j.1541-0420.2005.00356.x>. Some functions use the\n    'Gurobi' optimization back-end to improve the optimization problem\n    speed; the 'gurobi' R package and associated software can be\n    downloaded from <https://www.gurobi.com> after obtaining a license.  "
  },
  {
    "id": 19859,
    "package_name": "rsofun",
    "title": "The P-Model and BiomeE Modelling Framework",
    "description": "Implements the Simulating Optimal FUNctioning framework for \n              site-scale simulations of ecosystem processes, including model \n              calibration. It contains 'Fortran 90' modules for the P-model (Stocker\n              et al. (2020) <doi:10.5194/gmd-13-1545-2020>), SPLASH\n              (Davis et al. (2017) <doi:10.5194/gmd-10-689-2017>)\n              and BiomeE (Weng et al. (2015)\n              <doi:10.5194/bg-12-2655-2015>).",
    "version": "5.1.0",
    "maintainer": "Benjamin Stocker <benjamin.stocker@gmail.com>",
    "author": "Benjamin Stocker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2697-9096>),\n  Koen Hufkens [aut] (ORCID: <https://orcid.org/0000-0002-5070-8109>),\n  Josefa Ar\u00e1n Paredes [aut] (ORCID:\n    <https://orcid.org/0009-0006-7176-2311>),\n  Laura Marqu\u00e9s [ctb] (ORCID: <https://orcid.org/0000-0002-3593-5557>),\n  Mayeul Marcadella [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8555-3808>),\n  Ensheng Weng [ctb] (ORCID: <https://orcid.org/0000-0002-1858-4847>),\n  Fabian Bernhard [aut] (ORCID: <https://orcid.org/0000-0003-0338-0961>),\n  Geocomputation and Earth Observation, University of Bern [cph, fnd]",
    "url": "https://github.com/geco-bern/rsofun",
    "bug_reports": "https://github.com/geco-bern/rsofun/issues",
    "repository": "https://cran.r-project.org/package=rsofun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsofun The P-Model and BiomeE Modelling Framework Implements the Simulating Optimal FUNctioning framework for \n              site-scale simulations of ecosystem processes, including model \n              calibration. It contains 'Fortran 90' modules for the P-model (Stocker\n              et al. (2020) <doi:10.5194/gmd-13-1545-2020>), SPLASH\n              (Davis et al. (2017) <doi:10.5194/gmd-10-689-2017>)\n              and BiomeE (Weng et al. (2015)\n              <doi:10.5194/bg-12-2655-2015>).  "
  },
  {
    "id": 19871,
    "package_name": "rstac",
    "title": "Client Library for SpatioTemporal Asset Catalog",
    "description": "Provides functions to access, search and download spacetime earth\n    observation data via SpatioTemporal Asset Catalog (STAC). This package \n    supports the version 1.0.0 (and older) of the STAC specification\n    (<https://github.com/radiantearth/stac-spec>). \n    For further details see Simoes et al. (2021) <doi:10.1109/IGARSS47720.2021.9553518>.",
    "version": "1.0.1",
    "maintainer": "Felipe Carvalho <lipecaso@gmail.com>",
    "author": "Rolf Simoes [aut],\n  Felipe Carvalho [aut, cre],\n  Brazil Data Cube Team [aut],\n  National Institute for Space Research (INPE) [cph]",
    "url": "https://brazil-data-cube.github.io/rstac/",
    "bug_reports": "https://github.com/brazil-data-cube/rstac/issues",
    "repository": "https://cran.r-project.org/package=rstac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rstac Client Library for SpatioTemporal Asset Catalog Provides functions to access, search and download spacetime earth\n    observation data via SpatioTemporal Asset Catalog (STAC). This package \n    supports the version 1.0.0 (and older) of the STAC specification\n    (<https://github.com/radiantearth/stac-spec>). \n    For further details see Simoes et al. (2021) <doi:10.1109/IGARSS47720.2021.9553518>.  "
  },
  {
    "id": 19884,
    "package_name": "rsurv",
    "title": "Random Generation of Survival Data",
    "description": "Random generation of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package 'rsurv' also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package 'rsurv' lies in the fact that linear predictors are specified via a formula-based approach, facilitating the inclusion of categorical variables and interaction terms. The functions implemented in the package 'rsurv' can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivariate survival data, and competing risks survival data. Details about the R package 'rsurv' can be found in Demarqui (2024) <doi:10.48550/arXiv.2406.01750>.",
    "version": "0.0.2",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "author": "Fabio Demarqui [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9236-1986>)",
    "url": "https://github.com/fndemarqui/rsurv,\nhttps://fndemarqui.github.io/rsurv/",
    "bug_reports": "https://github.com/fndemarqui/rsurv/issues",
    "repository": "https://cran.r-project.org/package=rsurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsurv Random Generation of Survival Data Random generation of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package 'rsurv' also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package 'rsurv' lies in the fact that linear predictors are specified via a formula-based approach, facilitating the inclusion of categorical variables and interaction terms. The functions implemented in the package 'rsurv' can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivariate survival data, and competing risks survival data. Details about the R package 'rsurv' can be found in Demarqui (2024) <doi:10.48550/arXiv.2406.01750>.  "
  },
  {
    "id": 19929,
    "package_name": "rtsdata",
    "title": "R Time Series Intelligent Data Storage",
    "description": "A tool that allows to download and save historical time series data \n\tfor future use offline. The intelligent updating functionality will only download \n\tthe new available information; thus, saving you time and Internet bandwidth. \n\tIt will only re-download the full data-set if any inconsistencies are detected. \n\tThis package supports following data provides: 'Yahoo' (finance.yahoo.com), \n\t'FRED' (fred.stlouisfed.org), 'Quandl' (data.nasdaq.com), \n\t'AlphaVantage' (www.alphavantage.co), 'Tiingo' (www.tiingo.com).",
    "version": "0.1.4",
    "maintainer": "Irina Kapler <irkapler@gmail.com>",
    "author": "RTSVizTeam [aut, cph],\n  Irina Kapler [cre]",
    "url": "https://bitbucket.org/rtsvizteam/rtsdata",
    "bug_reports": "https://bitbucket.org/rtsvizteam/rtsdata/issues",
    "repository": "https://cran.r-project.org/package=rtsdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtsdata R Time Series Intelligent Data Storage A tool that allows to download and save historical time series data \n\tfor future use offline. The intelligent updating functionality will only download \n\tthe new available information; thus, saving you time and Internet bandwidth. \n\tIt will only re-download the full data-set if any inconsistencies are detected. \n\tThis package supports following data provides: 'Yahoo' (finance.yahoo.com), \n\t'FRED' (fred.stlouisfed.org), 'Quandl' (data.nasdaq.com), \n\t'AlphaVantage' (www.alphavantage.co), 'Tiingo' (www.tiingo.com).  "
  },
  {
    "id": 19933,
    "package_name": "rubias",
    "title": "Bayesian Inference from the Conditional Genetic Stock\nIdentification Model",
    "description": "Implements Bayesian inference for the conditional genetic \n    stock identification model.  It allows inference of mixed fisheries and also \n    simulation of mixtures to predict accuracy.  A full description of the underlying\n    methods is available in a recently published article in the\n    Canadian Journal of Fisheries and Aquatic Sciences: <doi:10.1139/cjfas-2018-0016>.",
    "version": "0.3.4",
    "maintainer": "Eric C. Anderson <eric.anderson@noaa.gov>",
    "author": "Eric C. Anderson [aut, cre],\n  Ben Moran [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rubias",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rubias Bayesian Inference from the Conditional Genetic Stock\nIdentification Model Implements Bayesian inference for the conditional genetic \n    stock identification model.  It allows inference of mixed fisheries and also \n    simulation of mixtures to predict accuracy.  A full description of the underlying\n    methods is available in a recently published article in the\n    Canadian Journal of Fisheries and Aquatic Sciences: <doi:10.1139/cjfas-2018-0016>.  "
  },
  {
    "id": 20078,
    "package_name": "samplesizeCMH",
    "title": "Power and Sample Size Calculation for the\nCochran-Mantel-Haenszel Test",
    "description": "\n    Calculates the power and sample size for Cochran-Mantel-Haenszel tests. \n    There are also several helper functions for working with probability,\n    odds, relative risk, and odds ratio values.",
    "version": "0.0.3",
    "maintainer": "Paul Egeler <paulegeler@gmail.com>",
    "author": "Paul Egeler [aut, cre],\n  Spectrum Health, Grand Rapids, MI [cph]",
    "url": "https://github.com/pegeler/samplesizeCMH",
    "bug_reports": "https://github.com/pegeler/samplesizeCMH/issues",
    "repository": "https://cran.r-project.org/package=samplesizeCMH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "samplesizeCMH Power and Sample Size Calculation for the\nCochran-Mantel-Haenszel Test \n    Calculates the power and sample size for Cochran-Mantel-Haenszel tests. \n    There are also several helper functions for working with probability,\n    odds, relative risk, and odds ratio values.  "
  },
  {
    "id": 20079,
    "package_name": "samplesizeestimator",
    "title": "Calculate Sample Size for Various Scenarios",
    "description": "Calculates sample size for various scenarios, such as sample size\n    to estimate population proportion with stated absolute or relative\n    precision, testing a single proportion with a reference value, to estimate\n    the population mean with stated absolute or relative precision, testing\n    single mean with a reference value and sample size for comparing two\n    unpaired or independent means, comparing two paired means, the sample size\n    For case control studies, estimating the odds ratio with stated precision,\n    testing the odds ratio with a reference value, estimating relative risk with\n    stated precision, testing relative risk with a reference value, testing\n    a correlation coefficient with a specified value, etc. \n    <https://www.academia.edu/39511442/Adequacy_of_Sample_Size_in_Health_Studies#:~:text=Determining%20the%20sample%20size%20for,may%20yield%20statistically%20inconclusive%20results.>. ",
    "version": "1.0.0",
    "maintainer": "R Amala <amalar.statistics@gmail.com>",
    "author": "R Amala [aut, cre, cph],\n  G Kumarapandiyan [aut],\n  A Srividya [ctb],\n  M Rajeswari [ctb],\n  Ashwani Kumar [ctb]",
    "url": "",
    "bug_reports": "https://forms.gle/NcznwYU9yx5HdJu29",
    "repository": "https://cran.r-project.org/package=samplesizeestimator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "samplesizeestimator Calculate Sample Size for Various Scenarios Calculates sample size for various scenarios, such as sample size\n    to estimate population proportion with stated absolute or relative\n    precision, testing a single proportion with a reference value, to estimate\n    the population mean with stated absolute or relative precision, testing\n    single mean with a reference value and sample size for comparing two\n    unpaired or independent means, comparing two paired means, the sample size\n    For case control studies, estimating the odds ratio with stated precision,\n    testing the odds ratio with a reference value, estimating relative risk with\n    stated precision, testing relative risk with a reference value, testing\n    a correlation coefficient with a specified value, etc. \n    <https://www.academia.edu/39511442/Adequacy_of_Sample_Size_in_Health_Studies#:~:text=Determining%20the%20sample%20size%20for,may%20yield%20statistically%20inconclusive%20results.>.   "
  },
  {
    "id": 20091,
    "package_name": "sampsizeval",
    "title": "Sample Size for Validation of Risk Models with Binary Outcomes",
    "description": "Estimation of the required sample size to validate a risk model for binary outcomes, based on the sample size equations proposed by Pavlou et al. (2021) <doi:10.1177/09622802211007522>. For precision-based sample size calculations, the user is required to enter the anticipated values of the C-statistic and outcome prevalence, which can be obtained from a previous study. The user also needs to specify the required precision (standard error) for the C-statistic, the calibration slope and the calibration in the large. The calculations are valid under the assumption of marginal normality for the distribution of the linear predictor. ",
    "version": "1.0.0.0",
    "maintainer": "Menelaos Pavlou <m.pavlou@ucl.ac.uk>",
    "author": "Menelaos Pavlou [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1161-1440>)",
    "url": "https://github.com/mpavlou/sampsizeval",
    "bug_reports": "https://github.com/mpavlou/sampsizeval/issues",
    "repository": "https://cran.r-project.org/package=sampsizeval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sampsizeval Sample Size for Validation of Risk Models with Binary Outcomes Estimation of the required sample size to validate a risk model for binary outcomes, based on the sample size equations proposed by Pavlou et al. (2021) <doi:10.1177/09622802211007522>. For precision-based sample size calculations, the user is required to enter the anticipated values of the C-statistic and outcome prevalence, which can be obtained from a previous study. The user also needs to specify the required precision (standard error) for the C-statistic, the calibration slope and the calibration in the large. The calculations are valid under the assumption of marginal normality for the distribution of the linear predictor.   "
  },
  {
    "id": 20105,
    "package_name": "sansa",
    "title": "Synthetic Data Generation for Imbalanced Learning in 'R'",
    "description": "Machine learning is widely used in information-systems design. Yet, training algorithms on imbalanced datasets may severely affect performance on unseen data. For example, in some cases in healthcare, financial, or internet-security contexts, certain sub-classes are difficult to learn because they are underrepresented in training data. This 'R' package offers a flexible and efficient solution based on a new synthetic average neighborhood sampling algorithm ('SANSA'), which, in contrast to other solutions, introduces a novel \u201cplacement\u201d parameter that can be tuned to adapt to each datasets unique manifestation of the imbalance. More information about the algorithm's parameters can be found at Nasir et al. (2022) <https://murtaza.cc/SANSA/>. ",
    "version": "0.0.1",
    "maintainer": "Murtaza Nasir <mail@murtaza.cc>",
    "author": "Murtaza Nasir [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4481-065X>),\n  Ali Dag [ctb],\n  Serhat Simsek [ctb],\n  Anton Ivanov [ctb],\n  Asil Oztekin [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sansa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sansa Synthetic Data Generation for Imbalanced Learning in 'R' Machine learning is widely used in information-systems design. Yet, training algorithms on imbalanced datasets may severely affect performance on unseen data. For example, in some cases in healthcare, financial, or internet-security contexts, certain sub-classes are difficult to learn because they are underrepresented in training data. This 'R' package offers a flexible and efficient solution based on a new synthetic average neighborhood sampling algorithm ('SANSA'), which, in contrast to other solutions, introduces a novel \u201cplacement\u201d parameter that can be tuned to adapt to each datasets unique manifestation of the imbalance. More information about the algorithm's parameters can be found at Nasir et al. (2022) <https://murtaza.cc/SANSA/>.   "
  },
  {
    "id": 20186,
    "package_name": "scape",
    "title": "Statistical Catch-at-Age Plotting Environment",
    "description": "Import, plot, and diagnose results from statistical\n  catch-at-age models, used in fisheries stock assessment.",
    "version": "2.3.5",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Arni Magnusson [aut, cre]",
    "url": "https://github.com/arni-magnusson/scape",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scape Statistical Catch-at-Age Plotting Environment Import, plot, and diagnose results from statistical\n  catch-at-age models, used in fisheries stock assessment.  "
  },
  {
    "id": 20201,
    "package_name": "sccr",
    "title": "The Self-Consistent, Competing Risks (SC-CR) Algorithms",
    "description": "The SC-SR Algorithm is used to calculate fully non-parametric and self-consistent estimators of the cause-specific failure probabilities in the presence of interval-censoring and possible making of the failure cause in a competing risks environment. In the version 2.0 the function creating the probability matrix from double-censored data is added.",
    "version": "2.1",
    "maintainer": "Alicja Wolny-Dominiak<woali@ue.katowice.pl>",
    "author": "Peter Adamic, Alicja Wolny-Dominiak",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sccr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sccr The Self-Consistent, Competing Risks (SC-CR) Algorithms The SC-SR Algorithm is used to calculate fully non-parametric and self-consistent estimators of the cause-specific failure probabilities in the presence of interval-censoring and possible making of the failure cause in a competing risks environment. In the version 2.0 the function creating the probability matrix from double-censored data is added.  "
  },
  {
    "id": 20207,
    "package_name": "scf",
    "title": "Analyzing the Survey of Consumer Finances",
    "description": "Analyze public-use micro data from the Survey of Consumer Finances.\n  Provides tools to download prepared data files, construct replicate-weighted\n  multiply imputed survey designs, compute descriptive statistics and model\n  estimates, and produce plots and tables. Methods follow design-based inference\n  for complex surveys and pooling across multiple imputations. See the package\n  website and the code book for background.",
    "version": "1.0.5",
    "maintainer": "Joseph Cohen <joseph.cohen@qc.cuny.edu>",
    "author": "Joseph Cohen [aut, cre]",
    "url": "https://github.com/jncohen/scf",
    "bug_reports": "https://github.com/jncohen/scf/issues",
    "repository": "https://cran.r-project.org/package=scf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scf Analyzing the Survey of Consumer Finances Analyze public-use micro data from the Survey of Consumer Finances.\n  Provides tools to download prepared data files, construct replicate-weighted\n  multiply imputed survey designs, compute descriptive statistics and model\n  estimates, and produce plots and tables. Methods follow design-based inference\n  for complex surveys and pooling across multiple imputations. See the package\n  website and the code book for background.  "
  },
  {
    "id": 20218,
    "package_name": "schwabr",
    "title": "'Schwab API' Interface",
    "description": "Use R to interface with the 'Charles Schwab Trade API' <https://developer.schwab.com/>.\n    Functions include authentication, trading, price requests, account information, and option \n    chains. A user will need a Schwab brokerage account and Schwab Individual Developer app. See README \n    for authentication process and examples.",
    "version": "0.1.4",
    "maintainer": "Anthony Trevisan <anthonytrevisan@gmail.com>",
    "author": "Anthony Trevisan [aut, cre]",
    "url": "https://altanalytics.github.io/schwabr/",
    "bug_reports": "https://github.com/altanalytics/schwabr/issues",
    "repository": "https://cran.r-project.org/package=schwabr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "schwabr 'Schwab API' Interface Use R to interface with the 'Charles Schwab Trade API' <https://developer.schwab.com/>.\n    Functions include authentication, trading, price requests, account information, and option \n    chains. A user will need a Schwab brokerage account and Schwab Individual Developer app. See README \n    for authentication process and examples.  "
  },
  {
    "id": 20233,
    "package_name": "scorecard",
    "title": "Credit Risk Scorecard",
    "description": "\n  The `scorecard` package makes the development of credit risk scorecard \n  easier and efficient by providing functions for some common tasks, \n  such as data partition, variable selection, woe binning, scorecard scaling,\n  performance evaluation and report generation. These functions can also used\n  in the development of machine learning models.\n    The references including: \n  1. Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: \n  Development and Implementation Using SAS. \n  2. Siddiqi, N. (2006, ISBN: 9780471754510). Credit risk scorecards. \n  Developing and Implementing Intelligent Credit Scoring.",
    "version": "0.4.5",
    "maintainer": "Shichen Xie <xie@shichen.name>",
    "author": "Shichen Xie [aut, cre]",
    "url": "https://github.com/ShichenXie/scorecard,\nhttp://shichen.name/scorecard/",
    "bug_reports": "https://github.com/ShichenXie/scorecard/issues",
    "repository": "https://cran.r-project.org/package=scorecard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scorecard Credit Risk Scorecard \n  The `scorecard` package makes the development of credit risk scorecard \n  easier and efficient by providing functions for some common tasks, \n  such as data partition, variable selection, woe binning, scorecard scaling,\n  performance evaluation and report generation. These functions can also used\n  in the development of machine learning models.\n    The references including: \n  1. Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: \n  Development and Implementation Using SAS. \n  2. Siddiqi, N. (2006, ISBN: 9780471754510). Credit risk scorecards. \n  Developing and Implementing Intelligent Credit Scoring.  "
  },
  {
    "id": 20234,
    "package_name": "scorecardModelUtils",
    "title": "Credit Scorecard Modelling Utils",
    "description": "Provides infrastructure functionalities such as missing value treatment, information value calculation, GINI calculation etc. which are used for developing a traditional credit scorecard as well as a machine learning based model. The functionalities defined are standard steps for any credit underwriting scorecard development, extensively used in financial domain.",
    "version": "0.0.1.0",
    "maintainer": "Arya Poddar <aryapoddar290990@gmail.com>",
    "author": "Arya Poddar [aut, cre],\n  Aiana Goyal [ctb],\n  Kanishk Dogar [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scorecardModelUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scorecardModelUtils Credit Scorecard Modelling Utils Provides infrastructure functionalities such as missing value treatment, information value calculation, GINI calculation etc. which are used for developing a traditional credit scorecard as well as a machine learning based model. The functionalities defined are standard steps for any credit underwriting scorecard development, extensively used in financial domain.  "
  },
  {
    "id": 20279,
    "package_name": "sdbuildR",
    "title": "Easily Build, Simulate, and Visualise Stock-and-Flow Models",
    "description": "Stock-and-flow models are a computational method from the field of \n    system dynamics. They represent how systems change over time and are \n    mathematically equivalent to ordinary differential equations. 'sdbuildR'\n    (system dynamics builder) provides an intuitive interface for constructing \n    stock-and-flow models without requiring extensive domain knowledge. Models \n    can quickly be simulated and revised, supporting iterative development. \n    'sdbuildR' simulates models in 'R' and 'Julia', where 'Julia' offers unit\n    support and large-scale ensemble simulations. Additionally, 'sdbuildR' can \n    import models created in 'Insight Maker' (<https://insightmaker.com/>). ",
    "version": "1.0.8",
    "maintainer": "Kyra Caitlin Evers <kyra.c.evers@gmail.com>",
    "author": "Kyra Caitlin Evers [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6890-3482>)",
    "url": "https://kcevers.github.io/sdbuildR/,\nhttps://github.com/KCEvers/sdbuildR",
    "bug_reports": "https://github.com/KCEvers/sdbuildR/issues",
    "repository": "https://cran.r-project.org/package=sdbuildR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdbuildR Easily Build, Simulate, and Visualise Stock-and-Flow Models Stock-and-flow models are a computational method from the field of \n    system dynamics. They represent how systems change over time and are \n    mathematically equivalent to ordinary differential equations. 'sdbuildR'\n    (system dynamics builder) provides an intuitive interface for constructing \n    stock-and-flow models without requiring extensive domain knowledge. Models \n    can quickly be simulated and revised, supporting iterative development. \n    'sdbuildR' simulates models in 'R' and 'Julia', where 'Julia' offers unit\n    support and large-scale ensemble simulations. Additionally, 'sdbuildR' can \n    import models created in 'Insight Maker' (<https://insightmaker.com/>).   "
  },
  {
    "id": 20282,
    "package_name": "sdcMicro",
    "title": "Statistical Disclosure Control Methods for Anonymization of Data\nand Risk Estimation",
    "description": "Data from statistical agencies and other institutions are mostly\n    confidential. This package, introduced in Templ, Kowarik and Meindl (2017) <doi:10.18637/jss.v067.i04>, can be used for the generation of anonymized\n    (micro)data, i.e. for the creation of public- and scientific-use files.\n    The theoretical basis for the methods implemented can be found in Templ (2017) <doi:10.1007/978-3-319-50272-4>.\n    Various risk estimation and anonymization methods are included. Note that the package\n    includes a graphical user interface published in Meindl and Templ (2019) <doi:10.3390/a12090191> that allows to use various methods of this\n    package.",
    "version": "5.7.9",
    "maintainer": "Matthias Templ <matthias.templ@gmail.com>",
    "author": "Matthias Templ [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8638-5276>),\n  Bernhard Meindl [aut],\n  Alexander Kowarik [aut] (ORCID:\n    <https://orcid.org/0000-0001-8598-4130>),\n  Johannes Gussenbauer [aut],\n  Organisation For Economic Co-Operation And Development [cph] (Initial\n    published c(++) code (under LGPL) code for rank swapping,\n    mdav-microaggregation, suda2 and other (hierarchical) risk\n    measures),\n  Statistics Netherlands [cph] (microAggregation cpp code (under EUPL\n    v1.1)),\n  Pascal Heus [cph] (original measure threshold cpp code (under LGPL))",
    "url": "https://github.com/sdcTools/sdcMicro",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sdcMicro",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdcMicro Statistical Disclosure Control Methods for Anonymization of Data\nand Risk Estimation Data from statistical agencies and other institutions are mostly\n    confidential. This package, introduced in Templ, Kowarik and Meindl (2017) <doi:10.18637/jss.v067.i04>, can be used for the generation of anonymized\n    (micro)data, i.e. for the creation of public- and scientific-use files.\n    The theoretical basis for the methods implemented can be found in Templ (2017) <doi:10.1007/978-3-319-50272-4>.\n    Various risk estimation and anonymization methods are included. Note that the package\n    includes a graphical user interface published in Meindl and Templ (2019) <doi:10.3390/a12090191> that allows to use various methods of this\n    package.  "
  },
  {
    "id": 20293,
    "package_name": "sdprisk",
    "title": "Measures of Risk for the Compound Poisson Risk Process with\nDiffusion",
    "description": "Based on the compound Poisson risk process that is perturbed by\n    a Brownian motion, saddlepoint approximations to some measures of risk are\n    provided. Various approximation methods for the probability of ruin are\n    also included. Furthermore, exact values of both the risk measures as well\n    as the probability of ruin are available if the individual claims follow\n    a hypo-exponential distribution (i. e., if it can be represented as a sum\n    of independent exponentially distributed random variables with different\n    rate parameters). For more details see Gatto and Baumgartner (2014)\n    <doi:10.1007/s11009-012-9316-5>.",
    "version": "1.1-6",
    "maintainer": "Benjamin Baumgartner <benjamin@baumgrt.com>",
    "author": "Benjamin Baumgartner [aut, cre],\n  Riccardo Gatto [ctb, ths],\n  Sebastian Szugat [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sdprisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdprisk Measures of Risk for the Compound Poisson Risk Process with\nDiffusion Based on the compound Poisson risk process that is perturbed by\n    a Brownian motion, saddlepoint approximations to some measures of risk are\n    provided. Various approximation methods for the probability of ruin are\n    also included. Furthermore, exact values of both the risk measures as well\n    as the probability of ruin are available if the individual claims follow\n    a hypo-exponential distribution (i. e., if it can be represented as a sum\n    of independent exponentially distributed random variables with different\n    rate parameters). For more details see Gatto and Baumgartner (2014)\n    <doi:10.1007/s11009-012-9316-5>.  "
  },
  {
    "id": 20309,
    "package_name": "seasonalityPlot",
    "title": "Seasonality Variation Plots of Stock Prices and Cryptocurrencies",
    "description": "The price action at any given time is determined by investor \n  sentiment and market conditions. Although there is no established principle, \n  over a long period of time, things often move with a certain periodicity.\n  This is sometimes referred to as anomaly. \n  The seasonPlot() function in this package calculates and visualizes the \n  average value of price movements over a year for any given period. \n  In addition, the monthly increase or decrease in price movement is \n  represented with a colored background. \n  This seasonPlot() function can use the same symbols as the 'quantmod' package \n  (e.g. ^IXIC, ^DJI, SPY, BTC-USD, and ETH-USD etc). ",
    "version": "1.3.1",
    "maintainer": "Satoshi Kume <satoshi.kume.1984@gmail.com>",
    "author": "Satoshi Kume [aut, cre]",
    "url": "https://github.com/kumeS/seasonalityPlot,\nhttps://kumes.github.io/seasonalityPlot/,\nhttps://skume-seasonalityplot.hf.space/__docs__/#/",
    "bug_reports": "https://github.com/kumeS/seasonalityPlot/issues",
    "repository": "https://cran.r-project.org/package=seasonalityPlot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seasonalityPlot Seasonality Variation Plots of Stock Prices and Cryptocurrencies The price action at any given time is determined by investor \n  sentiment and market conditions. Although there is no established principle, \n  over a long period of time, things often move with a certain periodicity.\n  This is sometimes referred to as anomaly. \n  The seasonPlot() function in this package calculates and visualizes the \n  average value of price movements over a year for any given period. \n  In addition, the monthly increase or decrease in price movement is \n  represented with a colored background. \n  This seasonPlot() function can use the same symbols as the 'quantmod' package \n  (e.g. ^IXIC, ^DJI, SPY, BTC-USD, and ETH-USD etc).   "
  },
  {
    "id": 20367,
    "package_name": "semantic.assets",
    "title": "Assets for 'shiny.semantic'",
    "description": "Style sheets and JavaScript assets for 'shiny.semantic' package.",
    "version": "1.1.0",
    "maintainer": "Jakub Nowicki <opensource+kuba@appsilon.com>",
    "author": "Jakub Nowicki [aut, cre]",
    "url": "https://github.com/Appsilon/semantic.assets",
    "bug_reports": "https://github.com/Appsilon/semantic.assets/issues",
    "repository": "https://cran.r-project.org/package=semantic.assets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semantic.assets Assets for 'shiny.semantic' Style sheets and JavaScript assets for 'shiny.semantic' package.  "
  },
  {
    "id": 20377,
    "package_name": "semicmprskcoxmsm",
    "title": "Use Inverse Probability Weighting to Estimate Treatment Effect\nfor Semi Competing Risks Data",
    "description": "Use inverse probability weighting methods to estimate treatment effect under marginal structure model (MSM) for the transition hazard of semi competing risk data, i.e. illness death model. We implement two specific such models, the usual Markov illness death structural model and the general Markov illness death structural model. We also provide the predicted three risks functions from the marginal structure models. Zhang, Y. and Xu, R. (2022) <arXiv:2204.10426>.",
    "version": "0.2.0",
    "maintainer": "Yiran Zhang <yiz038@health.ucsd.edu>",
    "author": "Yiran Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=semicmprskcoxmsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semicmprskcoxmsm Use Inverse Probability Weighting to Estimate Treatment Effect\nfor Semi Competing Risks Data Use inverse probability weighting methods to estimate treatment effect under marginal structure model (MSM) for the transition hazard of semi competing risk data, i.e. illness death model. We implement two specific such models, the usual Markov illness death structural model and the general Markov illness death structural model. We also provide the predicted three risks functions from the marginal structure models. Zhang, Y. and Xu, R. (2022) <arXiv:2204.10426>.  "
  },
  {
    "id": 20466,
    "package_name": "sfcr",
    "title": "Simulate Stock-Flow Consistent Models",
    "description": "Routines to write, simulate, and validate stock-flow consistent (SFC) models. The accounting structure of SFC models are described in Godley and Lavoie (2007, ISBN:978-1-137-08599-3). The algorithms implemented to solve the models (Gauss-Seidel and Broyden) are described in Kinsella and O'Shea (2010) <doi:10.2139/ssrn.1729205> and Peressini and Sullivan (1988, ISBN:0-387-96614-5).",
    "version": "0.2.3",
    "maintainer": "Joao Macalos <joaomacalos@gmail.com>",
    "author": "Joao Macalos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6050-6394>)",
    "url": "https://github.com/joaomacalos/sfcr",
    "bug_reports": "https://github.com/joaomacalos/sfcr/issues",
    "repository": "https://cran.r-project.org/package=sfcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sfcr Simulate Stock-Flow Consistent Models Routines to write, simulate, and validate stock-flow consistent (SFC) models. The accounting structure of SFC models are described in Godley and Lavoie (2007, ISBN:978-1-137-08599-3). The algorithms implemented to solve the models (Gauss-Seidel and Broyden) are described in Kinsella and O'Shea (2010) <doi:10.2139/ssrn.1729205> and Peressini and Sullivan (1988, ISBN:0-387-96614-5).  "
  },
  {
    "id": 20512,
    "package_name": "shapeR",
    "title": "Collection and Analysis of Otolith Shape Data",
    "description": "Studies otolith shape variation among fish populations.\n  Otoliths are calcified structures found in the inner ear of teleost fish and their shape has\n  been known to vary among several fish populations and stocks, making them very useful in taxonomy,\n  species identification and to study geographic variations. The package extends previously described\n  software used for otolith shape analysis by allowing the user to automatically extract closed\n  contour outlines from a large number of images, perform smoothing to eliminate pixel noise described in Haines and Crampton (2000) <doi:10.1111/1475-4983.00148>,\n  choose from conducting either a Fourier or wavelet see Gen\u00e7ay et al (2001) <doi:10.1016/S0378-4371(00)00463-5> transform to the outlines and visualize\n  the mean shape. The output of the package are independent Fourier or wavelet coefficients\n  which can be directly imported into a wide range of statistical packages in R. The package\n  might prove useful in studies of any two dimensional objects.",
    "version": "1.0-2",
    "maintainer": "Lisa Anne Libungan <lisa.libungan@gmail.com>",
    "author": "Lisa Anne Libungan [aut, cre],\n  Snaebjorn Palsson [aut, ths]",
    "url": "https://github.com/lisalibungan/shapeR,\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121102",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shapeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shapeR Collection and Analysis of Otolith Shape Data Studies otolith shape variation among fish populations.\n  Otoliths are calcified structures found in the inner ear of teleost fish and their shape has\n  been known to vary among several fish populations and stocks, making them very useful in taxonomy,\n  species identification and to study geographic variations. The package extends previously described\n  software used for otolith shape analysis by allowing the user to automatically extract closed\n  contour outlines from a large number of images, perform smoothing to eliminate pixel noise described in Haines and Crampton (2000) <doi:10.1111/1475-4983.00148>,\n  choose from conducting either a Fourier or wavelet see Gen\u00e7ay et al (2001) <doi:10.1016/S0378-4371(00)00463-5> transform to the outlines and visualize\n  the mean shape. The output of the package are independent Fourier or wavelet coefficients\n  which can be directly imported into a wide range of statistical packages in R. The package\n  might prove useful in studies of any two dimensional objects.  "
  },
  {
    "id": 20515,
    "package_name": "shapley",
    "title": "Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid",
    "description": "This R package introduces Weighted Mean SHapley Additive exPlanations (WMSHAP), an innovative method for calculating SHAP values for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not previously available due to the common reliance on single best-performing models. By integrating the weighted mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, the package weights SHAP contributions according to each model's performance, assessed by multiple either R squared (for both regression and classification models). alternatively, this software also offers weighting SHAP values based on the area under the precision-recall curve (AUCPR), the area under the curve (AUC), and F2 measures for binary classifiers. It further extends this framework to implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for the best model. This methodology is particularly beneficial for addressing the severe class imbalance (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance, where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical gap in feature selection literature by presenting criteria for the automatic feature selection of the most important features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, particularly within severely imbalanced outcomes where conventional methods fall short. Moreover, it is also expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and generalizable feature selection. The package further implements a novel method for visualizing SHAP values both at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.",
    "version": "0.5.1",
    "maintainer": "E. F. Haghish <haghish@hotmail.com>",
    "author": "E. F. Haghish [aut, cre, cph]",
    "url": "https://github.com/haghish/shapley",
    "bug_reports": "https://github.com/haghish/shapley/issues",
    "repository": "https://cran.r-project.org/package=shapley",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shapley Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid This R package introduces Weighted Mean SHapley Additive exPlanations (WMSHAP), an innovative method for calculating SHAP values for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not previously available due to the common reliance on single best-performing models. By integrating the weighted mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, the package weights SHAP contributions according to each model's performance, assessed by multiple either R squared (for both regression and classification models). alternatively, this software also offers weighting SHAP values based on the area under the precision-recall curve (AUCPR), the area under the curve (AUC), and F2 measures for binary classifiers. It further extends this framework to implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for the best model. This methodology is particularly beneficial for addressing the severe class imbalance (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance, where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical gap in feature selection literature by presenting criteria for the automatic feature selection of the most important features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, particularly within severely imbalanced outcomes where conventional methods fall short. Moreover, it is also expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and generalizable feature selection. The package further implements a novel method for visualizing SHAP values both at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.  "
  },
  {
    "id": 20526,
    "package_name": "shattering",
    "title": "Estimate the Shattering Coefficient for a Particular Dataset",
    "description": "The Statistical Learning Theory (SLT) provides the theoretical background to ensure that a supervised algorithm generalizes the mapping f:X -> Y given f is selected from its search space bias F. This formal result depends on the Shattering coefficient function N(F,2n) to upper bound the empirical risk minimization principle, from which one can estimate the necessary training sample size to ensure the probabilistic learning convergence and, most importantly, the characterization of the capacity of F, including its under and overfitting abilities while addressing specific target problems. In this context, we propose a new approach to estimate the maximal number of hyperplanes required to shatter a given sample, i.e., to separate every pair of points from one another, based on the recent contributions by Har-Peled and Jones in the dataset partitioning scenario, and use such foundation to analytically compute the Shattering coefficient function for both binary and multi-class problems. As main contributions, one can use our approach to study the complexity of the search space bias F, estimate training sample sizes, and parametrize the number of hyperplanes a learning algorithm needs to address some supervised task, what is specially appealing to deep neural networks. Reference: de Mello, R.F. (2019) \"On the Shattering Coefficient of Supervised Learning Algorithms\" <arXiv:1911.05461>; de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) \"Machine Learning: A Practical Approach on the Statistical Learning Theory\".",
    "version": "1.0.7",
    "maintainer": "Rodrigo F. de Mello <mellorf@gmail.com>",
    "author": "Rodrigo F. de Mello [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0435-3992>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shattering",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shattering Estimate the Shattering Coefficient for a Particular Dataset The Statistical Learning Theory (SLT) provides the theoretical background to ensure that a supervised algorithm generalizes the mapping f:X -> Y given f is selected from its search space bias F. This formal result depends on the Shattering coefficient function N(F,2n) to upper bound the empirical risk minimization principle, from which one can estimate the necessary training sample size to ensure the probabilistic learning convergence and, most importantly, the characterization of the capacity of F, including its under and overfitting abilities while addressing specific target problems. In this context, we propose a new approach to estimate the maximal number of hyperplanes required to shatter a given sample, i.e., to separate every pair of points from one another, based on the recent contributions by Har-Peled and Jones in the dataset partitioning scenario, and use such foundation to analytically compute the Shattering coefficient function for both binary and multi-class problems. As main contributions, one can use our approach to study the complexity of the search space bias F, estimate training sample sizes, and parametrize the number of hyperplanes a learning algorithm needs to address some supervised task, what is specially appealing to deep neural networks. Reference: de Mello, R.F. (2019) \"On the Shattering Coefficient of Supervised Learning Algorithms\" <arXiv:1911.05461>; de Mello, R.F., Ponti, M.A. (2018, ISBN: 978-3319949888) \"Machine Learning: A Practical Approach on the Statistical Learning Theory\".  "
  },
  {
    "id": 20571,
    "package_name": "shinyInvoice",
    "title": "Shiny App - Generate a Pdf Invoice with 'Rmarkdown'",
    "description": "Generate an invoice containing a header with invoice number and businesses details. The invoice table contains\n  any of: salary, one-liner costs, grouped costs. Under the table signature and bank account details appear. \n  Pages are numbered when more than one. Source .json and .Rmd files are editable in the app. A .csv file with raw data can be downloaded.\n  This package includes functions for getting exchange rates between currencies based on \n  'quantmod' (Ryan and Ulrich, 2023 <https://CRAN.R-project.org/package=quantmod>).",
    "version": "0.0.5",
    "maintainer": "Fernando Roa <froao@unal.edu.co>",
    "author": "Fernando Roa [aut, cre]",
    "url": "https://github.com/fernandoroa/invoice-public",
    "bug_reports": "https://github.com/fernandoroa/invoice-public/issues",
    "repository": "https://cran.r-project.org/package=shinyInvoice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyInvoice Shiny App - Generate a Pdf Invoice with 'Rmarkdown' Generate an invoice containing a header with invoice number and businesses details. The invoice table contains\n  any of: salary, one-liner costs, grouped costs. Under the table signature and bank account details appear. \n  Pages are numbered when more than one. Source .json and .Rmd files are editable in the app. A .csv file with raw data can be downloaded.\n  This package includes functions for getting exchange rates between currencies based on \n  'quantmod' (Ryan and Ulrich, 2023 <https://CRAN.R-project.org/package=quantmod>).  "
  },
  {
    "id": 20667,
    "package_name": "siconvr",
    "title": "Fetch Data from Plataforma +Brasil (SICONV)",
    "description": "Fetch data on targeted public investments from Plataforma +Brasil (SICONV) <http://plataformamaisbrasil.gov.br/>, \n    the responsible system for requests, execution, and monitoring of federal discretionary transfers in Brazil.",
    "version": "0.0.1",
    "maintainer": "Fernando Meireles <fmeireles@ufmg.br>",
    "author": "Fernando Meireles [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7027-2058>),\n  Marcus Vin\u00edcius de S\u00e1 Torres [aut] (ORCID:\n    <https://orcid.org/0000-0002-9647-1418>)",
    "url": "https://github.com/meirelesff/siconvr,\nhttps://fmeireles.com/siconvr/",
    "bug_reports": "https://github.com/meirelesff/siconvr/issues",
    "repository": "https://cran.r-project.org/package=siconvr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "siconvr Fetch Data from Plataforma +Brasil (SICONV) Fetch data on targeted public investments from Plataforma +Brasil (SICONV) <http://plataformamaisbrasil.gov.br/>, \n    the responsible system for requests, execution, and monitoring of federal discretionary transfers in Brazil.  "
  },
  {
    "id": 20668,
    "package_name": "sicure",
    "title": "Single-Index Mixture Cure Models",
    "description": "Single-index mixture cure models allow estimating the probability of cure and the latency depending on a vector (or functional) covariate, avoiding the curse of dimensionality. The vector of parameters that defines the model can be estimated by maximum likelihood. A nonparametric estimator for the conditional density of the susceptible population is provided. For more details, see Pi\u00f1eiro-Lamas (2024) (<https://ruc.udc.es/dspace/handle/2183/37035>). Funding: This work, integrated into the framework of PERTE for Vanguard Health, has been co-financed by the Spanish Ministry of Science, Innovation and Universities with funds from the European Union NextGenerationEU, from the Recovery, Transformation and Resilience Plan (PRTR-C17.I1) and from the Autonomous Community of Galicia within the framework of the Biotechnology Plan Applied to Health.",
    "version": "0.1.1",
    "maintainer": "Beatriz Pi\u00f1eiro-Lamas <b.pineiro.lamas@udc.es>",
    "author": "Beatriz Pi\u00f1eiro-Lamas [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0673-5377>),\n  Ana L\u00f3pez-Cheda [aut],\n  Ricardo Cao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sicure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sicure Single-Index Mixture Cure Models Single-index mixture cure models allow estimating the probability of cure and the latency depending on a vector (or functional) covariate, avoiding the curse of dimensionality. The vector of parameters that defines the model can be estimated by maximum likelihood. A nonparametric estimator for the conditional density of the susceptible population is provided. For more details, see Pi\u00f1eiro-Lamas (2024) (<https://ruc.udc.es/dspace/handle/2183/37035>). Funding: This work, integrated into the framework of PERTE for Vanguard Health, has been co-financed by the Spanish Ministry of Science, Innovation and Universities with funds from the European Union NextGenerationEU, from the Recovery, Transformation and Resilience Plan (PRTR-C17.I1) and from the Autonomous Community of Galicia within the framework of the Biotechnology Plan Applied to Health.  "
  },
  {
    "id": 20718,
    "package_name": "simaerep",
    "title": "Detect Clinical Trial Sites Over- or Under-Reporting Clinical\nEvents",
    "description": "Monitoring reporting rates of subject-level clinical events (e.g.\n  adverse events, protocol deviations) reported by clinical trial sites is an\n  important aspect of risk-based quality monitoring strategy. Sites that are \n  under-reporting or over-reporting events can be detected using bootstrap\n  simulations during which patients are redistributed between sites. Site-specific\n  distributions of event reporting rates  are generated that are used to assign\n  probabilities to the observed reporting rates.\n  (Koneswarakantha 2024 <doi:10.1007/s43441-024-00631-8>).",
    "version": "1.0.0",
    "maintainer": "Bjoern Koneswarakantha <bjoern.koneswarakantha@roche.com>",
    "author": "Bjoern Koneswarakantha [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4585-7799>),\n  F. Hoffmann-La Roche Ltd [cph]",
    "url": "https://openpharma.github.io/simaerep/,\nhttps://github.com/openpharma/simaerep/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simaerep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simaerep Detect Clinical Trial Sites Over- or Under-Reporting Clinical\nEvents Monitoring reporting rates of subject-level clinical events (e.g.\n  adverse events, protocol deviations) reported by clinical trial sites is an\n  important aspect of risk-based quality monitoring strategy. Sites that are \n  under-reporting or over-reporting events can be detected using bootstrap\n  simulations during which patients are redistributed between sites. Site-specific\n  distributions of event reporting rates  are generated that are used to assign\n  probabilities to the observed reporting rates.\n  (Koneswarakantha 2024 <doi:10.1007/s43441-024-00631-8>).  "
  },
  {
    "id": 20749,
    "package_name": "simpleFDR",
    "title": "Simple False Discovery Rate Calculation",
    "description": "Using the adjustment method from Benjamini & Hochberg (1995) <doi:10.1111/j.2517-6161.1995.tb02031.x>, this package determines which variables are significant under repeated testing with a given dataframe of p values and an user defined \"q\" threshold.  It then returns the original dataframe along with a significance column where an asterisk denotes a significant p value after FDR calculation, and NA denotes all other p values. This package uses the Benjamini & Hochberg method specifically as described in Lee, S., & Lee, D. K. (2018) <doi:10.4097/kja.d.18.00242>. ",
    "version": "1.1",
    "maintainer": "Stephen Wisser <swisser98@gmail.com>",
    "author": "Stephen C Wisser",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simpleFDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simpleFDR Simple False Discovery Rate Calculation Using the adjustment method from Benjamini & Hochberg (1995) <doi:10.1111/j.2517-6161.1995.tb02031.x>, this package determines which variables are significant under repeated testing with a given dataframe of p values and an user defined \"q\" threshold.  It then returns the original dataframe along with a significance column where an asterisk denotes a significant p value after FDR calculation, and NA denotes all other p values. This package uses the Benjamini & Hochberg method specifically as described in Lee, S., & Lee, D. K. (2018) <doi:10.4097/kja.d.18.00242>.   "
  },
  {
    "id": 20768,
    "package_name": "simrec",
    "title": "Simulation of Recurrent Event Data for Non-Constant Baseline\nHazard",
    "description": "Simulation of recurrent event data for non-constant baseline\n    hazard in the total time model with risk-free intervals and possibly a competing event.\n    Possibility to cut the data to an interim data set. Data can be plotted.\n    Details about the method can be found in Jahn-Eimermacher, A. et al. (2015) <doi:10.1186/s12874-015-0005-2>.",
    "version": "1.0.1",
    "maintainer": "Federico Marini <marinif@uni-mainz.de>",
    "author": "Katharina Ingel [aut],\n  Antje Jahn-Eimermacher [aut],\n  Stella Preussler [aut],\n  Federico Marini [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3252-7758>)",
    "url": "https://github.com/federicomarini/simrec",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simrec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simrec Simulation of Recurrent Event Data for Non-Constant Baseline\nHazard Simulation of recurrent event data for non-constant baseline\n    hazard in the total time model with risk-free intervals and possibly a competing event.\n    Possibility to cut the data to an interim data set. Data can be plotted.\n    Details about the method can be found in Jahn-Eimermacher, A. et al. (2015) <doi:10.1186/s12874-015-0005-2>.  "
  },
  {
    "id": 20791,
    "package_name": "singR",
    "title": "Simultaneous Non-Gaussian Component Analysis",
    "description": "Implementation of SING algorithm to extract joint and individual non-Gaussian components from two datasets. SING uses an objective function that maximizes the skewness and kurtosis of latent components with a penalty to enhance the similarity between subject scores. Unlike other existing methods, SING does not use PCA for dimension reduction, but rather uses non-Gaussianity, which can improve feature extraction. Benjamin B.Risk, Irina Gaynanova (2021) <doi:10.1214/21-AOAS1466>.  ",
    "version": "0.1.3",
    "maintainer": "Liangkang Wang <liangkang_wang@brown.edu>",
    "author": "Liangkang Wang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3393-243X>),\n  Irina Gaynanova [aut] (ORCID: <https://orcid.org/0000-0002-4116-0268>),\n  Benjamin Risk [aut] (ORCID: <https://orcid.org/0000-0003-1090-0777>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=singR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "singR Simultaneous Non-Gaussian Component Analysis Implementation of SING algorithm to extract joint and individual non-Gaussian components from two datasets. SING uses an objective function that maximizes the skewness and kurtosis of latent components with a penalty to enhance the similarity between subject scores. Unlike other existing methods, SING does not use PCA for dimension reduction, but rather uses non-Gaussianity, which can improve feature extraction. Benjamin B.Risk, Irina Gaynanova (2021) <doi:10.1214/21-AOAS1466>.    "
  },
  {
    "id": 20898,
    "package_name": "smidm",
    "title": "Statistical Modelling for Infectious Disease Management",
    "description": "Statistical models for specific coronavirus disease 2019 use cases at German local health authorities. All models of Statistical modelling for infectious disease management 'smidm' are part of the decision support toolkit in the 'EsteR' project. More information is published in Sonja J\u00e4ckle, Rieke Alpers, Lisa K\u00fchne, Jakob Schumacher, Benjamin Geisler, Max Westphal \"'EsteR' \u2013 A Digital Toolkit for COVID-19 Decision Support in Local Health Authorities\" (2022) <doi:10.3233/SHTI220799> and Sonja J\u00e4ckle, Elias R\u00f6ger, Volker Dicken, Benjamin Geisler, Jakob Schumacher, Max Westphal \"A Statistical Model to Assess Risk for Supporting COVID-19 Quarantine Decisions\" (2021) <doi:10.3390/ijerph18179166>.",
    "version": "1.0",
    "maintainer": "Sonja J\u00e4ckle <sonja.jaeckle@mevis.fraunhofer.de>",
    "author": "Max Westphal [aut] (ORCID: <https://orcid.org/0000-0002-8488-758X>),\n  Stefanie Grimm [aut],\n  Sonja J\u00e4ckle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2908-299X>),\n  Rieke Alpers [aut] (ORCID: <https://orcid.org/0000-0001-8317-1435>),\n  Hong Phuc Truong [aut],\n  Amelie Lucker [ctb],\n  Fraunhofer MEVIS [cph],\n  Fraunhofer ITWM [cph]",
    "url": "https://gitlab.cc-asp.fraunhofer.de/ester/smidm",
    "bug_reports": "https://gitlab.cc-asp.fraunhofer.de/ester/smidm/-/issues",
    "repository": "https://cran.r-project.org/package=smidm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smidm Statistical Modelling for Infectious Disease Management Statistical models for specific coronavirus disease 2019 use cases at German local health authorities. All models of Statistical modelling for infectious disease management 'smidm' are part of the decision support toolkit in the 'EsteR' project. More information is published in Sonja J\u00e4ckle, Rieke Alpers, Lisa K\u00fchne, Jakob Schumacher, Benjamin Geisler, Max Westphal \"'EsteR' \u2013 A Digital Toolkit for COVID-19 Decision Support in Local Health Authorities\" (2022) <doi:10.3233/SHTI220799> and Sonja J\u00e4ckle, Elias R\u00f6ger, Volker Dicken, Benjamin Geisler, Jakob Schumacher, Max Westphal \"A Statistical Model to Assess Risk for Supporting COVID-19 Quarantine Decisions\" (2021) <doi:10.3390/ijerph18179166>.  "
  },
  {
    "id": 20913,
    "package_name": "smoother",
    "title": "Functions Relating to the Smoothing of Numerical Data",
    "description": "A collection of methods for smoothing numerical data, commencing\n    with a port of the Matlab gaussian window smoothing function. In addition,\n    several functions typically used in smoothing of financial data are included.",
    "version": "1.3",
    "maintainer": "Nicholas Hamilton <n.hamilton@unsw.edu.au>",
    "author": "Nicholas Hamilton",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=smoother",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smoother Functions Relating to the Smoothing of Numerical Data A collection of methods for smoothing numerical data, commencing\n    with a port of the Matlab gaussian window smoothing function. In addition,\n    several functions typically used in smoothing of financial data are included.  "
  },
  {
    "id": 20968,
    "package_name": "socialranking",
    "title": "Social Ranking Solutions for Power Relations on Coalitions",
    "description": "The notion of power index has been widely used in literature to evaluate the influence of individual players (e.g., voters, political parties, nations, stockholders, etc.) involved in a collective decision situation like an electoral system, a parliament, a council, a management board, etc., where players may form coalitions. Traditionally this ranking is determined through numerical evaluation. More often than not however only ordinal data between coalitions is known. The package 'socialranking' offers a set of solutions to rank players based on a transitive ranking between coalitions, including through CP-Majority, ordinal Banzhaf or lexicographic excellence solution summarized by Tahar Allouche, Bruno Escoffier, Stefano Moretti and Meltem \u00d6zt\u00fcrk (2020, <doi:10.24963/ijcai.2020/3>).",
    "version": "1.2.0",
    "maintainer": "Felix Fritz <felix.fritz@dauphine.eu>",
    "author": "Felix Fritz [aut, cre],\n  Jochen Staudacher [aut, cph, ths],\n  Moretti Stefano [aut, cph, ths]",
    "url": "https://github.com/jassler/socialranking",
    "bug_reports": "https://github.com/jassler/socialranking/issues",
    "repository": "https://cran.r-project.org/package=socialranking",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "socialranking Social Ranking Solutions for Power Relations on Coalitions The notion of power index has been widely used in literature to evaluate the influence of individual players (e.g., voters, political parties, nations, stockholders, etc.) involved in a collective decision situation like an electoral system, a parliament, a council, a management board, etc., where players may form coalitions. Traditionally this ranking is determined through numerical evaluation. More often than not however only ordinal data between coalitions is known. The package 'socialranking' offers a set of solutions to rank players based on a transitive ranking between coalitions, including through CP-Majority, ordinal Banzhaf or lexicographic excellence solution summarized by Tahar Allouche, Bruno Escoffier, Stefano Moretti and Meltem \u00d6zt\u00fcrk (2020, <doi:10.24963/ijcai.2020/3>).  "
  },
  {
    "id": 20969,
    "package_name": "socialrisk",
    "title": "Identifying Patient Social Risk from Administrative Health Care\nData",
    "description": "Social risks are increasingly becoming a critical component of health\n    care research. One of the most common ways to identify social needs is by using\n    ICD-10-CM \"Z-codes.\" This package identifies social risks using varying taxonomies\n    of ICD-10-CM Z-codes from administrative health care data. The conceptual\n    taxonomies come from:\n    Centers for Medicare and Medicaid Services (2021) <https://www.cms.gov/files/document/zcodes-infographic.pdf>,\n    Reidhead (2018) <https://web.mhanet.com/>,\n    A Arons, S DeSilvey, C Fichtenberg, L Gottlieb (2018) <https://sirenetwork.ucsf.edu/tools-resources/resources/compendium-medical-terminology-codes-social-risk-factors>.",
    "version": "0.5.1",
    "maintainer": "Wyatt Bensken <wpb27@case.edu>",
    "author": "Wyatt Bensken [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2597-9732>)",
    "url": "https://github.com/WYATTBENSKEN/multimorbidity",
    "bug_reports": "https://github.com/WYATTBENSKEN/multimorbidity/issues",
    "repository": "https://cran.r-project.org/package=socialrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "socialrisk Identifying Patient Social Risk from Administrative Health Care\nData Social risks are increasingly becoming a critical component of health\n    care research. One of the most common ways to identify social needs is by using\n    ICD-10-CM \"Z-codes.\" This package identifies social risks using varying taxonomies\n    of ICD-10-CM Z-codes from administrative health care data. The conceptual\n    taxonomies come from:\n    Centers for Medicare and Medicaid Services (2021) <https://www.cms.gov/files/document/zcodes-infographic.pdf>,\n    Reidhead (2018) <https://web.mhanet.com/>,\n    A Arons, S DeSilvey, C Fichtenberg, L Gottlieb (2018) <https://sirenetwork.ucsf.edu/tools-resources/resources/compendium-medical-terminology-codes-social-risk-factors>.  "
  },
  {
    "id": 21060,
    "package_name": "spantest",
    "title": "Mean-Variance Spanning Tests",
    "description": "Provides a comprehensive suite of portfolio spanning tests for asset \n             pricing, such as Huberman and Kandel (1987) <doi:10.1111/j.1540-6261.1987.tb03917.x>, \n             Gibbons et al. (1989) <doi:10.2307/1913625>, Kempf and Memmel (2006) <doi:10.1007/BF03396737>, \n             Pesaran and Yamagata (2024) <doi:10.1093/jjfinec/nbad002>, and Gungor and \n             Luger (2016) <doi:10.1080/07350015.2015.1019510>.",
    "version": "1.1-3",
    "maintainer": "David Ardia <david.ardia.ch@gmail.com>",
    "author": "David Ardia [aut, cre] (ORCID: <https://orcid.org/0000-0003-2823-782X>),\n  Benjamin Seguin [aut],\n  Rosnel Sessinou [ctb],\n  Richard Luger [ctb]",
    "url": "https://github.com/ArdiaD/spantest",
    "bug_reports": "https://github.com/ArdiaD/spantest/issues",
    "repository": "https://cran.r-project.org/package=spantest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spantest Mean-Variance Spanning Tests Provides a comprehensive suite of portfolio spanning tests for asset \n             pricing, such as Huberman and Kandel (1987) <doi:10.1111/j.1540-6261.1987.tb03917.x>, \n             Gibbons et al. (1989) <doi:10.2307/1913625>, Kempf and Memmel (2006) <doi:10.1007/BF03396737>, \n             Pesaran and Yamagata (2024) <doi:10.1093/jjfinec/nbad002>, and Gungor and \n             Luger (2016) <doi:10.1080/07350015.2015.1019510>.  "
  },
  {
    "id": 21075,
    "package_name": "sparr",
    "title": "Spatial and Spatiotemporal Relative Risk",
    "description": "Provides functions to estimate kernel-smoothed spatial and spatio-temporal densities and relative risk functions, and perform subsequent inference. Methodological details can be found in the accompanying tutorial: Davies et al. (2018) <DOI:10.1002/sim.7577>.",
    "version": "2.3-16",
    "maintainer": "Tilman M. Davies <tilman.davies@otago.ac.nz>",
    "author": "Tilman M. Davies [aut, cre],\n  Jonathan C. Marshall [aut]",
    "url": "https://tilmandavies.github.io/sparr/,\nhttps://github.com/tilmandavies/sparr/",
    "bug_reports": "https://github.com/tilmandavies/sparr/issues/",
    "repository": "https://cran.r-project.org/package=sparr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparr Spatial and Spatiotemporal Relative Risk Provides functions to estimate kernel-smoothed spatial and spatio-temporal densities and relative risk functions, and perform subsequent inference. Methodological details can be found in the accompanying tutorial: Davies et al. (2018) <DOI:10.1002/sim.7577>.  "
  },
  {
    "id": 21076,
    "package_name": "sparrpowR",
    "title": "Power Analysis to Detect Spatial Relative Risk Clusters",
    "description": "Calculate the statistical power to detect clusters using kernel-based \n        spatial relative risk functions that are estimated using the 'sparr' package.\n        Details about the 'sparr' package methods can be found in the tutorial: Davies\n        et al. (2018) <doi:10.1002/sim.7577>. Details about kernel density estimation \n        can be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>. More \n        information about relative risk functions using kernel density estimation can \n        be found in J. F. Bithell (1991) <doi:10.1002/sim.4780101112>.",
    "version": "0.2.9",
    "maintainer": "Ian D. Buller <ian.buller@alumni.emory.edu>",
    "author": "Ian D. Buller [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9477-8582>),\n  Derek W. Brown [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-8393-1713>),\n  Mitchell J. Machiela [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6538-9705>),\n  Timothy A. Myers [ctb],\n  NCI [cph, fnd]",
    "url": "https://github.com/machiela-lab/sparrpowR",
    "bug_reports": "https://github.com/machiela-lab/sparrpowR/issues",
    "repository": "https://cran.r-project.org/package=sparrpowR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparrpowR Power Analysis to Detect Spatial Relative Risk Clusters Calculate the statistical power to detect clusters using kernel-based \n        spatial relative risk functions that are estimated using the 'sparr' package.\n        Details about the 'sparr' package methods can be found in the tutorial: Davies\n        et al. (2018) <doi:10.1002/sim.7577>. Details about kernel density estimation \n        can be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>. More \n        information about relative risk functions using kernel density estimation can \n        be found in J. F. Bithell (1991) <doi:10.1002/sim.4780101112>.  "
  },
  {
    "id": 21078,
    "package_name": "sparseDFM",
    "title": "Estimate Dynamic Factor Models with Sparse Loadings",
    "description": "Implementation of various estimation methods for dynamic factor models (DFMs) including principal components analysis (PCA) Stock and Watson (2002) <doi:10.1198/016214502388618960>, 2Stage Giannone et al. (2008) <doi:10.1016/j.jmoneco.2008.05.010>, expectation-maximisation (EM) Banbura and Modugno (2014) <doi:10.1002/jae.2306>, and the novel EM-sparse approach for sparse DFMs Mosley et al. (2023) <arXiv:2303.11892>. Options to use classic multivariate Kalman filter and smoother (KFS) equations from Shumway and Stoffer (1982) <doi:10.1111/j.1467-9892.1982.tb00349.x> or fast univariate KFS equations from Koopman and Durbin (2000) <doi:10.1111/1467-9892.00186>, and options for independent and identically distributed (IID) white noise or auto-regressive (AR(1)) idiosyncratic errors. Algorithms coded in 'C++' and linked to R via 'RcppArmadillo'.   ",
    "version": "1.0",
    "maintainer": "Alex Gibberd <a.gibberd@lancaster.ac.uk>",
    "author": "Luke Mosley [aut],\n  Tak-Shing Chan [aut],\n  Alex Gibberd [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sparseDFM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseDFM Estimate Dynamic Factor Models with Sparse Loadings Implementation of various estimation methods for dynamic factor models (DFMs) including principal components analysis (PCA) Stock and Watson (2002) <doi:10.1198/016214502388618960>, 2Stage Giannone et al. (2008) <doi:10.1016/j.jmoneco.2008.05.010>, expectation-maximisation (EM) Banbura and Modugno (2014) <doi:10.1002/jae.2306>, and the novel EM-sparse approach for sparse DFMs Mosley et al. (2023) <arXiv:2303.11892>. Options to use classic multivariate Kalman filter and smoother (KFS) equations from Shumway and Stoffer (1982) <doi:10.1111/j.1467-9892.1982.tb00349.x> or fast univariate KFS equations from Koopman and Durbin (2000) <doi:10.1111/1467-9892.00186>, and options for independent and identically distributed (IID) white noise or auto-regressive (AR(1)) idiosyncratic errors. Algorithms coded in 'C++' and linked to R via 'RcppArmadillo'.     "
  },
  {
    "id": 21083,
    "package_name": "sparseIndexTracking",
    "title": "Design of Portfolio of Stocks to Track an Index",
    "description": "Computation of sparse portfolios for financial index tracking, i.e., joint\n    selection of a subset of the assets that compose the index and computation\n    of their relative weights (capital allocation). The level of sparsity of the\n    portfolios, i.e., the number of selected assets, is controlled through a\n    regularization parameter. Different tracking measures are available, namely,\n    the empirical tracking error (ETE), downside risk (DR), Huber empirical\n    tracking error (HETE), and Huber downside risk (HDR). See vignette for a\n    detailed documentation and comparison, with several illustrative examples.\n    The package is based on the paper:\n    K. Benidis, Y. Feng, and D. P. Palomar, \"Sparse Portfolios for High-Dimensional\n    Financial Index Tracking,\" IEEE Trans. on Signal Processing, vol. 66, no. 1,\n    pp. 155-170, Jan. 2018. <doi:10.1109/TSP.2017.2762286>.",
    "version": "0.1.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Konstantinos Benidis [aut],\n  Daniel P. Palomar [cre, aut]",
    "url": "https://CRAN.R-project.org/package=sparseIndexTracking,\nhttps://github.com/dppalomar/sparseIndexTracking,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2017.2762286",
    "bug_reports": "https://github.com/dppalomar/sparseIndexTracking/issues",
    "repository": "https://cran.r-project.org/package=sparseIndexTracking",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseIndexTracking Design of Portfolio of Stocks to Track an Index Computation of sparse portfolios for financial index tracking, i.e., joint\n    selection of a subset of the assets that compose the index and computation\n    of their relative weights (capital allocation). The level of sparsity of the\n    portfolios, i.e., the number of selected assets, is controlled through a\n    regularization parameter. Different tracking measures are available, namely,\n    the empirical tracking error (ETE), downside risk (DR), Huber empirical\n    tracking error (HETE), and Huber downside risk (HDR). See vignette for a\n    detailed documentation and comparison, with several illustrative examples.\n    The package is based on the paper:\n    K. Benidis, Y. Feng, and D. P. Palomar, \"Sparse Portfolios for High-Dimensional\n    Financial Index Tracking,\" IEEE Trans. on Signal Processing, vol. 66, no. 1,\n    pp. 155-170, Jan. 2018. <doi:10.1109/TSP.2017.2762286>.  "
  },
  {
    "id": 21123,
    "package_name": "spatialrisk",
    "title": "Calculating Spatial Risk",
    "description": "Provides methods for spatial risk calculations, focusing on \n    efficient determination of the sum of observations within a circle of a \n    given radius. These methods are particularly relevant for applications such \n    as insurance, where recent European Commission regulations require the \n    calculation of the maximum insured value of fire risk policies for all \n    buildings that are partly or fully located within a 200 m radius. The \n    underlying problem is described by Church (1974) <doi:10.1007/BF01942293>.",
    "version": "0.7.3",
    "maintainer": "Martin Haringa <mtharinga@gmail.com>",
    "author": "Martin Haringa [aut, cre]",
    "url": "https://github.com/mharinga/spatialrisk,\nhttps://mharinga.github.io/spatialrisk/",
    "bug_reports": "https://github.com/mharinga/spatialrisk/issues",
    "repository": "https://cran.r-project.org/package=spatialrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialrisk Calculating Spatial Risk Provides methods for spatial risk calculations, focusing on \n    efficient determination of the sum of observations within a circle of a \n    given radius. These methods are particularly relevant for applications such \n    as insurance, where recent European Commission regulations require the \n    calculation of the maximum insured value of fire risk policies for all \n    buildings that are partly or fully located within a 200 m radius. The \n    underlying problem is described by Church (1974) <doi:10.1007/BF01942293>.  "
  },
  {
    "id": 21126,
    "package_name": "spatstat",
    "title": "Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests",
    "description": "Comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region. Also supports three-dimensional point patterns, space-time point patterns in any number of dimensions, point patterns on a linear network, and patterns of other geometrical objects. Supports spatial covariate data such as pixel images. \n\tContains over 3000 functions for plotting spatial data, exploratory data analysis, model-fitting, simulation, spatial sampling, model diagnostics, and formal inference. \n\tData types include point patterns, line segment patterns, spatial windows, pixel images, tessellations, and linear networks. \n\tExploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. \n\tA model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.",
    "version": "3.5-0",
    "maintainer": "Adrian Baddeley <Adrian.Baddeley@curtin.edu.au>",
    "author": "Adrian Baddeley [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9499-8382>),\n  Rolf Turner [aut] (ORCID: <https://orcid.org/0000-0001-5521-5218>),\n  Ege Rubak [aut] (ORCID: <https://orcid.org/0000-0002-6675-533X>)",
    "url": "http://spatstat.org/",
    "bug_reports": "https://github.com/spatstat/spatstat/issues",
    "repository": "https://cran.r-project.org/package=spatstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatstat Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests Comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region. Also supports three-dimensional point patterns, space-time point patterns in any number of dimensions, point patterns on a linear network, and patterns of other geometrical objects. Supports spatial covariate data such as pixel images. \n\tContains over 3000 functions for plotting spatial data, exploratory data analysis, model-fitting, simulation, spatial sampling, model diagnostics, and formal inference. \n\tData types include point patterns, line segment patterns, spatial windows, pixel images, tessellations, and linear networks. \n\tExploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. \n\tA model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.  "
  },
  {
    "id": 21129,
    "package_name": "spatstat.explore",
    "title": "Exploratory Data Analysis for the 'spatstat' Family",
    "description": "Functionality for exploratory data analysis and nonparametric analysis of\n\t     spatial data, mainly spatial point patterns,\n\t     in the 'spatstat' family of packages.\n\t     (Excludes analysis of spatial data on a linear network,\n\t     which is covered by the separate package 'spatstat.linnet'.)\n\t     Methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.",
    "version": "3.6-0",
    "maintainer": "Adrian Baddeley <Adrian.Baddeley@curtin.edu.au>",
    "author": "Adrian Baddeley [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9499-8382>),\n  Rolf Turner [aut, cph] (ORCID: <https://orcid.org/0000-0001-5521-5218>),\n  Ege Rubak [aut, cph] (ORCID: <https://orcid.org/0000-0002-6675-533X>),\n  Kasper Klitgaard Berthelsen [ctb],\n  Warick Brown [cph],\n  Achmad Choiruddin [ctb],\n  Ya-Mei Chang [ctb],\n  Jean-Francois Coeurjolly [ctb],\n  Lucia Cobo Sanchez [ctb, cph],\n  Ottmar Cronie [ctb],\n  Tilman Davies [ctb, cph],\n  Julian Gilbey [ctb],\n  Jonatan Gonzalez [ctb],\n  Yongtao Guan [ctb],\n  Ute Hahn [ctb],\n  Martin Hazelton [ctb],\n  Kassel Hingee [ctb, cph],\n  Abdollah Jalilian [ctb],\n  Frederic Lavancier [ctb],\n  Marie-Colette van Lieshout [ctb, cph],\n  Greg McSwiggan [ctb],\n  Robin K Milne [cph],\n  Tuomas Rajala [ctb],\n  Suman Rakshit [ctb, cph],\n  Dominic Schuhmacher [ctb],\n  Rasmus Plenge Waagepetersen [ctb],\n  Hangsheng Wang [ctb],\n  Tingting Zhan [ctb]",
    "url": "http://spatstat.org/",
    "bug_reports": "https://github.com/spatstat/spatstat.explore/issues",
    "repository": "https://cran.r-project.org/package=spatstat.explore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatstat.explore Exploratory Data Analysis for the 'spatstat' Family Functionality for exploratory data analysis and nonparametric analysis of\n\t     spatial data, mainly spatial point patterns,\n\t     in the 'spatstat' family of packages.\n\t     (Excludes analysis of spatial data on a linear network,\n\t     which is covered by the separate package 'spatstat.linnet'.)\n\t     Methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.  "
  },
  {
    "id": 21132,
    "package_name": "spatstat.linnet",
    "title": "Linear Networks Functionality of the 'spatstat' Family",
    "description": "Defines types of spatial data on a linear network\n\t     and provides functionality for geometrical operations,\n\t     data analysis and modelling of data on a linear network,\n\t     in the 'spatstat' family of packages.\n\t     Contains definitions and support for linear networks, including creation of networks, geometrical measurements, topological connectivity, geometrical operations such as inserting and deleting vertices, intersecting a network with another object, and interactive editing of networks.\n\t     Data types defined on a network include point patterns, pixel images, functions, and tessellations.\n\t     Exploratory methods include kernel estimation of intensity on a network, K-functions and pair correlation functions on a network, simulation envelopes, nearest neighbour distance and empty space distance, relative risk estimation with cross-validated bandwidth selection. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the function lppm() similar to glm(). Only Poisson models are implemented so far. Models may involve dependence on covariates and dependence on marks. Models are fitted by maximum likelihood.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.\n\tRandom point patterns on a network can be generated using a variety of models.",
    "version": "3.4-0",
    "maintainer": "Adrian Baddeley <Adrian.Baddeley@curtin.edu.au>",
    "author": "Adrian Baddeley [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9499-8382>),\n  Rolf Turner [aut, cph] (ORCID: <https://orcid.org/0000-0001-5521-5218>),\n  Ege Rubak [aut, cph] (ORCID: <https://orcid.org/0000-0002-6675-533X>),\n  Greg McSwiggan [aut, cph],\n  Tilman Davies [ctb, cph],\n  Mehdi Moradi [ctb, cph],\n  Suman Rakshit [ctb, cph],\n  Ottmar Cronie [ctb]",
    "url": "http://spatstat.org/",
    "bug_reports": "https://github.com/spatstat/spatstat.linnet/issues",
    "repository": "https://cran.r-project.org/package=spatstat.linnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatstat.linnet Linear Networks Functionality of the 'spatstat' Family Defines types of spatial data on a linear network\n\t     and provides functionality for geometrical operations,\n\t     data analysis and modelling of data on a linear network,\n\t     in the 'spatstat' family of packages.\n\t     Contains definitions and support for linear networks, including creation of networks, geometrical measurements, topological connectivity, geometrical operations such as inserting and deleting vertices, intersecting a network with another object, and interactive editing of networks.\n\t     Data types defined on a network include point patterns, pixel images, functions, and tessellations.\n\t     Exploratory methods include kernel estimation of intensity on a network, K-functions and pair correlation functions on a network, simulation envelopes, nearest neighbour distance and empty space distance, relative risk estimation with cross-validated bandwidth selection. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.\n\tParametric models can be fitted to point pattern data using the function lppm() similar to glm(). Only Poisson models are implemented so far. Models may involve dependence on covariates and dependence on marks. Models are fitted by maximum likelihood.\n\tFitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.\n\tRandom point patterns on a network can be generated using a variety of models.  "
  },
  {
    "id": 21154,
    "package_name": "spduration",
    "title": "Split-Population Duration (Cure) Regression",
    "description": "An implementation of split-population duration regression models. \n    Unlike regular duration models, split-population duration models are\n    mixture models that accommodate the presence of a sub-population that is \n    not at risk for failure, e.g. cancer patients who have been cured by \n    treatment. This package implements Weibull and Loglogistic forms for the \n    duration component, and focuses on data with time-varying covariates. \n    These models were originally formulated in Boag (1949) and Berkson and Gage \n    (1952), and extended in Schmidt and Witte (1989).",
    "version": "0.17.3",
    "maintainer": "Andreas Beger <adbeger@gmail.com>",
    "author": "Andreas Beger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1883-3169>),\n  Daina Chiba [aut] (ORCID: <https://orcid.org/0000-0002-9208-4373>),\n  Daniel W. Hill, Jr. [aut],\n  Nils W. Metternich [aut] (ORCID:\n    <https://orcid.org/0000-0001-8757-0409>),\n  Shahryar Minhas [aut],\n  Michael D. Ward [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6561-6186>)",
    "url": "https://github.com/andybega/spduration,\nhttps://www.andybeger.com/spduration/,\nhttp://www.andybeger.com/spduration/",
    "bug_reports": "https://github.com/andybega/spduration/issues",
    "repository": "https://cran.r-project.org/package=spduration",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spduration Split-Population Duration (Cure) Regression An implementation of split-population duration regression models. \n    Unlike regular duration models, split-population duration models are\n    mixture models that accommodate the presence of a sub-population that is \n    not at risk for failure, e.g. cancer patients who have been cured by \n    treatment. This package implements Weibull and Loglogistic forms for the \n    duration component, and focuses on data with time-varying covariates. \n    These models were originally formulated in Boag (1949) and Berkson and Gage \n    (1952), and extended in Schmidt and Witte (1989).  "
  },
  {
    "id": 21273,
    "package_name": "spsurvey",
    "title": "Spatial Sampling Design and Analysis",
    "description": "A design-based approach to statistical inference, with a focus on spatial data. Spatially balanced samples are selected using the Generalized Random Tessellation Stratified (GRTS) algorithm. The GRTS algorithm can be applied to finite resources (point geometries) and infinite resources (linear / linestring and areal / polygon geometries) and flexibly accommodates a diverse set of sampling design features, including stratification, unequal inclusion probabilities, proportional (to size) inclusion probabilities, legacy (historical) sites, a minimum distance between sites, and two options for replacement sites (reverse hierarchical order and nearest neighbor). Data are analyzed using a wide range of analysis functions that perform categorical variable analysis, continuous variable analysis, attributable risk analysis, risk difference analysis, relative risk analysis, change analysis, and trend analysis. spsurvey can also be used to summarize objects, visualize objects, select samples that are not spatially balanced, select panel samples, measure the amount of spatial balance in a sample, adjust design weights, and more. For additional details, see Dumelle et al. (2023) <doi:10.18637/jss.v105.i03>.",
    "version": "5.6.0",
    "maintainer": "Michael Dumelle <Dumelle.Michael@epa.gov>",
    "author": "Michael Dumelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3393-5529>),\n  Tom Kincaid [aut],\n  Anthony (Tony) R. Olsen [aut],\n  Marc Weber [aut],\n  Don Stevens [ctb],\n  Denis White [ctb],\n  Amanda M. Nahlik [ctb],\n  Sarah Lehmann [ctb]",
    "url": "https://usepa.github.io/spsurvey/,\nhttps://github.com/USEPA/spsurvey",
    "bug_reports": "https://github.com/USEPA/spsurvey/issues",
    "repository": "https://cran.r-project.org/package=spsurvey",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spsurvey Spatial Sampling Design and Analysis A design-based approach to statistical inference, with a focus on spatial data. Spatially balanced samples are selected using the Generalized Random Tessellation Stratified (GRTS) algorithm. The GRTS algorithm can be applied to finite resources (point geometries) and infinite resources (linear / linestring and areal / polygon geometries) and flexibly accommodates a diverse set of sampling design features, including stratification, unequal inclusion probabilities, proportional (to size) inclusion probabilities, legacy (historical) sites, a minimum distance between sites, and two options for replacement sites (reverse hierarchical order and nearest neighbor). Data are analyzed using a wide range of analysis functions that perform categorical variable analysis, continuous variable analysis, attributable risk analysis, risk difference analysis, relative risk analysis, change analysis, and trend analysis. spsurvey can also be used to summarize objects, visualize objects, select samples that are not spatially balanced, select panel samples, measure the amount of spatial balance in a sample, adjust design weights, and more. For additional details, see Dumelle et al. (2023) <doi:10.18637/jss.v105.i03>.  "
  },
  {
    "id": 21305,
    "package_name": "ss3sim",
    "title": "Fisheries Stock Assessment Simulation Testing with Stock\nSynthesis",
    "description": "Develops a framework for fisheries stock assessment simulation\n    testing with Stock Synthesis (SS) as described in Anderson et al.\n    (2014) <doi:10.1371/journal.pone.0092725>.",
    "version": "1.0.3",
    "maintainer": "Kelli F. Johnson <kelli.johnson@noaa.gov>",
    "author": "Kelli F. Johnson [aut, cre],\n  Sean C. Anderson [aut] (ORCID: <https://orcid.org/0000-0001-9563-1937>),\n  Kathryn Doering [aut],\n  Cole Monnahan [aut],\n  Christine Stawitz [aut],\n  Ian Taylor [aut],\n  Curry Cunningham [ctb],\n  Allan Hicks [ctb],\n  Felipe Hurtado-Ferro [ctb],\n  Peter Kuriyama [ctb],\n  Roberto Licandeo [ctb],\n  Carey McGilliard [ctb],\n  Melissa Murdian [ctb],\n  Kotaro Ono [ctb],\n  Merrill Rudd [ctb],\n  Cody Szuwalski [ctb],\n  Juan Valero [ctb],\n  Athol Whitten [ctb]",
    "url": "https://github.com/ss3sim/ss3sim",
    "bug_reports": "https://github.com/ss3sim/ss3sim/issues",
    "repository": "https://cran.r-project.org/package=ss3sim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ss3sim Fisheries Stock Assessment Simulation Testing with Stock\nSynthesis Develops a framework for fisheries stock assessment simulation\n    testing with Stock Synthesis (SS) as described in Anderson et al.\n    (2014) <doi:10.1371/journal.pone.0092725>.  "
  },
  {
    "id": 21332,
    "package_name": "sspm",
    "title": "Spatial Surplus Production Model Framework for Northern Shrimp\nPopulations",
    "description": "Implement a GAM-based (Generalized Additive Models) spatial surplus \n    production model (spatial SPM), aimed at modeling northern shrimp population \n    in Atlantic Canada but potentially to any stock in any location. The package \n    is opinionated in its implementation of SPMs as it internally makes the choice \n    to use penalized spatial gams with time lags. However, it also aims to provide \n    options for the user to customize their model. The methods are described in \n    Pedersen et al. (2022, <https://www.dfo-mpo.gc.ca/csas-sccs/Publications/ResDocs-DocRech/2022/2022_062-eng.html>).",
    "version": "1.1.0",
    "maintainer": "Valentin Lucet <valentin.lucet@gmail.com>",
    "author": "Valentin Lucet [aut, cre, cph],\n  Eric Pedersen [aut]",
    "url": "https://pedersen-fisheries-lab.github.io/sspm/",
    "bug_reports": "https://github.com/pedersen-fisheries-lab/sspm/issues",
    "repository": "https://cran.r-project.org/package=sspm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sspm Spatial Surplus Production Model Framework for Northern Shrimp\nPopulations Implement a GAM-based (Generalized Additive Models) spatial surplus \n    production model (spatial SPM), aimed at modeling northern shrimp population \n    in Atlantic Canada but potentially to any stock in any location. The package \n    is opinionated in its implementation of SPMs as it internally makes the choice \n    to use penalized spatial gams with time lags. However, it also aims to provide \n    options for the user to customize their model. The methods are described in \n    Pedersen et al. (2022, <https://www.dfo-mpo.gc.ca/csas-sccs/Publications/ResDocs-DocRech/2022/2022_062-eng.html>).  "
  },
  {
    "id": 21368,
    "package_name": "standartox",
    "title": "Ecotoxicological Information from the Standartox Database",
    "description": "The <http://standartox.uni-landau.de> database offers cleaned,\n    harmonized and aggregated ecotoxicological test data, which can\n    be used for assessing effects and risks of chemical concentrations\n    found in the environment.",
    "version": "0.0.2",
    "maintainer": "Andreas Scharm\u00fcller <andschar@protonmail.com>",
    "author": "Andreas Scharm\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9290-3965>)",
    "url": "https://github.com/andschar/standartox",
    "bug_reports": "https://github.com/andschar/standartox/issues",
    "repository": "https://cran.r-project.org/package=standartox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "standartox Ecotoxicological Information from the Standartox Database The <http://standartox.uni-landau.de> database offers cleaned,\n    harmonized and aggregated ecotoxicological test data, which can\n    be used for assessing effects and risks of chemical concentrations\n    found in the environment.  "
  },
  {
    "id": 21386,
    "package_name": "starvars",
    "title": "Vector Logistic Smooth Transition Models Estimation and\nPrediction",
    "description": "Allows the user to estimate a vector logistic smooth transition autoregressive model via maximum log-likelihood or nonlinear least squares. It further permits to test for linearity in the multivariate framework against a vector logistic smooth transition autoregressive model with a single transition variable. The estimation method is discussed in Terasvirta and Yang (2014, <doi:10.1108/S0731-9053(2013)0000031008>). Also, realized covariances can be constructed from stock market prices or returns, as explained in Andersen et al. (2001, <doi:10.1016/S0304-405X(01)00055-1>).",
    "version": "1.1.10",
    "maintainer": "Andrea Bucci <andrea.bucci@unich.it>",
    "author": "Andrea Bucci [aut, cre, cph],\n  Giulio Palomba [aut],\n  Eduardo Rossi [aut],\n  Andrea Faragalli [ctb]",
    "url": "https://github.com/andbucci/starvars",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=starvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starvars Vector Logistic Smooth Transition Models Estimation and\nPrediction Allows the user to estimate a vector logistic smooth transition autoregressive model via maximum log-likelihood or nonlinear least squares. It further permits to test for linearity in the multivariate framework against a vector logistic smooth transition autoregressive model with a single transition variable. The estimation method is discussed in Terasvirta and Yang (2014, <doi:10.1108/S0731-9053(2013)0000031008>). Also, realized covariances can be constructed from stock market prices or returns, as explained in Andersen et al. (2001, <doi:10.1016/S0304-405X(01)00055-1>).  "
  },
  {
    "id": 21418,
    "package_name": "statnipokladna",
    "title": "Use Data from the Czech Public Finance Database",
    "description": "Get programmatic access to data from the Czech public\n    budgeting and accounting database, St\u00e1tn\u00ed pokladna\n    <https://monitor.statnipokladna.gov.cz/>.",
    "version": "0.7.7",
    "maintainer": "Petr Bouchal <pbouchal@gmail.com>",
    "author": "Petr Bouchal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0471-716X>)",
    "url": "https://github.com/petrbouchal/statnipokladna,\nhttps://petrbouchal.xyz/statnipokladna/",
    "bug_reports": "https://github.com/petrbouchal/statnipokladna/issues",
    "repository": "https://cran.r-project.org/package=statnipokladna",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statnipokladna Use Data from the Czech Public Finance Database Get programmatic access to data from the Czech public\n    budgeting and accounting database, St\u00e1tn\u00ed pokladna\n    <https://monitor.statnipokladna.gov.cz/>.  "
  },
  {
    "id": 21481,
    "package_name": "stochLAB",
    "title": "Stochastic Collision Risk Model",
    "description": "Collision Risk Models for avian fauna (seabird and migratory birds) at\n    offshore wind farms. The base deterministic model is derived from \n    Band (2012) <https://tethys.pnnl.gov/publications/using-collision-risk-model-assess-bird-collision-risks-offshore-wind-farms>. \n    This was further expanded on by Masden (2015) <doi:10.7489/1659-1> and code \n    used here is heavily derived from this work with input from Dr A. Cook at the \n    British Trust for Ornithology. These collision risk models are useful for \n    marine ornithologists who are working in the offshore wind industry, particularly \n    in UK waters. However, many of the species included in the stochastic collision \n    risk models can also be found in the North Atlantic in the \n    United States and Canada, and could be applied there. ",
    "version": "1.1.2",
    "maintainer": "Grant Humphries <grwhumphries@blackbawks.net>",
    "author": "Grant Humphries [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5783-9892>),\n  Bruno Caneco [aut],\n  Aonghais Cook [aut],\n  Elizabeth Masden [aut],\n  Marine Scotland [fnd, cph],\n  HiDef [cph],\n  DMPstats [cph],\n  Kelly Street [rev] (GITHUB: @kstreet13)",
    "url": "https://github.com/HiDef-Aerial-Surveying/stochLAB,\nhttps://hidef-aerial-surveying.github.io/stochLAB/",
    "bug_reports": "https://github.com/HiDef-Aerial-Surveying/stochLAB/issues",
    "repository": "https://cran.r-project.org/package=stochLAB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stochLAB Stochastic Collision Risk Model Collision Risk Models for avian fauna (seabird and migratory birds) at\n    offshore wind farms. The base deterministic model is derived from \n    Band (2012) <https://tethys.pnnl.gov/publications/using-collision-risk-model-assess-bird-collision-risks-offshore-wind-farms>. \n    This was further expanded on by Masden (2015) <doi:10.7489/1659-1> and code \n    used here is heavily derived from this work with input from Dr A. Cook at the \n    British Trust for Ornithology. These collision risk models are useful for \n    marine ornithologists who are working in the offshore wind industry, particularly \n    in UK waters. However, many of the species included in the stochastic collision \n    risk models can also be found in the North Atlantic in the \n    United States and Canada, and could be applied there.   "
  },
  {
    "id": 21486,
    "package_name": "stockAnalyst",
    "title": "Equity Valuation using Methods of Fundamental Analysis",
    "description": "Methods of Fundamental Analysis for Valuation of Equity included here serve as a quick reference for undergraduate courses on Stock Valuation and Chartered Financial Analyst Levels 1 and 2 Readings on Equity Valuation.\n    Jerald E. Pinto (\u201cEquity Asset Valuation (4th Edition)\u201d, 2020, ISBN: 9781119628194).\n    Chartered Financial Analyst Institute (\"Chartered Financial Analyst Program Curriculum 2020 Level I Volumes 1-6. (Vol. 4, pp. 445-491)\", 2019, ISBN: 9781119593577).\n    Chartered Financial Analyst Institute (\"Chartered Financial Analyst Program Curriculum 2020 Level II Volumes 1-6. (Vol. 4, pp. 197-447)\", 2019, ISBN: 9781119593614).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stockAnalyst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stockAnalyst Equity Valuation using Methods of Fundamental Analysis Methods of Fundamental Analysis for Valuation of Equity included here serve as a quick reference for undergraduate courses on Stock Valuation and Chartered Financial Analyst Levels 1 and 2 Readings on Equity Valuation.\n    Jerald E. Pinto (\u201cEquity Asset Valuation (4th Edition)\u201d, 2020, ISBN: 9781119628194).\n    Chartered Financial Analyst Institute (\"Chartered Financial Analyst Program Curriculum 2020 Level I Volumes 1-6. (Vol. 4, pp. 445-491)\", 2019, ISBN: 9781119593577).\n    Chartered Financial Analyst Institute (\"Chartered Financial Analyst Program Curriculum 2020 Level II Volumes 1-6. (Vol. 4, pp. 197-447)\", 2019, ISBN: 9781119593614).  "
  },
  {
    "id": 21487,
    "package_name": "stockR",
    "title": "Identifying Stocks in Genetic Data",
    "description": "Provides a mixture model for clustering individuals (or sampling groups) into stocks based on their genetic profile. Here, sampling groups are individuals that are sure to come from the same stock (e.g. breeding adults or larvae). The mixture (log-)likelihood is maximised using the EM-algorithm after finding good starting values via a K-means clustering of the genetic data. Details can be found in: Foster, S. D.; Feutry, P.; Grewe, P. M.; Berry, O.; Hui, F. K. C. & Davies (2020) <doi:10.1111/1755-0998.12920>.",
    "version": "1.0.76",
    "maintainer": "Scott D. Foster <scott.foster@data61.csiro.au>",
    "author": "Scott D. Foster [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stockR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stockR Identifying Stocks in Genetic Data Provides a mixture model for clustering individuals (or sampling groups) into stocks based on their genetic profile. Here, sampling groups are individuals that are sure to come from the same stock (e.g. breeding adults or larvae). The mixture (log-)likelihood is maximised using the EM-algorithm after finding good starting values via a K-means clustering of the genetic data. Details can be found in: Foster, S. D.; Feutry, P.; Grewe, P. M.; Berry, O.; Hui, F. K. C. & Davies (2020) <doi:10.1111/1755-0998.12920>.  "
  },
  {
    "id": 21488,
    "package_name": "stocks",
    "title": "Stock Market Analysis",
    "description": "Functions for analyzing stocks or other investments. Main features are loading and aligning historical data for ticker symbols, calculating performance metrics for individual funds or portfolios (e.g. annualized growth, maximum drawdown, Sharpe/Sortino ratio), and creating graphs. C++ code is used to improve processing speed where possible.",
    "version": "1.1.4",
    "maintainer": "Dane R. Van Domelen <vandomed@gmail.com>",
    "author": "Dane R. Van Domelen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stocks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stocks Stock Market Analysis Functions for analyzing stocks or other investments. Main features are loading and aligning historical data for ticker symbols, calculating performance metrics for individual funds or portfolios (e.g. annualized growth, maximum drawdown, Sharpe/Sortino ratio), and creating graphs. C++ code is used to improve processing speed where possible.  "
  },
  {
    "id": 21506,
    "package_name": "strand",
    "title": "A Framework for Investment Strategy Simulation",
    "description": "Provides a framework for performing discrete (share-level) simulations of\n  investment strategies. Simulated portfolios optimize exposure to an input signal subject\n  to constraints such as position size and factor exposure. For background see L. Chincarini\n  and D. Kim (2010, ISBN:978-0-07-145939-6) \"Quantitative Equity Portfolio Management\".",
    "version": "0.2.2",
    "maintainer": "Jeff Enos <jeffrey.enos@gmail.com>",
    "author": "Jeff Enos [cre, aut, cph],\n  David Kane [aut],\n  Ben Czekanski [ctb],\n  Robert Hoover [ctb],\n  Jack Luby [ctb],\n  Nils Wallin [ctb]",
    "url": "https://github.com/strand-tech/strand",
    "bug_reports": "https://github.com/strand-tech/strand/issues",
    "repository": "https://cran.r-project.org/package=strand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strand A Framework for Investment Strategy Simulation Provides a framework for performing discrete (share-level) simulations of\n  investment strategies. Simulated portfolios optimize exposure to an input signal subject\n  to constraints such as position size and factor exposure. For background see L. Chincarini\n  and D. Kim (2010, ISBN:978-0-07-145939-6) \"Quantitative Equity Portfolio Management\".  "
  },
  {
    "id": 21535,
    "package_name": "stressr",
    "title": "Fetch and plot financial stress index and component data",
    "description": "Forms queries to submit to the Cleveland Federal Reserve Bank web\n    site's financial stress index data site.  Provides query functions for both\n    the composite stress index and the components data. By default the download\n    includes daily time series data starting September 25, 1991.  The functions\n    return a class of either type easing or cfsi which contain a list of items\n    related to the query and its graphical presentation.  The list includes the\n    time series data as an xts object.  The package provides four lattice time\n    series plots to render the time series data in a manner similar to the\n    bank's own presentation.",
    "version": "1.0.0",
    "maintainer": "Matt Barry <mrb@softisms.com>",
    "author": "Matt Barry <mrb@softisms.com>",
    "url": "https://github.com/mrbcuda/stressr",
    "bug_reports": "https://github.com/mrbcuda/stressr/issues",
    "repository": "https://cran.r-project.org/package=stressr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stressr Fetch and plot financial stress index and component data Forms queries to submit to the Cleveland Federal Reserve Bank web\n    site's financial stress index data site.  Provides query functions for both\n    the composite stress index and the components data. By default the download\n    includes daily time series data starting September 25, 1991.  The functions\n    return a class of either type easing or cfsi which contain a list of items\n    related to the query and its graphical presentation.  The list includes the\n    time series data as an xts object.  The package provides four lattice time\n    series plots to render the time series data in a manner similar to the\n    bank's own presentation.  "
  },
  {
    "id": 21578,
    "package_name": "success",
    "title": "Survival Control Charts Estimation Software",
    "description": "Quality control charts for survival outcomes.\n    Allows users to construct the Continuous Time Generalized\n    Rapid Response CUSUM (CGR-CUSUM) <doi:10.1093/biostatistics/kxac041>, \n    the Biswas & Kalbfleisch (2008)  <doi:10.1002/sim.3216> CUSUM, \n    the Bernoulli CUSUM and the risk-adjusted funnel plot for survival data \n    <doi:10.1002/sim.1970>. \n    These procedures can be used to monitor survival processes for a change \n    in the failure rate.",
    "version": "1.1.1",
    "maintainer": "Daniel Gomon <dgstatsoft@gmail.com>",
    "author": "Daniel Gomon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9011-3743>),\n  Mirko Signorelli [ctb] (ORCID: <https://orcid.org/0000-0002-8102-3356>)",
    "url": "https://github.com/d-gomon/success",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=success",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "success Survival Control Charts Estimation Software Quality control charts for survival outcomes.\n    Allows users to construct the Continuous Time Generalized\n    Rapid Response CUSUM (CGR-CUSUM) <doi:10.1093/biostatistics/kxac041>, \n    the Biswas & Kalbfleisch (2008)  <doi:10.1002/sim.3216> CUSUM, \n    the Bernoulli CUSUM and the risk-adjusted funnel plot for survival data \n    <doi:10.1002/sim.1970>. \n    These procedures can be used to monitor survival processes for a change \n    in the failure rate.  "
  },
  {
    "id": 21633,
    "package_name": "survC1",
    "title": "C-Statistics for Risk Prediction Models with Censored Survival\nData",
    "description": "Performs inference for C of risk prediction models with censored survival data, using the method proposed by Uno et al. (2011) <doi:10.1002/sim.4154>. Inference for the difference in C between two competing prediction models is also implemented.",
    "version": "1.0-3",
    "maintainer": "Hajime Uno <huno@jimmy.harvard.edu>",
    "author": "Hajime Uno",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survC1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survC1 C-Statistics for Risk Prediction Models with Censored Survival\nData Performs inference for C of risk prediction models with censored survival data, using the method proposed by Uno et al. (2011) <doi:10.1002/sim.4154>. Inference for the difference in C between two competing prediction models is also implemented.  "
  },
  {
    "id": 21634,
    "package_name": "survCurve",
    "title": "Plots Survival Curves Element by Element",
    "description": "Plots survival models from the 'survival' package. Additionally, it\n    plots curves of multistate models from the 'mstate' package. Typically, a\n    plot is drawn by the sequence survplot(), confIntArea(), survCurve() and\n    nrAtRisk(). The separation of the plot in this 4 functions allows for great\n    flexibility to make a custom plot for publication.",
    "version": "1.0",
    "maintainer": "Melchior Burri <melchiorburri@msn.com>",
    "author": "Melchior Burri [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survCurve",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survCurve Plots Survival Curves Element by Element Plots survival models from the 'survival' package. Additionally, it\n    plots curves of multistate models from the 'mstate' package. Typically, a\n    plot is drawn by the sequence survplot(), confIntArea(), survCurve() and\n    nrAtRisk(). The separation of the plot in this 4 functions allows for great\n    flexibility to make a custom plot for publication.  "
  },
  {
    "id": 21637,
    "package_name": "survIDINRI",
    "title": "IDI and NRI for Comparing Competing Risk Prediction Models with\nCensored Survival Data",
    "description": "Performs inference for a class of measures to compare competing risk prediction models with censored survival data. The class includes the integrated discrimination improvement index (IDI) and category-less net reclassification index (NRI).",
    "version": "1.1-2",
    "maintainer": "Hajime Uno <huno@jimmy.harvard.edu>",
    "author": "Hajime Uno, Tianxi Cai",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survIDINRI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survIDINRI IDI and NRI for Comparing Competing Risk Prediction Models with\nCensored Survival Data Performs inference for a class of measures to compare competing risk prediction models with censored survival data. The class includes the integrated discrimination improvement index (IDI) and category-less net reclassification index (NRI).  "
  },
  {
    "id": 21649,
    "package_name": "survSens",
    "title": "Sensitivity Analysis with Time-to-Event Outcomes",
    "description": "Performs a dual-parameter sensitivity analysis of treatment effect to unmeasured confounding in observational studies with either survival or competing risks outcomes. Huang, R., Xu, R. and Dulai, P.S.(2020) <doi:10.1002/sim.8672>.",
    "version": "1.1.0",
    "maintainer": "Rong Huang <roh019@ucsd.edu>",
    "author": "Rong Huang, Ronghui (Lily) Xu",
    "url": "https://github.com/Rong0707/survSens",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survSens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survSens Sensitivity Analysis with Time-to-Event Outcomes Performs a dual-parameter sensitivity analysis of treatment effect to unmeasured confounding in observational studies with either survival or competing risks outcomes. Huang, R., Xu, R. and Dulai, P.S.(2020) <doi:10.1002/sim.8672>.  "
  },
  {
    "id": 21653,
    "package_name": "surveil",
    "title": "Time Series Models for Disease Surveillance",
    "description": "Fits time trend models for routine disease surveillance tasks and returns probability distributions for a variety of quantities of interest, including age-standardized rates, period and cumulative percent change, and measures of health inequality. The models are appropriate for count data such as disease incidence and mortality data, employing a Poisson or binomial likelihood and the first-difference (random-walk) prior for unknown risk. Optionally add a covariance matrix for multiple, correlated time series models. Inference is completed using Markov chain Monte Carlo via the Stan modeling language. References: Donegan, Hughes, and Lee (2022) <doi:10.2196/34589>; Stan Development Team (2021) <https://mc-stan.org>; Theil (1972, ISBN:0-444-10378-3).",
    "version": "0.3.0",
    "maintainer": "Connor Donegan <connor.donegan@gmail.com>",
    "author": "Connor Donegan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9698-5443>)",
    "url": "https://connordonegan.github.io/surveil/,\nhttps://github.com/ConnorDonegan/surveil/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=surveil",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "surveil Time Series Models for Disease Surveillance Fits time trend models for routine disease surveillance tasks and returns probability distributions for a variety of quantities of interest, including age-standardized rates, period and cumulative percent change, and measures of health inequality. The models are appropriate for count data such as disease incidence and mortality data, employing a Poisson or binomial likelihood and the first-difference (random-walk) prior for unknown risk. Optionally add a covariance matrix for multiple, correlated time series models. Inference is completed using Markov chain Monte Carlo via the Stan modeling language. References: Donegan, Hughes, and Lee (2022) <doi:10.2196/34589>; Stan Development Team (2021) <https://mc-stan.org>; Theil (1972, ISBN:0-444-10378-3).  "
  },
  {
    "id": 21679,
    "package_name": "survivalmodels",
    "title": "Models for Survival Analysis",
    "description": "Implementations of classical and machine learning models for survival analysis, including deep neural networks via 'keras' and 'tensorflow'. Each model includes a separated fit and predict interface with consistent prediction types for predicting risk or survival probabilities. Models are either implemented from 'Python' via 'reticulate' <https://CRAN.R-project.org/package=reticulate>, from code in GitHub packages, or novel implementations using 'Rcpp' <https://CRAN.R-project.org/package=Rcpp>. Neural networks are implemented from the 'Python' package 'pycox' <https://github.com/havakv/pycox>.",
    "version": "0.1.191",
    "maintainer": "Yohann Foucher <yohann.foucher@univ-poitiers.fr>",
    "author": "Raphael Sonabend [aut] (ORCID: <https://orcid.org/0000-0001-9225-4654>),\n  Yohann Foucher [cre] (ORCID: <https://orcid.org/0000-0003-0330-7457>)",
    "url": "https://github.com/RaphaelS1/survivalmodels/",
    "bug_reports": "https://github.com/foucher-y/survivalmodels/issues",
    "repository": "https://cran.r-project.org/package=survivalmodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survivalmodels Models for Survival Analysis Implementations of classical and machine learning models for survival analysis, including deep neural networks via 'keras' and 'tensorflow'. Each model includes a separated fit and predict interface with consistent prediction types for predicting risk or survival probabilities. Models are either implemented from 'Python' via 'reticulate' <https://CRAN.R-project.org/package=reticulate>, from code in GitHub packages, or novel implementations using 'Rcpp' <https://CRAN.R-project.org/package=Rcpp>. Neural networks are implemented from the 'Python' package 'pycox' <https://github.com/havakv/pycox>.  "
  },
  {
    "id": 21683,
    "package_name": "survminer",
    "title": "Drawing Survival Curves using 'ggplot2'",
    "description": "Contains the function 'ggsurvplot()' for drawing easily beautiful\n    and 'ready-to-publish' survival curves with the 'number at risk' table\n    and 'censoring count plot'. Other functions are also available to plot \n    adjusted curves for `Cox` model and to visually examine 'Cox' model assumptions.",
    "version": "0.5.1",
    "maintainer": "Alboukadel Kassambara <alboukadel.kassambara@gmail.com>",
    "author": "Alboukadel Kassambara [aut, cre],\n  Marcin Kosinski [aut],\n  Przemyslaw Biecek [aut],\n  Scheipl Fabian [ctb]",
    "url": "https://rpkgs.datanovia.com/survminer/index.html",
    "bug_reports": "https://github.com/kassambara/survminer/issues",
    "repository": "https://cran.r-project.org/package=survminer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survminer Drawing Survival Curves using 'ggplot2' Contains the function 'ggsurvplot()' for drawing easily beautiful\n    and 'ready-to-publish' survival curves with the 'number at risk' table\n    and 'censoring count plot'. Other functions are also available to plot \n    adjusted curves for `Cox` model and to visually examine 'Cox' model assumptions.  "
  },
  {
    "id": 21688,
    "package_name": "survsim",
    "title": "Simulation of Simple and Complex Survival Data",
    "description": "Simulation of simple and complex survival data including recurrent and multiple events and competing risks. See Mori\u00f1a D, Navarro A. (2014) <doi:10.18637/jss.v059.i02> and Mori\u00f1a D, Navarro A. (2017) <doi:10.1080/03610918.2016.1175621>.",
    "version": "1.1.8",
    "maintainer": "David Mori\u00f1a Soler <dmorina@ub.edu>",
    "author": "David Mori\u00f1a Soler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5949-7443>),\n  Albert Navarro [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survsim Simulation of Simple and Complex Survival Data Simulation of simple and complex survival data including recurrent and multiple events and competing risks. See Mori\u00f1a D, Navarro A. (2014) <doi:10.18637/jss.v059.i02> and Mori\u00f1a D, Navarro A. (2017) <doi:10.1080/03610918.2016.1175621>.  "
  },
  {
    "id": 21706,
    "package_name": "svenssonm",
    "title": "Svensson's Method",
    "description": "Obtain parameters of Svensson's Method, including percentage agreement, \n    systematic change and individual change. Also, the contingency table can be generated. \n    Svensson's Method is a rank-invariant nonparametric method for the analysis of ordered scales \n    which measures the level of change both from systematic and individual aspects. For the details, \n    please refer to Svensson E. Analysis of systematic and random differences between paired ordinal \n    categorical data [dissertation]. Stockholm: Almqvist & Wiksell International; 1993.",
    "version": "0.1.0",
    "maintainer": "Yingyan Zhu <lexi.yy.zhu@gmail.com>",
    "author": "Yingyan Zhu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svenssonm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svenssonm Svensson's Method Obtain parameters of Svensson's Method, including percentage agreement, \n    systematic change and individual change. Also, the contingency table can be generated. \n    Svensson's Method is a rank-invariant nonparametric method for the analysis of ordered scales \n    which measures the level of change both from systematic and individual aspects. For the details, \n    please refer to Svensson E. Analysis of systematic and random differences between paired ordinal \n    categorical data [dissertation]. Stockholm: Almqvist & Wiksell International; 1993.  "
  },
  {
    "id": 21773,
    "package_name": "synthReturn",
    "title": "Synthetic Matching Method for Returns",
    "description": "Implements the revised Synthetic Matching Algorithm of Kreitmeir, Lane, and Raschky (2025) <doi:10.2139/ssrn.3751162>, building on \n  the original approach of Acemoglu, Johnson, Kermani, Kwak, and Mitton (2016) <doi:10.1016/j.jfineco.2015.10.001>, to estimate the cumulative \n  treatment effect of an event on treated firms\u2019 stock returns.",
    "version": "1.0.0",
    "maintainer": "David H Kreitmeir <david.kreitmeir1@monash.edu>",
    "author": "David H Kreitmeir [aut, cre],\n  Christian D\u00fcben [ctb]",
    "url": "https://github.com/davidkreitmeir/synthReturn",
    "bug_reports": "https://github.com/davidkreitmeir/synthReturn/issues",
    "repository": "https://cran.r-project.org/package=synthReturn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "synthReturn Synthetic Matching Method for Returns Implements the revised Synthetic Matching Algorithm of Kreitmeir, Lane, and Raschky (2025) <doi:10.2139/ssrn.3751162>, building on \n  the original approach of Acemoglu, Johnson, Kermani, Kwak, and Mitton (2016) <doi:10.1016/j.jfineco.2015.10.001>, to estimate the cumulative \n  treatment effect of an event on treated firms\u2019 stock returns.  "
  },
  {
    "id": 21777,
    "package_name": "synthpop",
    "title": "Generating Synthetic Versions of Sensitive Microdata for\nStatistical Disclosure Control",
    "description": "A tool for producing synthetic versions of microdata containing confidential information so that they are safe to be released to users for exploratory analysis. The key objective of generating synthetic data is to replace sensitive original values with synthetic ones causing minimal distortion of the statistical information contained in the data set. Variables, which can be categorical or continuous, are synthesised one-by-one using sequential modelling. Replacements are generated by drawing from conditional distributions fitted to the original data using parametric or classification and regression trees models. Data are synthesised via the function syn() which can be largely automated, if default settings are used, or with methods defined by the user. Optional parameters can be used to influence the disclosure risk and the analytical quality of the synthesised data. For a description of the implemented method see Nowok, Raab and Dibben (2016) <doi:10.18637/jss.v074.i11>. Functions to assess identity and attribute disclosure for the original and for the synthetic data are included in the package, and their use is illustrated in a vignette on disclosure (Practical Privacy Metrics for Synthetic Data).",
    "version": "1.9-2",
    "maintainer": "Beata Nowok <beata.nowok@gmail.com>",
    "author": "Beata Nowok [aut, cre],\n  Gillian M Raab [aut],\n  Chris Dibben [ctb],\n  Joshua Snoke [ctb],\n  Caspar van Lissa [ctb],\n  Lotte Pater [ctb]",
    "url": "<https://www.synthpop.org.uk/>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=synthpop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "synthpop Generating Synthetic Versions of Sensitive Microdata for\nStatistical Disclosure Control A tool for producing synthetic versions of microdata containing confidential information so that they are safe to be released to users for exploratory analysis. The key objective of generating synthetic data is to replace sensitive original values with synthetic ones causing minimal distortion of the statistical information contained in the data set. Variables, which can be categorical or continuous, are synthesised one-by-one using sequential modelling. Replacements are generated by drawing from conditional distributions fitted to the original data using parametric or classification and regression trees models. Data are synthesised via the function syn() which can be largely automated, if default settings are used, or with methods defined by the user. Optional parameters can be used to influence the disclosure risk and the analytical quality of the synthesised data. For a description of the implemented method see Nowok, Raab and Dibben (2016) <doi:10.18637/jss.v074.i11>. Functions to assess identity and attribute disclosure for the original and for the synthetic data are included in the package, and their use is illustrated in a vignette on disclosure (Practical Privacy Metrics for Synthetic Data).  "
  },
  {
    "id": 21784,
    "package_name": "systemicrisk",
    "title": "Systemic Risk and Network Reconstruction",
    "description": "Analysis of risk through liability matrices. Contains a Gibbs sampler for network reconstruction,  where only row and column sums of the liabilities matrix as well as some other fixed entries are observed, following the methodology of Gandy&Veraart (2016) <doi:10.1287/mnsc.2016.2546>. It also incorporates models that use a power law distribution on the degree distribution.",
    "version": "0.4.3",
    "maintainer": "Axel Gandy <a.gandy@imperial.ac.uk>",
    "author": "Axel Gandy [aut, cre],\n  Luitgard A.M. Veraart [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=systemicrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "systemicrisk Systemic Risk and Network Reconstruction Analysis of risk through liability matrices. Contains a Gibbs sampler for network reconstruction,  where only row and column sums of the liabilities matrix as well as some other fixed entries are observed, following the methodology of Gandy&Veraart (2016) <doi:10.1287/mnsc.2016.2546>. It also incorporates models that use a power law distribution on the degree distribution.  "
  },
  {
    "id": 21837,
    "package_name": "tailplots",
    "title": "Estimators and Plots for Gamma and Pareto Tail Detection",
    "description": "Estimators for two functionals used to detect Gamma, Pareto or Lognormal distributions, as well as distributions exhibiting similar tail behavior, as introduced by Iwashita and Klar (2023) <doi:10.1111/stan.12316> and \n            Klar (2024) <doi:10.1080/00031305.2024.2413081>. \n            One of these functionals, g, originally proposed by Asmussen and Lehtomaa (2017) <doi:10.3390/risks5010010>, distinguishes between log-convex and log-concave tail behavior. \n            Furthermore the characterization of the lognormal distribution is based on the work of Mosimann (1970) <doi:10.2307/2284599>.\n            The package also includes methods for visualizing these estimators and their associated confidence intervals across various threshold values.",
    "version": "0.1.1",
    "maintainer": "Bernhard Klar <Bernhard.Klar@kit.edu>",
    "author": "Bernhard Klar [aut, cre],\n  Lucas Iglesias [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tailplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tailplots Estimators and Plots for Gamma and Pareto Tail Detection Estimators for two functionals used to detect Gamma, Pareto or Lognormal distributions, as well as distributions exhibiting similar tail behavior, as introduced by Iwashita and Klar (2023) <doi:10.1111/stan.12316> and \n            Klar (2024) <doi:10.1080/00031305.2024.2413081>. \n            One of these functionals, g, originally proposed by Asmussen and Lehtomaa (2017) <doi:10.3390/risks5010010>, distinguishes between log-convex and log-concave tail behavior. \n            Furthermore the characterization of the lognormal distribution is based on the work of Mosimann (1970) <doi:10.2307/2284599>.\n            The package also includes methods for visualizing these estimators and their associated confidence intervals across various threshold values.  "
  },
  {
    "id": 21849,
    "package_name": "targeted",
    "title": "Targeted Inference",
    "description": "Various methods for targeted and semiparametric inference including\n\t     augmented inverse probability weighted (AIPW) estimators for missing data and\n\t     causal inference (Bang and Robins (2005) <doi:10.1111/j.1541-0420.2005.00377.x>),\n         variable importance and conditional average treatment effects (CATE)\n         (van der Laan (2006) <doi:10.2202/1557-4679.1008>),\n\t     estimators for risk differences and relative risks (Richardson et al. (2017)\n\t     <doi:10.1080/01621459.2016.1192546>), assumption lean inference for generalized\n         linear model parameters (Vansteelandt et al. (2022) <doi:10.1111/rssb.12504>).",
    "version": "0.7",
    "maintainer": "Klaus K. Holst <klaus@holst.it>",
    "author": "Klaus K. Holst [aut, cre],\n  Benedikt Sommer [aut],\n  Andreas Nordland [aut],\n  Christian B. Pipper [ctb]",
    "url": "https://kkholst.github.io/targeted/",
    "bug_reports": "https://github.com/kkholst/targeted/issues",
    "repository": "https://cran.r-project.org/package=targeted",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "targeted Targeted Inference Various methods for targeted and semiparametric inference including\n\t     augmented inverse probability weighted (AIPW) estimators for missing data and\n\t     causal inference (Bang and Robins (2005) <doi:10.1111/j.1541-0420.2005.00377.x>),\n         variable importance and conditional average treatment effects (CATE)\n         (van der Laan (2006) <doi:10.2202/1557-4679.1008>),\n\t     estimators for risk differences and relative risks (Richardson et al. (2017)\n\t     <doi:10.1080/01621459.2016.1192546>), assumption lean inference for generalized\n         linear model parameters (Vansteelandt et al. (2022) <doi:10.1111/rssb.12504>).  "
  },
  {
    "id": 21881,
    "package_name": "td",
    "title": "Access to the 'twelvedata' Financial Data API",
    "description": "The 'twelvedata' REST service offers access to current and historical\n data on stocks, standard as well as digital 'crypto' currencies, and other financial\n assets covering a wide variety of course and time spans. See <https://twelvedata.com/>\n for details, to create an account, and to request an API key for free-but-capped access\n to the data.",
    "version": "0.0.6",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel and Kenneth Rose",
    "url": "https://dirk.eddelbuettel.com/code/td.html,\nhttps://github.com/eddelbuettel/td",
    "bug_reports": "https://github.com/eddelbuettel/td/issues",
    "repository": "https://cran.r-project.org/package=td",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "td Access to the 'twelvedata' Financial Data API The 'twelvedata' REST service offers access to current and historical\n data on stocks, standard as well as digital 'crypto' currencies, and other financial\n assets covering a wide variety of course and time spans. See <https://twelvedata.com/>\n for details, to create an account, and to request an API key for free-but-capped access\n to the data.  "
  },
  {
    "id": 21883,
    "package_name": "tdROC",
    "title": "Nonparametric Estimation of Time-Dependent ROC, Brier Score, and\nSurvival Difference from Right Censored Time-to-Event Data with\nor without Competing Risks",
    "description": "The tdROC package facilitates the estimation of time-dependent ROC \n    (Receiver Operating Characteristic) curves and the Area Under the time-dependent \n    ROC Curve (AUC) in the context of survival data, accommodating scenarios with \n    right censored data and the option to account for competing risks. In addition \n    to the ROC/AUC estimation, the package also estimates time-dependent Brier score and \n    survival difference. Confidence intervals of various estimated quantities can be \n    obtained from bootstrap. The package also offers plotting functions for visualizing \n    time-dependent ROC curves.",
    "version": "2.0",
    "maintainer": "Xiaoyang Li <xli35@mdanderson.org>",
    "author": "Xiaoyang Li [aut, cre],\n  Zhe Yin [aut],\n  Liang Li [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tdROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tdROC Nonparametric Estimation of Time-Dependent ROC, Brier Score, and\nSurvival Difference from Right Censored Time-to-Event Data with\nor without Competing Risks The tdROC package facilitates the estimation of time-dependent ROC \n    (Receiver Operating Characteristic) curves and the Area Under the time-dependent \n    ROC Curve (AUC) in the context of survival data, accommodating scenarios with \n    right censored data and the option to account for competing risks. In addition \n    to the ROC/AUC estimation, the package also estimates time-dependent Brier score and \n    survival difference. Confidence intervals of various estimated quantities can be \n    obtained from bootstrap. The package also offers plotting functions for visualizing \n    time-dependent ROC curves.  "
  },
  {
    "id": 22071,
    "package_name": "tidycmprsk",
    "title": "Competing Risks Estimation",
    "description": "Provides an intuitive interface for working with the\n    competing risk endpoints. The package wraps the 'cmprsk' package, and\n    exports functions for univariate cumulative incidence estimates and\n    competing risk regression. Methods follow those introduced in Fine and\n    Gray (1999) <doi:10.1002/sim.7501>.",
    "version": "1.1.1",
    "maintainer": "Daniel D. Sjoberg <danield.sjoberg@gmail.com>",
    "author": "Daniel D. Sjoberg [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0862-2018>),\n  Teng Fei [aut] (ORCID: <https://orcid.org/0000-0001-7888-1715>)",
    "url": "https://mskcc-epi-bio.github.io/tidycmprsk/,\nhttps://github.com/MSKCC-Epi-Bio/tidycmprsk",
    "bug_reports": "https://github.com/MSKCC-Epi-Bio/tidycmprsk/issues",
    "repository": "https://cran.r-project.org/package=tidycmprsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidycmprsk Competing Risks Estimation Provides an intuitive interface for working with the\n    competing risk endpoints. The package wraps the 'cmprsk' package, and\n    exports functions for univariate cumulative incidence estimates and\n    competing risk regression. Methods follow those introduced in Fine and\n    Gray (1999) <doi:10.1002/sim.7501>.  "
  },
  {
    "id": 22083,
    "package_name": "tidyedgar",
    "title": "Tidy Fundamental Financial Data from 'SEC's 'EDGAR' 'API'",
    "description": "Streamline the process of accessing fundamental financial data from the United States Securities and Exchange Commission's ('SEC') Electronic Data Gathering, Analysis, and Retrieval system ('EDGAR') 'API' <https://www.sec.gov/edgar/sec-api-documentation>, transforming it into a tidy, analysis-ready format.",
    "version": "1.0.1",
    "maintainer": "Gerard Gimenez-Adsuar <gerard@solucionsdedades.cat>",
    "author": "Gerard Gimenez-Adsuar [aut, cre]",
    "url": "https://gerardgimenezadsuar.github.io/tidyedgar/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidyedgar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyedgar Tidy Fundamental Financial Data from 'SEC's 'EDGAR' 'API' Streamline the process of accessing fundamental financial data from the United States Securities and Exchange Commission's ('SEC') Electronic Data Gathering, Analysis, and Retrieval system ('EDGAR') 'API' <https://www.sec.gov/edgar/sec-api-documentation>, transforming it into a tidy, analysis-ready format.  "
  },
  {
    "id": 22086,
    "package_name": "tidyfinance",
    "title": "Tidy Finance Helper Functions",
    "description": "Helper functions for empirical research in financial\n    economics, addressing a variety of topics covered in Scheuch, Voigt,\n    and Weiss (2023) <doi:10.1201/b23237>.  The package is designed to\n    provide shortcuts for issues extensively discussed in the book,\n    facilitating easier application of its concepts. For more information\n    and resources related to the book, visit\n    <https://www.tidy-finance.org/r/index.html>.",
    "version": "0.4.4",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>),\n  Stefan Voigt [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-5619-3161>),\n  Patrick Weiss [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-9282-5872>),\n  Maximilian M\u00fccke [ctb] (ORCID: <https://orcid.org/0009-0000-9432-9795>)",
    "url": "https://www.tidy-finance.org/r/,\nhttps://github.com/tidy-finance/r-tidyfinance",
    "bug_reports": "https://github.com/tidy-finance/r-tidyfinance/issues",
    "repository": "https://cran.r-project.org/package=tidyfinance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyfinance Tidy Finance Helper Functions Helper functions for empirical research in financial\n    economics, addressing a variety of topics covered in Scheuch, Voigt,\n    and Weiss (2023) <doi:10.1201/b23237>.  The package is designed to\n    provide shortcuts for issues extensively discussed in the book,\n    facilitating easier application of its concepts. For more information\n    and resources related to the book, visit\n    <https://www.tidy-finance.org/r/index.html>.  "
  },
  {
    "id": 22160,
    "package_name": "timeDate",
    "title": "Rmetrics - Chronological and Calendar Objects",
    "description": "The 'timeDate' class fulfils the conventions of the ISO 8601 \n\tstandard as well as of the ANSI C and POSIX standards. Beyond\n\tthese standards it provides the \"Financial Center\" concept\n\twhich allows to handle data records collected in different time \n\tzones and mix them up to have always the proper time stamps with \n\trespect to your personal financial center, or alternatively to the GMT\n\treference time. It can thus also handle time stamps from historical \n\tdata records from the same time zone, even if the financial \n\tcenters changed day light saving times at different calendar\n\tdates.",
    "version": "4051.111",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\n  Tobias Setz [aut],\n  Yohan Chalabi [aut],\n  Martin Maechler [ctb] (ORCID: <https://orcid.org/0000-0002-8685-9910>),\n  Joe W. Byers [ctb],\n  Georgi N. Boshnakov [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/timeDateDoc/ (doc),\nhttps://r-forge.r-project.org/scm/viewvc.php/pkg/timeDate/?root=rmetrics\n(devel), https://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/projects/rmetrics",
    "repository": "https://cran.r-project.org/package=timeDate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeDate Rmetrics - Chronological and Calendar Objects The 'timeDate' class fulfils the conventions of the ISO 8601 \n\tstandard as well as of the ANSI C and POSIX standards. Beyond\n\tthese standards it provides the \"Financial Center\" concept\n\twhich allows to handle data records collected in different time \n\tzones and mix them up to have always the proper time stamps with \n\trespect to your personal financial center, or alternatively to the GMT\n\treference time. It can thus also handle time stamps from historical \n\tdata records from the same time zone, even if the financial \n\tcenters changed day light saving times at different calendar\n\tdates.  "
  },
  {
    "id": 22161,
    "package_name": "timeEL",
    "title": "Time to Event Analysis via Empirical Likelihood Inference",
    "description": "Computation of t-year survival probabilities and t-year\n    risks with right censored survival data. The Kaplan-Meier estimator\n    is used to provide estimates for data without competing risks and\n    the Aalen-Johansen estimator is used when there are competing risks.\n    Confidence intervals and p-values are obtained using either usual\n    Wald-type inference or empirical likelihood inference, as described\n    in Thomas and Grunkemeier (1975) <doi:10.1080/01621459.1975.10480315>\n    and Blanche (2020) <doi:10.1007/s10985-018-09458-6>. Functions for\n    both one-sample and two-sample inference are provided. Unlike Wald-type\n    inference, empirical likelihood inference always leads to consistent\n    conclusions, in terms of statistical significance, when comparing\n    two risks (or survival probabilities) via either a ratio or a difference.",
    "version": "0.9.1",
    "maintainer": "Paul Blanche <paulfblanche@gmail.com>",
    "author": "Paul Blanche [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1415-7976>),\n  Frank Eriksson [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timeEL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeEL Time to Event Analysis via Empirical Likelihood Inference Computation of t-year survival probabilities and t-year\n    risks with right censored survival data. The Kaplan-Meier estimator\n    is used to provide estimates for data without competing risks and\n    the Aalen-Johansen estimator is used when there are competing risks.\n    Confidence intervals and p-values are obtained using either usual\n    Wald-type inference or empirical likelihood inference, as described\n    in Thomas and Grunkemeier (1975) <doi:10.1080/01621459.1975.10480315>\n    and Blanche (2020) <doi:10.1007/s10985-018-09458-6>. Functions for\n    both one-sample and two-sample inference are provided. Unlike Wald-type\n    inference, empirical likelihood inference always leads to consistent\n    conclusions, in terms of statistical significance, when comparing\n    two risks (or survival probabilities) via either a ratio or a difference.  "
  },
  {
    "id": 22164,
    "package_name": "timeROC",
    "title": "Time-Dependent ROC Curve and AUC for Censored Survival Data",
    "description": "Estimation of time-dependent ROC curve and area under time dependent ROC curve (AUC) in the presence of censored data, with or without competing risks. Confidence intervals of AUCs and tests for comparing AUCs of two rival markers measured on the same subjects can be computed, using the iid-representation of the AUC estimator. Plot functions for time-dependent ROC curves and AUC curves are provided. Time-dependent Positive Predictive Values (PPV) and Negative Predictive Values (NPV) can also be computed. See Blanche et al. (2013) <doi:10.1002/sim.5958> and references therein for the details of the methods implemented in the package.",
    "version": "0.4",
    "maintainer": "Paul Blanche <paulfblanche@gmail.com>",
    "author": "Paul Blanche",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timeROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeROC Time-Dependent ROC Curve and AUC for Censored Survival Data Estimation of time-dependent ROC curve and area under time dependent ROC curve (AUC) in the presence of censored data, with or without competing risks. Confidence intervals of AUCs and tests for comparing AUCs of two rival markers measured on the same subjects can be computed, using the iid-representation of the AUC estimator. Plot functions for time-dependent ROC curves and AUC curves are provided. Time-dependent Positive Predictive Values (PPV) and Negative Predictive Values (NPV) can also be computed. See Blanche et al. (2013) <doi:10.1002/sim.5958> and references therein for the details of the methods implemented in the package.  "
  },
  {
    "id": 22165,
    "package_name": "timeSeriesDataSets",
    "title": "Time Series Data Sets",
    "description": "Provides a diverse collection of time series datasets\n    spanning various fields such as economics, finance, energy, healthcare, and more.\n    Designed to support time series analysis in R by offering datasets from\n    multiple disciplines, making it a valuable resource for researchers and analysts.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre]",
    "url": "https://github.com/lightbluetitan/timeseriesdatasets_R",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timeSeriesDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeSeriesDataSets Time Series Data Sets Provides a diverse collection of time series datasets\n    spanning various fields such as economics, finance, energy, healthcare, and more.\n    Designed to support time series analysis in R by offering datasets from\n    multiple disciplines, making it a valuable resource for researchers and analysts.  "
  },
  {
    "id": 22175,
    "package_name": "timereg",
    "title": "Flexible Regression Models for Survival Data",
    "description": "Programs for Martinussen and Scheike (2006), `Dynamic Regression\n    Models for Survival Data', Springer Verlag.  Plus more recent developments.\n    Additive survival model, semiparametric proportional odds model, fast\n    cumulative residuals, excess risk models and more. Flexible competing risks\n    regression including GOF-tests. Two-stage frailty modelling. PLS for the\n    additive risk model. Lasso in the 'ahaz' package.",
    "version": "2.0.7",
    "maintainer": "Thomas Scheike <ts@biostat.ku.dk>",
    "author": "Thomas Scheike [aut, cre],\n  Torben Martinussen [aut],\n  Jeremy Silver [ctb],\n  Klaus K. Holst [ctb]",
    "url": "https://github.com/scheike/timereg",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timereg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timereg Flexible Regression Models for Survival Data Programs for Martinussen and Scheike (2006), `Dynamic Regression\n    Models for Survival Data', Springer Verlag.  Plus more recent developments.\n    Additive survival model, semiparametric proportional odds model, fast\n    cumulative residuals, excess risk models and more. Flexible competing risks\n    regression including GOF-tests. Two-stage frailty modelling. PLS for the\n    additive risk model. Lasso in the 'ahaz' package.  "
  },
  {
    "id": 22236,
    "package_name": "tmle",
    "title": "Targeted Maximum Likelihood Estimation",
    "description": "Targeted maximum likelihood estimation of point treatment effects (Targeted Maximum Likelihood Learning, The International Journal of Biostatistics, 2(1), 2006.  This version automatically estimates the additive treatment effect among the treated (ATT) and among the controls (ATC).  The tmle() function calculates the adjusted marginal difference in mean outcome associated with a binary point treatment, for continuous or binary outcomes.  Relative risk and odds ratio estimates are also reported for binary outcomes. Missingness in the outcome is allowed, but not in treatment assignment or baseline covariate values.  The population mean is calculated when there is missingness, and no variation in the treatment assignment. The tmleMSM() function estimates the parameters of a marginal structural model for a binary point treatment effect. Effect estimation stratified by a binary mediating variable is also available. An ID argument can be used to identify repeated measures. Default settings call 'SuperLearner' to estimate the Q and g portions of the likelihood, unless values or a user-supplied regression function are passed in as arguments. ",
    "version": "2.1.1",
    "maintainer": "Susan Gruber <sgruber@cal.berkeley.edu>",
    "author": "Susan Gruber [aut, cre],\n  Mark van der Laan [aut],\n  Chris Kennedy [ctr]",
    "url": "https://CRAN.R-project.org/package=tmle",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmle Targeted Maximum Likelihood Estimation Targeted maximum likelihood estimation of point treatment effects (Targeted Maximum Likelihood Learning, The International Journal of Biostatistics, 2(1), 2006.  This version automatically estimates the additive treatment effect among the treated (ATT) and among the controls (ATC).  The tmle() function calculates the adjusted marginal difference in mean outcome associated with a binary point treatment, for continuous or binary outcomes.  Relative risk and odds ratio estimates are also reported for binary outcomes. Missingness in the outcome is allowed, but not in treatment assignment or baseline covariate values.  The population mean is calculated when there is missingness, and no variation in the treatment assignment. The tmleMSM() function estimates the parameters of a marginal structural model for a binary point treatment effect. Effect estimation stratified by a binary mediating variable is also available. An ID argument can be used to identify repeated measures. Default settings call 'SuperLearner' to estimate the Q and g portions of the likelihood, unless values or a user-supplied regression function are passed in as arguments.   "
  },
  {
    "id": 22309,
    "package_name": "tqk",
    "title": "Get Financial Data in Korea",
    "description": "Enables the acquisition of Korean financial market data, \n            designed to integrate seamlessly with the 'tidyquant' package.",
    "version": "0.1.8",
    "maintainer": "Chanyub Park <mrchypark@gmail.com>",
    "author": "Chanyub Park [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6474-2570>)",
    "url": "https://github.com/mrchypark/tqk, https://mrchypark.github.io/tqk/",
    "bug_reports": "https://github.com/mrchypark/tqk/issues",
    "repository": "https://cran.r-project.org/package=tqk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tqk Get Financial Data in Korea Enables the acquisition of Korean financial market data, \n            designed to integrate seamlessly with the 'tidyquant' package.  "
  },
  {
    "id": 22354,
    "package_name": "transplantr",
    "title": "Audit and Research Functions for Transplantation",
    "description": "A set of vectorised functions to calculate medical equations used in transplantation, \n    focused mainly on transplantation of abdominal organs. These functions include donor and recipient\n    risk indices as used by NHS Blood & Transplant, OPTN/UNOS and Eurotransplant, tools for \n    quantifying HLA mismatches, functions for calculating estimated glomerular filtration rate (eGFR), \n    a function to calculate the APRI (AST to platelet ratio) score used in initial screening of suitability to receive a \n    transplant from a hepatitis C seropositive donor and some biochemical unit converter functions. \n    All functions are designed to work with either US or international units.\n    References for the equations are provided in the vignettes and function documentation.",
    "version": "0.2.0",
    "maintainer": "John Asher <john.asher@outlook.com>",
    "author": "John Asher [aut, cre] (ORCID: <https://orcid.org/0000-0001-8735-6453>)",
    "url": "https://transplantr.txtools.net,\nhttps://github.com/johnasher/transplantr",
    "bug_reports": "https://github.com/johnasher/transplantr/issues",
    "repository": "https://cran.r-project.org/package=transplantr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transplantr Audit and Research Functions for Transplantation A set of vectorised functions to calculate medical equations used in transplantation, \n    focused mainly on transplantation of abdominal organs. These functions include donor and recipient\n    risk indices as used by NHS Blood & Transplant, OPTN/UNOS and Eurotransplant, tools for \n    quantifying HLA mismatches, functions for calculating estimated glomerular filtration rate (eGFR), \n    a function to calculate the APRI (AST to platelet ratio) score used in initial screening of suitability to receive a \n    transplant from a hepatitis C seropositive donor and some biochemical unit converter functions. \n    All functions are designed to work with either US or international units.\n    References for the equations are provided in the vignettes and function documentation.  "
  },
  {
    "id": 22432,
    "package_name": "truncnormbayes",
    "title": "Estimates Moments for a Truncated Normal Distribution using\n'Stan'",
    "description": "Finds the posterior modes for the mean and standard deviation for a\n  truncated normal distribution with one or two known truncation points.\n  The method used extends Bayesian methods for parameter estimation for a singly\n  truncated normal distribution under the Jeffreys prior (see Zhou X,\n  Giacometti R, Fabozzi FJ, Tucker AH (2014). \"Bayesian estimation of truncated\n  data with applications to operational risk measurement\".\n  <doi:10.1080/14697688.2012.752103>). This package additionally allows for a\n  doubly truncated normal distribution.",
    "version": "0.0.3",
    "maintainer": "Peter Solymos <peter@analythium.io>",
    "author": "Mika Braginsky [aut],\n  Leon Tran [aut],\n  Maya Mathur [ctb],\n  Peter Solymos [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0001-7337-1740>)",
    "url": "https://github.com/mathurlabstanford/truncnormbayes,\nhttps://mathurlabstanford.github.io/truncnormbayes/",
    "bug_reports": "https://github.com/mathurlabstanford/truncnormbayes/issues",
    "repository": "https://cran.r-project.org/package=truncnormbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "truncnormbayes Estimates Moments for a Truncated Normal Distribution using\n'Stan' Finds the posterior modes for the mean and standard deviation for a\n  truncated normal distribution with one or two known truncation points.\n  The method used extends Bayesian methods for parameter estimation for a singly\n  truncated normal distribution under the Jeffreys prior (see Zhou X,\n  Giacometti R, Fabozzi FJ, Tucker AH (2014). \"Bayesian estimation of truncated\n  data with applications to operational risk measurement\".\n  <doi:10.1080/14697688.2012.752103>). This package additionally allows for a\n  doubly truncated normal distribution.  "
  },
  {
    "id": 22449,
    "package_name": "tscopula",
    "title": "Time Series Copula Models",
    "description": "Functions for the analysis of time series using copula models.  \n    The package is based on methodology described in the following references.\n    McNeil, A.J. (2021) <doi:10.3390/risks9010014>,\n    Bladt, M., & McNeil, A.J. (2021) <doi:10.1016/j.ecosta.2021.07.004>,\n    Bladt, M., & McNeil, A.J. (2022) <doi:10.1515/demo-2022-0105>.",
    "version": "0.3.9",
    "maintainer": "Alexander McNeil <alexanderjmcneil@gmail.com>",
    "author": "Alexander McNeil [aut, cre],\n  Martin Bladt [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tscopula",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tscopula Time Series Copula Models Functions for the analysis of time series using copula models.  \n    The package is based on methodology described in the following references.\n    McNeil, A.J. (2021) <doi:10.3390/risks9010014>,\n    Bladt, M., & McNeil, A.J. (2021) <doi:10.1016/j.ecosta.2021.07.004>,\n    Bladt, M., & McNeil, A.J. (2022) <doi:10.1515/demo-2022-0105>.  "
  },
  {
    "id": 22459,
    "package_name": "tseries",
    "title": "Time Series Analysis and Computational Finance",
    "description": "Time series analysis and computational finance.",
    "version": "0.10-58",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Adrian Trapletti [aut],\n  Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  Blake LeBaron [ctb] (BDS test code)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tseries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tseries Time Series Analysis and Computational Finance Time series analysis and computational finance.  "
  },
  {
    "id": 22486,
    "package_name": "tsriadditive",
    "title": "Two Stage Residual Inclusion Additive Hazards Estimator",
    "description": "Additive hazards models with two stage residual inclusion method are fitted under either survival data or competing risks data. The estimator incorporates an instrumental variable and therefore can recover causal estimand in the presence of unmeasured confounding under some assumptions. A.Ying, R. Xu and J. Murphy. (2019) <doi:10.1002/sim.8071>.",
    "version": "1.0.0",
    "maintainer": "Andrew Ying <aying9339@gmail.com>",
    "author": "Andrew Ying [aut, cre]",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8071",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tsriadditive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsriadditive Two Stage Residual Inclusion Additive Hazards Estimator Additive hazards models with two stage residual inclusion method are fitted under either survival data or competing risks data. The estimator incorporates an instrumental variable and therefore can recover causal estimand in the presence of unmeasured confounding under some assumptions. A.Ying, R. Xu and J. Murphy. (2019) <doi:10.1002/sim.8071>.  "
  },
  {
    "id": 22489,
    "package_name": "tstests",
    "title": "Time Series Goodness of Fit and Forecast Evaluation Tests",
    "description": "Goodness of Fit and Forecast Evaluation Tests for timeseries models. Includes, among others, the Generalized Method of Moments (GMM) Orthogonality Test of Hansen (1982), the Nyblom (1989) parameter constancy test, the sign-bias test of Engle and Ng (1993), and a range of tests for value at risk and expected shortfall evaluation.",
    "version": "1.0.1",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "author": "Alexios Galanos [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-9308-0457>)",
    "url": "https://www.nopredict.com/packages/tstests,\nhttps://github.com/tsmodels/tstests",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tstests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tstests Time Series Goodness of Fit and Forecast Evaluation Tests Goodness of Fit and Forecast Evaluation Tests for timeseries models. Includes, among others, the Generalized Method of Moments (GMM) Orthogonality Test of Hansen (1982), the Nyblom (1989) parameter constancy test, the sign-bias test of Engle and Ng (1993), and a range of tests for value at risk and expected shortfall evaluation.  "
  },
  {
    "id": 22503,
    "package_name": "tteICE",
    "title": "Treatment Effect Estimation for Time-to-Event Data with\nIntercurrent Events",
    "description": "Analyzing treatment effects in clinical trials with time-to-event outcomes is complicated by intercurrent events. This package implements methods for estimating and inferring the cumulative incidence functions for time-to-event (TTE) outcomes with intercurrent events (ICEs) under the five strategies outlined in the ICH E9 (R1) addendum, see Deng (2025)<doi:10.1002/sim.70091>. This package can be used for analyzing data from both randomized controlled trials and observational studies. In general, we have a primary outcome event and possibly an intercurrent event. Two data structures are allowed: competing risks, where only the time to the first event is recorded, and semicompeting risks, where the times to both the primary outcome event and intercurrent event (or censoring) are recorded. For estimation methods, users can choose nonparametric estimation (which does not use covariates) and semiparametrically efficient estimation.",
    "version": "1.0.1",
    "maintainer": "Yi Zhou <yzhou@pku.edu.cn>",
    "author": "Yuhao Deng [aut],\n  Yi Zhou [cre]",
    "url": "https://github.com/mephas/tteICE",
    "bug_reports": "https://github.com/mephas/tteICE/issues",
    "repository": "https://cran.r-project.org/package=tteICE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tteICE Treatment Effect Estimation for Time-to-Event Data with\nIntercurrent Events Analyzing treatment effects in clinical trials with time-to-event outcomes is complicated by intercurrent events. This package implements methods for estimating and inferring the cumulative incidence functions for time-to-event (TTE) outcomes with intercurrent events (ICEs) under the five strategies outlined in the ICH E9 (R1) addendum, see Deng (2025)<doi:10.1002/sim.70091>. This package can be used for analyzing data from both randomized controlled trials and observational studies. In general, we have a primary outcome event and possibly an intercurrent event. Two data structures are allowed: competing risks, where only the time to the first event is recorded, and semicompeting risks, where the times to both the primary outcome event and intercurrent event (or censoring) are recorded. For estimation methods, users can choose nonparametric estimation (which does not use covariates) and semiparametrically efficient estimation.  "
  },
  {
    "id": 22532,
    "package_name": "tvmComp",
    "title": "Discounting and Compounding Calculations for Various Scenarios",
    "description": "Functions for compounding and discounting calculations included here serve as a complete reference for various scenarios of time value of money.\n    Raymond M. Brooks (\u201cFinancial Management,\u201d 2018, ISBN: 9780134730417).\n    Sheridan Titman, Arthur J. Keown, John D. Martin (\u201cFinancial Management: Principles and Applications,\u201d 2017, ISBN: 9780134417219).\n    Jonathan Berk, Peter DeMarzo, David Stangeland, Andras Marosi (\u201cFundamentals of Corporate Finance,\u201d 2019, ISBN: 9780134735313).\n    S. A. Hummelbrunner, Kelly Halliday, Ali R. Hassanlou (\u201cContemporary Business Mathematics with Canadian Applications,\u201d 2020, ISBN: 9780135285015).",
    "version": "1.0.2",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre],\n  MaheshP Kumar [aut],\n  MaheshP Kumar [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tvmComp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tvmComp Discounting and Compounding Calculations for Various Scenarios Functions for compounding and discounting calculations included here serve as a complete reference for various scenarios of time value of money.\n    Raymond M. Brooks (\u201cFinancial Management,\u201d 2018, ISBN: 9780134730417).\n    Sheridan Titman, Arthur J. Keown, John D. Martin (\u201cFinancial Management: Principles and Applications,\u201d 2017, ISBN: 9780134417219).\n    Jonathan Berk, Peter DeMarzo, David Stangeland, Andras Marosi (\u201cFundamentals of Corporate Finance,\u201d 2019, ISBN: 9780134735313).\n    S. A. Hummelbrunner, Kelly Halliday, Ali R. Hassanlou (\u201cContemporary Business Mathematics with Canadian Applications,\u201d 2020, ISBN: 9780135285015).  "
  },
  {
    "id": 22564,
    "package_name": "twoxtwo",
    "title": "Work with Two-by-Two Tables",
    "description": "A collection of functions for data analysis with two-by-two contingency tables. The package provides tools to compute measures of effect (odds ratio, risk ratio, and risk difference), calculate impact numbers and attributable fractions, and perform hypothesis testing. Statistical analysis methods are oriented towards epidemiological investigation of relationships between exposures and outcomes.",
    "version": "0.1.0",
    "maintainer": "VP Nagraj <nagraj@nagraj.net>",
    "author": "VP Nagraj [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twoxtwo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twoxtwo Work with Two-by-Two Tables A collection of functions for data analysis with two-by-two contingency tables. The package provides tools to compute measures of effect (odds ratio, risk ratio, and risk difference), calculate impact numbers and attributable fractions, and perform hypothesis testing. Statistical analysis methods are oriented towards epidemiological investigation of relationships between exposures and outcomes.  "
  },
  {
    "id": 22589,
    "package_name": "ufRisk",
    "title": "Risk Measure Calculation in Financial TS",
    "description": "Enables the user to calculate Value at Risk (VaR) and Expected \n    Shortfall (ES) by means of various parametric and semiparametric \n    GARCH-type models. For the latter the estimation of the nonparametric scale\n    function is carried out by means of a data-driven smoothing approach. Model\n    quality, in terms of forecasting VaR and ES, can be assessed by means of \n    various backtesting methods such as the traffic light test for VaR and a \n    newly developed traffic light test for ES. The approaches implemented in \n    this package are described in e.g. Feng Y., Beran J., Letmathe S. and \n    Ghosh S. (2020) <https://ideas.repec.org/p/pdn/ciepap/137.html> as well as \n    Letmathe S., Feng Y. and Uhde A. (2021) \n    <https://ideas.repec.org/p/pdn/ciepap/141.html>. ",
    "version": "1.0.7",
    "maintainer": "Sebastian Letmathe <sebastian.letmathe@uni-paderborn.de>",
    "author": "Yuanhua Feng [aut] (Paderborn University, Germany),\n  Xuehai Zhang [aut] (Former research associate at Paderborn University,\n    Germany),\n  Christian Peitz [aut] (Paderborn University, Germany),\n  Dominik Schulz [aut] (Paderborn University, Germany),\n  Shujie Li [aut] (Paderborn Universtiy, Germany),\n  Sebastian Letmathe [aut, cre] (Paderborn University, Germany)",
    "url": "https://wiwi.uni-paderborn.de/en/dep4/feng/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ufRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ufRisk Risk Measure Calculation in Financial TS Enables the user to calculate Value at Risk (VaR) and Expected \n    Shortfall (ES) by means of various parametric and semiparametric \n    GARCH-type models. For the latter the estimation of the nonparametric scale\n    function is carried out by means of a data-driven smoothing approach. Model\n    quality, in terms of forecasting VaR and ES, can be assessed by means of \n    various backtesting methods such as the traffic light test for VaR and a \n    newly developed traffic light test for ES. The approaches implemented in \n    this package are described in e.g. Feng Y., Beran J., Letmathe S. and \n    Ghosh S. (2020) <https://ideas.repec.org/p/pdn/ciepap/137.html> as well as \n    Letmathe S., Feng Y. and Uhde A. (2021) \n    <https://ideas.repec.org/p/pdn/ciepap/141.html>.   "
  },
  {
    "id": 22591,
    "package_name": "ugatsdb",
    "title": "Uganda Time Series Database API",
    "description": "An R API providing easy access to a relational database with macroeconomic, \n             financial and development related time series data for Uganda. \n             Overall more than 5000 series at varying frequency (daily, monthly, \n             quarterly, annual in fiscal or calendar years) can be accessed through \n             the API. The data is provided by the Bank of Uganda, \n             the Ugandan Ministry of Finance, Planning and Economic Development,\n             the IMF and the World Bank. The database is being updated once a month. ",
    "version": "0.2.3",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre]",
    "url": "https://mepd.finance.go.ug/apps.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ugatsdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ugatsdb Uganda Time Series Database API An R API providing easy access to a relational database with macroeconomic, \n             financial and development related time series data for Uganda. \n             Overall more than 5000 series at varying frequency (daily, monthly, \n             quarterly, annual in fiscal or calendar years) can be accessed through \n             the API. The data is provided by the Bank of Uganda, \n             the Ugandan Ministry of Finance, Planning and Economic Development,\n             the IMF and the World Bank. The database is being updated once a month.   "
  },
  {
    "id": 22615,
    "package_name": "uncorbets",
    "title": "Uncorrelated Bets via Minimum Torsion Algorithm",
    "description": "Implements Minimum Torsion for portfolio diversification as\n    described in Meucci, Attilio (2013) <doi:10.2139/ssrn.2276632>.",
    "version": "0.1.2",
    "maintainer": "Bernardo Reckziegel <bernardo_cse@hotmail.com>",
    "author": "Bernardo Reckziegel [aut, cre]",
    "url": "https://github.com/Reckziegel/uncorbets,\nhttps://reckziegel.github.io/uncorbets/",
    "bug_reports": "https://github.com/Reckziegel/uncorbets/issues",
    "repository": "https://cran.r-project.org/package=uncorbets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uncorbets Uncorrelated Bets via Minimum Torsion Algorithm Implements Minimum Torsion for portfolio diversification as\n    described in Meucci, Attilio (2013) <doi:10.2139/ssrn.2276632>.  "
  },
  {
    "id": 22662,
    "package_name": "unvs.med",
    "title": "A Universal Approach for Causal Mediation Analysis",
    "description": "\n    This program realizes a universal estimation approach that accommodates\n    multi-category variables and effect scales, making up for the deficiencies\n    of the existing approaches when dealing with non-binary exposures and\n    complex models. The estimation via bootstrapping can simultaneously provide results of\n    causal mediation on risk difference (RD), odds ratio (OR) and risk ratio (RR) scales with tests of the effects' difference.\n    The estimation is also applicable to many other settings, e.g., moderated mediation, inconsistent covariates,\n    panel data, etc. The high flexibility and compatibility\n    make it possible to apply for any type of model, greatly meeting the needs of\n    current empirical researches.",
    "version": "1.0.0",
    "maintainer": "Tianbao Zhou <michaelzhou@buaa.edu.cn>",
    "author": "Tianbao Zhou [aut, cre],\n  Xinghao Li [aut],\n  Lin Liu* [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=unvs.med",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unvs.med A Universal Approach for Causal Mediation Analysis \n    This program realizes a universal estimation approach that accommodates\n    multi-category variables and effect scales, making up for the deficiencies\n    of the existing approaches when dealing with non-binary exposures and\n    complex models. The estimation via bootstrapping can simultaneously provide results of\n    causal mediation on risk difference (RD), odds ratio (OR) and risk ratio (RR) scales with tests of the effects' difference.\n    The estimation is also applicable to many other settings, e.g., moderated mediation, inconsistent covariates,\n    panel data, etc. The high flexibility and compatibility\n    make it possible to apply for any type of model, greatly meeting the needs of\n    current empirical researches.  "
  },
  {
    "id": 22686,
    "package_name": "usdampr",
    "title": "Request USDA MPR Historical Data via the 'LMR' API",
    "description": "Interface to easily access data via the United States Department of Agriculture (USDA)'s Livestock Mandatory Reporting ('LMR')\n  Data API at <https://mpr.datamart.ams.usda.gov/>. The downloaded data can be saved for later off-line use. \n  Also provide relevant information and metadata for each of the input variables needed for sending the data inquiry.   ",
    "version": "1.0.1",
    "maintainer": "Bowen Chen <bwchen0719@gmail.com>",
    "author": "Bowen Chen [aut, cre] (ORCID: <https://orcid.org/0000-0003-0370-2756>),\n  Elliott Dennis [aut]",
    "url": "https://github.com/cbw1243/usdampr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=usdampr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usdampr Request USDA MPR Historical Data via the 'LMR' API Interface to easily access data via the United States Department of Agriculture (USDA)'s Livestock Mandatory Reporting ('LMR')\n  Data API at <https://mpr.datamart.ams.usda.gov/>. The downloaded data can be saved for later off-line use. \n  Also provide relevant information and metadata for each of the input variables needed for sending the data inquiry.     "
  },
  {
    "id": 22689,
    "package_name": "usdatasets",
    "title": "A Comprehensive Collection of U.S. Datasets",
    "description": "Provides a diverse collection of U.S. datasets \n    encompassing various fields such as crime, economics, education, finance, energy, healthcare, and more.\n    It serves as a valuable resource for researchers and analysts seeking to perform in-depth analyses and derive insights from U.S.-specific data.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre]",
    "url": "https://github.com/lightbluetitan/usdatasets",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=usdatasets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usdatasets A Comprehensive Collection of U.S. Datasets Provides a diverse collection of U.S. datasets \n    encompassing various fields such as crime, economics, education, finance, energy, healthcare, and more.\n    It serves as a valuable resource for researchers and analysts seeking to perform in-depth analyses and derive insights from U.S.-specific data.  "
  },
  {
    "id": 22696,
    "package_name": "usfertilizer",
    "title": "County-Level Estimates of Fertilizer Application in USA",
    "description": "Compiled and cleaned the county-level estimates of fertilizer, \n    nitrogen and phosphorus, from 1945 to 2012 in United States of America (USA).  The commercial fertilizer data were originally \n    generated by USGS based on the sales data of commercial fertilizer. The manure data were estimated\n    based on county-level population data of livestock, poultry, and other animals.\n    See the user manual for detailed data sources and cleaning methods.  \n    'usfertilizer' utilized the tidyverse to clean the original data and provide \n    user-friendly dataframe. Please note that USGS does not endorse this package. Also data from 1986 is not available for now.",
    "version": "0.1.5",
    "maintainer": "Wenlong Liu <wliu14@ncsu.edu>",
    "author": "Wenlong Liu [aut, cre]",
    "url": "https://github.com/wenlong-liu/usfertilizer",
    "bug_reports": "https://github.com/wenlong-liu/usfertilizer/issues",
    "repository": "https://cran.r-project.org/package=usfertilizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usfertilizer County-Level Estimates of Fertilizer Application in USA Compiled and cleaned the county-level estimates of fertilizer, \n    nitrogen and phosphorus, from 1945 to 2012 in United States of America (USA).  The commercial fertilizer data were originally \n    generated by USGS based on the sales data of commercial fertilizer. The manure data were estimated\n    based on county-level population data of livestock, poultry, and other animals.\n    See the user manual for detailed data sources and cleaning methods.  \n    'usfertilizer' utilized the tidyverse to clean the original data and provide \n    user-friendly dataframe. Please note that USGS does not endorse this package. Also data from 1986 is not available for now.  "
  },
  {
    "id": 22714,
    "package_name": "utilityFunctionTools",
    "title": "P-Spline Regression for Utility Functions and Derived Measures",
    "description": "Predicts a smooth and continuous (individual) utility function from utility points, and computes measures of intensity for risk and higher-order risk measures (or any other measure computed with user-written function) based on this utility function and its derivatives according to the method introduced in Schneider (2017) <http://hdl.handle.net/21.11130/00-1735-0000-002E-E306-0>.",
    "version": "1.0",
    "maintainer": "Sebastian O. Schneider <sschneider@coll.mpg.de>",
    "author": "Sebastian O. Schneider [aut, cre, prg],\n  Giulia Baldini [edt, prg],\n  Paul Eilers [ctb] (Functions tpower and bbase, see Package JOPS at\n    http://statweb.lsu.edu/faculty/marx/JOPS_0.1.0.tar.gz)",
    "url": "https://www.sebastianoschneider.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=utilityFunctionTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "utilityFunctionTools P-Spline Regression for Utility Functions and Derived Measures Predicts a smooth and continuous (individual) utility function from utility points, and computes measures of intensity for risk and higher-order risk measures (or any other measure computed with user-written function) based on this utility function and its derivatives according to the method introduced in Schneider (2017) <http://hdl.handle.net/21.11130/00-1735-0000-002E-E306-0>.  "
  },
  {
    "id": 22722,
    "package_name": "vaRiskScore",
    "title": "VA CVD Risk Score",
    "description": "Estimates the predicted 10-year cardiovascular (CVD) risk score (in probability) for civilian women, women military service \n    members and veterans by inputting patient profiles. The proposed women CVD risk score improves the accuracy of the \n    existing American College of Cardiology/American Heart Association CVD risk assessment\n    tool in predicting long\u2010term CVD risk for VA women, particularly in young and racial/ethnic \n    minority women. See the reference: Jeon\u2010Slaughter, H., Chen, X., Tsai, S., Ramanan, B., & Ebrahimi, R. \n    (2021) <doi:10.1161/JAHA.120.019217>.",
    "version": "2.0.0",
    "maintainer": "Xiaofei Chen <bill.nj08873@gmail.com>",
    "author": "Xiaofei Chen [aut, cre],\n  Haekyung Jeon-Slaughter [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vaRiskScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vaRiskScore VA CVD Risk Score Estimates the predicted 10-year cardiovascular (CVD) risk score (in probability) for civilian women, women military service \n    members and veterans by inputting patient profiles. The proposed women CVD risk score improves the accuracy of the \n    existing American College of Cardiology/American Heart Association CVD risk assessment\n    tool in predicting long\u2010term CVD risk for VA women, particularly in young and racial/ethnic \n    minority women. See the reference: Jeon\u2010Slaughter, H., Chen, X., Tsai, S., Ramanan, B., & Ebrahimi, R. \n    (2021) <doi:10.1161/JAHA.120.019217>.  "
  },
  {
    "id": 22725,
    "package_name": "vaccine",
    "title": "Statistical Tools for Immune Correlates Analysis of Vaccine\nClinical Trial Data",
    "description": "Various semiparametric and nonparametric statistical tools for\n    immune correlates analysis of vaccine clinical trial data. This includes\n    calculation of summary statistics and estimation of risk, vaccine efficacy,\n    controlled effects (controlled risk and controlled vaccine efficacy), and\n    mediation effects (natural direct effect, natural indirect effect,\n    proportion mediated). See Gilbert P, Fong Y, Kenny A, and Carone, M (2022)\n    <doi:10.1093/biostatistics/kxac024> and Fay MP and Follmann DA (2023)\n    <doi:10.48550/arXiv.2208.06465>.",
    "version": "1.3.1",
    "maintainer": "Avi Kenny <avi.kenny@gmail.com>",
    "author": "Avi Kenny [aut, cre]",
    "url": "https://avi-kenny.github.io/vaccine/",
    "bug_reports": "https://github.com/Avi-Kenny/vaccine/issues",
    "repository": "https://cran.r-project.org/package=vaccine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vaccine Statistical Tools for Immune Correlates Analysis of Vaccine\nClinical Trial Data Various semiparametric and nonparametric statistical tools for\n    immune correlates analysis of vaccine clinical trial data. This includes\n    calculation of summary statistics and estimation of risk, vaccine efficacy,\n    controlled effects (controlled risk and controlled vaccine efficacy), and\n    mediation effects (natural direct effect, natural indirect effect,\n    proportion mediated). See Gilbert P, Fong Y, Kenny A, and Carone, M (2022)\n    <doi:10.1093/biostatistics/kxac024> and Fay MP and Follmann DA (2023)\n    <doi:10.48550/arXiv.2208.06465>.  "
  },
  {
    "id": 22753,
    "package_name": "vamc",
    "title": "A Monte Carlo Valuation Framework for Variable Annuities",
    "description": "Implementation of a Monte Carlo simulation engine for valuing synthetic portfolios of \n    variable annuities, which reflect realistic features of common annuity contracts in practice. \n    It aims to facilitate the development and dissemination of research related to the efficient \n    valuation of a portfolio of large variable annuities. The main valuation methodology was \n    proposed by Gan (2017) <doi:10.1515/demo-2017-0021>.",
    "version": "0.2.1",
    "maintainer": "Mingyi Jiang <m64jiang@uwaterloo.ca>",
    "author": "Hengxin Li [aut, cph],\n  Ben Feng [aut, cph],\n  Mingyi Jiang [aut, cph, cre],\n  GuoJun Gan [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vamc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vamc A Monte Carlo Valuation Framework for Variable Annuities Implementation of a Monte Carlo simulation engine for valuing synthetic portfolios of \n    variable annuities, which reflect realistic features of common annuity contracts in practice. \n    It aims to facilitate the development and dissemination of research related to the efficient \n    valuation of a portfolio of large variable annuities. The main valuation methodology was \n    proposed by Gan (2017) <doi:10.1515/demo-2017-0021>.  "
  },
  {
    "id": 22767,
    "package_name": "vardpoor",
    "title": "Variance Estimation for Sample Surveys by the Ultimate Cluster\nMethod",
    "description": "Generation of domain variables, linearization of several non-linear population statistics (the ratio of two totals, weighted income percentile, relative median income ratio, at-risk-of-poverty rate, at-risk-of-poverty threshold, Gini coefficient, gender pay gap, the aggregate replacement ratio, the relative median income ratio, median income below at-risk-of-poverty gap, income quintile share ratio, relative median at-risk-of-poverty gap), computation of regression residuals in case of weight calibration, variance estimation of sample surveys by the ultimate cluster method (Hansen, Hurwitz and Madow, Sample Survey Methods And Theory, vol. I: Methods and Applications; vol. II: Theory. 1953, New York: John Wiley and Sons), variance estimation for longitudinal, cross-sectional measures and measures of change for single and multistage stage cluster sampling designs (Berger, Y. G., 2015, <doi:10.1111/rssa.12116>). Several other precision measures are derived - standard error, the coefficient of variation, the margin of error, confidence interval, design effect.",
    "version": "0.20.1",
    "maintainer": "Martins Liberts <martins.liberts@csb.gov.lv>",
    "author": "Juris Breidaks [aut],\n  Martins Liberts [aut, cre],\n  Santa Ivanova [aut],\n  Aleksis Jursevskis [ctb],\n  Anthony Damico [ctb],\n  Central Statistical Bureau of Latvia [cph, fnd]",
    "url": "https://csblatvia.github.io/vardpoor/,\nhttps://github.com/CSBLatvia/vardpoor/",
    "bug_reports": "https://github.com/CSBLatvia/vardpoor/issues/",
    "repository": "https://cran.r-project.org/package=vardpoor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vardpoor Variance Estimation for Sample Surveys by the Ultimate Cluster\nMethod Generation of domain variables, linearization of several non-linear population statistics (the ratio of two totals, weighted income percentile, relative median income ratio, at-risk-of-poverty rate, at-risk-of-poverty threshold, Gini coefficient, gender pay gap, the aggregate replacement ratio, the relative median income ratio, median income below at-risk-of-poverty gap, income quintile share ratio, relative median at-risk-of-poverty gap), computation of regression residuals in case of weight calibration, variance estimation of sample surveys by the ultimate cluster method (Hansen, Hurwitz and Madow, Sample Survey Methods And Theory, vol. I: Methods and Applications; vol. II: Theory. 1953, New York: John Wiley and Sons), variance estimation for longitudinal, cross-sectional measures and measures of change for single and multistage stage cluster sampling designs (Berger, Y. G., 2015, <doi:10.1111/rssa.12116>). Several other precision measures are derived - standard error, the coefficient of variation, the margin of error, confidence interval, design effect.  "
  },
  {
    "id": 22776,
    "package_name": "varoc",
    "title": "Value Added Receiver Operating Characteristics Curve",
    "description": "A continuous version of the receiver operating characteristics (ROC) curve to assess both classification and continuity performances of biomarkers, diagnostic tests, or risk prediction models.",
    "version": "1.0.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "author": "Yunro Chung [aut, cre] (ORCID: <https://orcid.org/0000-0001-9125-9277>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=varoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "varoc Value Added Receiver Operating Characteristics Curve A continuous version of the receiver operating characteristics (ROC) curve to assess both classification and continuity performances of biomarkers, diagnostic tests, or risk prediction models.  "
  },
  {
    "id": 22780,
    "package_name": "vasicek",
    "title": "Miscellaneous Functions for Vasicek Distribution",
    "description": "Provide a collection of miscellaneous R functions\n    related to the Vasicek distribution with the intent to make\n    the lives of risk modelers easier.",
    "version": "0.0.3",
    "maintainer": "WenSui Liu <liuwensui@gmail.com>",
    "author": "WenSui Liu ",
    "url": "https://github.com/statcompute/vasicek",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vasicek",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vasicek Miscellaneous Functions for Vasicek Distribution Provide a collection of miscellaneous R functions\n    related to the Vasicek distribution with the intent to make\n    the lives of risk modelers easier.  "
  },
  {
    "id": 22783,
    "package_name": "vaxineR",
    "title": "Vaccine Coverage and Outbreak Risk Analysis",
    "description": "Provides tools to analyze vaccine coverage data and simulate potential disease outbreak scenarios. It allows users to calculate key epidemiological metrics such as the effective reproduction number (Re), outbreak probabilities, and expected infection counts based on county-level vaccination rates, disease characteristics, and vaccine effectiveness. The package includes historical kindergarten vaccination data for Florida counties and offers functions for generating summary tables, visualizations, and exporting the underlying plot data.",
    "version": "0.1.0",
    "maintainer": "Peiyu Liu <pyliu0620@outlook.com>",
    "author": "Peiyu Liu [aut, cre],\n  Matt Hitchings [ctb],\n  Ira Longini [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vaxineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vaxineR Vaccine Coverage and Outbreak Risk Analysis Provides tools to analyze vaccine coverage data and simulate potential disease outbreak scenarios. It allows users to calculate key epidemiological metrics such as the effective reproduction number (Re), outbreak probabilities, and expected infection counts based on county-level vaccination rates, disease characteristics, and vaccine effectiveness. The package includes historical kindergarten vaccination data for Florida counties and offers functions for generating summary tables, visualizations, and exporting the underlying plot data.  "
  },
  {
    "id": 22915,
    "package_name": "volatilityTrader",
    "title": "High Volatility Environment Option Trading Strategies Graphs",
    "description": "Trading Strategies for high Option Volatility environment are represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).",
    "version": "1.0.1",
    "maintainer": "MaheshP Kumar <maheshparamjitkumar@gmail.com>",
    "author": "MaheshP Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=volatilityTrader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "volatilityTrader High Volatility Environment Option Trading Strategies Graphs Trading Strategies for high Option Volatility environment are represented here through their Graphs. The graphic indicators, strategies, calculations, functions and all the discussions are for academic, research, and educational purposes only and should not be construed as investment advice and come with absolutely no Liability.\n    Guy Cohen (\u201cThe Bible of Options Strategies (2nd ed.)\u201d, 2015, ISBN: 9780133964028).\n    Zura Kakushadze, Juan A. Serur (\u201c151 Trading Strategies\u201d, 2018, ISBN: 9783030027919).\n    John C. Hull (\u201cOptions, Futures, and Other Derivatives (11th ed.)\u201d, 2022, ISBN: 9780136939979).  "
  },
  {
    "id": 22917,
    "package_name": "volesti",
    "title": "Volume Approximation and Sampling of Convex Polytopes",
    "description": "Provides an R interface for 'volesti' C++ package. 'volesti' computes estimations of volume\n             of polytopes given by (i) a set of points, (ii) linear inequalities or (iii) Minkowski sum of segments\n             (a.k.a. zonotopes). There are three algorithms for volume estimation as well as algorithms\n             for sampling, rounding and rotating polytopes. Moreover, 'volesti' provides algorithms for\n             estimating copulas useful in computational finance. Methods implemented in 'volesti' are described\n             in A. Chalkis and V. Fisikopoulos (2022) <doi:10.32614/RJ-2021-077> and references therein.",
    "version": "1.1.2-9",
    "maintainer": "Vissarion Fisikopoulos <vissarion.fisikopoulos@gmail.com>",
    "author": "Vissarion Fisikopoulos [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0780-666X>),\n  Apostolos Chalkis [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-4628-1907>)",
    "url": "",
    "bug_reports": "https://github.com/GeomScale/volesti/issues",
    "repository": "https://cran.r-project.org/package=volesti",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "volesti Volume Approximation and Sampling of Convex Polytopes Provides an R interface for 'volesti' C++ package. 'volesti' computes estimations of volume\n             of polytopes given by (i) a set of points, (ii) linear inequalities or (iii) Minkowski sum of segments\n             (a.k.a. zonotopes). There are three algorithms for volume estimation as well as algorithms\n             for sampling, rounding and rotating polytopes. Moreover, 'volesti' provides algorithms for\n             estimating copulas useful in computational finance. Methods implemented in 'volesti' are described\n             in A. Chalkis and V. Fisikopoulos (2022) <doi:10.32614/RJ-2021-077> and references therein.  "
  },
  {
    "id": 22974,
    "package_name": "wal",
    "title": "Read and Write 'wal' Bitmap Image Files and Other 'Quake' Assets",
    "description": "Read 'Quake' assets including bitmap images and textures in 'wal' file format. This package also provides support for extracting these assets from 'WAD' and 'PAK' file archives. It can also read models in 'MDL' and 'MD2' formats.",
    "version": "0.1.1",
    "maintainer": "Tim Sch\u00e4fer <ts+code@rcmd.org>",
    "author": "Tim Sch\u00e4fer [aut, cre] (ORCID: <https://orcid.org/0000-0002-3683-8070>)",
    "url": "https://github.com/dfsp-spirit/wal",
    "bug_reports": "https://github.com/dfsp-spirit/wal/issues",
    "repository": "https://cran.r-project.org/package=wal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wal Read and Write 'wal' Bitmap Image Files and Other 'Quake' Assets Read 'Quake' assets including bitmap images and textures in 'wal' file format. This package also provides support for extracting these assets from 'WAD' and 'PAK' file archives. It can also read models in 'MDL' and 'MD2' formats.  "
  },
  {
    "id": 22980,
    "package_name": "wally",
    "title": "The Wally Calibration Plot for Risk Prediction Models",
    "description": "A prediction model is calibrated if, roughly, for any percentage x we can expect that x subjects out of 100 experience the event among all subjects that have a predicted risk of x%. A calibration plot provides a simple, yet useful, way of assessing the calibration assumption. The Wally plot consists of a sequence of usual calibration plots. Among the plots contained within the sequence, one is the actual calibration plot which has been obtained from the data and the others are obtained from similar simulated data under the calibration assumption. It provides the investigator with a direct visual understanding of the shape and sampling variability that are common under the calibration assumption. The original calibration plot from the data is included randomly among the simulated calibration plots, similarly to a police lineup. If the original calibration plot is not easily identified then the calibration assumption is not contradicted by the data. The method handles the common situations in which the data contain censored observations and occurrences of competing events.",
    "version": "1.0.10",
    "maintainer": "Paul F. Blanche <paulfblanche@gmail.com>",
    "author": "Paul F Blanche <paulfblanche@gmail.com>, Thomas A. Gerds\n    <tag@biostat.ku.dk>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wally",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wally The Wally Calibration Plot for Risk Prediction Models A prediction model is calibrated if, roughly, for any percentage x we can expect that x subjects out of 100 experience the event among all subjects that have a predicted risk of x%. A calibration plot provides a simple, yet useful, way of assessing the calibration assumption. The Wally plot consists of a sequence of usual calibration plots. Among the plots contained within the sequence, one is the actual calibration plot which has been obtained from the data and the others are obtained from similar simulated data under the calibration assumption. It provides the investigator with a direct visual understanding of the shape and sampling variability that are common under the calibration assumption. The original calibration plot from the data is included randomly among the simulated calibration plots, similarly to a police lineup. If the original calibration plot is not easily identified then the calibration assumption is not contradicted by the data. The method handles the common situations in which the data contain censored observations and occurrences of competing events.  "
  },
  {
    "id": 22985,
    "package_name": "warabandi",
    "title": "Roster Generation of Turn for Weekdays:'warabandi'",
    "description": "It generates the roster of turn for an outlet which is flowing \n        (water) 24X7 or 168 hours towards the area under command or agricutural \n        area (to be irrigated). The area under command is differentially owned \n        by different individual farmers. The Outlet runs for free of cost to \n        irrigate the area under command 24X7.\n        So, flow time of the outlet has to be divided based on an area owned by \n        an individual farmer and the location of his land or farm. This roster \n        is known as 'warabandi' and its generation in agriculture practices is a \n        very tedious task. Calculations of time in microseconds are more \n        error-prone, especially whenever it is performed by hands. That division\n        of flow time for an individual farmer can be calculated by 'warabandi'. \n        However, it generates a full publishable report for an outlet and all the \n        farmers who have farms subjected to be irrigated. \n        It reduces error risk and makes a more reproducible roster. For more \n        details about warabandi system you can found elsewhere in \n        Bandaragoda DJ(1995) <https://publications.iwmi.org/pdf/H_17571i.pdf>.",
    "version": "0.1.0",
    "maintainer": "Harvinder Singh <harvindermaan4@gmail.com>",
    "author": "Harvinder Singh",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=warabandi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "warabandi Roster Generation of Turn for Weekdays:'warabandi' It generates the roster of turn for an outlet which is flowing \n        (water) 24X7 or 168 hours towards the area under command or agricutural \n        area (to be irrigated). The area under command is differentially owned \n        by different individual farmers. The Outlet runs for free of cost to \n        irrigate the area under command 24X7.\n        So, flow time of the outlet has to be divided based on an area owned by \n        an individual farmer and the location of his land or farm. This roster \n        is known as 'warabandi' and its generation in agriculture practices is a \n        very tedious task. Calculations of time in microseconds are more \n        error-prone, especially whenever it is performed by hands. That division\n        of flow time for an individual farmer can be calculated by 'warabandi'. \n        However, it generates a full publishable report for an outlet and all the \n        farmers who have farms subjected to be irrigated. \n        It reduces error risk and makes a more reproducible roster. For more \n        details about warabandi system you can found elsewhere in \n        Bandaragoda DJ(1995) <https://publications.iwmi.org/pdf/H_17571i.pdf>.  "
  },
  {
    "id": 23063,
    "package_name": "wevid",
    "title": "Quantifying Performance of a Binary Classifier Through Weight of\nEvidence",
    "description": "The distributions of the weight of evidence (log Bayes factor) favouring case over noncase status in a test dataset (or test folds generated by cross-validation) can be used to quantify the performance of a diagnostic test (McKeigue (2019), <doi:10.1177/0962280218776989>). The package can be used with any test dataset on which you have observed case-control status and have computed prior and posterior probabilities of case status using a model learned on a training dataset. To quantify how the predictor will behave as a risk stratifier, the quantiles of the distributions of weight of evidence in cases and controls can be calculated and plotted.",
    "version": "0.6.2",
    "maintainer": "Marco Colombo <mar.colombo13@gmail.com>",
    "author": "Paul McKeigue [aut] (ORCID: <https://orcid.org/0000-0002-5217-1034>),\n  Marco Colombo [ctb, cre] (ORCID:\n    <https://orcid.org/0000-0001-6672-0623>)",
    "url": "http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wevid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wevid Quantifying Performance of a Binary Classifier Through Weight of\nEvidence The distributions of the weight of evidence (log Bayes factor) favouring case over noncase status in a test dataset (or test folds generated by cross-validation) can be used to quantify the performance of a diagnostic test (McKeigue (2019), <doi:10.1177/0962280218776989>). The package can be used with any test dataset on which you have observed case-control status and have computed prior and posterior probabilities of case status using a model learned on a training dataset. To quantify how the predictor will behave as a risk stratifier, the quantiles of the distributions of weight of evidence in cases and controls can be calculated and plotted.  "
  },
  {
    "id": 23168,
    "package_name": "wqc",
    "title": "Wavelet Quantile Correlation Analysis",
    "description": "Estimate and plot wavelet quantile correlations(Kumar and Padakandla,2022) between two time series. Wavelet quantile correlation is used to capture the dependency between two time series across quantiles and different frequencies. This method is useful in identifying potential hedges and safe-haven instruments for investment purposes. See Kumar and Padakandla(2022) <doi:10.1016/j.frl.2022.102707> for further details.",
    "version": "0.1.2",
    "maintainer": "Anoop S Kumar <akumar.sasikumar@gmail.com>",
    "author": "Anoop S Kumar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wqc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wqc Wavelet Quantile Correlation Analysis Estimate and plot wavelet quantile correlations(Kumar and Padakandla,2022) between two time series. Wavelet quantile correlation is used to capture the dependency between two time series across quantiles and different frequencies. This method is useful in identifying potential hedges and safe-haven instruments for investment purposes. See Kumar and Padakandla(2022) <doi:10.1016/j.frl.2022.102707> for further details.  "
  },
  {
    "id": 23204,
    "package_name": "xVA",
    "title": "Credit Risk Valuation Adjustments",
    "description": "Calculates a number of valuation adjustments including CVA, DVA,\n    FBA, FCA, MVA and KVA. A two-way margin agreement has been implemented. For\n    the KVA calculation four regulatory frameworks are supported: CEM, (simplified) SA-CCR, OEM\n\tand IMM. The probability of default is implied through the credit spreads curve.\n    The package supports an exposure calculation based on SA-CCR which includes several trade types\n    and a simulated path which is currently available only for Interest Rate Swaps. The latest regulatory capital charge methodologies\n    have been implementing including BA-CVA & SA-CVA.",
    "version": "1.3",
    "maintainer": "Tasos Grivas <info@openriskcalculator.com>",
    "author": "Tasos Grivas [aut, cre]",
    "url": "https://openriskcalculator.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xVA Credit Risk Valuation Adjustments Calculates a number of valuation adjustments including CVA, DVA,\n    FBA, FCA, MVA and KVA. A two-way margin agreement has been implemented. For\n    the KVA calculation four regulatory frameworks are supported: CEM, (simplified) SA-CCR, OEM\n\tand IMM. The probability of default is implied through the credit spreads curve.\n    The package supports an exposure calculation based on SA-CCR which includes several trade types\n    and a simulated path which is currently available only for Interest Rate Swaps. The latest regulatory capital charge methodologies\n    have been implementing including BA-CVA & SA-CVA.  "
  },
  {
    "id": 23247,
    "package_name": "xnet",
    "title": "Two-Step Kernel Ridge Regression for Network Predictions",
    "description": "Fit a two-step kernel ridge regression model for\n        predicting edges in networks, and carry out cross-validation\n        using shortcuts for swift and accurate performance assessment\n        (Stock et al, 2018 <doi:10.1093/bib/bby095> ).",
    "version": "0.1.11",
    "maintainer": "Joris Meys <Joris.Meys@UGent.be>",
    "author": "Joris Meys [cre, aut],\n  Michiel Stock [aut]",
    "url": "https://github.com/CenterForStatistics-UGent/xnet",
    "bug_reports": "https://github.com/CenterForStatistics-UGent/xnet/issues",
    "repository": "https://cran.r-project.org/package=xnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xnet Two-Step Kernel Ridge Regression for Network Predictions Fit a two-step kernel ridge regression model for\n        predicting edges in networks, and carry out cross-validation\n        using shortcuts for swift and accurate performance assessment\n        (Stock et al, 2018 <doi:10.1093/bib/bby095> ).  "
  },
  {
    "id": 23269,
    "package_name": "xxdi",
    "title": "Calculate Expertise Indices",
    "description": "Institutional performance assessment remains a key challenge to a multitude of stakeholders. Existing indicators such as h-type indicators, g-type indicators, and many others do not reflect expertise of institutions that defines their research portfolio. The package offers functionality to compute and visualise two novel indices: the x-index and the xd-index. The x-index evaluates an institution's scholarly expertise within a specific discipline or field, while the xd-index provides a broader assessment of overall scholarly expertise considering an institution's publication pattern and strengths across coarse thematic areas. These indices offer a nuanced understanding of institutional research capabilities, aiding stakeholders in research management and resource allocation decisions. Lathabai, H.H., Nandy, A., and Singh, V.K. (2021) <doi:10.1007/s11192-021-04188-3>. Nandy, A., Lathabai, H.H., and Singh, V.K. (2023) <doi:10.5281/zenodo.8305585>. This package provides the h, g, x, and xd indices for use with standard format of Web of Science (WoS) scrapped datasets.",
    "version": "1.2.4",
    "maintainer": "Nilabhra Rohan Das <nr.das@yahoo.com>",
    "author": "Nilabhra Rohan Das [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-8187-0080>),\n  Abhirup Nandy [aut] (ORCID: <https://orcid.org/0000-0001-8618-0847>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xxdi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xxdi Calculate Expertise Indices Institutional performance assessment remains a key challenge to a multitude of stakeholders. Existing indicators such as h-type indicators, g-type indicators, and many others do not reflect expertise of institutions that defines their research portfolio. The package offers functionality to compute and visualise two novel indices: the x-index and the xd-index. The x-index evaluates an institution's scholarly expertise within a specific discipline or field, while the xd-index provides a broader assessment of overall scholarly expertise considering an institution's publication pattern and strengths across coarse thematic areas. These indices offer a nuanced understanding of institutional research capabilities, aiding stakeholders in research management and resource allocation decisions. Lathabai, H.H., Nandy, A., and Singh, V.K. (2021) <doi:10.1007/s11192-021-04188-3>. Nandy, A., Lathabai, H.H., and Singh, V.K. (2023) <doi:10.5281/zenodo.8305585>. This package provides the h, g, x, and xd indices for use with standard format of Web of Science (WoS) scrapped datasets.  "
  },
  {
    "id": 23275,
    "package_name": "yahoofinancer",
    "title": "Fetch Data from Yahoo Finance API",
    "description": "Obtain historical and near real time data related to stocks, index \n    and currencies from the Yahoo Finance API. This package is community maintained \n    and is not officially supported by 'Yahoo'. The accuracy of data is only as \n    correct as provided on <https://finance.yahoo.com/>.",
    "version": "0.4.0",
    "maintainer": "Aravind Hebbali <hebbali.aravind@gmail.com>",
    "author": "Aravind Hebbali [aut, cre]",
    "url": "https://yahoofinancer.rsquaredacademy.com/,\nhttps://github.com/rsquaredacademy/yahoofinancer",
    "bug_reports": "https://github.com/rsquaredacademy/yahoofinancer/issues",
    "repository": "https://cran.r-project.org/package=yahoofinancer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yahoofinancer Fetch Data from Yahoo Finance API Obtain historical and near real time data related to stocks, index \n    and currencies from the Yahoo Finance API. This package is community maintained \n    and is not officially supported by 'Yahoo'. The accuracy of data is only as \n    correct as provided on <https://finance.yahoo.com/>.  "
  },
  {
    "id": 23283,
    "package_name": "yfinancer",
    "title": "'Yahoo Finance' API Wrapper",
    "description": "Download financial market data, company information, financial statements, options data, and more from the unofficial 'Yahoo Finance' API.",
    "version": "0.1.3",
    "maintainer": "Giovanni Colitti <g.a.colitti@gmail.com>",
    "author": "Giovanni Colitti [aut, cre, cph]",
    "url": "https://github.com/gacolitti/yfinancer",
    "bug_reports": "https://github.com/gacolitti/yfinancer/issues",
    "repository": "https://cran.r-project.org/package=yfinancer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yfinancer 'Yahoo Finance' API Wrapper Download financial market data, company information, financial statements, options data, and more from the unofficial 'Yahoo Finance' API.  "
  },
  {
    "id": 23284,
    "package_name": "yfscreen",
    "title": "Yahoo Finance 'screener' API",
    "description": "Simple and efficient access to Yahoo Finance's 'screener' API <https://finance.yahoo.com/research-hub/screener/> for querying and retrieval of financial data. The core functionality abstracts the complexities of interacting with Yahoo Finance APIs, such as session management, crumb and cookie handling, query construction, pagination, and JSON payload generation. This abstraction allows users to focus on filtering and retrieving data rather than managing API details. Use cases include screening across a range of security types including equities, mutual funds, ETFs, indices, and futures. The package supports advanced query capabilities, including logical operators, nested filters, and customizable payloads. It automatically handles pagination to ensure efficient retrieval of large datasets by fetching results in batches of up to 250 entries per request. Filters can be dynamically defined to accommodate a wide range of screening needs. The implementation leverages standard HTTP libraries to handle API interactions efficiently and provides support for both R and Python to ensure accessibility for a broad audience.",
    "version": "0.1.2",
    "maintainer": "Jason Foster <jason.j.foster@gmail.com>",
    "author": "Jason Foster [aut, cre]",
    "url": "https://github.com/jasonjfoster/screen",
    "bug_reports": "https://github.com/jasonjfoster/screen/issues",
    "repository": "https://cran.r-project.org/package=yfscreen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yfscreen Yahoo Finance 'screener' API Simple and efficient access to Yahoo Finance's 'screener' API <https://finance.yahoo.com/research-hub/screener/> for querying and retrieval of financial data. The core functionality abstracts the complexities of interacting with Yahoo Finance APIs, such as session management, crumb and cookie handling, query construction, pagination, and JSON payload generation. This abstraction allows users to focus on filtering and retrieving data rather than managing API details. Use cases include screening across a range of security types including equities, mutual funds, ETFs, indices, and futures. The package supports advanced query capabilities, including logical operators, nested filters, and customizable payloads. It automatically handles pagination to ensure efficient retrieval of large datasets by fetching results in batches of up to 250 entries per request. Filters can be dynamically defined to accommodate a wide range of screening needs. The implementation leverages standard HTTP libraries to handle API interactions efficiently and provides support for both R and Python to ensure accessibility for a broad audience.  "
  }
]