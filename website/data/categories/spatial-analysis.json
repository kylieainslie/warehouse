[
  {
    "id": 1198,
    "package_name": "sf",
    "title": "Simple Features for R",
    "description": "Support for simple feature access, a standardized way to\nencode and analyze spatial vector data. Binds to 'GDAL'\n<doi:10.5281/zenodo.5884351> for reading and writing data, to\n'GEOS' <doi:10.5281/zenodo.11396894> for geometrical\noperations, and to 'PROJ' <doi:10.5281/zenodo.5884394> for\nprojection conversions and datum transformations. Uses by\ndefault the 's2' package for geometry operations on geodetic\n(long/lat degree) coordinates.",
    "version": "1.0-24",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8049-7069>),\nRoger Bivand [ctb] (ORCID: <https://orcid.org/0000-0003-2392-6140>),\nEtienne Racine [ctb],\nMichael Sumner [ctb],\nIan Cook [ctb],\nTim Keitt [ctb],\nRobin Lovelace [ctb],\nHadley Wickham [ctb],\nJeroen Ooms [ctb] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nKirill M\u00fcller [ctb],\nThomas Lin Pedersen [ctb],\nDan Baston [ctb],\nDewey Dunnington [ctb] (ORCID: <https://orcid.org/0000-0002-9415-4582>)",
    "url": "https://r-spatial.github.io/sf/, https://github.com/r-spatial/sf",
    "bug_reports": "https://github.com/r-spatial/sf/issues",
    "repository": "",
    "exports": [
      [
        ".degAxis"
      ],
      [
        ".get_layout"
      ],
      [
        ".image_scale"
      ],
      [
        ".image_scale_factor"
      ],
      [
        ".stop_geos"
      ],
      [
        "%>%"
      ],
      [
        "as_Spatial"
      ],
      [
        "dbDataType"
      ],
      [
        "dbWriteTable"
      ],
      [
        "FULL_bbox_"
      ],
      [
        "gdal_addo"
      ],
      [
        "gdal_create"
      ],
      [
        "gdal_crs"
      ],
      [
        "gdal_extract"
      ],
      [
        "gdal_inv_geotransform"
      ],
      [
        "gdal_metadata"
      ],
      [
        "gdal_polygonize"
      ],
      [
        "gdal_rasterize"
      ],
      [
        "gdal_read"
      ],
      [
        "gdal_read_mdim"
      ],
      [
        "gdal_subdatasets"
      ],
      [
        "gdal_utils"
      ],
      [
        "gdal_write"
      ],
      [
        "gdal_write_mdim"
      ],
      [
        "get_key_pos"
      ],
      [
        "NA_agr_"
      ],
      [
        "NA_bbox_"
      ],
      [
        "NA_crs_"
      ],
      [
        "NA_m_range_"
      ],
      [
        "NA_z_range_"
      ],
      [
        "plot_sf"
      ],
      [
        "rawToHex"
      ],
      [
        "read_sf"
      ],
      [
        "sf_add_proj_units"
      ],
      [
        "sf_extSoftVersion"
      ],
      [
        "sf_proj_info"
      ],
      [
        "sf_proj_network"
      ],
      [
        "sf_proj_pipelines"
      ],
      [
        "sf_proj_search_paths"
      ],
      [
        "sf_project"
      ],
      [
        "sf_use_s2"
      ],
      [
        "sf.colors"
      ],
      [
        "st_agr"
      ],
      [
        "st_agr<-"
      ],
      [
        "st_area"
      ],
      [
        "st_as_binary"
      ],
      [
        "st_as_grob"
      ],
      [
        "st_as_s2"
      ],
      [
        "st_as_sf"
      ],
      [
        "st_as_sfc"
      ],
      [
        "st_as_text"
      ],
      [
        "st_axis_order"
      ],
      [
        "st_bbox"
      ],
      [
        "st_bind_cols"
      ],
      [
        "st_boundary"
      ],
      [
        "st_break_antimeridian"
      ],
      [
        "st_buffer"
      ],
      [
        "st_can_transform"
      ],
      [
        "st_cast"
      ],
      [
        "st_centroid"
      ],
      [
        "st_collection_extract"
      ],
      [
        "st_combine"
      ],
      [
        "st_concave_hull"
      ],
      [
        "st_contains"
      ],
      [
        "st_contains_properly"
      ],
      [
        "st_convex_hull"
      ],
      [
        "st_coordinates"
      ],
      [
        "st_covered_by"
      ],
      [
        "st_covers"
      ],
      [
        "st_crop"
      ],
      [
        "st_crosses"
      ],
      [
        "st_crs"
      ],
      [
        "st_crs<-"
      ],
      [
        "st_delete"
      ],
      [
        "st_difference"
      ],
      [
        "st_dimension"
      ],
      [
        "st_disjoint"
      ],
      [
        "st_distance"
      ],
      [
        "st_drivers"
      ],
      [
        "st_drop_geometry"
      ],
      [
        "st_equals"
      ],
      [
        "st_equals_exact"
      ],
      [
        "st_exterior_ring"
      ],
      [
        "st_filter"
      ],
      [
        "st_geometry"
      ],
      [
        "st_geometry_type"
      ],
      [
        "st_geometry<-"
      ],
      [
        "st_geometrycollection"
      ],
      [
        "st_graticule"
      ],
      [
        "st_inscribed_circle"
      ],
      [
        "st_interpolate_aw"
      ],
      [
        "st_intersection"
      ],
      [
        "st_intersects"
      ],
      [
        "st_is"
      ],
      [
        "st_is_empty"
      ],
      [
        "st_is_full"
      ],
      [
        "st_is_longlat"
      ],
      [
        "st_is_simple"
      ],
      [
        "st_is_valid"
      ],
      [
        "st_is_within_distance"
      ],
      [
        "st_jitter"
      ],
      [
        "st_join"
      ],
      [
        "st_layers"
      ],
      [
        "st_length"
      ],
      [
        "st_line_interpolate"
      ],
      [
        "st_line_merge"
      ],
      [
        "st_line_project"
      ],
      [
        "st_line_sample"
      ],
      [
        "st_linestring"
      ],
      [
        "st_m_range"
      ],
      [
        "st_make_grid"
      ],
      [
        "st_make_valid"
      ],
      [
        "st_minimum_bounding_circle"
      ],
      [
        "st_minimum_rotated_rectangle"
      ],
      [
        "st_multilinestring"
      ],
      [
        "st_multipoint"
      ],
      [
        "st_multipolygon"
      ],
      [
        "st_nearest_feature"
      ],
      [
        "st_nearest_points"
      ],
      [
        "st_node"
      ],
      [
        "st_normalize"
      ],
      [
        "st_overlaps"
      ],
      [
        "st_perimeter"
      ],
      [
        "st_point"
      ],
      [
        "st_point_on_surface"
      ],
      [
        "st_polygon"
      ],
      [
        "st_polygonize"
      ],
      [
        "st_precision"
      ],
      [
        "st_precision<-"
      ],
      [
        "st_read"
      ],
      [
        "st_read_db"
      ],
      [
        "st_relate"
      ],
      [
        "st_reverse"
      ],
      [
        "st_sample"
      ],
      [
        "st_segmentize"
      ],
      [
        "st_set_agr"
      ],
      [
        "st_set_crs"
      ],
      [
        "st_set_geometry"
      ],
      [
        "st_set_precision"
      ],
      [
        "st_sf"
      ],
      [
        "st_sfc"
      ],
      [
        "st_shift_longitude"
      ],
      [
        "st_simplify"
      ],
      [
        "st_snap"
      ],
      [
        "st_sym_difference"
      ],
      [
        "st_touches"
      ],
      [
        "st_transform"
      ],
      [
        "st_triangulate"
      ],
      [
        "st_triangulate_constrained"
      ],
      [
        "st_union"
      ],
      [
        "st_viewport"
      ],
      [
        "st_voronoi"
      ],
      [
        "st_within"
      ],
      [
        "st_wrap_dateline"
      ],
      [
        "st_write"
      ],
      [
        "st_write_db"
      ],
      [
        "st_z_range"
      ],
      [
        "st_zm"
      ],
      [
        "vec_cast.sfc"
      ],
      [
        "vec_ptype2.sfc"
      ],
      [
        "write_sf"
      ]
    ],
    "topics": [
      [
        "gdal"
      ],
      [
        "geos"
      ],
      [
        "proj"
      ],
      [
        "spatial"
      ],
      [
        "cpp"
      ]
    ],
    "score": 22.7048,
    "stars": 1409,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "sf Simple Features for R Support for simple feature access, a standardized way to\nencode and analyze spatial vector data. Binds to 'GDAL'\n<doi:10.5281/zenodo.5884351> for reading and writing data, to\n'GEOS' <doi:10.5281/zenodo.11396894> for geometrical\noperations, and to 'PROJ' <doi:10.5281/zenodo.5884394> for\nprojection conversions and datum transformations. Uses by\ndefault the 's2' package for geometry operations on geodetic\n(long/lat degree) coordinates. .degAxis .get_layout .image_scale .image_scale_factor .stop_geos %>% as_Spatial dbDataType dbWriteTable FULL_bbox_ gdal_addo gdal_create gdal_crs gdal_extract gdal_inv_geotransform gdal_metadata gdal_polygonize gdal_rasterize gdal_read gdal_read_mdim gdal_subdatasets gdal_utils gdal_write gdal_write_mdim get_key_pos NA_agr_ NA_bbox_ NA_crs_ NA_m_range_ NA_z_range_ plot_sf rawToHex read_sf sf_add_proj_units sf_extSoftVersion sf_proj_info sf_proj_network sf_proj_pipelines sf_proj_search_paths sf_project sf_use_s2 sf.colors st_agr st_agr<- st_area st_as_binary st_as_grob st_as_s2 st_as_sf st_as_sfc st_as_text st_axis_order st_bbox st_bind_cols st_boundary st_break_antimeridian st_buffer st_can_transform st_cast st_centroid st_collection_extract st_combine st_concave_hull st_contains st_contains_properly st_convex_hull st_coordinates st_covered_by st_covers st_crop st_crosses st_crs st_crs<- st_delete st_difference st_dimension st_disjoint st_distance st_drivers st_drop_geometry st_equals st_equals_exact st_exterior_ring st_filter st_geometry st_geometry_type st_geometry<- st_geometrycollection st_graticule st_inscribed_circle st_interpolate_aw st_intersection st_intersects st_is st_is_empty st_is_full st_is_longlat st_is_simple st_is_valid st_is_within_distance st_jitter st_join st_layers st_length st_line_interpolate st_line_merge st_line_project st_line_sample st_linestring st_m_range st_make_grid st_make_valid st_minimum_bounding_circle st_minimum_rotated_rectangle st_multilinestring st_multipoint st_multipolygon st_nearest_feature st_nearest_points st_node st_normalize st_overlaps st_perimeter st_point st_point_on_surface st_polygon st_polygonize st_precision st_precision<- st_read st_read_db st_relate st_reverse st_sample st_segmentize st_set_agr st_set_crs st_set_geometry st_set_precision st_sf st_sfc st_shift_longitude st_simplify st_snap st_sym_difference st_touches st_transform st_triangulate st_triangulate_constrained st_union st_viewport st_voronoi st_within st_wrap_dateline st_write st_write_db st_z_range st_zm vec_cast.sfc vec_ptype2.sfc write_sf gdal geos proj spatial cpp"
  },
  {
    "id": 1280,
    "package_name": "stringr",
    "title": "Simple, Consistent Wrappers for Common String Operations",
    "description": "A consistent, simple and easy to use set of wrappers\naround the fantastic 'stringi' package. All function and\nargument names (and positions) are consistent, all functions\ndeal with \"NA\"'s and zero length vectors in the same way, and\nthe output from one function is easy to feed into the input of\nanother.",
    "version": "1.6.0.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre, cph],\nPosit Software, PBC [cph, fnd]",
    "url": "https://stringr.tidyverse.org,\nhttps://github.com/tidyverse/stringr",
    "bug_reports": "https://github.com/tidyverse/stringr/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "boundary"
      ],
      [
        "coll"
      ],
      [
        "fixed"
      ],
      [
        "invert_match"
      ],
      [
        "regex"
      ],
      [
        "str_c"
      ],
      [
        "str_conv"
      ],
      [
        "str_count"
      ],
      [
        "str_detect"
      ],
      [
        "str_dup"
      ],
      [
        "str_ends"
      ],
      [
        "str_equal"
      ],
      [
        "str_escape"
      ],
      [
        "str_extract"
      ],
      [
        "str_extract_all"
      ],
      [
        "str_flatten"
      ],
      [
        "str_flatten_comma"
      ],
      [
        "str_glue"
      ],
      [
        "str_glue_data"
      ],
      [
        "str_ilike"
      ],
      [
        "str_interp"
      ],
      [
        "str_length"
      ],
      [
        "str_like"
      ],
      [
        "str_locate"
      ],
      [
        "str_locate_all"
      ],
      [
        "str_match"
      ],
      [
        "str_match_all"
      ],
      [
        "str_order"
      ],
      [
        "str_pad"
      ],
      [
        "str_rank"
      ],
      [
        "str_remove"
      ],
      [
        "str_remove_all"
      ],
      [
        "str_replace"
      ],
      [
        "str_replace_all"
      ],
      [
        "str_replace_na"
      ],
      [
        "str_sort"
      ],
      [
        "str_split"
      ],
      [
        "str_split_1"
      ],
      [
        "str_split_fixed"
      ],
      [
        "str_split_i"
      ],
      [
        "str_squish"
      ],
      [
        "str_starts"
      ],
      [
        "str_sub"
      ],
      [
        "str_sub_all"
      ],
      [
        "str_sub<-"
      ],
      [
        "str_subset"
      ],
      [
        "str_to_camel"
      ],
      [
        "str_to_kebab"
      ],
      [
        "str_to_lower"
      ],
      [
        "str_to_sentence"
      ],
      [
        "str_to_snake"
      ],
      [
        "str_to_title"
      ],
      [
        "str_to_upper"
      ],
      [
        "str_trim"
      ],
      [
        "str_trunc"
      ],
      [
        "str_unique"
      ],
      [
        "str_view"
      ],
      [
        "str_view_all"
      ],
      [
        "str_which"
      ],
      [
        "str_width"
      ],
      [
        "str_wrap"
      ],
      [
        "word"
      ]
    ],
    "topics": [
      [
        "regular-expression"
      ],
      [
        "strings"
      ]
    ],
    "score": 22.6469,
    "stars": 645,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "stringr Simple, Consistent Wrappers for Common String Operations A consistent, simple and easy to use set of wrappers\naround the fantastic 'stringi' package. All function and\nargument names (and positions) are consistent, all functions\ndeal with \"NA\"'s and zero length vectors in the same way, and\nthe output from one function is easy to feed into the input of\nanother. %>% boundary coll fixed invert_match regex str_c str_conv str_count str_detect str_dup str_ends str_equal str_escape str_extract str_extract_all str_flatten str_flatten_comma str_glue str_glue_data str_ilike str_interp str_length str_like str_locate str_locate_all str_match str_match_all str_order str_pad str_rank str_remove str_remove_all str_replace str_replace_all str_replace_na str_sort str_split str_split_1 str_split_fixed str_split_i str_squish str_starts str_sub str_sub_all str_sub<- str_subset str_to_camel str_to_kebab str_to_lower str_to_sentence str_to_snake str_to_title str_to_upper str_trim str_trunc str_unique str_view str_view_all str_which str_width str_wrap word regular-expression strings"
  },
  {
    "id": 1414,
    "package_name": "vctrs",
    "title": "Vector Helpers",
    "description": "Defines new notions of prototype and size that are used to\nprovide tools for consistent and well-founded type-coercion and\nsize-recycling, and are in turn connected to ideas of type- and\nsize-stability useful for analysing function interfaces.",
    "version": "0.6.5.9000",
    "maintainer": "Davis Vaughan <davis@posit.co>",
    "author": "Hadley Wickham [aut],\nLionel Henry [aut],\nDavis Vaughan [aut, cre],\ndata.table team [cph] (Radix sort based on data.table's forder() and\ntheir contribution to R's order()),\nPosit Software, PBC [cph, fnd]",
    "url": "https://vctrs.r-lib.org/, https://github.com/r-lib/vctrs",
    "bug_reports": "https://github.com/r-lib/vctrs/issues",
    "repository": "",
    "exports": [
      [
        "%0%"
      ],
      [
        "allow_lossy_cast"
      ],
      [
        "as_list_of"
      ],
      [
        "data_frame"
      ],
      [
        "df_cast"
      ],
      [
        "df_list"
      ],
      [
        "df_ptype2"
      ],
      [
        "field"
      ],
      [
        "field<-"
      ],
      [
        "fields"
      ],
      [
        "is_list_of"
      ],
      [
        "list_all_recyclable"
      ],
      [
        "list_all_size"
      ],
      [
        "list_all_vectors"
      ],
      [
        "list_check_all_recyclable"
      ],
      [
        "list_check_all_size"
      ],
      [
        "list_check_all_vectors"
      ],
      [
        "list_combine"
      ],
      [
        "list_drop_empty"
      ],
      [
        "list_of"
      ],
      [
        "list_of_ptype"
      ],
      [
        "list_of_size"
      ],
      [
        "list_of_transpose"
      ],
      [
        "list_sizes"
      ],
      [
        "list_unchop"
      ],
      [
        "maybe_lossy_cast"
      ],
      [
        "MISSING"
      ],
      [
        "n_fields"
      ],
      [
        "new_data_frame"
      ],
      [
        "new_date"
      ],
      [
        "new_datetime"
      ],
      [
        "new_duration"
      ],
      [
        "new_factor"
      ],
      [
        "new_list_of"
      ],
      [
        "new_ordered"
      ],
      [
        "new_rcrd"
      ],
      [
        "new_vctr"
      ],
      [
        "num_as_location"
      ],
      [
        "num_as_location2"
      ],
      [
        "obj_check_list"
      ],
      [
        "obj_check_vector"
      ],
      [
        "obj_is_list"
      ],
      [
        "obj_is_vector"
      ],
      [
        "obj_print"
      ],
      [
        "obj_print_data"
      ],
      [
        "obj_print_footer"
      ],
      [
        "obj_print_header"
      ],
      [
        "obj_str"
      ],
      [
        "obj_str_data"
      ],
      [
        "obj_str_footer"
      ],
      [
        "obj_str_header"
      ],
      [
        "s3_register"
      ],
      [
        "stop_incompatible_cast"
      ],
      [
        "stop_incompatible_op"
      ],
      [
        "stop_incompatible_size"
      ],
      [
        "stop_incompatible_type"
      ],
      [
        "tib_cast"
      ],
      [
        "tib_ptype2"
      ],
      [
        "unspecified"
      ],
      [
        "vec_any_missing"
      ],
      [
        "vec_arith"
      ],
      [
        "vec_arith_base"
      ],
      [
        "vec_arith.Date"
      ],
      [
        "vec_arith.difftime"
      ],
      [
        "vec_arith.logical"
      ],
      [
        "vec_arith.numeric"
      ],
      [
        "vec_arith.POSIXct"
      ],
      [
        "vec_arith.POSIXlt"
      ],
      [
        "vec_as_index"
      ],
      [
        "vec_as_location"
      ],
      [
        "vec_as_location2"
      ],
      [
        "vec_as_names"
      ],
      [
        "vec_as_names_legacy"
      ],
      [
        "vec_as_subscript"
      ],
      [
        "vec_as_subscript2"
      ],
      [
        "vec_assert"
      ],
      [
        "vec_assign"
      ],
      [
        "vec_c"
      ],
      [
        "vec_case_when"
      ],
      [
        "vec_cast"
      ],
      [
        "vec_cast_common"
      ],
      [
        "vec_cast.character"
      ],
      [
        "vec_cast.complex"
      ],
      [
        "vec_cast.data.frame"
      ],
      [
        "vec_cast.Date"
      ],
      [
        "vec_cast.difftime"
      ],
      [
        "vec_cast.double"
      ],
      [
        "vec_cast.factor"
      ],
      [
        "vec_cast.integer"
      ],
      [
        "vec_cast.integer64"
      ],
      [
        "vec_cast.list"
      ],
      [
        "vec_cast.logical"
      ],
      [
        "vec_cast.ordered"
      ],
      [
        "vec_cast.POSIXct"
      ],
      [
        "vec_cast.POSIXlt"
      ],
      [
        "vec_cast.raw"
      ],
      [
        "vec_cast.vctrs_list_of"
      ],
      [
        "vec_cbind"
      ],
      [
        "vec_cbind_frame_ptype"
      ],
      [
        "vec_check_list"
      ],
      [
        "vec_check_recyclable"
      ],
      [
        "vec_check_size"
      ],
      [
        "vec_chop"
      ],
      [
        "vec_compare"
      ],
      [
        "vec_count"
      ],
      [
        "vec_data"
      ],
      [
        "vec_default_cast"
      ],
      [
        "vec_default_ptype2"
      ],
      [
        "vec_detect_complete"
      ],
      [
        "vec_detect_missing"
      ],
      [
        "vec_duplicate_any"
      ],
      [
        "vec_duplicate_detect"
      ],
      [
        "vec_duplicate_id"
      ],
      [
        "vec_empty"
      ],
      [
        "vec_equal"
      ],
      [
        "vec_equal_na"
      ],
      [
        "vec_expand_grid"
      ],
      [
        "vec_fill_missing"
      ],
      [
        "vec_group_id"
      ],
      [
        "vec_group_loc"
      ],
      [
        "vec_group_rle"
      ],
      [
        "vec_identify_runs"
      ],
      [
        "vec_if_else"
      ],
      [
        "vec_in"
      ],
      [
        "vec_init"
      ],
      [
        "vec_init_along"
      ],
      [
        "vec_interleave"
      ],
      [
        "vec_is"
      ],
      [
        "vec_is_empty"
      ],
      [
        "vec_is_list"
      ],
      [
        "vec_locate_matches"
      ],
      [
        "vec_locate_sorted_groups"
      ],
      [
        "vec_match"
      ],
      [
        "vec_math"
      ],
      [
        "vec_math_base"
      ],
      [
        "vec_names"
      ],
      [
        "vec_names2"
      ],
      [
        "vec_order"
      ],
      [
        "vec_pall"
      ],
      [
        "vec_pany"
      ],
      [
        "vec_proxy"
      ],
      [
        "vec_proxy_compare"
      ],
      [
        "vec_proxy_equal"
      ],
      [
        "vec_proxy_order"
      ],
      [
        "vec_ptype"
      ],
      [
        "vec_ptype_abbr"
      ],
      [
        "vec_ptype_common"
      ],
      [
        "vec_ptype_finalise"
      ],
      [
        "vec_ptype_full"
      ],
      [
        "vec_ptype_show"
      ],
      [
        "vec_ptype2"
      ],
      [
        "vec_ptype2.AsIs"
      ],
      [
        "vec_ptype2.character"
      ],
      [
        "vec_ptype2.complex"
      ],
      [
        "vec_ptype2.data.frame"
      ],
      [
        "vec_ptype2.Date"
      ],
      [
        "vec_ptype2.difftime"
      ],
      [
        "vec_ptype2.double"
      ],
      [
        "vec_ptype2.factor"
      ],
      [
        "vec_ptype2.integer"
      ],
      [
        "vec_ptype2.integer64"
      ],
      [
        "vec_ptype2.list"
      ],
      [
        "vec_ptype2.logical"
      ],
      [
        "vec_ptype2.ordered"
      ],
      [
        "vec_ptype2.POSIXct"
      ],
      [
        "vec_ptype2.POSIXlt"
      ],
      [
        "vec_ptype2.raw"
      ],
      [
        "vec_ptype2.vctrs_list_of"
      ],
      [
        "vec_rank"
      ],
      [
        "vec_rbind"
      ],
      [
        "vec_recode_values"
      ],
      [
        "vec_recycle"
      ],
      [
        "vec_recycle_common"
      ],
      [
        "vec_rep"
      ],
      [
        "vec_rep_each"
      ],
      [
        "vec_repeat"
      ],
      [
        "vec_replace_values"
      ],
      [
        "vec_replace_when"
      ],
      [
        "vec_restore"
      ],
      [
        "vec_run_sizes"
      ],
      [
        "vec_seq_along"
      ],
      [
        "vec_set_difference"
      ],
      [
        "vec_set_intersect"
      ],
      [
        "vec_set_names"
      ],
      [
        "vec_set_symmetric_difference"
      ],
      [
        "vec_set_union"
      ],
      [
        "vec_size"
      ],
      [
        "vec_size_common"
      ],
      [
        "vec_slice"
      ],
      [
        "vec_slice<-"
      ],
      [
        "vec_sort"
      ],
      [
        "vec_split"
      ],
      [
        "vec_type"
      ],
      [
        "vec_type_common"
      ],
      [
        "vec_type2"
      ],
      [
        "vec_unchop"
      ],
      [
        "vec_unique"
      ],
      [
        "vec_unique_count"
      ],
      [
        "vec_unique_loc"
      ],
      [
        "vec_unrep"
      ]
    ],
    "topics": [
      [
        "s3-vectors"
      ]
    ],
    "score": 19.7776,
    "stars": 297,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "vctrs Vector Helpers Defines new notions of prototype and size that are used to\nprovide tools for consistent and well-founded type-coercion and\nsize-recycling, and are in turn connected to ideas of type- and\nsize-stability useful for analysing function interfaces. %0% allow_lossy_cast as_list_of data_frame df_cast df_list df_ptype2 field field<- fields is_list_of list_all_recyclable list_all_size list_all_vectors list_check_all_recyclable list_check_all_size list_check_all_vectors list_combine list_drop_empty list_of list_of_ptype list_of_size list_of_transpose list_sizes list_unchop maybe_lossy_cast MISSING n_fields new_data_frame new_date new_datetime new_duration new_factor new_list_of new_ordered new_rcrd new_vctr num_as_location num_as_location2 obj_check_list obj_check_vector obj_is_list obj_is_vector obj_print obj_print_data obj_print_footer obj_print_header obj_str obj_str_data obj_str_footer obj_str_header s3_register stop_incompatible_cast stop_incompatible_op stop_incompatible_size stop_incompatible_type tib_cast tib_ptype2 unspecified vec_any_missing vec_arith vec_arith_base vec_arith.Date vec_arith.difftime vec_arith.logical vec_arith.numeric vec_arith.POSIXct vec_arith.POSIXlt vec_as_index vec_as_location vec_as_location2 vec_as_names vec_as_names_legacy vec_as_subscript vec_as_subscript2 vec_assert vec_assign vec_c vec_case_when vec_cast vec_cast_common vec_cast.character vec_cast.complex vec_cast.data.frame vec_cast.Date vec_cast.difftime vec_cast.double vec_cast.factor vec_cast.integer vec_cast.integer64 vec_cast.list vec_cast.logical vec_cast.ordered vec_cast.POSIXct vec_cast.POSIXlt vec_cast.raw vec_cast.vctrs_list_of vec_cbind vec_cbind_frame_ptype vec_check_list vec_check_recyclable vec_check_size vec_chop vec_compare vec_count vec_data vec_default_cast vec_default_ptype2 vec_detect_complete vec_detect_missing vec_duplicate_any vec_duplicate_detect vec_duplicate_id vec_empty vec_equal vec_equal_na vec_expand_grid vec_fill_missing vec_group_id vec_group_loc vec_group_rle vec_identify_runs vec_if_else vec_in vec_init vec_init_along vec_interleave vec_is vec_is_empty vec_is_list vec_locate_matches vec_locate_sorted_groups vec_match vec_math vec_math_base vec_names vec_names2 vec_order vec_pall vec_pany vec_proxy vec_proxy_compare vec_proxy_equal vec_proxy_order vec_ptype vec_ptype_abbr vec_ptype_common vec_ptype_finalise vec_ptype_full vec_ptype_show vec_ptype2 vec_ptype2.AsIs vec_ptype2.character vec_ptype2.complex vec_ptype2.data.frame vec_ptype2.Date vec_ptype2.difftime vec_ptype2.double vec_ptype2.factor vec_ptype2.integer vec_ptype2.integer64 vec_ptype2.list vec_ptype2.logical vec_ptype2.ordered vec_ptype2.POSIXct vec_ptype2.POSIXlt vec_ptype2.raw vec_ptype2.vctrs_list_of vec_rank vec_rbind vec_recode_values vec_recycle vec_recycle_common vec_rep vec_rep_each vec_repeat vec_replace_values vec_replace_when vec_restore vec_run_sizes vec_seq_along vec_set_difference vec_set_intersect vec_set_names vec_set_symmetric_difference vec_set_union vec_size vec_size_common vec_slice vec_slice<- vec_sort vec_split vec_type vec_type_common vec_type2 vec_unchop vec_unique vec_unique_count vec_unique_loc vec_unrep s3-vectors"
  },
  {
    "id": 1273,
    "package_name": "stars",
    "title": "Spatiotemporal Arrays, Raster and Vector Data Cubes",
    "description": "Reading, manipulating, writing and plotting spatiotemporal\narrays (raster and vector data cubes) in 'R', using 'GDAL'\nbindings provided by 'sf', and 'NetCDF' bindings by 'ncmeta'\nand 'RNetCDF'.",
    "version": "0.7-0",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8049-7069>),\nMichael Sumner [ctb] (ORCID: <https://orcid.org/0000-0002-2471-7511>),\nEtienne Racine [ctb],\nAdriano Fantini [ctb],\nDavid Blodgett [ctb],\nKrzysztof Dyba [ctb] (ORCID: <https://orcid.org/0000-0002-8614-3816>)",
    "url": "https://r-spatial.github.io/stars/,\nhttps://github.com/r-spatial/stars/",
    "bug_reports": "https://github.com/r-spatial/stars/issues/",
    "repository": "",
    "exports": [
      [
        "%in%"
      ],
      [
        "as.tbl_cube.stars"
      ],
      [
        "detect.driver"
      ],
      [
        "expand_dimensions"
      ],
      [
        "geom_stars"
      ],
      [
        "make_intervals"
      ],
      [
        "read_mdim"
      ],
      [
        "read_ncdf"
      ],
      [
        "read_stars"
      ],
      [
        "st_apply"
      ],
      [
        "st_as_stars"
      ],
      [
        "st_cells"
      ],
      [
        "st_contour"
      ],
      [
        "st_dim_to_attr"
      ],
      [
        "st_dimensions"
      ],
      [
        "st_dimensions<-"
      ],
      [
        "st_downsample"
      ],
      [
        "st_extract"
      ],
      [
        "st_flip"
      ],
      [
        "st_geotransform"
      ],
      [
        "st_geotransform<-"
      ],
      [
        "st_get_dimension_values"
      ],
      [
        "st_mosaic"
      ],
      [
        "st_raster_type"
      ],
      [
        "st_rasterize"
      ],
      [
        "st_redimension"
      ],
      [
        "st_res"
      ],
      [
        "st_rgb"
      ],
      [
        "st_rotate"
      ],
      [
        "st_set_bbox"
      ],
      [
        "st_set_dimensions"
      ],
      [
        "st_sfc2xy"
      ],
      [
        "st_tile"
      ],
      [
        "st_warp"
      ],
      [
        "st_xy2sfc"
      ],
      [
        "write_mdim"
      ],
      [
        "write_stars"
      ]
    ],
    "topics": [
      [
        "raster"
      ],
      [
        "satellite-images"
      ],
      [
        "spatial"
      ]
    ],
    "score": 18.3493,
    "stars": 593,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "stars Spatiotemporal Arrays, Raster and Vector Data Cubes Reading, manipulating, writing and plotting spatiotemporal\narrays (raster and vector data cubes) in 'R', using 'GDAL'\nbindings provided by 'sf', and 'NetCDF' bindings by 'ncmeta'\nand 'RNetCDF'. %in% as.tbl_cube.stars detect.driver expand_dimensions geom_stars make_intervals read_mdim read_ncdf read_stars st_apply st_as_stars st_cells st_contour st_dim_to_attr st_dimensions st_dimensions<- st_downsample st_extract st_flip st_geotransform st_geotransform<- st_get_dimension_values st_mosaic st_raster_type st_rasterize st_redimension st_res st_rgb st_rotate st_set_bbox st_set_dimensions st_sfc2xy st_tile st_warp st_xy2sfc write_mdim write_stars raster satellite-images spatial"
  },
  {
    "id": 1315,
    "package_name": "terra",
    "title": "Spatial Data Analysis",
    "description": "Methods for spatial data analysis with vector (points,\nlines, polygons) and raster (grid) data. Methods for vector\ndata include geometric operations such as intersect and buffer.\nRaster methods include local, focal, global, zonal and\ngeometric operations. The predict and interpolate methods\nfacilitate the use of regression type (interpolation, machine\nlearning) models for spatial prediction, including with\nsatellite remote sensing data. Processing of very large files\nis supported. See the manual and tutorials on\n<https://rspatial.org/> to get started.",
    "version": "1.8-90",
    "maintainer": "Robert J. Hijmans <r.hijmans@gmail.com>",
    "author": "Robert J. Hijmans [cre, aut] (ORCID:\n<https://orcid.org/0000-0001-5872-2872>),\nM\u00e1rcia Barbosa [ctb] (ORCID: <https://orcid.org/0000-0001-8972-7713>),\nRoger Bivand [ctb] (ORCID: <https://orcid.org/0000-0003-2392-6140>),\nAndrew Brown [ctb] (ORCID: <https://orcid.org/0000-0002-4565-533X>),\nMichael Chirico [ctb] (ORCID: <https://orcid.org/0000-0003-0787-087X>),\nEmanuele Cordano [ctb] (ORCID: <https://orcid.org/0000-0002-3508-5898>),\nKrzysztof Dyba [ctb] (ORCID: <https://orcid.org/0000-0002-8614-3816>),\nEdzer Pebesma [ctb] (ORCID: <https://orcid.org/0000-0001-8049-7069>),\nBarry Rowlingson [ctb] (ORCID: <https://orcid.org/0000-0002-8586-6625>),\nMichael D. Sumner [ctb] (ORCID:\n<https://orcid.org/0000-0002-2471-7511>)",
    "url": "https://rspatial.org/, https://rspatial.github.io/terra/",
    "bug_reports": "https://github.com/rspatial/terra/issues",
    "repository": "",
    "exports": [
      [
        "%in%"
      ],
      [
        "activeCat"
      ],
      [
        "activeCat<-"
      ],
      [
        "add_abline"
      ],
      [
        "add_box"
      ],
      [
        "add_grid"
      ],
      [
        "add_legend"
      ],
      [
        "add_mtext"
      ],
      [
        "add<-"
      ],
      [
        "addCats"
      ],
      [
        "adjacent"
      ],
      [
        "aggregate"
      ],
      [
        "align"
      ],
      [
        "all.equal"
      ],
      [
        "allNA"
      ],
      [
        "animate"
      ],
      [
        "app"
      ],
      [
        "approximate"
      ],
      [
        "ar_info"
      ],
      [
        "area"
      ],
      [
        "Arith"
      ],
      [
        "as.array"
      ],
      [
        "as.bool"
      ],
      [
        "as.contour"
      ],
      [
        "as.data.frame"
      ],
      [
        "as.factor"
      ],
      [
        "as.int"
      ],
      [
        "as.lines"
      ],
      [
        "as.list"
      ],
      [
        "as.matrix"
      ],
      [
        "as.points"
      ],
      [
        "as.polygons"
      ],
      [
        "as.raster"
      ],
      [
        "atan_2"
      ],
      [
        "atan2"
      ],
      [
        "autocor"
      ],
      [
        "barplot"
      ],
      [
        "bestMatch"
      ],
      [
        "blocks"
      ],
      [
        "boundaries"
      ],
      [
        "boxplot"
      ],
      [
        "buffer"
      ],
      [
        "cartogram"
      ],
      [
        "catalyze"
      ],
      [
        "categories"
      ],
      [
        "cats"
      ],
      [
        "cbind2"
      ],
      [
        "cellFromRowCol"
      ],
      [
        "cellFromRowColCombine"
      ],
      [
        "cellFromXY"
      ],
      [
        "cells"
      ],
      [
        "cellSize"
      ],
      [
        "centroids"
      ],
      [
        "chunk"
      ],
      [
        "clamp"
      ],
      [
        "clamp_ts"
      ],
      [
        "classify"
      ],
      [
        "clearance"
      ],
      [
        "clearVSIcache"
      ],
      [
        "click"
      ],
      [
        "colFromCell"
      ],
      [
        "colFromX"
      ],
      [
        "colMeans"
      ],
      [
        "colorize"
      ],
      [
        "colSums"
      ],
      [
        "coltab"
      ],
      [
        "coltab<-"
      ],
      [
        "combineGeoms"
      ],
      [
        "combineLevels"
      ],
      [
        "compare"
      ],
      [
        "Compare"
      ],
      [
        "compareGeom"
      ],
      [
        "concats"
      ],
      [
        "contour"
      ],
      [
        "convHull"
      ],
      [
        "costDist"
      ],
      [
        "countNA"
      ],
      [
        "cover"
      ],
      [
        "crds"
      ],
      [
        "crop"
      ],
      [
        "crosstab"
      ],
      [
        "crs"
      ],
      [
        "crs<-"
      ],
      [
        "datatype"
      ],
      [
        "deepcopy"
      ],
      [
        "delaunay"
      ],
      [
        "densify"
      ],
      [
        "density"
      ],
      [
        "depth"
      ],
      [
        "depth<-"
      ],
      [
        "depthName"
      ],
      [
        "depthName<-"
      ],
      [
        "depthUnit"
      ],
      [
        "depthUnit<-"
      ],
      [
        "describe"
      ],
      [
        "diff"
      ],
      [
        "direction"
      ],
      [
        "disagg"
      ],
      [
        "distance"
      ],
      [
        "divide"
      ],
      [
        "dots"
      ],
      [
        "draw"
      ],
      [
        "droplevels"
      ],
      [
        "elongate"
      ],
      [
        "emptyGeoms"
      ],
      [
        "erase"
      ],
      [
        "expanse"
      ],
      [
        "ext"
      ],
      [
        "ext<-"
      ],
      [
        "extend"
      ],
      [
        "extract"
      ],
      [
        "extractAlong"
      ],
      [
        "extractRange"
      ],
      [
        "fileBlocksize"
      ],
      [
        "fillHoles"
      ],
      [
        "fillTime"
      ],
      [
        "flip"
      ],
      [
        "flowAccumulation"
      ],
      [
        "focal"
      ],
      [
        "focal3D"
      ],
      [
        "focalCpp"
      ],
      [
        "focalMat"
      ],
      [
        "focalPairs"
      ],
      [
        "focalReg"
      ],
      [
        "focalValues"
      ],
      [
        "forceCCW"
      ],
      [
        "free_RAM"
      ],
      [
        "freq"
      ],
      [
        "gaps"
      ],
      [
        "gdal"
      ],
      [
        "gdalCache"
      ],
      [
        "geom"
      ],
      [
        "geomtype"
      ],
      [
        "getGDALconfig"
      ],
      [
        "getTileExtents"
      ],
      [
        "global"
      ],
      [
        "graticule"
      ],
      [
        "gridDist"
      ],
      [
        "gridDistance"
      ],
      [
        "halo"
      ],
      [
        "has.colors"
      ],
      [
        "has.RGB"
      ],
      [
        "has.time"
      ],
      [
        "hasMinMax"
      ],
      [
        "hasValues"
      ],
      [
        "head"
      ],
      [
        "hist"
      ],
      [
        "hull"
      ],
      [
        "identical"
      ],
      [
        "ifel"
      ],
      [
        "image"
      ],
      [
        "impose"
      ],
      [
        "inext"
      ],
      [
        "init"
      ],
      [
        "inMemory"
      ],
      [
        "inset"
      ],
      [
        "interpIDW"
      ],
      [
        "interpNear"
      ],
      [
        "interpolate"
      ],
      [
        "intersect"
      ],
      [
        "is.bool"
      ],
      [
        "is.empty"
      ],
      [
        "is.factor"
      ],
      [
        "is.flipped"
      ],
      [
        "is.int"
      ],
      [
        "is.lines"
      ],
      [
        "is.lonlat"
      ],
      [
        "is.num"
      ],
      [
        "is.points"
      ],
      [
        "is.polygons"
      ],
      [
        "is.related"
      ],
      [
        "is.rotated"
      ],
      [
        "is.valid"
      ],
      [
        "isFALSE"
      ],
      [
        "isTRUE"
      ],
      [
        "k_means"
      ],
      [
        "lapp"
      ],
      [
        "layerCor"
      ],
      [
        "levels"
      ],
      [
        "libVersion"
      ],
      [
        "linearUnits"
      ],
      [
        "lines"
      ],
      [
        "logic"
      ],
      [
        "Logic"
      ],
      [
        "longnames"
      ],
      [
        "longnames<-"
      ],
      [
        "makeNodes"
      ],
      [
        "makeTiles"
      ],
      [
        "makeValid"
      ],
      [
        "makeVRT"
      ],
      [
        "map_extent"
      ],
      [
        "map.pal"
      ],
      [
        "mask"
      ],
      [
        "match"
      ],
      [
        "math"
      ],
      [
        "Math"
      ],
      [
        "Math2"
      ],
      [
        "mean"
      ],
      [
        "median"
      ],
      [
        "mem_info"
      ],
      [
        "merge"
      ],
      [
        "mergeLines"
      ],
      [
        "mergeTime"
      ],
      [
        "meta"
      ],
      [
        "metags"
      ],
      [
        "metags<-"
      ],
      [
        "minmax"
      ],
      [
        "modal"
      ],
      [
        "mosaic"
      ],
      [
        "na.omit"
      ],
      [
        "NAflag"
      ],
      [
        "NAflag<-"
      ],
      [
        "names"
      ],
      [
        "ncell"
      ],
      [
        "ncol"
      ],
      [
        "ncol<-"
      ],
      [
        "nearby"
      ],
      [
        "nearest"
      ],
      [
        "NIDP"
      ],
      [
        "nlyr"
      ],
      [
        "nlyr<-"
      ],
      [
        "noNA"
      ],
      [
        "normalize.longitude"
      ],
      [
        "north"
      ],
      [
        "not.na"
      ],
      [
        "nrow"
      ],
      [
        "nrow<-"
      ],
      [
        "nseg"
      ],
      [
        "nsrc"
      ],
      [
        "origin"
      ],
      [
        "origin<-"
      ],
      [
        "pairs"
      ],
      [
        "panel"
      ],
      [
        "patches"
      ],
      [
        "perim"
      ],
      [
        "persp"
      ],
      [
        "pitfinder"
      ],
      [
        "plet"
      ],
      [
        "plot"
      ],
      [
        "plotRGB"
      ],
      [
        "points"
      ],
      [
        "polys"
      ],
      [
        "prcomp"
      ],
      [
        "predict"
      ],
      [
        "princomp"
      ],
      [
        "project"
      ],
      [
        "quantile"
      ],
      [
        "query"
      ],
      [
        "rangeFill"
      ],
      [
        "rapp"
      ],
      [
        "rast"
      ],
      [
        "rasterize"
      ],
      [
        "rasterizeGeom"
      ],
      [
        "rasterizeWin"
      ],
      [
        "rcl"
      ],
      [
        "readRDS"
      ],
      [
        "readStart"
      ],
      [
        "readStop"
      ],
      [
        "readValues"
      ],
      [
        "rectify"
      ],
      [
        "regress"
      ],
      [
        "relate"
      ],
      [
        "removeDupNodes"
      ],
      [
        "res"
      ],
      [
        "res<-"
      ],
      [
        "resample"
      ],
      [
        "rescale"
      ],
      [
        "rev"
      ],
      [
        "RGB"
      ],
      [
        "RGB<-"
      ],
      [
        "roll"
      ],
      [
        "rotate"
      ],
      [
        "round"
      ],
      [
        "rowColCombine"
      ],
      [
        "rowColFromCell"
      ],
      [
        "rowFromCell"
      ],
      [
        "rowFromY"
      ],
      [
        "rowMeans"
      ],
      [
        "rowSums"
      ],
      [
        "same.crs"
      ],
      [
        "sapp"
      ],
      [
        "saveRDS"
      ],
      [
        "sbar"
      ],
      [
        "scale"
      ],
      [
        "scale_linear"
      ],
      [
        "scoff"
      ],
      [
        "scoff<-"
      ],
      [
        "sds"
      ],
      [
        "segregate"
      ],
      [
        "sel"
      ],
      [
        "selectHighest"
      ],
      [
        "selectRange"
      ],
      [
        "serialize"
      ],
      [
        "set.cats"
      ],
      [
        "set.crs"
      ],
      [
        "set.ext"
      ],
      [
        "set.names"
      ],
      [
        "set.RGB"
      ],
      [
        "set.values"
      ],
      [
        "set.window"
      ],
      [
        "setGDALconfig"
      ],
      [
        "setMinMax"
      ],
      [
        "setValues"
      ],
      [
        "shade"
      ],
      [
        "sharedPaths"
      ],
      [
        "shift"
      ],
      [
        "sieve"
      ],
      [
        "simplifyGeom"
      ],
      [
        "simplifyLevels"
      ],
      [
        "size"
      ],
      [
        "snap"
      ],
      [
        "sort"
      ],
      [
        "sources"
      ],
      [
        "spatSample"
      ],
      [
        "spin"
      ],
      [
        "split"
      ],
      [
        "sprc"
      ],
      [
        "stdev"
      ],
      [
        "stretch"
      ],
      [
        "subset"
      ],
      [
        "subst"
      ],
      [
        "summary"
      ],
      [
        "Summary"
      ],
      [
        "surfArea"
      ],
      [
        "svc"
      ],
      [
        "symdif"
      ],
      [
        "t"
      ],
      [
        "tail"
      ],
      [
        "tapp"
      ],
      [
        "terrain"
      ],
      [
        "terraOptions"
      ],
      [
        "text"
      ],
      [
        "thresh"
      ],
      [
        "tighten"
      ],
      [
        "time"
      ],
      [
        "time<-"
      ],
      [
        "timeInfo"
      ],
      [
        "tmpFiles"
      ],
      [
        "toMemory"
      ],
      [
        "trans"
      ],
      [
        "trim"
      ],
      [
        "union"
      ],
      [
        "unique"
      ],
      [
        "units"
      ],
      [
        "units<-"
      ],
      [
        "unloadGDALdrivers"
      ],
      [
        "unserialize"
      ],
      [
        "unwrap"
      ],
      [
        "update"
      ],
      [
        "values"
      ],
      [
        "values<-"
      ],
      [
        "varnames"
      ],
      [
        "varnames<-"
      ],
      [
        "vect"
      ],
      [
        "vector_layers"
      ],
      [
        "viewshed"
      ],
      [
        "voronoi"
      ],
      [
        "vrt"
      ],
      [
        "vrt_tiles"
      ],
      [
        "watershed"
      ],
      [
        "weighted.mean"
      ],
      [
        "where.max"
      ],
      [
        "where.min"
      ],
      [
        "which.lyr"
      ],
      [
        "which.max"
      ],
      [
        "which.min"
      ],
      [
        "width"
      ],
      [
        "window"
      ],
      [
        "window<-"
      ],
      [
        "wrap"
      ],
      [
        "wrapCache"
      ],
      [
        "writeCDF"
      ],
      [
        "writeRaster"
      ],
      [
        "writeStart"
      ],
      [
        "writeStop"
      ],
      [
        "writeValues"
      ],
      [
        "writeVector"
      ],
      [
        "xapp"
      ],
      [
        "xFromCell"
      ],
      [
        "xFromCol"
      ],
      [
        "xmax"
      ],
      [
        "xmax<-"
      ],
      [
        "xmin"
      ],
      [
        "xmin<-"
      ],
      [
        "xres"
      ],
      [
        "xyFromCell"
      ],
      [
        "yFromCell"
      ],
      [
        "yFromRow"
      ],
      [
        "ymax"
      ],
      [
        "ymax<-"
      ],
      [
        "ymin"
      ],
      [
        "ymin<-"
      ],
      [
        "yres"
      ],
      [
        "zonal"
      ],
      [
        "zoom"
      ]
    ],
    "topics": [
      [
        "geospatial"
      ],
      [
        "raster"
      ],
      [
        "spatial"
      ],
      [
        "vector"
      ],
      [
        "proj"
      ],
      [
        "gdal"
      ],
      [
        "geos"
      ],
      [
        "cpp"
      ]
    ],
    "score": 18.0522,
    "stars": 594,
    "primary_category": "spatial",
    "source_universe": "rspatial",
    "search_text": "terra Spatial Data Analysis Methods for spatial data analysis with vector (points,\nlines, polygons) and raster (grid) data. Methods for vector\ndata include geometric operations such as intersect and buffer.\nRaster methods include local, focal, global, zonal and\ngeometric operations. The predict and interpolate methods\nfacilitate the use of regression type (interpolation, machine\nlearning) models for spatial prediction, including with\nsatellite remote sensing data. Processing of very large files\nis supported. See the manual and tutorials on\n<https://rspatial.org/> to get started. %in% activeCat activeCat<- add_abline add_box add_grid add_legend add_mtext add<- addCats adjacent aggregate align all.equal allNA animate app approximate ar_info area Arith as.array as.bool as.contour as.data.frame as.factor as.int as.lines as.list as.matrix as.points as.polygons as.raster atan_2 atan2 autocor barplot bestMatch blocks boundaries boxplot buffer cartogram catalyze categories cats cbind2 cellFromRowCol cellFromRowColCombine cellFromXY cells cellSize centroids chunk clamp clamp_ts classify clearance clearVSIcache click colFromCell colFromX colMeans colorize colSums coltab coltab<- combineGeoms combineLevels compare Compare compareGeom concats contour convHull costDist countNA cover crds crop crosstab crs crs<- datatype deepcopy delaunay densify density depth depth<- depthName depthName<- depthUnit depthUnit<- describe diff direction disagg distance divide dots draw droplevels elongate emptyGeoms erase expanse ext ext<- extend extract extractAlong extractRange fileBlocksize fillHoles fillTime flip flowAccumulation focal focal3D focalCpp focalMat focalPairs focalReg focalValues forceCCW free_RAM freq gaps gdal gdalCache geom geomtype getGDALconfig getTileExtents global graticule gridDist gridDistance halo has.colors has.RGB has.time hasMinMax hasValues head hist hull identical ifel image impose inext init inMemory inset interpIDW interpNear interpolate intersect is.bool is.empty is.factor is.flipped is.int is.lines is.lonlat is.num is.points is.polygons is.related is.rotated is.valid isFALSE isTRUE k_means lapp layerCor levels libVersion linearUnits lines logic Logic longnames longnames<- makeNodes makeTiles makeValid makeVRT map_extent map.pal mask match math Math Math2 mean median mem_info merge mergeLines mergeTime meta metags metags<- minmax modal mosaic na.omit NAflag NAflag<- names ncell ncol ncol<- nearby nearest NIDP nlyr nlyr<- noNA normalize.longitude north not.na nrow nrow<- nseg nsrc origin origin<- pairs panel patches perim persp pitfinder plet plot plotRGB points polys prcomp predict princomp project quantile query rangeFill rapp rast rasterize rasterizeGeom rasterizeWin rcl readRDS readStart readStop readValues rectify regress relate removeDupNodes res res<- resample rescale rev RGB RGB<- roll rotate round rowColCombine rowColFromCell rowFromCell rowFromY rowMeans rowSums same.crs sapp saveRDS sbar scale scale_linear scoff scoff<- sds segregate sel selectHighest selectRange serialize set.cats set.crs set.ext set.names set.RGB set.values set.window setGDALconfig setMinMax setValues shade sharedPaths shift sieve simplifyGeom simplifyLevels size snap sort sources spatSample spin split sprc stdev stretch subset subst summary Summary surfArea svc symdif t tail tapp terrain terraOptions text thresh tighten time time<- timeInfo tmpFiles toMemory trans trim union unique units units<- unloadGDALdrivers unserialize unwrap update values values<- varnames varnames<- vect vector_layers viewshed voronoi vrt vrt_tiles watershed weighted.mean where.max where.min which.lyr which.max which.min width window window<- wrap wrapCache writeCDF writeRaster writeStart writeStop writeValues writeVector xapp xFromCell xFromCol xmax xmax<- xmin xmin<- xres xyFromCell yFromCell yFromRow ymax ymax<- ymin ymin<- yres zonal zoom geospatial raster spatial vector proj gdal geos cpp"
  },
  {
    "id": 782,
    "package_name": "magick",
    "title": "Advanced Graphics and Image-Processing in R",
    "description": "Bindings to 'ImageMagick': the most comprehensive\nopen-source image processing library available. Supports many\ncommon formats (png, jpeg, tiff, pdf, etc) and manipulations\n(rotate, scale, crop, trim, flip, blur, etc). All operations\nare vectorized via the Magick++ STL meaning they operate either\non a single frame or a series of frames for working with\nlayers, collages, or animation. In RStudio images are\nautomatically previewed when printed to the console, resulting\nin an interactive editing environment. Also includes a graphics\ndevice for creating drawing onto images using pixel\ncoordinates.",
    "version": "2.9.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://docs.ropensci.org/magick/\nhttps://ropensci.r-universe.dev/magick",
    "bug_reports": "https://github.com/ropensci/magick/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "as_EBImage"
      ],
      [
        "autoviewer_disable"
      ],
      [
        "autoviewer_enable"
      ],
      [
        "channel_types"
      ],
      [
        "coder_info"
      ],
      [
        "colorspace_types"
      ],
      [
        "compose_types"
      ],
      [
        "compress_types"
      ],
      [
        "decoration_types"
      ],
      [
        "demo_image"
      ],
      [
        "dispose_types"
      ],
      [
        "distort_types"
      ],
      [
        "dump_option_info"
      ],
      [
        "filter_types"
      ],
      [
        "geometry_area"
      ],
      [
        "geometry_point"
      ],
      [
        "geometry_size_percent"
      ],
      [
        "geometry_size_pixels"
      ],
      [
        "granite"
      ],
      [
        "gravity_types"
      ],
      [
        "image_animate"
      ],
      [
        "image_annotate"
      ],
      [
        "image_append"
      ],
      [
        "image_apply"
      ],
      [
        "image_attributes"
      ],
      [
        "image_average"
      ],
      [
        "image_background"
      ],
      [
        "image_blank"
      ],
      [
        "image_blur"
      ],
      [
        "image_border"
      ],
      [
        "image_browse"
      ],
      [
        "image_canny"
      ],
      [
        "image_capture"
      ],
      [
        "image_channel"
      ],
      [
        "image_charcoal"
      ],
      [
        "image_chop"
      ],
      [
        "image_coalesce"
      ],
      [
        "image_colorize"
      ],
      [
        "image_combine"
      ],
      [
        "image_comment"
      ],
      [
        "image_compare"
      ],
      [
        "image_compare_dist"
      ],
      [
        "image_composite"
      ],
      [
        "image_connect"
      ],
      [
        "image_contrast"
      ],
      [
        "image_convert"
      ],
      [
        "image_convolve"
      ],
      [
        "image_crop"
      ],
      [
        "image_data"
      ],
      [
        "image_deskew"
      ],
      [
        "image_deskew_angle"
      ],
      [
        "image_despeckle"
      ],
      [
        "image_destroy"
      ],
      [
        "image_device"
      ],
      [
        "image_display"
      ],
      [
        "image_distort"
      ],
      [
        "image_draw"
      ],
      [
        "image_edge"
      ],
      [
        "image_emboss"
      ],
      [
        "image_enhance"
      ],
      [
        "image_equalize"
      ],
      [
        "image_extent"
      ],
      [
        "image_fft"
      ],
      [
        "image_fill"
      ],
      [
        "image_flatten"
      ],
      [
        "image_flip"
      ],
      [
        "image_flop"
      ],
      [
        "image_frame"
      ],
      [
        "image_fuzzycmeans"
      ],
      [
        "image_fx"
      ],
      [
        "image_fx_sequence"
      ],
      [
        "image_get_artifact"
      ],
      [
        "image_ggplot"
      ],
      [
        "image_graph"
      ],
      [
        "image_hough_draw"
      ],
      [
        "image_hough_txt"
      ],
      [
        "image_implode"
      ],
      [
        "image_info"
      ],
      [
        "image_join"
      ],
      [
        "image_lat"
      ],
      [
        "image_level"
      ],
      [
        "image_map"
      ],
      [
        "image_median"
      ],
      [
        "image_modulate"
      ],
      [
        "image_montage"
      ],
      [
        "image_morph"
      ],
      [
        "image_morphology"
      ],
      [
        "image_mosaic"
      ],
      [
        "image_motion_blur"
      ],
      [
        "image_negate"
      ],
      [
        "image_noise"
      ],
      [
        "image_normalize"
      ],
      [
        "image_ocr"
      ],
      [
        "image_ocr_data"
      ],
      [
        "image_oilpaint"
      ],
      [
        "image_ordered_dither"
      ],
      [
        "image_orient"
      ],
      [
        "image_page"
      ],
      [
        "image_quantize"
      ],
      [
        "image_raster"
      ],
      [
        "image_read"
      ],
      [
        "image_read_pdf"
      ],
      [
        "image_read_svg"
      ],
      [
        "image_read_video"
      ],
      [
        "image_reducenoise"
      ],
      [
        "image_repage"
      ],
      [
        "image_resize"
      ],
      [
        "image_rotate"
      ],
      [
        "image_sample"
      ],
      [
        "image_scale"
      ],
      [
        "image_separate"
      ],
      [
        "image_set_defines"
      ],
      [
        "image_shade"
      ],
      [
        "image_shadow"
      ],
      [
        "image_shadow_mask"
      ],
      [
        "image_shear"
      ],
      [
        "image_split"
      ],
      [
        "image_strip"
      ],
      [
        "image_threshold"
      ],
      [
        "image_transparent"
      ],
      [
        "image_trim"
      ],
      [
        "image_types"
      ],
      [
        "image_virtual_pixel"
      ],
      [
        "image_write"
      ],
      [
        "image_write_gif"
      ],
      [
        "image_write_video"
      ],
      [
        "kernel_types"
      ],
      [
        "logo"
      ],
      [
        "magick_config"
      ],
      [
        "magick_fonts"
      ],
      [
        "magick_options"
      ],
      [
        "magick_set_seed"
      ],
      [
        "metric_types"
      ],
      [
        "morphology_types"
      ],
      [
        "noise_types"
      ],
      [
        "option_types"
      ],
      [
        "orientation_types"
      ],
      [
        "rose"
      ],
      [
        "style_types"
      ],
      [
        "virtual_pixel_methods"
      ],
      [
        "wizard"
      ]
    ],
    "topics": [
      [
        "image-manipulation"
      ],
      [
        "image-processing"
      ],
      [
        "imagemagick"
      ],
      [
        "cpp"
      ]
    ],
    "score": 17.8857,
    "stars": 474,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "magick Advanced Graphics and Image-Processing in R Bindings to 'ImageMagick': the most comprehensive\nopen-source image processing library available. Supports many\ncommon formats (png, jpeg, tiff, pdf, etc) and manipulations\n(rotate, scale, crop, trim, flip, blur, etc). All operations\nare vectorized via the Magick++ STL meaning they operate either\non a single frame or a series of frames for working with\nlayers, collages, or animation. In RStudio images are\nautomatically previewed when printed to the console, resulting\nin an interactive editing environment. Also includes a graphics\ndevice for creating drawing onto images using pixel\ncoordinates. %>% as_EBImage autoviewer_disable autoviewer_enable channel_types coder_info colorspace_types compose_types compress_types decoration_types demo_image dispose_types distort_types dump_option_info filter_types geometry_area geometry_point geometry_size_percent geometry_size_pixels granite gravity_types image_animate image_annotate image_append image_apply image_attributes image_average image_background image_blank image_blur image_border image_browse image_canny image_capture image_channel image_charcoal image_chop image_coalesce image_colorize image_combine image_comment image_compare image_compare_dist image_composite image_connect image_contrast image_convert image_convolve image_crop image_data image_deskew image_deskew_angle image_despeckle image_destroy image_device image_display image_distort image_draw image_edge image_emboss image_enhance image_equalize image_extent image_fft image_fill image_flatten image_flip image_flop image_frame image_fuzzycmeans image_fx image_fx_sequence image_get_artifact image_ggplot image_graph image_hough_draw image_hough_txt image_implode image_info image_join image_lat image_level image_map image_median image_modulate image_montage image_morph image_morphology image_mosaic image_motion_blur image_negate image_noise image_normalize image_ocr image_ocr_data image_oilpaint image_ordered_dither image_orient image_page image_quantize image_raster image_read image_read_pdf image_read_svg image_read_video image_reducenoise image_repage image_resize image_rotate image_sample image_scale image_separate image_set_defines image_shade image_shadow image_shadow_mask image_shear image_split image_strip image_threshold image_transparent image_trim image_types image_virtual_pixel image_write image_write_gif image_write_video kernel_types logo magick_config magick_fonts magick_options magick_set_seed metric_types morphology_types noise_types option_types orientation_types rose style_types virtual_pixel_methods wizard image-manipulation image-processing imagemagick cpp"
  },
  {
    "id": 421,
    "package_name": "cpp11",
    "title": "A C++11 Interface for R's C Interface",
    "description": "Provides a header only, C++11 interface to R's C\ninterface.  Compared to other approaches 'cpp11' strives to be\nsafe against long jumps from the C API as well as C++\nexceptions, conform to normal R function semantics and supports\ninteraction with 'ALTREP' vectors.",
    "version": "0.5.2.9000",
    "maintainer": "Davis Vaughan <davis@posit.co>",
    "author": "Davis Vaughan [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4777-038X>),\nJim Hester [aut] (ORCID: <https://orcid.org/0000-0002-2739-7082>),\nRomain Fran\u00e7ois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\nBenjamin Kietzman [ctb],\nPosit Software, PBC [cph, fnd]",
    "url": "https://cpp11.r-lib.org, https://github.com/r-lib/cpp11",
    "bug_reports": "https://github.com/r-lib/cpp11/issues",
    "repository": "",
    "exports": [
      [
        "cpp_eval"
      ],
      [
        "cpp_function"
      ],
      [
        "cpp_register"
      ],
      [
        "cpp_source"
      ],
      [
        "cpp_vendor"
      ]
    ],
    "topics": [
      [
        "cpp"
      ],
      [
        "cpp11"
      ]
    ],
    "score": 17.3273,
    "stars": 221,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "cpp11 A C++11 Interface for R's C Interface Provides a header only, C++11 interface to R's C\ninterface.  Compared to other approaches 'cpp11' strives to be\nsafe against long jumps from the C API as well as C++\nexceptions, conform to normal R function semantics and supports\ninteraction with 'ALTREP' vectors. cpp_eval cpp_function cpp_register cpp_source cpp_vendor cpp cpp11"
  },
  {
    "id": 1232,
    "package_name": "skimr",
    "title": "Compact and Flexible Summaries of Data",
    "description": "A simple to use summary function that can be used with\npipes and displays nicely in the console. The default summary\nstatistics may be modified by the user as can the default\nformatting.  Support for data frames and vectors is included,\nand users can implement their own skim methods for specific\nobject types as described in a vignette. Default summaries\ninclude support for inline spark graphs. Instructions for\nmanaging these on specific operating systems are given in the\n\"Using skimr\" vignette and the README.",
    "version": "2.2.1.9000",
    "maintainer": "Elin Waring <elin.waring@gmail.com>",
    "author": "Elin Waring [cre, aut],\nMichael Quinn [aut],\nAmelia McNamara [aut],\nEduardo Arino de la Rubia [aut],\nHao Zhu [aut],\nJulia Lowndes [ctb],\nShannon Ellis [aut],\nHope McLeod [ctb],\nHadley Wickham [ctb],\nKirill M\u00fcller [ctb],\nRStudio, Inc. [cph] (Spark functions),\nConnor Kirkpatrick [ctb],\nScott Brenstuhl [ctb],\nPatrick Schratz [ctb],\nlbusett [ctb],\nMikko Korpela [ctb],\nJennifer Thompson [ctb],\nHarris McGehee [ctb],\nMark Roepke [ctb],\nPatrick Kennedy [ctb],\nDaniel Possenriede [ctb],\nDavid Zimmermann [ctb],\nKyle Butts [ctb],\nBastian Torges [ctb],\nRick Saporta [ctb],\nHenry Morgan Stewart [ctb],\nOlivier Roy [ctb]",
    "url": "https://docs.ropensci.org/skimr/,\nhttps://github.com/ropensci/skimr/",
    "bug_reports": "https://github.com/ropensci/skimr/issues",
    "repository": "",
    "exports": [
      [
        "assert_is_one_skim_df"
      ],
      [
        "assert_is_skim_df"
      ],
      [
        "assert_is_skim_list"
      ],
      [
        "base_skimmers"
      ],
      [
        "bind"
      ],
      [
        "complete_rate"
      ],
      [
        "could_be_skim_df"
      ],
      [
        "data_cols"
      ],
      [
        "data_rows"
      ],
      [
        "df_name"
      ],
      [
        "dt_key"
      ],
      [
        "fix_windows_histograms"
      ],
      [
        "focus"
      ],
      [
        "get_default_skimmer_names"
      ],
      [
        "get_default_skimmers"
      ],
      [
        "get_one_default_skimmer"
      ],
      [
        "get_one_default_skimmer_names"
      ],
      [
        "get_sfl"
      ],
      [
        "get_skimmers"
      ],
      [
        "group_names"
      ],
      [
        "has_skim_type_attribute"
      ],
      [
        "has_skimmers"
      ],
      [
        "has_skimr_attributes"
      ],
      [
        "has_type_column"
      ],
      [
        "has_variable_column"
      ],
      [
        "inline_hist"
      ],
      [
        "inline_linegraph"
      ],
      [
        "is_data_frame"
      ],
      [
        "is_one_skim_df"
      ],
      [
        "is_skim_df"
      ],
      [
        "is_skim_list"
      ],
      [
        "list_lengths_max"
      ],
      [
        "list_lengths_median"
      ],
      [
        "list_lengths_min"
      ],
      [
        "list_max_length"
      ],
      [
        "list_min_length"
      ],
      [
        "max_char"
      ],
      [
        "max_date"
      ],
      [
        "min_char"
      ],
      [
        "min_date"
      ],
      [
        "modify_default_skimmers"
      ],
      [
        "n_complete"
      ],
      [
        "n_empty"
      ],
      [
        "n_missing"
      ],
      [
        "n_unique"
      ],
      [
        "n_whitespace"
      ],
      [
        "partition"
      ],
      [
        "sfl"
      ],
      [
        "skim"
      ],
      [
        "skim_format"
      ],
      [
        "skim_tee"
      ],
      [
        "skim_to_list"
      ],
      [
        "skim_to_wide"
      ],
      [
        "skim_with"
      ],
      [
        "skim_without_charts"
      ],
      [
        "skimmers_used"
      ],
      [
        "sorted_count"
      ],
      [
        "to_long"
      ],
      [
        "top_counts"
      ],
      [
        "ts_end"
      ],
      [
        "ts_start"
      ],
      [
        "yank"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ],
      [
        "ropensci"
      ],
      [
        "summary-statistics"
      ],
      [
        "unconf"
      ],
      [
        "unconf17"
      ]
    ],
    "score": 17.2276,
    "stars": 1138,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "skimr Compact and Flexible Summaries of Data A simple to use summary function that can be used with\npipes and displays nicely in the console. The default summary\nstatistics may be modified by the user as can the default\nformatting.  Support for data frames and vectors is included,\nand users can implement their own skim methods for specific\nobject types as described in a vignette. Default summaries\ninclude support for inline spark graphs. Instructions for\nmanaging these on specific operating systems are given in the\n\"Using skimr\" vignette and the README. assert_is_one_skim_df assert_is_skim_df assert_is_skim_list base_skimmers bind complete_rate could_be_skim_df data_cols data_rows df_name dt_key fix_windows_histograms focus get_default_skimmer_names get_default_skimmers get_one_default_skimmer get_one_default_skimmer_names get_sfl get_skimmers group_names has_skim_type_attribute has_skimmers has_skimr_attributes has_type_column has_variable_column inline_hist inline_linegraph is_data_frame is_one_skim_df is_skim_df is_skim_list list_lengths_max list_lengths_median list_lengths_min list_max_length list_min_length max_char max_date min_char min_date modify_default_skimmers n_complete n_empty n_missing n_unique n_whitespace partition sfl skim skim_format skim_tee skim_to_list skim_to_wide skim_with skim_without_charts skimmers_used sorted_count to_long top_counts ts_end ts_start yank peer-reviewed ropensci summary-statistics unconf unconf17"
  },
  {
    "id": 1056,
    "package_name": "raster",
    "title": "Geographic Data Analysis and Modeling",
    "description": "Reading, writing, manipulating, analyzing and modeling of\nspatial data. This package has been superseded by the \"terra\"\npackage <https://CRAN.R-project.org/package=terra>.",
    "version": "3.6-32",
    "maintainer": "Robert J. Hijmans <r.hijmans@gmail.com>",
    "author": "Robert J. Hijmans [cre, aut] (ORCID:\n<https://orcid.org/0000-0001-5872-2872>),\nJacob van Etten [ctb],\nMichael Sumner [ctb],\nJoe Cheng [ctb],\nDan Baston [ctb],\nAndrew Bevan [ctb],\nRoger Bivand [ctb],\nLorenzo Busetto [ctb],\nMort Canty [ctb],\nBen Fasoli [ctb],\nDavid Forrest [ctb],\nAniruddha Ghosh [ctb],\nDuncan Golicher [ctb],\nJosh Gray [ctb],\nJonathan A. Greenberg [ctb],\nPaul Hiemstra [ctb],\nKassel Hingee [ctb],\nAlex Ilich [ctb],\nInstitute for Mathematics Applied Geosciences [cph],\nCharles Karney [ctb],\nMatteo Mattiuzzi [ctb],\nSteven Mosher [ctb],\nBabak Naimi [ctb],\nJakub Nowosad [ctb],\nEdzer Pebesma [ctb],\nOscar Perpinan Lamigueiro [ctb],\nEtienne B. Racine [ctb],\nBarry Rowlingson [ctb],\nAshton Shortridge [ctb],\nBill Venables [ctb],\nRafael Wueest [ctb]",
    "url": "https://rspatial.org/raster",
    "bug_reports": "https://github.com/rspatial/raster/issues/",
    "repository": "",
    "exports": [
      [
        "%in%"
      ],
      [
        "addLayer"
      ],
      [
        "adjacent"
      ],
      [
        "aggregate"
      ],
      [
        "alignExtent"
      ],
      [
        "all.equal"
      ],
      [
        "animate"
      ],
      [
        "approxNA"
      ],
      [
        "area"
      ],
      [
        "Arith"
      ],
      [
        "as.array"
      ],
      [
        "as.data.frame"
      ],
      [
        "as.factor"
      ],
      [
        "as.list"
      ],
      [
        "as.matrix"
      ],
      [
        "as.raster"
      ],
      [
        "asFactor"
      ],
      [
        "atan2"
      ],
      [
        "bandnr"
      ],
      [
        "barplot"
      ],
      [
        "bbox"
      ],
      [
        "beginCluster"
      ],
      [
        "bind"
      ],
      [
        "blockSize"
      ],
      [
        "boundaries"
      ],
      [
        "boxplot"
      ],
      [
        "brick"
      ],
      [
        "buffer"
      ],
      [
        "calc"
      ],
      [
        "canProcessInMemory"
      ],
      [
        "ccodes"
      ],
      [
        "cellFromCol"
      ],
      [
        "cellFromLine"
      ],
      [
        "cellFromPolygon"
      ],
      [
        "cellFromRow"
      ],
      [
        "cellFromRowCol"
      ],
      [
        "cellFromRowColCombine"
      ],
      [
        "cellFromXY"
      ],
      [
        "cellsFromExtent"
      ],
      [
        "cellStats"
      ],
      [
        "clamp"
      ],
      [
        "clearValues"
      ],
      [
        "click"
      ],
      [
        "clump"
      ],
      [
        "clusterR"
      ],
      [
        "colFromCell"
      ],
      [
        "colFromX"
      ],
      [
        "colortable"
      ],
      [
        "colortable<-"
      ],
      [
        "colSums"
      ],
      [
        "Compare"
      ],
      [
        "compareCRS"
      ],
      [
        "compareRaster"
      ],
      [
        "contour"
      ],
      [
        "coordinates"
      ],
      [
        "corLocal"
      ],
      [
        "couldBeLonLat"
      ],
      [
        "cover"
      ],
      [
        "crop"
      ],
      [
        "crosstab"
      ],
      [
        "crs"
      ],
      [
        "crs<-"
      ],
      [
        "cut"
      ],
      [
        "cv"
      ],
      [
        "dataSigned"
      ],
      [
        "dataSize"
      ],
      [
        "dataType"
      ],
      [
        "dataType<-"
      ],
      [
        "density"
      ],
      [
        "deratify"
      ],
      [
        "direction"
      ],
      [
        "disaggregate"
      ],
      [
        "distance"
      ],
      [
        "distanceFromPoints"
      ],
      [
        "drawExtent"
      ],
      [
        "drawLine"
      ],
      [
        "drawPoly"
      ],
      [
        "dropLayer"
      ],
      [
        "endCluster"
      ],
      [
        "erase"
      ],
      [
        "extend"
      ],
      [
        "extension"
      ],
      [
        "extension<-"
      ],
      [
        "extent"
      ],
      [
        "extent<-"
      ],
      [
        "extentFromCells"
      ],
      [
        "extract"
      ],
      [
        "factorValues"
      ],
      [
        "filename"
      ],
      [
        "filledContour"
      ],
      [
        "flip"
      ],
      [
        "flowPath"
      ],
      [
        "focal"
      ],
      [
        "focalWeight"
      ],
      [
        "fourCellsFromXY"
      ],
      [
        "freq"
      ],
      [
        "fromDisk"
      ],
      [
        "gain"
      ],
      [
        "gain<-"
      ],
      [
        "Geary"
      ],
      [
        "GearyLocal"
      ],
      [
        "geom"
      ],
      [
        "getCluster"
      ],
      [
        "getData"
      ],
      [
        "getValues"
      ],
      [
        "getValuesBlock"
      ],
      [
        "getValuesFocal"
      ],
      [
        "getZ"
      ],
      [
        "gridDistance"
      ],
      [
        "hasValues"
      ],
      [
        "hdr"
      ],
      [
        "head"
      ],
      [
        "hillShade"
      ],
      [
        "hist"
      ],
      [
        "image"
      ],
      [
        "init"
      ],
      [
        "inMemory"
      ],
      [
        "interpolate"
      ],
      [
        "intersect"
      ],
      [
        "is.factor"
      ],
      [
        "isLonLat"
      ],
      [
        "KML"
      ],
      [
        "labels"
      ],
      [
        "layerize"
      ],
      [
        "layerStats"
      ],
      [
        "levels"
      ],
      [
        "lines"
      ],
      [
        "localFun"
      ],
      [
        "Logic"
      ],
      [
        "mask"
      ],
      [
        "match"
      ],
      [
        "Math"
      ],
      [
        "Math2"
      ],
      [
        "maxValue"
      ],
      [
        "mean"
      ],
      [
        "merge"
      ],
      [
        "metadata"
      ],
      [
        "metadata<-"
      ],
      [
        "minValue"
      ],
      [
        "modal"
      ],
      [
        "Moran"
      ],
      [
        "MoranLocal"
      ],
      [
        "mosaic"
      ],
      [
        "movingFun"
      ],
      [
        "NAvalue"
      ],
      [
        "NAvalue<-"
      ],
      [
        "nbands"
      ],
      [
        "ncell"
      ],
      [
        "ncol"
      ],
      [
        "ncol<-"
      ],
      [
        "nlayers"
      ],
      [
        "nrow"
      ],
      [
        "nrow<-"
      ],
      [
        "offs"
      ],
      [
        "offs<-"
      ],
      [
        "origin"
      ],
      [
        "origin<-"
      ],
      [
        "overlay"
      ],
      [
        "pairs"
      ],
      [
        "pbClose"
      ],
      [
        "pbCreate"
      ],
      [
        "pbStep"
      ],
      [
        "persp"
      ],
      [
        "plot"
      ],
      [
        "plotRGB"
      ],
      [
        "pointDistance"
      ],
      [
        "predict"
      ],
      [
        "print"
      ],
      [
        "proj4string"
      ],
      [
        "projectExtent"
      ],
      [
        "projection"
      ],
      [
        "projection<-"
      ],
      [
        "projectRaster"
      ],
      [
        "quantile"
      ],
      [
        "raster"
      ],
      [
        "rasterFromCells"
      ],
      [
        "rasterFromXYZ"
      ],
      [
        "rasterize"
      ],
      [
        "rasterOptions"
      ],
      [
        "rasterTmpFile"
      ],
      [
        "rasterToContour"
      ],
      [
        "rasterToPoints"
      ],
      [
        "rasterToPolygons"
      ],
      [
        "ratify"
      ],
      [
        "readAll"
      ],
      [
        "readIniFile"
      ],
      [
        "readStart"
      ],
      [
        "readStop"
      ],
      [
        "reclassify"
      ],
      [
        "rectify"
      ],
      [
        "removeTmpFiles"
      ],
      [
        "res"
      ],
      [
        "res<-"
      ],
      [
        "resample"
      ],
      [
        "returnCluster"
      ],
      [
        "RGB"
      ],
      [
        "rotate"
      ],
      [
        "rotated"
      ],
      [
        "rowColFromCell"
      ],
      [
        "rowFromCell"
      ],
      [
        "rowFromY"
      ],
      [
        "rowSums"
      ],
      [
        "sampleInt"
      ],
      [
        "sampleRandom"
      ],
      [
        "sampleRegular"
      ],
      [
        "sampleStratified"
      ],
      [
        "scale"
      ],
      [
        "scalebar"
      ],
      [
        "select"
      ],
      [
        "setExtent"
      ],
      [
        "setMinMax"
      ],
      [
        "setValues"
      ],
      [
        "setZ"
      ],
      [
        "shapefile"
      ],
      [
        "shift"
      ],
      [
        "showTmpFiles"
      ],
      [
        "slopeAspect"
      ],
      [
        "SpExtent"
      ],
      [
        "spLines"
      ],
      [
        "spplot"
      ],
      [
        "SpPoly"
      ],
      [
        "spPolygons"
      ],
      [
        "SpPolygons"
      ],
      [
        "SpPolyPart"
      ],
      [
        "stack"
      ],
      [
        "stackApply"
      ],
      [
        "stackOpen"
      ],
      [
        "stackSave"
      ],
      [
        "stackSelect"
      ],
      [
        "stretch"
      ],
      [
        "subs"
      ],
      [
        "subset"
      ],
      [
        "summary"
      ],
      [
        "Summary"
      ],
      [
        "symdif"
      ],
      [
        "t"
      ],
      [
        "tail"
      ],
      [
        "terrain"
      ],
      [
        "text"
      ],
      [
        "tmpDir"
      ],
      [
        "trim"
      ],
      [
        "union"
      ],
      [
        "unique"
      ],
      [
        "unstack"
      ],
      [
        "update"
      ],
      [
        "validCell"
      ],
      [
        "validCol"
      ],
      [
        "validNames"
      ],
      [
        "validRow"
      ],
      [
        "values"
      ],
      [
        "values<-"
      ],
      [
        "weighted.mean"
      ],
      [
        "Which"
      ],
      [
        "which.max"
      ],
      [
        "which.min"
      ],
      [
        "whiches.max"
      ],
      [
        "whiches.min"
      ],
      [
        "wkt"
      ],
      [
        "writeFormats"
      ],
      [
        "writeRaster"
      ],
      [
        "writeStart"
      ],
      [
        "writeStop"
      ],
      [
        "writeValues"
      ],
      [
        "xFromCell"
      ],
      [
        "xFromCol"
      ],
      [
        "xmax"
      ],
      [
        "xmax<-"
      ],
      [
        "xmin"
      ],
      [
        "xmin<-"
      ],
      [
        "xres"
      ],
      [
        "xyFromCell"
      ],
      [
        "yFromCell"
      ],
      [
        "yFromRow"
      ],
      [
        "ymax"
      ],
      [
        "ymax<-"
      ],
      [
        "ymin"
      ],
      [
        "ymin<-"
      ],
      [
        "yres"
      ],
      [
        "zApply"
      ],
      [
        "zonal"
      ],
      [
        "zoom"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 16.9768,
    "stars": 164,
    "primary_category": "spatial",
    "source_universe": "rspatial",
    "search_text": "raster Geographic Data Analysis and Modeling Reading, writing, manipulating, analyzing and modeling of\nspatial data. This package has been superseded by the \"terra\"\npackage <https://CRAN.R-project.org/package=terra>. %in% addLayer adjacent aggregate alignExtent all.equal animate approxNA area Arith as.array as.data.frame as.factor as.list as.matrix as.raster asFactor atan2 bandnr barplot bbox beginCluster bind blockSize boundaries boxplot brick buffer calc canProcessInMemory ccodes cellFromCol cellFromLine cellFromPolygon cellFromRow cellFromRowCol cellFromRowColCombine cellFromXY cellsFromExtent cellStats clamp clearValues click clump clusterR colFromCell colFromX colortable colortable<- colSums Compare compareCRS compareRaster contour coordinates corLocal couldBeLonLat cover crop crosstab crs crs<- cut cv dataSigned dataSize dataType dataType<- density deratify direction disaggregate distance distanceFromPoints drawExtent drawLine drawPoly dropLayer endCluster erase extend extension extension<- extent extent<- extentFromCells extract factorValues filename filledContour flip flowPath focal focalWeight fourCellsFromXY freq fromDisk gain gain<- Geary GearyLocal geom getCluster getData getValues getValuesBlock getValuesFocal getZ gridDistance hasValues hdr head hillShade hist image init inMemory interpolate intersect is.factor isLonLat KML labels layerize layerStats levels lines localFun Logic mask match Math Math2 maxValue mean merge metadata metadata<- minValue modal Moran MoranLocal mosaic movingFun NAvalue NAvalue<- nbands ncell ncol ncol<- nlayers nrow nrow<- offs offs<- origin origin<- overlay pairs pbClose pbCreate pbStep persp plot plotRGB pointDistance predict print proj4string projectExtent projection projection<- projectRaster quantile raster rasterFromCells rasterFromXYZ rasterize rasterOptions rasterTmpFile rasterToContour rasterToPoints rasterToPolygons ratify readAll readIniFile readStart readStop reclassify rectify removeTmpFiles res res<- resample returnCluster RGB rotate rotated rowColFromCell rowFromCell rowFromY rowSums sampleInt sampleRandom sampleRegular sampleStratified scale scalebar select setExtent setMinMax setValues setZ shapefile shift showTmpFiles slopeAspect SpExtent spLines spplot SpPoly spPolygons SpPolygons SpPolyPart stack stackApply stackOpen stackSave stackSelect stretch subs subset summary Summary symdif t tail terrain text tmpDir trim union unique unstack update validCell validCol validNames validRow values values<- weighted.mean Which which.max which.min whiches.max whiches.min wkt writeFormats writeRaster writeStart writeStop writeValues xFromCell xFromCol xmax xmax<- xmin xmin<- xres xyFromCell yFromCell yFromRow ymax ymax<- ymin ymin<- yres zApply zonal zoom cpp"
  },
  {
    "id": 1258,
    "package_name": "spdep",
    "title": "Spatial Dependence: Weighting Schemes, Statistics",
    "description": "A collection of functions to create spatial weights matrix\nobjects from polygon 'contiguities', from point patterns by\ndistance and tessellations, for summarizing these objects, and\nfor permitting their use in spatial data analysis, including\nregional aggregation by minimum spanning tree; a collection of\ntests for spatial 'autocorrelation', including global 'Morans\nI' and 'Gearys C' proposed by 'Cliff' and 'Ord' (1973, ISBN:\n0850860369) and (1981, ISBN: 0850860814), 'Hubert/Mantel'\ngeneral cross product statistic, Empirical Bayes estimates and\n'Assun\u00e7\u00e3o/Reis' (1999)\n<doi:10.1002/(SICI)1097-0258(19990830)18:16%3C2147::AID-SIM179%3E3.0.CO;2-I>\nIndex, 'Getis/Ord' G ('Getis' and 'Ord' 1992)\n<doi:10.1111/j.1538-4632.1992.tb00261.x> and multicoloured join\ncount statistics, 'APLE' ('Li et al.' )\n<doi:10.1111/j.1538-4632.2007.00708.x>, local 'Moran's I',\n'Gearys C' ('Anselin' 1995)\n<doi:10.1111/j.1538-4632.1995.tb00338.x> and 'Getis/Ord' G\n('Ord' and 'Getis' 1995)\n<doi:10.1111/j.1538-4632.1995.tb00912.x>, 'saddlepoint'\napproximations ('Tiefelsdorf' 2002)\n<doi:10.1111/j.1538-4632.2002.tb01084.x> and exact tests for\nglobal and local 'Moran's I' ('Bivand et al.' 2009)\n<doi:10.1016/j.csda.2008.07.021> and 'LOSH' local indicators of\nspatial heteroscedasticity ('Ord' and 'Getis')\n<doi:10.1007/s00168-011-0492-y>. The implementation of most of\nthese measures is described in 'Bivand' and 'Wong' (2018)\n<doi:10.1007/s11749-018-0599-x>, with further extensions in\n'Bivand' (2022) <doi:10.1111/gean.12319>. 'Lagrange' multiplier\ntests for spatial dependence in linear models are provided\n('Anselin et al'. 1996) <doi:10.1016/0166-0462(95)02111-6>, as\nare 'Rao' score tests for hypothesised spatial 'Durbin' models\nbased on linear models ('Koley' and 'Bera' 2023)\n<doi:10.1080/17421772.2023.2256810>. Additions in 2024 include\nLocal Indicators for Categorical Data based on 'Carrer et al.'\n(2021) <doi:10.1016/j.jas.2020.105306> and 'Bivand et al.'\n(2017) <doi:10.1016/j.spasta.2017.03.003>; also Weighted\nMultivariate Spatial Autocorrelation Measures ('Bavaud' 2024)\n<doi:10.1111/gean.12390>. <doi:10.1080/17421772.2023.2256810>.\nA local indicators for categorical data (LICD) implementation\nbased on 'Carrer et al.' (2021) <doi:10.1016/j.jas.2020.105306>\nand 'Bivand et al.' (2017) <doi:10.1016/j.spasta.2017.03.003>\nwas added in 1.3-7. Multivariate 'spatialdelta' ('Bavaud' 2024)\n<doi:10.1111/gean.12390> was added in 1.3-13 ('Bivand' 2025\n<doi:10.26034/la.cdclsl.2025.8343>). From 'spdep' and\n'spatialreg' versions >= 1.2-1, the model fitting functions\npreviously present in this package are defunct in 'spdep' and\nmay be found in 'spatialreg'.",
    "version": "1.4-2",
    "maintainer": "Roger Bivand <Roger.Bivand@nhh.no>",
    "author": "Roger Bivand [cre, aut] (ORCID:\n<https://orcid.org/0000-0003-2392-6140>, ROR:\n<https://ror.org/04v53s997>),\nMicah Altman [ctb],\nLuc Anselin [ctb],\nRenato Assun\u00e7\u00e3o [ctb],\nAnil Bera [ctb],\nOlaf Berke [ctb],\nF. Guillaume Blanchet [ctb],\nMarilia Carvalho [ctb],\nBjarke Christensen [ctb],\nYongwan Chun [ctb],\nCarsten Dormann [ctb],\nSt\u00e9phane Dray [ctb],\nDewey Dunnington [ctb] (ORCID: <https://orcid.org/0000-0002-9415-4582>),\nVirgilio G\u00f3mez-Rubio [ctb],\nMalabika Koley [ctb],\nTomasz Kossowski [ctb] (ORCID: <https://orcid.org/0000-0002-9976-4398>),\nElias Krainski [ctb],\nPierre Legendre [ctb],\nNicholas Lewin-Koh [ctb],\nAngela Li [ctb],\nGiovanni Millo [ctb],\nWerner Mueller [ctb],\nHisaji Ono [ctb],\nJosiah Parry [ctb] (ORCID: <https://orcid.org/0000-0001-9910-865X>),\nPedro Peres-Neto [ctb],\nMicha\u0142 Pietrzak [ctb] (ORCID: <https://orcid.org/0000-0002-9263-4478>),\nGianfranco Piras [ctb],\nMarkus Reder [ctb],\nJeff Sauer [ctb],\nMichael Tiefelsdorf [ctb],\nRen\u00e9 Westerholt [ctb],\nJustyna Wilk [ctb] (ORCID: <https://orcid.org/0000-0003-1495-2910>),\nLevi Wolf [ctb],\nDanlin Yu [ctb]",
    "url": "https://github.com/r-spatial/spdep/,\nhttps://r-spatial.github.io/spdep/",
    "bug_reports": "https://github.com/r-spatial/spdep/issues/",
    "repository": "",
    "exports": [
      [
        "addlinks1"
      ],
      [
        "aggregate.nb"
      ],
      [
        "airdist"
      ],
      [
        "as.data.frame.localmoranex"
      ],
      [
        "as.data.frame.localmoransad"
      ],
      [
        "autocov_dist"
      ],
      [
        "card"
      ],
      [
        "cell2nb"
      ],
      [
        "chkIDs"
      ],
      [
        "choynowski"
      ],
      [
        "coerce"
      ],
      [
        "complement.nb"
      ],
      [
        "cornish_fisher"
      ],
      [
        "df2sn"
      ],
      [
        "diffnb"
      ],
      [
        "dnearneigh"
      ],
      [
        "droplinks"
      ],
      [
        "EBest"
      ],
      [
        "EBImoran.mc"
      ],
      [
        "EBlocal"
      ],
      [
        "edit.nb"
      ],
      [
        "factorial_coordinates"
      ],
      [
        "gabrielneigh"
      ],
      [
        "geary"
      ],
      [
        "geary.mc"
      ],
      [
        "geary.test"
      ],
      [
        "get.ClusterOption"
      ],
      [
        "get.coresOption"
      ],
      [
        "get.mcOption"
      ],
      [
        "get.NoNeighbourOption"
      ],
      [
        "get.spChkOption"
      ],
      [
        "get.SubgraphCeiling"
      ],
      [
        "get.SubgraphOption"
      ],
      [
        "get.VerboseOption"
      ],
      [
        "get.ZeroPolicyOption"
      ],
      [
        "globalG.test"
      ],
      [
        "graph_distance_weights"
      ],
      [
        "graph2nb"
      ],
      [
        "grid2nb"
      ],
      [
        "have_factor_preds_mf"
      ],
      [
        "hotspot"
      ],
      [
        "include.self"
      ],
      [
        "intersect.nb"
      ],
      [
        "is.symmetric.glist"
      ],
      [
        "is.symmetric.nb"
      ],
      [
        "iterative_proportional_fitting_weights"
      ],
      [
        "joincount.mc"
      ],
      [
        "joincount.multi"
      ],
      [
        "joincount.test"
      ],
      [
        "knearneigh"
      ],
      [
        "knn2nb"
      ],
      [
        "lag.listw"
      ],
      [
        "lee"
      ],
      [
        "lee.mc"
      ],
      [
        "lee.test"
      ],
      [
        "licd_multi"
      ],
      [
        "linearised_diffusive_weights"
      ],
      [
        "listw2lines"
      ],
      [
        "listw2mat"
      ],
      [
        "listw2sn"
      ],
      [
        "listw2star"
      ],
      [
        "listw2U"
      ],
      [
        "listw2WB"
      ],
      [
        "lm.LMtests"
      ],
      [
        "lm.morantest"
      ],
      [
        "lm.morantest.exact"
      ],
      [
        "lm.morantest.sad"
      ],
      [
        "lm.RStests"
      ],
      [
        "local_joincount_bv"
      ],
      [
        "local_joincount_uni"
      ],
      [
        "localC"
      ],
      [
        "localC_perm"
      ],
      [
        "localdelta"
      ],
      [
        "localG"
      ],
      [
        "localG_perm"
      ],
      [
        "localGS"
      ],
      [
        "localmoran"
      ],
      [
        "localmoran_bv"
      ],
      [
        "localmoran_perm"
      ],
      [
        "localmoran.exact"
      ],
      [
        "localmoran.exact.alt"
      ],
      [
        "localmoran.sad"
      ],
      [
        "LOSH"
      ],
      [
        "LOSH.cs"
      ],
      [
        "LOSH.mc"
      ],
      [
        "make.sym.nb"
      ],
      [
        "mat2listw"
      ],
      [
        "metropolis_hastings_weights"
      ],
      [
        "moran"
      ],
      [
        "moran_bv"
      ],
      [
        "moran.mc"
      ],
      [
        "moran.plot"
      ],
      [
        "moran.test"
      ],
      [
        "mstree"
      ],
      [
        "n.comp.nb"
      ],
      [
        "nb2blocknb"
      ],
      [
        "nb2INLA"
      ],
      [
        "nb2lines"
      ],
      [
        "nb2listw"
      ],
      [
        "nb2listwdist"
      ],
      [
        "nb2mat"
      ],
      [
        "nb2WB"
      ],
      [
        "nbcost"
      ],
      [
        "nbcosts"
      ],
      [
        "nbdists"
      ],
      [
        "nblag"
      ],
      [
        "nblag_cumul"
      ],
      [
        "old.make.sym.nb"
      ],
      [
        "p.adjustSP"
      ],
      [
        "plot_factorialcoords"
      ],
      [
        "plot_factorialscree"
      ],
      [
        "plot_moran"
      ],
      [
        "plot_spatialcoords"
      ],
      [
        "plot_spatialscree"
      ],
      [
        "plot.Gabriel"
      ],
      [
        "plot.listw"
      ],
      [
        "plot.mc.sim"
      ],
      [
        "plot.mst"
      ],
      [
        "plot.nb"
      ],
      [
        "plot.relative"
      ],
      [
        "plot.skater"
      ],
      [
        "plot.spcor"
      ],
      [
        "poly2nb"
      ],
      [
        "print.jclist"
      ],
      [
        "print.jcmulti"
      ],
      [
        "print.localmoranex"
      ],
      [
        "print.localmoransad"
      ],
      [
        "print.moranex"
      ],
      [
        "print.moransad"
      ],
      [
        "print.spcor"
      ],
      [
        "print.summary.localmoransad"
      ],
      [
        "print.summary.moransad"
      ],
      [
        "probmap"
      ],
      [
        "prunecost"
      ],
      [
        "prunemst"
      ],
      [
        "read_swm_dbf"
      ],
      [
        "read.dat2listw"
      ],
      [
        "read.gal"
      ],
      [
        "read.geoda"
      ],
      [
        "read.gwt2nb"
      ],
      [
        "read.swmdbf2listw"
      ],
      [
        "relativeneigh"
      ],
      [
        "remove.self"
      ],
      [
        "Rotation"
      ],
      [
        "SD.RStests"
      ],
      [
        "set.ClusterOption"
      ],
      [
        "set.coresOption"
      ],
      [
        "set.mcOption"
      ],
      [
        "set.NoNeighbourOption"
      ],
      [
        "set.spChkOption"
      ],
      [
        "set.SubgraphCeiling"
      ],
      [
        "set.SubgraphOption"
      ],
      [
        "set.VerboseOption"
      ],
      [
        "set.ZeroPolicyOption"
      ],
      [
        "setdiff.nb"
      ],
      [
        "skater"
      ],
      [
        "sn2listw"
      ],
      [
        "soi.graph"
      ],
      [
        "sp.correlogram"
      ],
      [
        "sp.mantel.mc"
      ],
      [
        "spatialdelta"
      ],
      [
        "spdep"
      ],
      [
        "spNamedVec"
      ],
      [
        "spweights.constants"
      ],
      [
        "ssw"
      ],
      [
        "subset.listw"
      ],
      [
        "subset.nb"
      ],
      [
        "summary.localmoransad"
      ],
      [
        "summary.moransad"
      ],
      [
        "sym.attr.nb"
      ],
      [
        "Szero"
      ],
      [
        "tolerance.nb"
      ],
      [
        "tri2nb"
      ],
      [
        "union.nb"
      ],
      [
        "vi2mrc"
      ],
      [
        "warn_factor_preds"
      ],
      [
        "write_swm_dbf"
      ],
      [
        "write.nb.gal"
      ],
      [
        "write.sn2dat"
      ],
      [
        "write.sn2DBF"
      ],
      [
        "write.sn2gwt"
      ],
      [
        "write.swmdbf"
      ]
    ],
    "topics": [
      [
        "spatial-autocorrelation"
      ],
      [
        "spatial-dependence"
      ],
      [
        "spatial-weights"
      ]
    ],
    "score": 16.9747,
    "stars": 139,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "spdep Spatial Dependence: Weighting Schemes, Statistics A collection of functions to create spatial weights matrix\nobjects from polygon 'contiguities', from point patterns by\ndistance and tessellations, for summarizing these objects, and\nfor permitting their use in spatial data analysis, including\nregional aggregation by minimum spanning tree; a collection of\ntests for spatial 'autocorrelation', including global 'Morans\nI' and 'Gearys C' proposed by 'Cliff' and 'Ord' (1973, ISBN:\n0850860369) and (1981, ISBN: 0850860814), 'Hubert/Mantel'\ngeneral cross product statistic, Empirical Bayes estimates and\n'Assun\u00e7\u00e3o/Reis' (1999)\n<doi:10.1002/(SICI)1097-0258(19990830)18:16%3C2147::AID-SIM179%3E3.0.CO;2-I>\nIndex, 'Getis/Ord' G ('Getis' and 'Ord' 1992)\n<doi:10.1111/j.1538-4632.1992.tb00261.x> and multicoloured join\ncount statistics, 'APLE' ('Li et al.' )\n<doi:10.1111/j.1538-4632.2007.00708.x>, local 'Moran's I',\n'Gearys C' ('Anselin' 1995)\n<doi:10.1111/j.1538-4632.1995.tb00338.x> and 'Getis/Ord' G\n('Ord' and 'Getis' 1995)\n<doi:10.1111/j.1538-4632.1995.tb00912.x>, 'saddlepoint'\napproximations ('Tiefelsdorf' 2002)\n<doi:10.1111/j.1538-4632.2002.tb01084.x> and exact tests for\nglobal and local 'Moran's I' ('Bivand et al.' 2009)\n<doi:10.1016/j.csda.2008.07.021> and 'LOSH' local indicators of\nspatial heteroscedasticity ('Ord' and 'Getis')\n<doi:10.1007/s00168-011-0492-y>. The implementation of most of\nthese measures is described in 'Bivand' and 'Wong' (2018)\n<doi:10.1007/s11749-018-0599-x>, with further extensions in\n'Bivand' (2022) <doi:10.1111/gean.12319>. 'Lagrange' multiplier\ntests for spatial dependence in linear models are provided\n('Anselin et al'. 1996) <doi:10.1016/0166-0462(95)02111-6>, as\nare 'Rao' score tests for hypothesised spatial 'Durbin' models\nbased on linear models ('Koley' and 'Bera' 2023)\n<doi:10.1080/17421772.2023.2256810>. Additions in 2024 include\nLocal Indicators for Categorical Data based on 'Carrer et al.'\n(2021) <doi:10.1016/j.jas.2020.105306> and 'Bivand et al.'\n(2017) <doi:10.1016/j.spasta.2017.03.003>; also Weighted\nMultivariate Spatial Autocorrelation Measures ('Bavaud' 2024)\n<doi:10.1111/gean.12390>. <doi:10.1080/17421772.2023.2256810>.\nA local indicators for categorical data (LICD) implementation\nbased on 'Carrer et al.' (2021) <doi:10.1016/j.jas.2020.105306>\nand 'Bivand et al.' (2017) <doi:10.1016/j.spasta.2017.03.003>\nwas added in 1.3-7. Multivariate 'spatialdelta' ('Bavaud' 2024)\n<doi:10.1111/gean.12390> was added in 1.3-13 ('Bivand' 2025\n<doi:10.26034/la.cdclsl.2025.8343>). From 'spdep' and\n'spatialreg' versions >= 1.2-1, the model fitting functions\npreviously present in this package are defunct in 'spdep' and\nmay be found in 'spatialreg'. addlinks1 aggregate.nb airdist as.data.frame.localmoranex as.data.frame.localmoransad autocov_dist card cell2nb chkIDs choynowski coerce complement.nb cornish_fisher df2sn diffnb dnearneigh droplinks EBest EBImoran.mc EBlocal edit.nb factorial_coordinates gabrielneigh geary geary.mc geary.test get.ClusterOption get.coresOption get.mcOption get.NoNeighbourOption get.spChkOption get.SubgraphCeiling get.SubgraphOption get.VerboseOption get.ZeroPolicyOption globalG.test graph_distance_weights graph2nb grid2nb have_factor_preds_mf hotspot include.self intersect.nb is.symmetric.glist is.symmetric.nb iterative_proportional_fitting_weights joincount.mc joincount.multi joincount.test knearneigh knn2nb lag.listw lee lee.mc lee.test licd_multi linearised_diffusive_weights listw2lines listw2mat listw2sn listw2star listw2U listw2WB lm.LMtests lm.morantest lm.morantest.exact lm.morantest.sad lm.RStests local_joincount_bv local_joincount_uni localC localC_perm localdelta localG localG_perm localGS localmoran localmoran_bv localmoran_perm localmoran.exact localmoran.exact.alt localmoran.sad LOSH LOSH.cs LOSH.mc make.sym.nb mat2listw metropolis_hastings_weights moran moran_bv moran.mc moran.plot moran.test mstree n.comp.nb nb2blocknb nb2INLA nb2lines nb2listw nb2listwdist nb2mat nb2WB nbcost nbcosts nbdists nblag nblag_cumul old.make.sym.nb p.adjustSP plot_factorialcoords plot_factorialscree plot_moran plot_spatialcoords plot_spatialscree plot.Gabriel plot.listw plot.mc.sim plot.mst plot.nb plot.relative plot.skater plot.spcor poly2nb print.jclist print.jcmulti print.localmoranex print.localmoransad print.moranex print.moransad print.spcor print.summary.localmoransad print.summary.moransad probmap prunecost prunemst read_swm_dbf read.dat2listw read.gal read.geoda read.gwt2nb read.swmdbf2listw relativeneigh remove.self Rotation SD.RStests set.ClusterOption set.coresOption set.mcOption set.NoNeighbourOption set.spChkOption set.SubgraphCeiling set.SubgraphOption set.VerboseOption set.ZeroPolicyOption setdiff.nb skater sn2listw soi.graph sp.correlogram sp.mantel.mc spatialdelta spdep spNamedVec spweights.constants ssw subset.listw subset.nb summary.localmoransad summary.moransad sym.attr.nb Szero tolerance.nb tri2nb union.nb vi2mrc warn_factor_preds write_swm_dbf write.nb.gal write.sn2dat write.sn2DBF write.sn2gwt write.swmdbf spatial-autocorrelation spatial-dependence spatial-weights"
  },
  {
    "id": 1289,
    "package_name": "svglite",
    "title": "An 'SVG' Graphics Device",
    "description": "A graphics device for R that produces 'Scalable Vector\nGraphics'. 'svglite' is a fork of the older 'RSvgDevice'\npackage.",
    "version": "2.2.2.9000",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Hadley Wickham [aut],\nLionel Henry [aut],\nThomas Lin Pedersen [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nT Jake Luciani [aut],\nMatthieu Decorde [aut],\nVaudor Lise [aut],\nTony Plate [ctb] (Early line dashing code),\nDavid Gohel [ctb] (Line dashing code and early raster code),\nYixuan Qiu [ctb] (Improved styles; polypath implementation),\nH\u00e5kon Malmedal [ctb] (Opacity code),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://svglite.r-lib.org, https://github.com/r-lib/svglite",
    "bug_reports": "https://github.com/r-lib/svglite/issues",
    "repository": "",
    "exports": [
      [
        "add_fonts"
      ],
      [
        "add_web_fonts"
      ],
      [
        "create_svgz"
      ],
      [
        "editSVG"
      ],
      [
        "font_face"
      ],
      [
        "font_feature"
      ],
      [
        "fonts_as_import"
      ],
      [
        "htmlSVG"
      ],
      [
        "register_font"
      ],
      [
        "register_variant"
      ],
      [
        "require_font"
      ],
      [
        "stringSVG"
      ],
      [
        "svglite"
      ],
      [
        "svgstring"
      ],
      [
        "xmlSVG"
      ]
    ],
    "topics": [
      [
        "svg"
      ],
      [
        "libpng"
      ],
      [
        "cpp"
      ]
    ],
    "score": 16.705,
    "stars": 200,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "svglite An 'SVG' Graphics Device A graphics device for R that produces 'Scalable Vector\nGraphics'. 'svglite' is a fork of the older 'RSvgDevice'\npackage. add_fonts add_web_fonts create_svgz editSVG font_face font_feature fonts_as_import htmlSVG register_font register_variant require_font stringSVG svglite svgstring xmlSVG svg libpng cpp"
  },
  {
    "id": 1478,
    "package_name": "zoo",
    "title": "S3 Infrastructure for Regular and Irregular Time Series (Z's\nOrdered Observations)",
    "description": "An S3 class with methods for totally ordered indexed\nobservations. It is particularly aimed at irregular time series\nof numeric vectors/matrices and factors. zoo's key design goals\nare independence of a particular index/date/time class and\nconsistency with ts and base R by providing methods to extend\nstandard generics.",
    "version": "1.8-14",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-0918-3766>),\nGabor Grothendieck [aut],\nJeffrey A. Ryan [aut],\nJoshua M. Ulrich [ctb],\nFelix Andrews [ctb]",
    "url": "https://zoo.R-Forge.R-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "as.Date"
      ],
      [
        "as.Date.numeric"
      ],
      [
        "as.Date.ts"
      ],
      [
        "as.Date.yearmon"
      ],
      [
        "as.Date.yearqtr"
      ],
      [
        "as.yearmon"
      ],
      [
        "as.yearmon.default"
      ],
      [
        "as.yearqtr"
      ],
      [
        "as.yearqtr.default"
      ],
      [
        "as.zoo"
      ],
      [
        "as.zoo.default"
      ],
      [
        "as.zooreg"
      ],
      [
        "as.zooreg.default"
      ],
      [
        "autoplot.zoo"
      ],
      [
        "cbind.zoo"
      ],
      [
        "coredata"
      ],
      [
        "coredata.default"
      ],
      [
        "coredata<-"
      ],
      [
        "facet_free"
      ],
      [
        "format.yearqtr"
      ],
      [
        "fortify.zoo"
      ],
      [
        "frequency<-"
      ],
      [
        "ifelse.zoo"
      ],
      [
        "index"
      ],
      [
        "index<-"
      ],
      [
        "index2char"
      ],
      [
        "is.regular"
      ],
      [
        "is.zoo"
      ],
      [
        "make.par.list"
      ],
      [
        "MATCH"
      ],
      [
        "MATCH.default"
      ],
      [
        "MATCH.times"
      ],
      [
        "median.zoo"
      ],
      [
        "merge.zoo"
      ],
      [
        "na.aggregate"
      ],
      [
        "na.aggregate.default"
      ],
      [
        "na.approx"
      ],
      [
        "na.approx.default"
      ],
      [
        "na.fill"
      ],
      [
        "na.fill.default"
      ],
      [
        "na.fill0"
      ],
      [
        "na.locf"
      ],
      [
        "na.locf.default"
      ],
      [
        "na.locf0"
      ],
      [
        "na.spline"
      ],
      [
        "na.spline.default"
      ],
      [
        "na.StructTS"
      ],
      [
        "na.trim"
      ],
      [
        "na.trim.default"
      ],
      [
        "na.trim.ts"
      ],
      [
        "ORDER"
      ],
      [
        "ORDER.default"
      ],
      [
        "panel.lines.its"
      ],
      [
        "panel.lines.tis"
      ],
      [
        "panel.lines.ts"
      ],
      [
        "panel.lines.zoo"
      ],
      [
        "panel.plot.custom"
      ],
      [
        "panel.plot.default"
      ],
      [
        "panel.points.its"
      ],
      [
        "panel.points.tis"
      ],
      [
        "panel.points.ts"
      ],
      [
        "panel.points.zoo"
      ],
      [
        "panel.polygon.its"
      ],
      [
        "panel.polygon.tis"
      ],
      [
        "panel.polygon.ts"
      ],
      [
        "panel.polygon.zoo"
      ],
      [
        "panel.rect.its"
      ],
      [
        "panel.rect.tis"
      ],
      [
        "panel.rect.ts"
      ],
      [
        "panel.rect.zoo"
      ],
      [
        "panel.segments.its"
      ],
      [
        "panel.segments.tis"
      ],
      [
        "panel.segments.ts"
      ],
      [
        "panel.segments.zoo"
      ],
      [
        "panel.text.its"
      ],
      [
        "panel.text.tis"
      ],
      [
        "panel.text.ts"
      ],
      [
        "panel.text.zoo"
      ],
      [
        "plot.zoo"
      ],
      [
        "quantile.zoo"
      ],
      [
        "rbind.zoo"
      ],
      [
        "read.csv.zoo"
      ],
      [
        "read.csv2.zoo"
      ],
      [
        "read.delim.zoo"
      ],
      [
        "read.delim2.zoo"
      ],
      [
        "read.table.zoo"
      ],
      [
        "read.zoo"
      ],
      [
        "rev.zoo"
      ],
      [
        "rollapply"
      ],
      [
        "rollapplyr"
      ],
      [
        "rollmax"
      ],
      [
        "rollmax.default"
      ],
      [
        "rollmaxr"
      ],
      [
        "rollmean"
      ],
      [
        "rollmean.default"
      ],
      [
        "rollmeanr"
      ],
      [
        "rollmedian"
      ],
      [
        "rollmedian.default"
      ],
      [
        "rollmedianr"
      ],
      [
        "rollsum"
      ],
      [
        "rollsum.default"
      ],
      [
        "rollsumr"
      ],
      [
        "scale_type.yearmon"
      ],
      [
        "scale_type.yearqtr"
      ],
      [
        "scale_x_yearmon"
      ],
      [
        "scale_x_yearqtr"
      ],
      [
        "scale_y_yearmon"
      ],
      [
        "scale_y_yearqtr"
      ],
      [
        "Sys.yearmon"
      ],
      [
        "Sys.yearqtr"
      ],
      [
        "time<-"
      ],
      [
        "tinyplot.zoo"
      ],
      [
        "write.zoo"
      ],
      [
        "xblocks"
      ],
      [
        "xblocks.default"
      ],
      [
        "xtfrm.zoo"
      ],
      [
        "yearmon"
      ],
      [
        "yearmon_trans"
      ],
      [
        "yearqtr"
      ],
      [
        "yearqtr_trans"
      ],
      [
        "zoo"
      ],
      [
        "zooreg"
      ]
    ],
    "topics": [],
    "score": 16.5984,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "zoo S3 Infrastructure for Regular and Irregular Time Series (Z's\nOrdered Observations) An S3 class with methods for totally ordered indexed\nobservations. It is particularly aimed at irregular time series\nof numeric vectors/matrices and factors. zoo's key design goals\nare independence of a particular index/date/time class and\nconsistency with ts and base R by providing methods to extend\nstandard generics. as.Date as.Date.numeric as.Date.ts as.Date.yearmon as.Date.yearqtr as.yearmon as.yearmon.default as.yearqtr as.yearqtr.default as.zoo as.zoo.default as.zooreg as.zooreg.default autoplot.zoo cbind.zoo coredata coredata.default coredata<- facet_free format.yearqtr fortify.zoo frequency<- ifelse.zoo index index<- index2char is.regular is.zoo make.par.list MATCH MATCH.default MATCH.times median.zoo merge.zoo na.aggregate na.aggregate.default na.approx na.approx.default na.fill na.fill.default na.fill0 na.locf na.locf.default na.locf0 na.spline na.spline.default na.StructTS na.trim na.trim.default na.trim.ts ORDER ORDER.default panel.lines.its panel.lines.tis panel.lines.ts panel.lines.zoo panel.plot.custom panel.plot.default panel.points.its panel.points.tis panel.points.ts panel.points.zoo panel.polygon.its panel.polygon.tis panel.polygon.ts panel.polygon.zoo panel.rect.its panel.rect.tis panel.rect.ts panel.rect.zoo panel.segments.its panel.segments.tis panel.segments.ts panel.segments.zoo panel.text.its panel.text.tis panel.text.ts panel.text.zoo plot.zoo quantile.zoo rbind.zoo read.csv.zoo read.csv2.zoo read.delim.zoo read.delim2.zoo read.table.zoo read.zoo rev.zoo rollapply rollapplyr rollmax rollmax.default rollmaxr rollmean rollmean.default rollmeanr rollmedian rollmedian.default rollmedianr rollsum rollsum.default rollsumr scale_type.yearmon scale_type.yearqtr scale_x_yearmon scale_x_yearqtr scale_y_yearmon scale_y_yearqtr Sys.yearmon Sys.yearqtr time<- tinyplot.zoo write.zoo xblocks xblocks.default xtfrm.zoo yearmon yearmon_trans yearqtr yearqtr_trans zoo zooreg "
  },
  {
    "id": 667,
    "package_name": "gstat",
    "title": "Spatial and Spatio-Temporal Geostatistical Modelling, Prediction\nand Simulation",
    "description": "Variogram modelling; simple, ordinary and universal point\nor block (co)kriging; spatio-temporal kriging; sequential\nGaussian or indicator (co)simulation; variogram and variogram\nmap plotting utility functions; supports sf and stars.",
    "version": "2.1-5",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8049-7069>), Benedikt Graeler\n[aut]",
    "url": "https://github.com/r-spatial/gstat/,\nhttps://r-spatial.github.io/gstat/",
    "bug_reports": "https://github.com/r-spatial/gstat/issues/",
    "repository": "",
    "exports": [
      [
        "[.gstat"
      ],
      [
        "as.vgm.variomodel"
      ],
      [
        "cross.name"
      ],
      [
        "estiStAni"
      ],
      [
        "extractPar"
      ],
      [
        "extractParNames"
      ],
      [
        "fit.lmc"
      ],
      [
        "fit.StVariogram"
      ],
      [
        "fit.variogram"
      ],
      [
        "fit.variogram.gls"
      ],
      [
        "fit.variogram.reml"
      ],
      [
        "get_gstat_progress"
      ],
      [
        "get.contr"
      ],
      [
        "gstat"
      ],
      [
        "gstat.cv"
      ],
      [
        "hscat"
      ],
      [
        "idw"
      ],
      [
        "idw0"
      ],
      [
        "krige"
      ],
      [
        "krige.cv"
      ],
      [
        "krige0"
      ],
      [
        "krigeSimCE"
      ],
      [
        "krigeST"
      ],
      [
        "krigeSTSimTB"
      ],
      [
        "krigeSTTg"
      ],
      [
        "krigeTg"
      ],
      [
        "map.to.lev"
      ],
      [
        "ossfim"
      ],
      [
        "panel.pointPairs"
      ],
      [
        "set_gstat_progress"
      ],
      [
        "show.vgms"
      ],
      [
        "spplot.vcov"
      ],
      [
        "variogram"
      ],
      [
        "variogramLine"
      ],
      [
        "variogramST"
      ],
      [
        "variogramSurface"
      ],
      [
        "vgm"
      ],
      [
        "vgm.panel.xyplot"
      ],
      [
        "vgmArea"
      ],
      [
        "vgmAreaST"
      ],
      [
        "vgmST"
      ],
      [
        "xyz2img"
      ]
    ],
    "topics": [
      [
        "openblas"
      ]
    ],
    "score": 15.8623,
    "stars": 205,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "gstat Spatial and Spatio-Temporal Geostatistical Modelling, Prediction\nand Simulation Variogram modelling; simple, ordinary and universal point\nor block (co)kriging; spatio-temporal kriging; sequential\nGaussian or indicator (co)simulation; variogram and variogram\nmap plotting utility functions; supports sf and stars. [.gstat as.vgm.variomodel cross.name estiStAni extractPar extractParNames fit.lmc fit.StVariogram fit.variogram fit.variogram.gls fit.variogram.reml get_gstat_progress get.contr gstat gstat.cv hscat idw idw0 krige krige.cv krige0 krigeSimCE krigeST krigeSTSimTB krigeSTTg krigeTg map.to.lev ossfim panel.pointPairs set_gstat_progress show.vgms spplot.vcov variogram variogramLine variogramST variogramSurface vgm vgm.panel.xyplot vgmArea vgmAreaST vgmST xyz2img openblas"
  },
  {
    "id": 583,
    "package_name": "fontawesome",
    "title": "Easily Work with 'Font Awesome' Icons",
    "description": "Easily and flexibly insert 'Font Awesome' icons into 'R\nMarkdown' documents and 'Shiny' apps. These icons can be\ninserted into HTML content through inline 'SVG' tags or 'i'\ntags. There is also a utility function for exporting 'Font\nAwesome' icons as 'PNG' images for those situations where\nraster graphics are needed.",
    "version": "0.5.3.9000",
    "maintainer": "Richard Iannone <rich@posit.co>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3925-190X>),\nChristophe Dervieux [ctb] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nWinston Chang [ctb],\nDave Gandy [ctb, cph] (Font-Awesome font),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/fontawesome,\nhttps://rstudio.github.io/fontawesome/",
    "bug_reports": "https://github.com/rstudio/fontawesome/issues",
    "repository": "",
    "exports": [
      [
        "fa"
      ],
      [
        "fa_html_dependency"
      ],
      [
        "fa_i"
      ],
      [
        "fa_metadata"
      ],
      [
        "fa_png"
      ]
    ],
    "topics": [
      [
        "font-awesome"
      ],
      [
        "svg-icons"
      ]
    ],
    "score": 15.7921,
    "stars": 300,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "fontawesome Easily Work with 'Font Awesome' Icons Easily and flexibly insert 'Font Awesome' icons into 'R\nMarkdown' documents and 'Shiny' apps. These icons can be\ninserted into HTML content through inline 'SVG' tags or 'i'\ntags. There is also a utility function for exporting 'Font\nAwesome' icons as 'PNG' images for those situations where\nraster graphics are needed. fa fa_html_dependency fa_i fa_metadata fa_png font-awesome svg-icons"
  },
  {
    "id": 1050,
    "package_name": "ragg",
    "title": "Graphic Devices Based on AGG",
    "description": "Anti-Grain Geometry (AGG) is a high-quality and\nhigh-performance 2D drawing library. The 'ragg' package\nprovides a set of graphic devices based on AGG to use as\nalternative to the raster devices provided through the\n'grDevices' package.",
    "version": "1.5.0.9000",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Thomas Lin Pedersen [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nMaxim Shemanarev [aut, cph] (Author of AGG),\nTony Juricic [ctb, cph] (Contributor to AGG),\nMilan Marusinec [ctb, cph] (Contributor to AGG),\nSpencer Garrett [ctb] (Contributor to AGG),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://ragg.r-lib.org, https://github.com/r-lib/ragg",
    "bug_reports": "https://github.com/r-lib/ragg/issues",
    "repository": "",
    "exports": [
      [
        "agg_capture"
      ],
      [
        "agg_jpeg"
      ],
      [
        "agg_png"
      ],
      [
        "agg_ppm"
      ],
      [
        "agg_record"
      ],
      [
        "agg_supertransparent"
      ],
      [
        "agg_tiff"
      ],
      [
        "agg_webp"
      ],
      [
        "agg_webp_anim"
      ],
      [
        "font_feature"
      ],
      [
        "get_font_features"
      ],
      [
        "register_font"
      ],
      [
        "register_variant"
      ]
    ],
    "topics": [
      [
        "drawing"
      ],
      [
        "graphics"
      ],
      [
        "vector-graphics"
      ],
      [
        "freetype"
      ],
      [
        "libpng"
      ],
      [
        "tiff"
      ],
      [
        "libjpeg-turbo"
      ],
      [
        "libwebp"
      ],
      [
        "cpp"
      ]
    ],
    "score": 15.5783,
    "stars": 181,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "ragg Graphic Devices Based on AGG Anti-Grain Geometry (AGG) is a high-quality and\nhigh-performance 2D drawing library. The 'ragg' package\nprovides a set of graphic devices based on AGG to use as\nalternative to the raster devices provided through the\n'grDevices' package. agg_capture agg_jpeg agg_png agg_ppm agg_record agg_supertransparent agg_tiff agg_webp agg_webp_anim font_feature get_font_features register_font register_variant drawing graphics vector-graphics freetype libpng tiff libjpeg-turbo libwebp cpp"
  },
  {
    "id": 300,
    "package_name": "bit64",
    "title": "A S3 Class for Vectors of 64bit Integers",
    "description": "Package 'bit64' provides serializable S3 atomic 64bit\n(signed) integers. These are useful for handling database keys\nand exact counting in +-2^63. WARNING: do not use them as\nreplacement for 32bit integers, integer64 are not supported for\nsubscripting by R-core and they have different semantics when\ncombined with double, e.g. integer64 + double => integer64.\nClass integer64 can be used in vectors, matrices, arrays and\ndata.frames. Methods are available for coercion from and to\nlogicals, integers, doubles, characters and factors as well as\nmany elementwise and summary functions. Many fast algorithmic\noperations such as 'match' and 'order' support inter- active\ndata exploration and manipulation and optionally leverage\ncaching.",
    "version": "4.7.99",
    "maintainer": "Michael Chirico <michaelchirico4@gmail.com>",
    "author": "Michael Chirico [aut, cre],\nJens Oehlschl\u00e4gel [aut],\nLeonardo Silvestri [ctb],\nOfek Shilon [ctb]",
    "url": "https://github.com/r-lib/bit64, https://bit64.r-lib.org",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        ":"
      ],
      [
        ":.default"
      ],
      [
        ":.integer64"
      ],
      [
        "[.integer64"
      ],
      [
        "[[.integer64"
      ],
      [
        "[[<-.integer64"
      ],
      [
        "[<-.integer64"
      ],
      [
        "%in%"
      ],
      [
        "%in%.default"
      ],
      [
        "%in%.integer64"
      ],
      [
        "abs.integer64"
      ],
      [
        "all.equal.integer64"
      ],
      [
        "all.integer64"
      ],
      [
        "any.integer64"
      ],
      [
        "as.bitstring"
      ],
      [
        "as.bitstring.integer64"
      ],
      [
        "as.character.integer64"
      ],
      [
        "as.data.frame.integer64"
      ],
      [
        "as.double.integer64"
      ],
      [
        "as.integer.integer64"
      ],
      [
        "as.integer64"
      ],
      [
        "as.integer64.bitstring"
      ],
      [
        "as.integer64.character"
      ],
      [
        "as.integer64.double"
      ],
      [
        "as.integer64.factor"
      ],
      [
        "as.integer64.integer"
      ],
      [
        "as.integer64.integer64"
      ],
      [
        "as.integer64.logical"
      ],
      [
        "as.integer64.NULL"
      ],
      [
        "as.list.integer64"
      ],
      [
        "as.logical.integer64"
      ],
      [
        "benchmark64"
      ],
      [
        "binattr"
      ],
      [
        "c.integer64"
      ],
      [
        "cache"
      ],
      [
        "cbind.integer64"
      ],
      [
        "colSums"
      ],
      [
        "diff.integer64"
      ],
      [
        "duplicated.integer64"
      ],
      [
        "format.integer64"
      ],
      [
        "getcache"
      ],
      [
        "hashcache"
      ],
      [
        "hashdup"
      ],
      [
        "hashdup.cache_integer64"
      ],
      [
        "hashfin"
      ],
      [
        "hashfin.cache_integer64"
      ],
      [
        "hashfun"
      ],
      [
        "hashfun.integer64"
      ],
      [
        "hashmap"
      ],
      [
        "hashmap.integer64"
      ],
      [
        "hashmaptab"
      ],
      [
        "hashmaptab.integer64"
      ],
      [
        "hashmapuni"
      ],
      [
        "hashmapuni.integer64"
      ],
      [
        "hashmapupo"
      ],
      [
        "hashmapupo.integer64"
      ],
      [
        "hashpos"
      ],
      [
        "hashpos.cache_integer64"
      ],
      [
        "hashrev"
      ],
      [
        "hashrev.cache_integer64"
      ],
      [
        "hashrin"
      ],
      [
        "hashrin.cache_integer64"
      ],
      [
        "hashtab"
      ],
      [
        "hashtab.cache_integer64"
      ],
      [
        "hashuni"
      ],
      [
        "hashuni.cache_integer64"
      ],
      [
        "hashupo"
      ],
      [
        "hashupo.cache_integer64"
      ],
      [
        "identical.integer64"
      ],
      [
        "integer64"
      ],
      [
        "is.double"
      ],
      [
        "is.double.default"
      ],
      [
        "is.double.integer64"
      ],
      [
        "is.finite.integer64"
      ],
      [
        "is.infinite.integer64"
      ],
      [
        "is.integer64"
      ],
      [
        "is.na.integer64"
      ],
      [
        "is.nan.integer64"
      ],
      [
        "is.sorted.integer64"
      ],
      [
        "is.vector.integer64"
      ],
      [
        "jamcache"
      ],
      [
        "keypos"
      ],
      [
        "keypos.integer64"
      ],
      [
        "length<-.integer64"
      ],
      [
        "lim.integer64"
      ],
      [
        "log.integer64"
      ],
      [
        "match"
      ],
      [
        "match.default"
      ],
      [
        "match.integer64"
      ],
      [
        "max.integer64"
      ],
      [
        "mean.integer64"
      ],
      [
        "median.integer64"
      ],
      [
        "mergeorder.integer64"
      ],
      [
        "mergesort.integer64"
      ],
      [
        "mergesortorder.integer64"
      ],
      [
        "min.integer64"
      ],
      [
        "minusclass"
      ],
      [
        "NA_integer64_"
      ],
      [
        "na.count.integer64"
      ],
      [
        "newcache"
      ],
      [
        "nties.integer64"
      ],
      [
        "nunique.integer64"
      ],
      [
        "nvalid.integer64"
      ],
      [
        "optimizer64"
      ],
      [
        "order"
      ],
      [
        "order.default"
      ],
      [
        "order.integer64"
      ],
      [
        "ordercache"
      ],
      [
        "orderdup"
      ],
      [
        "orderdup.integer64"
      ],
      [
        "orderfin"
      ],
      [
        "orderfin.integer64"
      ],
      [
        "orderkey"
      ],
      [
        "orderkey.integer64"
      ],
      [
        "ordernut"
      ],
      [
        "ordernut.integer64"
      ],
      [
        "orderpos"
      ],
      [
        "orderpos.integer64"
      ],
      [
        "orderqtl"
      ],
      [
        "orderqtl.integer64"
      ],
      [
        "orderrnk"
      ],
      [
        "orderrnk.integer64"
      ],
      [
        "ordertab"
      ],
      [
        "ordertab.integer64"
      ],
      [
        "ordertie"
      ],
      [
        "ordertie.integer64"
      ],
      [
        "orderuni"
      ],
      [
        "orderuni.integer64"
      ],
      [
        "orderupo"
      ],
      [
        "orderupo.integer64"
      ],
      [
        "plusclass"
      ],
      [
        "prank"
      ],
      [
        "prank.integer64"
      ],
      [
        "print.bitstring"
      ],
      [
        "print.cache"
      ],
      [
        "print.integer64"
      ],
      [
        "qtile"
      ],
      [
        "qtile.integer64"
      ],
      [
        "quantile.integer64"
      ],
      [
        "quickorder.integer64"
      ],
      [
        "quicksort.integer64"
      ],
      [
        "quicksortorder.integer64"
      ],
      [
        "radixorder.integer64"
      ],
      [
        "radixsort.integer64"
      ],
      [
        "radixsortorder.integer64"
      ],
      [
        "ramorder.integer64"
      ],
      [
        "ramsort.integer64"
      ],
      [
        "ramsortorder.integer64"
      ],
      [
        "rank"
      ],
      [
        "rank.default"
      ],
      [
        "rank.integer64"
      ],
      [
        "rbind.integer64"
      ],
      [
        "remcache"
      ],
      [
        "rep.integer64"
      ],
      [
        "rowSums"
      ],
      [
        "runif64"
      ],
      [
        "scale.integer64"
      ],
      [
        "seq.integer64"
      ],
      [
        "setcache"
      ],
      [
        "shellorder.integer64"
      ],
      [
        "shellsort.integer64"
      ],
      [
        "shellsortorder.integer64"
      ],
      [
        "sort.integer64"
      ],
      [
        "sortcache"
      ],
      [
        "sortfin"
      ],
      [
        "sortfin.integer64"
      ],
      [
        "sortnut"
      ],
      [
        "sortnut.integer64"
      ],
      [
        "sortordercache"
      ],
      [
        "sortorderdup"
      ],
      [
        "sortorderdup.integer64"
      ],
      [
        "sortorderkey"
      ],
      [
        "sortorderkey.integer64"
      ],
      [
        "sortorderpos"
      ],
      [
        "sortorderpos.integer64"
      ],
      [
        "sortorderrnk"
      ],
      [
        "sortorderrnk.integer64"
      ],
      [
        "sortordertab"
      ],
      [
        "sortordertab.integer64"
      ],
      [
        "sortordertie"
      ],
      [
        "sortordertie.integer64"
      ],
      [
        "sortorderuni"
      ],
      [
        "sortorderuni.integer64"
      ],
      [
        "sortorderupo"
      ],
      [
        "sortorderupo.integer64"
      ],
      [
        "sortqtl"
      ],
      [
        "sortqtl.integer64"
      ],
      [
        "sorttab"
      ],
      [
        "sorttab.integer64"
      ],
      [
        "sortuni"
      ],
      [
        "sortuni.integer64"
      ],
      [
        "str.integer64"
      ],
      [
        "sum.integer64"
      ],
      [
        "summary.integer64"
      ],
      [
        "table.integer64"
      ],
      [
        "tiepos"
      ],
      [
        "tiepos.integer64"
      ],
      [
        "unipos"
      ],
      [
        "unipos.integer64"
      ],
      [
        "unique.integer64"
      ],
      [
        "xor.integer64"
      ]
    ],
    "topics": [],
    "score": 15.3512,
    "stars": 38,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "bit64 A S3 Class for Vectors of 64bit Integers Package 'bit64' provides serializable S3 atomic 64bit\n(signed) integers. These are useful for handling database keys\nand exact counting in +-2^63. WARNING: do not use them as\nreplacement for 32bit integers, integer64 are not supported for\nsubscripting by R-core and they have different semantics when\ncombined with double, e.g. integer64 + double => integer64.\nClass integer64 can be used in vectors, matrices, arrays and\ndata.frames. Methods are available for coercion from and to\nlogicals, integers, doubles, characters and factors as well as\nmany elementwise and summary functions. Many fast algorithmic\noperations such as 'match' and 'order' support inter- active\ndata exploration and manipulation and optionally leverage\ncaching. : :.default :.integer64 [.integer64 [[.integer64 [[<-.integer64 [<-.integer64 %in% %in%.default %in%.integer64 abs.integer64 all.equal.integer64 all.integer64 any.integer64 as.bitstring as.bitstring.integer64 as.character.integer64 as.data.frame.integer64 as.double.integer64 as.integer.integer64 as.integer64 as.integer64.bitstring as.integer64.character as.integer64.double as.integer64.factor as.integer64.integer as.integer64.integer64 as.integer64.logical as.integer64.NULL as.list.integer64 as.logical.integer64 benchmark64 binattr c.integer64 cache cbind.integer64 colSums diff.integer64 duplicated.integer64 format.integer64 getcache hashcache hashdup hashdup.cache_integer64 hashfin hashfin.cache_integer64 hashfun hashfun.integer64 hashmap hashmap.integer64 hashmaptab hashmaptab.integer64 hashmapuni hashmapuni.integer64 hashmapupo hashmapupo.integer64 hashpos hashpos.cache_integer64 hashrev hashrev.cache_integer64 hashrin hashrin.cache_integer64 hashtab hashtab.cache_integer64 hashuni hashuni.cache_integer64 hashupo hashupo.cache_integer64 identical.integer64 integer64 is.double is.double.default is.double.integer64 is.finite.integer64 is.infinite.integer64 is.integer64 is.na.integer64 is.nan.integer64 is.sorted.integer64 is.vector.integer64 jamcache keypos keypos.integer64 length<-.integer64 lim.integer64 log.integer64 match match.default match.integer64 max.integer64 mean.integer64 median.integer64 mergeorder.integer64 mergesort.integer64 mergesortorder.integer64 min.integer64 minusclass NA_integer64_ na.count.integer64 newcache nties.integer64 nunique.integer64 nvalid.integer64 optimizer64 order order.default order.integer64 ordercache orderdup orderdup.integer64 orderfin orderfin.integer64 orderkey orderkey.integer64 ordernut ordernut.integer64 orderpos orderpos.integer64 orderqtl orderqtl.integer64 orderrnk orderrnk.integer64 ordertab ordertab.integer64 ordertie ordertie.integer64 orderuni orderuni.integer64 orderupo orderupo.integer64 plusclass prank prank.integer64 print.bitstring print.cache print.integer64 qtile qtile.integer64 quantile.integer64 quickorder.integer64 quicksort.integer64 quicksortorder.integer64 radixorder.integer64 radixsort.integer64 radixsortorder.integer64 ramorder.integer64 ramsort.integer64 ramsortorder.integer64 rank rank.default rank.integer64 rbind.integer64 remcache rep.integer64 rowSums runif64 scale.integer64 seq.integer64 setcache shellorder.integer64 shellsort.integer64 shellsortorder.integer64 sort.integer64 sortcache sortfin sortfin.integer64 sortnut sortnut.integer64 sortordercache sortorderdup sortorderdup.integer64 sortorderkey sortorderkey.integer64 sortorderpos sortorderpos.integer64 sortorderrnk sortorderrnk.integer64 sortordertab sortordertab.integer64 sortordertie sortordertie.integer64 sortorderuni sortorderuni.integer64 sortorderupo sortorderupo.integer64 sortqtl sortqtl.integer64 sorttab sorttab.integer64 sortuni sortuni.integer64 str.integer64 sum.integer64 summary.integer64 table.integer64 tiepos tiepos.integer64 unipos unipos.integer64 unique.integer64 xor.integer64 "
  },
  {
    "id": 299,
    "package_name": "bit",
    "title": "Classes and Methods for Fast Memory-Efficient Boolean Selections",
    "description": "Provided are classes for boolean and skewed boolean\nvectors, fast boolean methods, fast unique and non-unique\ninteger sorting, fast set operations on sorted and unsorted\nsets of integers, and foundations for ff (range index,\ncompression, chunked processing).",
    "version": "4.6.99",
    "maintainer": "Michael Chirico <MichaelChirico4@gmail.com>",
    "author": "Michael Chirico [aut, cre],\nJens Oehlschl\u00e4gel [aut],\nBrian Ripley [ctb]",
    "url": "https://github.com/r-lib/bit",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "!=.booltype"
      ],
      [
        ".BITS"
      ],
      [
        "&.booltype"
      ],
      [
        "==.booltype"
      ],
      [
        "|.booltype"
      ],
      [
        "all.booltype"
      ],
      [
        "any.booltype"
      ],
      [
        "anyNA.booltype"
      ],
      [
        "as.bit"
      ],
      [
        "as.bitwhich"
      ],
      [
        "as.booltype"
      ],
      [
        "as.ri"
      ],
      [
        "as.which"
      ],
      [
        "bbatch"
      ],
      [
        "bit"
      ],
      [
        "bit_anyDuplicated"
      ],
      [
        "bit_done"
      ],
      [
        "bit_duplicated"
      ],
      [
        "bit_in"
      ],
      [
        "bit_init"
      ],
      [
        "bit_intersect"
      ],
      [
        "bit_rangediff"
      ],
      [
        "bit_setdiff"
      ],
      [
        "bit_setequal"
      ],
      [
        "bit_sort"
      ],
      [
        "bit_sort_unique"
      ],
      [
        "bit_sumDuplicated"
      ],
      [
        "bit_symdiff"
      ],
      [
        "bit_union"
      ],
      [
        "bit_unique"
      ],
      [
        "bitsort"
      ],
      [
        "bitwhich"
      ],
      [
        "bitwhich_representation"
      ],
      [
        "booltype"
      ],
      [
        "booltypes"
      ],
      [
        "c.booltype"
      ],
      [
        "chunk"
      ],
      [
        "chunks"
      ],
      [
        "clone"
      ],
      [
        "copy_vector"
      ],
      [
        "countsort"
      ],
      [
        "firstNA"
      ],
      [
        "get_length"
      ],
      [
        "getsetattr"
      ],
      [
        "in.bitwhich"
      ],
      [
        "intisasc"
      ],
      [
        "intisdesc"
      ],
      [
        "intrle"
      ],
      [
        "is.bit"
      ],
      [
        "is.bitwhich"
      ],
      [
        "is.booltype"
      ],
      [
        "is.hi"
      ],
      [
        "is.ri"
      ],
      [
        "is.sorted"
      ],
      [
        "is.sorted<-"
      ],
      [
        "is.which"
      ],
      [
        "keyorder"
      ],
      [
        "keysort"
      ],
      [
        "keysortorder"
      ],
      [
        "max.booltype"
      ],
      [
        "maxindex"
      ],
      [
        "merge_anyDuplicated"
      ],
      [
        "merge_duplicated"
      ],
      [
        "merge_first"
      ],
      [
        "merge_firstin"
      ],
      [
        "merge_firstnotin"
      ],
      [
        "merge_in"
      ],
      [
        "merge_intersect"
      ],
      [
        "merge_last"
      ],
      [
        "merge_lastin"
      ],
      [
        "merge_lastnotin"
      ],
      [
        "merge_match"
      ],
      [
        "merge_notin"
      ],
      [
        "merge_rangediff"
      ],
      [
        "merge_rangein"
      ],
      [
        "merge_rangenotin"
      ],
      [
        "merge_rangesect"
      ],
      [
        "merge_rev"
      ],
      [
        "merge_setdiff"
      ],
      [
        "merge_setequal"
      ],
      [
        "merge_sumDuplicated"
      ],
      [
        "merge_symdiff"
      ],
      [
        "merge_union"
      ],
      [
        "merge_unique"
      ],
      [
        "mergeorder"
      ],
      [
        "mergesort"
      ],
      [
        "mergesortorder"
      ],
      [
        "min.booltype"
      ],
      [
        "na.count"
      ],
      [
        "na.count<-"
      ],
      [
        "nties"
      ],
      [
        "nties<-"
      ],
      [
        "nunique"
      ],
      [
        "nunique<-"
      ],
      [
        "nvalid"
      ],
      [
        "physical"
      ],
      [
        "physical<-"
      ],
      [
        "poslength"
      ],
      [
        "quickorder"
      ],
      [
        "quicksort"
      ],
      [
        "quicksort2"
      ],
      [
        "quicksort3"
      ],
      [
        "quicksortorder"
      ],
      [
        "radixorder"
      ],
      [
        "radixsort"
      ],
      [
        "radixsortorder"
      ],
      [
        "ramorder"
      ],
      [
        "ramsort"
      ],
      [
        "ramsortorder"
      ],
      [
        "range_na"
      ],
      [
        "range_nanozero"
      ],
      [
        "range_sortna"
      ],
      [
        "range.booltype"
      ],
      [
        "repeat.time"
      ],
      [
        "repfromto"
      ],
      [
        "repfromto<-"
      ],
      [
        "reverse_vector"
      ],
      [
        "ri"
      ],
      [
        "rlepack"
      ],
      [
        "rleunpack"
      ],
      [
        "setattr"
      ],
      [
        "setattributes"
      ],
      [
        "shellorder"
      ],
      [
        "shellsort"
      ],
      [
        "shellsortorder"
      ],
      [
        "still.identical"
      ],
      [
        "sum.booltype"
      ],
      [
        "summary.booltype"
      ],
      [
        "symdiff"
      ],
      [
        "unattr"
      ],
      [
        "vecseq"
      ],
      [
        "virtual"
      ],
      [
        "virtual<-"
      ],
      [
        "xor"
      ],
      [
        "xor.booltype"
      ]
    ],
    "topics": [],
    "score": 15.1458,
    "stars": 12,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "bit Classes and Methods for Fast Memory-Efficient Boolean Selections Provided are classes for boolean and skewed boolean\nvectors, fast boolean methods, fast unique and non-unique\ninteger sorting, fast set operations on sorted and unsorted\nsets of integers, and foundations for ff (range index,\ncompression, chunked processing). !=.booltype .BITS &.booltype ==.booltype |.booltype all.booltype any.booltype anyNA.booltype as.bit as.bitwhich as.booltype as.ri as.which bbatch bit bit_anyDuplicated bit_done bit_duplicated bit_in bit_init bit_intersect bit_rangediff bit_setdiff bit_setequal bit_sort bit_sort_unique bit_sumDuplicated bit_symdiff bit_union bit_unique bitsort bitwhich bitwhich_representation booltype booltypes c.booltype chunk chunks clone copy_vector countsort firstNA get_length getsetattr in.bitwhich intisasc intisdesc intrle is.bit is.bitwhich is.booltype is.hi is.ri is.sorted is.sorted<- is.which keyorder keysort keysortorder max.booltype maxindex merge_anyDuplicated merge_duplicated merge_first merge_firstin merge_firstnotin merge_in merge_intersect merge_last merge_lastin merge_lastnotin merge_match merge_notin merge_rangediff merge_rangein merge_rangenotin merge_rangesect merge_rev merge_setdiff merge_setequal merge_sumDuplicated merge_symdiff merge_union merge_unique mergeorder mergesort mergesortorder min.booltype na.count na.count<- nties nties<- nunique nunique<- nvalid physical physical<- poslength quickorder quicksort quicksort2 quicksort3 quicksortorder radixorder radixsort radixsortorder ramorder ramsort ramsortorder range_na range_nanozero range_sortna range.booltype repeat.time repfromto repfromto<- reverse_vector ri rlepack rleunpack setattr setattributes shellorder shellsort shellsortorder still.identical sum.booltype summary.booltype symdiff unattr vecseq virtual virtual<- xor xor.booltype "
  },
  {
    "id": 793,
    "package_name": "mapview",
    "title": "Interactive Viewing of Spatial Data in R",
    "description": "Quickly and conveniently create interactive visualisations\nof spatial data with or without background maps. Attributes of\ndisplayed features are fully queryable via pop-up windows.\nAdditional functionality includes methods to visualise true-\nand false-color raster images and bounding boxes.",
    "version": "2.11.4",
    "maintainer": "Tim Appelhans <tim.appelhans@gmail.com>",
    "author": "Tim Appelhans [cre, aut],\nFlorian Detsch [aut],\nChristoph Reudenbach [aut],\nStefan Woellauer [aut],\nSpaska Forteva [ctb],\nThomas Nauss [ctb],\nEdzer Pebesma [ctb],\nKenton Russell [ctb],\nMichael Sumner [ctb],\nJochen Darley [ctb],\nPierre Roudier [ctb],\nPatrick Schratz [ctb],\nEnvironmental Informatics Marburg [ctb],\nLorenzo Busetto [ctb]",
    "url": "https://github.com/r-spatial/mapview,\nhttps://r-spatial.github.io/mapview/",
    "bug_reports": "https://github.com/r-spatial/mapview/issues",
    "repository": "",
    "exports": [
      [
        "mapshot"
      ],
      [
        "mapshot2"
      ],
      [
        "mapview"
      ],
      [
        "mapView"
      ],
      [
        "mapviewColors"
      ],
      [
        "mapviewGetOption"
      ],
      [
        "mapviewOptions"
      ],
      [
        "mapviewOutput"
      ],
      [
        "mapviewPalette"
      ],
      [
        "mapViewPalette"
      ],
      [
        "mapviewWatcher"
      ],
      [
        "npts"
      ],
      [
        "removeMapJunk"
      ],
      [
        "renderMapview"
      ],
      [
        "startWatching"
      ],
      [
        "stopWatching"
      ],
      [
        "viewExtent"
      ],
      [
        "viewRGB"
      ]
    ],
    "topics": [
      [
        "gis"
      ],
      [
        "leaflet"
      ],
      [
        "maps"
      ],
      [
        "spatial"
      ],
      [
        "visualization"
      ],
      [
        "web-mapping"
      ]
    ],
    "score": 14.8724,
    "stars": 541,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "mapview Interactive Viewing of Spatial Data in R Quickly and conveniently create interactive visualisations\nof spatial data with or without background maps. Attributes of\ndisplayed features are fully queryable via pop-up windows.\nAdditional functionality includes methods to visualise true-\nand false-color raster images and bounding boxes. mapshot mapshot2 mapview mapView mapviewColors mapviewGetOption mapviewOptions mapviewOutput mapviewPalette mapViewPalette mapviewWatcher npts removeMapJunk renderMapview startWatching stopWatching viewExtent viewRGB gis leaflet maps spatial visualization web-mapping"
  },
  {
    "id": 618,
    "package_name": "gert",
    "title": "Simple Git Client for R",
    "description": "Simple git client for R based on 'libgit2'\n<https://libgit2.org> with support for SSH and HTTPS remotes.\nAll functions in 'gert' use basic R data types (such as vectors\nand data-frames) for their arguments and return values. User\ncredentials are shared with command line 'git' through the\ngit-credential store and ssh keys stored on disk or ssh-agent.",
    "version": "2.2.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nJennifer Bryan [ctb] (ORCID: <https://orcid.org/0000-0002-6983-2759>)",
    "url": "https://docs.ropensci.org/gert/,\nhttps://ropensci.r-universe.dev/gert",
    "bug_reports": "https://github.com/r-lib/gert/issues",
    "repository": "",
    "exports": [
      [
        "git_add"
      ],
      [
        "git_ahead_behind"
      ],
      [
        "git_archive_zip"
      ],
      [
        "git_branch"
      ],
      [
        "git_branch_checkout"
      ],
      [
        "git_branch_create"
      ],
      [
        "git_branch_delete"
      ],
      [
        "git_branch_exists"
      ],
      [
        "git_branch_fast_forward"
      ],
      [
        "git_branch_list"
      ],
      [
        "git_branch_move"
      ],
      [
        "git_branch_set_upstream"
      ],
      [
        "git_checkout_pull_request"
      ],
      [
        "git_cherry_pick"
      ],
      [
        "git_clone"
      ],
      [
        "git_commit"
      ],
      [
        "git_commit_all"
      ],
      [
        "git_commit_descendant_of"
      ],
      [
        "git_commit_id"
      ],
      [
        "git_commit_info"
      ],
      [
        "git_commit_stats"
      ],
      [
        "git_config"
      ],
      [
        "git_config_global"
      ],
      [
        "git_config_global_set"
      ],
      [
        "git_config_set"
      ],
      [
        "git_conflicts"
      ],
      [
        "git_diff"
      ],
      [
        "git_diff_patch"
      ],
      [
        "git_fetch"
      ],
      [
        "git_fetch_pull_requests"
      ],
      [
        "git_find"
      ],
      [
        "git_ignore_path_is_ignored"
      ],
      [
        "git_info"
      ],
      [
        "git_init"
      ],
      [
        "git_log"
      ],
      [
        "git_ls"
      ],
      [
        "git_merge"
      ],
      [
        "git_merge_abort"
      ],
      [
        "git_merge_analysis"
      ],
      [
        "git_merge_find_base"
      ],
      [
        "git_merge_stage_only"
      ],
      [
        "git_open"
      ],
      [
        "git_pull"
      ],
      [
        "git_push"
      ],
      [
        "git_rebase_commit"
      ],
      [
        "git_rebase_list"
      ],
      [
        "git_remote_add"
      ],
      [
        "git_remote_info"
      ],
      [
        "git_remote_list"
      ],
      [
        "git_remote_ls"
      ],
      [
        "git_remote_refspecs"
      ],
      [
        "git_remote_remove"
      ],
      [
        "git_remote_set_pushurl"
      ],
      [
        "git_remote_set_url"
      ],
      [
        "git_reset_hard"
      ],
      [
        "git_reset_mixed"
      ],
      [
        "git_reset_soft"
      ],
      [
        "git_rm"
      ],
      [
        "git_signature"
      ],
      [
        "git_signature_default"
      ],
      [
        "git_signature_parse"
      ],
      [
        "git_stash_drop"
      ],
      [
        "git_stash_list"
      ],
      [
        "git_stash_pop"
      ],
      [
        "git_stash_save"
      ],
      [
        "git_stat_files"
      ],
      [
        "git_status"
      ],
      [
        "git_submodule_add"
      ],
      [
        "git_submodule_fetch"
      ],
      [
        "git_submodule_info"
      ],
      [
        "git_submodule_init"
      ],
      [
        "git_submodule_list"
      ],
      [
        "git_submodule_set_to"
      ],
      [
        "git_tag_create"
      ],
      [
        "git_tag_delete"
      ],
      [
        "git_tag_list"
      ],
      [
        "git_tag_push"
      ],
      [
        "libgit2_config"
      ],
      [
        "user_is_configured"
      ]
    ],
    "topics": [],
    "score": 14.687,
    "stars": 156,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "gert Simple Git Client for R Simple git client for R based on 'libgit2'\n<https://libgit2.org> with support for SSH and HTTPS remotes.\nAll functions in 'gert' use basic R data types (such as vectors\nand data-frames) for their arguments and return values. User\ncredentials are shared with command line 'git' through the\ngit-credential store and ssh keys stored on disk or ssh-agent. git_add git_ahead_behind git_archive_zip git_branch git_branch_checkout git_branch_create git_branch_delete git_branch_exists git_branch_fast_forward git_branch_list git_branch_move git_branch_set_upstream git_checkout_pull_request git_cherry_pick git_clone git_commit git_commit_all git_commit_descendant_of git_commit_id git_commit_info git_commit_stats git_config git_config_global git_config_global_set git_config_set git_conflicts git_diff git_diff_patch git_fetch git_fetch_pull_requests git_find git_ignore_path_is_ignored git_info git_init git_log git_ls git_merge git_merge_abort git_merge_analysis git_merge_find_base git_merge_stage_only git_open git_pull git_push git_rebase_commit git_rebase_list git_remote_add git_remote_info git_remote_list git_remote_ls git_remote_refspecs git_remote_remove git_remote_set_pushurl git_remote_set_url git_reset_hard git_reset_mixed git_reset_soft git_rm git_signature git_signature_default git_signature_parse git_stash_drop git_stash_list git_stash_pop git_stash_save git_stat_files git_status git_submodule_add git_submodule_fetch git_submodule_info git_submodule_init git_submodule_list git_submodule_set_to git_tag_create git_tag_delete git_tag_list git_tag_push libgit2_config user_is_configured "
  },
  {
    "id": 1234,
    "package_name": "slider",
    "title": "Sliding Window Functions",
    "description": "Provides type-stable rolling window functions over any R\ndata type. Cumulative and expanding windows are also supported.\nFor more advanced usage, an index can be used as a secondary\nvector that defines how sliding windows are to be created.",
    "version": "0.3.3.9000",
    "maintainer": "Davis Vaughan <davis@posit.co>",
    "author": "Davis Vaughan [aut, cre],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/r-lib/slider, https://slider.r-lib.org",
    "bug_reports": "https://github.com/r-lib/slider/issues",
    "repository": "",
    "exports": [
      [
        "block"
      ],
      [
        "hop"
      ],
      [
        "hop_index"
      ],
      [
        "hop_index_vec"
      ],
      [
        "hop_index2"
      ],
      [
        "hop_index2_vec"
      ],
      [
        "hop_vec"
      ],
      [
        "hop2"
      ],
      [
        "hop2_vec"
      ],
      [
        "phop"
      ],
      [
        "phop_index"
      ],
      [
        "phop_index_vec"
      ],
      [
        "phop_vec"
      ],
      [
        "pslide"
      ],
      [
        "pslide_chr"
      ],
      [
        "pslide_dbl"
      ],
      [
        "pslide_dfc"
      ],
      [
        "pslide_dfr"
      ],
      [
        "pslide_index"
      ],
      [
        "pslide_index_chr"
      ],
      [
        "pslide_index_dbl"
      ],
      [
        "pslide_index_dfc"
      ],
      [
        "pslide_index_dfr"
      ],
      [
        "pslide_index_int"
      ],
      [
        "pslide_index_lgl"
      ],
      [
        "pslide_index_vec"
      ],
      [
        "pslide_int"
      ],
      [
        "pslide_lgl"
      ],
      [
        "pslide_period"
      ],
      [
        "pslide_period_chr"
      ],
      [
        "pslide_period_dbl"
      ],
      [
        "pslide_period_dfc"
      ],
      [
        "pslide_period_dfr"
      ],
      [
        "pslide_period_int"
      ],
      [
        "pslide_period_lgl"
      ],
      [
        "pslide_period_vec"
      ],
      [
        "pslide_vec"
      ],
      [
        "slide"
      ],
      [
        "slide_all"
      ],
      [
        "slide_any"
      ],
      [
        "slide_chr"
      ],
      [
        "slide_dbl"
      ],
      [
        "slide_dfc"
      ],
      [
        "slide_dfr"
      ],
      [
        "slide_index"
      ],
      [
        "slide_index_all"
      ],
      [
        "slide_index_any"
      ],
      [
        "slide_index_chr"
      ],
      [
        "slide_index_dbl"
      ],
      [
        "slide_index_dfc"
      ],
      [
        "slide_index_dfr"
      ],
      [
        "slide_index_int"
      ],
      [
        "slide_index_lgl"
      ],
      [
        "slide_index_max"
      ],
      [
        "slide_index_mean"
      ],
      [
        "slide_index_min"
      ],
      [
        "slide_index_prod"
      ],
      [
        "slide_index_sum"
      ],
      [
        "slide_index_vec"
      ],
      [
        "slide_index2"
      ],
      [
        "slide_index2_chr"
      ],
      [
        "slide_index2_dbl"
      ],
      [
        "slide_index2_dfc"
      ],
      [
        "slide_index2_dfr"
      ],
      [
        "slide_index2_int"
      ],
      [
        "slide_index2_lgl"
      ],
      [
        "slide_index2_vec"
      ],
      [
        "slide_int"
      ],
      [
        "slide_lgl"
      ],
      [
        "slide_max"
      ],
      [
        "slide_mean"
      ],
      [
        "slide_min"
      ],
      [
        "slide_period"
      ],
      [
        "slide_period_chr"
      ],
      [
        "slide_period_dbl"
      ],
      [
        "slide_period_dfc"
      ],
      [
        "slide_period_dfr"
      ],
      [
        "slide_period_int"
      ],
      [
        "slide_period_lgl"
      ],
      [
        "slide_period_vec"
      ],
      [
        "slide_period2"
      ],
      [
        "slide_period2_chr"
      ],
      [
        "slide_period2_dbl"
      ],
      [
        "slide_period2_dfc"
      ],
      [
        "slide_period2_dfr"
      ],
      [
        "slide_period2_int"
      ],
      [
        "slide_period2_lgl"
      ],
      [
        "slide_period2_vec"
      ],
      [
        "slide_prod"
      ],
      [
        "slide_sum"
      ],
      [
        "slide_vec"
      ],
      [
        "slide2"
      ],
      [
        "slide2_chr"
      ],
      [
        "slide2_dbl"
      ],
      [
        "slide2_dfc"
      ],
      [
        "slide2_dfr"
      ],
      [
        "slide2_int"
      ],
      [
        "slide2_lgl"
      ],
      [
        "slide2_vec"
      ],
      [
        "slider_minus"
      ],
      [
        "slider_plus"
      ]
    ],
    "topics": [],
    "score": 14.6567,
    "stars": 310,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "slider Sliding Window Functions Provides type-stable rolling window functions over any R\ndata type. Cumulative and expanding windows are also supported.\nFor more advanced usage, an index can be used as a secondary\nvector that defines how sliding windows are to be created. block hop hop_index hop_index_vec hop_index2 hop_index2_vec hop_vec hop2 hop2_vec phop phop_index phop_index_vec phop_vec pslide pslide_chr pslide_dbl pslide_dfc pslide_dfr pslide_index pslide_index_chr pslide_index_dbl pslide_index_dfc pslide_index_dfr pslide_index_int pslide_index_lgl pslide_index_vec pslide_int pslide_lgl pslide_period pslide_period_chr pslide_period_dbl pslide_period_dfc pslide_period_dfr pslide_period_int pslide_period_lgl pslide_period_vec pslide_vec slide slide_all slide_any slide_chr slide_dbl slide_dfc slide_dfr slide_index slide_index_all slide_index_any slide_index_chr slide_index_dbl slide_index_dfc slide_index_dfr slide_index_int slide_index_lgl slide_index_max slide_index_mean slide_index_min slide_index_prod slide_index_sum slide_index_vec slide_index2 slide_index2_chr slide_index2_dbl slide_index2_dfc slide_index2_dfr slide_index2_int slide_index2_lgl slide_index2_vec slide_int slide_lgl slide_max slide_mean slide_min slide_period slide_period_chr slide_period_dbl slide_period_dfc slide_period_dfr slide_period_int slide_period_lgl slide_period_vec slide_period2 slide_period2_chr slide_period2_dbl slide_period2_dfc slide_period2_dfr slide_period2_int slide_period2_lgl slide_period2_vec slide_prod slide_sum slide_vec slide2 slide2_chr slide2_dbl slide2_dfc slide2_dfr slide2_int slide2_lgl slide2_vec slider_minus slider_plus "
  },
  {
    "id": 954,
    "package_name": "pdftools",
    "title": "Text Extraction, Rendering and Converting of PDF Documents",
    "description": "Utilities based on 'libpoppler'\n<https://poppler.freedesktop.org> for extracting text, fonts,\nattachments and metadata from a PDF file. Also supports high\nquality rendering of PDF documents into PNG, JPEG, TIFF format,\nor into raw bitmap vectors for further processing in R.",
    "version": "3.7.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://ropensci.r-universe.dev/pdftools,\nhttps://docs.ropensci.org/pdftools/",
    "bug_reports": "https://github.com/ropensci/pdftools/issues",
    "repository": "",
    "exports": [
      [
        "pdf_attachments"
      ],
      [
        "pdf_combine"
      ],
      [
        "pdf_compress"
      ],
      [
        "pdf_convert"
      ],
      [
        "pdf_data"
      ],
      [
        "pdf_fonts"
      ],
      [
        "pdf_info"
      ],
      [
        "pdf_length"
      ],
      [
        "pdf_ocr_data"
      ],
      [
        "pdf_ocr_text"
      ],
      [
        "pdf_pagesize"
      ],
      [
        "pdf_render_page"
      ],
      [
        "pdf_split"
      ],
      [
        "pdf_subset"
      ],
      [
        "pdf_text"
      ],
      [
        "pdf_toc"
      ],
      [
        "poppler_config"
      ]
    ],
    "topics": [
      [
        "pdf-files"
      ],
      [
        "pdf-format"
      ],
      [
        "pdftools"
      ],
      [
        "poppler"
      ],
      [
        "poppler-library"
      ],
      [
        "text-extraction"
      ],
      [
        "cpp"
      ]
    ],
    "score": 14.1436,
    "stars": 544,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pdftools Text Extraction, Rendering and Converting of PDF Documents Utilities based on 'libpoppler'\n<https://poppler.freedesktop.org> for extracting text, fonts,\nattachments and metadata from a PDF file. Also supports high\nquality rendering of PDF documents into PNG, JPEG, TIFF format,\nor into raw bitmap vectors for further processing in R. pdf_attachments pdf_combine pdf_compress pdf_convert pdf_data pdf_fonts pdf_info pdf_length pdf_ocr_data pdf_ocr_text pdf_pagesize pdf_render_page pdf_split pdf_subset pdf_text pdf_toc poppler_config pdf-files pdf-format pdftools poppler poppler-library text-extraction cpp"
  },
  {
    "id": 892,
    "package_name": "numDeriv",
    "title": "Accurate Numerical Derivatives",
    "description": "Methods for calculating (usually) accurate numerical first\nand second order derivatives. Accurate calculations are done\nusing 'Richardson''s' extrapolation or, when applicable, a\ncomplex step derivative is available. A simple difference\nmethod is also provided. Simple difference is (usually) less\naccurate but is much quicker than 'Richardson''s' extrapolation\nand provides a useful cross-check. Methods are provided for\nreal scalar and vector valued functions.",
    "version": "2022.9-1",
    "maintainer": "Paul Gilbert <pgilbert.ttv9z@ncf.ca>",
    "author": "Paul Gilbert and Ravi Varadhan",
    "url": "http://optimizer.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "genD"
      ],
      [
        "grad"
      ],
      [
        "hessian"
      ],
      [
        "jacobian"
      ]
    ],
    "topics": [],
    "score": 13.7717,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "numDeriv Accurate Numerical Derivatives Methods for calculating (usually) accurate numerical first\nand second order derivatives. Accurate calculations are done\nusing 'Richardson''s' extrapolation or, when applicable, a\ncomplex step derivative is available. A simple difference\nmethod is also provided. Simple difference is (usually) less\naccurate but is much quicker than 'Richardson''s' extrapolation\nand provides a useful cross-check. Methods are provided for\nreal scalar and vector valued functions. genD grad hessian jacobian "
  },
  {
    "id": 302,
    "package_name": "blob",
    "title": "A Simple S3 Class for Representing Vectors of Binary Data\n('BLOBS')",
    "description": "R's raw vector is useful for storing a single binary\nobject. What if you want to put a vector of them in a data\nframe? The 'blob' package provides the blob object, a list of\nraw vectors, suitable for use as a column in data frame.",
    "version": "1.2.4.9019",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Hadley Wickham [aut],\nKirill M\u00fcller [cre],\nRStudio [cph, fnd]",
    "url": "https://blob.tidyverse.org, https://github.com/tidyverse/blob",
    "bug_reports": "https://github.com/tidyverse/blob/issues",
    "repository": "",
    "exports": [
      [
        "as_blob"
      ],
      [
        "as.blob"
      ],
      [
        "blob"
      ],
      [
        "is_blob"
      ],
      [
        "new_blob"
      ],
      [
        "validate_blob"
      ],
      [
        "vec_cast.blob"
      ],
      [
        "vec_ptype2.blob"
      ]
    ],
    "topics": [
      [
        "database"
      ]
    ],
    "score": 13.6862,
    "stars": 47,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "blob A Simple S3 Class for Representing Vectors of Binary Data\n('BLOBS') R's raw vector is useful for storing a single binary\nobject. What if you want to put a vector of them in a data\nframe? The 'blob' package provides the blob object, a list of\nraw vectors, suitable for use as a column in data frame. as_blob as.blob blob is_blob new_blob validate_blob vec_cast.blob vec_ptype2.blob database"
  },
  {
    "id": 1103,
    "package_name": "rgbif",
    "title": "Interface to the Global Biodiversity Information Facility API",
    "description": "A programmatic interface to the Web Service methods\nprovided by the Global Biodiversity Information Facility (GBIF;\n<https://www.gbif.org/developer/summary>). GBIF is a database\nof species occurrence records from sources all over the globe.\nrgbif includes functions for searching for taxonomic names,\nretrieving information on data providers, getting species\noccurrence records, getting counts of occurrence records, and\nusing the GBIF tile map service to make rasters summarizing\nhuge amounts of data.",
    "version": "3.8.4.2",
    "maintainer": "John Waller <jwaller@gbif.org>",
    "author": "Scott Chamberlain [aut] (0000-0003-1444-9135),\nDamiano Oldoni [aut] (0000-0003-3445-7562),\nVijay Barve [ctb] (0000-0002-4852-2567),\nPeter Desmet [ctb] (0000-0002-8442-8025),\nLaurens Geffert [ctb],\nDan Mcglinn [ctb] (0000-0003-2359-3526),\nKarthik Ram [ctb] (0000-0002-0233-1757),\nrOpenSci [fnd] (019jywm96),\nJohn Waller [aut, cre] (0000-0002-7302-5976)",
    "url": "https://github.com/ropensci/rgbif (devel),\nhttps://docs.ropensci.org/rgbif/ (documentation)",
    "bug_reports": "https://github.com/ropensci/rgbif/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "as.download"
      ],
      [
        "blanktheme"
      ],
      [
        "check_wkt"
      ],
      [
        "collection_export"
      ],
      [
        "collection_search"
      ],
      [
        "count_facet"
      ],
      [
        "dataset"
      ],
      [
        "dataset_comment"
      ],
      [
        "dataset_constituents"
      ],
      [
        "dataset_contact"
      ],
      [
        "dataset_doi"
      ],
      [
        "dataset_duplicate"
      ],
      [
        "dataset_endpoint"
      ],
      [
        "dataset_export"
      ],
      [
        "dataset_get"
      ],
      [
        "dataset_gridded"
      ],
      [
        "dataset_identifier"
      ],
      [
        "dataset_machinetag"
      ],
      [
        "dataset_metrics"
      ],
      [
        "dataset_networks"
      ],
      [
        "dataset_noendpoint"
      ],
      [
        "dataset_process"
      ],
      [
        "dataset_search"
      ],
      [
        "dataset_suggest"
      ],
      [
        "dataset_tag"
      ],
      [
        "datasets"
      ],
      [
        "derived_dataset"
      ],
      [
        "derived_dataset_prep"
      ],
      [
        "DownReq"
      ],
      [
        "elevation"
      ],
      [
        "enumeration"
      ],
      [
        "enumeration_country"
      ],
      [
        "gbif_bbox2wkt"
      ],
      [
        "gbif_citation"
      ],
      [
        "gbif_geocode"
      ],
      [
        "gbif_issues"
      ],
      [
        "gbif_issues_lookup"
      ],
      [
        "gbif_names"
      ],
      [
        "gbif_oai_get_records"
      ],
      [
        "gbif_oai_identify"
      ],
      [
        "gbif_oai_list_identifiers"
      ],
      [
        "gbif_oai_list_metadataformats"
      ],
      [
        "gbif_oai_list_records"
      ],
      [
        "gbif_oai_list_sets"
      ],
      [
        "gbif_photos"
      ],
      [
        "gbif_wkt2bbox"
      ],
      [
        "GbifQueue"
      ],
      [
        "installation_comment"
      ],
      [
        "installation_contact"
      ],
      [
        "installation_dataset"
      ],
      [
        "installation_endpoint"
      ],
      [
        "installation_identifier"
      ],
      [
        "installation_machinetag"
      ],
      [
        "installation_search"
      ],
      [
        "installation_tag"
      ],
      [
        "installations"
      ],
      [
        "institution_export"
      ],
      [
        "institution_search"
      ],
      [
        "lit_count"
      ],
      [
        "lit_export"
      ],
      [
        "lit_search"
      ],
      [
        "map_fetch"
      ],
      [
        "mvt_fetch"
      ],
      [
        "name_backbone"
      ],
      [
        "name_backbone_checklist"
      ],
      [
        "name_backbone_verbose"
      ],
      [
        "name_issues"
      ],
      [
        "name_lookup"
      ],
      [
        "name_parse"
      ],
      [
        "name_suggest"
      ],
      [
        "name_usage"
      ],
      [
        "network"
      ],
      [
        "network_constituents"
      ],
      [
        "networks"
      ],
      [
        "nodes"
      ],
      [
        "occ_count"
      ],
      [
        "occ_count_basis_of_record"
      ],
      [
        "occ_count_country"
      ],
      [
        "occ_count_pub_country"
      ],
      [
        "occ_count_year"
      ],
      [
        "occ_data"
      ],
      [
        "occ_download"
      ],
      [
        "occ_download_cached"
      ],
      [
        "occ_download_cancel"
      ],
      [
        "occ_download_cancel_staged"
      ],
      [
        "occ_download_dataset_activity"
      ],
      [
        "occ_download_datasets"
      ],
      [
        "occ_download_describe"
      ],
      [
        "occ_download_doi"
      ],
      [
        "occ_download_get"
      ],
      [
        "occ_download_import"
      ],
      [
        "occ_download_list"
      ],
      [
        "occ_download_meta"
      ],
      [
        "occ_download_prep"
      ],
      [
        "occ_download_queue"
      ],
      [
        "occ_download_sql"
      ],
      [
        "occ_download_sql_prep"
      ],
      [
        "occ_download_sql_validate"
      ],
      [
        "occ_download_wait"
      ],
      [
        "occ_facet"
      ],
      [
        "occ_get"
      ],
      [
        "occ_get_verbatim"
      ],
      [
        "occ_issues"
      ],
      [
        "occ_metadata"
      ],
      [
        "occ_search"
      ],
      [
        "occ_spellcheck"
      ],
      [
        "organizations"
      ],
      [
        "parsenames"
      ],
      [
        "pred"
      ],
      [
        "pred_and"
      ],
      [
        "pred_default"
      ],
      [
        "pred_gt"
      ],
      [
        "pred_gte"
      ],
      [
        "pred_in"
      ],
      [
        "pred_isnull"
      ],
      [
        "pred_like"
      ],
      [
        "pred_lt"
      ],
      [
        "pred_lte"
      ],
      [
        "pred_not"
      ],
      [
        "pred_notnull"
      ],
      [
        "pred_or"
      ],
      [
        "pred_within"
      ],
      [
        "rgb_country_codes"
      ],
      [
        "suggestfields"
      ],
      [
        "taxrank"
      ],
      [
        "wkt_parse"
      ]
    ],
    "topics": [
      [
        "gbif"
      ],
      [
        "specimens"
      ],
      [
        "api"
      ],
      [
        "web-services"
      ],
      [
        "occurrences"
      ],
      [
        "species"
      ],
      [
        "taxonomy"
      ],
      [
        "biodiversity"
      ],
      [
        "data"
      ],
      [
        "lifewatch"
      ],
      [
        "oscibio"
      ],
      [
        "spocc"
      ]
    ],
    "score": 13.6363,
    "stars": 171,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rgbif Interface to the Global Biodiversity Information Facility API A programmatic interface to the Web Service methods\nprovided by the Global Biodiversity Information Facility (GBIF;\n<https://www.gbif.org/developer/summary>). GBIF is a database\nof species occurrence records from sources all over the globe.\nrgbif includes functions for searching for taxonomic names,\nretrieving information on data providers, getting species\noccurrence records, getting counts of occurrence records, and\nusing the GBIF tile map service to make rasters summarizing\nhuge amounts of data. %>% as.download blanktheme check_wkt collection_export collection_search count_facet dataset dataset_comment dataset_constituents dataset_contact dataset_doi dataset_duplicate dataset_endpoint dataset_export dataset_get dataset_gridded dataset_identifier dataset_machinetag dataset_metrics dataset_networks dataset_noendpoint dataset_process dataset_search dataset_suggest dataset_tag datasets derived_dataset derived_dataset_prep DownReq elevation enumeration enumeration_country gbif_bbox2wkt gbif_citation gbif_geocode gbif_issues gbif_issues_lookup gbif_names gbif_oai_get_records gbif_oai_identify gbif_oai_list_identifiers gbif_oai_list_metadataformats gbif_oai_list_records gbif_oai_list_sets gbif_photos gbif_wkt2bbox GbifQueue installation_comment installation_contact installation_dataset installation_endpoint installation_identifier installation_machinetag installation_search installation_tag installations institution_export institution_search lit_count lit_export lit_search map_fetch mvt_fetch name_backbone name_backbone_checklist name_backbone_verbose name_issues name_lookup name_parse name_suggest name_usage network network_constituents networks nodes occ_count occ_count_basis_of_record occ_count_country occ_count_pub_country occ_count_year occ_data occ_download occ_download_cached occ_download_cancel occ_download_cancel_staged occ_download_dataset_activity occ_download_datasets occ_download_describe occ_download_doi occ_download_get occ_download_import occ_download_list occ_download_meta occ_download_prep occ_download_queue occ_download_sql occ_download_sql_prep occ_download_sql_validate occ_download_wait occ_facet occ_get occ_get_verbatim occ_issues occ_metadata occ_search occ_spellcheck organizations parsenames pred pred_and pred_default pred_gt pred_gte pred_in pred_isnull pred_like pred_lt pred_lte pred_not pred_notnull pred_or pred_within rgb_country_codes suggestfields taxrank wkt_parse gbif specimens api web-services occurrences species taxonomy biodiversity data lifewatch oscibio spocc"
  },
  {
    "id": 1251,
    "package_name": "sparsevctrs",
    "title": "Sparse Vectors for Use in Data Frames",
    "description": "Provides sparse vectors powered by ALTREP (Alternative\nRepresentations for R Objects) that behave like regular\nvectors, and can thus be used in data frames. Also provides\ntools to convert between sparse matrices and data frames with\nsparse columns and functions to interact with sparse vectors.",
    "version": "0.3.5.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nDavis Vaughan [ctb],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/r-lib/sparsevctrs,\nhttps://r-lib.github.io/sparsevctrs/",
    "bug_reports": "https://github.com/r-lib/sparsevctrs/issues",
    "repository": "",
    "exports": [
      [
        "as_sparse_character"
      ],
      [
        "as_sparse_double"
      ],
      [
        "as_sparse_integer"
      ],
      [
        "as_sparse_logical"
      ],
      [
        "coerce_to_sparse_data_frame"
      ],
      [
        "coerce_to_sparse_matrix"
      ],
      [
        "coerce_to_sparse_tibble"
      ],
      [
        "has_sparse_elements"
      ],
      [
        "is_sparse_character"
      ],
      [
        "is_sparse_double"
      ],
      [
        "is_sparse_integer"
      ],
      [
        "is_sparse_logical"
      ],
      [
        "is_sparse_numeric"
      ],
      [
        "is_sparse_vector"
      ],
      [
        "sparse_addition_scalar"
      ],
      [
        "sparse_character"
      ],
      [
        "sparse_default"
      ],
      [
        "sparse_division_scalar"
      ],
      [
        "sparse_double"
      ],
      [
        "sparse_dummy"
      ],
      [
        "sparse_integer"
      ],
      [
        "sparse_is_na"
      ],
      [
        "sparse_lag"
      ],
      [
        "sparse_logical"
      ],
      [
        "sparse_mean"
      ],
      [
        "sparse_median"
      ],
      [
        "sparse_multiplication"
      ],
      [
        "sparse_multiplication_scalar"
      ],
      [
        "sparse_positions"
      ],
      [
        "sparse_replace_na"
      ],
      [
        "sparse_sd"
      ],
      [
        "sparse_sqrt"
      ],
      [
        "sparse_subtraction_scalar"
      ],
      [
        "sparse_values"
      ],
      [
        "sparse_var"
      ],
      [
        "sparse_which_na"
      ],
      [
        "sparsity"
      ]
    ],
    "topics": [],
    "score": 12.5201,
    "stars": 22,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "sparsevctrs Sparse Vectors for Use in Data Frames Provides sparse vectors powered by ALTREP (Alternative\nRepresentations for R Objects) that behave like regular\nvectors, and can thus be used in data frames. Also provides\ntools to convert between sparse matrices and data frames with\nsparse columns and functions to interact with sparse vectors. as_sparse_character as_sparse_double as_sparse_integer as_sparse_logical coerce_to_sparse_data_frame coerce_to_sparse_matrix coerce_to_sparse_tibble has_sparse_elements is_sparse_character is_sparse_double is_sparse_integer is_sparse_logical is_sparse_numeric is_sparse_vector sparse_addition_scalar sparse_character sparse_default sparse_division_scalar sparse_double sparse_dummy sparse_integer sparse_is_na sparse_lag sparse_logical sparse_mean sparse_median sparse_multiplication sparse_multiplication_scalar sparse_positions sparse_replace_na sparse_sd sparse_sqrt sparse_subtraction_scalar sparse_values sparse_var sparse_which_na sparsity "
  },
  {
    "id": 1161,
    "package_name": "rsvg",
    "title": "Render SVG Images into PDF, PNG, (Encapsulated) PostScript, or\nBitmap Arrays",
    "description": "Renders vector-based svg images into high-quality\ncustom-size bitmap arrays using 'librsvg2'. The resulting\nbitmap can be written to e.g. png, jpeg or webp format. In\naddition, the package can convert images directly to various\nformats such as pdf or postscript.",
    "version": "2.7.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nSalim Br\u00fcggemann [ctb] (ORCID: <https://orcid.org/0000-0002-5329-5987>)",
    "url": "https://docs.ropensci.org/rsvg/\nhttps://ropensci.r-universe.dev/rsvg",
    "bug_reports": "https://github.com/ropensci/rsvg/issues",
    "repository": "",
    "exports": [
      [
        "librsvg_version"
      ],
      [
        "rsvg"
      ],
      [
        "rsvg_eps"
      ],
      [
        "rsvg_nativeraster"
      ],
      [
        "rsvg_pdf"
      ],
      [
        "rsvg_png"
      ],
      [
        "rsvg_ps"
      ],
      [
        "rsvg_raw"
      ],
      [
        "rsvg_svg"
      ],
      [
        "rsvg_webp"
      ]
    ],
    "topics": [
      [
        "librsvg"
      ],
      [
        "glib"
      ],
      [
        "cairo"
      ]
    ],
    "score": 12.172,
    "stars": 98,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rsvg Render SVG Images into PDF, PNG, (Encapsulated) PostScript, or\nBitmap Arrays Renders vector-based svg images into high-quality\ncustom-size bitmap arrays using 'librsvg2'. The resulting\nbitmap can be written to e.g. png, jpeg or webp format. In\naddition, the package can convert images directly to various\nformats such as pdf or postscript. librsvg_version rsvg rsvg_eps rsvg_nativeraster rsvg_pdf rsvg_png rsvg_ps rsvg_raw rsvg_svg rsvg_webp librsvg glib cairo"
  },
  {
    "id": 607,
    "package_name": "geodata",
    "title": "Access Geographic Data",
    "description": "Functions for downloading of geographic data for use in\nspatial analysis and mapping. The package facilitates access to\nclimate, crops, elevation, land use, soil, species occurrence,\naccessibility, administrative boundaries and other data.",
    "version": "0.6-7",
    "maintainer": "Robert J. Hijmans <r.hijmans@gmail.com>",
    "author": "Robert J. Hijmans [cre, aut] (ORCID:\n<https://orcid.org/0000-0001-5872-2872>),\nM\u00e1rcia Barbosa [ctb] (ORCID: <https://orcid.org/0000-0001-8972-7713>),\nAniruddha Ghosh [ctb],\nAlex Mandel [ctb]",
    "url": "https://rspatial.github.io/geodata/",
    "bug_reports": "https://github.com/rspatial/geodata/issues/",
    "repository": "",
    "exports": [
      [
        "bio_oracle"
      ],
      [
        "cmip6_tile"
      ],
      [
        "cmip6_world"
      ],
      [
        "country_codes"
      ],
      [
        "crop_calendar_sacks"
      ],
      [
        "crop_monfreda"
      ],
      [
        "crop_spam"
      ],
      [
        "cropland"
      ],
      [
        "elevation_30s"
      ],
      [
        "elevation_3s"
      ],
      [
        "elevation_global"
      ],
      [
        "footprint"
      ],
      [
        "gadm"
      ],
      [
        "geodata_path"
      ],
      [
        "landcover"
      ],
      [
        "monfredaCrops"
      ],
      [
        "osm"
      ],
      [
        "population"
      ],
      [
        "rice_calendar"
      ],
      [
        "sacksCrops"
      ],
      [
        "soil_af"
      ],
      [
        "soil_af_elements"
      ],
      [
        "soil_af_isda"
      ],
      [
        "soil_af_isda_vsi"
      ],
      [
        "soil_af_water"
      ],
      [
        "soil_world"
      ],
      [
        "soil_world_vsi"
      ],
      [
        "sp_genus"
      ],
      [
        "sp_occurrence"
      ],
      [
        "sp_occurrence_split"
      ],
      [
        "spamCrops"
      ],
      [
        "travel_time"
      ],
      [
        "world"
      ],
      [
        "worldclim_country"
      ],
      [
        "worldclim_global"
      ],
      [
        "worldclim_tile"
      ]
    ],
    "topics": [
      [
        "agriculture"
      ],
      [
        "climate"
      ],
      [
        "data"
      ],
      [
        "ecology"
      ],
      [
        "soil"
      ],
      [
        "spatial"
      ]
    ],
    "score": 11.5301,
    "stars": 184,
    "primary_category": "spatial",
    "source_universe": "rspatial",
    "search_text": "geodata Access Geographic Data Functions for downloading of geographic data for use in\nspatial analysis and mapping. The package facilitates access to\nclimate, crops, elevation, land use, soil, species occurrence,\naccessibility, administrative boundaries and other data. bio_oracle cmip6_tile cmip6_world country_codes crop_calendar_sacks crop_monfreda crop_spam cropland elevation_30s elevation_3s elevation_global footprint gadm geodata_path landcover monfredaCrops osm population rice_calendar sacksCrops soil_af soil_af_elements soil_af_isda soil_af_isda_vsi soil_af_water soil_world soil_world_vsi sp_genus sp_occurrence sp_occurrence_split spamCrops travel_time world worldclim_country worldclim_global worldclim_tile agriculture climate data ecology soil spatial"
  },
  {
    "id": 610,
    "package_name": "geojsonio",
    "title": "Convert Data from and to 'GeoJSON' or 'TopoJSON'",
    "description": "Convert data to 'GeoJSON' or 'TopoJSON' from various R\nclasses, including vectors, lists, data frames, shape files,\nand spatial classes.  'geojsonio' does not aim to replace\npackages like 'sp', 'rgdal', 'rgeos', but rather aims to be a\nhigh level client to simplify conversions of data from and to\n'GeoJSON' and 'TopoJSON'.",
    "version": "0.11.3.9000",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Scott Chamberlain [aut],\nAndy Teucher [aut],\nMichael Mahoney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2402-304X>)",
    "url": "https://github.com/ropensci/geojsonio,\nhttps://docs.ropensci.org/geojsonio/",
    "bug_reports": "https://github.com/ropensci/geojsonio/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "as.json"
      ],
      [
        "as.location"
      ],
      [
        "bounds"
      ],
      [
        "centroid"
      ],
      [
        "file_to_geojson"
      ],
      [
        "geo2topo"
      ],
      [
        "geojson_atomize"
      ],
      [
        "geojson_json"
      ],
      [
        "geojson_list"
      ],
      [
        "geojson_read"
      ],
      [
        "geojson_sf"
      ],
      [
        "geojson_sp"
      ],
      [
        "geojson_style"
      ],
      [
        "geojson_write"
      ],
      [
        "map_gist"
      ],
      [
        "map_leaf"
      ],
      [
        "pretty"
      ],
      [
        "projections"
      ],
      [
        "topo2geo"
      ],
      [
        "topojson_json"
      ],
      [
        "topojson_list"
      ],
      [
        "topojson_read"
      ],
      [
        "topojson_write"
      ]
    ],
    "topics": [
      [
        "geojson"
      ],
      [
        "topojson"
      ],
      [
        "geospatial"
      ],
      [
        "conversion"
      ],
      [
        "data"
      ],
      [
        "input-output"
      ],
      [
        "io"
      ]
    ],
    "score": 11.1259,
    "stars": 154,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "geojsonio Convert Data from and to 'GeoJSON' or 'TopoJSON' Convert data to 'GeoJSON' or 'TopoJSON' from various R\nclasses, including vectors, lists, data frames, shape files,\nand spatial classes.  'geojsonio' does not aim to replace\npackages like 'sp', 'rgdal', 'rgeos', but rather aims to be a\nhigh level client to simplify conversions of data from and to\n'GeoJSON' and 'TopoJSON'. %>% as.json as.location bounds centroid file_to_geojson geo2topo geojson_atomize geojson_json geojson_list geojson_read geojson_sf geojson_sp geojson_style geojson_write map_gist map_leaf pretty projections topo2geo topojson_json topojson_list topojson_read topojson_write geojson topojson geospatial conversion data input-output io"
  },
  {
    "id": 1125,
    "package_name": "rnaturalearthdata",
    "title": "World Vector Map Data from Natural Earth Used in 'rnaturalearth'",
    "description": "Vector map data from <https://www.naturalearthdata.com/>.\nAccess functions are provided in the accompanying package\n'rnaturalearth'.",
    "version": "1.0.0.9000",
    "maintainer": "Philippe Massicotte <pmassicotte@hotmail.com>",
    "author": "Andy South [aut] (ORCID: <https://orcid.org/0000-0003-4051-6135>),\nSchramm Michael [aut],\nPhilippe Massicotte [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5919-4116>)",
    "url": "https://docs.ropensci.org/rnaturalearthdata/,\nhttps://github.com/ropensci/rnaturalearthdata",
    "bug_reports": "https://github.com/ropensci/rnaturalearthdata/issues",
    "repository": "",
    "exports": [],
    "topics": [],
    "score": 10.8633,
    "stars": 13,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rnaturalearthdata World Vector Map Data from Natural Earth Used in 'rnaturalearth' Vector map data from <https://www.naturalearthdata.com/>.\nAccess functions are provided in the accompanying package\n'rnaturalearth'.  "
  },
  {
    "id": 1199,
    "package_name": "sftime",
    "title": "Classes and Methods for Simple Feature Objects that Have a Time\nColumn",
    "description": "Classes and methods for spatial objects that have a\nregistered time column, in particular for irregular\nspatiotemporal data. The time column can be of any type, but\nneeds to be ordinal. Regularly laid out spatiotemporal data\n(vector or raster data cubes) are handled by package 'stars'.",
    "version": "0.3.1.9000",
    "maintainer": "Henning Teickner <henning.teickner@uni-muenster.de>",
    "author": "Henning Teickner [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-3993-1182>),\nEdzer Pebesma [aut, cph] (ORCID:\n<https://orcid.org/0000-0001-8049-7069>),\nBenedikt Graeler [aut, cph] (ORCID:\n<https://orcid.org/0000-0001-5443-4304>)",
    "url": "https://r-spatial.github.io/sftime/,\nhttps://github.com/r-spatial/sftime",
    "bug_reports": "https://github.com/r-spatial/sftime/issues/",
    "repository": "",
    "exports": [
      [
        "is_sortable"
      ],
      [
        "st_as_sftime"
      ],
      [
        "st_drop_time"
      ],
      [
        "st_set_time"
      ],
      [
        "st_sftime"
      ],
      [
        "st_time"
      ],
      [
        "st_time<-"
      ]
    ],
    "topics": [],
    "score": 10.6317,
    "stars": 50,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "sftime Classes and Methods for Simple Feature Objects that Have a Time\nColumn Classes and methods for spatial objects that have a\nregistered time column, in particular for irregular\nspatiotemporal data. The time column can be of any type, but\nneeds to be ordinal. Regularly laid out spatiotemporal data\n(vector or raster data cubes) are handled by package 'stars'. is_sortable st_as_sftime st_drop_time st_set_time st_sftime st_time st_time<- "
  },
  {
    "id": 1033,
    "package_name": "qgisprocess",
    "title": "Use 'QGIS' Processing Algorithms",
    "description": "Provides seamless access to the 'QGIS'\n(<https://qgis.org>) processing toolbox using the standalone\n'qgis_process' command-line utility.  Both native and\nthird-party (plugin) processing providers are supported.\nBeside referring data sources from file, also common objects\nfrom 'sf', 'terra' and 'stars' are supported. The native\nprocessing algorithms are documented by QGIS.org (2024)\n<https://docs.qgis.org/latest/en/docs/user_manual/processing_algs/>.",
    "version": "0.4.1.9000",
    "maintainer": "Floris Vanderhaeghe <floris.vanderhaeghe@inbo.be>",
    "author": "Dewey Dunnington [aut] (ORCID: <https://orcid.org/0000-0002-9415-4582>,\naffiliation: Voltron Data),\nFloris Vanderhaeghe [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6378-6229>, affiliation: Research\nInstitute for Nature and Forest (INBO)),\nJan Caha [aut] (ORCID: <https://orcid.org/0000-0003-0165-0606>),\nJannes Muenchow [aut] (ORCID: <https://orcid.org/0000-0001-7834-4717>),\nAntony Barja [ctb] (ORCID: <https://orcid.org/0000-0001-5921-2858>),\nRobin Lovelace [ctb] (ORCID: <https://orcid.org/0000-0001-5679-6536>),\nJakub Nowosad [ctb] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\nResearch Institute for Nature and Forest (INBO) [cph, fnd]\n(https://www.inbo.be/en/)",
    "url": "https://r-spatial.github.io/qgisprocess/,\nhttps://github.com/r-spatial/qgisprocess",
    "bug_reports": "https://github.com/r-spatial/qgisprocess/issues",
    "repository": "",
    "exports": [
      [
        "as_qgis_argument"
      ],
      [
        "has_qgis"
      ],
      [
        "qgis_algorithms"
      ],
      [
        "qgis_arguments"
      ],
      [
        "qgis_as_brick"
      ],
      [
        "qgis_as_raster"
      ],
      [
        "qgis_as_terra"
      ],
      [
        "qgis_clean_argument"
      ],
      [
        "qgis_clean_result"
      ],
      [
        "qgis_clean_tmp"
      ],
      [
        "qgis_configure"
      ],
      [
        "qgis_description"
      ],
      [
        "qgis_detect_macos"
      ],
      [
        "qgis_detect_macos_paths"
      ],
      [
        "qgis_detect_paths"
      ],
      [
        "qgis_detect_windows"
      ],
      [
        "qgis_detect_windows_paths"
      ],
      [
        "qgis_dict_input"
      ],
      [
        "qgis_disable_plugins"
      ],
      [
        "qgis_enable_plugins"
      ],
      [
        "qgis_extract_output"
      ],
      [
        "qgis_extract_output_by_class"
      ],
      [
        "qgis_extract_output_by_name"
      ],
      [
        "qgis_extract_output_by_position"
      ],
      [
        "qgis_function"
      ],
      [
        "qgis_get_argument_specs"
      ],
      [
        "qgis_get_description"
      ],
      [
        "qgis_get_output_specs"
      ],
      [
        "qgis_has_algorithm"
      ],
      [
        "qgis_has_plugin"
      ],
      [
        "qgis_has_provider"
      ],
      [
        "qgis_list_input"
      ],
      [
        "qgis_output"
      ],
      [
        "qgis_outputs"
      ],
      [
        "qgis_path"
      ],
      [
        "qgis_pipe"
      ],
      [
        "qgis_plugins"
      ],
      [
        "qgis_providers"
      ],
      [
        "qgis_result_args"
      ],
      [
        "qgis_result_clean"
      ],
      [
        "qgis_result_single"
      ],
      [
        "qgis_result_status"
      ],
      [
        "qgis_result_stderr"
      ],
      [
        "qgis_result_stdout"
      ],
      [
        "qgis_run"
      ],
      [
        "qgis_run_algorithm"
      ],
      [
        "qgis_run_algorithm_p"
      ],
      [
        "qgis_search_algorithms"
      ],
      [
        "qgis_show_help"
      ],
      [
        "qgis_tmp_base"
      ],
      [
        "qgis_tmp_clean"
      ],
      [
        "qgis_tmp_file"
      ],
      [
        "qgis_tmp_folder"
      ],
      [
        "qgis_tmp_raster"
      ],
      [
        "qgis_tmp_vector"
      ],
      [
        "qgis_unconfigure"
      ],
      [
        "qgis_use_json_input"
      ],
      [
        "qgis_use_json_output"
      ],
      [
        "qgis_using_json_input"
      ],
      [
        "qgis_using_json_output"
      ],
      [
        "qgis_version"
      ]
    ],
    "topics": [],
    "score": 9.2203,
    "stars": 214,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "qgisprocess Use 'QGIS' Processing Algorithms Provides seamless access to the 'QGIS'\n(<https://qgis.org>) processing toolbox using the standalone\n'qgis_process' command-line utility.  Both native and\nthird-party (plugin) processing providers are supported.\nBeside referring data sources from file, also common objects\nfrom 'sf', 'terra' and 'stars' are supported. The native\nprocessing algorithms are documented by QGIS.org (2024)\n<https://docs.qgis.org/latest/en/docs/user_manual/processing_algs/>. as_qgis_argument has_qgis qgis_algorithms qgis_arguments qgis_as_brick qgis_as_raster qgis_as_terra qgis_clean_argument qgis_clean_result qgis_clean_tmp qgis_configure qgis_description qgis_detect_macos qgis_detect_macos_paths qgis_detect_paths qgis_detect_windows qgis_detect_windows_paths qgis_dict_input qgis_disable_plugins qgis_enable_plugins qgis_extract_output qgis_extract_output_by_class qgis_extract_output_by_name qgis_extract_output_by_position qgis_function qgis_get_argument_specs qgis_get_description qgis_get_output_specs qgis_has_algorithm qgis_has_plugin qgis_has_provider qgis_list_input qgis_output qgis_outputs qgis_path qgis_pipe qgis_plugins qgis_providers qgis_result_args qgis_result_clean qgis_result_single qgis_result_status qgis_result_stderr qgis_result_stdout qgis_run qgis_run_algorithm qgis_run_algorithm_p qgis_search_algorithms qgis_show_help qgis_tmp_base qgis_tmp_clean qgis_tmp_file qgis_tmp_folder qgis_tmp_raster qgis_tmp_vector qgis_unconfigure qgis_use_json_input qgis_use_json_output qgis_using_json_input qgis_using_json_output qgis_version "
  },
  {
    "id": 1230,
    "package_name": "sits",
    "title": "Satellite Image Time Series Analysis for Earth Observation Data\nCubes",
    "description": "An end-to-end toolkit for land use and land cover\nclassification using big Earth observation data. Builds\nsatellite image data cubes from cloud collections. Supports\nvisualization methods for images and time series and smoothing\nfilters for dealing with noisy time series. Enables merging of\nmulti-source imagery (SAR, optical, DEM). Includes functions\nfor quality assessment of training samples using self-organized\nmaps and to reduce training samples imbalance. Provides machine\nlearning algorithms including support vector machines, random\nforests, extreme gradient boosting, multi-layer perceptrons,\ntemporal convolution neural networks, and temporal attention\nencoders. Performs efficient classification of big Earth\nobservation data cubes and includes functions for\npost-classification smoothing based on Bayesian inference.\nEnables best practices for estimating area and assessing\naccuracy of land change. Includes object-based spatio-temporal\nsegmentation for space-time OBIA. Minimum recommended\nrequirements: 16 GB RAM and 4 CPU dual-core.",
    "version": "1.5.3-1",
    "maintainer": "Gilberto Camara <gilberto.camara.inpe@gmail.com>",
    "author": "Rolf Simoes [aut],\nGilberto Camara [aut, cre, ths],\nFelipe Souza [aut],\nFelipe Carlos [aut],\nLorena Santos [ctb],\nCharlotte Pelletier [ctb],\nEstefania Pizarro [ctb],\nKarine Ferreira [ctb, ths],\nAlber Sanchez [ctb],\nAlexandre Assuncao [ctb],\nDaniel Falbel [ctb],\nGilberto Queiroz [ctb],\nJohannes Reiche [ctb],\nPedro Andrade [ctb],\nPedro Brito [ctb],\nRenato Assuncao [ctb],\nRicardo Cartaxo [ctb]",
    "url": "https://github.com/e-sensing/sits/,\nhttps://e-sensing.github.io/sitsbook/,\nhttps://e-sensing.github.io/sits/",
    "bug_reports": "https://github.com/e-sensing/sits/issues",
    "repository": "",
    "exports": [
      [
        "impute_linear"
      ],
      [
        "sits_accuracy"
      ],
      [
        "sits_accuracy_summary"
      ],
      [
        "sits_add_base_cube"
      ],
      [
        "sits_apply"
      ],
      [
        "sits_as_sf"
      ],
      [
        "sits_as_stars"
      ],
      [
        "sits_as_terra"
      ],
      [
        "sits_bands"
      ],
      [
        "sits_bands<-"
      ],
      [
        "sits_bbox"
      ],
      [
        "sits_classify"
      ],
      [
        "sits_clean"
      ],
      [
        "sits_cluster_clean"
      ],
      [
        "sits_cluster_dendro"
      ],
      [
        "sits_cluster_frequency"
      ],
      [
        "sits_colors"
      ],
      [
        "sits_colors_qgis"
      ],
      [
        "sits_colors_reset"
      ],
      [
        "sits_colors_set"
      ],
      [
        "sits_colors_show"
      ],
      [
        "sits_combine_predictions"
      ],
      [
        "sits_confidence_sampling"
      ],
      [
        "sits_config"
      ],
      [
        "sits_config_show"
      ],
      [
        "sits_config_user_file"
      ],
      [
        "sits_cube"
      ],
      [
        "sits_cube_copy"
      ],
      [
        "sits_factory_function"
      ],
      [
        "sits_filter"
      ],
      [
        "sits_formula_linear"
      ],
      [
        "sits_formula_logref"
      ],
      [
        "sits_geo_dist"
      ],
      [
        "sits_get_class"
      ],
      [
        "sits_get_data"
      ],
      [
        "sits_get_probs"
      ],
      [
        "sits_impute"
      ],
      [
        "sits_kfold_validate"
      ],
      [
        "sits_label_classification"
      ],
      [
        "sits_labels"
      ],
      [
        "sits_labels_summary"
      ],
      [
        "sits_labels<-"
      ],
      [
        "sits_lightgbm"
      ],
      [
        "sits_lighttae"
      ],
      [
        "sits_list_collections"
      ],
      [
        "sits_merge"
      ],
      [
        "sits_mgrs_to_roi"
      ],
      [
        "sits_mixture_model"
      ],
      [
        "sits_mlp"
      ],
      [
        "sits_model_export"
      ],
      [
        "sits_mosaic"
      ],
      [
        "sits_patterns"
      ],
      [
        "sits_pred_features"
      ],
      [
        "sits_pred_normalize"
      ],
      [
        "sits_pred_references"
      ],
      [
        "sits_pred_sample"
      ],
      [
        "sits_predictors"
      ],
      [
        "sits_reclassify"
      ],
      [
        "sits_reduce"
      ],
      [
        "sits_reduce_imbalance"
      ],
      [
        "sits_regularize"
      ],
      [
        "sits_resnet"
      ],
      [
        "sits_rfor"
      ],
      [
        "sits_roi_to_mgrs"
      ],
      [
        "sits_roi_to_tiles"
      ],
      [
        "sits_run_examples"
      ],
      [
        "sits_run_tests"
      ],
      [
        "sits_sample"
      ],
      [
        "sits_sampling_design"
      ],
      [
        "sits_segment"
      ],
      [
        "sits_select"
      ],
      [
        "sits_sgolay"
      ],
      [
        "sits_show_prediction"
      ],
      [
        "sits_slic"
      ],
      [
        "sits_smooth"
      ],
      [
        "sits_som_clean_samples"
      ],
      [
        "sits_som_evaluate_cluster"
      ],
      [
        "sits_som_map"
      ],
      [
        "sits_som_remove_samples"
      ],
      [
        "sits_stats"
      ],
      [
        "sits_stratified_sampling"
      ],
      [
        "sits_svm"
      ],
      [
        "sits_tae"
      ],
      [
        "sits_tempcnn"
      ],
      [
        "sits_texture"
      ],
      [
        "sits_tiles_to_roi"
      ],
      [
        "sits_timeline"
      ],
      [
        "sits_timeseries_to_csv"
      ],
      [
        "sits_to_csv"
      ],
      [
        "sits_to_xlsx"
      ],
      [
        "sits_train"
      ],
      [
        "sits_tuning"
      ],
      [
        "sits_tuning_hparams"
      ],
      [
        "sits_uncertainty"
      ],
      [
        "sits_uncertainty_sampling"
      ],
      [
        "sits_validate"
      ],
      [
        "sits_variance"
      ],
      [
        "sits_view"
      ],
      [
        "sits_whittaker"
      ],
      [
        "sits_xgboost"
      ]
    ],
    "topics": [
      [
        "big-earth-data"
      ],
      [
        "cbers"
      ],
      [
        "earth-observation"
      ],
      [
        "eo-datacubes"
      ],
      [
        "geospatial"
      ],
      [
        "image-time-series"
      ],
      [
        "land-cover-classification"
      ],
      [
        "landsat"
      ],
      [
        "planetary-computer"
      ],
      [
        "r-spatial"
      ],
      [
        "remote-sensing"
      ],
      [
        "rspatial"
      ],
      [
        "satellite-image-time-series"
      ],
      [
        "satellite-imagery"
      ],
      [
        "sentinel-2"
      ],
      [
        "stac-api"
      ],
      [
        "stac-catalog"
      ],
      [
        "openblas"
      ],
      [
        "cpp"
      ],
      [
        "openmp"
      ]
    ],
    "score": 9.1713,
    "stars": 521,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "sits Satellite Image Time Series Analysis for Earth Observation Data\nCubes An end-to-end toolkit for land use and land cover\nclassification using big Earth observation data. Builds\nsatellite image data cubes from cloud collections. Supports\nvisualization methods for images and time series and smoothing\nfilters for dealing with noisy time series. Enables merging of\nmulti-source imagery (SAR, optical, DEM). Includes functions\nfor quality assessment of training samples using self-organized\nmaps and to reduce training samples imbalance. Provides machine\nlearning algorithms including support vector machines, random\nforests, extreme gradient boosting, multi-layer perceptrons,\ntemporal convolution neural networks, and temporal attention\nencoders. Performs efficient classification of big Earth\nobservation data cubes and includes functions for\npost-classification smoothing based on Bayesian inference.\nEnables best practices for estimating area and assessing\naccuracy of land change. Includes object-based spatio-temporal\nsegmentation for space-time OBIA. Minimum recommended\nrequirements: 16 GB RAM and 4 CPU dual-core. impute_linear sits_accuracy sits_accuracy_summary sits_add_base_cube sits_apply sits_as_sf sits_as_stars sits_as_terra sits_bands sits_bands<- sits_bbox sits_classify sits_clean sits_cluster_clean sits_cluster_dendro sits_cluster_frequency sits_colors sits_colors_qgis sits_colors_reset sits_colors_set sits_colors_show sits_combine_predictions sits_confidence_sampling sits_config sits_config_show sits_config_user_file sits_cube sits_cube_copy sits_factory_function sits_filter sits_formula_linear sits_formula_logref sits_geo_dist sits_get_class sits_get_data sits_get_probs sits_impute sits_kfold_validate sits_label_classification sits_labels sits_labels_summary sits_labels<- sits_lightgbm sits_lighttae sits_list_collections sits_merge sits_mgrs_to_roi sits_mixture_model sits_mlp sits_model_export sits_mosaic sits_patterns sits_pred_features sits_pred_normalize sits_pred_references sits_pred_sample sits_predictors sits_reclassify sits_reduce sits_reduce_imbalance sits_regularize sits_resnet sits_rfor sits_roi_to_mgrs sits_roi_to_tiles sits_run_examples sits_run_tests sits_sample sits_sampling_design sits_segment sits_select sits_sgolay sits_show_prediction sits_slic sits_smooth sits_som_clean_samples sits_som_evaluate_cluster sits_som_map sits_som_remove_samples sits_stats sits_stratified_sampling sits_svm sits_tae sits_tempcnn sits_texture sits_tiles_to_roi sits_timeline sits_timeseries_to_csv sits_to_csv sits_to_xlsx sits_train sits_tuning sits_tuning_hparams sits_uncertainty sits_uncertainty_sampling sits_validate sits_variance sits_view sits_whittaker sits_xgboost big-earth-data cbers earth-observation eo-datacubes geospatial image-time-series land-cover-classification landsat planetary-computer r-spatial remote-sensing rspatial satellite-image-time-series satellite-imagery sentinel-2 stac-api stac-catalog openblas cpp openmp"
  },
  {
    "id": 153,
    "package_name": "RSAGA",
    "title": "SAGA Geoprocessing and Terrain Analysis",
    "description": "Provides access to geocomputing and terrain analysis\nfunctions of the geographical information system (GIS) 'SAGA'\n(System for Automated Geoscientific Analyses) from within R by\nrunning the command line version of SAGA. This package\nfurthermore provides several R functions for handling ASCII\ngrids, including a flexible framework for applying local\nfunctions (including predict methods of fitted models) and\nfocal functions to multiple grids. SAGA GIS is available under\nGPL-2 / LGPL-2 licences from\n<https://sourceforge.net/projects/saga-gis/>.",
    "version": "1.4.2",
    "maintainer": "Alexander Brenning <alexander.brenning@uni-jena.de>",
    "author": "Alexander Brenning [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6640-679X>),\nDonovan Bangs [aut],\nMarc Becker [aut],\nPatrick Schratz [ctb] (ORCID: <https://orcid.org/0000-0003-0748-6624>),\nFabian Polakowski [ctb]",
    "url": "https://github.com/r-spatial/RSAGA",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "centervalue"
      ],
      [
        "create.variable.name"
      ],
      [
        "default.file.extension"
      ],
      [
        "focal.function"
      ],
      [
        "gapply"
      ],
      [
        "get.file.extension"
      ],
      [
        "grid.predict"
      ],
      [
        "grid.to.xyz"
      ],
      [
        "internal.pick.from.ascii.grid"
      ],
      [
        "local.function"
      ],
      [
        "match.arg.ext"
      ],
      [
        "multi.focal.function"
      ],
      [
        "multi.local.function"
      ],
      [
        "pick.from.ascii.grid"
      ],
      [
        "pick.from.ascii.grids"
      ],
      [
        "pick.from.points"
      ],
      [
        "pick.from.saga.grid"
      ],
      [
        "pick.from.shapefile"
      ],
      [
        "read.ascii.grid"
      ],
      [
        "read.ascii.grid.header"
      ],
      [
        "read.Rd.grid"
      ],
      [
        "read.sgrd"
      ],
      [
        "relative.position"
      ],
      [
        "relative.rank"
      ],
      [
        "resid.median"
      ],
      [
        "resid.minmedmax"
      ],
      [
        "resid.quantile"
      ],
      [
        "resid.quartiles"
      ],
      [
        "rsaga.add.grid.values.to.points"
      ],
      [
        "rsaga.aspect"
      ],
      [
        "rsaga.close.gaps"
      ],
      [
        "rsaga.close.one.cell.gaps"
      ],
      [
        "rsaga.contour"
      ],
      [
        "rsaga.copy.sgrd"
      ],
      [
        "rsaga.curvature"
      ],
      [
        "rsaga.env"
      ],
      [
        "rsaga.esri.to.sgrd"
      ],
      [
        "rsaga.esri.wrapper"
      ],
      [
        "rsaga.fill.sinks"
      ],
      [
        "rsaga.filter.gauss"
      ],
      [
        "rsaga.filter.simple"
      ],
      [
        "rsaga.geoprocessor"
      ],
      [
        "rsaga.get.lib.modules"
      ],
      [
        "rsaga.get.libraries"
      ],
      [
        "rsaga.get.modules"
      ],
      [
        "rsaga.get.modules.path"
      ],
      [
        "rsaga.get.usage"
      ],
      [
        "rsaga.get.version"
      ],
      [
        "rsaga.grid.calculus"
      ],
      [
        "rsaga.grid.to.points"
      ],
      [
        "rsaga.grid.to.points.randomly"
      ],
      [
        "rsaga.hillshade"
      ],
      [
        "rsaga.html.help"
      ],
      [
        "rsaga.import.gdal"
      ],
      [
        "rsaga.insolation"
      ],
      [
        "rsaga.intersect.polygons"
      ],
      [
        "rsaga.inverse.distance"
      ],
      [
        "rsaga.lib.prefix"
      ],
      [
        "rsaga.linear.combination"
      ],
      [
        "rsaga.local.morphometry"
      ],
      [
        "rsaga.modified.quadratic.shephard"
      ],
      [
        "rsaga.module.exists"
      ],
      [
        "rsaga.nearest.neighbour"
      ],
      [
        "rsaga.parallel.processing"
      ],
      [
        "rsaga.pisr"
      ],
      [
        "rsaga.pisr2"
      ],
      [
        "rsaga.plan.curvature"
      ],
      [
        "rsaga.profile.curvature"
      ],
      [
        "rsaga.search.modules"
      ],
      [
        "rsaga.set.env"
      ],
      [
        "rsaga.sgrd.to.esri"
      ],
      [
        "rsaga.sink.removal"
      ],
      [
        "rsaga.sink.route"
      ],
      [
        "rsaga.slope"
      ],
      [
        "rsaga.slope.asp.curv"
      ],
      [
        "rsaga.solar.radiation"
      ],
      [
        "rsaga.target"
      ],
      [
        "rsaga.topdown.processing"
      ],
      [
        "rsaga.triangulation"
      ],
      [
        "rsaga.union.polygons"
      ],
      [
        "rsaga.wetness.index"
      ],
      [
        "set.file.extension"
      ],
      [
        "wind.shelter"
      ],
      [
        "wind.shelter.prep"
      ],
      [
        "write.ascii.grid"
      ],
      [
        "write.ascii.grid.header"
      ],
      [
        "write.Rd.grid"
      ],
      [
        "write.sgrd"
      ]
    ],
    "topics": [],
    "score": 8.7851,
    "stars": 25,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "RSAGA SAGA Geoprocessing and Terrain Analysis Provides access to geocomputing and terrain analysis\nfunctions of the geographical information system (GIS) 'SAGA'\n(System for Automated Geoscientific Analyses) from within R by\nrunning the command line version of SAGA. This package\nfurthermore provides several R functions for handling ASCII\ngrids, including a flexible framework for applying local\nfunctions (including predict methods of fitted models) and\nfocal functions to multiple grids. SAGA GIS is available under\nGPL-2 / LGPL-2 licences from\n<https://sourceforge.net/projects/saga-gis/>. centervalue create.variable.name default.file.extension focal.function gapply get.file.extension grid.predict grid.to.xyz internal.pick.from.ascii.grid local.function match.arg.ext multi.focal.function multi.local.function pick.from.ascii.grid pick.from.ascii.grids pick.from.points pick.from.saga.grid pick.from.shapefile read.ascii.grid read.ascii.grid.header read.Rd.grid read.sgrd relative.position relative.rank resid.median resid.minmedmax resid.quantile resid.quartiles rsaga.add.grid.values.to.points rsaga.aspect rsaga.close.gaps rsaga.close.one.cell.gaps rsaga.contour rsaga.copy.sgrd rsaga.curvature rsaga.env rsaga.esri.to.sgrd rsaga.esri.wrapper rsaga.fill.sinks rsaga.filter.gauss rsaga.filter.simple rsaga.geoprocessor rsaga.get.lib.modules rsaga.get.libraries rsaga.get.modules rsaga.get.modules.path rsaga.get.usage rsaga.get.version rsaga.grid.calculus rsaga.grid.to.points rsaga.grid.to.points.randomly rsaga.hillshade rsaga.html.help rsaga.import.gdal rsaga.insolation rsaga.intersect.polygons rsaga.inverse.distance rsaga.lib.prefix rsaga.linear.combination rsaga.local.morphometry rsaga.modified.quadratic.shephard rsaga.module.exists rsaga.nearest.neighbour rsaga.parallel.processing rsaga.pisr rsaga.pisr2 rsaga.plan.curvature rsaga.profile.curvature rsaga.search.modules rsaga.set.env rsaga.sgrd.to.esri rsaga.sink.removal rsaga.sink.route rsaga.slope rsaga.slope.asp.curv rsaga.solar.radiation rsaga.target rsaga.topdown.processing rsaga.triangulation rsaga.union.polygons rsaga.wetness.index set.file.extension wind.shelter wind.shelter.prep write.ascii.grid write.ascii.grid.header write.Rd.grid write.sgrd "
  },
  {
    "id": 615,
    "package_name": "geotargets",
    "title": "'targets' Extensions for Geographic Spatial Formats",
    "description": "Provides extensions for various geographic spatial file\nformats, such as shape files and rasters. Currently provides\nsupport for the 'terra' geographic spatial formats. See the\nvignettes for worked examples, demonstrations, and explanations\nof how to use the various package extensions.",
    "version": "0.3.1.9000",
    "maintainer": "Nicholas Tierney <nicholas.tierney@gmail.com>",
    "author": "Nicholas Tierney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1460-8722>),\nEric Scott [aut] (ORCID: <https://orcid.org/0000-0002-7430-7879>),\nAndrew Brown [aut] (ORCID: <https://orcid.org/0000-0002-4565-533X>)",
    "url": "https://github.com/ropensci/geotargets,\nhttps://docs.ropensci.org/geotargets/",
    "bug_reports": "https://github.com/ropensci/geotargets/issues",
    "repository": "",
    "exports": [
      [
        "geotargets_option_get"
      ],
      [
        "geotargets_option_set"
      ],
      [
        "set_window"
      ],
      [
        "tar_stars"
      ],
      [
        "tar_stars_proxy"
      ],
      [
        "tar_terra_rast"
      ],
      [
        "tar_terra_sds"
      ],
      [
        "tar_terra_sprc"
      ],
      [
        "tar_terra_tiles"
      ],
      [
        "tar_terra_vect"
      ],
      [
        "tar_terra_vrt"
      ],
      [
        "tile_blocksize"
      ],
      [
        "tile_grid"
      ],
      [
        "tile_n"
      ]
    ],
    "topics": [
      [
        "geospatial"
      ],
      [
        "pipeline"
      ],
      [
        "r-targetopia"
      ],
      [
        "raster"
      ],
      [
        "reproducibility"
      ],
      [
        "reproducible-research"
      ],
      [
        "targets"
      ],
      [
        "vector"
      ],
      [
        "workflow"
      ]
    ],
    "score": 8.701,
    "stars": 91,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "geotargets 'targets' Extensions for Geographic Spatial Formats Provides extensions for various geographic spatial file\nformats, such as shape files and rasters. Currently provides\nsupport for the 'terra' geographic spatial formats. See the\nvignettes for worked examples, demonstrations, and explanations\nof how to use the various package extensions. geotargets_option_get geotargets_option_set set_window tar_stars tar_stars_proxy tar_terra_rast tar_terra_sds tar_terra_sprc tar_terra_tiles tar_terra_vect tar_terra_vrt tile_blocksize tile_grid tile_n geospatial pipeline r-targetopia raster reproducibility reproducible-research targets vector workflow"
  },
  {
    "id": 86,
    "package_name": "MODIStsp",
    "title": "Find, Download and Process MODIS Land Products Data",
    "description": "Allows automating the creation of time series of rasters\nderived from MODIS satellite land products data. It performs\nseveral typical preprocessing steps such as download,\nmosaicking, reprojecting and resizing data acquired on a\nspecified time period. All processing parameters can be set\nusing a user-friendly GUI. Users can select which layers of the\noriginal MODIS HDF files they want to process, which additional\nquality indicators should be extracted from aggregated MODIS\nquality assurance layers and, in the case of surface\nreflectance products, which spectral indexes should be computed\nfrom the original reflectance bands. For each output layer,\noutputs are saved as single-band raster files corresponding to\neach available acquisition date. Virtual files allowing access\nto the entire time series as a single file are also created.\nCommand-line execution exploiting a previously saved processing\noptions file is also possible, allowing users to automatically\nupdate time series related to a MODIS product whenever a new\nimage is available. For additional documentation refer to the\nfollowing article: Busetto and Ranghetti (2016)\n<doi:10.1016/j.cageo.2016.08.020>.",
    "version": "2.1.0.9001",
    "maintainer": "Luigi Ranghetti <rpackages.ranghetti@gmail.com>",
    "author": "Lorenzo Busetto [aut] (ORCID: <https://orcid.org/0000-0001-9634-6038>),\nLuigi Ranghetti [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6207-5188>),\nLeah Wasser [rev] (Leah Wasser reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/184),\nJeff Hanson [rev] (Jeff Hanson reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/184),\nBabak Naimi [ctb] (Babak Naimi wrote the function ModisDownload(), on\nwhich some MODIStsp internal functions are based)",
    "url": "https://github.com/ropensci/MODIStsp/,\nhttps://docs.ropensci.org/MODIStsp/",
    "bug_reports": "https://github.com/ropensci/MODIStsp/issues",
    "repository": "",
    "exports": [
      [
        "check_projection"
      ],
      [
        "install_MODIStsp_launcher"
      ],
      [
        "MODIStsp"
      ],
      [
        "MODIStsp_addindex"
      ],
      [
        "MODIStsp_extract"
      ],
      [
        "MODIStsp_get_prodlayers"
      ],
      [
        "MODIStsp_get_prodnames"
      ],
      [
        "MODIStsp_process"
      ],
      [
        "MODIStsp_resetindexes"
      ]
    ],
    "topics": [
      [
        "gdal"
      ],
      [
        "modis"
      ],
      [
        "modis-data"
      ],
      [
        "modis-land-products"
      ],
      [
        "peer-reviewed"
      ],
      [
        "preprocessing"
      ],
      [
        "remote-sensing"
      ],
      [
        "satellite-imagery"
      ],
      [
        "time-series"
      ]
    ],
    "score": 7.9501,
    "stars": 159,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "MODIStsp Find, Download and Process MODIS Land Products Data Allows automating the creation of time series of rasters\nderived from MODIS satellite land products data. It performs\nseveral typical preprocessing steps such as download,\nmosaicking, reprojecting and resizing data acquired on a\nspecified time period. All processing parameters can be set\nusing a user-friendly GUI. Users can select which layers of the\noriginal MODIS HDF files they want to process, which additional\nquality indicators should be extracted from aggregated MODIS\nquality assurance layers and, in the case of surface\nreflectance products, which spectral indexes should be computed\nfrom the original reflectance bands. For each output layer,\noutputs are saved as single-band raster files corresponding to\neach available acquisition date. Virtual files allowing access\nto the entire time series as a single file are also created.\nCommand-line execution exploiting a previously saved processing\noptions file is also possible, allowing users to automatically\nupdate time series related to a MODIS product whenever a new\nimage is available. For additional documentation refer to the\nfollowing article: Busetto and Ranghetti (2016)\n<doi:10.1016/j.cageo.2016.08.020>. check_projection install_MODIStsp_launcher MODIStsp MODIStsp_addindex MODIStsp_extract MODIStsp_get_prodlayers MODIStsp_get_prodnames MODIStsp_process MODIStsp_resetindexes gdal modis modis-data modis-land-products peer-reviewed preprocessing remote-sensing satellite-imagery time-series"
  },
  {
    "id": 1354,
    "package_name": "tiler",
    "title": "Create Geographic and Non-Geographic Map Tiles",
    "description": "Creates geographic map tiles from geospatial map files or\nnon-geographic map tiles from simple image files. This package\nprovides a tile generator function for creating map tile sets\nfor use with packages such as 'leaflet'. In addition to\ngenerating map tiles based on a common raster layer source, it\nalso handles the non-geographic edge case, producing map tiles\nfrom arbitrary images. These map tiles, which have a\nnon-geographic, simple coordinate reference system (CRS), can\nalso be used with 'leaflet' when applying the simple CRS\noption. Map tiles can be created from an input file with any of\nthe following extensions: tif, grd and nc for spatial maps and\npng, jpg and bmp for basic images. This package requires\n'Python' and the 'gdal' library for 'Python'. 'Windows' users\nare recommended to install 'OSGeo4W'\n(<https://trac.osgeo.org/osgeo4w/>) as an easy way to obtain\nthe required 'gdal' support for 'Python'.",
    "version": "0.3.3",
    "maintainer": "Matthew Leonawicz <rpkgs@pm.me>",
    "author": "Matthew Leonawicz [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-9452-2771>),\nAlex M Chubaty [ctb] (ORCID: <https://orcid.org/0000-0001-7146-8135>)",
    "url": "https://docs.ropensci.org/tiler/,\nhttps://github.com/ropensci/tiler",
    "bug_reports": "https://github.com/ropensci/tiler/issues",
    "repository": "",
    "exports": [
      [
        "tile"
      ],
      [
        "tile_viewer"
      ],
      [
        "tiler_options"
      ],
      [
        "view_tiles"
      ]
    ],
    "topics": [],
    "score": 7.7885,
    "stars": 64,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tiler Create Geographic and Non-Geographic Map Tiles Creates geographic map tiles from geospatial map files or\nnon-geographic map tiles from simple image files. This package\nprovides a tile generator function for creating map tile sets\nfor use with packages such as 'leaflet'. In addition to\ngenerating map tiles based on a common raster layer source, it\nalso handles the non-geographic edge case, producing map tiles\nfrom arbitrary images. These map tiles, which have a\nnon-geographic, simple coordinate reference system (CRS), can\nalso be used with 'leaflet' when applying the simple CRS\noption. Map tiles can be created from an input file with any of\nthe following extensions: tif, grd and nc for spatial maps and\npng, jpg and bmp for basic images. This package requires\n'Python' and the 'gdal' library for 'Python'. 'Windows' users\nare recommended to install 'OSGeo4W'\n(<https://trac.osgeo.org/osgeo4w/>) as an easy way to obtain\nthe required 'gdal' support for 'Python'. tile tile_viewer tiler_options view_tiles "
  },
  {
    "id": 360,
    "package_name": "chopin",
    "title": "Spatial Parallel Computing by Hierarchical Data Partitioning",
    "description": "Geospatial data computation is parallelized by grid,\nhierarchy, or raster files. Based on 'future' (Bengtsson, 2024\n<doi:10.32614/CRAN.package.future>) and 'mirai' (Gao et al.,\n2025 <doi:10.32614/CRAN.package.mirai>) parallel back-ends,\n'terra' (Hijmans et al., 2025\n<doi:10.32614/CRAN.package.terra>) and 'sf' (Pebesma et al.,\n2024 <doi:10.32614/CRAN.package.sf>) functions as well as\nconvenience functions in the package can be distributed over\nmultiple threads. The simplest way of parallelizing generic\ngeospatial computation is to start from par_pad_*() functions\nto par_grid(), par_hierarchy(), or par_multirasters()\nfunctions. Virtually any functions accepting classes in 'terra'\nor 'sf' packages can be used in the three parallelization\nfunctions. A common raster-vector overlay operation is provided\nas a function extract_at(), which uses 'exactextractr' (Baston,\n2023 <doi:10.32614/CRAN.package.exactextractr>), with options\nfor kernel weights for summarizing raster values at vector\ngeometries. Other convenience functions for vector-vector\noperations including simple areal interpolation\n(summarize_aw()) and summation of exponentially decaying\nweights (summarize_sedc()) are also provided.",
    "version": "0.9.9",
    "maintainer": "Insang Song <geoissong@gmail.com>",
    "author": "Insang Song [aut, cre] (ORCID: <https://orcid.org/0000-0001-8732-3256>),\nKyle Messier [aut, ctb] (ORCID:\n<https://orcid.org/0000-0001-9508-9623>),\nAlec L. Robitaille [rev] (Alec reviewed the package version 0.6.3 for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/638>),\nEric R. Scott [rev] (Eric reviewed the package version 0.6.3 for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/638>)",
    "url": "https://docs.ropensci.org/chopin/,\nhttps://github.com/ropensci/chopin",
    "bug_reports": "https://github.com/ropensci/chopin/issues",
    "repository": "",
    "exports": [
      [
        "extract_at"
      ],
      [
        "kernelfunction"
      ],
      [
        "par_convert_f"
      ],
      [
        "par_grid"
      ],
      [
        "par_grid_mirai"
      ],
      [
        "par_hierarchy"
      ],
      [
        "par_hierarchy_mirai"
      ],
      [
        "par_make_dggrid"
      ],
      [
        "par_make_h3"
      ],
      [
        "par_merge_grid"
      ],
      [
        "par_multirasters"
      ],
      [
        "par_multirasters_mirai"
      ],
      [
        "par_pad_balanced"
      ],
      [
        "par_pad_grid"
      ],
      [
        "par_split_list"
      ],
      [
        "summarize_aw"
      ],
      [
        "summarize_sedc"
      ]
    ],
    "topics": [],
    "score": 7.4362,
    "stars": 26,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "chopin Spatial Parallel Computing by Hierarchical Data Partitioning Geospatial data computation is parallelized by grid,\nhierarchy, or raster files. Based on 'future' (Bengtsson, 2024\n<doi:10.32614/CRAN.package.future>) and 'mirai' (Gao et al.,\n2025 <doi:10.32614/CRAN.package.mirai>) parallel back-ends,\n'terra' (Hijmans et al., 2025\n<doi:10.32614/CRAN.package.terra>) and 'sf' (Pebesma et al.,\n2024 <doi:10.32614/CRAN.package.sf>) functions as well as\nconvenience functions in the package can be distributed over\nmultiple threads. The simplest way of parallelizing generic\ngeospatial computation is to start from par_pad_*() functions\nto par_grid(), par_hierarchy(), or par_multirasters()\nfunctions. Virtually any functions accepting classes in 'terra'\nor 'sf' packages can be used in the three parallelization\nfunctions. A common raster-vector overlay operation is provided\nas a function extract_at(), which uses 'exactextractr' (Baston,\n2023 <doi:10.32614/CRAN.package.exactextractr>), with options\nfor kernel weights for summarizing raster values at vector\ngeometries. Other convenience functions for vector-vector\noperations including simple areal interpolation\n(summarize_aw()) and summation of exponentially decaying\nweights (summarize_sedc()) are also provided. extract_at kernelfunction par_convert_f par_grid par_grid_mirai par_hierarchy par_hierarchy_mirai par_make_dggrid par_make_h3 par_merge_grid par_multirasters par_multirasters_mirai par_pad_balanced par_pad_grid par_split_list summarize_aw summarize_sedc "
  },
  {
    "id": 1429,
    "package_name": "waywiser",
    "title": "Ergonomic Methods for Assessing Spatial Models",
    "description": "Assessing predictive models of spatial data can be\nchallenging, both because these models are typically built for\nextrapolating outside the original region represented by\ntraining data and due to potential spatially structured errors,\nwith \"hot spots\" of higher than expected error clustered\ngeographically due to spatial structure in the underlying data.\nMethods are provided for assessing models fit to spatial data,\nincluding approaches for measuring the spatial structure of\nmodel errors, assessing model predictions at multiple spatial\nscales, and evaluating where predictions can be made safely.\nMethods are particularly useful for models fit using the\n'tidymodels' framework. Methods include Moran's I ('Moran'\n(1950) <doi:10.2307/2332142>), Geary's C ('Geary' (1954)\n<doi:10.2307/2986645>), Getis-Ord's G ('Ord' and 'Getis' (1995)\n<doi:10.1111/j.1538-4632.1995.tb00912.x>), agreement\ncoefficients from 'Ji' and Gallo (2006) (<doi:\n10.14358/PERS.72.7.823>), agreement metrics from 'Willmott'\n(1981) (<doi: 10.1080/02723646.1981.10642213>) and 'Willmott'\n'et' 'al'. (2012) (<doi: 10.1002/joc.2419>), an implementation\nof the area of applicability methodology from 'Meyer' and\n'Pebesma' (2021) (<doi:10.1111/2041-210X.13650>), and an\nimplementation of multi-scale assessment as described in\n'Riemann' 'et' 'al'. (2010) (<doi:10.1016/j.rse.2010.05.010>).",
    "version": "0.6.3",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Michael Mahoney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2402-304X>),\nLucas Johnson [ctb] (ORCID: <https://orcid.org/0000-0002-7953-0260>),\nVirgilio G\u00f3mez-Rubio [rev] (Virgilio reviewed the package (v.\n0.2.0.9000) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/571>),\nJakub Nowosad [rev] (Jakub reviewed the package (v. 0.2.0.9000) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/571>),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/ropensci/waywiser,\nhttps://docs.ropensci.org/waywiser/",
    "bug_reports": "https://github.com/ropensci/waywiser/issues",
    "repository": "",
    "exports": [
      [
        "ww_agreement_coefficient"
      ],
      [
        "ww_agreement_coefficient_vec"
      ],
      [
        "ww_area_of_applicability"
      ],
      [
        "ww_build_neighbors"
      ],
      [
        "ww_build_weights"
      ],
      [
        "ww_global_geary_c"
      ],
      [
        "ww_global_geary_c_vec"
      ],
      [
        "ww_global_geary_pvalue"
      ],
      [
        "ww_global_geary_pvalue_vec"
      ],
      [
        "ww_global_moran_i"
      ],
      [
        "ww_global_moran_i_vec"
      ],
      [
        "ww_global_moran_pvalue"
      ],
      [
        "ww_global_moran_pvalue_vec"
      ],
      [
        "ww_local_geary_c"
      ],
      [
        "ww_local_geary_c_vec"
      ],
      [
        "ww_local_geary_pvalue"
      ],
      [
        "ww_local_geary_pvalue_vec"
      ],
      [
        "ww_local_getis_ord_g"
      ],
      [
        "ww_local_getis_ord_g_pvalue"
      ],
      [
        "ww_local_getis_ord_g_pvalue_vec"
      ],
      [
        "ww_local_getis_ord_g_vec"
      ],
      [
        "ww_local_moran_i"
      ],
      [
        "ww_local_moran_i_vec"
      ],
      [
        "ww_local_moran_pvalue"
      ],
      [
        "ww_local_moran_pvalue_vec"
      ],
      [
        "ww_make_point_neighbors"
      ],
      [
        "ww_make_polygon_neighbors"
      ],
      [
        "ww_multi_scale"
      ],
      [
        "ww_systematic_agreement_coefficient"
      ],
      [
        "ww_systematic_agreement_coefficient_vec"
      ],
      [
        "ww_systematic_mpd"
      ],
      [
        "ww_systematic_mpd_vec"
      ],
      [
        "ww_systematic_mse"
      ],
      [
        "ww_systematic_mse_vec"
      ],
      [
        "ww_systematic_rmpd"
      ],
      [
        "ww_systematic_rmpd_vec"
      ],
      [
        "ww_systematic_rmse"
      ],
      [
        "ww_systematic_rmse_vec"
      ],
      [
        "ww_unsystematic_agreement_coefficient"
      ],
      [
        "ww_unsystematic_agreement_coefficient_vec"
      ],
      [
        "ww_unsystematic_mpd"
      ],
      [
        "ww_unsystematic_mpd_vec"
      ],
      [
        "ww_unsystematic_mse"
      ],
      [
        "ww_unsystematic_mse_vec"
      ],
      [
        "ww_unsystematic_rmpd"
      ],
      [
        "ww_unsystematic_rmpd_vec"
      ],
      [
        "ww_unsystematic_rmse"
      ],
      [
        "ww_unsystematic_rmse_vec"
      ],
      [
        "ww_willmott_d"
      ],
      [
        "ww_willmott_d_vec"
      ],
      [
        "ww_willmott_d1"
      ],
      [
        "ww_willmott_d1_vec"
      ],
      [
        "ww_willmott_dr"
      ],
      [
        "ww_willmott_dr_vec"
      ]
    ],
    "topics": [
      [
        "spatial"
      ],
      [
        "spatial-analysis"
      ],
      [
        "tidymodels"
      ],
      [
        "tidyverse"
      ]
    ],
    "score": 7.4275,
    "stars": 39,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "waywiser Ergonomic Methods for Assessing Spatial Models Assessing predictive models of spatial data can be\nchallenging, both because these models are typically built for\nextrapolating outside the original region represented by\ntraining data and due to potential spatially structured errors,\nwith \"hot spots\" of higher than expected error clustered\ngeographically due to spatial structure in the underlying data.\nMethods are provided for assessing models fit to spatial data,\nincluding approaches for measuring the spatial structure of\nmodel errors, assessing model predictions at multiple spatial\nscales, and evaluating where predictions can be made safely.\nMethods are particularly useful for models fit using the\n'tidymodels' framework. Methods include Moran's I ('Moran'\n(1950) <doi:10.2307/2332142>), Geary's C ('Geary' (1954)\n<doi:10.2307/2986645>), Getis-Ord's G ('Ord' and 'Getis' (1995)\n<doi:10.1111/j.1538-4632.1995.tb00912.x>), agreement\ncoefficients from 'Ji' and Gallo (2006) (<doi:\n10.14358/PERS.72.7.823>), agreement metrics from 'Willmott'\n(1981) (<doi: 10.1080/02723646.1981.10642213>) and 'Willmott'\n'et' 'al'. (2012) (<doi: 10.1002/joc.2419>), an implementation\nof the area of applicability methodology from 'Meyer' and\n'Pebesma' (2021) (<doi:10.1111/2041-210X.13650>), and an\nimplementation of multi-scale assessment as described in\n'Riemann' 'et' 'al'. (2010) (<doi:10.1016/j.rse.2010.05.010>). ww_agreement_coefficient ww_agreement_coefficient_vec ww_area_of_applicability ww_build_neighbors ww_build_weights ww_global_geary_c ww_global_geary_c_vec ww_global_geary_pvalue ww_global_geary_pvalue_vec ww_global_moran_i ww_global_moran_i_vec ww_global_moran_pvalue ww_global_moran_pvalue_vec ww_local_geary_c ww_local_geary_c_vec ww_local_geary_pvalue ww_local_geary_pvalue_vec ww_local_getis_ord_g ww_local_getis_ord_g_pvalue ww_local_getis_ord_g_pvalue_vec ww_local_getis_ord_g_vec ww_local_moran_i ww_local_moran_i_vec ww_local_moran_pvalue ww_local_moran_pvalue_vec ww_make_point_neighbors ww_make_polygon_neighbors ww_multi_scale ww_systematic_agreement_coefficient ww_systematic_agreement_coefficient_vec ww_systematic_mpd ww_systematic_mpd_vec ww_systematic_mse ww_systematic_mse_vec ww_systematic_rmpd ww_systematic_rmpd_vec ww_systematic_rmse ww_systematic_rmse_vec ww_unsystematic_agreement_coefficient ww_unsystematic_agreement_coefficient_vec ww_unsystematic_mpd ww_unsystematic_mpd_vec ww_unsystematic_mse ww_unsystematic_mse_vec ww_unsystematic_rmpd ww_unsystematic_rmpd_vec ww_unsystematic_rmse ww_unsystematic_rmse_vec ww_willmott_d ww_willmott_d_vec ww_willmott_d1 ww_willmott_d1_vec ww_willmott_dr ww_willmott_dr_vec spatial spatial-analysis tidymodels tidyverse"
  },
  {
    "id": 908,
    "package_name": "oompaBase",
    "title": "Class Unions, Matrix Operations, and Color Schemes for OOMPA",
    "description": "Provides the class unions that must be preloaded in order\nfor the basic tools in the OOMPA (Object-Oriented Microarray\nand Proteomics Analysis) project to be defined and loaded. It\nalso includes vectorized operations for row-by-row means,\nvariances, and t-tests. Finally, it provides new color schemes.\nDetails on the packages in the OOMPA project can be found at\n<http://oompa.r-forge.r-project.org/>.",
    "version": "3.2.10",
    "maintainer": "Kevin R. Coombes <krc@silicovore.com>",
    "author": "Kevin R. Coombes [aut, cre]",
    "url": "http://oompa.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "as.data.frame"
      ],
      [
        "bluescale"
      ],
      [
        "blueyellow"
      ],
      [
        "colorCode"
      ],
      [
        "ColorCodedPair"
      ],
      [
        "ColorCoding"
      ],
      [
        "cyanyellow"
      ],
      [
        "grayscale"
      ],
      [
        "greenscale"
      ],
      [
        "greyscale"
      ],
      [
        "hist"
      ],
      [
        "image"
      ],
      [
        "jetColors"
      ],
      [
        "matrixMean"
      ],
      [
        "matrixPairedT"
      ],
      [
        "matrixT"
      ],
      [
        "matrixUnequalT"
      ],
      [
        "matrixVar"
      ],
      [
        "oompaColor"
      ],
      [
        "plot"
      ],
      [
        "print"
      ],
      [
        "redgreen"
      ],
      [
        "redscale"
      ],
      [
        "summary"
      ],
      [
        "wheel"
      ]
    ],
    "topics": [
      [
        "infrastructure"
      ]
    ],
    "score": 7.2962,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "oompaBase Class Unions, Matrix Operations, and Color Schemes for OOMPA Provides the class unions that must be preloaded in order\nfor the basic tools in the OOMPA (Object-Oriented Microarray\nand Proteomics Analysis) project to be defined and loaded. It\nalso includes vectorized operations for row-by-row means,\nvariances, and t-tests. Finally, it provides new color schemes.\nDetails on the packages in the OOMPA project can be found at\n<http://oompa.r-forge.r-project.org/>. as.data.frame bluescale blueyellow colorCode ColorCodedPair ColorCoding cyanyellow grayscale greenscale greyscale hist image jetColors matrixMean matrixPairedT matrixT matrixUnequalT matrixVar oompaColor plot print redgreen redscale summary wheel infrastructure"
  },
  {
    "id": 1308,
    "package_name": "taxlist",
    "title": "Handling Taxonomic Lists",
    "description": "Handling taxonomic lists through objects of class\n'taxlist'. This package provides functions to import species\nlists from 'Turboveg'\n(<https://www.synbiosys.alterra.nl/turboveg/>) and the\npossibility to create backups from resulting R-objects. Also\nquick displays are implemented as summary-methods.",
    "version": "0.3.4",
    "maintainer": "Miguel Alvarez <kamapu78@gmail.com>",
    "author": "Miguel Alvarez [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1500-1834>),\nZachary Foster [ctb] (ORCID: <https://orcid.org/0000-0002-5075-0948>),\nSam Levin [rev],\nMargaret Siple [rev]",
    "url": "https://cran.r-project.org/package=taxlist,\nhttps://github.com/ropensci/taxlist,\nhttps://docs.ropensci.org/taxlist/",
    "bug_reports": "https://github.com/ropensci/taxlist/issues",
    "repository": "",
    "exports": [
      [
        "accepted_name"
      ],
      [
        "accepted_name<-"
      ],
      [
        "add_concept"
      ],
      [
        "add_level"
      ],
      [
        "add_parent"
      ],
      [
        "add_synonym"
      ],
      [
        "add_trait"
      ],
      [
        "add_view"
      ],
      [
        "backup_object"
      ],
      [
        "basionym"
      ],
      [
        "basionym<-"
      ],
      [
        "change_concept<-"
      ],
      [
        "clean"
      ],
      [
        "clean_strings"
      ],
      [
        "count_taxa"
      ],
      [
        "delete_name"
      ],
      [
        "df2taxlist"
      ],
      [
        "dissect_name"
      ],
      [
        "get_children"
      ],
      [
        "get_parents"
      ],
      [
        "id_generator"
      ],
      [
        "id_solver"
      ],
      [
        "indented_list"
      ],
      [
        "insert_rows"
      ],
      [
        "levels"
      ],
      [
        "levels<-"
      ],
      [
        "levels<-.taxlist"
      ],
      [
        "load_last"
      ],
      [
        "match_names"
      ],
      [
        "merge_taxa"
      ],
      [
        "merge_to_parent"
      ],
      [
        "parents"
      ],
      [
        "print"
      ],
      [
        "print_name"
      ],
      [
        "prune_levels"
      ],
      [
        "reindex"
      ],
      [
        "reindex<-"
      ],
      [
        "replace_idx"
      ],
      [
        "replace_na"
      ],
      [
        "replace_view"
      ],
      [
        "replace_x"
      ],
      [
        "S4_to_list"
      ],
      [
        "show"
      ],
      [
        "sort_backups"
      ],
      [
        "sort_taxa"
      ],
      [
        "subset"
      ],
      [
        "summary"
      ],
      [
        "synonyms"
      ],
      [
        "tax2traits"
      ],
      [
        "taxlist2df"
      ],
      [
        "taxlist2taxmap"
      ],
      [
        "taxmap2taxlist"
      ],
      [
        "taxon_names"
      ],
      [
        "taxon_names<-"
      ],
      [
        "taxon_relations"
      ],
      [
        "taxon_relations<-"
      ],
      [
        "taxon_traits"
      ],
      [
        "taxon_traits<-"
      ],
      [
        "taxon_views"
      ],
      [
        "taxon_views<-"
      ],
      [
        "tnrs"
      ],
      [
        "tv2taxlist"
      ],
      [
        "update_concept"
      ],
      [
        "update_name"
      ],
      [
        "update_trait"
      ]
    ],
    "topics": [],
    "score": 7.1744,
    "stars": 12,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxlist Handling Taxonomic Lists Handling taxonomic lists through objects of class\n'taxlist'. This package provides functions to import species\nlists from 'Turboveg'\n(<https://www.synbiosys.alterra.nl/turboveg/>) and the\npossibility to create backups from resulting R-objects. Also\nquick displays are implemented as summary-methods. accepted_name accepted_name<- add_concept add_level add_parent add_synonym add_trait add_view backup_object basionym basionym<- change_concept<- clean clean_strings count_taxa delete_name df2taxlist dissect_name get_children get_parents id_generator id_solver indented_list insert_rows levels levels<- levels<-.taxlist load_last match_names merge_taxa merge_to_parent parents print print_name prune_levels reindex reindex<- replace_idx replace_na replace_view replace_x S4_to_list show sort_backups sort_taxa subset summary synonyms tax2traits taxlist2df taxlist2taxmap taxmap2taxlist taxon_names taxon_names<- taxon_relations taxon_relations<- taxon_traits taxon_traits<- taxon_views taxon_views<- tnrs tv2taxlist update_concept update_name update_trait "
  },
  {
    "id": 1245,
    "package_name": "sofa",
    "title": "Connector to 'CouchDB'",
    "description": "Provides an interface to the 'NoSQL' database 'CouchDB'\n(<http://couchdb.apache.org>). Methods are provided for\nmanaging databases within 'CouchDB', including\ncreating/deleting/updating/transferring, and managing documents\nwithin databases. One can connect with a local 'CouchDB'\ninstance, or a remote 'CouchDB' databases such as 'Cloudant'.\nDocuments can be inserted directly from vectors, lists,\ndata.frames, and 'JSON'. Targeted at 'CouchDB' v2 or greater.",
    "version": "0.4.1",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-9200-1016>),\nEduard Sz\u00f6cs [aut] (ORCID: <https://orcid.org/0000-0003-1444-9135>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/sofa (devel)\nhttps://docs.ropensci.org/sofa (docs)",
    "bug_reports": "https://github.com/ropensci/sofa/issues",
    "repository": "",
    "exports": [
      [
        "active_tasks"
      ],
      [
        "attach_get"
      ],
      [
        "Cushion"
      ],
      [
        "db_alldocs"
      ],
      [
        "db_bulk_create"
      ],
      [
        "db_bulk_get"
      ],
      [
        "db_bulk_update"
      ],
      [
        "db_changes"
      ],
      [
        "db_compact"
      ],
      [
        "db_create"
      ],
      [
        "db_delete"
      ],
      [
        "db_explain"
      ],
      [
        "db_index"
      ],
      [
        "db_index_create"
      ],
      [
        "db_index_delete"
      ],
      [
        "db_info"
      ],
      [
        "db_list"
      ],
      [
        "db_query"
      ],
      [
        "db_replicate"
      ],
      [
        "db_revisions"
      ],
      [
        "design_create"
      ],
      [
        "design_create_"
      ],
      [
        "design_delete"
      ],
      [
        "design_get"
      ],
      [
        "design_head"
      ],
      [
        "design_info"
      ],
      [
        "design_search"
      ],
      [
        "design_search_many"
      ],
      [
        "doc_attach_create"
      ],
      [
        "doc_attach_delete"
      ],
      [
        "doc_attach_get"
      ],
      [
        "doc_attach_info"
      ],
      [
        "doc_create"
      ],
      [
        "doc_delete"
      ],
      [
        "doc_get"
      ],
      [
        "doc_head"
      ],
      [
        "doc_update"
      ],
      [
        "doc_upsert"
      ],
      [
        "membership"
      ],
      [
        "parse_df"
      ],
      [
        "ping"
      ],
      [
        "restart"
      ],
      [
        "session"
      ],
      [
        "uuids"
      ]
    ],
    "topics": [
      [
        "couchdb"
      ],
      [
        "database"
      ],
      [
        "nosql"
      ],
      [
        "documents"
      ],
      [
        "cloudant"
      ],
      [
        "couchdb-client"
      ]
    ],
    "score": 7.037,
    "stars": 33,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "sofa Connector to 'CouchDB' Provides an interface to the 'NoSQL' database 'CouchDB'\n(<http://couchdb.apache.org>). Methods are provided for\nmanaging databases within 'CouchDB', including\ncreating/deleting/updating/transferring, and managing documents\nwithin databases. One can connect with a local 'CouchDB'\ninstance, or a remote 'CouchDB' databases such as 'Cloudant'.\nDocuments can be inserted directly from vectors, lists,\ndata.frames, and 'JSON'. Targeted at 'CouchDB' v2 or greater. active_tasks attach_get Cushion db_alldocs db_bulk_create db_bulk_get db_bulk_update db_changes db_compact db_create db_delete db_explain db_index db_index_create db_index_delete db_info db_list db_query db_replicate db_revisions design_create design_create_ design_delete design_get design_head design_info design_search design_search_many doc_attach_create doc_attach_delete doc_attach_get doc_attach_info doc_create doc_delete doc_get doc_head doc_update doc_upsert membership parse_df ping restart session uuids couchdb database nosql documents cloudant couchdb-client"
  },
  {
    "id": 743,
    "package_name": "landscapetools",
    "title": "Landscape Utility Toolbox",
    "description": "Provides utility functions for some of the less-glamorous\ntasks involved in landscape analysis. It includes functions to\ncoerce raster data to the common tibble format and vice versa,\nit helps with flexible reclassification tasks of raster data\nand it provides a function to merge multiple raster.\nFurthermore, 'landscapetools' helps landscape scientists to\nvisualize their data by providing optional themes and utility\nfunctions to plot single landscapes, rasterstacks, -bricks and\nlists of raster.",
    "version": "0.6.2.9001",
    "maintainer": "Marco Sciaini <sciaini.marco@gmail.com>",
    "author": "Marco Sciaini [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-3042-5435>),\nMatthias Fritsch [aut],\nMaximilian H.K. Hesselbarth [aut] (ORCID:\n<https://orcid.org/0000-0003-1125-9918>),\nJakub Nowosad [aut] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\nLaura Graham [rev] (Laura reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/onboarding/issues/188),\nJeffrey Hollister [rev] (Jeffrey reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/onboarding/issues/188)",
    "url": "https://docs.ropensci.org/landscapetools/",
    "bug_reports": "https://github.com/ropensci/landscapetools/issues",
    "repository": "",
    "exports": [
      [
        "show_landscape"
      ],
      [
        "show_shareplot"
      ],
      [
        "theme_facetplot"
      ],
      [
        "theme_facetplot_discrete"
      ],
      [
        "theme_nlm"
      ],
      [
        "theme_nlm_discrete"
      ],
      [
        "theme_nlm_grey"
      ],
      [
        "theme_nlm_grey_discrete"
      ],
      [
        "util_as_integer"
      ],
      [
        "util_binarize"
      ],
      [
        "util_calc_boundaries"
      ],
      [
        "util_classify"
      ],
      [
        "util_extract_multibuffer"
      ],
      [
        "util_merge"
      ],
      [
        "util_raster2tibble"
      ],
      [
        "util_rescale"
      ],
      [
        "util_tibble2raster"
      ],
      [
        "util_w2cp"
      ],
      [
        "util_writeESRI"
      ]
    ],
    "topics": [
      [
        "landscape"
      ],
      [
        "landscape-ecology"
      ],
      [
        "raster"
      ],
      [
        "visualization"
      ],
      [
        "workflow"
      ]
    ],
    "score": 7.0099,
    "stars": 50,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "landscapetools Landscape Utility Toolbox Provides utility functions for some of the less-glamorous\ntasks involved in landscape analysis. It includes functions to\ncoerce raster data to the common tibble format and vice versa,\nit helps with flexible reclassification tasks of raster data\nand it provides a function to merge multiple raster.\nFurthermore, 'landscapetools' helps landscape scientists to\nvisualize their data by providing optional themes and utility\nfunctions to plot single landscapes, rasterstacks, -bricks and\nlists of raster. show_landscape show_shareplot theme_facetplot theme_facetplot_discrete theme_nlm theme_nlm_discrete theme_nlm_grey theme_nlm_grey_discrete util_as_integer util_binarize util_calc_boundaries util_classify util_extract_multibuffer util_merge util_raster2tibble util_rescale util_tibble2raster util_w2cp util_writeESRI landscape landscape-ecology raster visualization workflow"
  },
  {
    "id": 1304,
    "package_name": "taxa",
    "title": "Classes for Storing and Manipulating Taxonomic Data",
    "description": "Provides classes for storing and manipulating taxonomic\ndata. Most of the classes can be treated like base R vectors\n(e.g. can be used in tables as columns and can be named).\nVectorized classes can store taxon names and authorities, taxon\nIDs from databases, taxon ranks, and other types of\ninformation. More complex classes are provided to store\ntaxonomic trees and user-defined data associated with them.",
    "version": "0.4.4",
    "maintainer": "Zachary Foster <zacharyfoster1989@gmail.com>",
    "author": "Scott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nZachary Foster [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5075-0948>)",
    "url": "https://docs.ropensci.org/taxa/, https://github.com/ropensci/taxa",
    "bug_reports": "https://github.com/ropensci/taxa/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "%in%"
      ],
      [
        "as_data_frame"
      ],
      [
        "as_taxon"
      ],
      [
        "classification"
      ],
      [
        "contains"
      ],
      [
        "db_ref"
      ],
      [
        "ends_with"
      ],
      [
        "everything"
      ],
      [
        "internodes"
      ],
      [
        "is_classification"
      ],
      [
        "is_internode"
      ],
      [
        "is_leaf"
      ],
      [
        "is_root"
      ],
      [
        "is_stem"
      ],
      [
        "is_taxon"
      ],
      [
        "is_taxon_authority"
      ],
      [
        "is_taxon_db"
      ],
      [
        "is_taxon_id"
      ],
      [
        "is_taxon_rank"
      ],
      [
        "is_taxonomy"
      ],
      [
        "leaves"
      ],
      [
        "matches"
      ],
      [
        "n_leaves"
      ],
      [
        "n_subtaxa"
      ],
      [
        "n_supertaxa"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "roots"
      ],
      [
        "starts_with"
      ],
      [
        "stems"
      ],
      [
        "subtaxa"
      ],
      [
        "supertaxa"
      ],
      [
        "tax_auth"
      ],
      [
        "tax_auth<-"
      ],
      [
        "tax_author"
      ],
      [
        "tax_author<-"
      ],
      [
        "tax_cite"
      ],
      [
        "tax_cite<-"
      ],
      [
        "tax_date"
      ],
      [
        "tax_date<-"
      ],
      [
        "tax_db"
      ],
      [
        "tax_db<-"
      ],
      [
        "tax_id"
      ],
      [
        "tax_id<-"
      ],
      [
        "tax_name"
      ],
      [
        "tax_name<-"
      ],
      [
        "tax_rank"
      ],
      [
        "tax_rank<-"
      ],
      [
        "taxon"
      ],
      [
        "taxon_authority"
      ],
      [
        "taxon_db"
      ],
      [
        "taxon_id"
      ],
      [
        "taxon_rank"
      ],
      [
        "taxonomy"
      ]
    ],
    "topics": [
      [
        "taxonomy"
      ],
      [
        "biology"
      ],
      [
        "hierarchy"
      ],
      [
        "data-cleaning"
      ],
      [
        "taxon"
      ]
    ],
    "score": 6.9957,
    "stars": 50,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxa Classes for Storing and Manipulating Taxonomic Data Provides classes for storing and manipulating taxonomic\ndata. Most of the classes can be treated like base R vectors\n(e.g. can be used in tables as columns and can be named).\nVectorized classes can store taxon names and authorities, taxon\nIDs from databases, taxon ranks, and other types of\ninformation. More complex classes are provided to store\ntaxonomic trees and user-defined data associated with them. %>% %in% as_data_frame as_taxon classification contains db_ref ends_with everything internodes is_classification is_internode is_leaf is_root is_stem is_taxon is_taxon_authority is_taxon_db is_taxon_id is_taxon_rank is_taxonomy leaves matches n_leaves n_subtaxa n_supertaxa num_range one_of roots starts_with stems subtaxa supertaxa tax_auth tax_auth<- tax_author tax_author<- tax_cite tax_cite<- tax_date tax_date<- tax_db tax_db<- tax_id tax_id<- tax_name tax_name<- tax_rank tax_rank<- taxon taxon_authority taxon_db taxon_id taxon_rank taxonomy taxonomy biology hierarchy data-cleaning taxon"
  },
  {
    "id": 657,
    "package_name": "grImport",
    "title": "Importing Vector Graphics",
    "description": "Functions for converting, importing, and drawing\nPostScript pictures in R plots.",
    "version": "0.9-7",
    "maintainer": "Paul Murrell <p.murrell@auckland.ac.nz>",
    "author": "Paul Murrell [aut, cre],\nRichard Walton [aut],\nSimon Potter [ctb],\nTomas Kalibera [ctb],\nHarold Gutch [ctb]",
    "url": "https://r-forge.r-project.org/projects/grimport/,https://doi.org/10.18637/jss.v030.i04,https://stattech.wordpress.fos.auckland.ac.nz/2018/09/20/2018-09-importing-general-purpose-graphics-in-r/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "drawPath"
      ],
      [
        "explodePaths"
      ],
      [
        "grid.picture"
      ],
      [
        "grid.symbols"
      ],
      [
        "grobify"
      ],
      [
        "oldGrobify"
      ],
      [
        "picture"
      ],
      [
        "pictureGrob"
      ],
      [
        "picturePaths"
      ],
      [
        "PostScriptTrace"
      ],
      [
        "readPicture"
      ],
      [
        "symbolize"
      ],
      [
        "symbolsGrob"
      ]
    ],
    "topics": [],
    "score": 6.8753,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "grImport Importing Vector Graphics Functions for converting, importing, and drawing\nPostScript pictures in R plots. drawPath explodePaths grid.picture grid.symbols grobify oldGrobify picture pictureGrob picturePaths PostScriptTrace readPicture symbolize symbolsGrob "
  },
  {
    "id": 1153,
    "package_name": "rsi",
    "title": "Efficiently Retrieve and Process Satellite Imagery",
    "description": "Downloads spatial data from spatiotemporal asset catalogs\n('STAC'), computes standard spectral indices from the Awesome\nSpectral Indices project (Montero et al. (2023)\n<doi:10.1038/s41597-023-02096-0>) against raster data, and\nglues the outputs together into predictor bricks. Methods focus\non interoperability with the broader spatial ecosystem;\nfunction arguments and outputs use classes from 'sf' and\n'terra', and data downloading functions support complex 'CQL2'\nqueries using 'rstac'.",
    "version": "0.3.2.9000",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Michael Mahoney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2402-304X>),\nFelipe Carvalho [rev] (Felipe reviewed the package (v. 0.3.0) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/636>),\nMichael Sumner [rev] (Michael reviewed the package (v. 0.3.0) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/636>),\nPermian Global [cph, fnd]",
    "url": "https://github.com/Permian-Global-Research/rsi,\nhttps://permian-global-research.github.io/rsi/",
    "bug_reports": "https://github.com/Permian-Global-Research/rsi/issues",
    "repository": "",
    "exports": [
      [
        "alos_palsar_mask_function"
      ],
      [
        "calculate_indices"
      ],
      [
        "default_query_function"
      ],
      [
        "filter_bands"
      ],
      [
        "filter_platforms"
      ],
      [
        "get_alos_palsar_imagery"
      ],
      [
        "get_dem"
      ],
      [
        "get_landsat_imagery"
      ],
      [
        "get_naip_imagery"
      ],
      [
        "get_sentinel1_imagery"
      ],
      [
        "get_sentinel2_imagery"
      ],
      [
        "get_stac_data"
      ],
      [
        "landsat_mask_function"
      ],
      [
        "landsat_platform_filter"
      ],
      [
        "rsi_download_rasters"
      ],
      [
        "rsi_gdal_config_options"
      ],
      [
        "rsi_gdalwarp_options"
      ],
      [
        "rsi_query_api"
      ],
      [
        "sentinel2_mask_function"
      ],
      [
        "sign_planetary_computer"
      ],
      [
        "spectral_indices"
      ],
      [
        "spectral_indices_url"
      ],
      [
        "stack_rasters"
      ]
    ],
    "topics": [],
    "score": 6.7404,
    "stars": 55,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rsi Efficiently Retrieve and Process Satellite Imagery Downloads spatial data from spatiotemporal asset catalogs\n('STAC'), computes standard spectral indices from the Awesome\nSpectral Indices project (Montero et al. (2023)\n<doi:10.1038/s41597-023-02096-0>) against raster data, and\nglues the outputs together into predictor bricks. Methods focus\non interoperability with the broader spatial ecosystem;\nfunction arguments and outputs use classes from 'sf' and\n'terra', and data downloading functions support complex 'CQL2'\nqueries using 'rstac'. alos_palsar_mask_function calculate_indices default_query_function filter_bands filter_platforms get_alos_palsar_imagery get_dem get_landsat_imagery get_naip_imagery get_sentinel1_imagery get_sentinel2_imagery get_stac_data landsat_mask_function landsat_platform_filter rsi_download_rasters rsi_gdal_config_options rsi_gdalwarp_options rsi_query_api sentinel2_mask_function sign_planetary_computer spectral_indices spectral_indices_url stack_rasters "
  },
  {
    "id": 1126,
    "package_name": "rnaturalearthhires",
    "title": "High Resolution World Vector Map Data from Natural Earth used in\nrnaturalearth",
    "description": "Facilitates mapping by making natural earth map data from\nhttp:// www.naturalearthdata.com/ more easily available to R\nusers. Focuses on vector data.",
    "version": "1.0.0.9000",
    "maintainer": "Andy South <southandy@gmail.com>",
    "author": "Andy South [aut, cre] (ORCID: <https://orcid.org/0000-0003-4051-6135>),\nSchramm Michael [aut],\nPhilippe Massicotte [aut] (ORCID:\n<https://orcid.org/0000-0002-5919-4116>)",
    "url": "https://docs.ropensci.org/rnaturalearthhires,\nhttps://github.com/ropensci/rnaturalearthhires",
    "bug_reports": "https://github.com/ropensci/rnaturalearthhires/issues",
    "repository": "",
    "exports": [],
    "topics": [],
    "score": 6.6847,
    "stars": 30,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rnaturalearthhires High Resolution World Vector Map Data from Natural Earth used in\nrnaturalearth Facilitates mapping by making natural earth map data from\nhttp:// www.naturalearthdata.com/ more easily available to R\nusers. Focuses on vector data.  "
  },
  {
    "id": 94,
    "package_name": "NLMR",
    "title": "Simulating Neutral Landscape Models",
    "description": "Provides neutral landscape models\n(<doi:10.1007/BF02275262>,\n<http://sci-hub.tw/10.1007/bf02275262>).  Neutral landscape\nmodels range from \"hard\" neutral models (completely random\ndistributed), to \"soft\" neutral models (definable spatial\ncharacteristics) and generate landscape patterns that are\nindependent of ecological processes. Thus, these patterns can\nbe used as null models in landscape ecology. 'NLMR' combines a\nlarge number of algorithms from other published software for\nsimulating neutral landscapes. The simulation results are\nobtained in a spatial data format (raster* objects from the\n'raster' package) and can, therefore, be used in any sort of\nraster data operation that is performed with standard\nobservation data.",
    "version": "1.1.1",
    "maintainer": "Marco Sciaini <marco.sciaini@posteo.net>",
    "author": "Marco Sciaini [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-3042-5435>),\nMatthias Fritsch [aut],\nMaximilian Hesselbarth [aut],\nCraig Simpkins [aut] (ORCID: <https://orcid.org/0000-0003-3212-1379>),\nC\u00e9dric Scherer [aut] (ORCID: <https://orcid.org/0000-0003-0465-2543>),\nSebastian Han\u00df [aut] (ORCID: <https://orcid.org/0000-0002-3990-4897>),\nLaura Graham [rev] (Laura reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/onboarding/issues/188),\nJeffrey Hollister [rev] (Jeffrey reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/onboarding/issues/188)",
    "url": "https://ropensci.github.io/NLMR/",
    "bug_reports": "https://github.com/ropensci/NLMR/issues/",
    "repository": "",
    "exports": [
      [
        "nlm_curds"
      ],
      [
        "nlm_distancegradient"
      ],
      [
        "nlm_edgegradient"
      ],
      [
        "nlm_fbm"
      ],
      [
        "nlm_gaussianfield"
      ],
      [
        "nlm_mosaicfield"
      ],
      [
        "nlm_mosaicgibbs"
      ],
      [
        "nlm_mosaictess"
      ],
      [
        "nlm_mpd"
      ],
      [
        "nlm_neigh"
      ],
      [
        "nlm_percolation"
      ],
      [
        "nlm_planargradient"
      ],
      [
        "nlm_random"
      ],
      [
        "nlm_randomcluster"
      ],
      [
        "nlm_randomrectangularcluster"
      ]
    ],
    "topics": [
      [
        "landscape-ecology"
      ],
      [
        "neutral-landscape-model"
      ],
      [
        "peer-reviewed"
      ],
      [
        "spatial"
      ],
      [
        "cpp"
      ]
    ],
    "score": 6.4407,
    "stars": 66,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "NLMR Simulating Neutral Landscape Models Provides neutral landscape models\n(<doi:10.1007/BF02275262>,\n<http://sci-hub.tw/10.1007/bf02275262>).  Neutral landscape\nmodels range from \"hard\" neutral models (completely random\ndistributed), to \"soft\" neutral models (definable spatial\ncharacteristics) and generate landscape patterns that are\nindependent of ecological processes. Thus, these patterns can\nbe used as null models in landscape ecology. 'NLMR' combines a\nlarge number of algorithms from other published software for\nsimulating neutral landscapes. The simulation results are\nobtained in a spatial data format (raster* objects from the\n'raster' package) and can, therefore, be used in any sort of\nraster data operation that is performed with standard\nobservation data. nlm_curds nlm_distancegradient nlm_edgegradient nlm_fbm nlm_gaussianfield nlm_mosaicfield nlm_mosaicgibbs nlm_mosaictess nlm_mpd nlm_neigh nlm_percolation nlm_planargradient nlm_random nlm_randomcluster nlm_randomrectangularcluster landscape-ecology neutral-landscape-model peer-reviewed spatial cpp"
  },
  {
    "id": 1400,
    "package_name": "unifir",
    "title": "A Unifying API for Calling the 'Unity' '3D' Video Game Engine",
    "description": "Functions for the creation and manipulation of scenes and\nobjects within the 'Unity' '3D' video game engine\n(<https://unity.com/>). Specific focuses include the creation\nand import of terrain data and 'GameObjects' as well as scene\nmanagement.",
    "version": "0.2.4.9000",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Michael Mahoney [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2402-304X>),\nWill Jones [rev] (Will reviewed the package (v. 0.2.0) for rOpenSci,\nsee <https://github.com/ropensci/software-review/issues/521>),\nTan Tran [rev] (Tan reviewed the package (v. 0.2.0) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/521>)",
    "url": "https://docs.ropensci.org/unifir/,\nhttps://github.com/ropensci/unifir",
    "bug_reports": "https://github.com/ropensci/unifir/issues",
    "repository": "",
    "exports": [
      [
        "action"
      ],
      [
        "add_default_player"
      ],
      [
        "add_default_tree"
      ],
      [
        "add_light"
      ],
      [
        "add_prop"
      ],
      [
        "add_texture"
      ],
      [
        "associate_coordinates"
      ],
      [
        "create_terrain"
      ],
      [
        "create_unity_project"
      ],
      [
        "find_unity"
      ],
      [
        "get_asset"
      ],
      [
        "import_asset"
      ],
      [
        "instantiate_prefab"
      ],
      [
        "load_png"
      ],
      [
        "load_scene"
      ],
      [
        "make_script"
      ],
      [
        "new_scene"
      ],
      [
        "read_raw"
      ],
      [
        "save_scene"
      ],
      [
        "set_active_scene"
      ],
      [
        "unifir_prop"
      ],
      [
        "unity_version"
      ],
      [
        "validate_path"
      ],
      [
        "validate_single_path"
      ],
      [
        "waiver"
      ]
    ],
    "topics": [
      [
        "unifir"
      ],
      [
        "unity"
      ],
      [
        "unity3d"
      ],
      [
        "visualization"
      ]
    ],
    "score": 6.3197,
    "stars": 29,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "unifir A Unifying API for Calling the 'Unity' '3D' Video Game Engine Functions for the creation and manipulation of scenes and\nobjects within the 'Unity' '3D' video game engine\n(<https://unity.com/>). Specific focuses include the creation\nand import of terrain data and 'GameObjects' as well as scene\nmanagement. action add_default_player add_default_tree add_light add_prop add_texture associate_coordinates create_terrain create_unity_project find_unity get_asset import_asset instantiate_prefab load_png load_scene make_script new_scene read_raw save_scene set_active_scene unifir_prop unity_version validate_path validate_single_path waiver unifir unity unity3d visualization"
  },
  {
    "id": 596,
    "package_name": "fuzzySim",
    "title": "Fuzzy Similarity in Species Distributions",
    "description": "Functions to compute fuzzy versions of species occurrence\npatterns based on presence-absence data (including inverse\ndistance interpolation, trend surface analysis, and\nprevalence-independent favourability obtained from probability\nof presence), as well as pair-wise fuzzy similarity (based on\nfuzzy logic versions of commonly used similarity indices) among\nthose occurrence patterns. Includes also functions for model\nconsensus and comparison (overlap and fuzzy similarity, fuzzy\nloss, fuzzy gain), and for data preparation, such as obtaining\nunique abbreviations of species names, defining the background\nregion, cleaning and gridding (thinning) point occurrence data\nonto raster maps, selecting among (pseudo)absences to address\nsurvey bias, converting species lists (long format) to\npresence-absence tables (wide format), transposing part of a\ndata frame, selecting relevant variables for models, assessing\nthe false discovery rate, or analysing and dealing with\nmulticollinearity. Initially described in Barbosa (2015)\n<doi:10.1111/2041-210X.12372>.",
    "version": "4.42",
    "maintainer": "A. Marcia Barbosa <ana.marcia.barbosa@gmail.com>",
    "author": "A. Marcia Barbosa [aut],\nAlba Estrada [ctb],\nPaul Melloy [ctb],\nJose Carlos Guerrero [fnd],\nA. Marcia Barbosa [cre]",
    "url": "http://fuzzysim.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "appendData"
      ],
      [
        "biasLayer"
      ],
      [
        "bioThreat"
      ],
      [
        "cleanCoords"
      ],
      [
        "corSelect"
      ],
      [
        "distMat"
      ],
      [
        "distPres"
      ],
      [
        "dms2dec"
      ],
      [
        "entropy"
      ],
      [
        "Fav"
      ],
      [
        "favClass"
      ],
      [
        "FDR"
      ],
      [
        "fuzSim"
      ],
      [
        "fuzzyConsensus"
      ],
      [
        "fuzzyOverlay"
      ],
      [
        "fuzzyRangeChange"
      ],
      [
        "getPreds"
      ],
      [
        "getRegion"
      ],
      [
        "gridRecords"
      ],
      [
        "integerCols"
      ],
      [
        "modelTrim"
      ],
      [
        "modOverlap"
      ],
      [
        "multConvert"
      ],
      [
        "multGLM"
      ],
      [
        "multicol"
      ],
      [
        "multTSA"
      ],
      [
        "pairwiseRangemaps"
      ],
      [
        "partialResp"
      ],
      [
        "percentTestData"
      ],
      [
        "prevalence"
      ],
      [
        "rangemapSim"
      ],
      [
        "rarity"
      ],
      [
        "selectAbsences"
      ],
      [
        "sharedFav"
      ],
      [
        "simFromSetOps"
      ],
      [
        "simMat"
      ],
      [
        "spCodes"
      ],
      [
        "splist2presabs"
      ],
      [
        "stepByStep"
      ],
      [
        "stepwise"
      ],
      [
        "summaryWald"
      ],
      [
        "timer"
      ],
      [
        "transpose"
      ],
      [
        "triMatInd"
      ],
      [
        "vulnerability"
      ]
    ],
    "topics": [],
    "score": 6.2369,
    "stars": 2,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "fuzzySim Fuzzy Similarity in Species Distributions Functions to compute fuzzy versions of species occurrence\npatterns based on presence-absence data (including inverse\ndistance interpolation, trend surface analysis, and\nprevalence-independent favourability obtained from probability\nof presence), as well as pair-wise fuzzy similarity (based on\nfuzzy logic versions of commonly used similarity indices) among\nthose occurrence patterns. Includes also functions for model\nconsensus and comparison (overlap and fuzzy similarity, fuzzy\nloss, fuzzy gain), and for data preparation, such as obtaining\nunique abbreviations of species names, defining the background\nregion, cleaning and gridding (thinning) point occurrence data\nonto raster maps, selecting among (pseudo)absences to address\nsurvey bias, converting species lists (long format) to\npresence-absence tables (wide format), transposing part of a\ndata frame, selecting relevant variables for models, assessing\nthe false discovery rate, or analysing and dealing with\nmulticollinearity. Initially described in Barbosa (2015)\n<doi:10.1111/2041-210X.12372>. appendData biasLayer bioThreat cleanCoords corSelect distMat distPres dms2dec entropy Fav favClass FDR fuzSim fuzzyConsensus fuzzyOverlay fuzzyRangeChange getPreds getRegion gridRecords integerCols modelTrim modOverlap multConvert multGLM multicol multTSA pairwiseRangemaps partialResp percentTestData prevalence rangemapSim rarity selectAbsences sharedFav simFromSetOps simMat spCodes splist2presabs stepByStep stepwise summaryWald timer transpose triMatInd vulnerability "
  },
  {
    "id": 1038,
    "package_name": "quadkeyr",
    "title": "Generate Raster Images from QuadKey-Identified Datasets",
    "description": "A set of functions of increasing complexity allows users\nto (1) convert QuadKey-identified datasets, based on\n'Microsoft's Bing Maps Tile System', into Simple Features data\nframes, (2) transform Simple Features data frames into rasters,\nand (3) process multiple 'Meta' ('Facebook') QuadKey-identified\nhuman mobility files directly into raster files. For more\ndetails, see D\u2019Andrea et al. (2024) <doi:10.21105/joss.06500>.",
    "version": "0.1.0",
    "maintainer": "Florencia D'Andrea <florencia.dandrea@gmail.com>",
    "author": "Florencia D'Andrea [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0041-097X>),\nPilar Fernandez [aut] (ORCID: <https://orcid.org/0000-0001-8645-2267>),\nMaria Paula Caldas [rev] (ORCID:\n<https://orcid.org/0000-0002-1938-6471>, Maria Paula Caldas\nreviewed the package (v. 0.0.0.9000) for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/619),\nVincent van Hees [rev] (ORCID: <https://orcid.org/0000-0003-0182-9008>,\nVincent van Hees reviewed the package (v. 0.0.0.9000) for rOpenSci,\nsee https://github.com/ropensci/software-review/issues/619),\nAndrew Pulsipher [ctb] (ORCID: <https://orcid.org/0000-0002-0773-3210>),\nCDC's Center for Forecasting and Outbreak Analytics [fnd] (This project\nwas made possible by cooperative agreement CDC-RFA-FT-23-0069\n(grant # NU38FT000009-01-00) from the CDC's Center for Forecasting\nand Outbreak Analytics. Its contents are solely the responsibility\nof the authors and do not necessarily represent the official views\nof the Centers for Disease Control and Prevention.),\nMIDAS-NIH COVID-19 urgent grant program [fnd],\nPaul G. Allen School for Global Health, Washington State University\n[cph]",
    "url": "https://docs.ropensci.org/quadkeyr/,\nhttps://github.com/ropensci/quadkeyr",
    "bug_reports": "https://github.com/ropensci/quadkeyr/issues",
    "repository": "",
    "exports": [
      [
        "add_regular_polygon_grid"
      ],
      [
        "apply_weekly_lag"
      ],
      [
        "create_qk_grid"
      ],
      [
        "create_stars_raster"
      ],
      [
        "format_fb_data"
      ],
      [
        "get_qk_coord"
      ],
      [
        "get_regular_polygon_grid"
      ],
      [
        "grid_to_polygon"
      ],
      [
        "ground_res"
      ],
      [
        "latlong_to_pixelXY"
      ],
      [
        "latlong_to_quadkey"
      ],
      [
        "mapscale"
      ],
      [
        "mapsize"
      ],
      [
        "missing_combinations"
      ],
      [
        "pixelXY_to_latlong"
      ],
      [
        "pixelXY_to_tileXY"
      ],
      [
        "polygon_to_raster"
      ],
      [
        "qkmap_app"
      ],
      [
        "quadkey_df_to_polygon"
      ],
      [
        "quadkey_to_latlong"
      ],
      [
        "quadkey_to_polygon"
      ],
      [
        "quadkey_to_tileXY"
      ],
      [
        "read_fb_mobility_files"
      ],
      [
        "regular_qk_grid"
      ],
      [
        "tileXY_to_pixelXY"
      ],
      [
        "tileXY_to_quadkey"
      ]
    ],
    "topics": [
      [
        "geospatial"
      ],
      [
        "quadkey"
      ],
      [
        "raster"
      ],
      [
        "tilemap"
      ]
    ],
    "score": 6.2095,
    "stars": 15,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "quadkeyr Generate Raster Images from QuadKey-Identified Datasets A set of functions of increasing complexity allows users\nto (1) convert QuadKey-identified datasets, based on\n'Microsoft's Bing Maps Tile System', into Simple Features data\nframes, (2) transform Simple Features data frames into rasters,\nand (3) process multiple 'Meta' ('Facebook') QuadKey-identified\nhuman mobility files directly into raster files. For more\ndetails, see D\u2019Andrea et al. (2024) <doi:10.21105/joss.06500>. add_regular_polygon_grid apply_weekly_lag create_qk_grid create_stars_raster format_fb_data get_qk_coord get_regular_polygon_grid grid_to_polygon ground_res latlong_to_pixelXY latlong_to_quadkey mapscale mapsize missing_combinations pixelXY_to_latlong pixelXY_to_tileXY polygon_to_raster qkmap_app quadkey_df_to_polygon quadkey_to_latlong quadkey_to_polygon quadkey_to_tileXY read_fb_mobility_files regular_qk_grid tileXY_to_pixelXY tileXY_to_quadkey geospatial quadkey raster tilemap"
  },
  {
    "id": 524,
    "package_name": "epiCo",
    "title": "Statistical and Viz Tools for Vector-Borne Diseases in Colombia",
    "description": "Provides statistical and visualization tools for the\nanalysis of demographic indicators, and spatio-temporal\nbehavior and characterization of outbreaks of vector-borne\ndiseases (VBDs) in Colombia. It implements travel times\nestimated in Bravo-Vega C., Santos-Vega M., & Cordovez J.M.\n(2022), and the endemic channel method (Bortman, M.  (1999)\n<https://iris.paho.org/handle/10665.2/8562>).",
    "version": "1.0.1.9000",
    "maintainer": "Juan D. Uma\u00f1a <jd.umana10@uniandes.edu.co>",
    "author": "Juan D. Uma\u00f1a [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-0316-6164>),\nJuan Montenegro-Torres [aut] (ORCID:\n<https://orcid.org/0000-0003-2755-4743>),\nJulian Otero [aut] (ORCID: <https://orcid.org/0009-0006-0429-7747>),\nHugo Gruson [ctb] (ORCID: <https://orcid.org/0000-0002-4094-1476>)",
    "url": "https://epiverse-trace.github.io/epiCo/,\nhttps://github.com/epiverse-trace/epiCo",
    "bug_reports": "https://github.com/epiverse-trace/epiCo/issues",
    "repository": "",
    "exports": [
      [
        "age_risk"
      ],
      [
        "describe_ethnicity"
      ],
      [
        "describe_occupation"
      ],
      [
        "endemic_channel"
      ],
      [
        "epi_calendar"
      ],
      [
        "geometric_mean"
      ],
      [
        "geometric_sd"
      ],
      [
        "incidence_rate"
      ],
      [
        "morans_index"
      ],
      [
        "neighborhoods"
      ],
      [
        "population_pyramid"
      ]
    ],
    "topics": [
      [
        "colombia"
      ],
      [
        "decision-support"
      ],
      [
        "demographics"
      ],
      [
        "epiverse"
      ],
      [
        "outbreak-analysis"
      ],
      [
        "sdg-3"
      ],
      [
        "spatio-temporal-analysis"
      ],
      [
        "vector-borne-diseases"
      ]
    ],
    "score": 6.1931,
    "stars": 13,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "epiCo Statistical and Viz Tools for Vector-Borne Diseases in Colombia Provides statistical and visualization tools for the\nanalysis of demographic indicators, and spatio-temporal\nbehavior and characterization of outbreaks of vector-borne\ndiseases (VBDs) in Colombia. It implements travel times\nestimated in Bravo-Vega C., Santos-Vega M., & Cordovez J.M.\n(2022), and the endemic channel method (Bortman, M.  (1999)\n<https://iris.paho.org/handle/10665.2/8562>). age_risk describe_ethnicity describe_occupation endemic_channel epi_calendar geometric_mean geometric_sd incidence_rate morans_index neighborhoods population_pyramid colombia decision-support demographics epiverse outbreak-analysis sdg-3 spatio-temporal-analysis vector-borne-diseases"
  },
  {
    "id": 984,
    "package_name": "plainview",
    "title": "Plot Raster Images Interactively on a Plain HTML Canvas",
    "description": "Provides methods for plotting potentially large (raster)\nimages interactively on a plain HTML canvas. In contrast to\npackage 'mapview' data are plotted without background map, but\ndata can be projected to any spatial coordinate reference\nsystem. Supports plotting of classes 'RasterLayer',\n'RasterStack', 'RasterBrick' (from package 'raster') as well as\n'png' files located on disk. Interactivity includes zooming,\npanning, and mouse location information. In case of multi-layer\n'RasterStacks' or 'RasterBricks', RGB image plots are created\n(similar to 'raster::plotRGB' - but interactive).",
    "version": "0.2.2",
    "maintainer": "Tim Appelhans <tim.appelhans@gmail.com>",
    "author": "Tim Appelhans [cre, aut],\nStefan Woellauer [aut]",
    "url": "https://r-spatial.github.io/plainview/,\nhttps://github.com/r-spatial/plainview",
    "bug_reports": "https://github.com/r-spatial/plainview/issues",
    "repository": "",
    "exports": [
      [
        "plainview"
      ],
      [
        "plainView"
      ],
      [
        "plainViewOutput"
      ],
      [
        "renderPlainView"
      ]
    ],
    "topics": [],
    "score": 5.9959,
    "stars": 13,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "plainview Plot Raster Images Interactively on a Plain HTML Canvas Provides methods for plotting potentially large (raster)\nimages interactively on a plain HTML canvas. In contrast to\npackage 'mapview' data are plotted without background map, but\ndata can be projected to any spatial coordinate reference\nsystem. Supports plotting of classes 'RasterLayer',\n'RasterStack', 'RasterBrick' (from package 'raster') as well as\n'png' files located on disk. Interactivity includes zooming,\npanning, and mouse location information. In case of multi-layer\n'RasterStacks' or 'RasterBricks', RGB image plots are created\n(similar to 'raster::plotRGB' - but interactive). plainview plainView plainViewOutput renderPlainView "
  },
  {
    "id": 658,
    "package_name": "grImport2",
    "title": "Importing 'SVG' Graphics",
    "description": "Functions for importing external vector images and drawing\nthem as part of 'R' plots.  This package is different from the\n'grImport' package because, where that package imports\n'PostScript' format images, this package imports 'SVG' format\nimages.  Furthermore, this package imports a specific subset of\n'SVG', so external images must be preprocessed using a package\nlike 'rsvg' to produce 'SVG' that this package can import.\n'SVG' features that are not supported by 'R' graphics, e.g.,\ngradient fills, can be imported and then exported via the\n'gridSVG' package.",
    "version": "0.3-3",
    "maintainer": "Paul Murrell <paul@stat.auckland.ac.nz>",
    "author": "Simon Potter [aut],\nPaul Murrell [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-3224-8858>)",
    "url": "https://r-forge.r-project.org/projects/grimport/,https://stattech.wordpress.fos.auckland.ac.nz/2013/09/,https://stattech.wordpress.fos.auckland.ac.nz/2016/08/30/2016-11-the-butterfly-affectation/,https://stattech.wordpress.fos.auckland.ac.nz/2019/11/25/2019-02-svg-in-svg-out/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "applyTransform"
      ],
      [
        "closePath"
      ],
      [
        "curveTo"
      ],
      [
        "getDef"
      ],
      [
        "grid.picture"
      ],
      [
        "grid.symbols"
      ],
      [
        "grobify"
      ],
      [
        "lineTo"
      ],
      [
        "moveTo"
      ],
      [
        "pictureGrob"
      ],
      [
        "readPicture"
      ],
      [
        "setDef"
      ],
      [
        "symbolsGrob"
      ]
    ],
    "topics": [],
    "score": 5.7391,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "grImport2 Importing 'SVG' Graphics Functions for importing external vector images and drawing\nthem as part of 'R' plots.  This package is different from the\n'grImport' package because, where that package imports\n'PostScript' format images, this package imports 'SVG' format\nimages.  Furthermore, this package imports a specific subset of\n'SVG', so external images must be preprocessed using a package\nlike 'rsvg' to produce 'SVG' that this package can import.\n'SVG' features that are not supported by 'R' graphics, e.g.,\ngradient fills, can be imported and then exported via the\n'gridSVG' package. applyTransform closePath curveTo getDef grid.picture grid.symbols grobify lineTo moveTo pictureGrob readPicture setDef symbolsGrob "
  },
  {
    "id": 619,
    "package_name": "getCRUCLdata",
    "title": "'CRU' 'CL' v. 2.0 Climatology Client",
    "description": "Provides functions that automate downloading and importing\nUniversity of East Anglia Climate Research Unit ('CRU') 'CL' v.\n2.0 climatology data, facilitates the calculation of minimum\ntemperature and maximum temperature and formats the data into a\ndata.table object or a list of 'terra' 'rast' objects for use.\n'CRU' 'CL' v. 2.0 data are a gridded climatology of 1961-1990\nmonthly means released in 2002 and cover all land areas\n(excluding Antarctica) at 10 arc minutes (0.1666667 degree)\nresolution.  For more information see the description of the\ndata provided by the University of East Anglia Climate Research\nUnit, <https://crudata.uea.ac.uk/cru/data/hrg/tmc/readme.txt>.",
    "version": "1.0.3",
    "maintainer": "Adam H. Sparks <adamhsparks@gmail.com>",
    "author": "Adam H. Sparks [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0061-8359>),\nCurtin University of Technology [cph] (Provided support through Adam\nSparks's time.),\nGrains Research and Development Corporation [cph] (GRDC Project\nCUR2210-005OPX (AAGI-CU))",
    "url": "https://github.com/ropensci/getCRUCLdata,\nhttps://docs.ropensci.org/getCRUCLdata/",
    "bug_reports": "https://github.com/ropensci/getCRUCLdata/issues",
    "repository": "",
    "exports": [
      [
        "create_cru_df"
      ],
      [
        "create_CRU_df"
      ],
      [
        "create_cru_stack"
      ],
      [
        "create_CRU_stack"
      ],
      [
        "get_cru_df"
      ],
      [
        "get_CRU_df"
      ],
      [
        "get_cru_stack"
      ],
      [
        "get_CRU_stack"
      ],
      [
        "manage_cache"
      ]
    ],
    "topics": [
      [
        "anglia-cru"
      ],
      [
        "climate-data"
      ],
      [
        "cru-cl2"
      ],
      [
        "temperature"
      ],
      [
        "rainfall"
      ],
      [
        "elevation"
      ],
      [
        "data-access"
      ],
      [
        "wind"
      ],
      [
        "relative-humidity"
      ],
      [
        "solar-radiation"
      ],
      [
        "diurnal-temperature"
      ],
      [
        "frost"
      ],
      [
        "cru"
      ],
      [
        "peer-reviewed"
      ]
    ],
    "score": 5.5649,
    "stars": 17,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "getCRUCLdata 'CRU' 'CL' v. 2.0 Climatology Client Provides functions that automate downloading and importing\nUniversity of East Anglia Climate Research Unit ('CRU') 'CL' v.\n2.0 climatology data, facilitates the calculation of minimum\ntemperature and maximum temperature and formats the data into a\ndata.table object or a list of 'terra' 'rast' objects for use.\n'CRU' 'CL' v. 2.0 data are a gridded climatology of 1961-1990\nmonthly means released in 2002 and cover all land areas\n(excluding Antarctica) at 10 arc minutes (0.1666667 degree)\nresolution.  For more information see the description of the\ndata provided by the University of East Anglia Climate Research\nUnit, <https://crudata.uea.ac.uk/cru/data/hrg/tmc/readme.txt>. create_cru_df create_CRU_df create_cru_stack create_CRU_stack get_cru_df get_CRU_df get_cru_stack get_CRU_stack manage_cache anglia-cru climate-data cru-cl2 temperature rainfall elevation data-access wind relative-humidity solar-radiation diurnal-temperature frost cru peer-reviewed"
  },
  {
    "id": 661,
    "package_name": "grainchanger",
    "title": "Moving-Window and Direct Data Aggregation",
    "description": "Data aggregation via moving window or direct methods.\nAggregate a fine-resolution raster to a grid. The moving window\nmethod smooths the surface using a specified function within a\nmoving window of a specified size and shape prior to\naggregation. The direct method simply aggregates to the grid\nusing the specified function.",
    "version": "0.3.2",
    "maintainer": "Laura Graham <LauraJaneEGraham@gmail.com>",
    "author": "Laura Graham [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-3611-7281>),\nFelix Eigenbrod [ctb] (Input on initial conceptual development),\nMarco Sciaini [ctb] (Input on package development and structure)",
    "url": "https://docs.ropensci.org/grainchanger/,\nhttps://github.com/ropensci/grainchanger",
    "bug_reports": "https://github.com/ropensci/grainchanger/issues",
    "repository": "",
    "exports": [
      [
        "create_torus"
      ],
      [
        "mean"
      ],
      [
        "nomove_agg"
      ],
      [
        "prop"
      ],
      [
        "shdi"
      ],
      [
        "shei"
      ],
      [
        "var_range"
      ],
      [
        "winmove"
      ],
      [
        "winmove_agg"
      ]
    ],
    "topics": [],
    "score": 5.5271,
    "stars": 51,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "grainchanger Moving-Window and Direct Data Aggregation Data aggregation via moving window or direct methods.\nAggregate a fine-resolution raster to a grid. The moving window\nmethod smooths the surface using a specified function within a\nmoving window of a specified size and shape prior to\naggregation. The direct method simply aggregates to the grid\nusing the specified function. create_torus mean nomove_agg prop shdi shei var_range winmove winmove_agg "
  },
  {
    "id": 394,
    "package_name": "concstats",
    "title": "Market Structure, Concentration and Inequality Measures",
    "description": "Based on individual market shares of all participants in a\nmarket or space, the package offers a set of different\nstructural and concentration measures frequently - and not so\nfrequently - used in research and in practice. Measures can be\ncalculated in groups or individually. The calculated measure or\nthe resulting vector in table format should help practitioners\nmake more informed decisions. Methods used in this package are\nfrom: 1.  Chang, E. J., Guerra, S. M., de Souza Penaloza, R. A.\n& Tabak, B. M. (2005) \"Banking concentration: the Brazilian\ncase\". 2.  Cobham, A. and A. Summer (2013). \"Is It All About\nthe Tails? The Palma Measure of Income Inequality\". 3.  Garcia\nAlba Idunate, P. (1994). \"Un Indice de dominancia para el\nanalisis de la estructura de los mercados\". 4.  Ginevicius, R.\nand S. Cirba (2009). \"Additive measurement of market\nconcentration\" <doi:10.3846/1611-1699.2009.10.191-198>. 5.\nHerfindahl, O. C. (1950), \"Concentration in the steel industry\"\n(PhD thesis). 6.  Hirschmann, A. O. (1945), \"National power and\nstructure of foreign trade\". 7.  Melnik, A., O. Shy, and R.\nStenbacka (2008), \"Assessing market dominance\"\n<doi:10.1016/j.jebo.2008.03.010>. 8.  Palma, J. G. (2006).\n\"Globalizing Inequality: 'Centrifugal' and 'Centripetal' Forces\nat Work\". 9.  Shannon, C. E. (1948). \"A Mathematical Theory of\nCommunication\". 10.  Simpson, E. H. (1949). \"Measurement of\nDiversity\" <doi:10.1038/163688a0>.",
    "version": "0.2.1",
    "maintainer": "Andreas Schneider <schneiderconsultingpy@gmail.com>",
    "author": "Andreas Schneider [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-5630-1097>),\nSebastian Wojcik [rev],\nChristopher T. Kenny [rev]",
    "url": "https://github.com/ropensci/concstats/,\nhttps://docs.ropensci.org/concstats/ (website)",
    "bug_reports": "https://github.com/ropensci/concstats/issues/",
    "repository": "",
    "exports": [
      [
        "concstats_all_comp"
      ],
      [
        "concstats_all_inequ"
      ],
      [
        "concstats_all_mstruct"
      ],
      [
        "concstats_comp"
      ],
      [
        "concstats_concstats"
      ],
      [
        "concstats_dom"
      ],
      [
        "concstats_entropy"
      ],
      [
        "concstats_firm"
      ],
      [
        "concstats_gini"
      ],
      [
        "concstats_grs"
      ],
      [
        "concstats_hhi"
      ],
      [
        "concstats_hhi_d"
      ],
      [
        "concstats_hhi_min"
      ],
      [
        "concstats_inequ"
      ],
      [
        "concstats_mstruct"
      ],
      [
        "concstats_nrs_eq"
      ],
      [
        "concstats_palma"
      ],
      [
        "concstats_shares"
      ],
      [
        "concstats_simpson"
      ],
      [
        "concstats_sten"
      ],
      [
        "concstats_top"
      ],
      [
        "concstats_top_df"
      ],
      [
        "concstats_top3"
      ],
      [
        "concstats_top3_df"
      ],
      [
        "concstats_top5"
      ],
      [
        "concstats_top5_df"
      ]
    ],
    "topics": [
      [
        "business-analytics"
      ],
      [
        "competition"
      ],
      [
        "concentration"
      ],
      [
        "diversity"
      ],
      [
        "inequality"
      ],
      [
        "package-development"
      ]
    ],
    "score": 5.4683,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "concstats Market Structure, Concentration and Inequality Measures Based on individual market shares of all participants in a\nmarket or space, the package offers a set of different\nstructural and concentration measures frequently - and not so\nfrequently - used in research and in practice. Measures can be\ncalculated in groups or individually. The calculated measure or\nthe resulting vector in table format should help practitioners\nmake more informed decisions. Methods used in this package are\nfrom: 1.  Chang, E. J., Guerra, S. M., de Souza Penaloza, R. A.\n& Tabak, B. M. (2005) \"Banking concentration: the Brazilian\ncase\". 2.  Cobham, A. and A. Summer (2013). \"Is It All About\nthe Tails? The Palma Measure of Income Inequality\". 3.  Garcia\nAlba Idunate, P. (1994). \"Un Indice de dominancia para el\nanalisis de la estructura de los mercados\". 4.  Ginevicius, R.\nand S. Cirba (2009). \"Additive measurement of market\nconcentration\" <doi:10.3846/1611-1699.2009.10.191-198>. 5.\nHerfindahl, O. C. (1950), \"Concentration in the steel industry\"\n(PhD thesis). 6.  Hirschmann, A. O. (1945), \"National power and\nstructure of foreign trade\". 7.  Melnik, A., O. Shy, and R.\nStenbacka (2008), \"Assessing market dominance\"\n<doi:10.1016/j.jebo.2008.03.010>. 8.  Palma, J. G. (2006).\n\"Globalizing Inequality: 'Centrifugal' and 'Centripetal' Forces\nat Work\". 9.  Shannon, C. E. (1948). \"A Mathematical Theory of\nCommunication\". 10.  Simpson, E. H. (1949). \"Measurement of\nDiversity\" <doi:10.1038/163688a0>. concstats_all_comp concstats_all_inequ concstats_all_mstruct concstats_comp concstats_concstats concstats_dom concstats_entropy concstats_firm concstats_gini concstats_grs concstats_hhi concstats_hhi_d concstats_hhi_min concstats_inequ concstats_mstruct concstats_nrs_eq concstats_palma concstats_shares concstats_simpson concstats_sten concstats_top concstats_top_df concstats_top3 concstats_top3_df concstats_top5 concstats_top5_df business-analytics competition concentration diversity inequality package-development"
  },
  {
    "id": 354,
    "package_name": "cereal",
    "title": "Serialize 'vctrs' Objects to 'JSON'",
    "description": "The 'vctrs' package provides a concept of vector prototype\nthat can be especially useful when deploying models and code.\nSerialize these object prototypes to 'JSON' so they can be used\nto check and coerce data in production systems, and deserialize\n'JSON' back to the correct object prototypes.",
    "version": "0.1.0.9000",
    "maintainer": "Julia Silge <julia.silge@posit.co>",
    "author": "Julia Silge [cre, aut] (ORCID: <https://orcid.org/0000-0002-3671-836X>),\nDavis Vaughan [aut],\nPosit Software, PBC [cph, fnd]",
    "url": "https://r-lib.github.io/cereal/, https://github.com/r-lib/cereal/",
    "bug_reports": "https://github.com/r-lib/cereal/issues",
    "repository": "",
    "exports": [
      [
        "cereal_decode"
      ],
      [
        "cereal_details"
      ],
      [
        "cereal_encode"
      ],
      [
        "cereal_from_json"
      ],
      [
        "cereal_to_json"
      ]
    ],
    "topics": [],
    "score": 5.4005,
    "stars": 26,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "cereal Serialize 'vctrs' Objects to 'JSON' The 'vctrs' package provides a concept of vector prototype\nthat can be especially useful when deploying models and code.\nSerialize these object prototypes to 'JSON' so they can be used\nto check and coerce data in production systems, and deserialize\n'JSON' back to the correct object prototypes. cereal_decode cereal_details cereal_encode cereal_from_json cereal_to_json "
  },
  {
    "id": 1235,
    "package_name": "slideview",
    "title": "Compare Raster Images Side by Side with a Slider",
    "description": "Create a side-by-side view of raster(image)s with an\ninteractive slider to switch between regions of the images.\nThis can be especially useful for image comparison of the same\nregion at different time stamps.",
    "version": "0.2.1",
    "maintainer": "Tim Appelhans <tim.appelhans@gmail.com>",
    "author": "Tim Appelhans [cre, aut],\nStefan Woellauer [aut]",
    "url": "https://r-spatial.github.io/slideview/,\nhttps://github.com/r-spatial/slideview",
    "bug_reports": "https://github.com/r-spatial/slideview/issues",
    "repository": "",
    "exports": [
      [
        "slideview"
      ],
      [
        "slideView"
      ]
    ],
    "topics": [],
    "score": 5.2175,
    "stars": 25,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "slideview Compare Raster Images Side by Side with a Slider Create a side-by-side view of raster(image)s with an\ninteractive slider to switch between regions of the images.\nThis can be especially useful for image comparison of the same\nregion at different time stamps. slideview slideView "
  },
  {
    "id": 1239,
    "package_name": "smapr",
    "title": "Acquisition and Processing of NASA Soil Moisture Active-Passive\n(SMAP) Data",
    "description": "Facilitates programmatic access to NASA Soil Moisture\nActive Passive (SMAP) data with R. It includes functions to\nsearch for, acquire, and extract SMAP data.",
    "version": "0.3.0",
    "maintainer": "Maxwell Joseph <maxwellbjoseph@gmail.com>",
    "author": "Maxwell Joseph [aut, cre, cph],\nMatthew Oakley [aut],\nZachary Schira [aut]",
    "url": "https://docs.ropensci.org/smapr/,\nhttps://github.com/ropensci/smapr",
    "bug_reports": "https://github.com/ropensci/smapr/issues",
    "repository": "",
    "exports": [
      [
        "download_smap"
      ],
      [
        "extract_smap"
      ],
      [
        "find_smap"
      ],
      [
        "list_smap"
      ],
      [
        "set_smap_credentials"
      ]
    ],
    "topics": [
      [
        "acquisition"
      ],
      [
        "extract-data"
      ],
      [
        "nasa"
      ],
      [
        "peer-reviewed"
      ],
      [
        "raster"
      ],
      [
        "smap-data"
      ],
      [
        "soil-mapping"
      ],
      [
        "soil-moisture"
      ],
      [
        "soil-moisture-sensor"
      ]
    ],
    "score": 5.1156,
    "stars": 87,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "smapr Acquisition and Processing of NASA Soil Moisture Active-Passive\n(SMAP) Data Facilitates programmatic access to NASA Soil Moisture\nActive Passive (SMAP) data with R. It includes functions to\nsearch for, acquire, and extract SMAP data. download_smap extract_smap find_smap list_smap set_smap_credentials acquisition extract-data nasa peer-reviewed raster smap-data soil-mapping soil-moisture soil-moisture-sensor"
  },
  {
    "id": 1278,
    "package_name": "stops",
    "title": "Structure Optimized Proximity Scaling",
    "description": "Methods that use flexible variants of multidimensional\nscaling (MDS) which incorporate parametric nonlinear distance\ntransformations and trade-off the goodness-of-fit fit with\nstructure considerations to find optimal hyperparameters, also\nknown as structure optimized proximity scaling (STOPS) (Rusch,\nMair & Hornik, 2023,<doi:10.1007/s11222-022-10197-w>). The\npackage contains various functions, wrappers, methods and\nclasses for fitting, plotting and displaying different 1-way\nMDS models with ratio, interval, ordinal optimal scaling in a\nSTOPS framework. These cover essentially the functionality of\nthe package smacofx, including Torgerson (classical) scaling\nwith power transformations of dissimilarities, SMACOF MDS with\npowers of dissimilarities, Sammon mapping with powers of\ndissimilarities, elastic scaling with powers of\ndissimilarities, spherical SMACOF with powers of\ndissimilarities, (ALSCAL) s-stress MDS with powers of\ndissimilarities, r-stress MDS, MDS with powers of\ndissimilarities and configuration distances, elastic scaling\npowers of dissimilarities and configuration distances, Sammon\nmapping powers of dissimilarities and configuration distances,\npower stress MDS (POST-MDS), approximate power stress, Box-Cox\nMDS, local MDS, Isomap, curvilinear component analysis (CLCA),\ncurvilinear distance analysis (CLDA) and sparsified (power)\nmultidimensional scaling and (power) multidimensional distance\nanalysis (experimental models from smacofx influenced by CLCA).\nAll of these models can also be fit by optimizing over\nhyperparameters based on goodness-of-fit fit only (i.e., no\nstructure considerations). The package further contains\nfunctions for optimization, specifically the adaptive\nLuus-Jaakola algorithm and a wrapper for Bayesian optimization\nwith treed Gaussian process with jumps to linear models, and\nfunctions for various c-structuredness indices. Hyperparameter\noptimization can be done with a number of techniques but we\nrecommend either Bayesian optimization or particle swarm. For\nusing \"Kriging\", users need to install a version of the\narchived 'DiceOptim' R package.",
    "version": "1.10-1",
    "maintainer": "Thomas Rusch <thomas.rusch@wu.ac.at>",
    "author": "Thomas Rusch [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7773-2096>),\nPatrick Mair [aut] (ORCID: <https://orcid.org/0000-0003-0100-6511>),\nKurt Hornik [ctb] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "https://r-forge.r-project.org/projects/stops/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "c_association"
      ],
      [
        "c_clumpiness"
      ],
      [
        "c_clusteredness"
      ],
      [
        "c_complexity"
      ],
      [
        "c_convexity"
      ],
      [
        "c_dependence"
      ],
      [
        "c_faithfulness"
      ],
      [
        "c_functionality"
      ],
      [
        "c_hierarchy"
      ],
      [
        "c_inequality"
      ],
      [
        "c_linearity"
      ],
      [
        "c_manifoldness"
      ],
      [
        "c_nonmonotonicity"
      ],
      [
        "c_outlying"
      ],
      [
        "c_regularity"
      ],
      [
        "c_shepardness"
      ],
      [
        "c_skinniness"
      ],
      [
        "c_sparsity"
      ],
      [
        "c_striatedness"
      ],
      [
        "c_stringiness"
      ],
      [
        "ljoptim"
      ],
      [
        "stop_bcmds"
      ],
      [
        "stop_clca"
      ],
      [
        "stop_cldae"
      ],
      [
        "stop_cldak"
      ],
      [
        "stop_elastic"
      ],
      [
        "stop_isomap1"
      ],
      [
        "stop_isomap2"
      ],
      [
        "stop_lmds"
      ],
      [
        "stop_powerelastic"
      ],
      [
        "stop_powermds"
      ],
      [
        "stop_powersammon"
      ],
      [
        "stop_powerstress"
      ],
      [
        "stop_rstress"
      ],
      [
        "stop_sammon"
      ],
      [
        "stop_sammon2"
      ],
      [
        "stop_smacofSphere"
      ],
      [
        "stop_smacofSym"
      ],
      [
        "stop_smddae"
      ],
      [
        "stop_smddak"
      ],
      [
        "stop_smds"
      ],
      [
        "stop_spmddae"
      ],
      [
        "stop_spmddak"
      ],
      [
        "stop_spmds"
      ],
      [
        "stop_sstress"
      ],
      [
        "stoploss"
      ],
      [
        "stops"
      ],
      [
        "tgpoptim"
      ]
    ],
    "topics": [
      [
        "openjdk"
      ]
    ],
    "score": 4.4886,
    "stars": 1,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "stops Structure Optimized Proximity Scaling Methods that use flexible variants of multidimensional\nscaling (MDS) which incorporate parametric nonlinear distance\ntransformations and trade-off the goodness-of-fit fit with\nstructure considerations to find optimal hyperparameters, also\nknown as structure optimized proximity scaling (STOPS) (Rusch,\nMair & Hornik, 2023,<doi:10.1007/s11222-022-10197-w>). The\npackage contains various functions, wrappers, methods and\nclasses for fitting, plotting and displaying different 1-way\nMDS models with ratio, interval, ordinal optimal scaling in a\nSTOPS framework. These cover essentially the functionality of\nthe package smacofx, including Torgerson (classical) scaling\nwith power transformations of dissimilarities, SMACOF MDS with\npowers of dissimilarities, Sammon mapping with powers of\ndissimilarities, elastic scaling with powers of\ndissimilarities, spherical SMACOF with powers of\ndissimilarities, (ALSCAL) s-stress MDS with powers of\ndissimilarities, r-stress MDS, MDS with powers of\ndissimilarities and configuration distances, elastic scaling\npowers of dissimilarities and configuration distances, Sammon\nmapping powers of dissimilarities and configuration distances,\npower stress MDS (POST-MDS), approximate power stress, Box-Cox\nMDS, local MDS, Isomap, curvilinear component analysis (CLCA),\ncurvilinear distance analysis (CLDA) and sparsified (power)\nmultidimensional scaling and (power) multidimensional distance\nanalysis (experimental models from smacofx influenced by CLCA).\nAll of these models can also be fit by optimizing over\nhyperparameters based on goodness-of-fit fit only (i.e., no\nstructure considerations). The package further contains\nfunctions for optimization, specifically the adaptive\nLuus-Jaakola algorithm and a wrapper for Bayesian optimization\nwith treed Gaussian process with jumps to linear models, and\nfunctions for various c-structuredness indices. Hyperparameter\noptimization can be done with a number of techniques but we\nrecommend either Bayesian optimization or particle swarm. For\nusing \"Kriging\", users need to install a version of the\narchived 'DiceOptim' R package. c_association c_clumpiness c_clusteredness c_complexity c_convexity c_dependence c_faithfulness c_functionality c_hierarchy c_inequality c_linearity c_manifoldness c_nonmonotonicity c_outlying c_regularity c_shepardness c_skinniness c_sparsity c_striatedness c_stringiness ljoptim stop_bcmds stop_clca stop_cldae stop_cldak stop_elastic stop_isomap1 stop_isomap2 stop_lmds stop_powerelastic stop_powermds stop_powersammon stop_powerstress stop_rstress stop_sammon stop_sammon2 stop_smacofSphere stop_smacofSym stop_smddae stop_smddak stop_smds stop_spmddae stop_spmddak stop_spmds stop_sstress stoploss stops tgpoptim openjdk"
  },
  {
    "id": 1224,
    "package_name": "simest",
    "title": "Constrained Single Index Model Estimation",
    "description": "Estimation of function and index vector in single index\nmodel ('sim') with (and w/o) shape constraints including\ndifferent smoothness conditions. See, e.g., Kuchibhotla and\nPatra (2020) <doi:10.3150/19-BEJ1183>.",
    "version": "0.4-1-1",
    "maintainer": "Martin Maechler <maechler@stat.math.ethz.ch>",
    "author": "Arun Kumar Kuchibhotla [aut] (ORCID:\n<https://orcid.org/0000-0003-4459-5352>),\nRohit Kumar Patra [aut],\nMartin Maechler [ctb, cre] (fix *.Rd, etc to get back on CRAN, ORCID:\n<https://orcid.org/0000-0002-8685-9910>)",
    "url": "https://curves-etc.r-forge.r-project.org/,\nhttps://r-forge.r-project.org/R/?group_id=846,\nhttps://r-forge.r-project.org/scm/viewvc.php/pkg/simest/?root=curves-etc,\nsvn://svn.r-forge.r-project.org/svnroot/curves-etc/pkg/simest",
    "bug_reports": "https://r-forge.r-project.org/tracker/?group_id=846",
    "repository": "",
    "exports": [
      [
        "cpen"
      ],
      [
        "cvx.lip.reg"
      ],
      [
        "cvx.lse.con.reg"
      ],
      [
        "cvx.lse.conreg"
      ],
      [
        "cvx.lse.reg"
      ],
      [
        "cvx.pen.reg"
      ],
      [
        "derivcvxpec"
      ],
      [
        "fastmerge"
      ],
      [
        "penta"
      ],
      [
        "plot.cvx.lip.reg"
      ],
      [
        "plot.cvx.lse.reg"
      ],
      [
        "plot.cvx.pen.reg"
      ],
      [
        "plot.sim.est"
      ],
      [
        "plot.smooth.pen.reg"
      ],
      [
        "predcvxpen"
      ],
      [
        "predict.cvx.lip.reg"
      ],
      [
        "predict.cvx.lse.reg"
      ],
      [
        "predict.cvx.pen.reg"
      ],
      [
        "predict.sim.est"
      ],
      [
        "predict.smooth.pen.reg"
      ],
      [
        "print.cvx.lip.reg"
      ],
      [
        "print.cvx.lse.reg"
      ],
      [
        "print.cvx.pen.reg"
      ],
      [
        "print.sim.est"
      ],
      [
        "print.smooth.pen.reg"
      ],
      [
        "sim.est"
      ],
      [
        "simestgcv"
      ],
      [
        "smooth.pen.reg"
      ],
      [
        "solve_pentadiag"
      ],
      [
        "solve.pentadiag"
      ],
      [
        "spen_egcv"
      ]
    ],
    "topics": [],
    "score": 3.2253,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "simest Constrained Single Index Model Estimation Estimation of function and index vector in single index\nmodel ('sim') with (and w/o) shape constraints including\ndifferent smoothness conditions. See, e.g., Kuchibhotla and\nPatra (2020) <doi:10.3150/19-BEJ1183>. cpen cvx.lip.reg cvx.lse.con.reg cvx.lse.conreg cvx.lse.reg cvx.pen.reg derivcvxpec fastmerge penta plot.cvx.lip.reg plot.cvx.lse.reg plot.cvx.pen.reg plot.sim.est plot.smooth.pen.reg predcvxpen predict.cvx.lip.reg predict.cvx.lse.reg predict.cvx.pen.reg predict.sim.est predict.smooth.pen.reg print.cvx.lip.reg print.cvx.lse.reg print.cvx.pen.reg print.sim.est print.smooth.pen.reg sim.est simestgcv smooth.pen.reg solve_pentadiag solve.pentadiag spen_egcv "
  },
  {
    "id": 971,
    "package_name": "pixelclasser",
    "title": "Classifies Image Pixels by Colour",
    "description": "Contains functions to classify the pixels of an image file\n(jpeg or tiff) by its colour. It implements a simple form of\nthe techniques known as Support Vector Machine adapted to this\nparticular problem.",
    "version": "1.0.0",
    "maintainer": "Carlos Real <carlos.real@usc.es>",
    "author": "Carlos Real [aut, cre] (ORCID: <https://orcid.org/0000-0002-5433-6728>),\nQuentin Read [rev]",
    "url": "https://github.com/ropensci/pixelclasser",
    "bug_reports": "https://github.com/ropensci/pixelclasser",
    "repository": "",
    "exports": [
      [
        "classify_pixels"
      ],
      [
        "define_cat"
      ],
      [
        "define_rule"
      ],
      [
        "define_subcat"
      ],
      [
        "label_rule"
      ],
      [
        "place_rule"
      ],
      [
        "plot_pixels"
      ],
      [
        "plot_rgb_plane"
      ],
      [
        "plot_rule"
      ],
      [
        "read_image"
      ],
      [
        "save_classif_image"
      ]
    ],
    "topics": [],
    "score": 3,
    "stars": 2,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pixelclasser Classifies Image Pixels by Colour Contains functions to classify the pixels of an image file\n(jpeg or tiff) by its colour. It implements a simple form of\nthe techniques known as Support Vector Machine adapted to this\nparticular problem. classify_pixels define_cat define_rule define_subcat label_rule place_rule plot_pixels plot_rgb_plane plot_rule read_image save_classif_image "
  },
  {
    "id": 1155,
    "package_name": "rspat",
    "title": "rspatial.org data -- terra version",
    "description": "Data to support the examples on rspatial.org",
    "version": "1.0-1",
    "maintainer": "Robert J. Hijmans <r.hijmans@gmail.com>",
    "author": "Robert J. Hijmans [cre, aut],\nAlex Mandel [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "spat_data"
      ],
      [
        "spat_download"
      ]
    ],
    "topics": [],
    "score": 2.6435,
    "stars": 2,
    "primary_category": "spatial",
    "source_universe": "rspatial",
    "search_text": "rspat rspatial.org data -- terra version Data to support the examples on rspatial.org spat_data spat_download "
  },
  {
    "id": 342,
    "package_name": "cart",
    "title": "Malaria Cartographic Information",
    "description": "Extract population, prevalence and vector cartographic\ninformation for use with malarisimulation.",
    "version": "0.1.0",
    "maintainer": "Pete Winskill <p.winskill@imperial.ac.uk>",
    "author": "Pete Winskill [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        ":="
      ],
      [
        ".data"
      ],
      [
        "%>%"
      ],
      [
        "as_label"
      ],
      [
        "as_name"
      ],
      [
        "enquo"
      ],
      [
        "enquos"
      ],
      [
        "get_pop"
      ],
      [
        "get_prev"
      ],
      [
        "get_spatial_limits"
      ],
      [
        "get_vectors"
      ],
      [
        "pull_cart"
      ],
      [
        "unpack_cart"
      ]
    ],
    "topics": [],
    "score": 1.699,
    "stars": 1,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "cart Malaria Cartographic Information Extract population, prevalence and vector cartographic\ninformation for use with malarisimulation. := .data %>% as_label as_name enquo enquos get_pop get_prev get_spatial_limits get_vectors pull_cart unpack_cart "
  },
  {
    "id": 22,
    "package_name": "Colossus",
    "title": "\"Risk Model Regression and Analysis with Complex Non-Linear\nModels\"",
    "description": "Performs survival analysis using general non-linear models. Risk models can be the sum or product of terms. Each term is the product of exponential/linear functions of covariates. Additionally sub-terms can be defined as a sum of exponential, linear threshold, and step functions. Cox Proportional hazards <https://en.wikipedia.org/wiki/Proportional_hazards_model>, Poisson <https://en.wikipedia.org/wiki/Poisson_regression>, and Fine-Gray competing risks <https://www.publichealth.columbia.edu/research/population-health-methods/competing-risk-analysis> regression are supported. This work was sponsored by NASA Grants 80NSSC19M0161 and 80NSSC23M0129 through a subcontract from the National Council on Radiation Protection and Measurements (NCRP). The computing for this project was performed on the Beocat Research Cluster at Kansas State University, which is funded in part by NSF grants CNS-1006860, EPS-1006860, EPS-0919443, ACI-1440548, CHE-1726332, and NIH P20GM113109.",
    "version": "1.4.6",
    "maintainer": "Eric Giunta <egiunta@ksu.edu>",
    "author": "Eric Giunta [aut, cre] (ORCID: <https://orcid.org/0000-0002-1577-766X>),\n  Amir Bahadori [ctb] (ORCID: <https://orcid.org/0000-0002-4589-105X>),\n  Dan Andresen [ctb] (ORCID: <https://orcid.org/0000-0003-2345-6695>),\n  Linda Walsh [ctb] (ORCID: <https://orcid.org/0000-0001-7399-9191>),\n  Benjamin French [ctb] (ORCID: <https://orcid.org/0000-0001-9265-5378>),\n  Lawrence Dauer [ctb] (ORCID: <https://orcid.org/0000-0002-5629-8462>),\n  John Boice Jr [ctb] (ORCID: <https://orcid.org/0000-0002-8755-1299>),\n  Kansas State University [cph],\n  NASA [fnd],\n  NCRP [fnd],\n  NRC [fnd]",
    "url": "https://ericgiunta.github.io/Colossus/,\nhttps://github.com/ericgiunta/Colossus",
    "bug_reports": "https://github.com/ericgiunta/Colossus/issues",
    "repository": "https://cran.r-project.org/package=Colossus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Colossus \"Risk Model Regression and Analysis with Complex Non-Linear\nModels\" Performs survival analysis using general non-linear models. Risk models can be the sum or product of terms. Each term is the product of exponential/linear functions of covariates. Additionally sub-terms can be defined as a sum of exponential, linear threshold, and step functions. Cox Proportional hazards <https://en.wikipedia.org/wiki/Proportional_hazards_model>, Poisson <https://en.wikipedia.org/wiki/Poisson_regression>, and Fine-Gray competing risks <https://www.publichealth.columbia.edu/research/population-health-methods/competing-risk-analysis> regression are supported. This work was sponsored by NASA Grants 80NSSC19M0161 and 80NSSC23M0129 through a subcontract from the National Council on Radiation Protection and Measurements (NCRP). The computing for this project was performed on the Beocat Research Cluster at Kansas State University, which is funded in part by NSF grants CNS-1006860, EPS-1006860, EPS-0919443, ACI-1440548, CHE-1726332, and NIH P20GM113109.  "
  },
  {
    "id": 85,
    "package_name": "MLwrap",
    "title": "Machine Learning Modelling for Everyone",
    "description": "A minimal library specifically designed to make the estimation of Machine Learning\n             (ML) techniques as easy and accessible as possible, particularly within the framework of\n             the Knowledge Discovery in Databases (KDD) process in data mining. The package provides\n             essential tools to structure and execute each stage of a predictive or classification\n             modeling workflow, aligning closely with the fundamental steps of the KDD methodology,\n             from data selection and preparation, through model building and tuning, to the\n             interpretation and evaluation of results using Sensitivity Analysis. The 'MLwrap' workflow\n             is organized into four core steps; preprocessing(), build_model(), fine_tuning(), and\n             sensitivity_analysis(). It also includes global and pairwise interaction analysis based on\n             Friedman\u2019s H-statistic to support a more detailed interpretation of complex feature\n             relationships.These steps correspond, respectively, to data preparation and transformation,\n             model construction, hyperparameter optimization, and sensitivity analysis. The user can\n             access comprehensive model evaluation results including fit assessment metrics, plots,\n             predictions, and performance diagnostics for ML models implemented through 'Neural\n             Networks', 'Random Forest', 'XGBoost' (Extreme Gradient Boosting), and 'Support Vector\n             Machines' (SVM) algorithms. By streamlining these phases, 'MLwrap' aims to simplify the\n             implementation of ML techniques, allowing analysts and data scientists to focus on\n             extracting actionable insights and meaningful patterns from large datasets, in line with\n             the objectives of the KDD process.",
    "version": "0.3.0",
    "maintainer": "Albert Ses\u00e9 <albert.sese@uib.es>",
    "author": "Javier Mart\u00ednez Garc\u00eda [aut] (ORCID:\n    <https://orcid.org/0009-0007-7861-5274>),\n  Juan Jos\u00e9 Monta\u00f1o Moreno [ctb] (ORCID:\n    <https://orcid.org/0000-0002-1116-1964>),\n  Albert Ses\u00e9 [cre, ctb] (ORCID: <https://orcid.org/0000-0003-3771-1749>)",
    "url": "https://github.com/AlbertSesePsy/MLwrap",
    "bug_reports": "https://github.com/AlbertSesePsy/MLwrap/issues",
    "repository": "https://cran.r-project.org/package=MLwrap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MLwrap Machine Learning Modelling for Everyone A minimal library specifically designed to make the estimation of Machine Learning\n             (ML) techniques as easy and accessible as possible, particularly within the framework of\n             the Knowledge Discovery in Databases (KDD) process in data mining. The package provides\n             essential tools to structure and execute each stage of a predictive or classification\n             modeling workflow, aligning closely with the fundamental steps of the KDD methodology,\n             from data selection and preparation, through model building and tuning, to the\n             interpretation and evaluation of results using Sensitivity Analysis. The 'MLwrap' workflow\n             is organized into four core steps; preprocessing(), build_model(), fine_tuning(), and\n             sensitivity_analysis(). It also includes global and pairwise interaction analysis based on\n             Friedman\u2019s H-statistic to support a more detailed interpretation of complex feature\n             relationships.These steps correspond, respectively, to data preparation and transformation,\n             model construction, hyperparameter optimization, and sensitivity analysis. The user can\n             access comprehensive model evaluation results including fit assessment metrics, plots,\n             predictions, and performance diagnostics for ML models implemented through 'Neural\n             Networks', 'Random Forest', 'XGBoost' (Extreme Gradient Boosting), and 'Support Vector\n             Machines' (SVM) algorithms. By streamlining these phases, 'MLwrap' aims to simplify the\n             implementation of ML techniques, allowing analysts and data scientists to focus on\n             extracting actionable insights and meaningful patterns from large datasets, in line with\n             the objectives of the KDD process.  "
  },
  {
    "id": 88,
    "package_name": "MachineShop",
    "title": "Machine Learning Models and Tools",
    "description": "Meta-package for statistical and machine learning with a unified\n    interface for model fitting, prediction, performance assessment, and\n    presentation of results.  Approaches for model fitting and prediction of\n    numerical, categorical, or censored time-to-event outcomes include\n    traditional regression models, regularization methods, tree-based methods,\n    support vector machines, neural networks, ensembles, data preprocessing,\n    filtering, and model tuning and selection.  Performance metrics are provided\n    for model assessment and can be estimated with independent test sets, split\n    sampling, cross-validation, or bootstrap resampling.  Resample estimation\n    can be executed in parallel for faster processing and nested in cases of\n    model tuning and selection.  Modeling results can be summarized with\n    descriptive statistics; calibration curves; variable importance; partial\n    dependence plots; confusion matrices; and ROC, lift, and other performance\n    curves.",
    "version": "3.9.1",
    "maintainer": "Brian J Smith <brian-j-smith@uiowa.edu>",
    "author": "Brian J Smith [aut, cre]",
    "url": "https://brian-j-smith.github.io/MachineShop/",
    "bug_reports": "https://github.com/brian-j-smith/MachineShop/issues",
    "repository": "https://cran.r-project.org/package=MachineShop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MachineShop Machine Learning Models and Tools Meta-package for statistical and machine learning with a unified\n    interface for model fitting, prediction, performance assessment, and\n    presentation of results.  Approaches for model fitting and prediction of\n    numerical, categorical, or censored time-to-event outcomes include\n    traditional regression models, regularization methods, tree-based methods,\n    support vector machines, neural networks, ensembles, data preprocessing,\n    filtering, and model tuning and selection.  Performance metrics are provided\n    for model assessment and can be estimated with independent test sets, split\n    sampling, cross-validation, or bootstrap resampling.  Resample estimation\n    can be executed in parallel for faster processing and nested in cases of\n    model tuning and selection.  Modeling results can be summarized with\n    descriptive statistics; calibration curves; variable importance; partial\n    dependence plots; confusion matrices; and ROC, lift, and other performance\n    curves.  "
  },
  {
    "id": 95,
    "package_name": "NUETON",
    "title": "Nitrogen Use Efficiency Toolkit on Numerics",
    "description": "A comprehensive toolkit for calculating and visualizing Nitrogen Use Efficiency (NUE) indicators in agricultural research. The package implements 23 parameters categorized into fertilizer-based, plant-based, soil-based, isotope-based, ecology-based, and system-based indicators based on Congreves et al. (2021) <doi:10.3389/fpls.2021.637108>. Key features include vectorized calculations for paired-plot experimental designs, batch processing capabilities for handling large datasets, and built-in visualization tools using 'ggplot2'. Designed to streamline the workflow from raw agronomic data to publication-ready metrics and plots.",
    "version": "0.2.0",
    "maintainer": "Shubham Love <shubhamlove2101@gmail.com>",
    "author": "Shubham Love [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NUETON",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NUETON Nitrogen Use Efficiency Toolkit on Numerics A comprehensive toolkit for calculating and visualizing Nitrogen Use Efficiency (NUE) indicators in agricultural research. The package implements 23 parameters categorized into fertilizer-based, plant-based, soil-based, isotope-based, ecology-based, and system-based indicators based on Congreves et al. (2021) <doi:10.3389/fpls.2021.637108>. Key features include vectorized calculations for paired-plot experimental designs, batch processing capabilities for handling large datasets, and built-in visualization tools using 'ggplot2'. Designed to streamline the workflow from raw agronomic data to publication-ready metrics and plots.  "
  },
  {
    "id": 96,
    "package_name": "NlinTS",
    "title": "Models for Non Linear Causality Detection in Time Series",
    "description": "Models for non-linear time series analysis and causality detection. The main functionalities of this package consist of an implementation of the classical causality test (C.W.J.Granger 1980) <doi:10.1016/0165-1889(80)90069-X>,  and a non-linear version of it based on feed-forward neural networks. This package contains also an implementation of the Transfer Entropy <doi:10.1103/PhysRevLett.85.461>, and the continuous Transfer Entropy using an approximation based on the k-nearest neighbors <doi:10.1103/PhysRevE.69.066138>. There are also some other useful tools, like the VARNN (Vector Auto-Regressive Neural Network) prediction model, the Augmented test of stationarity, and the discrete and continuous entropy and mutual information.",
    "version": "1.4.6",
    "maintainer": "Youssef Hmamouche <hmamoucheyussef@gmail.com>",
    "author": "Youssef Hmamouche [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NlinTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NlinTS Models for Non Linear Causality Detection in Time Series Models for non-linear time series analysis and causality detection. The main functionalities of this package consist of an implementation of the classical causality test (C.W.J.Granger 1980) <doi:10.1016/0165-1889(80)90069-X>,  and a non-linear version of it based on feed-forward neural networks. This package contains also an implementation of the Transfer Entropy <doi:10.1103/PhysRevLett.85.461>, and the continuous Transfer Entropy using an approximation based on the k-nearest neighbors <doi:10.1103/PhysRevE.69.066138>. There are also some other useful tools, like the VARNN (Vector Auto-Regressive Neural Network) prediction model, the Augmented test of stationarity, and the discrete and continuous entropy and mutual information.  "
  },
  {
    "id": 161,
    "package_name": "RcppArmadillo",
    "title": "'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra\nLibrary",
    "description": "'Armadillo' is a templated C++ linear algebra library aiming towards\n a good balance between speed and ease of use. It provides high-level syntax and\n functionality deliberately similar to Matlab. It is useful for algorithm development\n directly in C++, or quick conversion of research code into production environments.\n It provides efficient classes for vectors, matrices and cubes where dense and sparse\n matrices are supported. Integer, floating point and complex numbers are supported.\n A sophisticated expression evaluator (based on template meta-programming) automatically\n combines several operations to increase speed and efficiency. Dynamic evaluation\n automatically chooses optimal code paths based on detected matrix structures.\n Matrix decompositions are provided through integration with LAPACK, or one of its\n high performance drop-in replacements (such as 'MKL' or 'OpenBLAS'). It can\n automatically use 'OpenMP' multi-threading (parallelisation) to speed up\n computationally expensive operations.\n\n   The 'RcppArmadillo' package includes the header files from the 'Armadillo' library;\n users do not need to install 'Armadillo' itself in order to use 'RcppArmadillo'.\n Starting from release 15.0.0, the minimum compilation standard is C++14 so 'Armadillo'\n version 14.6.3 is included as a fallback when an R package forces the C++11 standard.\n Package authors should set a '#define' to select the 'current' version, or select the\n 'legacy' version (also chosen as default) if they must. See 'GitHub issue #475' for\n details.\n\n   Since release 7.800.0, 'Armadillo' is licensed under Apache License 2; previous\n releases were under licensed as MPL 2.0 from version 3.800.0 onwards and LGPL-3\n prior to that; 'RcppArmadillo' (the 'Rcpp' bindings/bridge to Armadillo) is licensed\n under the GNU GPL version 2 or later, as is the rest of 'Rcpp'.",
    "version": "15.2.3-1",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Doug Bates [aut] (ORCID: <https://orcid.org/0000-0001-8316-9503>),\n  Binxiang Ni [aut],\n  Conrad Sanderson [aut] (ORCID: <https://orcid.org/0000-0002-0049-4501>)",
    "url": "https://github.com/RcppCore/RcppArmadillo,\nhttps://dirk.eddelbuettel.com/code/rcpp.armadillo.html",
    "bug_reports": "https://github.com/RcppCore/RcppArmadillo/issues",
    "repository": "https://cran.r-project.org/package=RcppArmadillo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppArmadillo 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra\nLibrary 'Armadillo' is a templated C++ linear algebra library aiming towards\n a good balance between speed and ease of use. It provides high-level syntax and\n functionality deliberately similar to Matlab. It is useful for algorithm development\n directly in C++, or quick conversion of research code into production environments.\n It provides efficient classes for vectors, matrices and cubes where dense and sparse\n matrices are supported. Integer, floating point and complex numbers are supported.\n A sophisticated expression evaluator (based on template meta-programming) automatically\n combines several operations to increase speed and efficiency. Dynamic evaluation\n automatically chooses optimal code paths based on detected matrix structures.\n Matrix decompositions are provided through integration with LAPACK, or one of its\n high performance drop-in replacements (such as 'MKL' or 'OpenBLAS'). It can\n automatically use 'OpenMP' multi-threading (parallelisation) to speed up\n computationally expensive operations.\n\n   The 'RcppArmadillo' package includes the header files from the 'Armadillo' library;\n users do not need to install 'Armadillo' itself in order to use 'RcppArmadillo'.\n Starting from release 15.0.0, the minimum compilation standard is C++14 so 'Armadillo'\n version 14.6.3 is included as a fallback when an R package forces the C++11 standard.\n Package authors should set a '#define' to select the 'current' version, or select the\n 'legacy' version (also chosen as default) if they must. See 'GitHub issue #475' for\n details.\n\n   Since release 7.800.0, 'Armadillo' is licensed under Apache License 2; previous\n releases were under licensed as MPL 2.0 from version 3.800.0 onwards and LGPL-3\n prior to that; 'RcppArmadillo' (the 'Rcpp' bindings/bridge to Armadillo) is licensed\n under the GNU GPL version 2 or later, as is the rest of 'Rcpp'.  "
  },
  {
    "id": 197,
    "package_name": "SlimR",
    "title": "Machine Learning-Assisted, Marker-Based Tool for Single-Cell and\nSpatial Transcriptomics Annotation",
    "description": "Annotates single-cell and spatial-transcriptomic (ST) data using marker datasets. Supports unified markers list ('Markers_list') creation from built-in databases (e.g., 'Cellmarker2', 'PanglaoDB', 'scIBD', 'TCellSI', 'PCTIT', 'PCTAM'), Seurat objects, or user-supplied Excel files. SlimR can predict calculation parameters by machine learning algorithms (e.g., 'Random Forest', 'Gradient Boosting', 'Support Vector Machine', 'Ensemble Learning'), and based on Markers_list, calculate gene expression of different cell types and predict annotation information, and calculate corresponding AUC and annotate it, then verify it. At the same time, it can calculate gene expression corresponding to the cell type to generate a reference map for manual annotation (e.g., 'Heat Map', 'Feature Plots', 'Combined Plots'). For more details, see Kabacoff (2020, ISBN:9787115420572).",
    "version": "1.0.9",
    "maintainer": "Zhaoqing Wang <zhaoqingwang@mail.sdu.edu.cn>",
    "author": "Zhaoqing Wang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8348-7245>)",
    "url": "https://github.com/Zhaoqing-wang/SlimR",
    "bug_reports": "https://github.com/Zhaoqing-wang/SlimR/issues",
    "repository": "https://cran.r-project.org/package=SlimR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SlimR Machine Learning-Assisted, Marker-Based Tool for Single-Cell and\nSpatial Transcriptomics Annotation Annotates single-cell and spatial-transcriptomic (ST) data using marker datasets. Supports unified markers list ('Markers_list') creation from built-in databases (e.g., 'Cellmarker2', 'PanglaoDB', 'scIBD', 'TCellSI', 'PCTIT', 'PCTAM'), Seurat objects, or user-supplied Excel files. SlimR can predict calculation parameters by machine learning algorithms (e.g., 'Random Forest', 'Gradient Boosting', 'Support Vector Machine', 'Ensemble Learning'), and based on Markers_list, calculate gene expression of different cell types and predict annotation information, and calculate corresponding AUC and annotate it, then verify it. At the same time, it can calculate gene expression corresponding to the cell type to generate a reference map for manual annotation (e.g., 'Heat Map', 'Feature Plots', 'Combined Plots'). For more details, see Kabacoff (2020, ISBN:9787115420572).  "
  },
  {
    "id": 203,
    "package_name": "TFORGE",
    "title": "Tests for Geophysical Eigenvalues",
    "description": "The eigenvalues of observed symmetric matrices are often of intense scientific interest. This package offers single sample tests for the eigenvalues of the population mean or the eigenvalue-multiplicity of the population mean. For k-samples, this package offers tests for equal eigenvalues between samples. Included is support for matrices with constraints common to geophysical tensors (constant trace, sum of squared eigenvalues, or both) and eigenvectors are usually considered nuisance parameters. Pivotal bootstrap methods enable these tests to have good performance for small samples (n=15 for 3x3 matrices). These methods were developed and studied by Hingee, Scealy and Wood (2026, \"Nonparametric bootstrap inference for the eigenvalues of geophysical tensors\", accepted by the Journal of American Statistical Association). Also available is a 2-sample test using a Gaussian orthogonal ensemble approximation and an eigenvalue-multiplicity test that assumes orthogonally-invariant covariance. ",
    "version": "0.1.16",
    "maintainer": "Kassel Liam Hingee <kassel.hingee@anu.edu.au>",
    "author": "Kassel Liam Hingee [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9894-2407>),\n  Art B. Owen [cph] (./R/scel.R only),\n  Board of Trustees Leland Stanford Junior University [cph] (./R/scel.R\n    only)",
    "url": "https://github.com/kasselhingee/TFORGE",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TFORGE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TFORGE Tests for Geophysical Eigenvalues The eigenvalues of observed symmetric matrices are often of intense scientific interest. This package offers single sample tests for the eigenvalues of the population mean or the eigenvalue-multiplicity of the population mean. For k-samples, this package offers tests for equal eigenvalues between samples. Included is support for matrices with constraints common to geophysical tensors (constant trace, sum of squared eigenvalues, or both) and eigenvectors are usually considered nuisance parameters. Pivotal bootstrap methods enable these tests to have good performance for small samples (n=15 for 3x3 matrices). These methods were developed and studied by Hingee, Scealy and Wood (2026, \"Nonparametric bootstrap inference for the eigenvalues of geophysical tensors\", accepted by the Journal of American Statistical Association). Also available is a 2-sample test using a Gaussian orthogonal ensemble approximation and an eigenvalue-multiplicity test that assumes orthogonally-invariant covariance.   "
  },
  {
    "id": 204,
    "package_name": "TMB",
    "title": "Template Model Builder: A General Random Effect Tool Inspired by\n'ADMB'",
    "description": "With this tool, a user should be able to quickly implement\n    complex random effect models through simple C++ templates. The package combines\n    'CppAD' (C++ automatic differentiation), 'Eigen' (templated matrix-vector\n    library) and 'CHOLMOD' (sparse matrix routines available from R) to obtain\n    an efficient implementation of the applied Laplace approximation with exact\n    derivatives. Key features are: Automatic sparseness detection, parallelism\n    through 'BLAS' and parallel user templates.",
    "version": "1.9.19",
    "maintainer": "Kasper Kristensen <kaskr@dtu.dk>",
    "author": "Kasper Kristensen [aut, cre, cph],\n  Brad Bell [cph],\n  Hans Skaug [ctb],\n  Arni Magnusson [ctb],\n  Casper Berg [ctb],\n  Anders Nielsen [ctb],\n  Martin Maechler [ctb],\n  Theo Michelot [ctb],\n  Mollie Brooks [ctb],\n  Alex Forrence [ctb],\n  Christoffer Moesgaard Albertsen [ctb],\n  Cole Monnahan [ctb]",
    "url": "https://github.com/kaskr/adcomp/wiki",
    "bug_reports": "https://github.com/kaskr/adcomp/issues",
    "repository": "https://cran.r-project.org/package=TMB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TMB Template Model Builder: A General Random Effect Tool Inspired by\n'ADMB' With this tool, a user should be able to quickly implement\n    complex random effect models through simple C++ templates. The package combines\n    'CppAD' (C++ automatic differentiation), 'Eigen' (templated matrix-vector\n    library) and 'CHOLMOD' (sparse matrix routines available from R) to obtain\n    an efficient implementation of the applied Laplace approximation with exact\n    derivatives. Key features are: Automatic sparseness detection, parallelism\n    through 'BLAS' and parallel user templates.  "
  },
  {
    "id": 314,
    "package_name": "bpvars",
    "title": "Forecasting with Bayesian Panel Vector Autoregressions",
    "description": "\n  Provides Bayesian estimation and forecasting of dynamic panel data using \n  Bayesian Panel Vector Autoregressions with hierarchical prior distributions. \n  The models include country-specific VARs that share a global prior \n  distribution that extend the model by Jaroci\u0144ski (2010) <doi:10.1002/jae.1082>. \n  Under this prior expected value, each country's system follows \n  a global VAR with country-invariant parameters. Further flexibility is \n  provided by the hierarchical prior structure that retains the Minnesota prior \n  interpretation for the global VAR and features estimated prior covariance \n  matrices, shrinkage, and persistence levels. Bayesian forecasting is developed \n  for models including exogenous variables, allowing conditional forecasts given \n  the future trajectories of some variables and restricted forecasts assuring \n  that rates are forecasted to stay positive and less than 100. The package \n  implements the model specification, estimation, and forecasting routines, \n  facilitating coherent workflows and reproducibility. It also includes automated\n  pseudo-out-of-sample forecasting and computation of forecasting performance \n  measures. Beautiful plots, \n  informative summary functions, and extensive documentation complement all \n  this. An extraordinary computational speed is achieved thanks to employing \n  frontier econometric and numerical techniques and algorithms written in 'C++'. \n  The 'bpvars' package is aligned regarding objects, workflows, and code \n  structure with the 'R' packages 'bsvars' by \n  Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars> and 'bsvarSIGNs' by \n  Wang & Wo\u017aniak (2025) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they \n  constitute an integrated toolset. Copyright: 2025 International Labour \n  Organization.",
    "version": "1.0",
    "maintainer": "Tomasz Wo\u017aniak <wozniak.tom@pm.me>",
    "author": "Tomasz Wo\u017aniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2212-2378>),\n  Miguel Sanchez-Martinez [ctb],\n  International Labour Organization [cph]",
    "url": "https://bsvars.org/bpvars/",
    "bug_reports": "https://github.com/bsvars/bpvars/issues",
    "repository": "https://cran.r-project.org/package=bpvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bpvars Forecasting with Bayesian Panel Vector Autoregressions \n  Provides Bayesian estimation and forecasting of dynamic panel data using \n  Bayesian Panel Vector Autoregressions with hierarchical prior distributions. \n  The models include country-specific VARs that share a global prior \n  distribution that extend the model by Jaroci\u0144ski (2010) <doi:10.1002/jae.1082>. \n  Under this prior expected value, each country's system follows \n  a global VAR with country-invariant parameters. Further flexibility is \n  provided by the hierarchical prior structure that retains the Minnesota prior \n  interpretation for the global VAR and features estimated prior covariance \n  matrices, shrinkage, and persistence levels. Bayesian forecasting is developed \n  for models including exogenous variables, allowing conditional forecasts given \n  the future trajectories of some variables and restricted forecasts assuring \n  that rates are forecasted to stay positive and less than 100. The package \n  implements the model specification, estimation, and forecasting routines, \n  facilitating coherent workflows and reproducibility. It also includes automated\n  pseudo-out-of-sample forecasting and computation of forecasting performance \n  measures. Beautiful plots, \n  informative summary functions, and extensive documentation complement all \n  this. An extraordinary computational speed is achieved thanks to employing \n  frontier econometric and numerical techniques and algorithms written in 'C++'. \n  The 'bpvars' package is aligned regarding objects, workflows, and code \n  structure with the 'R' packages 'bsvars' by \n  Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars> and 'bsvarSIGNs' by \n  Wang & Wo\u017aniak (2025) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they \n  constitute an integrated toolset. Copyright: 2025 International Labour \n  Organization.  "
  },
  {
    "id": 410,
    "package_name": "cosCorr",
    "title": "Cosine-Correlation Coefficient for Vector Variables",
    "description": "Computes the cosine-correlation coefficient for measuring \n    the degree of linear dependence among variables in a multidimensional context. \n    The package implements the generalized cosine-correlation theorem for p-1 variables, \n    providing a quantitative assessment of interrelationships within experimental frameworks. \n    This methodology extends classical correlation measures to higher-dimensional spaces \n    using a dimensional exploration approach based on time scale calculus.",
    "version": "1.0.0",
    "maintainer": "Mehmet Niyazi Cankaya <mehmet.cankaya@usak.edu.tr>",
    "author": "Mehmet Niyazi Cankaya [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cosCorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cosCorr Cosine-Correlation Coefficient for Vector Variables Computes the cosine-correlation coefficient for measuring \n    the degree of linear dependence among variables in a multidimensional context. \n    The package implements the generalized cosine-correlation theorem for p-1 variables, \n    providing a quantitative assessment of interrelationships within experimental frameworks. \n    This methodology extends classical correlation measures to higher-dimensional spaces \n    using a dimensional exploration approach based on time scale calculus.  "
  },
  {
    "id": 420,
    "package_name": "cpfa",
    "title": "Classification with Parallel Factor Analysis",
    "description": "Classification using Richard A. Harshman's Parallel Factor Analysis-1 (Parafac) model or Parallel Factor Analysis-2 (Parafac2) model fit to a three-way or four-way data array. See Harshman and Lundy (1994): <doi:10.1016/0167-9473(94)90132-5>. Uses component weights from one mode of a Parafac or Parafac2 model as features to tune parameters for one or more classification methods via a k-fold cross-validation procedure. Allows for constraints on different tensor modes. Supports penalized logistic regression, support vector machine, random forest, feed-forward neural network, regularized discriminant analysis, and gradient boosting machine. Supports binary and multiclass classification. Predicts class labels or class probabilities and calculates multiple classification performance measures. Implements parallel computing via the 'parallel', 'doParallel', and 'doRNG' packages.",
    "version": "1.2-4",
    "maintainer": "Matthew A. Asisgress <mattgress@protonmail.ch>",
    "author": "Matthew A. Asisgress [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cpfa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cpfa Classification with Parallel Factor Analysis Classification using Richard A. Harshman's Parallel Factor Analysis-1 (Parafac) model or Parallel Factor Analysis-2 (Parafac2) model fit to a three-way or four-way data array. See Harshman and Lundy (1994): <doi:10.1016/0167-9473(94)90132-5>. Uses component weights from one mode of a Parafac or Parafac2 model as features to tune parameters for one or more classification methods via a k-fold cross-validation procedure. Allows for constraints on different tensor modes. Supports penalized logistic regression, support vector machine, random forest, feed-forward neural network, regularized discriminant analysis, and gradient boosting machine. Supports binary and multiclass classification. Predicts class labels or class probabilities and calculates multiple classification performance measures. Implements parallel computing via the 'parallel', 'doParallel', and 'doRNG' packages.  "
  },
  {
    "id": 431,
    "package_name": "cvar",
    "title": "Compute Expected Shortfall and Value at Risk for Continuous\nDistributions",
    "description": "Compute expected shortfall (ES) and Value at Risk (VaR) from a\n    quantile function, distribution function, random number generator,\n    probability density function, or data.  ES is also known as Conditional\n    Value at Risk (CVaR). Virtually any continuous distribution can be\n    specified.  The functions are vectorized over the arguments. The\n    computations are done directly from the definitions, see e.g. Acerbi and\n    Tasche (2002) <doi:10.1111/1468-0300.00091>.  Some support for GARCH models\n    is provided, as well.",
    "version": "0.6",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Georgi N. Boshnakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/cvar/ (doc),\nhttps://github.com/GeoBosh/cvar (devel)",
    "bug_reports": "https://github.com/GeoBosh/cvar/issues",
    "repository": "https://cran.r-project.org/package=cvar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvar Compute Expected Shortfall and Value at Risk for Continuous\nDistributions Compute expected shortfall (ES) and Value at Risk (VaR) from a\n    quantile function, distribution function, random number generator,\n    probability density function, or data.  ES is also known as Conditional\n    Value at Risk (CVaR). Virtually any continuous distribution can be\n    specified.  The functions are vectorized over the arguments. The\n    computations are done directly from the definitions, see e.g. Acerbi and\n    Tasche (2002) <doi:10.1111/1468-0300.00091>.  Some support for GARCH models\n    is provided, as well.  "
  },
  {
    "id": 445,
    "package_name": "datetimeutils",
    "title": "Utilities for Dates and Times",
    "description": "Utilities for handling dates and times, such\n   as selecting particular days of the week or month,\n   formatting timestamps as required by RSS feeds, or\n   converting timestamp representations of other software\n   (such as 'MATLAB' and 'Excel') to R. The package is\n   lightweight (no dependencies, pure R implementations) and\n   relies only on R's standard classes to represent dates\n   and times ('Date' and 'POSIXt'); it aims to provide\n   efficient implementations, through vectorisation and the\n   use of R's native numeric representations of timestamps\n   where possible.",
    "version": "0.6-6",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>),\n  Unicode, Inc. [dtc, cph] (source of timezone names in 'tznames')",
    "url": "https://enricoschumann.net/R/packages/datetimeutils/ ,\nhttps://git.sr.ht/~enricoschumann/datetimeutils ,\nhttps://github.com/enricoschumann/datetimeutils",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=datetimeutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datetimeutils Utilities for Dates and Times Utilities for handling dates and times, such\n   as selecting particular days of the week or month,\n   formatting timestamps as required by RSS feeds, or\n   converting timestamp representations of other software\n   (such as 'MATLAB' and 'Excel') to R. The package is\n   lightweight (no dependencies, pure R implementations) and\n   relies only on R's standard classes to represent dates\n   and times ('Date' and 'POSIXt'); it aims to provide\n   efficient implementations, through vectorisation and the\n   use of R's native numeric representations of timestamps\n   where possible.  "
  },
  {
    "id": 643,
    "package_name": "glmnetr",
    "title": "Nested Cross Validation for the Relaxed Lasso and Other Machine\nLearning Models",
    "description": "\n    Cross validation informed Relaxed LASSO (or more generally elastic net), gradient boosting machine ('xgboost'), Random Forest ('RandomForestSRC'), Oblique Random Forest ('aorsf'), Artificial Neural Network (ANN), Recursive Partitioning ('RPART') or step wise regression models are fit.  Cross validation leave out samples (leading to nested cross validation) or bootstrap out-of-bag samples are used to evaluate and compare performances between these models with results presented in tabular or graphical means.  Calibration plots can also be generated, again based upon (outer nested) cross validation or bootstrap leave out (out of bag) samples.\n    Note, at the time of this writing, in order to fit gradient boosting machine models one must install the packages 'DiceKriging' and 'rgenoud' using the install.packages() function. \n    For some datasets, for example when the design matrix is not of full rank, 'glmnet' may have very long run times when fitting the relaxed lasso model, from our experience when fitting Cox models on data with many predictors and many patients, making it difficult to get solutions from either glmnet() or cv.glmnet().  This may be remedied by using the 'path=TRUE' option when calling glmnet() and cv.glmnet().  Within the 'glmnetr' package the approach of path=TRUE is taken by default. \n    other packages doing similar include 'nestedcv' <https://cran.r-project.org/package=nestedcv>, 'glmnetSE' <https://cran.r-project.org/package=glmnetSE> which may provide different functionality when performing a nested CV. \n    Use of the 'glmnetr' has many similarities to the 'glmnet' package and it could be helpful for the user of 'glmnetr' also become familiar with the 'glmnet' package <https://cran.r-project.org/package=glmnet>, with the \"An Introduction to 'glmnet'\" and \"The Relaxed Lasso\" being especially useful in this regard. ",
    "version": "0.6-3",
    "maintainer": "Walter K Kremers <kremers.walter@mayo.edu>",
    "author": "Walter K Kremers [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5714-3473>),\n  Nicholas B Larson [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glmnetr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glmnetr Nested Cross Validation for the Relaxed Lasso and Other Machine\nLearning Models \n    Cross validation informed Relaxed LASSO (or more generally elastic net), gradient boosting machine ('xgboost'), Random Forest ('RandomForestSRC'), Oblique Random Forest ('aorsf'), Artificial Neural Network (ANN), Recursive Partitioning ('RPART') or step wise regression models are fit.  Cross validation leave out samples (leading to nested cross validation) or bootstrap out-of-bag samples are used to evaluate and compare performances between these models with results presented in tabular or graphical means.  Calibration plots can also be generated, again based upon (outer nested) cross validation or bootstrap leave out (out of bag) samples.\n    Note, at the time of this writing, in order to fit gradient boosting machine models one must install the packages 'DiceKriging' and 'rgenoud' using the install.packages() function. \n    For some datasets, for example when the design matrix is not of full rank, 'glmnet' may have very long run times when fitting the relaxed lasso model, from our experience when fitting Cox models on data with many predictors and many patients, making it difficult to get solutions from either glmnet() or cv.glmnet().  This may be remedied by using the 'path=TRUE' option when calling glmnet() and cv.glmnet().  Within the 'glmnetr' package the approach of path=TRUE is taken by default. \n    other packages doing similar include 'nestedcv' <https://cran.r-project.org/package=nestedcv>, 'glmnetSE' <https://cran.r-project.org/package=glmnetSE> which may provide different functionality when performing a nested CV. \n    Use of the 'glmnetr' has many similarities to the 'glmnet' package and it could be helpful for the user of 'glmnetr' also become familiar with the 'glmnet' package <https://cran.r-project.org/package=glmnet>, with the \"An Introduction to 'glmnet'\" and \"The Relaxed Lasso\" being especially useful in this regard.   "
  },
  {
    "id": 727,
    "package_name": "journalR",
    "title": "Formatting Tools for Scientific Journal Writing",
    "description": "Scientific journal numeric formatting policies implemented in code.\n    Emphasis on formatting mean/upper/lower sets of values.  Convert raw numeric\n    triplet value vectors to formatted text for journal submission. For example \n    c(2e6, 1e6, 3e6) becomes \"2.00 million (1.00--3.00)\".  \n    Lancet and Nature have built-in styles for rounding and punctuation marks. \n    Users may extend journal styles arbitrarily.\n    Three metrics are supported; proportions, percentage points, and counts. \n    Magnitudes for all metrics are discovered automatically.",
    "version": "0.2.1",
    "maintainer": "Sam Byrne <ssbyrne@uw.edu>",
    "author": "Sam Byrne [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0008-1067-307X>)",
    "url": "https://github.com/epi-sam/journalR",
    "bug_reports": "https://github.com/epi-sam/journalR/issues",
    "repository": "https://cran.r-project.org/package=journalR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "journalR Formatting Tools for Scientific Journal Writing Scientific journal numeric formatting policies implemented in code.\n    Emphasis on formatting mean/upper/lower sets of values.  Convert raw numeric\n    triplet value vectors to formatted text for journal submission. For example \n    c(2e6, 1e6, 3e6) becomes \"2.00 million (1.00--3.00)\".  \n    Lancet and Nature have built-in styles for rounding and punctuation marks. \n    Users may extend journal styles arbitrarily.\n    Three metrics are supported; proportions, percentage points, and counts. \n    Magnitudes for all metrics are discovered automatically.  "
  },
  {
    "id": 820,
    "package_name": "mlr3learners",
    "title": "Recommended Learners for 'mlr3'",
    "description": "Recommended Learners for 'mlr3'. Extends 'mlr3' with\n    interfaces to essential machine learning packages on CRAN.  This\n    includes, but is not limited to: (penalized) linear and logistic\n    regression, linear and quadratic discriminant analysis, k-nearest\n    neighbors, naive Bayes, support vector machines, and gradient\n    boosting.",
    "version": "0.14.0",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Quay Au [aut] (ORCID: <https://orcid.org/0000-0002-5252-8902>),\n  Stefan Coors [aut] (ORCID: <https://orcid.org/0000-0002-7465-2146>),\n  Patrick Schratz [aut] (ORCID: <https://orcid.org/0000-0003-0748-6624>),\n  Marc Becker [cre, aut] (ORCID: <https://orcid.org/0000-0002-8115-0400>),\n  John Zobolas [aut] (ORCID: <https://orcid.org/0000-0002-3609-8674>)",
    "url": "https://mlr3learners.mlr-org.com,\nhttps://github.com/mlr-org/mlr3learners",
    "bug_reports": "https://github.com/mlr-org/mlr3learners/issues",
    "repository": "https://cran.r-project.org/package=mlr3learners",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3learners Recommended Learners for 'mlr3' Recommended Learners for 'mlr3'. Extends 'mlr3' with\n    interfaces to essential machine learning packages on CRAN.  This\n    includes, but is not limited to: (penalized) linear and logistic\n    regression, linear and quadratic discriminant analysis, k-nearest\n    neighbors, naive Bayes, support vector machines, and gradient\n    boosting.  "
  },
  {
    "id": 895,
    "package_name": "nvctr",
    "title": "The n-vector Approach to Geographical Position Calculations\nusing an Ellipsoidal Model of Earth",
    "description": "The n-vector framework uses the normal vector to\n    the Earth ellipsoid (called n-vector) as a non-singular position\n    representation that turns out to be very convenient for practical\n    position calculations.  The n-vector is simple to use and gives exact\n    answers for all global positions, and all distances, for both\n    ellipsoidal and spherical Earth models.  This package is a translation\n    of the 'Matlab' library from FFI, the Norwegian Defence Research\n    Establishment, as described in Gade (2010)\n    <doi:10.1017/S0373463309990415>.",
    "version": "0.1.7",
    "maintainer": "Enrico Spinielli <enrico.spinielli@eurocontrol.int>",
    "author": "Enrico Spinielli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8584-9131>),\n  EUROCONTROL [cph, fnd]",
    "url": "https://nvctr.ansperformance.eu,\nhttps://github.com/euctrl-pru/nvctr",
    "bug_reports": "https://github.com/euctrl-pru/nvctr/issues",
    "repository": "https://cran.r-project.org/package=nvctr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nvctr The n-vector Approach to Geographical Position Calculations\nusing an Ellipsoidal Model of Earth The n-vector framework uses the normal vector to\n    the Earth ellipsoid (called n-vector) as a non-singular position\n    representation that turns out to be very convenient for practical\n    position calculations.  The n-vector is simple to use and gives exact\n    answers for all global positions, and all distances, for both\n    ellipsoidal and spherical Earth models.  This package is a translation\n    of the 'Matlab' library from FFI, the Norwegian Defence Research\n    Establishment, as described in Gade (2010)\n    <doi:10.1017/S0373463309990415>.  "
  },
  {
    "id": 1209,
    "package_name": "shinyfilters",
    "title": "Use 'shiny' to Filter Data",
    "description": "Provides an interface to 'shiny' inputs used for filtering vectors,\n  data.frames, and other objects. 'S7'-based implementation allows for seamless \n  extensibility.",
    "version": "0.2.0",
    "maintainer": "Josh Livingston <joshwlivingston@gmail.com>",
    "author": "Josh Livingston [cre, aut]",
    "url": "https://joshwlivingston.github.io/shinyfilters/,\nhttps://github.com/joshwlivingston/shinyfilters",
    "bug_reports": "https://github.com/joshwlivingston/shinyfilters/issues",
    "repository": "https://cran.r-project.org/package=shinyfilters",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyfilters Use 'shiny' to Filter Data Provides an interface to 'shiny' inputs used for filtering vectors,\n  data.frames, and other objects. 'S7'-based implementation allows for seamless \n  extensibility.  "
  },
  {
    "id": 1223,
    "package_name": "simFastBOIN",
    "title": "Fast Bayesian Optimal Interval Design for Phase I Dose-Finding\nTrials",
    "description": "\n    Conducting Bayesian Optimal Interval (BOIN) design for phase I \n    dose-finding trials. 'simFastBOIN' provides functions for pre-computing \n    decision tables, conducting trial simulations, and evaluating operating \n    characteristics. The package uses vectorized operations and the \n    Iso::pava() function for isotonic regression to achieve efficient \n    performance while maintaining full compatibility with BOIN methodology. \n    Version 1.3.2 adds p_saf and p_tox parameters for customizable safety and \n    toxicity thresholds. Version 1.3.1 fixes Date field. Version 1.2.1 adds \n    comprehensive 'roxygen2' documentation and enhanced print formatting with \n    flexible table output options. Version 1.2.0 integrated C-based PAVA for \n    isotonic regression. Version 1.1.0 introduced conservative MTD selection \n    (boundMTD) and flexible early stopping rules (n_earlystop_rule). Methods \n    are described in Liu and Yuan (2015) <doi:10.1111/rssc.12089>.",
    "version": "1.3.2",
    "maintainer": "Gosuke Homma <my.name.is.gosuke@gmail.com>",
    "author": "Gosuke Homma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6854-5627>)",
    "url": "https://github.com/gosukehommaEX/simFastBOIN",
    "bug_reports": "https://github.com/gosukehommaEX/simFastBOIN/issues",
    "repository": "https://cran.r-project.org/package=simFastBOIN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simFastBOIN Fast Bayesian Optimal Interval Design for Phase I Dose-Finding\nTrials \n    Conducting Bayesian Optimal Interval (BOIN) design for phase I \n    dose-finding trials. 'simFastBOIN' provides functions for pre-computing \n    decision tables, conducting trial simulations, and evaluating operating \n    characteristics. The package uses vectorized operations and the \n    Iso::pava() function for isotonic regression to achieve efficient \n    performance while maintaining full compatibility with BOIN methodology. \n    Version 1.3.2 adds p_saf and p_tox parameters for customizable safety and \n    toxicity thresholds. Version 1.3.1 fixes Date field. Version 1.2.1 adds \n    comprehensive 'roxygen2' documentation and enhanced print formatting with \n    flexible table output options. Version 1.2.0 integrated C-based PAVA for \n    isotonic regression. Version 1.1.0 introduced conservative MTD selection \n    (boundMTD) and flexible early stopping rules (n_earlystop_rule). Methods \n    are described in Liu and Yuan (2015) <doi:10.1111/rssc.12089>.  "
  },
  {
    "id": 1242,
    "package_name": "snic",
    "title": "Superpixel Segmentation with the Simple Non-Iterative Clustering\nAlgorithm",
    "description": "Implements the Simple Non-Iterative Clustering algorithm for \n    superpixel segmentation of multi-band images, as introduced by Achanta \n    and Susstrunk (2017) <doi:10.1109/CVPR.2017.520>. Supports both standard \n    image arrays and geospatial raster objects, with a design that can be \n    extended to other spatial data frameworks. The algorithm groups adjacent \n    pixels into compact, coherent regions based on spectral similarity and \n    spatial proximity. A high-performance implementation supports images \n    with arbitrary spectral bands.",
    "version": "0.6.1",
    "maintainer": "Rolf Simoes <rolfsimoes@gmail.com>",
    "author": "Rolf Simoes [aut, cre] (ORCID: <https://orcid.org/0000-0003-0953-4132>),\n  Felipe Souza [aut] (ORCID: <https://orcid.org/0000-0002-5826-1700>),\n  Felipe Carlos [aut] (ORCID: <https://orcid.org/0000-0002-3334-4315>),\n  Gilberto Camara [aut, rth] (ORCID:\n    <https://orcid.org/0000-0002-3681-487X>)",
    "url": "https://github.com/rolfsimoes/snic,\nhttps://rolfsimoes.github.io/snic/",
    "bug_reports": "https://github.com/rolfsimoes/snic/issues",
    "repository": "https://cran.r-project.org/package=snic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snic Superpixel Segmentation with the Simple Non-Iterative Clustering\nAlgorithm Implements the Simple Non-Iterative Clustering algorithm for \n    superpixel segmentation of multi-band images, as introduced by Achanta \n    and Susstrunk (2017) <doi:10.1109/CVPR.2017.520>. Supports both standard \n    image arrays and geospatial raster objects, with a design that can be \n    extended to other spatial data frameworks. The algorithm groups adjacent \n    pixels into compact, coherent regions based on spectral similarity and \n    spatial proximity. A high-performance implementation supports images \n    with arbitrary spectral bands.  "
  },
  {
    "id": 1253,
    "package_name": "spatialAtomizeR",
    "title": "Spatial Analysis with Misaligned Data Using Atom-Based\nRegression Models",
    "description": "Implements atom-based regression models (ABRM) for analyzing spatially \n    misaligned data. Provides functions for simulating misaligned spatial data, \n    preparing NIMBLE model inputs, running MCMC diagnostics, and comparing different \n    spatial analysis methods including dasymetric mapping. All main functions return\n    S3 objects with print(), summary(), and plot() methods for intuitive result exploration.\n    Methods are described in Nethery et al. (2023) <doi:10.1101/2023.01.10.23284410>.\n    Further methodological details and software implementation are described in Qian et al. (in review).",
    "version": "0.2.4",
    "maintainer": "Yunzhe Qian <qyzanemos@gmail.com>",
    "author": "Yunzhe Qian [aut, cre],\n  Rachel Nethery [aut],\n  Nancy Krieger [ctb] (Contributed to the project conceptualization and\n    manuscript),\n  Nykesha Johnson [ctb] (Contributed to the project conceptualization and\n    manuscript)",
    "url": "https://github.com/bellayqian/spatialAtomizeR",
    "bug_reports": "https://github.com/bellayqian/spatialAtomizeR/issues",
    "repository": "https://cran.r-project.org/package=spatialAtomizeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialAtomizeR Spatial Analysis with Misaligned Data Using Atom-Based\nRegression Models Implements atom-based regression models (ABRM) for analyzing spatially \n    misaligned data. Provides functions for simulating misaligned spatial data, \n    preparing NIMBLE model inputs, running MCMC diagnostics, and comparing different \n    spatial analysis methods including dasymetric mapping. All main functions return\n    S3 objects with print(), summary(), and plot() methods for intuitive result exploration.\n    Methods are described in Nethery et al. (2023) <doi:10.1101/2023.01.10.23284410>.\n    Further methodological details and software implementation are described in Qian et al. (in review).  "
  },
  {
    "id": 1323,
    "package_name": "textutils",
    "title": "Utilities for Handling Strings and Text",
    "description": "Utilities for handling character vectors\n  that store human-readable text (either plain or with\n  markup, such as HTML or LaTeX). The package provides,\n  in particular, functions that help with the\n  preparation of plain-text reports, e.g. for expanding\n  and aligning strings that form the lines of such\n  reports. The package also provides generic functions for\n  transforming R objects to HTML and to plain text.",
    "version": "0.4-3",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/R/packages/textutils/ ,\nhttps://git.sr.ht/~enricoschumann/textutils ,\nhttps://github.com/enricoschumann/textutils",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textutils Utilities for Handling Strings and Text Utilities for handling character vectors\n  that store human-readable text (either plain or with\n  markup, such as HTML or LaTeX). The package provides,\n  in particular, functions that help with the\n  preparation of plain-text reports, e.g. for expanding\n  and aligning strings that form the lines of such\n  reports. The package also provides generic functions for\n  transforming R objects to HTML and to plain text.  "
  },
  {
    "id": 1372,
    "package_name": "traineR",
    "title": "Predictive (Classification and Regression) Models Homologator",
    "description": "Methods to unify the different ways of creating predictive models and their different predictive formats for classification and regression. It includes  methods such as K-Nearest Neighbors Schliep, K. P. (2004) <doi:10.5282/ubm/epub.1769>, Decision Trees Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (2017) <doi:10.1201/9781315139470>,  ADA Boosting Esteban Alfaro, Matias Gamez, Noelia Garc\u00eda (2013) <doi:10.18637/jss.v054.i02>, Extreme Gradient Boosting Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>,  Random Forest Breiman (2001) <doi:10.1023/A:1010933404324>, Neural Networks Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Support Vector Machines Bennett, K. P. & Campbell, C. (2000) <doi:10.1145/380995.380999>, Bayesian Methods Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995) <doi:10.1201/9780429258411>,  Linear Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Quadratic Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>,  Logistic Regression Dobson, A. J., & Barnett, A. G. (2018) <doi:10.1201/9781315182780> and Penalized Logistic Regression Friedman, J. H., Hastie, T., & Tibshirani, R. (2010) <doi:10.18637/jss.v033.i01>.",
    "version": "2.2.9",
    "maintainer": "Oldemar Rodriguez R. <oldemar.rodriguez@ucr.ac.cr>",
    "author": "Oldemar Rodriguez R. [aut, cre],\n  Andres Navarro D. [aut],\n  Ariel Arroyo S. [aut],\n  Diego Jimenez A. [aut],\n  Jennifer Lobo V. [aut]",
    "url": "https://promidat.website/",
    "bug_reports": "https://github.com/PROMiDAT/traineR/issues",
    "repository": "https://cran.r-project.org/package=traineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "traineR Predictive (Classification and Regression) Models Homologator Methods to unify the different ways of creating predictive models and their different predictive formats for classification and regression. It includes  methods such as K-Nearest Neighbors Schliep, K. P. (2004) <doi:10.5282/ubm/epub.1769>, Decision Trees Leo Breiman, Jerome H. Friedman, Richard A. Olshen, Charles J. Stone (2017) <doi:10.1201/9781315139470>,  ADA Boosting Esteban Alfaro, Matias Gamez, Noelia Garc\u00eda (2013) <doi:10.18637/jss.v054.i02>, Extreme Gradient Boosting Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>,  Random Forest Breiman (2001) <doi:10.1023/A:1010933404324>, Neural Networks Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Support Vector Machines Bennett, K. P. & Campbell, C. (2000) <doi:10.1145/380995.380999>, Bayesian Methods Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995) <doi:10.1201/9780429258411>,  Linear Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>, Quadratic Discriminant Analysis Venables, W. N., & Ripley, B. D. (2002) <ISBN:0-387-95457-0>,  Logistic Regression Dobson, A. J., & Barnett, A. G. (2018) <doi:10.1201/9781315182780> and Penalized Logistic Regression Friedman, J. H., Hastie, T., & Tibshirani, R. (2010) <doi:10.18637/jss.v033.i01>.  "
  },
  {
    "id": 1511,
    "package_name": "ADPF",
    "title": "Use Least Squares Polynomial Regression and Statistical Testing\nto Improve Savitzky-Golay",
    "description": "This function takes a vector or matrix of data and smooths\n    the data with an improved Savitzky Golay transform. The Savitzky-Golay\n    method for data smoothing and differentiation calculates convolution\n    weights using Gram polynomials that exactly reproduce the results of\n    least-squares polynomial regression. Use of the Savitzky-Golay\n    method requires specification of both filter length and\n    polynomial degree to calculate convolution weights. For maximum\n    smoothing of statistical noise in data, polynomials with\n    low degrees are desirable, while a high polynomial degree\n    is necessary for accurate reproduction of peaks in the data.\n    Extension of the least-squares regression formalism with\n    statistical testing of additional terms of polynomial degree\n    to a heuristically chosen minimum for each data window leads\n    to an adaptive-degree polynomial filter (ADPF). Based on noise\n    reduction for data that consist of pure noise and on signal\n    reproduction for data that is purely signal, ADPF performed\n    nearly as well as the optimally chosen fixed-degree\n    Savitzky-Golay filter and outperformed sub-optimally chosen\n    Savitzky-Golay filters. For synthetic data consisting of noise\n    and signal, ADPF outperformed both optimally chosen and\n    sub-optimally chosen fixed-degree Savitzky-Golay filters. See Barak, P. (1995) <doi:10.1021/ac00113a006> for more information.",
    "version": "0.0.1",
    "maintainer": "Samuel Kruse <samdkruse@gmail.com>",
    "author": "Phillip Barak [aut],\n  Samuel Kruse [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ADPF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ADPF Use Least Squares Polynomial Regression and Statistical Testing\nto Improve Savitzky-Golay This function takes a vector or matrix of data and smooths\n    the data with an improved Savitzky Golay transform. The Savitzky-Golay\n    method for data smoothing and differentiation calculates convolution\n    weights using Gram polynomials that exactly reproduce the results of\n    least-squares polynomial regression. Use of the Savitzky-Golay\n    method requires specification of both filter length and\n    polynomial degree to calculate convolution weights. For maximum\n    smoothing of statistical noise in data, polynomials with\n    low degrees are desirable, while a high polynomial degree\n    is necessary for accurate reproduction of peaks in the data.\n    Extension of the least-squares regression formalism with\n    statistical testing of additional terms of polynomial degree\n    to a heuristically chosen minimum for each data window leads\n    to an adaptive-degree polynomial filter (ADPF). Based on noise\n    reduction for data that consist of pure noise and on signal\n    reproduction for data that is purely signal, ADPF performed\n    nearly as well as the optimally chosen fixed-degree\n    Savitzky-Golay filter and outperformed sub-optimally chosen\n    Savitzky-Golay filters. For synthetic data consisting of noise\n    and signal, ADPF outperformed both optimally chosen and\n    sub-optimally chosen fixed-degree Savitzky-Golay filters. See Barak, P. (1995) <doi:10.1021/ac00113a006> for more information.  "
  },
  {
    "id": 1514,
    "package_name": "ADTSA",
    "title": "Time Series Analysis",
    "description": "Analyzes autocorrelation and partial autocorrelation using surrogate methods and \n  bootstrapping, and computes the acceleration constants for the vectorized moving block \n  bootstrap provided by this package. It generates percentile, bias-corrected, and accelerated \n  intervals and estimates partial autocorrelations using Durbin-Levinson. This package \n  calculates the autocorrelation power spectrum, computes cross-correlations between two \n  time series, computes bandwidth for any time series, and performs autocorrelation frequency \n  analysis. It also calculates the periodicity of a time series.",
    "version": "1.0.1",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Hossein Hassani [aut],\n  Masoud Yarmohammadi [aut],\n  Mohammad Reza Yeganegi [aut],\n  Leila Marvian Mashhad [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ADTSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ADTSA Time Series Analysis Analyzes autocorrelation and partial autocorrelation using surrogate methods and \n  bootstrapping, and computes the acceleration constants for the vectorized moving block \n  bootstrap provided by this package. It generates percentile, bias-corrected, and accelerated \n  intervals and estimates partial autocorrelations using Durbin-Levinson. This package \n  calculates the autocorrelation power spectrum, computes cross-correlations between two \n  time series, computes bandwidth for any time series, and performs autocorrelation frequency \n  analysis. It also calculates the periodicity of a time series.  "
  },
  {
    "id": 1530,
    "package_name": "AHPWR",
    "title": "Compute Analytic Hierarchy Process",
    "description": "Compute a tree level hierarchy, judgment matrix, consistency index and ratio, priority vectors, hierarchic synthesis and rank. Based on the book entitled \"Models, Methods, Concepts and Applications of the Analytic Hierarchy Process\" by Saaty and Vargas (2012, ISBN 978-1-4614-3597-6).",
    "version": "0.1.0",
    "maintainer": "Luciane Ferreira Alcoforado <lucianea@id.uff.br>",
    "author": "Luciane Ferreira Alcoforado [aut, cre],\n  Lyncoln Sousa [aut],\n  Orlando Celso Longo [aut],\n  Alessandra Sim\u00e3o [ctb],\n  Steven Dutt Ross [ctb],\n  Leonardo Filgueira [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AHPWR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AHPWR Compute Analytic Hierarchy Process Compute a tree level hierarchy, judgment matrix, consistency index and ratio, priority vectors, hierarchic synthesis and rank. Based on the book entitled \"Models, Methods, Concepts and Applications of the Analytic Hierarchy Process\" by Saaty and Vargas (2012, ISBN 978-1-4614-3597-6).  "
  },
  {
    "id": 1552,
    "package_name": "AMISforInfectiousDiseases",
    "title": "Implement the AMIS Algorithm for Infectious Disease Models",
    "description": "Implements the Adaptive Multiple Importance Sampling (AMIS) algorithm, as described by Retkute et al. (2021, <doi:10.1214/21-AOAS1486>), to estimate key epidemiological parameters by combining outputs from a geostatistical model of infectious diseases (such as prevalence, incidence, or relative risk) with a disease transmission model. Utilising the resulting posterior distributions, the package enables forward projections at the local level.",
    "version": "0.1.0",
    "maintainer": "Simon Spencer <s.e.f.spencer@warwick.ac.uk>",
    "author": "Evandro Konzen [aut] (ORCID: <https://orcid.org/0000-0002-6275-1681>),\n  Renata Retkute [aut] (ORCID: <https://orcid.org/0000-0002-3877-6440>),\n  Raiha Browning [aut] (ORCID: <https://orcid.org/0000-0002-6175-2244>),\n  Thilbault Lestang [aut],\n  Simon Spencer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8375-5542>),\n  University of Warwick [cph],\n  Oxford Research Software Engineering [cph]",
    "url": "https://github.com/drsimonspencer/AMISforInfectiousDiseases-dev",
    "bug_reports": "https://github.com/drsimonspencer/AMISforInfectiousDiseases-dev/issues",
    "repository": "https://cran.r-project.org/package=AMISforInfectiousDiseases",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AMISforInfectiousDiseases Implement the AMIS Algorithm for Infectious Disease Models Implements the Adaptive Multiple Importance Sampling (AMIS) algorithm, as described by Retkute et al. (2021, <doi:10.1214/21-AOAS1486>), to estimate key epidemiological parameters by combining outputs from a geostatistical model of infectious diseases (such as prevalence, incidence, or relative risk) with a disease transmission model. Utilising the resulting posterior distributions, the package enables forward projections at the local level.  "
  },
  {
    "id": 1556,
    "package_name": "ANN2",
    "title": "Artificial Neural Networks for Anomaly Detection",
    "description": "Training of neural networks for classification and regression tasks\n    using mini-batch gradient descent. Special features include a function for \n    training autoencoders, which can be used to detect anomalies, and some \n    related plotting functions. Multiple activation functions are supported, \n    including tanh, relu, step and ramp. For the use of the step and ramp \n    activation functions in detecting anomalies using autoencoders, see \n    Hawkins et al. (2002) <doi:10.1007/3-540-46145-0_17>. Furthermore, \n    several loss functions are supported, including robust ones such as Huber \n    and pseudo-Huber loss, as well as L1 and L2 regularization. The possible \n    options for optimization algorithms are RMSprop, Adam and SGD with momentum.\n    The package contains a vectorized C++ implementation that facilitates \n    fast training through mini-batch learning.",
    "version": "2.4.0",
    "maintainer": "Bart Lammers <bart.f.lammers@gmail.com>",
    "author": "Bart Lammers [aut, cre]",
    "url": "https://github.com/bflammers/ANN2",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ANN2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ANN2 Artificial Neural Networks for Anomaly Detection Training of neural networks for classification and regression tasks\n    using mini-batch gradient descent. Special features include a function for \n    training autoencoders, which can be used to detect anomalies, and some \n    related plotting functions. Multiple activation functions are supported, \n    including tanh, relu, step and ramp. For the use of the step and ramp \n    activation functions in detecting anomalies using autoencoders, see \n    Hawkins et al. (2002) <doi:10.1007/3-540-46145-0_17>. Furthermore, \n    several loss functions are supported, including robust ones such as Huber \n    and pseudo-Huber loss, as well as L1 and L2 regularization. The possible \n    options for optimization algorithms are RMSprop, Adam and SGD with momentum.\n    The package contains a vectorized C++ implementation that facilitates \n    fast training through mini-batch learning.  "
  },
  {
    "id": 1574,
    "package_name": "APRScenario",
    "title": "Structural Scenario Analysis for Bayesian Structural Vector\nAutoregression Models",
    "description": "Implements the scenario analysis proposed by Antolin-Diaz, \n    Petrella and Rubio-Ramirez (2021) \n    \"Structural scenario analysis with SVARs\" <doi:10.1016/j.jmoneco.2020.06.001>.",
    "version": "0.0.3.0",
    "maintainer": "Giovanni Lombardo <giannilmbd@gmail.com>",
    "author": "Giovanni Lombardo [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=APRScenario",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "APRScenario Structural Scenario Analysis for Bayesian Structural Vector\nAutoregression Models Implements the scenario analysis proposed by Antolin-Diaz, \n    Petrella and Rubio-Ramirez (2021) \n    \"Structural scenario analysis with SVARs\" <doi:10.1016/j.jmoneco.2020.06.001>.  "
  },
  {
    "id": 1585,
    "package_name": "ARCokrig",
    "title": "Autoregressive Cokriging Models for Multifidelity Codes",
    "description": "For emulating multifidelity computer models. The major methods include univariate autoregressive cokriging and multivariate autoregressive cokriging. The autoregressive cokriging methods are implemented for both hierarchically nested design and non-nested design. For hierarchically nested design, the model parameters are estimated via standard optimization algorithms; For non-nested design, the model parameters are estimated via Monte Carlo expectation-maximization (MCEM) algorithms. In both cases, the priors are chosen such that the posterior distributions are proper. Notice that the uniform priors on range parameters in the correlation function lead to improper posteriors. This should be avoided when Bayesian analysis is adopted. The development of objective priors for autoregressive cokriging models can be found in Pulong Ma (2020) <DOI:10.1137/19M1289893>. The development of the multivariate autoregressive cokriging models with possibly non-nested design can be found in Pulong Ma, Georgios Karagiannis, Bledar A Konomi, Taylor G Asher, Gabriel R Toro, and Andrew T Cox (2019) <arXiv:1909.01836>.",
    "version": "0.1.2",
    "maintainer": "Pulong Ma <mpulong@gmail.com>",
    "author": "Pulong Ma [aut, cre]",
    "url": "https://CRAN.R-project.org/package=ARCokrig",
    "bug_reports": "https://github.com/pulongma/ARCokrig/issues",
    "repository": "https://cran.r-project.org/package=ARCokrig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ARCokrig Autoregressive Cokriging Models for Multifidelity Codes For emulating multifidelity computer models. The major methods include univariate autoregressive cokriging and multivariate autoregressive cokriging. The autoregressive cokriging methods are implemented for both hierarchically nested design and non-nested design. For hierarchically nested design, the model parameters are estimated via standard optimization algorithms; For non-nested design, the model parameters are estimated via Monte Carlo expectation-maximization (MCEM) algorithms. In both cases, the priors are chosen such that the posterior distributions are proper. Notice that the uniform priors on range parameters in the correlation function lead to improper posteriors. This should be avoided when Bayesian analysis is adopted. The development of objective priors for autoregressive cokriging models can be found in Pulong Ma (2020) <DOI:10.1137/19M1289893>. The development of the multivariate autoregressive cokriging models with possibly non-nested design can be found in Pulong Ma, Georgios Karagiannis, Bledar A Konomi, Taylor G Asher, Gabriel R Toro, and Andrew T Cox (2019) <arXiv:1909.01836>.  "
  },
  {
    "id": 1589,
    "package_name": "ARHT",
    "title": "Adaptable Regularized Hotelling's T^2 Test for High-Dimensional\nData",
    "description": "Perform the Adaptable Regularized Hotelling's T^2 test (ARHT) proposed by\n    Li et al., (2016) <arXiv:1609.08725>. Both one-sample and two-sample mean test are available with\n    various probabilistic alternative prior models. It contains a function to consistently\n    estimate higher order moments of the population covariance spectral distribution using\n    the spectral of the sample covariance matrix (Bai et al. (2010) <doi:10.1111/j.1467-842X.2010.00590.x>).\n    In addition, it contains a function to sample from 3-variate chi-squared random vectors approximately \n    with a given correlation matrix when the degrees of freedom are large. ",
    "version": "0.1.0",
    "maintainer": "Haoran Li <hrli@ucdavis.edu>",
    "author": "Haoran Li [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ARHT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ARHT Adaptable Regularized Hotelling's T^2 Test for High-Dimensional\nData Perform the Adaptable Regularized Hotelling's T^2 test (ARHT) proposed by\n    Li et al., (2016) <arXiv:1609.08725>. Both one-sample and two-sample mean test are available with\n    various probabilistic alternative prior models. It contains a function to consistently\n    estimate higher order moments of the population covariance spectral distribution using\n    the spectral of the sample covariance matrix (Bai et al. (2010) <doi:10.1111/j.1467-842X.2010.00590.x>).\n    In addition, it contains a function to sample from 3-variate chi-squared random vectors approximately \n    with a given correlation matrix when the degrees of freedom are large.   "
  },
  {
    "id": 1661,
    "package_name": "AhoCorasickTrie",
    "title": "Fast Searching for Multiple Keywords in Multiple Texts",
    "description": "Aho-Corasick is an optimal algorithm for finding many\n    keywords in a text. It can locate all matches in a text in O(N+M) time; i.e.,\n    the time needed scales linearly with the number of keywords (N) and the size of\n    the text (M). Compare this to the naive approach which takes O(N*M) time to loop\n    through each pattern and scan for it in the text. This implementation builds the\n    trie (the generic name of the data structure) and runs the search in a single\n    function call. If you want to search multiple texts with the same trie, the\n    function will take a list or vector of texts and return a list of matches to\n    each text. By default, all 128 ASCII characters are allowed in both the keywords\n    and the text. A more efficient trie is possible if the alphabet size can be\n    reduced. For example, DNA sequences use at most 19 distinct characters and\n    usually only 4; protein sequences use at most 26 distinct characters and usually\n    only 20. UTF-8 (Unicode) matching is not currently supported.",
    "version": "0.1.3",
    "maintainer": "Matt Chambers <matt.chambers42@gmail.com>",
    "author": "Matt Chambers [aut, cre],\n  Tomas Petricek [aut, cph],\n  Vanderbilt University [cph]",
    "url": "https://github.com/chambm/AhoCorasickTrie",
    "bug_reports": "https://github.com/chambm/AhoCorasickTrie/issues",
    "repository": "https://cran.r-project.org/package=AhoCorasickTrie",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AhoCorasickTrie Fast Searching for Multiple Keywords in Multiple Texts Aho-Corasick is an optimal algorithm for finding many\n    keywords in a text. It can locate all matches in a text in O(N+M) time; i.e.,\n    the time needed scales linearly with the number of keywords (N) and the size of\n    the text (M). Compare this to the naive approach which takes O(N*M) time to loop\n    through each pattern and scan for it in the text. This implementation builds the\n    trie (the generic name of the data structure) and runs the search in a single\n    function call. If you want to search multiple texts with the same trie, the\n    function will take a list or vector of texts and return a list of matches to\n    each text. By default, all 128 ASCII characters are allowed in both the keywords\n    and the text. A more efficient trie is possible if the alphabet size can be\n    reduced. For example, DNA sequences use at most 19 distinct characters and\n    usually only 4; protein sequences use at most 26 distinct characters and usually\n    only 20. UTF-8 (Unicode) matching is not currently supported.  "
  },
  {
    "id": 1662,
    "package_name": "AiES",
    "title": "Axon Integrity Evaluation System for Microscopy Images",
    "description": "Provides tools for the quantitative analysis of axon integrity in microscopy images. \n  It implements image pre-processing, adaptive thresholding, feature extraction, and support vector machine-based classification to compute indices such as the Axon Integrity Index (AII) and Degeneration Index (DI).\n  The package is designed for reproducible and automated analysis in neuroscience research.",
    "version": "0.99.6",
    "maintainer": "Shinji Tokunaga <tokunaga@ncnp.go.jp>",
    "author": "Shinji Tokunaga [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8398-3054>),\n  Masafumi Funakoshi [ctb],\n  Toshiyuki Araki [ctb] (ORCID: <https://orcid.org/0000-0003-3625-2042>)",
    "url": "https://github.com/BreezyCave/AiES",
    "bug_reports": "https://github.com/BreezyCave/AiES/issues",
    "repository": "https://cran.r-project.org/package=AiES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AiES Axon Integrity Evaluation System for Microscopy Images Provides tools for the quantitative analysis of axon integrity in microscopy images. \n  It implements image pre-processing, adaptive thresholding, feature extraction, and support vector machine-based classification to compute indices such as the Axon Integrity Index (AII) and Degeneration Index (DI).\n  The package is designed for reproducible and automated analysis in neuroscience research.  "
  },
  {
    "id": 1666,
    "package_name": "AirportProblems",
    "title": "Analysis of Cost Allocation for Airport Problems",
    "description": "Airport problems, introduced by Littlechild and Owen (1973)\n    <https://www.jstor.org/stable/2629727>, are cost allocation \n    problems where agents share the cost of a facility (or service) based on \n    their ordered needs. Valid allocations must satisfy no-subsidy constraints, \n    meaning that no group of agents contributes more than the highest cost of \n    its members (i.e., no agent is allowed to subsidize another). A rule is a \n    mechanism that selects an allocation vector for a given problem. This \n    package computes several rules proposed in the literature, including both \n    standard rules and their variants, such as weighted versions, rules for \n    clones, and rules based on the agents\u2019 hierarchy order. These rules can be \n    applied to various problems of interest, including the allocation of \n    liabilities and the maintenance of irrigation systems, among others.  \n    Moreover, the package provides functions for graphical representation, \n    enabling users to visually compare the outcomes produced by each rule, \n    or to display the no-subsidy set. In addition, it includes four datasets \n    illustrating different applications and examples of airport problems. For a \n    more detailed explanation of all concepts, see Thomson (2024)\n    <doi:10.1016/j.mathsocsci.2024.03.007>.",
    "version": "0.1.0",
    "maintainer": "Alejandro Bern\u00e1rdez Ferrad\u00e1s <alejandro.bernardez@uvigo.es>",
    "author": "Alejandro Bern\u00e1rdez Ferrad\u00e1s [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-0960-3555>, SiDOR. Departamento de\n    Estat\u00edstica e Investigaci\u00f3n Operativa. Universidade de Vigo.\n    CITMAga. Spain),\n  Estela S\u00e1nchez Rodr\u00edguez [aut] (ORCID:\n    <https://orcid.org/0000-0002-0933-6411>, SiDOR. Departamento de\n    Estat\u00edstica e Investigaci\u00f3n Operativa. Universidade de Vigo.\n    CITMAga. Spain),\n  Miguel \u00c1ngel Mir\u00e1s Calvo [aut] (ORCID:\n    <https://orcid.org/0000-0001-7247-1926>, RGEAF. Departamento de\n    Matem\u00e1ticas. Universidade de Vigo. Spain),\n  Carmen Quinteiro Sandomingo [aut] (ORCID:\n    <https://orcid.org/0000-0002-2711-1945>, Departamento de\n    Matem\u00e1ticas. Universidade de Vigo. Spain),\n  MCIN/AEI/10.13039/501100011033 [fnd] (Project PID2021-124030NB-C33.\n    ERDF A way of making Europe/EU)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AirportProblems",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AirportProblems Analysis of Cost Allocation for Airport Problems Airport problems, introduced by Littlechild and Owen (1973)\n    <https://www.jstor.org/stable/2629727>, are cost allocation \n    problems where agents share the cost of a facility (or service) based on \n    their ordered needs. Valid allocations must satisfy no-subsidy constraints, \n    meaning that no group of agents contributes more than the highest cost of \n    its members (i.e., no agent is allowed to subsidize another). A rule is a \n    mechanism that selects an allocation vector for a given problem. This \n    package computes several rules proposed in the literature, including both \n    standard rules and their variants, such as weighted versions, rules for \n    clones, and rules based on the agents\u2019 hierarchy order. These rules can be \n    applied to various problems of interest, including the allocation of \n    liabilities and the maintenance of irrigation systems, among others.  \n    Moreover, the package provides functions for graphical representation, \n    enabling users to visually compare the outcomes produced by each rule, \n    or to display the no-subsidy set. In addition, it includes four datasets \n    illustrating different applications and examples of airport problems. For a \n    more detailed explanation of all concepts, see Thomson (2024)\n    <doi:10.1016/j.mathsocsci.2024.03.007>.  "
  },
  {
    "id": 1693,
    "package_name": "AncReg",
    "title": "Ancestor Regression",
    "description": "Causal discovery in linear structural equation models (Schultheiss, and B\u00fchlmann (2023) <doi:10.1093/biomet/asad008>) and vector autoregressive models (Schultheiss, Ulmer, and B\u00fchlmann (2025) <doi:10.1515/jci-2024-0011>) with explicit error control for false discovery, at least asymptotically.",
    "version": "1.0.1",
    "maintainer": "Markus Ulmer <markus.ulmer@stat.math.ethz.ch>",
    "author": "Christoph Schultheiss [aut],\n  Markus Ulmer [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7783-8475>)",
    "url": "http://www.markus-ulmer.ch/AncReg/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AncReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AncReg Ancestor Regression Causal discovery in linear structural equation models (Schultheiss, and B\u00fchlmann (2023) <doi:10.1093/biomet/asad008>) and vector autoregressive models (Schultheiss, Ulmer, and B\u00fchlmann (2025) <doi:10.1515/jci-2024-0011>) with explicit error control for false discovery, at least asymptotically.  "
  },
  {
    "id": 1727,
    "package_name": "AriGaMyANNSVR",
    "title": "Hybrid ARIMA-GARCH and Two Specially Designed ML-Based Models",
    "description": "Describes a series first. After that does time series analysis using one hybrid model and two specially structured Machine Learning (ML) (Artificial Neural Network or ANN and Support Vector Regression or SVR) models. More information can be obtained from Paul and Garai (2022) <doi:10.1007/s41096-022-00128-3>. ",
    "version": "0.1.0",
    "maintainer": "Mr. Sandip Garai <sandipnicksandy@gmail.com>",
    "author": "Mr. Sandip Garai [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AriGaMyANNSVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AriGaMyANNSVR Hybrid ARIMA-GARCH and Two Specially Designed ML-Based Models Describes a series first. After that does time series analysis using one hybrid model and two specially structured Machine Learning (ML) (Artificial Neural Network or ANN and Support Vector Regression or SVR) models. More information can be obtained from Paul and Garai (2022) <doi:10.1007/s41096-022-00128-3>.   "
  },
  {
    "id": 1754,
    "package_name": "AutoregressionMDE",
    "title": "Minimum Distance Estimation in Autoregressive Model",
    "description": "Consider autoregressive model of order p where the distribution function of innovation is unknown, but innovations are independent and symmetrically distributed. The package contains a function named ARMDE which takes X (vector of n observations) and p (order of the model) as input argument and returns minimum distance estimator of the parameters in the model.",
    "version": "1.0",
    "maintainer": "Jiwoong Kim <kimjiwo2@stt.msu.edu>",
    "author": "Jiwoong Kim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AutoregressionMDE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AutoregressionMDE Minimum Distance Estimation in Autoregressive Model Consider autoregressive model of order p where the distribution function of innovation is unknown, but innovations are independent and symmetrically distributed. The package contains a function named ARMDE which takes X (vector of n observations) and p (order of the model) as input argument and returns minimum distance estimator of the parameters in the model.  "
  },
  {
    "id": 1835,
    "package_name": "BGVAR",
    "title": "Bayesian Global Vector Autoregressions",
    "description": "Estimation of Bayesian Global Vector Autoregressions (BGVAR) with different prior setups and the possibility to introduce stochastic volatility. Built-in priors include the Minnesota, the stochastic search variable selection and Normal-Gamma (NG) prior. For a reference see also Crespo Cuaresma, J., Feldkircher, M. and F. Huber (2016) \"Forecasting with Global Vector Autoregressive Models: a Bayesian Approach\", Journal of Applied Econometrics, Vol. 31(7), pp. 1371-1391 <doi:10.1002/jae.2504>. Post-processing functions allow for doing predictions, structurally identify the model with short-run or sign-restrictions and compute impulse response functions, historical decompositions and forecast error variance decompositions. Plotting functions are also available. The package has a companion paper: Boeck, M., Feldkircher, M. and F. Huber (2022) \"BGVAR: Bayesian Global Vector Autoregressions with Shrinkage Priors in R\", Journal of Statistical Software, Vol. 104(9), pp. 1-28 <doi:10.18637/jss.v104.i09>.",
    "version": "2.5.9",
    "maintainer": "Maximilian Boeck <maximilian.boeck@fau.de>",
    "author": "Maximilian Boeck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6024-8305>),\n  Martin Feldkircher [aut] (ORCID:\n    <https://orcid.org/0000-0002-5511-9215>),\n  Florian Huber [aut] (ORCID: <https://orcid.org/0000-0002-2896-7921>),\n  Darjus Hosszejni [ctb] (ORCID: <https://orcid.org/0000-0002-3803-691X>)",
    "url": "https://github.com/mboeck11/BGVAR",
    "bug_reports": "https://github.com/mboeck11/BGVAR/issues",
    "repository": "https://cran.r-project.org/package=BGVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BGVAR Bayesian Global Vector Autoregressions Estimation of Bayesian Global Vector Autoregressions (BGVAR) with different prior setups and the possibility to introduce stochastic volatility. Built-in priors include the Minnesota, the stochastic search variable selection and Normal-Gamma (NG) prior. For a reference see also Crespo Cuaresma, J., Feldkircher, M. and F. Huber (2016) \"Forecasting with Global Vector Autoregressive Models: a Bayesian Approach\", Journal of Applied Econometrics, Vol. 31(7), pp. 1371-1391 <doi:10.1002/jae.2504>. Post-processing functions allow for doing predictions, structurally identify the model with short-run or sign-restrictions and compute impulse response functions, historical decompositions and forecast error variance decompositions. Plotting functions are also available. The package has a companion paper: Boeck, M., Feldkircher, M. and F. Huber (2022) \"BGVAR: Bayesian Global Vector Autoregressions with Shrinkage Priors in R\", Journal of Statistical Software, Vol. 104(9), pp. 1-28 <doi:10.18637/jss.v104.i09>.  "
  },
  {
    "id": 1840,
    "package_name": "BHSBVAR",
    "title": "Structural Bayesian Vector Autoregression Models",
    "description": "Provides a function for estimating the parameters of Structural Bayesian Vector Autoregression models with the method developed by Baumeister and Hamilton (2015) <doi:10.3982/ECTA12356>, Baumeister and Hamilton (2017) <doi:10.3386/w24167>, and Baumeister and Hamilton (2018) <doi:10.1016/j.jmoneco.2018.06.005>. Functions for plotting impulse responses, historical decompositions, and posterior distributions of model parameters are also provided.",
    "version": "3.1.3",
    "maintainer": "Paul Richardson <p.richardson.54391@gmail.com>",
    "author": "Paul Richardson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BHSBVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BHSBVAR Structural Bayesian Vector Autoregression Models Provides a function for estimating the parameters of Structural Bayesian Vector Autoregression models with the method developed by Baumeister and Hamilton (2015) <doi:10.3982/ECTA12356>, Baumeister and Hamilton (2017) <doi:10.3386/w24167>, and Baumeister and Hamilton (2018) <doi:10.1016/j.jmoneco.2018.06.005>. Functions for plotting impulse responses, historical decompositions, and posterior distributions of model parameters are also provided.  "
  },
  {
    "id": 1857,
    "package_name": "BKTR",
    "title": "Bayesian Kernelized Tensor Regression",
    "description": "Facilitates scalable spatiotemporally varying coefficient\n    modelling with Bayesian kernelized tensor regression.\n    The important features of this package are:\n    (a) Enabling local temporal and spatial modeling of the relationship between\n    the response variable and covariates.\n    (b) Implementing the model described by Lei et al. (2023) <doi:10.48550/arXiv.2109.00046>.\n    (c) Using a Bayesian Markov Chain Monte Carlo (MCMC) algorithm to sample from the posterior\n    distribution of the model parameters.\n    (d) Employing a tensor decomposition to reduce the number of estimated parameters.\n    (e) Accelerating tensor operations and enabling graphics processing unit (GPU) acceleration\n    with the 'torch' package.",
    "version": "0.2.0",
    "maintainer": "Julien Lanthier <julien.lanthier@hec.ca>",
    "author": "Julien Lanthier [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0008-8728-4996>),\n  Mengying Lei [aut] (ORCID: <https://orcid.org/0000-0001-7343-3323>),\n  Aur\u00e9lie Labbe [aut] (ORCID: <https://orcid.org/0000-0002-4207-8143>),\n  Lijun Sun [aut] (ORCID: <https://orcid.org/0000-0001-9488-0712>)",
    "url": "",
    "bug_reports": "https://github.com/julien-hec/BKTR/issues",
    "repository": "https://cran.r-project.org/package=BKTR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BKTR Bayesian Kernelized Tensor Regression Facilitates scalable spatiotemporally varying coefficient\n    modelling with Bayesian kernelized tensor regression.\n    The important features of this package are:\n    (a) Enabling local temporal and spatial modeling of the relationship between\n    the response variable and covariates.\n    (b) Implementing the model described by Lei et al. (2023) <doi:10.48550/arXiv.2109.00046>.\n    (c) Using a Bayesian Markov Chain Monte Carlo (MCMC) algorithm to sample from the posterior\n    distribution of the model parameters.\n    (d) Employing a tensor decomposition to reduce the number of estimated parameters.\n    (e) Accelerating tensor operations and enabling graphics processing unit (GPU) acceleration\n    with the 'torch' package.  "
  },
  {
    "id": 1926,
    "package_name": "BVAR",
    "title": "Hierarchical Bayesian Vector Autoregression",
    "description": "Estimation of hierarchical Bayesian vector autoregressive models\n    following Kuschnig & Vashold (2021) <doi:10.18637/jss.v100.i14>.\n    Implements hierarchical prior selection for conjugate priors in the fashion\n    of Giannone, Lenza & Primiceri (2015) <doi:10.1162/REST_a_00483>.\n    Functions to compute and identify impulse responses, calculate forecasts,\n    forecast error variance decompositions and scenarios are available.\n    Several methods to print, plot and summarise results facilitate analysis.",
    "version": "1.0.5",
    "maintainer": "Nikolas Kuschnig <nikolas.kuschnig@wu.ac.at>",
    "author": "Nikolas Kuschnig [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6642-2543>),\n  Lukas Vashold [aut] (ORCID: <https://orcid.org/0000-0002-3562-3414>),\n  Nirai Tomass [ctb],\n  Michael McCracken [dtc],\n  Serena Ng [dtc]",
    "url": "https://github.com/nk027/bvar",
    "bug_reports": "https://github.com/nk027/bvar/issues",
    "repository": "https://cran.r-project.org/package=BVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BVAR Hierarchical Bayesian Vector Autoregression Estimation of hierarchical Bayesian vector autoregressive models\n    following Kuschnig & Vashold (2021) <doi:10.18637/jss.v100.i14>.\n    Implements hierarchical prior selection for conjugate priors in the fashion\n    of Giannone, Lenza & Primiceri (2015) <doi:10.1162/REST_a_00483>.\n    Functions to compute and identify impulse responses, calculate forecasts,\n    forecast error variance decompositions and scenarios are available.\n    Several methods to print, plot and summarise results facilitate analysis.  "
  },
  {
    "id": 1927,
    "package_name": "BVARverse",
    "title": "Tidy Bayesian Vector Autoregression",
    "description": "Functions to prepare tidy objects from estimated models via 'BVAR'\n    (see Kuschnig & Vashold, 2019 <doi:10.13140/RG.2.2.25541.60643>) and\n    visualisation thereof. Bridges the gap between estimating models with 'BVAR'\n    and plotting the results in a more sophisticated way with 'ggplot2' as well\n    as passing them on in a tidy format. ",
    "version": "0.0.1",
    "maintainer": "Lukas Vashold <lukas.vashold@wu.ac.at>",
    "author": "Lukas Vashold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3562-3414>),\n  Nikolas Kuschnig [aut] (ORCID: <https://orcid.org/0000-0002-6642-2543>)",
    "url": "https://github.com/nk027/bvarverse",
    "bug_reports": "https://github.com/nk027/bvarverse/issues",
    "repository": "https://cran.r-project.org/package=BVARverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BVARverse Tidy Bayesian Vector Autoregression Functions to prepare tidy objects from estimated models via 'BVAR'\n    (see Kuschnig & Vashold, 2019 <doi:10.13140/RG.2.2.25541.60643>) and\n    visualisation thereof. Bridges the gap between estimating models with 'BVAR'\n    and plotting the results in a more sophisticated way with 'ggplot2' as well\n    as passing them on in a tidy format.   "
  },
  {
    "id": 1989,
    "package_name": "BayesMultMeta",
    "title": "Bayesian Multivariate Meta-Analysis",
    "description": "Objective Bayesian inference procedures for the parameters of the\n    multivariate random effects model with application to multivariate\n    meta-analysis. The posterior for the model parameters, namely the overall\n    mean vector and the between-study covariance matrix, are assessed by\n    constructing Markov chains based on the Metropolis-Hastings algorithms as\n    developed in Bodnar and Bodnar (2021) (<arXiv:2104.02105>). The\n    Metropolis-Hastings algorithm is designed under the assumption of the\n    normal distribution and the t-distribution when the Berger and Bernardo\n    reference prior and the Jeffreys prior are assigned to the model parameters.\n    Convergence properties of the generated Markov chains are investigated by\n    the rank plots and the split hat-R estimate based on the rank normalization,\n    which are proposed in Vehtari et al. (2021) (<DOI:10.1214/20-BA1221>).",
    "version": "0.1.1",
    "maintainer": "Erik Thors\u00e9n <erik.thorsen@math.su.se>",
    "author": "Olha Bodnar [aut] (ORCID: <https://orcid.org/0000-0003-1359-3311>),\n  Taras Bodnar [aut] (ORCID: <https://orcid.org/0000-0001-7855-8221>),\n  Erik Thors\u00e9n [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5992-1216>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesMultMeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesMultMeta Bayesian Multivariate Meta-Analysis Objective Bayesian inference procedures for the parameters of the\n    multivariate random effects model with application to multivariate\n    meta-analysis. The posterior for the model parameters, namely the overall\n    mean vector and the between-study covariance matrix, are assessed by\n    constructing Markov chains based on the Metropolis-Hastings algorithms as\n    developed in Bodnar and Bodnar (2021) (<arXiv:2104.02105>). The\n    Metropolis-Hastings algorithm is designed under the assumption of the\n    normal distribution and the t-distribution when the Berger and Bernardo\n    reference prior and the Jeffreys prior are assigned to the model parameters.\n    Convergence properties of the generated Markov chains are investigated by\n    the rank plots and the split hat-R estimate based on the rank normalization,\n    which are proposed in Vehtari et al. (2021) (<DOI:10.1214/20-BA1221>).  "
  },
  {
    "id": 2052,
    "package_name": "BiCausality",
    "title": "Binary Causality Inference Framework",
    "description": "A framework to infer causality on binary data using techniques in frequent pattern mining and estimation statistics. Given a set of individual vectors S={x} where x(i) is a realization value of binary variable i, the framework infers empirical causal relations of binary variables i,j from S in a form of causal graph G=(V,E) where V is a set of nodes representing binary variables and there is an edge from i to j in E if the variable i causes j. The framework determines dependency among variables as well as analyzing confounding factors before deciding whether i causes j.  The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2023) <doi:10.1016/j.heliyon.2023.e15947>.",
    "version": "0.1.4",
    "maintainer": "Chainarong Amornbunchornvej <grandca@gmail.com>",
    "author": "Chainarong Amornbunchornvej [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3131-0370>)",
    "url": "https://github.com/DarkEyes/BiCausality",
    "bug_reports": "https://github.com/DarkEyes/BiCausality/issues",
    "repository": "https://cran.r-project.org/package=BiCausality",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BiCausality Binary Causality Inference Framework A framework to infer causality on binary data using techniques in frequent pattern mining and estimation statistics. Given a set of individual vectors S={x} where x(i) is a realization value of binary variable i, the framework infers empirical causal relations of binary variables i,j from S in a form of causal graph G=(V,E) where V is a set of nodes representing binary variables and there is an edge from i to j in E if the variable i causes j. The framework determines dependency among variables as well as analyzing confounding factors before deciding whether i causes j.  The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2023) <doi:10.1016/j.heliyon.2023.e15947>.  "
  },
  {
    "id": 2118,
    "package_name": "BlockwiseRankTest",
    "title": "Block-Wise Rank in Similarity Graph Edge-Count Two-Sample Test\n(BRISE)",
    "description": "Implements the Block-wise Rank in Similarity Graph Edge-count \n    test (BRISE), a rank-based two-sample test designed for block-wise \n    missing data. The method constructs (pattern) pair-wise similarity graphs \n    and derives quadratic test statistics with asymptotic chi-square \n    distribution or permutation-based p-values. It provides both \n    vectorized and congregated versions for flexible inference. \n    The methodology is described in Zhang, Liang, Maile, and \n    Zhou (2025) <doi:10.48550/arXiv.2508.17411>.",
    "version": "0.1.0",
    "maintainer": "Kejian Zhang <kejianzhang@u.nus.edu>",
    "author": "Kejian Zhang [aut, cre],\n  Doudou Zhou [aut] (ORCID: <https://orcid.org/0000-0002-0830-2287>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BlockwiseRankTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BlockwiseRankTest Block-Wise Rank in Similarity Graph Edge-Count Two-Sample Test\n(BRISE) Implements the Block-wise Rank in Similarity Graph Edge-count \n    test (BRISE), a rank-based two-sample test designed for block-wise \n    missing data. The method constructs (pattern) pair-wise similarity graphs \n    and derives quadratic test statistics with asymptotic chi-square \n    distribution or permutation-based p-values. It provides both \n    vectorized and congregated versions for flexible inference. \n    The methodology is described in Zhang, Liang, Maile, and \n    Zhou (2025) <doi:10.48550/arXiv.2508.17411>.  "
  },
  {
    "id": 2180,
    "package_name": "CAOP.RAA.2024",
    "title": "Official Administrative Map of the Azores (CAOP 2024)",
    "description": "Provides the official administrative boundaries of the Azores \n    (Regi\u00e3o Aut\u00f3noma dos A\u00e7ores (RAA)) as defined in the 2024 edition of the \n    Carta Administrativa Oficial de Portugal (CAOP), published by the \n    Dire\u00e7\u00e3o-Geral do Territ\u00f3rio (DGT). The package includes convenience\n    functions to import these boundaries as 'sf' objects for spatial analysis in\n    R.\n    Source: <https://geo2.dgterritorio.gov.pt/caop/CAOP_RAA_2024-gpkg.zip>.",
    "version": "0.0.5",
    "maintainer": "Ramiro Magno <rmagno@pattern.institute>",
    "author": "Ramiro Magno [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5226-3441>),\n  Pattern Institute [cph, fnd]",
    "url": "https://github.com/patterninstitute/CAOP.RAA.2024,\nhttps://www.pattern.institute/CAOP.RAA.2024/",
    "bug_reports": "https://github.com/patterninstitute/CAOP.RAA.2024/issues",
    "repository": "https://cran.r-project.org/package=CAOP.RAA.2024",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CAOP.RAA.2024 Official Administrative Map of the Azores (CAOP 2024) Provides the official administrative boundaries of the Azores \n    (Regi\u00e3o Aut\u00f3noma dos A\u00e7ores (RAA)) as defined in the 2024 edition of the \n    Carta Administrativa Oficial de Portugal (CAOP), published by the \n    Dire\u00e7\u00e3o-Geral do Territ\u00f3rio (DGT). The package includes convenience\n    functions to import these boundaries as 'sf' objects for spatial analysis in\n    R.\n    Source: <https://geo2.dgterritorio.gov.pt/caop/CAOP_RAA_2024-gpkg.zip>.  "
  },
  {
    "id": 2181,
    "package_name": "CARBayes",
    "title": "Spatial Generalised Linear Mixed Models for Areal Unit Data",
    "description": "Implements a class of univariate and multivariate spatial generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation using a single or multiple Markov chains. The response variable can be binomial, Gaussian, multinomial, Poisson or zero-inflated Poisson (ZIP), and spatial autocorrelation is modelled by a set of random effects that are assigned a conditional autoregressive (CAR) prior distribution. A number of different models are available for univariate spatial data, including models with no random effects as well as random effects modelled by different types of CAR prior, including the BYM model (Besag et al., 1991, <doi:10.1007/BF00116466>) and Leroux model (Leroux et al., 2000, <doi:10.1007/978-1-4612-1284-3_4>). Additionally,  a multivariate CAR (MCAR) model for multivariate spatial data is available, as is a two-level hierarchical model for modelling data relating to individuals within areas. Full details are given in the vignette accompanying this package. The initial creation of this package was supported by the Economic and Social Research Council (ESRC) grant RES-000-22-4256, and on-going development has been supported by the Engineering and Physical Science Research Council (EPSRC) grant EP/J017442/1, ESRC grant ES/K006460/1, Innovate UK / Natural Environment Research Council (NERC) grant NE/N007352/1 and the TB Alliance. ",
    "version": "6.1.1",
    "maintainer": "Duncan Lee <Duncan.Lee@glasgow.ac.uk>",
    "author": "Duncan Lee",
    "url": "https://github.com/duncanplee/CARBayes",
    "bug_reports": "https://github.com/duncanplee/CARBayes/issues",
    "repository": "https://cran.r-project.org/package=CARBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CARBayes Spatial Generalised Linear Mixed Models for Areal Unit Data Implements a class of univariate and multivariate spatial generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation using a single or multiple Markov chains. The response variable can be binomial, Gaussian, multinomial, Poisson or zero-inflated Poisson (ZIP), and spatial autocorrelation is modelled by a set of random effects that are assigned a conditional autoregressive (CAR) prior distribution. A number of different models are available for univariate spatial data, including models with no random effects as well as random effects modelled by different types of CAR prior, including the BYM model (Besag et al., 1991, <doi:10.1007/BF00116466>) and Leroux model (Leroux et al., 2000, <doi:10.1007/978-1-4612-1284-3_4>). Additionally,  a multivariate CAR (MCAR) model for multivariate spatial data is available, as is a two-level hierarchical model for modelling data relating to individuals within areas. Full details are given in the vignette accompanying this package. The initial creation of this package was supported by the Economic and Social Research Council (ESRC) grant RES-000-22-4256, and on-going development has been supported by the Engineering and Physical Science Research Council (EPSRC) grant EP/J017442/1, ESRC grant ES/K006460/1, Innovate UK / Natural Environment Research Council (NERC) grant NE/N007352/1 and the TB Alliance.   "
  },
  {
    "id": 2183,
    "package_name": "CARBayesdata",
    "title": "Data Used in the Vignettes Accompanying the CARBayes and\nCARBayesST Packages",
    "description": "Spatio-temporal data from Scotland used in the vignettes accompanying the CARBayes (spatial modelling) and CARBayesST (spatio-temporal modelling) packages. Most of the data relate to the set of 271 Intermediate Zones (IZ)  that make up the 2001 definition of the  Greater Glasgow and Clyde health board. ",
    "version": "3.0",
    "maintainer": "Duncan Lee <Duncan.Lee@glasgow.ac.uk>",
    "author": "Duncan Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CARBayesdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CARBayesdata Data Used in the Vignettes Accompanying the CARBayes and\nCARBayesST Packages Spatio-temporal data from Scotland used in the vignettes accompanying the CARBayes (spatial modelling) and CARBayesST (spatio-temporal modelling) packages. Most of the data relate to the set of 271 Intermediate Zones (IZ)  that make up the 2001 definition of the  Greater Glasgow and Clyde health board.   "
  },
  {
    "id": 2188,
    "package_name": "CASCORE",
    "title": "Covariate Assisted Spectral Clustering on Ratios of Eigenvectors",
    "description": "Functions for implementing the novel algorithm CASCORE, which is designed to detect latent community structure in graphs with node covariates. This algorithm can handle models such as the covariate-assisted degree corrected stochastic block model (CADCSBM). CASCORE specifically addresses the disagreement between the community structure inferred from the adjacency information and the community structure inferred from the covariate information. For more detailed information, please refer to the reference paper: Yaofang Hu and Wanjie Wang (2022) <arXiv:2306.15616>. \n    In addition to CASCORE, this package includes several classical community detection algorithms that are compared to CASCORE in our paper. These algorithms are: Spectral Clustering On Ratios-of Eigenvectors (SCORE), normalized PCA, ordinary PCA, network-based clustering, covariates-based clustering and covariate-assisted spectral clustering (CASC). By providing these additional algorithms, the package enables users to compare their performance with CASCORE in community detection tasks.",
    "version": "0.1.2",
    "maintainer": "Yaofang Hu <yaofangh@smu.edu>",
    "author": "Yaofang Hu [aut, cre],\n  Wanjie Wang [aut]",
    "url": "https://arxiv.org/abs/2306.15616",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CASCORE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CASCORE Covariate Assisted Spectral Clustering on Ratios of Eigenvectors Functions for implementing the novel algorithm CASCORE, which is designed to detect latent community structure in graphs with node covariates. This algorithm can handle models such as the covariate-assisted degree corrected stochastic block model (CADCSBM). CASCORE specifically addresses the disagreement between the community structure inferred from the adjacency information and the community structure inferred from the covariate information. For more detailed information, please refer to the reference paper: Yaofang Hu and Wanjie Wang (2022) <arXiv:2306.15616>. \n    In addition to CASCORE, this package includes several classical community detection algorithms that are compared to CASCORE in our paper. These algorithms are: Spectral Clustering On Ratios-of Eigenvectors (SCORE), normalized PCA, ordinary PCA, network-based clustering, covariates-based clustering and covariate-assisted spectral clustering (CASC). By providing these additional algorithms, the package enables users to compare their performance with CASCORE in community detection tasks.  "
  },
  {
    "id": 2191,
    "package_name": "CAST",
    "title": "'caret' Applications for Spatial-Temporal Models",
    "description": "Supporting functionality to run 'caret' with spatial or spatial-temporal data. 'caret' is a frequently used package for model training and prediction using machine learning. CAST includes functions to improve spatial or spatial-temporal modelling tasks using 'caret'. It includes the newly suggested 'Nearest neighbor distance matching' cross-validation to estimate the performance of spatial prediction models and allows for spatial variable selection to selects suitable predictor variables in view to their contribution to the spatial model performance. CAST further includes functionality to estimate the (spatial) area of applicability of prediction models. Methods are described in Meyer et al. (2018) <doi:10.1016/j.envsoft.2017.12.001>; Meyer et al. (2019) <doi:10.1016/j.ecolmodel.2019.108815>; Meyer and Pebesma (2021) <doi:10.1111/2041-210X.13650>; Mil\u00e0 et al. (2022) <doi:10.1111/2041-210X.13851>; Meyer and Pebesma (2022) <doi:10.1038/s41467-022-29838-9>; Linnenbrink et al. (2023) <doi:10.5194/egusphere-2023-1308>; Schumacher et al. (2024) <doi:10.5194/egusphere-2024-2730>. The package is described in detail in Meyer et al. (2024) <doi:10.48550/arXiv.2404.06978>.",
    "version": "1.0.3",
    "maintainer": "Hanna Meyer <hanna.meyer@uni-muenster.de>",
    "author": "Hanna Meyer [cre, aut],\n  Carles Mil\u00e0 [aut],\n  Marvin Ludwig [aut],\n  Jan Linnenbrink [aut],\n  Fabian Schumacher [aut],\n  Philipp Otto [ctb],\n  Chris Reudenbach [ctb],\n  Thomas Nauss [ctb],\n  Edzer Pebesma [ctb],\n  Jakub Nowosad [ctb]",
    "url": "https://github.com/HannaMeyer/CAST,\nhttps://hannameyer.github.io/CAST/",
    "bug_reports": "https://github.com/HannaMeyer/CAST/issues/",
    "repository": "https://cran.r-project.org/package=CAST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CAST 'caret' Applications for Spatial-Temporal Models Supporting functionality to run 'caret' with spatial or spatial-temporal data. 'caret' is a frequently used package for model training and prediction using machine learning. CAST includes functions to improve spatial or spatial-temporal modelling tasks using 'caret'. It includes the newly suggested 'Nearest neighbor distance matching' cross-validation to estimate the performance of spatial prediction models and allows for spatial variable selection to selects suitable predictor variables in view to their contribution to the spatial model performance. CAST further includes functionality to estimate the (spatial) area of applicability of prediction models. Methods are described in Meyer et al. (2018) <doi:10.1016/j.envsoft.2017.12.001>; Meyer et al. (2019) <doi:10.1016/j.ecolmodel.2019.108815>; Meyer and Pebesma (2021) <doi:10.1111/2041-210X.13650>; Mil\u00e0 et al. (2022) <doi:10.1111/2041-210X.13851>; Meyer and Pebesma (2022) <doi:10.1038/s41467-022-29838-9>; Linnenbrink et al. (2023) <doi:10.5194/egusphere-2023-1308>; Schumacher et al. (2024) <doi:10.5194/egusphere-2024-2730>. The package is described in detail in Meyer et al. (2024) <doi:10.48550/arXiv.2404.06978>.  "
  },
  {
    "id": 2220,
    "package_name": "CDFt",
    "title": "Downscaling and Bias Correction via Non-Parametric CDF-Transform",
    "description": "Statistical downscaling and bias correction (model output statistics) method based on cumulative distribution functions (CDF) transformation. See Michelangeli, Vrac, Loukos (2009) Probabilistic downscaling approaches: Application to wind cumulative distribution functions. Geophysical Research Letters, 36, L11708, <doi:10.1029/2009GL038401>. ; and Vrac, Drobinski, Merlo, Herrmann, Lavaysse, Li, Somot (2012) Dynamical and statistical downscaling of the French Mediterranean climate: uncertainty assessment. Nat. Hazards Earth Syst. Sci., 12, 2769-2784, www.nat-hazards-earth-syst-sci.net/12/2769/2012/, <doi:10.5194/nhess-12-2769-2012>.",
    "version": "1.2",
    "maintainer": "Mathieu Vrac <mathieu.vrac@lsce.ipsl.fr>",
    "author": "Mathieu Vrac <mathieu.vrac@lsce.ipsl.fr> and Paul-Antoine Michelangeli <pam@climpact.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CDFt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CDFt Downscaling and Bias Correction via Non-Parametric CDF-Transform Statistical downscaling and bias correction (model output statistics) method based on cumulative distribution functions (CDF) transformation. See Michelangeli, Vrac, Loukos (2009) Probabilistic downscaling approaches: Application to wind cumulative distribution functions. Geophysical Research Letters, 36, L11708, <doi:10.1029/2009GL038401>. ; and Vrac, Drobinski, Merlo, Herrmann, Lavaysse, Li, Somot (2012) Dynamical and statistical downscaling of the French Mediterranean climate: uncertainty assessment. Nat. Hazards Earth Syst. Sci., 12, 2769-2784, www.nat-hazards-earth-syst-sci.net/12/2769/2012/, <doi:10.5194/nhess-12-2769-2012>.  "
  },
  {
    "id": 2232,
    "package_name": "CEEMDANML",
    "title": "CEEMDAN Decomposition Based Hybrid Machine Learning Models",
    "description": "Noise in the time-series data significantly affects the accuracy of the Machine Learning (ML) models (Artificial Neural Network and Support Vector Regression are considered here). Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) decomposes the time series data into sub-series and help to improve the model performance. The models can achieve higher prediction accuracy than the traditional ML models. Two models have been provided here for time series forecasting. More information may be obtained from Garai and Paul (2023) <doi:10.1016/j.iswa.2023.200202>.",
    "version": "0.1.0",
    "maintainer": "Mr. Sandip Garai <sandipnicksandy@gmail.com>",
    "author": "Mr. Sandip Garai [aut, cre],\n  Dr. Ranjit Kumar Paul [aut],\n  Dr. Md Yeasin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CEEMDANML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CEEMDANML CEEMDAN Decomposition Based Hybrid Machine Learning Models Noise in the time-series data significantly affects the accuracy of the Machine Learning (ML) models (Artificial Neural Network and Support Vector Regression are considered here). Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) decomposes the time series data into sub-series and help to improve the model performance. The models can achieve higher prediction accuracy than the traditional ML models. Two models have been provided here for time series forecasting. More information may be obtained from Garai and Paul (2023) <doi:10.1016/j.iswa.2023.200202>.  "
  },
  {
    "id": 2260,
    "package_name": "CIMTx",
    "title": "Causal Inference for Multiple Treatments with a Binary Outcome",
    "description": "Different methods to conduct causal inference for multiple treatments with a binary outcome, including regression adjustment, vector matching, Bayesian additive regression trees, targeted maximum likelihood and inverse probability of treatment weighting using different generalized propensity score models such as multinomial logistic regression, generalized boosted models and super learner. For more details, see the paper by Hu et al. <doi:10.1177/0962280220921909>.",
    "version": "1.2.0",
    "maintainer": "Jiayi Ji <jjy2876@gmail.com>",
    "author": "Liangyuan Hu [aut],\n  Chenyang Gu [aut],\n  Michael Lopez [aut],\n  Jiayi Ji [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CIMTx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CIMTx Causal Inference for Multiple Treatments with a Binary Outcome Different methods to conduct causal inference for multiple treatments with a binary outcome, including regression adjustment, vector matching, Bayesian additive regression trees, targeted maximum likelihood and inverse probability of treatment weighting using different generalized propensity score models such as multinomial logistic regression, generalized boosted models and super learner. For more details, see the paper by Hu et al. <doi:10.1177/0962280220921909>.  "
  },
  {
    "id": 2264,
    "package_name": "CINmetrics",
    "title": "Calculate Chromosomal Instability Metrics",
    "description": "Implement various chromosomal instability metrics. 'CINmetrics' (Chromosomal INstability metrics) provides functions to \n    calculate various chromosomal instability metrics on masked Copy Number Variation(CNV) data at individual sample level. The \n    chromosomal instability metrics have been implemented as described in the following studies: \n    Baumbusch LO et al. 2013 <doi:10.1371/journal.pone.0054356>, \n    Davidson JM et al. 2014 <doi:10.1371/journal.pone.0079079>, \n    Chin SF et al. 2007 <doi:10.1186/gb-2007-8-10-r215>.",
    "version": "0.1.0",
    "maintainer": "Vishal H. Oza <vishoza@uab.edu>",
    "author": "Vishal H. Oza, Roshan Darji, Brittany N. Lasseigne",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CINmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CINmetrics Calculate Chromosomal Instability Metrics Implement various chromosomal instability metrics. 'CINmetrics' (Chromosomal INstability metrics) provides functions to \n    calculate various chromosomal instability metrics on masked Copy Number Variation(CNV) data at individual sample level. The \n    chromosomal instability metrics have been implemented as described in the following studies: \n    Baumbusch LO et al. 2013 <doi:10.1371/journal.pone.0054356>, \n    Davidson JM et al. 2014 <doi:10.1371/journal.pone.0079079>, \n    Chin SF et al. 2007 <doi:10.1186/gb-2007-8-10-r215>.  "
  },
  {
    "id": 2277,
    "package_name": "CLAST",
    "title": "Exact Confidence Limits after a Sequential Trial",
    "description": "The user first provides design vectors n, a and b as well as null (p0) and alternative (p1) benchmark values for the probability of success. The key function \"mv.plots.SM()\" calculates mean values of exact upper and lower limits based on four different rank ordering methods. These plots form the basis of selecting a rank ordering. The function \"inference()\" calculates exact limits from a provided realisation and ordering choice. For more information, see \"Exact confidence limits after a group sequential single arm binary trial\" by Lloyd, C.J. (2020), Statistics in Medicine, Volume 38, 2389-2399, <doi:10.1002/sim.8909>.",
    "version": "1.0.1",
    "maintainer": "Chris J. Lloyd <c.lloyd@mbs.edu>",
    "author": "Chris J. Lloyd",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CLAST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CLAST Exact Confidence Limits after a Sequential Trial The user first provides design vectors n, a and b as well as null (p0) and alternative (p1) benchmark values for the probability of success. The key function \"mv.plots.SM()\" calculates mean values of exact upper and lower limits based on four different rank ordering methods. These plots form the basis of selecting a rank ordering. The function \"inference()\" calculates exact limits from a provided realisation and ordering choice. For more information, see \"Exact confidence limits after a group sequential single arm binary trial\" by Lloyd, C.J. (2020), Statistics in Medicine, Volume 38, 2389-2399, <doi:10.1002/sim.8909>.  "
  },
  {
    "id": 2284,
    "package_name": "CLimd",
    "title": "Generating Rainfall Rasters from IMD NetCDF Data",
    "description": "The developed function is a comprehensive tool for the analysis of India Meteorological Department (IMD) NetCDF rainfall data. Specifically designed to process high-resolution daily\n             gridded rainfall datasets. It provides four key functions to process IMD NetCDF rainfall data and create rasters for various temporal scales, including annual, seasonal, monthly, and weekly\n             rainfall. For method details see, Malik, A. (2019).<DOI:10.1007/s12517-019-4454-5>. It supports different aggregation methods, such as sum, min, max, mean, and standard deviation. These functions\n             are designed for spatio-temporal analysis of rainfall patterns, trend analysis,geostatistical modeling of rainfall variability, identifying rainfall anomalies and extreme events and can be an input\n             for hydrological and agricultural models.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nirmal Kumar [aut, cph],\n  Nobin Chandra Paul [aut, cre],\n  G.P. Obi Reddy [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CLimd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CLimd Generating Rainfall Rasters from IMD NetCDF Data The developed function is a comprehensive tool for the analysis of India Meteorological Department (IMD) NetCDF rainfall data. Specifically designed to process high-resolution daily\n             gridded rainfall datasets. It provides four key functions to process IMD NetCDF rainfall data and create rasters for various temporal scales, including annual, seasonal, monthly, and weekly\n             rainfall. For method details see, Malik, A. (2019).<DOI:10.1007/s12517-019-4454-5>. It supports different aggregation methods, such as sum, min, max, mean, and standard deviation. These functions\n             are designed for spatio-temporal analysis of rainfall patterns, trend analysis,geostatistical modeling of rainfall variability, identifying rainfall anomalies and extreme events and can be an input\n             for hydrological and agricultural models.  "
  },
  {
    "id": 2318,
    "package_name": "COR",
    "title": "The COR for Optimal Subset Selection in Distributed Estimation",
    "description": "An algorithm of optimal subset selection, related to Covariance matrices, observation matrices and Response vectors (COR) to select the optimal subsets in distributed estimation. The philosophy of the package is described in Guo G. (2024) <doi:10.1007/s11222-024-10471-z>. ",
    "version": "0.2.0",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4115-6218>),\n  Haoyue Song [aut],\n  Lixing Zhu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=COR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COR The COR for Optimal Subset Selection in Distributed Estimation An algorithm of optimal subset selection, related to Covariance matrices, observation matrices and Response vectors (COR) to select the optimal subsets in distributed estimation. The philosophy of the package is described in Guo G. (2024) <doi:10.1007/s11222-024-10471-z>.   "
  },
  {
    "id": 2372,
    "package_name": "CUFF",
    "title": "Charles's Utility Function using Formula",
    "description": "Utility functions that provides wrapper to descriptive base functions\n  like cor, mean and table.  It makes use of the formula interface to pass\n  variables to functions.  It also provides operators to concatenate (%+%), to\n  repeat (%n%) and manage character vectors for nice display.",
    "version": "1.9",
    "maintainer": "Charles-\u00c9douard Gigu\u00e8re <ce.giguere@gmail.com>",
    "author": "Charles-\u00c9douard Gigu\u00e8re",
    "url": "https://github.com/giguerch/CUFF",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CUFF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CUFF Charles's Utility Function using Formula Utility functions that provides wrapper to descriptive base functions\n  like cor, mean and table.  It makes use of the formula interface to pass\n  variables to functions.  It also provides operators to concatenate (%+%), to\n  repeat (%n%) and manage character vectors for nice display.  "
  },
  {
    "id": 2386,
    "package_name": "Cairo",
    "title": "R Graphics Device using Cairo Graphics Library for Creating\nHigh-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG,\nPostScript) and Display (X11 and Win32) Output",
    "description": "R graphics device using cairographics library that can be used to create high-quality vector (PDF, PostScript and SVG) and bitmap output (PNG,JPEG,TIFF), and high-quality rendering in displays (X11 and Win32). Since it uses the same back-end for all output, copying across formats is WYSIWYG. Files are created without the dependence on X11 or other external programs. This device supports alpha channel (semi-transparent drawing) and resulting images can contain transparent and semi-transparent regions. It is ideal for use in server environments (file output) and as a replacement for other devices that don't have Cairo's capabilities such as alpha support or anti-aliasing. Backends are modular such that any subset of backends is supported.",
    "version": "1.7-0",
    "maintainer": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "author": "Simon Urbanek [aut, cre, cph] (https://urbanek.org, ORCID:\n    <https://orcid.org/0000-0003-2297-1732>),\n  Jeffrey Horner [aut]",
    "url": "http://www.rforge.net/Cairo/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Cairo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Cairo R Graphics Device using Cairo Graphics Library for Creating\nHigh-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG,\nPostScript) and Display (X11 and Win32) Output R graphics device using cairographics library that can be used to create high-quality vector (PDF, PostScript and SVG) and bitmap output (PNG,JPEG,TIFF), and high-quality rendering in displays (X11 and Win32). Since it uses the same back-end for all output, copying across formats is WYSIWYG. Files are created without the dependence on X11 or other external programs. This device supports alpha channel (semi-transparent drawing) and resulting images can contain transparent and semi-transparent regions. It is ideal for use in server environments (file output) and as a replacement for other devices that don't have Cairo's capabilities such as alpha support or anti-aliasing. Backends are modular such that any subset of backends is supported.  "
  },
  {
    "id": 2427,
    "package_name": "CensSpatial",
    "title": "Censored Spatial Models",
    "description": "It fits linear regression models for censored spatial data. It provides different estimation methods as the SAEM (Stochastic Approximation of Expectation Maximization) algorithm and seminaive that uses Kriging prediction to estimate the response at censored locations and predict new values at unknown locations. It also offers graphical tools for assessing the fitted model. More details can be found in Ordonez et al. (2018) <doi:10.1016/j.spasta.2017.12.001>.",
    "version": "3.6",
    "maintainer": "Alejandro Ordonez <ordonezjosealejandro@gmail.com>",
    "author": "Alejandro Ordonez, Christian E. Galarza, Victor H. Lachos",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CensSpatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CensSpatial Censored Spatial Models It fits linear regression models for censored spatial data. It provides different estimation methods as the SAEM (Stochastic Approximation of Expectation Maximization) algorithm and seminaive that uses Kriging prediction to estimate the response at censored locations and predict new values at unknown locations. It also offers graphical tools for assessing the fitted model. More details can be found in Ordonez et al. (2018) <doi:10.1016/j.spasta.2017.12.001>.  "
  },
  {
    "id": 2482,
    "package_name": "CleanBSequences",
    "title": "Curing of Biological Sequences",
    "description": "Curates biological sequences massively, quickly, without errors and \n without internet connection.  Biological sequences curing is performed by \n aligning the forward and / or revers primers or ends of cloning vectors with the sequences \n to be cleaned. After the alignment, new subsequences are generated without biological fragment \n not desired by the user.\n Pozzi et al (2020) <doi:10.1007/s00438-020-01671-z>.",
    "version": "2.3.0",
    "maintainer": "Florencia I. Pozzi <florenciapoz@hotmail.com>",
    "author": "Florencia I. Pozzi, Silvina A. Felitti",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CleanBSequences",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CleanBSequences Curing of Biological Sequences Curates biological sequences massively, quickly, without errors and \n without internet connection.  Biological sequences curing is performed by \n aligning the forward and / or revers primers or ends of cloning vectors with the sequences \n to be cleaned. After the alignment, new subsequences are generated without biological fragment \n not desired by the user.\n Pozzi et al (2020) <doi:10.1007/s00438-020-01671-z>.  "
  },
  {
    "id": 2521,
    "package_name": "ClusterVAR",
    "title": "Fitting Latent Class Vector-Autoregressive (VAR) Models",
    "description": "Estimates latent class vector-autoregressive models via EM algorithm on time-series data for model-based clustering and classification. Includes model selection criteria for selecting the number of lags and clusters.",
    "version": "0.0.8",
    "maintainer": "Anja Ernst <a.f.ernst@rug.nl>",
    "author": "Anja Ernst [aut, cre],\n  Jonas Haslbeck [aut]",
    "url": "",
    "bug_reports": "https://github.com/anieBee/ClusterVAR/issues",
    "repository": "https://cran.r-project.org/package=ClusterVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClusterVAR Fitting Latent Class Vector-Autoregressive (VAR) Models Estimates latent class vector-autoregressive models via EM algorithm on time-series data for model-based clustering and classification. Includes model selection criteria for selecting the number of lags and clusters.  "
  },
  {
    "id": 2557,
    "package_name": "CommonMean.Copula",
    "title": "Common Mean Vector under Copula Models",
    "description": "Estimate bivariate common mean vector under copula models with known correlation. In the current version, available copulas are the Clayton, Gumbel, Frank, Farlie-Gumbel-Morgenstern (FGM), and normal copulas. See Shih et al. (2019) <doi:10.1080/02331888.2019.1581782> and Shih et al. (2021) <under review> for details under the FGM and general copulas, respectively.",
    "version": "1.0.4",
    "maintainer": "Jia-Han Shih <tommy355097@gmail.com>",
    "author": "Jia-Han Shih",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CommonMean.Copula",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CommonMean.Copula Common Mean Vector under Copula Models Estimate bivariate common mean vector under copula models with known correlation. In the current version, available copulas are the Clayton, Gumbel, Frank, Farlie-Gumbel-Morgenstern (FGM), and normal copulas. See Shih et al. (2019) <doi:10.1080/02331888.2019.1581782> and Shih et al. (2021) <under review> for details under the FGM and general copulas, respectively.  "
  },
  {
    "id": 2579,
    "package_name": "CompositionalML",
    "title": "Machine Learning with Compositional Data",
    "description": "Machine learning algorithms for predictor variables that are compositional data and the response variable is either continuous or categorical. Specifically, the Boruta variable selection algorithm, random forest, support vector machines and projection pursuit regression are included. Relevant papers include: Tsagris M.T., Preston S. and Wood A.T.A. (2011). \"A data-based power transformation for compositional data\". Fourth International International Workshop on Compositional Data Analysis. <doi:10.48550/arXiv.1106.1451> and Alenazi, A. (2023). \"A review of compositional data analysis and recent advances\". Communications in Statistics--Theory and Methods, 52(16): 5535--5567. <doi:10.1080/03610926.2021.2014890>.",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CompositionalML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CompositionalML Machine Learning with Compositional Data Machine learning algorithms for predictor variables that are compositional data and the response variable is either continuous or categorical. Specifically, the Boruta variable selection algorithm, random forest, support vector machines and projection pursuit regression are included. Relevant papers include: Tsagris M.T., Preston S. and Wood A.T.A. (2011). \"A data-based power transformation for compositional data\". Fourth International International Workshop on Compositional Data Analysis. <doi:10.48550/arXiv.1106.1451> and Alenazi, A. (2023). \"A review of compositional data analysis and recent advances\". Communications in Statistics--Theory and Methods, 52(16): 5535--5567. <doi:10.1080/03610926.2021.2014890>.  "
  },
  {
    "id": 2609,
    "package_name": "ConsReg",
    "title": "Fits Regression & ARMA Models Subject to Constraints to the\nCoefficient",
    "description": "Fits or generalized linear models either a regression with Autoregressive moving-average (ARMA) errors for time series data. \n       The package makes it easy to incorporate constraints into the model's coefficients. \n          The model is specified by an objective function (Gaussian, Binomial or Poisson) or an ARMA order (p,q), \n          a vector of bound constraints \n          for the coefficients (i.e beta1 > 0) and the possibility to incorporate restrictions\n          among coefficients (i.e beta1 > beta2).\n          The references of this packages are the same as 'stats' package for glm() and arima() functions.\n          See Brockwell, P. J. and Davis, R. A. (1996, ISBN-10: 9783319298528).\n          For the different optimizers implemented, it is recommended to consult the documentation of the corresponding packages. ",
    "version": "0.1.0",
    "maintainer": "Josep Puig <puigjos@gmail.com>",
    "author": "Josep Puig Sall\u00e9s",
    "url": "https://github.com/puigjos/ConsReg",
    "bug_reports": "https://github.com/puigjos/ConsReg/issues",
    "repository": "https://cran.r-project.org/package=ConsReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConsReg Fits Regression & ARMA Models Subject to Constraints to the\nCoefficient Fits or generalized linear models either a regression with Autoregressive moving-average (ARMA) errors for time series data. \n       The package makes it easy to incorporate constraints into the model's coefficients. \n          The model is specified by an objective function (Gaussian, Binomial or Poisson) or an ARMA order (p,q), \n          a vector of bound constraints \n          for the coefficients (i.e beta1 > 0) and the possibility to incorporate restrictions\n          among coefficients (i.e beta1 > beta2).\n          The references of this packages are the same as 'stats' package for glm() and arima() functions.\n          See Brockwell, P. J. and Davis, R. A. (1996, ISBN-10: 9783319298528).\n          For the different optimizers implemented, it is recommended to consult the documentation of the corresponding packages.   "
  },
  {
    "id": 2633,
    "package_name": "CorMID",
    "title": "Correct Mass Isotopologue Distribution Vectors",
    "description": "In metabolic flux experiments tracer molecules (often glucose\n    containing labelled carbon) are incorporated in compounds measured\n    using mass spectrometry. The mass isotopologue distributions of these\n    compounds needs to be corrected for natural abundance of labelled\n    carbon and other effects, which are specific on the compound and\n    ionization technique applied. This package provides functions to\n    correct such effects in gas chromatography atmospheric pressure\n    chemical ionization mass spectrometry analyses.",
    "version": "0.3.1",
    "maintainer": "Jan Lisec <jan.lisec@bam.de>",
    "author": "Jan Lisec [aut, cre] (ORCID: <https://orcid.org/0000-0003-1220-2286>)",
    "url": "https://github.com/janlisec/CorMID",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CorMID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CorMID Correct Mass Isotopologue Distribution Vectors In metabolic flux experiments tracer molecules (often glucose\n    containing labelled carbon) are incorporated in compounds measured\n    using mass spectrometry. The mass isotopologue distributions of these\n    compounds needs to be corrected for natural abundance of labelled\n    carbon and other effects, which are specific on the compound and\n    ionization technique applied. This package provides functions to\n    correct such effects in gas chromatography atmospheric pressure\n    chemical ionization mass spectrometry analyses.  "
  },
  {
    "id": 2646,
    "package_name": "CovCorTest",
    "title": "Statistical Tests for Covariance and Correlation Matrices and\ntheir Structures",
    "description": "A compilation of tests for hypotheses regarding covariance\n    and correlation matrices for one or more groups. The hypothesis can\n    be specified through a corresponding hypothesis matrix and a vector or\n    by choosing one of the basic hypotheses, while for the structure test,\n    only the latter works. Thereby Monte-Carlo and Bootstrap-techniques\n    are used, and the respective method must be chosen, and the functions\n    provide p-values and mostly also estimators of calculated covariance\n    matrices of test statistics. For more details on the methodology, see \n    Sattler et al. (2022) <doi:10.1016/j.jspi.2021.12.001>,\n    Sattler and Pauly (2024) <doi:10.1007/s11749-023-00906-6>, and\n    Sattler and Dobler (2025) <doi:10.48550/arXiv.2310.11799>.",
    "version": "1.1.0",
    "maintainer": "Svenja Jedhoff <jedhoff@statistik.tu-dortmund.de>",
    "author": "Paavo Sattler [aut] (ORCID: <https://orcid.org/0000-0001-8731-0893>),\n  Svenja Jedhoff [cre, aut] (ORCID:\n    <https://orcid.org/0009-0008-2939-9103>),\n  Markus Pauly [ctb] (ORCID: <https://orcid.org/0000-0002-0976-7190>),\n  Arne C Bathke [ctb] (ORCID: <https://orcid.org/0000-0002-6260-3726>),\n  Dennis Dobler [ctb] (ORCID: <https://orcid.org/0000-0002-9040-0854>)",
    "url": "https://github.com/sjedhoff/CovCorTest",
    "bug_reports": "https://github.com/sjedhoff/CovCorTest/issues",
    "repository": "https://cran.r-project.org/package=CovCorTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CovCorTest Statistical Tests for Covariance and Correlation Matrices and\ntheir Structures A compilation of tests for hypotheses regarding covariance\n    and correlation matrices for one or more groups. The hypothesis can\n    be specified through a corresponding hypothesis matrix and a vector or\n    by choosing one of the basic hypotheses, while for the structure test,\n    only the latter works. Thereby Monte-Carlo and Bootstrap-techniques\n    are used, and the respective method must be chosen, and the functions\n    provide p-values and mostly also estimators of calculated covariance\n    matrices of test statistics. For more details on the methodology, see \n    Sattler et al. (2022) <doi:10.1016/j.jspi.2021.12.001>,\n    Sattler and Pauly (2024) <doi:10.1007/s11749-023-00906-6>, and\n    Sattler and Dobler (2025) <doi:10.48550/arXiv.2310.11799>.  "
  },
  {
    "id": 2690,
    "package_name": "D3mirt",
    "title": "Descriptive 3D Multidimensional Item Response Theory Modelling",
    "description": "For identifying, estimating, and plotting descriptive multidimensional item response theory models, restricted to 3D and dichotomous or polytomous data that fit the two-parameter logistic model or the graded response model. The method is foremost explorative and centered around the plot function that exposes item characteristics and constructs, represented by vector arrows, located in a three-dimensional interactive latent space. The results can be useful for item-level analysis as well as test development.",
    "version": "2.0.4",
    "maintainer": "Erik Forsberg <forsbergpsychometrics@gmail.com>",
    "author": "Erik Forsberg [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5228-9729>)",
    "url": "https://github.com/ForsbergPyschometrics/D3mirt",
    "bug_reports": "https://github.com/ForsbergPyschometrics/D3mirt/issues",
    "repository": "https://cran.r-project.org/package=D3mirt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "D3mirt Descriptive 3D Multidimensional Item Response Theory Modelling For identifying, estimating, and plotting descriptive multidimensional item response theory models, restricted to 3D and dichotomous or polytomous data that fit the two-parameter logistic model or the graded response model. The method is foremost explorative and centered around the plot function that exposes item characteristics and constructs, represented by vector arrows, located in a three-dimensional interactive latent space. The results can be useful for item-level analysis as well as test development.  "
  },
  {
    "id": 2750,
    "package_name": "DEPONS2R",
    "title": "Read, Plot and Analyse Output from the DEPONS Model",
    "description": "Methods for analyzing population dynamics and movement tracks simulated using the DEPONS model <https://www.depons.eu> (v.3.0), for manipulating input raster files, shipping routes and for analyzing sound propagated from ships.",
    "version": "1.2.8",
    "maintainer": "Jacob Nabe-Nielsen <jnn@ecos.au.dk>",
    "author": "Jacob Nabe-Nielsen [aut, cre],\n  Caitlin K. Frankish [aut],\n  Axelle Cordier [aut],\n  Florian G. Weller [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DEPONS2R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DEPONS2R Read, Plot and Analyse Output from the DEPONS Model Methods for analyzing population dynamics and movement tracks simulated using the DEPONS model <https://www.depons.eu> (v.3.0), for manipulating input raster files, shipping routes and for analyzing sound propagated from ships.  "
  },
  {
    "id": 2757,
    "package_name": "DEoptim",
    "title": "Global Optimization by Differential Evolution",
    "description": "Implements the Differential Evolution algorithm for global\n    optimization of a real-valued function of a real-valued parameter\n    vector as described in Mullen et al. (2011) <doi:10.18637/jss.v040.i06>.",
    "version": "2.2-8",
    "maintainer": "Katharine Mullen <mullenkate@gmail.com>",
    "author": "David Ardia [aut] (ORCID: <https://orcid.org/0000-0003-2823-782X>),\n  Katharine Mullen [aut, cre],\n  Brian Peterson [aut],\n  Joshua Ulrich [aut],\n  Kris Boudt [ctb]",
    "url": "https://github.com/ArdiaD/DEoptim",
    "bug_reports": "https://github.com/ArdiaD/DEoptim/issues",
    "repository": "https://cran.r-project.org/package=DEoptim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DEoptim Global Optimization by Differential Evolution Implements the Differential Evolution algorithm for global\n    optimization of a real-valued function of a real-valued parameter\n    vector as described in Mullen et al. (2011) <doi:10.18637/jss.v040.i06>.  "
  },
  {
    "id": 2776,
    "package_name": "DHSr",
    "title": "Create Large Scale Repeated Regression Summary Statistics\nDataset and Visualization Seamlessly",
    "description": "Mapping, spatial analysis, and statistical modeling of microdata from sources such as the Demographic and Health Surveys <https://www.dhsprogram.com/> and Integrated Public Use Microdata Series <https://www.ipums.org/>. It can also be extended to other datasets. The package supports spatial correlation index construction and visualization, along with empirical Bayes approximation of regression coefficients in a multistage setup.  The main functionality is repeated regression \u2014 for example, if we have to run regression for n groups, the group ID should be vertically composed into the variable for the parameter `location_var`. It can perform various kinds of regression, such as Generalized Regression Models, logit, probit, and more. Additionally, it can incorporate interaction effects. The key benefit of the package is its ability to store the regression results performed repeatedly on a dataset by the group ID, along with respective p-values and map those estimates. ",
    "version": "0.1.0",
    "maintainer": "Arnab Samanta <arnob.shamanta62@gmail.com>",
    "author": "Arnab Samanta [aut, cre] (<arnob.shamanta62@gmail.com>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DHSr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DHSr Create Large Scale Repeated Regression Summary Statistics\nDataset and Visualization Seamlessly Mapping, spatial analysis, and statistical modeling of microdata from sources such as the Demographic and Health Surveys <https://www.dhsprogram.com/> and Integrated Public Use Microdata Series <https://www.ipums.org/>. It can also be extended to other datasets. The package supports spatial correlation index construction and visualization, along with empirical Bayes approximation of regression coefficients in a multistage setup.  The main functionality is repeated regression \u2014 for example, if we have to run regression for n groups, the group ID should be vertically composed into the variable for the parameter `location_var`. It can perform various kinds of regression, such as Generalized Regression Models, logit, probit, and more. Additionally, it can incorporate interaction effects. The key benefit of the package is its ability to store the regression results performed repeatedly on a dataset by the group ID, along with respective p-values and map those estimates.   "
  },
  {
    "id": 2803,
    "package_name": "DJL",
    "title": "Distance Measure Based Judgment and Learning",
    "description": "Implements various decision support tools related to the Econometrics & Technometrics.\n             Subroutines include correlation reliability test, Mahalanobis distance measure for outlier detection, combinatorial search (all possible subset regression), non-parametric efficiency analysis measures: DDF (directional distance function), DEA (data envelopment analysis), HDF (hyperbolic distance function), SBM (slack-based measure), and SF (shortage function), benchmarking, Malmquist productivity analysis, risk analysis, technology adoption model, new product target setting, network DEA, dynamic DEA, intertemporal budgeting, etc.",
    "version": "3.9",
    "maintainer": "Dong-Joon Lim <tgno3.com@gmail.com>",
    "author": "Dong-Joon Lim, Ph.D. <technometrics.org>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DJL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DJL Distance Measure Based Judgment and Learning Implements various decision support tools related to the Econometrics & Technometrics.\n             Subroutines include correlation reliability test, Mahalanobis distance measure for outlier detection, combinatorial search (all possible subset regression), non-parametric efficiency analysis measures: DDF (directional distance function), DEA (data envelopment analysis), HDF (hyperbolic distance function), SBM (slack-based measure), and SF (shortage function), benchmarking, Malmquist productivity analysis, risk analysis, technology adoption model, new product target setting, network DEA, dynamic DEA, intertemporal budgeting, etc.  "
  },
  {
    "id": 2828,
    "package_name": "DOT",
    "title": "Render and Export DOT Graphs in R",
    "description": "Renders DOT diagram markup language in R and also provides the possibility to\n    export the graphs in PostScript and SVG (Scalable Vector Graphics) formats.\n    In addition, it supports literate programming packages such as 'knitr' and\n    'rmarkdown'.",
    "version": "0.1",
    "maintainer": "E. F. Haghish <haghish@imbi.uni-freiburg.de>",
    "author": "E. F. Haghish",
    "url": "http://haghish.com/dot",
    "bug_reports": "http://github.com/haghish/DOT",
    "repository": "https://cran.r-project.org/package=DOT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DOT Render and Export DOT Graphs in R Renders DOT diagram markup language in R and also provides the possibility to\n    export the graphs in PostScript and SVG (Scalable Vector Graphics) formats.\n    In addition, it supports literate programming packages such as 'knitr' and\n    'rmarkdown'.  "
  },
  {
    "id": 2833,
    "package_name": "DPP",
    "title": "Inference of Parameters of Normal Distributions from a Mixture\nof Normals",
    "description": "This MCMC method takes a data numeric vector (Y) and assigns the elements of Y\n  to a (potentially infinite) number of normal distributions. The individual normal distributions from a mixture of normals can be inferred.\n  Following the method described in Escobar (1994) <doi:10.2307/2291223> we use a Dirichlet Process Prior (DPP) to describe stochastically our prior assumptions about the dimensionality of the data.",
    "version": "0.1.2",
    "maintainer": "Luis M. Avila <lmavila@gmail.com>",
    "author": "Luis M. Avila [aut, cre],\n  Michael R. May [aut],\n  Jeff Ross-Ibarra [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DPP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DPP Inference of Parameters of Normal Distributions from a Mixture\nof Normals This MCMC method takes a data numeric vector (Y) and assigns the elements of Y\n  to a (potentially infinite) number of normal distributions. The individual normal distributions from a mixture of normals can be inferred.\n  Following the method described in Escobar (1994) <doi:10.2307/2291223> we use a Dirichlet Process Prior (DPP) to describe stochastically our prior assumptions about the dimensionality of the data.  "
  },
  {
    "id": 2877,
    "package_name": "DTRKernSmooth",
    "title": "Estimate and Make Inference About Optimal Treatment Regimes via\nSmoothed Methods",
    "description": "Methods to estimate the optimal treatment regime among all linear regimes via smoothed estimation methods, and construct element-wise confidence intervals for the optimal linear treatment regime vector, as well as the confidence interval for the optimal value via wild bootstrap procedures, if the population follows treatments recommended by the optimal linear regime. See more details in: Wu, Y. and Wang, L. (2021), \"Resampling-based Confidence Intervals for Model-free Robust Inference on Optimal Treatment Regimes\", Biometrics, 77: 465\u2013 476, <doi:10.1111/biom.13337>.",
    "version": "1.1.0",
    "maintainer": "Yunan Wu <yunan.wu@utdallas.edu>",
    "author": "Yunan Wu [aut, cre, cph],\n  Lan Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DTRKernSmooth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DTRKernSmooth Estimate and Make Inference About Optimal Treatment Regimes via\nSmoothed Methods Methods to estimate the optimal treatment regime among all linear regimes via smoothed estimation methods, and construct element-wise confidence intervals for the optimal linear treatment regime vector, as well as the confidence interval for the optimal value via wild bootstrap procedures, if the population follows treatments recommended by the optimal linear regime. See more details in: Wu, Y. and Wang, L. (2021), \"Resampling-based Confidence Intervals for Model-free Robust Inference on Optimal Treatment Regimes\", Biometrics, 77: 465\u2013 476, <doi:10.1111/biom.13337>.  "
  },
  {
    "id": 2900,
    "package_name": "DasGuptR",
    "title": "Das Gupta Standardisation and Decomposition",
    "description": "Implementation of Das Gupta's standardisation and decomposition of population rates, as set out \"Standardization and decomposition of rates: A user\u2019s manual\", Das Gupta (1993) <https://www2.census.gov/library/publications/1993/demographics/p23-186.pdf>. The goal of these methods is to calculate adjusted rates based on compositional 'factors' and quantify the contribution of each factor to the difference in crude rates between populations. The package offers functionality to handle various scenarios for any number of factors and populations, where said factors can be comprised of vectors across sub-populations (including cross-classified population breakdowns), and with the option to specify user-defined rate functions.  ",
    "version": "2.1.0",
    "maintainer": "Josiah King <josiah.king@ed.ac.uk>",
    "author": "Josiah King [aut, cre],\n  Ben Matthews [aut]",
    "url": "https://github.com/josiahpjking/DasGuptR",
    "bug_reports": "https://github.com/josiahpjking/DasGuptR/issues",
    "repository": "https://cran.r-project.org/package=DasGuptR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DasGuptR Das Gupta Standardisation and Decomposition Implementation of Das Gupta's standardisation and decomposition of population rates, as set out \"Standardization and decomposition of rates: A user\u2019s manual\", Das Gupta (1993) <https://www2.census.gov/library/publications/1993/demographics/p23-186.pdf>. The goal of these methods is to calculate adjusted rates based on compositional 'factors' and quantify the contribution of each factor to the difference in crude rates between populations. The package offers functionality to handle various scenarios for any number of factors and populations, where said factors can be comprised of vectors across sub-populations (including cross-classified population breakdowns), and with the option to specify user-defined rate functions.    "
  },
  {
    "id": 2907,
    "package_name": "DataGraph",
    "title": "Export Data from 'R' to 'DataGraph'",
    "description": "Functions to pipe data from 'R' to 'DataGraph', a graphing and analysis application for mac OS. Create a live connection using either '.dtable' or '.dtbin' files that can be read by 'DataGraph'. Can save a data frame, collection of data frames and sequences of data frames and individual vectors. For more information see <https://community.visualdatatools.com/datagraph/knowledge-base/r-package/>.",
    "version": "1.2.15",
    "maintainer": "David Adalsteinsson <david@visualdatatools.com>",
    "author": "David Adalsteinsson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DataGraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataGraph Export Data from 'R' to 'DataGraph' Functions to pipe data from 'R' to 'DataGraph', a graphing and analysis application for mac OS. Create a live connection using either '.dtable' or '.dtbin' files that can be read by 'DataGraph'. Can save a data frame, collection of data frames and sequences of data frames and individual vectors. For more information see <https://community.visualdatatools.com/datagraph/knowledge-base/r-package/>.  "
  },
  {
    "id": 2963,
    "package_name": "DiPALM",
    "title": "Differential Pattern Analysis via Linear Modeling",
    "description": "Individual gene expression patterns are encoded into a series of eigenvector patterns ('WGCNA' package). Using the framework of linear model-based differential expression comparisons ('limma' package), time-course expression patterns for genes in different conditions are compared and analyzed for significant pattern changes. For reference, see: Greenham K, Sartor RC, Zorich S, Lou P, Mockler TC and McClung CR. eLife. 2020 Sep 30;9(4). <doi:10.7554/eLife.58993>.",
    "version": "1.2",
    "maintainer": "Ryan C. Sartor <sartorry@gmail.com>",
    "author": "Ryan C. Sartor <sartorry@gmail.com>, Kathleen Greenham <greenham@umn.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiPALM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiPALM Differential Pattern Analysis via Linear Modeling Individual gene expression patterns are encoded into a series of eigenvector patterns ('WGCNA' package). Using the framework of linear model-based differential expression comparisons ('limma' package), time-course expression patterns for genes in different conditions are compared and analyzed for significant pattern changes. For reference, see: Greenham K, Sartor RC, Zorich S, Lou P, Mockler TC and McClung CR. eLife. 2020 Sep 30;9(4). <doi:10.7554/eLife.58993>.  "
  },
  {
    "id": 2971,
    "package_name": "DiceEval",
    "title": "Construction and Evaluation of Metamodels",
    "description": "Estimation, validation and prediction of models of different types : linear models, additive models, MARS,PolyMARS and Kriging.",
    "version": "1.6.1",
    "maintainer": "C. Helbert <Celine.Helbert@ec-lyon.fr>",
    "author": "D. Dupuy and C. Helbert",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiceEval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiceEval Construction and Evaluation of Metamodels Estimation, validation and prediction of models of different types : linear models, additive models, MARS,PolyMARS and Kriging.  "
  },
  {
    "id": 2972,
    "package_name": "DiceKriging",
    "title": "Kriging Methods for Computer Experiments",
    "description": "Estimation, validation and prediction of kriging models. Important functions : km, print.km, plot.km, predict.km.",
    "version": "1.6.1",
    "maintainer": "Olivier Roustant <roustant@insa-toulouse.fr>",
    "author": "Olivier Roustant [aut, cre] (ORCID:\n    <https://orcid.org/0009-0004-4709-7177>),\n  David Ginsbourger [aut] (ORCID:\n    <https://orcid.org/0000-0003-2724-2678>),\n  Yves Deville [aut] (ORCID: <https://orcid.org/0000-0002-1233-488X>),\n  Cl\u00e9ment Chevalier [ctb],\n  Yann Richet [ctb] (ORCID: <https://orcid.org/0000-0002-5677-8458>)",
    "url": "https://dicekrigingclub.github.io/www/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiceKriging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiceKriging Kriging Methods for Computer Experiments Estimation, validation and prediction of kriging models. Important functions : km, print.km, plot.km, predict.km.  "
  },
  {
    "id": 2973,
    "package_name": "DiceOptim",
    "title": "Kriging-Based Optimization for Computer Experiments",
    "description": "Efficient Global Optimization (EGO) algorithm as described in \"Roustant et al. (2012)\" <doi:10.18637/jss.v051.i01> and adaptations for problems with noise (\"Picheny and Ginsbourger, 2012\") <doi:10.1016/j.csda.2013.03.018>, parallel infill, and problems with constraints.",
    "version": "2.1.2",
    "maintainer": "Mickael Binois <mickael.binois@inria.fr>",
    "author": "Mickael Binois [cre, ctb],\n  Victor Picheny [aut],\n  David Ginsbourger [aut],\n  Olivier Roustant [aut],\n  Sebastien Marmin [ctb],\n  Tobias Wagner [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiceOptim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiceOptim Kriging-Based Optimization for Computer Experiments Efficient Global Optimization (EGO) algorithm as described in \"Roustant et al. (2012)\" <doi:10.18637/jss.v051.i01> and adaptations for problems with noise (\"Picheny and Ginsbourger, 2012\") <doi:10.1016/j.csda.2013.03.018>, parallel infill, and problems with constraints.  "
  },
  {
    "id": 2987,
    "package_name": "Directional",
    "title": "A Collection of Functions for Directional Data Analysis",
    "description": "A collection of functions for directional data (including massive data, with millions of observations) analysis. \n             Hypothesis testing, discriminant and regression analysis, MLE of distributions and more are included. \n\t\t\t The standard textbook for such data is the \"Directional Statistics\" by Mardia, K. V. and Jupp, P. E. (2000). \n\t\t\t Other references include:\n\t\t\t a) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2018). \"An elliptically symmetric angular Gaussian distribution\". Statistics and Computing 28(3): 689-697. <doi:10.1007/s11222-017-9756-4>. \n\t\t\t b) Tsagris M. and Alenazi A. (2019). \"Comparison of discriminant analysis methods on the sphere\". Communications in Statistics: Case Studies, Data Analysis and Applications 5(4):467--491. <doi:10.1080/23737484.2019.1684854>. \n\t\t\t c) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2020). \"Spherical regression models with general covariates and anisotropic errors\". Statistics and Computing 30(1): 153--165. <doi:10.1007/s11222-019-09872-2>. \n\t\t\t d) Tsagris M. and Alenazi A. (2024). \"An investigation of hypothesis testing procedures for circular and spherical mean vectors\". Communications in Statistics-Simulation and Computation, 53(3): 1387--1408. <doi:10.1080/03610918.2022.2045499>. \n\t\t\t e) Yu Z. and Huang X. (2024). A new parameterization for elliptically symmetric angular Gaussian distributions of arbitrary dimension. Electronic Journal of Statistics, 18(1): 301--334. <doi:10.1214/23-EJS2210>. \n\t\t\t f) Tsagris M. and Alzeley O. (2025). \"Circular and spherical projected Cauchy distributions: A Novel Framework for Circular and Directional Data Modeling\". Australian & New Zealand Journal of Statistics, 67(1): 77--103. <doi:10.1111/anzs.12434>. \n\t\t\t g) Tsagris M., Papastamoulis P. and Kato S. (2025). \"Directional data analysis: spherical Cauchy or Poisson kernel-based distribution\". Statistics and Computing, 35:51. <doi:10.1007/s11222-025-10583-0>.",
    "version": "7.3",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Giorgos Athineou [aut],\n  Christos Adam [aut],\n  Zehao Yu [aut],\n  Anamul Sajib [ctb],\n  Eli Amson [ctb],\n  Micah J. Waldstein [ctb],\n  Panagiotis Papastamoulis [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Directional",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Directional A Collection of Functions for Directional Data Analysis A collection of functions for directional data (including massive data, with millions of observations) analysis. \n             Hypothesis testing, discriminant and regression analysis, MLE of distributions and more are included. \n\t\t\t The standard textbook for such data is the \"Directional Statistics\" by Mardia, K. V. and Jupp, P. E. (2000). \n\t\t\t Other references include:\n\t\t\t a) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2018). \"An elliptically symmetric angular Gaussian distribution\". Statistics and Computing 28(3): 689-697. <doi:10.1007/s11222-017-9756-4>. \n\t\t\t b) Tsagris M. and Alenazi A. (2019). \"Comparison of discriminant analysis methods on the sphere\". Communications in Statistics: Case Studies, Data Analysis and Applications 5(4):467--491. <doi:10.1080/23737484.2019.1684854>. \n\t\t\t c) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2020). \"Spherical regression models with general covariates and anisotropic errors\". Statistics and Computing 30(1): 153--165. <doi:10.1007/s11222-019-09872-2>. \n\t\t\t d) Tsagris M. and Alenazi A. (2024). \"An investigation of hypothesis testing procedures for circular and spherical mean vectors\". Communications in Statistics-Simulation and Computation, 53(3): 1387--1408. <doi:10.1080/03610918.2022.2045499>. \n\t\t\t e) Yu Z. and Huang X. (2024). A new parameterization for elliptically symmetric angular Gaussian distributions of arbitrary dimension. Electronic Journal of Statistics, 18(1): 301--334. <doi:10.1214/23-EJS2210>. \n\t\t\t f) Tsagris M. and Alzeley O. (2025). \"Circular and spherical projected Cauchy distributions: A Novel Framework for Circular and Directional Data Modeling\". Australian & New Zealand Journal of Statistics, 67(1): 77--103. <doi:10.1111/anzs.12434>. \n\t\t\t g) Tsagris M., Papastamoulis P. and Kato S. (2025). \"Directional data analysis: spherical Cauchy or Poisson kernel-based distribution\". Statistics and Computing, 35:51. <doi:10.1007/s11222-025-10583-0>.  "
  },
  {
    "id": 3002,
    "package_name": "DiscreteTests",
    "title": "Vectorised Computation of P-Values and Their Supports for\nSeveral Discrete Statistical Tests",
    "description": "Provides vectorised functions for computing p-values of various\n  common discrete statistical tests, as described e.g. in Agresti (2002)\n  <doi:10.1002/0471249688>, including their distributions. Exact and\n  approximate computation methods are provided. For exact p-values, several\n  procedures of determining two-sided p-values are included, which are\n  outlined in more detail in Hirji (2006) <doi:10.1201/9781420036190>.",
    "version": "0.2.1",
    "maintainer": "Florian Junge <diso.fbmn@h-da.de>",
    "author": "Florian Junge [cre, aut] (ORCID:\n    <https://orcid.org/0009-0001-6856-6938>),\n  Christina Kihn [aut],\n  Sebastian D\u00f6hler [ctb] (ORCID: <https://orcid.org/0000-0002-0321-6355>)",
    "url": "https://github.com/DISOhda/DiscreteTests",
    "bug_reports": "https://github.com/DISOhda/DiscreteTests/issues",
    "repository": "https://cran.r-project.org/package=DiscreteTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiscreteTests Vectorised Computation of P-Values and Their Supports for\nSeveral Discrete Statistical Tests Provides vectorised functions for computing p-values of various\n  common discrete statistical tests, as described e.g. in Agresti (2002)\n  <doi:10.1002/0471249688>, including their distributions. Exact and\n  approximate computation methods are provided. For exact p-values, several\n  procedures of determining two-sided p-values are included, which are\n  outlined in more detail in Hirji (2006) <doi:10.1201/9781420036190>.  "
  },
  {
    "id": 3073,
    "package_name": "ECTSVR",
    "title": "Cointegration Based Support Vector Regression Model",
    "description": "The cointegration based support vector regression model enables researchers to use data obtained from the cointegrating vector as input in the support vector regression model.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "author": "Pankaj Das [aut, cre] (<https://orcid.org/0000-0003-1672-2502>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ECTSVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ECTSVR Cointegration Based Support Vector Regression Model The cointegration based support vector regression model enables researchers to use data obtained from the cointegrating vector as input in the support vector regression model.  "
  },
  {
    "id": 3074,
    "package_name": "ECTTDNN",
    "title": "Cointegration Based Timedelay Neural Network Model",
    "description": "This cointegration based Time Delay Neural Network Model hybrid model allows the researcher to make use of the information extracted by the cointegrating vector as an input in the neural network model.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "author": "Pankaj Das [aut, cre],\n  Achal Lama [aut],\n  Girish Kumar Jha [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ECTTDNN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ECTTDNN Cointegration Based Timedelay Neural Network Model This cointegration based Time Delay Neural Network Model hybrid model allows the researcher to make use of the information extracted by the cointegrating vector as an input in the neural network model.  "
  },
  {
    "id": 3084,
    "package_name": "EEMDSVR",
    "title": "Ensemble Empirical Mode Decomposition and Its Variant Based\nSupport Vector Regression Model",
    "description": "Application of Ensemble Empirical Mode Decomposition and its variant based Support Vector regression model for univariate time series forecasting. For method details see Das (2020).<http://krishi.icar.gov.in/jspui/handle/123456789/44138>.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "author": "Pankaj Das [aut, cre],\n  Kapil Choudhary [aut],\n  Girish Kumar Jha [aut],\n  Achal Lama [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EEMDSVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EEMDSVR Ensemble Empirical Mode Decomposition and Its Variant Based\nSupport Vector Regression Model Application of Ensemble Empirical Mode Decomposition and its variant based Support Vector regression model for univariate time series forecasting. For method details see Das (2020).<http://krishi.icar.gov.in/jspui/handle/123456789/44138>.  "
  },
  {
    "id": 3087,
    "package_name": "EESPCA",
    "title": "Eigenvectors from Eigenvalues Sparse Principal Component\nAnalysis (EESPCA)",
    "description": "Contains logic for computing sparse principal components via the EESPCA method, \n    which is based on an approximation of the eigenvector/eigenvalue identity. \n    Includes logic to support execution of the TPower and rifle sparse PCA methods,\n    as well as logic to estimate the sparsity parameters used by EESPCA, TPower and rifle\n    via cross-validation to minimize the out-of-sample reconstruction error.\n    H. Robert Frost (2021) <doi:10.1080/10618600.2021.1987254>.",
    "version": "0.8.0",
    "maintainer": "H. Robert Frost <rob.frost@dartmouth.edu>",
    "author": "H. Robert Frost [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EESPCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EESPCA Eigenvectors from Eigenvalues Sparse Principal Component\nAnalysis (EESPCA) Contains logic for computing sparse principal components via the EESPCA method, \n    which is based on an approximation of the eigenvector/eigenvalue identity. \n    Includes logic to support execution of the TPower and rifle sparse PCA methods,\n    as well as logic to estimate the sparsity parameters used by EESPCA, TPower and rifle\n    via cross-validation to minimize the out-of-sample reconstruction error.\n    H. Robert Frost (2021) <doi:10.1080/10618600.2021.1987254>.  "
  },
  {
    "id": 3122,
    "package_name": "EMDSVRhybrid",
    "title": "Empirical Mode Decomposition Based Support Vector Regression\nModel",
    "description": "Description: Application of empirical mode decomposition based support vector regression model for nonlinear and non stationary univariate time series forecasting. For method details see (i) Choudhury (2019) <http://krishi.icar.gov.in/jspui/handle/123456789/44873>; (ii) Das (2020) <http://krishi.icar.gov.in/jspui/handle/123456789/43174>; (iii) Das (2023) <http://krishi.icar.gov.in/jspui/handle/123456789/77772>.",
    "version": "0.2.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "author": "Pankaj Das [aut, cre] (ORCID: <https://orcid.org/0000-0003-1672-2502>),\n  Achal Lama [aut],\n  Girish Kumar Jha [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EMDSVRhybrid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EMDSVRhybrid Empirical Mode Decomposition Based Support Vector Regression\nModel Description: Application of empirical mode decomposition based support vector regression model for nonlinear and non stationary univariate time series forecasting. For method details see (i) Choudhury (2019) <http://krishi.icar.gov.in/jspui/handle/123456789/44873>; (ii) Das (2020) <http://krishi.icar.gov.in/jspui/handle/123456789/43174>; (iii) Das (2023) <http://krishi.icar.gov.in/jspui/handle/123456789/77772>.  "
  },
  {
    "id": 3167,
    "package_name": "ETLUtils",
    "title": "Utility Functions to Execute Standard Extract/Transform/Load\nOperations (using Package 'ff') on Large Data",
    "description": "Provides functions to facilitate the use of the 'ff' package\n    in interaction with big data in 'SQL' databases (e.g. in 'Oracle', 'MySQL',\n    'PostgreSQL', 'Hive') by allowing easy importing directly into 'ffdf' objects\n    using 'DBI', 'RODBC' and 'RJDBC'. Also contains some basic utility functions to\n    do fast left outer join merging based on 'match', factorisation of data and a\n    basic function for re-coding vectors.",
    "version": "1.6",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre]",
    "url": "https://github.com/jwijffels/ETLUtils",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ETLUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ETLUtils Utility Functions to Execute Standard Extract/Transform/Load\nOperations (using Package 'ff') on Large Data Provides functions to facilitate the use of the 'ff' package\n    in interaction with big data in 'SQL' databases (e.g. in 'Oracle', 'MySQL',\n    'PostgreSQL', 'Hive') by allowing easy importing directly into 'ffdf' objects\n    using 'DBI', 'RODBC' and 'RJDBC'. Also contains some basic utility functions to\n    do fast left outer join merging based on 'match', factorisation of data and a\n    basic function for re-coding vectors.  "
  },
  {
    "id": 3182,
    "package_name": "EZtune",
    "title": "Tunes AdaBoost, Elastic Net, Support Vector Machines, and\nGradient Boosting Machines",
    "description": "Contains two functions that are intended to make\n    tuning supervised learning methods easy. The eztune function uses a\n    genetic algorithm or Hooke-Jeeves optimizer to find the best \n    set of tuning parameters. The user can choose the optimizer, the \n    learning method, and if optimization will be based on accuracy \n    obtained through validation error, cross validation, or resubstitution. \n    The function eztune.cv will compute a cross validated error rate. The purpose \n    of eztune_cv is to provide a cross validated accuracy or MSE when \n    resubstitution or validation data are used for optimization because \n    error measures from both approaches can be misleading.",
    "version": "3.1.1",
    "maintainer": "Jill Lundell <jflundell@gmail.com>",
    "author": "Jill Lundell [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EZtune",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EZtune Tunes AdaBoost, Elastic Net, Support Vector Machines, and\nGradient Boosting Machines Contains two functions that are intended to make\n    tuning supervised learning methods easy. The eztune function uses a\n    genetic algorithm or Hooke-Jeeves optimizer to find the best \n    set of tuning parameters. The user can choose the optimizer, the \n    learning method, and if optimization will be based on accuracy \n    obtained through validation error, cross validation, or resubstitution. \n    The function eztune.cv will compute a cross validated error rate. The purpose \n    of eztune_cv is to provide a cross validated accuracy or MSE when \n    resubstitution or validation data are used for optimization because \n    error measures from both approaches can be misleading.  "
  },
  {
    "id": 3186,
    "package_name": "EbayesThresh",
    "title": "Empirical Bayes Thresholding and Related Methods",
    "description": "Empirical Bayes thresholding using the methods developed\n    by I. M. Johnstone and B. W. Silverman. The basic problem is to\n    estimate a mean vector given a vector of observations of the mean\n    vector plus white noise, taking advantage of possible sparsity in\n    the mean vector. Within a Bayesian formulation, the elements of\n    the mean vector are modelled as having, independently, a\n    distribution that is a mixture of an atom of probability at zero\n    and a suitable heavy-tailed distribution. The mixing parameter can\n    be estimated by a marginal maximum likelihood approach. This leads\n    to an adaptive thresholding approach on the original data.\n    Extensions of the basic method, in particular to wavelet\n    thresholding, are also implemented within the package.",
    "version": "1.4-12",
    "maintainer": "Peter Carbonetto <peter.carbonetto@gmail.com>",
    "author": "Bernard W. Silverman [aut],\n  Ludger Evers [aut],\n  Kan Xu [aut],\n  Peter Carbonetto [aut, cre],\n  Matthew Stephens [aut]",
    "url": "https://github.com/stephenslab/EbayesThresh",
    "bug_reports": "https://github.com/stephenslab/EbayesThresh/issues",
    "repository": "https://cran.r-project.org/package=EbayesThresh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EbayesThresh Empirical Bayes Thresholding and Related Methods Empirical Bayes thresholding using the methods developed\n    by I. M. Johnstone and B. W. Silverman. The basic problem is to\n    estimate a mean vector given a vector of observations of the mean\n    vector plus white noise, taking advantage of possible sparsity in\n    the mean vector. Within a Bayesian formulation, the elements of\n    the mean vector are modelled as having, independently, a\n    distribution that is a mixture of an atom of probability at zero\n    and a suitable heavy-tailed distribution. The mixing parameter can\n    be estimated by a marginal maximum likelihood approach. This leads\n    to an adaptive thresholding approach on the original data.\n    Extensions of the basic method, in particular to wavelet\n    thresholding, are also implemented within the package.  "
  },
  {
    "id": 3224,
    "package_name": "EnergyOnlineCPM",
    "title": "Distribution Free Multivariate Control Chart Based on Energy\nTest",
    "description": "Provides a function for distribution free control chart based on the change point model, for multivariate statistical process control. The main constituent of the chart is the energy test that focuses on the discrepancy between empirical characteristic functions of two random vectors. This new control chart highlights in three aspects. Firstly, it is distribution free, requiring no knowledge of the random processes. Secondly, this control chart can monitor mean and variance simultaneously. Thirdly it is devised for multivariate time series which is more practical in real data application. Fourthly, it is designed for online detection (Phase II), which is central for real time surveillance of stream data. For more information please refer to O. Okhrin and Y.F. Xu (2017) <https://github.com/YafeiXu/working_paper/raw/master/CPM102.pdf>.",
    "version": "1.0",
    "maintainer": "Yafei Xu <yafei.xu@hotmail.de>",
    "author": "Yafei Xu",
    "url": "https://sites.google.com/site/EnergyOnlineCPM/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EnergyOnlineCPM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EnergyOnlineCPM Distribution Free Multivariate Control Chart Based on Energy\nTest Provides a function for distribution free control chart based on the change point model, for multivariate statistical process control. The main constituent of the chart is the energy test that focuses on the discrepancy between empirical characteristic functions of two random vectors. This new control chart highlights in three aspects. Firstly, it is distribution free, requiring no knowledge of the random processes. Secondly, this control chart can monitor mean and variance simultaneously. Thirdly it is devised for multivariate time series which is more practical in real data application. Fourthly, it is designed for online detection (Phase II), which is central for real time surveillance of stream data. For more information please refer to O. Okhrin and Y.F. Xu (2017) <https://github.com/YafeiXu/working_paper/raw/master/CPM102.pdf>.  "
  },
  {
    "id": 3232,
    "package_name": "EnvNJ",
    "title": "Whole Genome Phylogenies Using Sequence Environments",
    "description": "Contains utilities for the analysis of protein sequences in a phylogenetic context. \n    Allows   the generation of phylogenetic trees base on protein sequences in an alignment-independent way. \n    Two different methods have been implemented. One approach is based on the frequency analysis of n-grams, \n    previously described in Stuart et al. (2002) <doi:10.1093/bioinformatics/18.1.100>. The other approach is based on the species-specific neighborhood preference around amino acids. Features include the conversion of a protein set into a vector \n    reflecting these neighborhood preferences, pairwise distances (dissimilarity) between these vectors, \n    and the generation of trees based on these distance matrices.",
    "version": "0.1.3",
    "maintainer": "Juan Carlos Aledo <caledo@uma.es>",
    "author": "Juan Carlos Aledo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3497-9945>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EnvNJ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EnvNJ Whole Genome Phylogenies Using Sequence Environments Contains utilities for the analysis of protein sequences in a phylogenetic context. \n    Allows   the generation of phylogenetic trees base on protein sequences in an alignment-independent way. \n    Two different methods have been implemented. One approach is based on the frequency analysis of n-grams, \n    previously described in Stuart et al. (2002) <doi:10.1093/bioinformatics/18.1.100>. The other approach is based on the species-specific neighborhood preference around amino acids. Features include the conversion of a protein set into a vector \n    reflecting these neighborhood preferences, pairwise distances (dissimilarity) between these vectors, \n    and the generation of trees based on these distance matrices.  "
  },
  {
    "id": 3248,
    "package_name": "EpiSemble",
    "title": "Ensemble Based Machine Learning Approach for Predicting\nMethylation States",
    "description": "DNA methylation (6mA) is a major epigenetic process by which alteration in gene expression took place without changing the DNA sequence. Predicting these sites in-vitro is laborious, time consuming as well as costly. This 'EpiSemble' package is an in-silico pipeline for predicting DNA sequences containing the 6mA sites. It uses an ensemble-based machine learning approach by combining Support Vector Machine (SVM), Random Forest (RF) and Gradient Boosting approach to predict the sequences with 6mA sites in it. This package has been developed by using the concept of Chen et al. (2019) <doi:10.1093/bioinformatics/btz015>.",
    "version": "0.1.1",
    "maintainer": "Dipro Sinha <diprosinha@gmail.com>",
    "author": "Dipro Sinha [aut, cre],\n  Sunil Archak [aut],\n  Dwijesh Chandra Mishra [aut],\n  Tanwy Dasmandal [aut],\n  Md Yeasin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpiSemble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpiSemble Ensemble Based Machine Learning Approach for Predicting\nMethylation States DNA methylation (6mA) is a major epigenetic process by which alteration in gene expression took place without changing the DNA sequence. Predicting these sites in-vitro is laborious, time consuming as well as costly. This 'EpiSemble' package is an in-silico pipeline for predicting DNA sequences containing the 6mA sites. It uses an ensemble-based machine learning approach by combining Support Vector Machine (SVM), Random Forest (RF) and Gradient Boosting approach to predict the sequences with 6mA sites in it. This package has been developed by using the concept of Chen et al. (2019) <doi:10.1093/bioinformatics/btz015>.  "
  },
  {
    "id": 3253,
    "package_name": "EpidigiR",
    "title": "Digital Epidemiological Analysis and Visualization Tools",
    "description": "Integrates methods for epidemiological analysis, modeling, and visualization, including functions for summary statistics, SIR (Susceptible-Infectious-Recovered) modeling, DALY (Disability-Adjusted Life Years) estimation, age standardization, diagnostic test evaluation, NLP (Natural Language Processing) keyword extraction, clinical trial power analysis, survival analysis, SNP (Single Nucleotide Polymorphism) association, and machine learning methods such as logistic regression, k-means clustering, Random Forest, and Support Vector Machine (SVM). Includes datasets for prevalence estimation, SIR modeling, genomic analysis, clinical trials, DALY, diagnostic tests, and survival analysis. Methods are based on Gelman et al. (2013) <doi:10.1201/b16018> and Wickham et al. (2019, ISBN:9781492052040>.",
    "version": "0.1.2",
    "maintainer": "Esther Atsabina Wanjala <digitalepidemiologist23@gmail.com>",
    "author": "Esther Atsabina Wanjala [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpidigiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpidigiR Digital Epidemiological Analysis and Visualization Tools Integrates methods for epidemiological analysis, modeling, and visualization, including functions for summary statistics, SIR (Susceptible-Infectious-Recovered) modeling, DALY (Disability-Adjusted Life Years) estimation, age standardization, diagnostic test evaluation, NLP (Natural Language Processing) keyword extraction, clinical trial power analysis, survival analysis, SNP (Single Nucleotide Polymorphism) association, and machine learning methods such as logistic regression, k-means clustering, Random Forest, and Support Vector Machine (SVM). Includes datasets for prevalence estimation, SIR modeling, genomic analysis, clinical trials, DALY, diagnostic tests, and survival analysis. Methods are based on Gelman et al. (2013) <doi:10.1201/b16018> and Wickham et al. (2019, ISBN:9781492052040>.  "
  },
  {
    "id": 3255,
    "package_name": "Epoch",
    "title": "IEEG (Intracranial Electroencephalography) Epoch Data Tools",
    "description": "Provides tools for working with iEEG matrix data, including downloading curated iEEG data from OSF (The Open Science Framework <https://osf.io/>) (EpochDownloader()), making new objects (Epoch()), processing (crop() and resample()), and visualizing the data (plot()). ",
    "version": "1.0.7",
    "maintainer": "Jiefei Wang <szwjf08@gmail.com>",
    "author": "Jiefei Wang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Epoch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Epoch IEEG (Intracranial Electroencephalography) Epoch Data Tools Provides tools for working with iEEG matrix data, including downloading curated iEEG data from OSF (The Open Science Framework <https://osf.io/>) (EpochDownloader()), making new objects (Epoch()), processing (crop() and resample()), and visualizing the data (plot()).   "
  },
  {
    "id": 3268,
    "package_name": "Euclimatch",
    "title": "Euclidean Climatch Algorithm",
    "description": "An interface for performing climate matching using the Euclidean \"Climatch\" algorithm. Functions provide a vector of climatch scores (0-10) for each location (i.e., grid cell) within the recipient region, the percent of climatch scores >= a threshold value, and mean climatch score. Tools for parallelization and visualizations are also provided. Note that the floor function that rounds the climatch score down to the nearest integer has been removed in this implementation and the \u201cClimatch\u201d algorithm, also referred to as the \u201cClimate\u201d algorithm, is described in: Crombie, J., Brown, L., Lizzio, J., & Hood, G. (2008). \u201cClimatch user manual\u201d. The method for the percent score is described in: Howeth, J.G., Gantz, C.A., Angermeier, P.L., Frimpong, E.A., Hoff, M.H., Keller, R.P., Mandrak, N.E., Marchetti, M.P., Olden, J.D., Romagosa, C.M., and Lodge, D.M. (2016). <doi:10.1111/ddi.12391>.",
    "version": "1.0.2",
    "maintainer": "Justin A. G. Hubbard <justin.hubbard@mail.utoronto.ca>",
    "author": "Justin A. G. Hubbard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1510-9546>),\n  D. Andrew R. Drake [aut],\n  Nicholas E. Mandrak [aut] (ORCID:\n    <https://orcid.org/0000-0001-8335-9681>)",
    "url": "",
    "bug_reports": "https://github.com/JustinHubbard/Euclimatch/issues",
    "repository": "https://cran.r-project.org/package=Euclimatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Euclimatch Euclidean Climatch Algorithm An interface for performing climate matching using the Euclidean \"Climatch\" algorithm. Functions provide a vector of climatch scores (0-10) for each location (i.e., grid cell) within the recipient region, the percent of climatch scores >= a threshold value, and mean climatch score. Tools for parallelization and visualizations are also provided. Note that the floor function that rounds the climatch score down to the nearest integer has been removed in this implementation and the \u201cClimatch\u201d algorithm, also referred to as the \u201cClimate\u201d algorithm, is described in: Crombie, J., Brown, L., Lizzio, J., & Hood, G. (2008). \u201cClimatch user manual\u201d. The method for the percent score is described in: Howeth, J.G., Gantz, C.A., Angermeier, P.L., Frimpong, E.A., Hoff, M.H., Keller, R.P., Mandrak, N.E., Marchetti, M.P., Olden, J.D., Romagosa, C.M., and Lodge, D.M. (2016). <doi:10.1111/ddi.12391>.  "
  },
  {
    "id": 3316,
    "package_name": "ExtractTrainData",
    "title": "Extract Values from Raster",
    "description": "By using a multispectral image and ESRI shapefile (Point/ Line/ Polygon), a data table will be generated for classification, regression or other processing. The data table will be contained by band wise raster values and shapefile ids (User Defined).",
    "version": "9.1.6",
    "maintainer": "Subhadip Datta <subhadipdatta007@gmail.com>",
    "author": "Suhadip Datta",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ExtractTrainData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExtractTrainData Extract Values from Raster By using a multispectral image and ESRI shapefile (Point/ Line/ Polygon), a data table will be generated for classification, regression or other processing. The data table will be contained by band wise raster values and shapefile ids (User Defined).  "
  },
  {
    "id": 3332,
    "package_name": "FAS",
    "title": "Factor-Augmented Sparse Regression Tuning-Free Testing",
    "description": "The 'FAS' package implements the bootstrap method for the tuning parameter selection and tuning-free inference on sparse regression coefficient vectors. Currently, the test could be applied to linear and factor-augmented sparse regressions, see Lederer & Vogt (2021, JMLR) <https://www.jmlr.org/papers/volume22/20-539/20-539.pdf> and Beyhum & Striaukas (2023) <arXiv:2307.13364>. ",
    "version": "1.0.0",
    "maintainer": "Jonas Striaukas <jonas.striaukas@gmail.com>",
    "author": "Jonas Striaukas [cre, aut],\n  Jad Beyhum [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FAS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAS Factor-Augmented Sparse Regression Tuning-Free Testing The 'FAS' package implements the bootstrap method for the tuning parameter selection and tuning-free inference on sparse regression coefficient vectors. Currently, the test could be applied to linear and factor-augmented sparse regressions, see Lederer & Vogt (2021, JMLR) <https://www.jmlr.org/papers/volume22/20-539/20-539.pdf> and Beyhum & Striaukas (2023) <arXiv:2307.13364>.   "
  },
  {
    "id": 3335,
    "package_name": "FAVA",
    "title": "Quantify Compositional Variability Across Relative Abundance\nVectors",
    "description": "Implements the statistic FAVA, an Fst-based Assessment of Variability across \n  vectors of relative Abundances, as well as a suite of helper functions which enable the \n  visualization and statistical analysis of relative abundance data. The 'FAVA' R package \n  accompanies the paper, \u201cQuantifying compositional variability in microbial communities\n  with FAVA\u201d by Morrison, Xue, and Rosenberg (2025) <doi:10.1073/pnas.2413211122>.",
    "version": "1.0.9",
    "maintainer": "Maike Morrison <maike.morrison@gmail.com>",
    "author": "Maike Morrison [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0430-1401>)",
    "url": "https://maikemorrison.github.io/FAVA/,\nhttps://maikemorrison.github.io/FAVA/articles/microbiome_tutorial.html",
    "bug_reports": "https://github.com/MaikeMorrison/FAVA/issues",
    "repository": "https://cran.r-project.org/package=FAVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAVA Quantify Compositional Variability Across Relative Abundance\nVectors Implements the statistic FAVA, an Fst-based Assessment of Variability across \n  vectors of relative Abundances, as well as a suite of helper functions which enable the \n  visualization and statistical analysis of relative abundance data. The 'FAVA' R package \n  accompanies the paper, \u201cQuantifying compositional variability in microbial communities\n  with FAVA\u201d by Morrison, Xue, and Rosenberg (2025) <doi:10.1073/pnas.2413211122>.  "
  },
  {
    "id": 3347,
    "package_name": "FCVAR",
    "title": "Estimation and Inference for the Fractionally Cointegrated VAR",
    "description": "Estimation and inference using the Fractionally Cointegrated \n    Vector Autoregressive (VAR) model. It includes functions for model specification, \n    including lag selection and cointegration rank selection, as well as a comprehensive\n    set of options for hypothesis testing, including tests of hypotheses on the \n    cointegrating relations, the adjustment coefficients and the fractional \n    differencing parameters. \n    An article describing the FCVAR model with examples is available on the Webpage \n    <https://sites.google.com/view/mortennielsen/software>.",
    "version": "0.1.4",
    "maintainer": "Lealand Morin <lealand.morin@ucf.edu>",
    "author": "Lealand Morin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8539-1386>),\n  Morten Nielsen [aut] (ORCID: <https://orcid.org/0000-0002-1337-9844>),\n  Michal Popiel [aut]",
    "url": "https://github.com/LeeMorinUCF/FCVAR",
    "bug_reports": "https://github.com/LeeMorinUCF/FCVAR/issues",
    "repository": "https://cran.r-project.org/package=FCVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FCVAR Estimation and Inference for the Fractionally Cointegrated VAR Estimation and inference using the Fractionally Cointegrated \n    Vector Autoregressive (VAR) model. It includes functions for model specification, \n    including lag selection and cointegration rank selection, as well as a comprehensive\n    set of options for hypothesis testing, including tests of hypotheses on the \n    cointegrating relations, the adjustment coefficients and the fractional \n    differencing parameters. \n    An article describing the FCVAR model with examples is available on the Webpage \n    <https://sites.google.com/view/mortennielsen/software>.  "
  },
  {
    "id": 3350,
    "package_name": "FDRestimation",
    "title": "Estimate, Plot, and Summarize False Discovery Rates",
    "description": "The user can directly compute and display false discovery rates from inputted p-values or z-scores under a variety of assumptions. p.fdr() computes FDRs, adjusted p-values and decision reject vectors from inputted p-values or z-values. get.pi0() estimates the proportion of data that are truly null. plot.p.fdr() plots the FDRs, adjusted p-values, and the raw p-values points against their rejection threshold lines.  ",
    "version": "1.0.1",
    "maintainer": "Megan Murray <megan.c.hollister@vanderbilt.edu>",
    "author": "Megan Murray [aut, cre],\n  Jeffrey Blume [aut]",
    "url": "<doi:10.12688/f1000research.52999.2>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FDRestimation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FDRestimation Estimate, Plot, and Summarize False Discovery Rates The user can directly compute and display false discovery rates from inputted p-values or z-scores under a variety of assumptions. p.fdr() computes FDRs, adjusted p-values and decision reject vectors from inputted p-values or z-values. get.pi0() estimates the proportion of data that are truly null. plot.p.fdr() plots the FDRs, adjusted p-values, and the raw p-values points against their rejection threshold lines.    "
  },
  {
    "id": 3366,
    "package_name": "FHtest",
    "title": "Tests for Right and Interval-Censored Survival Data Based on the\nFleming-Harrington Class",
    "description": "Functions to compare two or more survival curves with:\n             a) The Fleming-Harrington test for right-censored data based on permutations and on counting processes.\n             b) An extension of the Fleming-Harrington test for interval-censored data based on a permutation distribution and on a score vector distribution.",
    "version": "1.5.1",
    "maintainer": "Ramon Oller <ramon.oller@uvic.cat>",
    "author": "Ramon Oller, Klaus Langohr",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FHtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FHtest Tests for Right and Interval-Censored Survival Data Based on the\nFleming-Harrington Class Functions to compare two or more survival curves with:\n             a) The Fleming-Harrington test for right-censored data based on permutations and on counting processes.\n             b) An extension of the Fleming-Harrington test for interval-censored data based on a permutation distribution and on a score vector distribution.  "
  },
  {
    "id": 3372,
    "package_name": "FITSio",
    "title": "FITS (Flexible Image Transport System) Utilities",
    "description": "Utilities to read and write files in the FITS (Flexible\n  Image Transport System) format, a standard format in astronomy (see\n  e.g. <https://en.wikipedia.org/wiki/FITS> for more information).\n  Present low-level routines allow: reading, parsing, and modifying\n  FITS headers; reading FITS images (multi-dimensional arrays);\n  reading FITS binary and ASCII tables; and writing FITS images\n  (multi-dimensional arrays).  Higher-level functions allow: reading\n  files composed of one or more headers and a single (perhaps\n  multidimensional) image or single table; reading tables into\n  data frames; generating vectors for image array axes; scaling and\n  writing images as 16-bit integers.  Known incompletenesses are\n  reading random group extensions, as well as\n  complex and array descriptor data types in binary tables.",
    "version": "2.1-6",
    "maintainer": "Andrew Harris <harris@astro.umd.edu>",
    "author": "Andrew Harris",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FITSio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FITSio FITS (Flexible Image Transport System) Utilities Utilities to read and write files in the FITS (Flexible\n  Image Transport System) format, a standard format in astronomy (see\n  e.g. <https://en.wikipedia.org/wiki/FITS> for more information).\n  Present low-level routines allow: reading, parsing, and modifying\n  FITS headers; reading FITS images (multi-dimensional arrays);\n  reading FITS binary and ASCII tables; and writing FITS images\n  (multi-dimensional arrays).  Higher-level functions allow: reading\n  files composed of one or more headers and a single (perhaps\n  multidimensional) image or single table; reading tables into\n  data frames; generating vectors for image array axes; scaling and\n  writing images as 16-bit integers.  Known incompletenesses are\n  reading random group extensions, as well as\n  complex and array descriptor data types in binary tables.  "
  },
  {
    "id": 3373,
    "package_name": "FKF",
    "title": "Fast Kalman Filter",
    "description": "This is a fast and flexible implementation of the Kalman\n        filter and smoother, which can deal with NAs. It is entirely written in C and relies fully on linear algebra subroutines contained in\n        BLAS and LAPACK. Due to the speed of the filter, the fitting of\n        high-dimensional linear state space models to large datasets\n        becomes possible. This package also contains a plot function\n        for the visualization of the state vector and graphical\n        diagnostics of the residuals.",
    "version": "0.2.6",
    "maintainer": "Paul Smith <paul@waternumbers.co.uk>",
    "author": "David Luethi [aut],\n  Philipp Erb [aut],\n  Simon Otziger [aut],\n  Daniel McDonald [aut],\n  Paul Smith [aut, cre] (ORCID: <https://orcid.org/0000-0002-0034-3412>)",
    "url": "https://waternumbers.github.io/FKF/,\nhttps://github.com/waternumbers/FKF",
    "bug_reports": "https://github.com/waternumbers/FKF/issues",
    "repository": "https://cran.r-project.org/package=FKF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FKF Fast Kalman Filter This is a fast and flexible implementation of the Kalman\n        filter and smoother, which can deal with NAs. It is entirely written in C and relies fully on linear algebra subroutines contained in\n        BLAS and LAPACK. Due to the speed of the filter, the fitting of\n        high-dimensional linear state space models to large datasets\n        becomes possible. This package also contains a plot function\n        for the visualization of the state vector and graphical\n        diagnostics of the residuals.  "
  },
  {
    "id": 3383,
    "package_name": "FLSSS",
    "title": "Mining Rigs for Problems in the Subset Sum Family",
    "description": "Specialized solvers for combinatorial optimization problems in the Subset Sum family. The solvers differ from the mainstream in the options of (i) restricting subset size, (ii) bounding subset elements, (iii) mining real-value multisets with predefined subset sum errors, (iv) finding one or more subsets in limited time. A novel algorithm for mining the one-dimensional Subset Sum induced algorithms for the multi-Subset Sum and the multidimensional Subset Sum. The multi-threaded framework for the latter offers exact algorithms to the multidimensional Knapsack and the Generalized Assignment problems. Historical updates include (a) renewed implementation of the multi-Subset Sum, multidimensional Knapsack and Generalized Assignment solvers; (b) availability of bounding solution space in the multidimensional Subset Sum; (c) fundamental data structure and architectural changes for enhanced cache locality and better chance of SIMD vectorization; (d) option of mapping floating-point instance to compressed 64-bit integer instance with user-controlled precision loss, which could yield substantial speedup due to the dimension reduction and efficient compressed integer arithmetic via bit-manipulations; (e) distributed computing infrastructure for multidimensional subset sum; (f) arbitrary-precision zero-margin-of-error multidimensional Subset Sum accelerated by a simplified Bloom filter. The package contains a copy of 'xxHash' from <https://github.com/Cyan4973/xxHash>. Package vignette (<doi:10.48550/arXiv.1612.04484>) detailed a few historical updates. Functions prefixed with 'aux' (auxiliary) are independent implementations of published algorithms for solving optimization problems less relevant to Subset Sum.",
    "version": "9.2.8",
    "maintainer": "Charlie Wusuo Liu <liuwusuo@gmail.com>",
    "author": "Charlie Wusuo Liu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FLSSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FLSSS Mining Rigs for Problems in the Subset Sum Family Specialized solvers for combinatorial optimization problems in the Subset Sum family. The solvers differ from the mainstream in the options of (i) restricting subset size, (ii) bounding subset elements, (iii) mining real-value multisets with predefined subset sum errors, (iv) finding one or more subsets in limited time. A novel algorithm for mining the one-dimensional Subset Sum induced algorithms for the multi-Subset Sum and the multidimensional Subset Sum. The multi-threaded framework for the latter offers exact algorithms to the multidimensional Knapsack and the Generalized Assignment problems. Historical updates include (a) renewed implementation of the multi-Subset Sum, multidimensional Knapsack and Generalized Assignment solvers; (b) availability of bounding solution space in the multidimensional Subset Sum; (c) fundamental data structure and architectural changes for enhanced cache locality and better chance of SIMD vectorization; (d) option of mapping floating-point instance to compressed 64-bit integer instance with user-controlled precision loss, which could yield substantial speedup due to the dimension reduction and efficient compressed integer arithmetic via bit-manipulations; (e) distributed computing infrastructure for multidimensional subset sum; (f) arbitrary-precision zero-margin-of-error multidimensional Subset Sum accelerated by a simplified Bloom filter. The package contains a copy of 'xxHash' from <https://github.com/Cyan4973/xxHash>. Package vignette (<doi:10.48550/arXiv.1612.04484>) detailed a few historical updates. Functions prefixed with 'aux' (auxiliary) are independent implementations of published algorithms for solving optimization problems less relevant to Subset Sum.  "
  },
  {
    "id": 3410,
    "package_name": "FRK",
    "title": "Fixed Rank Kriging",
    "description": "A tool for spatial/spatio-temporal modelling and prediction with large datasets. The approach models the field, and hence the covariance function, using a set of basis functions. This fixed-rank basis-function representation facilitates the modelling of big data, and the method naturally allows for non-stationary, anisotropic covariance functions. Discretisation of the spatial domain into so-called basic areal units (BAUs) facilitates the use of observations with varying support (i.e., both point-referenced and areal supports, potentially simultaneously), and prediction over arbitrary user-specified regions. `FRK` also supports inference over various manifolds, including the 2D plane and 3D sphere, and it provides helper functions to model, fit, predict, and plot with relative ease. Version 2.0.0 and above also supports the modelling of non-Gaussian data (e.g., Poisson, binomial, negative-binomial, gamma, and inverse-Gaussian) by employing a generalised linear mixed model (GLMM) framework. Zammit-Mangion and Cressie <doi:10.18637/jss.v098.i04> describe `FRK` in a Gaussian setting, and detail its use of basis functions and BAUs, while Sainsbury-Dale, Zammit-Mangion, and Cressie <doi:10.18637/jss.v108.i10> describe `FRK` in a non-Gaussian setting; two vignettes are available that summarise these papers and provide additional examples.",
    "version": "2.3.1",
    "maintainer": "Andrew Zammit-Mangion <andrewzm@gmail.com>",
    "author": "Andrew Zammit-Mangion [aut, cre],\n  Matthew Sainsbury-Dale [aut]",
    "url": "https://andrewzm.github.io/FRK/, https://github.com/andrewzm/FRK/",
    "bug_reports": "https://github.com/andrewzm/FRK/issues/",
    "repository": "https://cran.r-project.org/package=FRK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FRK Fixed Rank Kriging A tool for spatial/spatio-temporal modelling and prediction with large datasets. The approach models the field, and hence the covariance function, using a set of basis functions. This fixed-rank basis-function representation facilitates the modelling of big data, and the method naturally allows for non-stationary, anisotropic covariance functions. Discretisation of the spatial domain into so-called basic areal units (BAUs) facilitates the use of observations with varying support (i.e., both point-referenced and areal supports, potentially simultaneously), and prediction over arbitrary user-specified regions. `FRK` also supports inference over various manifolds, including the 2D plane and 3D sphere, and it provides helper functions to model, fit, predict, and plot with relative ease. Version 2.0.0 and above also supports the modelling of non-Gaussian data (e.g., Poisson, binomial, negative-binomial, gamma, and inverse-Gaussian) by employing a generalised linear mixed model (GLMM) framework. Zammit-Mangion and Cressie <doi:10.18637/jss.v098.i04> describe `FRK` in a Gaussian setting, and detail its use of basis functions and BAUs, while Sainsbury-Dale, Zammit-Mangion, and Cressie <doi:10.18637/jss.v108.i10> describe `FRK` in a non-Gaussian setting; two vignettes are available that summarise these papers and provide additional examples.  "
  },
  {
    "id": 3424,
    "package_name": "FVDDPpkg",
    "title": "Implement Fleming-Viot-Dependent Dirichlet Processes",
    "description": "\n      A Bayesian Nonparametric model for the study of time-evolving frequencies, which has become renowned in the study of population genetics.\n      The model consists of a Hidden Markov Model (HMM) in which the latent signal is a distribution-valued stochastic process that takes the form of a finite mixture of Dirichlet Processes, indexed by vectors that count how many times each value is observed in the population.\n      The package implements methodologies presented in Ascolani, Lijoi and Ruggiero (2021) <doi:10.1214/20-BA1206> and Ascolani, Lijoi and Ruggiero (2023) <doi:10.3150/22-BEJ1504> that make it possible to study the process at the time of data collection or to predict its evolution in future or in the past.",
    "version": "0.1.2",
    "maintainer": "Stefano Damato <stefano.damato@idsia.ch>",
    "author": "Stefano Damato [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FVDDPpkg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FVDDPpkg Implement Fleming-Viot-Dependent Dirichlet Processes \n      A Bayesian Nonparametric model for the study of time-evolving frequencies, which has become renowned in the study of population genetics.\n      The model consists of a Hidden Markov Model (HMM) in which the latent signal is a distribution-valued stochastic process that takes the form of a finite mixture of Dirichlet Processes, indexed by vectors that count how many times each value is observed in the population.\n      The package implements methodologies presented in Ascolani, Lijoi and Ruggiero (2021) <doi:10.1214/20-BA1206> and Ascolani, Lijoi and Ruggiero (2023) <doi:10.3150/22-BEJ1504> that make it possible to study the process at the time of data collection or to predict its evolution in future or in the past.  "
  },
  {
    "id": 3426,
    "package_name": "FWRGB",
    "title": "Fresh Weight Determination from Visual Image of the Plant",
    "description": "Fresh biomass determination is the key to evaluating crop genotypes' response to diverse input and stress conditions and forms the basis for calculating net primary production. However, as conventional phenotyping approaches for measuring fresh biomass is time-consuming, laborious and destructive, image-based phenotyping methods are being widely used now. In the image-based approach, the fresh weight of the above-ground part of the plant depends on the projected area. For determining the projected area, the visual image of the plant is converted into the grayscale image by simply averaging the Red(R), Green (G) and Blue (B) pixel values. Grayscale image is then converted into a binary image using Otsu\u2019s thresholding method Otsu, N. (1979) <doi:10.1109/TSMC.1979.4310076> to separate plant area from the background (image segmentation). The segmentation process was accomplished by selecting the pixels with values over the threshold value belonging to the plant region and other pixels to the background region. The resulting binary image consists of white and black pixels representing the plant and background regions. Finally, the number of pixels inside the plant region was counted and converted to square centimetres (cm2) using the reference object (any object whose actual area is known previously) to get the projected area. After that, the projected area is used as input to the machine learning model (Linear Model, Artificial Neural Network, and Support Vector Regression) to determine the plant's fresh weight.",
    "version": "0.1.0",
    "maintainer": "Tanuj Misra <tanujmisra102@gmail.com>",
    "author": "Tanuj Misra [aut, cre],\n  Alka Arora [aut],\n  Sudeep Marwaha [aut],\n  Shailendra Kumar [aut],\n  Mrinmoy Ray [aut],\n  Sudhir Kumar [aut],\n  Sayanti Guha Majumder [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FWRGB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FWRGB Fresh Weight Determination from Visual Image of the Plant Fresh biomass determination is the key to evaluating crop genotypes' response to diverse input and stress conditions and forms the basis for calculating net primary production. However, as conventional phenotyping approaches for measuring fresh biomass is time-consuming, laborious and destructive, image-based phenotyping methods are being widely used now. In the image-based approach, the fresh weight of the above-ground part of the plant depends on the projected area. For determining the projected area, the visual image of the plant is converted into the grayscale image by simply averaging the Red(R), Green (G) and Blue (B) pixel values. Grayscale image is then converted into a binary image using Otsu\u2019s thresholding method Otsu, N. (1979) <doi:10.1109/TSMC.1979.4310076> to separate plant area from the background (image segmentation). The segmentation process was accomplished by selecting the pixels with values over the threshold value belonging to the plant region and other pixels to the background region. The resulting binary image consists of white and black pixels representing the plant and background regions. Finally, the number of pixels inside the plant region was counted and converted to square centimetres (cm2) using the reference object (any object whose actual area is known previously) to get the projected area. After that, the projected area is used as input to the machine learning model (Linear Model, Artificial Neural Network, and Support Vector Regression) to determine the plant's fresh weight.  "
  },
  {
    "id": 3446,
    "package_name": "FastGP",
    "title": "Efficiently Using Gaussian Processes with Rcpp and RcppEigen",
    "description": "Contains Rcpp and RcppEigen implementations of matrix operations useful for Gaussian process models, such as the inversion of a symmetric Toeplitz matrix, sampling from multivariate normal distributions, evaluation of the log-density of a multivariate normal vector, and Bayesian inference for latent variable Gaussian process models with elliptical slice sampling (Murray, Adams, and MacKay 2010).",
    "version": "1.2",
    "maintainer": "Giri Gopalan <gopalan88@gmail.com>",
    "author": "Giri Gopalan, Luke Bornn",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FastGP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FastGP Efficiently Using Gaussian Processes with Rcpp and RcppEigen Contains Rcpp and RcppEigen implementations of matrix operations useful for Gaussian process models, such as the inversion of a symmetric Toeplitz matrix, sampling from multivariate normal distributions, evaluation of the log-density of a multivariate normal vector, and Bayesian inference for latent variable Gaussian process models with elliptical slice sampling (Murray, Adams, and MacKay 2010).  "
  },
  {
    "id": 3448,
    "package_name": "FastHamming",
    "title": "Fast Computation of Pairwise Hamming Distances",
    "description": "Pairwise Hamming distances are computed between the rows of a binary (0/1) matrix using highly optimized 'C' code. The input is an integer matrix where each row represents a binary feature vector and returns a symmetric integer matrix of pairwise distances. Internally, rows are bit-packed into 64-bit words for fast XOR-based comparisons, with hardware-accelerated popcount operations to count differences. 'OpenMP' parallelization ensures efficient performance for large matrices. ",
    "version": "1.2",
    "maintainer": "Ravi Varadhan <ravi.varadhan@jhu.edu>",
    "author": "Ravi Varadhan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FastHamming",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FastHamming Fast Computation of Pairwise Hamming Distances Pairwise Hamming distances are computed between the rows of a binary (0/1) matrix using highly optimized 'C' code. The input is an integer matrix where each row represents a binary feature vector and returns a symmetric integer matrix of pairwise distances. Internally, rows are bit-packed into 64-bit words for fast XOR-based comparisons, with hardware-accelerated popcount operations to count differences. 'OpenMP' parallelization ensures efficient performance for large matrices.   "
  },
  {
    "id": 3458,
    "package_name": "FastUtils",
    "title": "Fast, Readable Utility Functions",
    "description": "A wide variety of tools for general data analysis, wrangling, spelling, statistics, visualizations, package development, and more. All functions have vectorized implementations whenever possible. Exported names are designed to be readable, with longer names possessing short aliases.",
    "version": "0.2.1",
    "maintainer": "Qile Yang <qile.yang@berkeley.edu>",
    "author": "Qile Yang [cre, aut, cph]",
    "url": "https://github.com/Qile0317/FastUtils,\nhttps://qile0317.github.io/FastUtils/",
    "bug_reports": "https://github.com/Qile0317/FastUtils/issues/",
    "repository": "https://cran.r-project.org/package=FastUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FastUtils Fast, Readable Utility Functions A wide variety of tools for general data analysis, wrangling, spelling, statistics, visualizations, package development, and more. All functions have vectorized implementations whenever possible. Exported names are designed to be readable, with longer names possessing short aliases.  "
  },
  {
    "id": 3481,
    "package_name": "FindIt",
    "title": "Finding Heterogeneous Treatment Effects",
    "description": "The heterogeneous treatment effect estimation procedure \n        proposed by Imai and Ratkovic (2013)<DOI: 10.1214/12-AOAS593>.  \n        The proposed method is applicable, for\n        example, when selecting a small number of most (or least)\n        efficacious treatments from a large number of alternative\n        treatments as well as when identifying subsets of the\n        population who benefit (or are harmed by) a treatment of\n        interest. The method adapts the Support Vector Machine\n        classifier by placing separate LASSO constraints over the\n        pre-treatment parameters and causal heterogeneity parameters of\n        interest. This allows for the qualitative distinction between\n        causal and other parameters, thereby making the variable\n        selection suitable for the exploration of causal heterogeneity. \t\n        The package also contains a class of functions, CausalANOVA, \n        which estimates the average marginal interaction effects (AMIEs)\n        by a regularized ANOVA as proposed by Egami and Imai (2019). \n        It contains a variety of regularization techniques to facilitate \n\tanalysis of large factorial experiments. ",
    "version": "1.3.0",
    "maintainer": "Naoki Egami <naoki.egami5@gmail.com>",
    "author": "Naoki Egami [aut, cre],\n  Marc Ratkovic [aut],\n  Kosuke Imai [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FindIt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FindIt Finding Heterogeneous Treatment Effects The heterogeneous treatment effect estimation procedure \n        proposed by Imai and Ratkovic (2013)<DOI: 10.1214/12-AOAS593>.  \n        The proposed method is applicable, for\n        example, when selecting a small number of most (or least)\n        efficacious treatments from a large number of alternative\n        treatments as well as when identifying subsets of the\n        population who benefit (or are harmed by) a treatment of\n        interest. The method adapts the Support Vector Machine\n        classifier by placing separate LASSO constraints over the\n        pre-treatment parameters and causal heterogeneity parameters of\n        interest. This allows for the qualitative distinction between\n        causal and other parameters, thereby making the variable\n        selection suitable for the exploration of causal heterogeneity. \t\n        The package also contains a class of functions, CausalANOVA, \n        which estimates the average marginal interaction effects (AMIEs)\n        by a regularized ANOVA as proposed by Egami and Imai (2019). \n        It contains a variety of regularization techniques to facilitate \n\tanalysis of large factorial experiments.   "
  },
  {
    "id": 3491,
    "package_name": "FixedPoint",
    "title": "Algorithms for Finding Fixed Point Vectors of Functions",
    "description": "For functions that take and return vectors (or scalars), this package provides 8 algorithms for finding fixed point vectors (vectors for which the inputs and outputs to the function are the same vector). These algorithms include Anderson (1965) acceleration <doi:10.1145/321296.321305>, epsilon extrapolation methods (Wynn 1962 <doi:10.2307/2004051>) and minimal polynomial methods (Cabay and Jackson 1976 <doi:10.1137/0713060>).",
    "version": "0.6.3",
    "maintainer": "Stuart Baumann <Stuart@StuartBaumann.com>",
    "author": "Stuart Baumann & Margaryta Klymak",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FixedPoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FixedPoint Algorithms for Finding Fixed Point Vectors of Functions For functions that take and return vectors (or scalars), this package provides 8 algorithms for finding fixed point vectors (vectors for which the inputs and outputs to the function are the same vector). These algorithms include Anderson (1965) acceleration <doi:10.1145/321296.321305>, epsilon extrapolation methods (Wynn 1962 <doi:10.2307/2004051>) and minimal polynomial methods (Cabay and Jackson 1976 <doi:10.1137/0713060>).  "
  },
  {
    "id": 3493,
    "package_name": "FlexDir",
    "title": "Tools to Work with the Flexible Dirichlet Distribution",
    "description": "Provides tools to work with the Flexible Dirichlet\n    distribution. The main features are an E-M algorithm for computing the maximum\n    likelihood estimate of the parameter vector and a function based on conditional\n    bootstrap to estimate its asymptotic variance-covariance matrix. It contains\n    also functions to plot graphs, to generate random observations and to handle\n    compositional data.",
    "version": "1.0",
    "maintainer": "Agnese Maria Di Brisco <agnese.dibrisco@unimib.it>",
    "author": "Sonia Migliorati [aut],\n  Agnese Maria Di Brisco [aut, cre],\n  Matteo Vestrucci [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FlexDir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FlexDir Tools to Work with the Flexible Dirichlet Distribution Provides tools to work with the Flexible Dirichlet\n    distribution. The main features are an E-M algorithm for computing the maximum\n    likelihood estimate of the parameter vector and a function based on conditional\n    bootstrap to estimate its asymptotic variance-covariance matrix. It contains\n    also functions to plot graphs, to generate random observations and to handle\n    compositional data.  "
  },
  {
    "id": 3534,
    "package_name": "FracKrigingR",
    "title": "Spatial Multivariate Data Modeling",
    "description": "Aim is to provide fractional Brownian vector field generation algorithm, Hurst parameter estimation method and fractional kriging model for multivariate data modeling.",
    "version": "1.0.0",
    "maintainer": "Neringa Urbonaite <neringa.urbonaite@mif.vu.lt>",
    "author": "Neringa Urbonaite [aut, cre],\n  Leonidas Sakalauskas [aut]",
    "url": "https://github.com/NidaGreen/FracKriging",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FracKrigingR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FracKrigingR Spatial Multivariate Data Modeling Aim is to provide fractional Brownian vector field generation algorithm, Hurst parameter estimation method and fractional kriging model for multivariate data modeling.  "
  },
  {
    "id": 3540,
    "package_name": "Fstability",
    "title": "Calculate Feature Stability",
    "description": "Has two functions to help with calculating feature selection stability. 'Lump' is a function that groups subset vectors into a dataframe, and adds NA to shorter vectors so they all have the same length.\n    'ASM' is a function that takes a dataframe of subset vectors and the original vector of features as inputs, and calculates the Stability of the feature selection.\n    The calculation for 'asm' uses the Adjusted Stability Measure proposed in: 'Lustgarten', 'Gopalakrishnan', & 'Visweswaran' (2009)<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815476/>.",
    "version": "0.1.2",
    "maintainer": "Nicolas Ewen <nicolas.ewen.math@gmail.com>",
    "author": "Nicolas Ewen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Fstability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Fstability Calculate Feature Stability Has two functions to help with calculating feature selection stability. 'Lump' is a function that groups subset vectors into a dataframe, and adds NA to shorter vectors so they all have the same length.\n    'ASM' is a function that takes a dataframe of subset vectors and the original vector of features as inputs, and calculates the Stability of the feature selection.\n    The calculation for 'asm' uses the Adjusted Stability Measure proposed in: 'Lustgarten', 'Gopalakrishnan', & 'Visweswaran' (2009)<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815476/>.  "
  },
  {
    "id": 3605,
    "package_name": "GDILM.ME",
    "title": "Spatial Modeling of Infectious Diseases with Co-Variate Error",
    "description": "Provides tools for simulating from spatial modeling of individual level  of infectious disease transmission when co-variates measured with error, and carrying out infectious disease data analyses with the same models. The epidemic models considered are distance-based model within Susceptible-Infectious-Removed (SIR) compartmental frameworks.",
    "version": "1.2.1",
    "maintainer": "Ruwani Herath <ruwanirasanjalih@gmail.com>",
    "author": "Ruwani Herath [aut, cre],\n  Leila Amiri [ctb],\n  Mahmoud Torabi [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GDILM.ME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GDILM.ME Spatial Modeling of Infectious Diseases with Co-Variate Error Provides tools for simulating from spatial modeling of individual level  of infectious disease transmission when co-variates measured with error, and carrying out infectious disease data analyses with the same models. The epidemic models considered are distance-based model within Susceptible-Infectious-Removed (SIR) compartmental frameworks.  "
  },
  {
    "id": 3606,
    "package_name": "GDILM.SEIRS",
    "title": "Spatial Modeling of Infectious Disease with Reinfection",
    "description": "Geographically Dependent Individual Level Models (GDILMs) within the Susceptible-Exposed-Infectious-Recovered-Susceptible (SEIRS) framework are applied to model infectious disease transmission, incorporating reinfection dynamics. This package employs a likelihood based Monte Carlo Expectation Conditional Maximization (MCECM) algorithm for estimating model parameters. It also provides tools for GDILM fitting, parameter estimation, AIC calculation on real pandemic data, and simulation studies customized to user-defined model settings.",
    "version": "0.0.5",
    "maintainer": "Amin Abed <abeda@myumanitoba.ca>",
    "author": "Amin Abed [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7381-4721>),\n  Mahmoud Torabi [ths],\n  Zeinab Mashreghi [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GDILM.SEIRS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GDILM.SEIRS Spatial Modeling of Infectious Disease with Reinfection Geographically Dependent Individual Level Models (GDILMs) within the Susceptible-Exposed-Infectious-Recovered-Susceptible (SEIRS) framework are applied to model infectious disease transmission, incorporating reinfection dynamics. This package employs a likelihood based Monte Carlo Expectation Conditional Maximization (MCECM) algorithm for estimating model parameters. It also provides tools for GDILM fitting, parameter estimation, AIC calculation on real pandemic data, and simulation studies customized to user-defined model settings.  "
  },
  {
    "id": 3607,
    "package_name": "GDILM.SIR",
    "title": "Inference for Infectious Disease Transmission in SIR Framework",
    "description": "Model and estimate the model parameters \n            for the spatial model of individual-level infectious disease \n            transmission in Susceptible-Infected-Recovered (SIR) framework.",
    "version": "1.2.1",
    "maintainer": "Ruwani Herath <ruwanirasanjalih@gmail.com>",
    "author": "Ruwani Herath [aut, cre],\n  Leila Amiri [ctb],\n  Mahmoud Torabi [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GDILM.SIR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GDILM.SIR Inference for Infectious Disease Transmission in SIR Framework Model and estimate the model parameters \n            for the spatial model of individual-level infectious disease \n            transmission in Susceptible-Infected-Recovered (SIR) framework.  "
  },
  {
    "id": 3625,
    "package_name": "GET",
    "title": "Global Envelopes",
    "description": "Implementation of global envelopes for a set of general d-dimensional vectors T\n    in various applications. A 100(1-alpha)% global envelope is a band bounded by two\n    vectors such that the probability that T falls outside this envelope in any of the d\n    points is equal to alpha. Global means that the probability is controlled simultaneously\n    for all the d elements of the vectors. The global envelopes can be used for graphical\n    Monte Carlo and permutation tests where the test statistic is a multivariate vector or\n    function (e.g. goodness-of-fit testing for point patterns and random sets, functional\n    analysis of variance, functional general linear model, n-sample test of correspondence\n    of distribution functions), for central regions of functional or multivariate data (e.g.\n    outlier detection, functional boxplot) and for global confidence and prediction bands\n    (e.g. confidence band in polynomial regression, Bayesian posterior prediction). See\n    Myllym\u00e4ki and Mrkvi\u010dka (2024) <doi:10.18637/jss.v111.i03>,\n    Myllym\u00e4ki et al. (2017) <doi:10.1111/rssb.12172>,\n    Mrkvi\u010dka and Myllym\u00e4ki (2023) <doi:10.1007/s11222-023-10275-7>,\n    Mrkvi\u010dka et al. (2016) <doi:10.1016/j.spasta.2016.04.005>,\n    Mrkvi\u010dka et al. (2017) <doi:10.1007/s11222-016-9683-9>,\n    Mrkvi\u010dka et al. (2020) <doi:10.14736/kyb-2020-3-0432>,\n    Mrkvi\u010dka et al. (2021) <doi:10.1007/s11009-019-09756-y>,\n    Myllym\u00e4ki et al. (2021) <doi:10.1016/j.spasta.2020.100436>,\n    Mrkvi\u010dka et al. (2022) <doi:10.1002/sim.9236>,\n    Dai et al. (2022) <doi:10.5772/intechopen.100124>,\n    Dvo\u0159\u00e1k and Mrkvi\u010dka (2022) <doi:10.1007/s00180-021-01134-y>,\n    Mrkvi\u010dka et al. (2023) <doi:10.48550/arXiv.2309.04746>, and\n    Konstantinou et al. (2024) <doi: 10.1007/s00180-024-01569-z>.",
    "version": "1.0-7",
    "maintainer": "Mari Myllym\u00e4ki <mari.myllymaki@luke.fi>",
    "author": "Mari Myllym\u00e4ki [aut, cre],\n  Tom\u00e1\u0161 Mrkvi\u010dka [aut],\n  Mikko Kuronen [ctb],\n  Ji\u0159\u00ed Dvo\u0159\u00e1k [ctb],\n  Pavel Grabarnik [ctb],\n  Ute Hahn [ctb],\n  Michael Rost [ctb],\n  Henri Seijo [ctb]",
    "url": "https://github.com/myllym/GET",
    "bug_reports": "https://github.com/myllym/GET/issues",
    "repository": "https://cran.r-project.org/package=GET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GET Global Envelopes Implementation of global envelopes for a set of general d-dimensional vectors T\n    in various applications. A 100(1-alpha)% global envelope is a band bounded by two\n    vectors such that the probability that T falls outside this envelope in any of the d\n    points is equal to alpha. Global means that the probability is controlled simultaneously\n    for all the d elements of the vectors. The global envelopes can be used for graphical\n    Monte Carlo and permutation tests where the test statistic is a multivariate vector or\n    function (e.g. goodness-of-fit testing for point patterns and random sets, functional\n    analysis of variance, functional general linear model, n-sample test of correspondence\n    of distribution functions), for central regions of functional or multivariate data (e.g.\n    outlier detection, functional boxplot) and for global confidence and prediction bands\n    (e.g. confidence band in polynomial regression, Bayesian posterior prediction). See\n    Myllym\u00e4ki and Mrkvi\u010dka (2024) <doi:10.18637/jss.v111.i03>,\n    Myllym\u00e4ki et al. (2017) <doi:10.1111/rssb.12172>,\n    Mrkvi\u010dka and Myllym\u00e4ki (2023) <doi:10.1007/s11222-023-10275-7>,\n    Mrkvi\u010dka et al. (2016) <doi:10.1016/j.spasta.2016.04.005>,\n    Mrkvi\u010dka et al. (2017) <doi:10.1007/s11222-016-9683-9>,\n    Mrkvi\u010dka et al. (2020) <doi:10.14736/kyb-2020-3-0432>,\n    Mrkvi\u010dka et al. (2021) <doi:10.1007/s11009-019-09756-y>,\n    Myllym\u00e4ki et al. (2021) <doi:10.1016/j.spasta.2020.100436>,\n    Mrkvi\u010dka et al. (2022) <doi:10.1002/sim.9236>,\n    Dai et al. (2022) <doi:10.5772/intechopen.100124>,\n    Dvo\u0159\u00e1k and Mrkvi\u010dka (2022) <doi:10.1007/s00180-021-01134-y>,\n    Mrkvi\u010dka et al. (2023) <doi:10.48550/arXiv.2309.04746>, and\n    Konstantinou et al. (2024) <doi: 10.1007/s00180-024-01569-z>.  "
  },
  {
    "id": 3656,
    "package_name": "GIFT",
    "title": "Access to the Global Inventory of Floras and Traits (GIFT)",
    "description": "Retrieving regional plant checklists, species traits and\n  distributions, and environmental data from the Global Inventory of Floras and\n  Traits (GIFT). More information about the GIFT database can be found at\n  <https://gift.uni-goettingen.de/about> and the map of available floras can be\n  visualized at <https://gift.uni-goettingen.de/map>. The API and associated\n  queries can be accessed according the following scheme:\n  <https://gift.uni-goettingen.de/api/extended/index2.0.php?query=env_raster>.",
    "version": "1.3.3",
    "maintainer": "Pierre Denelle <pierre.denelle@gmail.com>",
    "author": "Pierre Denelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5037-2281>),\n  Patrick Weigelt [aut] (ORCID: <https://orcid.org/0000-0002-2485-3708>)",
    "url": "https://github.com/BioGeoMacro/GIFT,\nhttps://biogeomacro.github.io/GIFT/",
    "bug_reports": "https://github.com/BioGeoMacro/GIFT/issues",
    "repository": "https://cran.r-project.org/package=GIFT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GIFT Access to the Global Inventory of Floras and Traits (GIFT) Retrieving regional plant checklists, species traits and\n  distributions, and environmental data from the Global Inventory of Floras and\n  Traits (GIFT). More information about the GIFT database can be found at\n  <https://gift.uni-goettingen.de/about> and the map of available floras can be\n  visualized at <https://gift.uni-goettingen.de/map>. The API and associated\n  queries can be accessed according the following scheme:\n  <https://gift.uni-goettingen.de/api/extended/index2.0.php?query=env_raster>.  "
  },
  {
    "id": 3659,
    "package_name": "GIMMEgVAR",
    "title": "Group Iterative Multiple Model Estimation with 'graphicalVAR'",
    "description": "Data-driven approach for arriving at person-specific time series models from within a Graphical Vector Autoregression (VAR) framework. The method first identifies which relations replicate across the majority of individuals to detect signal from noise. These group-level relations are then used as a foundation for starting the search for person-specific (or individual-level) relations. All estimates are obtained uniquely for each individual in the final models. The method for the 'graphicalVAR' approach is found in Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>. ",
    "version": "0.1.0",
    "maintainer": "Sandra Williams Lee <wsandra@live.unc.edu>",
    "author": "Sandra Williams Lee [aut, cre],\n  Kathleen M. Gates [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GIMMEgVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GIMMEgVAR Group Iterative Multiple Model Estimation with 'graphicalVAR' Data-driven approach for arriving at person-specific time series models from within a Graphical Vector Autoregression (VAR) framework. The method first identifies which relations replicate across the majority of individuals to detect signal from noise. These group-level relations are then used as a foundation for starting the search for person-specific (or individual-level) relations. All estimates are obtained uniquely for each individual in the final models. The method for the 'graphicalVAR' approach is found in Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>.   "
  },
  {
    "id": 3669,
    "package_name": "GLCMTextures",
    "title": "GLCM Textures of Raster Layers",
    "description": "Calculates grey level co-occurrence matrix (GLCM) based texture measures (Hall-Beyer (2017) <https://prism.ucalgary.ca/bitstream/handle/1880/51900/texture%20tutorial%20v%203_0%20180206.pdf>; Haralick et al. (1973) <doi:10.1109/TSMC.1973.4309314>) of raster layers using a sliding rectangular window. It also includes functions to quantize a raster into grey levels as well as tabulate a glcm and calculate glcm texture metrics for a matrix.",
    "version": "0.6.3",
    "maintainer": "Alexander Ilich <ailich@usf.edu>",
    "author": "Alexander Ilich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1758-8499>)",
    "url": "https://ailich.github.io/GLCMTextures/,\nhttps://github.com/ailich/GLCMTextures",
    "bug_reports": "https://github.com/ailich/GLCMTextures/issues",
    "repository": "https://cran.r-project.org/package=GLCMTextures",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GLCMTextures GLCM Textures of Raster Layers Calculates grey level co-occurrence matrix (GLCM) based texture measures (Hall-Beyer (2017) <https://prism.ucalgary.ca/bitstream/handle/1880/51900/texture%20tutorial%20v%203_0%20180206.pdf>; Haralick et al. (1973) <doi:10.1109/TSMC.1973.4309314>) of raster layers using a sliding rectangular window. It also includes functions to quantize a raster into grey levels as well as tabulate a glcm and calculate glcm texture metrics for a matrix.  "
  },
  {
    "id": 3714,
    "package_name": "GPSCDF",
    "title": "Generalized Propensity Score Cumulative Distribution Function",
    "description": "Implements the generalized propensity score cumulative distribution\n    function proposed by Greene (2017)\n    <https://digitalcommons.library.tmc.edu/dissertations/AAI10681743/>.\n    A single scalar balancing score is calculated for any generalized propensity\n    score vector with three or more treatments. This balancing score is used for\n    propensity score matching and stratification in outcome analyses when analyzing\n    either ordinal or multinomial treatments.",
    "version": "0.1.1",
    "maintainer": "Derek W. Brown <derek9@gwu.edu>",
    "author": "Derek W. Brown [aut, cre],\n  Thomas J. Greene [aut],\n  Stacia M. DeSantis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GPSCDF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GPSCDF Generalized Propensity Score Cumulative Distribution Function Implements the generalized propensity score cumulative distribution\n    function proposed by Greene (2017)\n    <https://digitalcommons.library.tmc.edu/dissertations/AAI10681743/>.\n    A single scalar balancing score is calculated for any generalized propensity\n    score vector with three or more treatments. This balancing score is used for\n    propensity score matching and stratification in outcome analyses when analyzing\n    either ordinal or multinomial treatments.  "
  },
  {
    "id": 3719,
    "package_name": "GPareto",
    "title": "Gaussian Processes for Pareto Front Estimation and Optimization",
    "description": "Gaussian process regression models, a.k.a. Kriging models, are\n    applied to global multi-objective optimization of black-box functions.\n    Multi-objective Expected Improvement and Step-wise Uncertainty Reduction\n    sequential infill criteria are available. A quantification of uncertainty\n    on Pareto fronts is provided using conditional simulations.",
    "version": "1.1.9",
    "maintainer": "Mickael Binois <mickael.binois@inria.fr>",
    "author": "Mickael Binois [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7225-1680>),\n  Victor Picheny [aut]",
    "url": "https://github.com/mbinois/GPareto",
    "bug_reports": "https://github.com/mbinois/GPareto/issues",
    "repository": "https://cran.r-project.org/package=GPareto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GPareto Gaussian Processes for Pareto Front Estimation and Optimization Gaussian process regression models, a.k.a. Kriging models, are\n    applied to global multi-objective optimization of black-box functions.\n    Multi-objective Expected Improvement and Step-wise Uncertainty Reduction\n    sequential infill criteria are available. A quantification of uncertainty\n    on Pareto fronts is provided using conditional simulations.  "
  },
  {
    "id": 3728,
    "package_name": "GRAPE",
    "title": "Gene-Ranking Analysis of Pathway Expression",
    "description": "Gene-Ranking Analysis of Pathway Expression (GRAPE) is a tool for\n   summarizing the consensus behavior of biological pathways in the form of a\n   template, and for quantifying the extent to which individual samples deviate\n   from the template. GRAPE templates are based only on the relative rankings\n   of the genes within the pathway and can be used for classification of tissue\n   types or disease subtypes. GRAPE can be used to represent gene-expression\n   samples as vectors of pathway scores, where each pathway score indicates the\n   departure from a given collection of reference samples. The resulting pathway-\n   space representation can be used as the feature set for various applications,\n   including survival analysis and drug-response prediction. \n   Users of GRAPE should use the following citation:\n   Klein MI, Stern DF, and Zhao H. GRAPE: A pathway template method to characterize \n   tissue-specific functionality from gene expression profiles. \n   BMC Bioinformatics, 18:317 (June 2017).",
    "version": "0.1.1",
    "maintainer": "Michael Klein <michael.klein@yale.edu>",
    "author": "Michael Klein <michael.klein@yale.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GRAPE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GRAPE Gene-Ranking Analysis of Pathway Expression Gene-Ranking Analysis of Pathway Expression (GRAPE) is a tool for\n   summarizing the consensus behavior of biological pathways in the form of a\n   template, and for quantifying the extent to which individual samples deviate\n   from the template. GRAPE templates are based only on the relative rankings\n   of the genes within the pathway and can be used for classification of tissue\n   types or disease subtypes. GRAPE can be used to represent gene-expression\n   samples as vectors of pathway scores, where each pathway score indicates the\n   departure from a given collection of reference samples. The resulting pathway-\n   space representation can be used as the feature set for various applications,\n   including survival analysis and drug-response prediction. \n   Users of GRAPE should use the following citation:\n   Klein MI, Stern DF, and Zhao H. GRAPE: A pathway template method to characterize \n   tissue-specific functionality from gene expression profiles. \n   BMC Bioinformatics, 18:317 (June 2017).  "
  },
  {
    "id": 3766,
    "package_name": "GVARX",
    "title": "Perform Global Vector Autoregression Estimation and Inference",
    "description": "Light procedures for learning Global Vector Autoregression model (GVAR) of Pesaran, Schuermann and Weiner (2004) <DOI:10.1198/073500104000000019> and Dees, di Mauro, Pesaran and Smith (2007) <DOI:10.1002/jae.932>.",
    "version": "1.4",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "author": "Ho Tsung-wu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GVARX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GVARX Perform Global Vector Autoregression Estimation and Inference Light procedures for learning Global Vector Autoregression model (GVAR) of Pesaran, Schuermann and Weiner (2004) <DOI:10.1198/073500104000000019> and Dees, di Mauro, Pesaran and Smith (2007) <DOI:10.1002/jae.932>.  "
  },
  {
    "id": 3775,
    "package_name": "GWRLASSO",
    "title": "A Hybrid Model for Spatial Prediction Through Local Regression",
    "description": "It implements a hybrid spatial model for improved spatial prediction by combining the variable selection capability of\n             LASSO (Least Absolute Shrinkage and Selection Operator) with the Geographically Weighted Regression (GWR) model that \n             captures the spatially varying relationship efficiently. For method details see, Wheeler, D.C.(2009).<DOI:10.1068/a40256>.\n             The developed hybrid model efficiently selects the relevant variables by using LASSO as the first step; these selected variables\n             are then incorporated into the GWR framework, allowing the estimation of spatially varying regression coefficients at unknown locations\n             and finally predicting the values of the response variable at unknown test locations while taking into account the spatial heterogeneity of the data. \n             Integrating the LASSO and GWR models enhances prediction accuracy by considering spatial heterogeneity and capturing the local relationships between \n             the predictors and the response variable. The developed hybrid spatial model can be useful for spatial modeling, especially in scenarios involving complex \n             spatial patterns and large datasets with multiple predictor variables.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nobin Chandra Paul [aut, cre, cph],\n  Anil Rai [aut],\n  Ankur Biswas [aut],\n  Tauqueer Ahmad [aut],\n  Bhaskar B. Gaikwad [aut],\n  Dhananjay D. Nangare [aut],\n  K. Sammi Reddy [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GWRLASSO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GWRLASSO A Hybrid Model for Spatial Prediction Through Local Regression It implements a hybrid spatial model for improved spatial prediction by combining the variable selection capability of\n             LASSO (Least Absolute Shrinkage and Selection Operator) with the Geographically Weighted Regression (GWR) model that \n             captures the spatially varying relationship efficiently. For method details see, Wheeler, D.C.(2009).<DOI:10.1068/a40256>.\n             The developed hybrid model efficiently selects the relevant variables by using LASSO as the first step; these selected variables\n             are then incorporated into the GWR framework, allowing the estimation of spatially varying regression coefficients at unknown locations\n             and finally predicting the values of the response variable at unknown test locations while taking into account the spatial heterogeneity of the data. \n             Integrating the LASSO and GWR models enhances prediction accuracy by considering spatial heterogeneity and capturing the local relationships between \n             the predictors and the response variable. The developed hybrid spatial model can be useful for spatial modeling, especially in scenarios involving complex \n             spatial patterns and large datasets with multiple predictor variables.  "
  },
  {
    "id": 3791,
    "package_name": "GeNetIt",
    "title": "Spatial Graph-Theoretic Genetic Gravity Modelling",
    "description": "Implementation of spatial graph-theoretic genetic gravity models.\n    The model framework is applicable for other types of spatial flow questions.\n    Includes functions for constructing spatial graphs, sampling and summarizing\n    associated raster variables and building unconstrained and singly constrained\n    gravity models.",
    "version": "0.1-6",
    "maintainer": "Jeffrey S. Evans <jeffrey_evans@tnc.org>",
    "author": "Jeffrey S. Evans [aut, cre],\n  Melanie Murphy [aut]",
    "url": "https://github.com/jeffreyevans/GeNetIt",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GeNetIt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeNetIt Spatial Graph-Theoretic Genetic Gravity Modelling Implementation of spatial graph-theoretic genetic gravity models.\n    The model framework is applicable for other types of spatial flow questions.\n    Includes functions for constructing spatial graphs, sampling and summarizing\n    associated raster variables and building unconstrained and singly constrained\n    gravity models.  "
  },
  {
    "id": 3815,
    "package_name": "GeneralisedCovarianceMeasure",
    "title": "Test for Conditional Independence Based on the Generalized\nCovariance Measure (GCM)",
    "description": "A statistical hypothesis test for conditional independence. It performs nonlinear regressions on the conditioning variable and then tests for a vanishing covariance between the resulting residuals. It can be applied to both univariate random variables and multivariate random vectors. Details of the method can be found in Rajen D. Shah and Jonas Peters: The Hardness of Conditional Independence Testing and the Generalised Covariance Measure, Annals of Statistics 48(3), 1514--1538, 2020.",
    "version": "0.2.0",
    "maintainer": "Jonas Peters <jonas.peters@math.ku.dk>",
    "author": "Jonas Peters and Rajen D. Shah",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GeneralisedCovarianceMeasure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeneralisedCovarianceMeasure Test for Conditional Independence Based on the Generalized\nCovariance Measure (GCM) A statistical hypothesis test for conditional independence. It performs nonlinear regressions on the conditioning variable and then tests for a vanishing covariance between the resulting residuals. It can be applied to both univariate random variables and multivariate random vectors. Details of the method can be found in Rajen D. Shah and Jonas Peters: The Hardness of Conditional Independence Testing and the Generalised Covariance Measure, Annals of Statistics 48(3), 1514--1538, 2020.  "
  },
  {
    "id": 3827,
    "package_name": "GeoAdjust",
    "title": "Accounting for Random Displacements of True GPS Coordinates of\nData",
    "description": "The purpose is to account for the random displacements \n (jittering) of true survey household cluster center coordinates in geostatistical \n analyses of Demographic and Health Surveys program (DHS) data. Adjustment for \n jittering can be implemented either in the spatial random effect, or in the \n raster/distance based covariates, or in both. Detailed information about the methods \n behind the package functionality can be found in our two papers.\n Umut Altay, John Paige, Andrea Riebler, Geir-Arne Fuglstad (2024) <doi:10.32614/RJ-2024-027>. \n Umut Altay, John Paige, Andrea Riebler, Geir-Arne Fuglstad (2023) <doi:10.1177/1471082X231219847>. ",
    "version": "2.0.1",
    "maintainer": "Umut Altay <altayumut.ua@gmail.com>",
    "author": "Umut Altay [cre, aut],\n  John Paige [aut],\n  Geir-Arne Fuglstad [aut],\n  Andrea Riebler [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GeoAdjust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeoAdjust Accounting for Random Displacements of True GPS Coordinates of\nData The purpose is to account for the random displacements \n (jittering) of true survey household cluster center coordinates in geostatistical \n analyses of Demographic and Health Surveys program (DHS) data. Adjustment for \n jittering can be implemented either in the spatial random effect, or in the \n raster/distance based covariates, or in both. Detailed information about the methods \n behind the package functionality can be found in our two papers.\n Umut Altay, John Paige, Andrea Riebler, Geir-Arne Fuglstad (2024) <doi:10.32614/RJ-2024-027>. \n Umut Altay, John Paige, Andrea Riebler, Geir-Arne Fuglstad (2023) <doi:10.1177/1471082X231219847>.   "
  },
  {
    "id": 3829,
    "package_name": "GeoModels",
    "title": "Procedures for Gaussian and Non Gaussian Geostatistical (Large)\nData Analysis",
    "description": "Functions for Gaussian and Non Gaussian (bivariate) spatial and spatio-temporal data analysis are provided for a) (fast) simulation of random fields,  b) inference  for random fields using standard likelihood and a likelihood approximation  method called  weighted composite likelihood based on pairs and b) prediction using (local) best linear unbiased prediction. Weighted composite likelihood can be very efficient for estimating massive datasets. Both regression and spatial (temporal) dependence analysis can be jointly performed. Flexible covariance models for spatial and spatial-temporal data on Euclidean domains and spheres are provided. There are also many useful functions for plotting and performing diagnostic analysis. Different non Gaussian random fields can be considered in the analysis. Among them, random fields with marginal distributions such as Skew-Gaussian, Student-t, Tukey-h, Sin-Arcsin, Two-piece, Weibull, Gamma, Log-Gaussian, Binomial, Negative Binomial  and Poisson. See the URL for the papers associated with this package, as for instance, Bevilacqua and Gaetan (2015) <doi:10.1007/s11222-014-9460-6>, Bevilacqua et al. (2016) <doi:10.1007/s13253-016-0256-3>, Vallejos et al. (2020) <doi:10.1007/978-3-030-56681-4>, Bevilacqua et. al (2020) <doi:10.1002/env.2632>, Bevilacqua et. al (2021) <doi:10.1111/sjos.12447>, Bevilacqua et al. (2022) <doi:10.1016/j.jmva.2022.104949>, Morales-Navarrete et al. (2023) <doi:10.1080/01621459.2022.2140053>, and a large class of examples and tutorials.",
    "version": "2.2.1",
    "maintainer": "Moreno Bevilacqua <moreno.bevilacqua89@gmail.com>",
    "author": "Moreno Bevilacqua [aut, cre, cph],\n  V\u00edctor Morales-O\u00f1ate [ctb],\n  Francisco Cuevas-Pacheco [ctb],\n  Christian Caama\u00f1o-Carrillo [ctb]",
    "url": "https://vmoprojs.github.io/GeoModels-page/",
    "bug_reports": "https://github.com/vmoprojs/GeoModels/issues",
    "repository": "https://cran.r-project.org/package=GeoModels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeoModels Procedures for Gaussian and Non Gaussian Geostatistical (Large)\nData Analysis Functions for Gaussian and Non Gaussian (bivariate) spatial and spatio-temporal data analysis are provided for a) (fast) simulation of random fields,  b) inference  for random fields using standard likelihood and a likelihood approximation  method called  weighted composite likelihood based on pairs and b) prediction using (local) best linear unbiased prediction. Weighted composite likelihood can be very efficient for estimating massive datasets. Both regression and spatial (temporal) dependence analysis can be jointly performed. Flexible covariance models for spatial and spatial-temporal data on Euclidean domains and spheres are provided. There are also many useful functions for plotting and performing diagnostic analysis. Different non Gaussian random fields can be considered in the analysis. Among them, random fields with marginal distributions such as Skew-Gaussian, Student-t, Tukey-h, Sin-Arcsin, Two-piece, Weibull, Gamma, Log-Gaussian, Binomial, Negative Binomial  and Poisson. See the URL for the papers associated with this package, as for instance, Bevilacqua and Gaetan (2015) <doi:10.1007/s11222-014-9460-6>, Bevilacqua et al. (2016) <doi:10.1007/s13253-016-0256-3>, Vallejos et al. (2020) <doi:10.1007/978-3-030-56681-4>, Bevilacqua et. al (2020) <doi:10.1002/env.2632>, Bevilacqua et. al (2021) <doi:10.1111/sjos.12447>, Bevilacqua et al. (2022) <doi:10.1016/j.jmva.2022.104949>, Morales-Navarrete et al. (2023) <doi:10.1080/01621459.2022.2140053>, and a large class of examples and tutorials.  "
  },
  {
    "id": 3835,
    "package_name": "GeodRegr",
    "title": "Geodesic Regression",
    "description": "Provides a gradient descent algorithm to find a geodesic relationship between real-valued independent variables and a manifold-valued dependent variable (i.e. geodesic regression). Available manifolds are Euclidean space, the sphere, hyperbolic space, and Kendall's 2-dimensional shape space. Besides the standard least-squares loss, the least absolute deviations, Huber, and Tukey biweight loss functions can also be used to perform robust geodesic regression. Functions to help choose appropriate cutoff parameters to maintain high efficiency for the Huber and Tukey biweight estimators are included, as are functions for generating random tangent vectors from the Riemannian normal distributions on the sphere and hyperbolic space. The n-sphere is a n-dimensional manifold: we represent it as a sphere of radius 1 and center 0 embedded in (n+1)-dimensional space. Using the hyperboloid model of hyperbolic space, n-dimensional hyperbolic space is embedded in (n+1)-dimensional Minkowski space as the upper sheet of a hyperboloid of two sheets. Kendall's 2D shape space with K landmarks is of real dimension 2K-4; preshapes are represented as complex K-vectors with mean 0 and magnitude 1. Details are described in Shin, H.-Y. and Oh, H.-S. (2020) <arXiv:2007.04518>. Also see Fletcher, P. T. (2013) <doi:10.1007/s11263-012-0591-y>.",
    "version": "0.2.0",
    "maintainer": "Ha-Young Shin <hayoung.shin@gmail.com>",
    "author": "Ha-Young Shin [aut, cre],\n  Hee-Seok Oh [aut]",
    "url": "https://github.com/hayoungshin1/GeodRegr",
    "bug_reports": "https://github.com/hayoungshin1/GeodRegr/issues",
    "repository": "https://cran.r-project.org/package=GeodRegr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeodRegr Geodesic Regression Provides a gradient descent algorithm to find a geodesic relationship between real-valued independent variables and a manifold-valued dependent variable (i.e. geodesic regression). Available manifolds are Euclidean space, the sphere, hyperbolic space, and Kendall's 2-dimensional shape space. Besides the standard least-squares loss, the least absolute deviations, Huber, and Tukey biweight loss functions can also be used to perform robust geodesic regression. Functions to help choose appropriate cutoff parameters to maintain high efficiency for the Huber and Tukey biweight estimators are included, as are functions for generating random tangent vectors from the Riemannian normal distributions on the sphere and hyperbolic space. The n-sphere is a n-dimensional manifold: we represent it as a sphere of radius 1 and center 0 embedded in (n+1)-dimensional space. Using the hyperboloid model of hyperbolic space, n-dimensional hyperbolic space is embedded in (n+1)-dimensional Minkowski space as the upper sheet of a hyperboloid of two sheets. Kendall's 2D shape space with K landmarks is of real dimension 2K-4; preshapes are represented as complex K-vectors with mean 0 and magnitude 1. Details are described in Shin, H.-Y. and Oh, H.-S. (2020) <arXiv:2007.04518>. Also see Fletcher, P. T. (2013) <doi:10.1007/s11263-012-0591-y>.  "
  },
  {
    "id": 3836,
    "package_name": "GeomArchetypal",
    "title": "Finds the Geometrical Archetypal Analysis of a Data Frame",
    "description": "Performs Geometrical Archetypal Analysis after creating Grid Archetypes \n    which are the Cartesian Product of all minimum, maximum variable values. Since the archetypes \n    are fixed now, we have the ability to compute the convex composition coefficients for all \n    our available data points much faster by using the half part of Principal Convex Hull Archetypal method. \n    Additionally we can decide to keep as archetypes the closer to the Grid Archetypes ones. Finally \n    the number of archetypes is always 2 to the power of the dimension of our data points \n    if we consider them as a vector space. \n    Cutler, A., Breiman, L. (1994)      <doi:10.1080/00401706.1994.10485840>.\n    Morup, M., Hansen, LK. (2012) <doi:10.1016/j.neucom.2011.06.033>.\n    Christopoulos, DT. (2024) <doi:10.13140/RG.2.2.14030.88642>.   ",
    "version": "1.0.3",
    "maintainer": "Demetris Christopoulos <dchristop@econ.uoa.gr>",
    "author": "Demetris Christopoulos [aut, cre, cph],\n  David Midgley [ctb, cph],\n  Sunil Venaik [ctb],\n  INSEAD Hoffmann Institute France [fnd],\n  The University of Queensland Australia [fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GeomArchetypal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeomArchetypal Finds the Geometrical Archetypal Analysis of a Data Frame Performs Geometrical Archetypal Analysis after creating Grid Archetypes \n    which are the Cartesian Product of all minimum, maximum variable values. Since the archetypes \n    are fixed now, we have the ability to compute the convex composition coefficients for all \n    our available data points much faster by using the half part of Principal Convex Hull Archetypal method. \n    Additionally we can decide to keep as archetypes the closer to the Grid Archetypes ones. Finally \n    the number of archetypes is always 2 to the power of the dimension of our data points \n    if we consider them as a vector space. \n    Cutler, A., Breiman, L. (1994)      <doi:10.1080/00401706.1994.10485840>.\n    Morup, M., Hansen, LK. (2012) <doi:10.1016/j.neucom.2011.06.033>.\n    Christopoulos, DT. (2024) <doi:10.13140/RG.2.2.14030.88642>.     "
  },
  {
    "id": 3858,
    "package_name": "GitAI",
    "title": "Extracts Knowledge from 'Git' Repositories",
    "description": "Scan multiple 'Git' repositories, pull specified files content and process it with large language models. You can summarize the content in specific way, extract information and data, or find answers to your questions about the repositories. The output can be stored in vector database and used for semantic search or as a part of a RAG (Retrieval Augmented Generation) prompt.",
    "version": "0.1.3",
    "maintainer": "Kamil Wais <kamil.wais@gmail.com>",
    "author": "Kamil Wais [aut, cre],\n  Krystian Igras [aut],\n  Maciej Banas [aut]",
    "url": "https://github.com/r-world-devs/GitAI",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GitAI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GitAI Extracts Knowledge from 'Git' Repositories Scan multiple 'Git' repositories, pull specified files content and process it with large language models. You can summarize the content in specific way, extract information and data, or find answers to your questions about the repositories. The output can be stored in vector database and used for semantic search or as a part of a RAG (Retrieval Augmented Generation) prompt.  "
  },
  {
    "id": 3894,
    "package_name": "GroupComparisons",
    "title": "Paired/Unpaired Parametric/Non-Parametric Group Comparisons",
    "description": "Receives two vectors, computes appropriate function for group comparison (i.e., t-test, Mann-Whitney; equality of variances), and reports the findings (mean/median, standard deviation, test statistic, p-value, effect size) in APA format (Fay, M.P., & Proschan, M.A. (2010)<DOI: 10.1214/09-SS051>).",
    "version": "0.1.0",
    "maintainer": "Aaron England <aaron.england24@gmail.com>",
    "author": "Aaron England <aaron.england24@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GroupComparisons",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GroupComparisons Paired/Unpaired Parametric/Non-Parametric Group Comparisons Receives two vectors, computes appropriate function for group comparison (i.e., t-test, Mann-Whitney; equality of variances), and reports the findings (mean/median, standard deviation, test statistic, p-value, effect size) in APA format (Fay, M.P., & Proschan, M.A. (2010)<DOI: 10.1214/09-SS051>).  "
  },
  {
    "id": 3931,
    "package_name": "HDNRA",
    "title": "High-Dimensional Location Testing with Normal-Reference\nApproaches",
    "description": "We provide a collection of various classical tests and latest normal-reference tests for comparing high-dimensional mean vectors including two-sample and general linear hypothesis testing (GLHT) problem. Some existing tests for two-sample problem [see Bai, Zhidong, and Hewa Saranadasa.(1996) <https://www.jstor.org/stable/24306018>; Chen, Song Xi, and Ying-Li Qin.(2010) <doi:10.1214/09-aos716>; Srivastava, Muni S., and Meng Du.(2008) <doi:10.1016/j.jmva.2006.11.002>; Srivastava, Muni S., Shota Katayama, and Yutaka Kano.(2013)<doi:10.1016/j.jmva.2012.08.014>]. Normal-reference tests for two-sample problem [see Zhang, Jin-Ting, Jia Guo, Bu Zhou, and Ming-Yen Cheng.(2020) <doi:10.1080/01621459.2019.1604366>; Zhang, Jin-Ting, Bu Zhou, Jia Guo, and Tianming Zhu.(2021) <doi:10.1016/j.jspi.2020.11.008>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2020) <doi:10.1016/j.ecosta.2019.12.002>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2023) <doi:10.1080/02664763.2020.1834516>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1080/10485252.2021.2015768>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1007/s42519-021-00232-w>; Zhu, Tianming, Pengfei Wang, and Jin-Ting Zhang.(2023) <doi:10.1007/s00180-023-01433-6>]. Some existing tests for GLHT problem [see Fujikoshi, Yasunori, Tetsuto Himeno, and Hirofumi Wakaki.(2004) <doi:10.14490/jjss.34.19>; Srivastava, Muni S., and Yasunori Fujikoshi.(2006) <doi:10.1016/j.jmva.2005.08.010>; Yamada, Takayuki, and Muni S. Srivastava.(2012) <doi:10.1080/03610926.2011.581786>; Schott, James R.(2007) <doi:10.1016/j.jmva.2006.11.007>; Zhou, Bu, Jia Guo, and Jin-Ting Zhang.(2017) <doi:10.1016/j.jspi.2017.03.005>]. Normal-reference  tests for GLHT problem [see Zhang, Jin-Ting, Jia Guo, and Bu Zhou.(2017) <doi:10.1016/j.jmva.2017.01.002>; Zhang, Jin-Ting, Bu Zhou, and Jia Guo.(2022) <doi:10.1016/j.jmva.2021.104816>; Zhu, Tianming, Liang Zhang, and Jin-Ting Zhang.(2022) <doi:10.5705/ss.202020.0362>; Zhu, Tianming, and Jin-Ting Zhang.(2022) <doi:10.1007/s00180-021-01110-6>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1016/j.csda.2021.107385>].",
    "version": "2.0.1",
    "maintainer": "Pengfei Wang <nie23.wp8738@e.ntu.edu.sg>",
    "author": "Pengfei Wang [aut, cre],\n  Shuqi Luo [aut],\n  Tianming Zhu [aut],\n  Bu Zhou [aut]",
    "url": "https://nie23wp8738.github.io/HDNRA/",
    "bug_reports": "https://github.com/nie23wp8738/HDNRA/issues",
    "repository": "https://cran.r-project.org/package=HDNRA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDNRA High-Dimensional Location Testing with Normal-Reference\nApproaches We provide a collection of various classical tests and latest normal-reference tests for comparing high-dimensional mean vectors including two-sample and general linear hypothesis testing (GLHT) problem. Some existing tests for two-sample problem [see Bai, Zhidong, and Hewa Saranadasa.(1996) <https://www.jstor.org/stable/24306018>; Chen, Song Xi, and Ying-Li Qin.(2010) <doi:10.1214/09-aos716>; Srivastava, Muni S., and Meng Du.(2008) <doi:10.1016/j.jmva.2006.11.002>; Srivastava, Muni S., Shota Katayama, and Yutaka Kano.(2013)<doi:10.1016/j.jmva.2012.08.014>]. Normal-reference tests for two-sample problem [see Zhang, Jin-Ting, Jia Guo, Bu Zhou, and Ming-Yen Cheng.(2020) <doi:10.1080/01621459.2019.1604366>; Zhang, Jin-Ting, Bu Zhou, Jia Guo, and Tianming Zhu.(2021) <doi:10.1016/j.jspi.2020.11.008>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2020) <doi:10.1016/j.ecosta.2019.12.002>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2023) <doi:10.1080/02664763.2020.1834516>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1080/10485252.2021.2015768>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1007/s42519-021-00232-w>; Zhu, Tianming, Pengfei Wang, and Jin-Ting Zhang.(2023) <doi:10.1007/s00180-023-01433-6>]. Some existing tests for GLHT problem [see Fujikoshi, Yasunori, Tetsuto Himeno, and Hirofumi Wakaki.(2004) <doi:10.14490/jjss.34.19>; Srivastava, Muni S., and Yasunori Fujikoshi.(2006) <doi:10.1016/j.jmva.2005.08.010>; Yamada, Takayuki, and Muni S. Srivastava.(2012) <doi:10.1080/03610926.2011.581786>; Schott, James R.(2007) <doi:10.1016/j.jmva.2006.11.007>; Zhou, Bu, Jia Guo, and Jin-Ting Zhang.(2017) <doi:10.1016/j.jspi.2017.03.005>]. Normal-reference  tests for GLHT problem [see Zhang, Jin-Ting, Jia Guo, and Bu Zhou.(2017) <doi:10.1016/j.jmva.2017.01.002>; Zhang, Jin-Ting, Bu Zhou, and Jia Guo.(2022) <doi:10.1016/j.jmva.2021.104816>; Zhu, Tianming, Liang Zhang, and Jin-Ting Zhang.(2022) <doi:10.5705/ss.202020.0362>; Zhu, Tianming, and Jin-Ting Zhang.(2022) <doi:10.1007/s00180-021-01110-6>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1016/j.csda.2021.107385>].  "
  },
  {
    "id": 3934,
    "package_name": "HDShOP",
    "title": "High-Dimensional Shrinkage Optimal Portfolios",
    "description": "Constructs shrinkage estimators of high-dimensional mean-variance portfolios and performs \n    high-dimensional tests on optimality of a given portfolio. The techniques developed in \n    Bodnar et al. (2018 <doi:10.1016/j.ejor.2017.09.028>, 2019 <doi:10.1109/TSP.2019.2929964>, \n    2020 <doi:10.1109/TSP.2020.3037369>, 2021 <doi:10.1080/07350015.2021.2004897>) \n    are central to the package. They provide simple and feasible estimators and tests for optimal \n    portfolio weights, which are applicable for 'large p and large n' situations where p is the \n    portfolio dimension (number of stocks) and n is the sample size. The package also includes tools\n    for constructing portfolios based on shrinkage estimators of the mean vector and covariance matrix\n    as well as a new Bayesian estimator for the Markowitz efficient frontier recently developed by \n    Bauder et al. (2021) <doi:10.1080/14697688.2020.1748214>.",
    "version": "0.1.7",
    "maintainer": "Dmitry Otryakhin <d.otryakhin.acad@protonmail.ch>",
    "author": "Taras Bodnar [aut] (ORCID: <https://orcid.org/0000-0001-7855-8221>),\n  Solomiia Dmytriv [aut] (ORCID: <https://orcid.org/0000-0003-1855-3044>),\n  Yarema Okhrin [aut] (ORCID: <https://orcid.org/0000-0003-4704-5233>),\n  Dmitry Otryakhin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4700-7221>),\n  Nestor Parolya [aut] (ORCID: <https://orcid.org/0000-0003-2147-2288>)",
    "url": "https://github.com/Otryakhin-Dmitry/global-minimum-variance-portfolio",
    "bug_reports": "https://github.com/Otryakhin-Dmitry/global-minimum-variance-portfolio/issues",
    "repository": "https://cran.r-project.org/package=HDShOP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDShOP High-Dimensional Shrinkage Optimal Portfolios Constructs shrinkage estimators of high-dimensional mean-variance portfolios and performs \n    high-dimensional tests on optimality of a given portfolio. The techniques developed in \n    Bodnar et al. (2018 <doi:10.1016/j.ejor.2017.09.028>, 2019 <doi:10.1109/TSP.2019.2929964>, \n    2020 <doi:10.1109/TSP.2020.3037369>, 2021 <doi:10.1080/07350015.2021.2004897>) \n    are central to the package. They provide simple and feasible estimators and tests for optimal \n    portfolio weights, which are applicable for 'large p and large n' situations where p is the \n    portfolio dimension (number of stocks) and n is the sample size. The package also includes tools\n    for constructing portfolios based on shrinkage estimators of the mean vector and covariance matrix\n    as well as a new Bayesian estimator for the Markowitz efficient frontier recently developed by \n    Bauder et al. (2021) <doi:10.1080/14697688.2020.1748214>.  "
  },
  {
    "id": 3937,
    "package_name": "HDTSA",
    "title": "High Dimensional Time Series Analysis Tools",
    "description": "An implementation for high-dimensional time series analysis methods, including factor model for vector time series \n      proposed by Lam and Yao (2012) <doi:10.1214/12-AOS970> and Chang, Guo and Yao (2015)\n      <doi:10.1016/j.jeconom.2015.03.024>, martingale difference test proposed by \n      Chang, Jiang and Shao (2023) <doi:10.1016/j.jeconom.2022.09.001>, principal \n      component analysis for vector time series proposed by Chang, Guo and Yao (2018) <doi:10.1214/17-AOS1613>,\n      cointegration analysis proposed by Zhang, Robinson and Yao (2019)\n      <doi:10.1080/01621459.2018.1458620>, unit root test proposed by Chang, Cheng and Yao (2022)\n      <doi:10.1093/biomet/asab034>, white noise test proposed by Chang, Yao and Zhou (2017)\n      <doi:10.1093/biomet/asw066>, CP-decomposition for matrix time \n      series proposed by Chang et al. (2023) <doi:10.1093/jrsssb/qkac011> and\n      Chang et al. (2024) <doi:10.48550/arXiv.2410.05634>, and statistical inference for\n      spectral density matrix proposed by Chang et al. (2022) \n      <doi:10.48550/arXiv.2212.13686>.",
    "version": "1.0.5-1",
    "maintainer": "Chen Lin <linchen@smail.swufe.edu.cn>",
    "author": "Jinyuan Chang [aut],\n  Jing He [aut],\n  Chen Lin [aut, cre],\n  Qiwei Yao [aut]",
    "url": "https://github.com/Linc2021/HDTSA",
    "bug_reports": "https://github.com/Linc2021/HDTSA/issues",
    "repository": "https://cran.r-project.org/package=HDTSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDTSA High Dimensional Time Series Analysis Tools An implementation for high-dimensional time series analysis methods, including factor model for vector time series \n      proposed by Lam and Yao (2012) <doi:10.1214/12-AOS970> and Chang, Guo and Yao (2015)\n      <doi:10.1016/j.jeconom.2015.03.024>, martingale difference test proposed by \n      Chang, Jiang and Shao (2023) <doi:10.1016/j.jeconom.2022.09.001>, principal \n      component analysis for vector time series proposed by Chang, Guo and Yao (2018) <doi:10.1214/17-AOS1613>,\n      cointegration analysis proposed by Zhang, Robinson and Yao (2019)\n      <doi:10.1080/01621459.2018.1458620>, unit root test proposed by Chang, Cheng and Yao (2022)\n      <doi:10.1093/biomet/asab034>, white noise test proposed by Chang, Yao and Zhou (2017)\n      <doi:10.1093/biomet/asw066>, CP-decomposition for matrix time \n      series proposed by Chang et al. (2023) <doi:10.1093/jrsssb/qkac011> and\n      Chang et al. (2024) <doi:10.48550/arXiv.2410.05634>, and statistical inference for\n      spectral density matrix proposed by Chang et al. (2022) \n      <doi:10.48550/arXiv.2212.13686>.  "
  },
  {
    "id": 3945,
    "package_name": "HEDA",
    "title": "'Hydropeaking Events Detection Algorithm'",
    "description": "This tool identifies hydropeaking events from raw time-series flow record, a rapid flow variation induced by the hourly-adjusted electricity market. The novelty of 'HEDA' is to use vector angle instead of the first-order derivative to detect change points which not only largely improves the computing efficiency but also accounts for the rate of change of the flow variation. More details <doi:10.1016/j.jhydrol.2021.126392>.",
    "version": "0.1.5",
    "maintainer": "Tingyu Li <styli@ucdavis.edu>",
    "author": "Tingyu Li [aut, cre],\n  Xiaotian Zou [aut],\n  Gregory Pasternack [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HEDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HEDA 'Hydropeaking Events Detection Algorithm' This tool identifies hydropeaking events from raw time-series flow record, a rapid flow variation induced by the hourly-adjusted electricity market. The novelty of 'HEDA' is to use vector angle instead of the first-order derivative to detect change points which not only largely improves the computing efficiency but also accounts for the rate of change of the flow variation. More details <doi:10.1016/j.jhydrol.2021.126392>.  "
  },
  {
    "id": 3987,
    "package_name": "HTGM",
    "title": "High Throughput 'GoMiner'",
    "description": "Two papers published in the early 2000's (Zeeberg, B.R., Feng, W., Wang, G. et al. \n  (2003) <doi:10.1186/gb-2003-4-4-r28>) and (Zeeberg, B.R., Qin, H., Narashimhan, S., et al. (2005)\n  <doi:10.1186/1471-2105-6-168>) implement 'GoMiner' and 'High Throughput GoMiner'\n  ('HTGM') to map lists of genes to the Gene Ontology (GO) <https://geneontology.org>. Until recently,\n  these were hosted on a server at The National Cancer Institute (NCI). In order to continue\n  providing these services to the bio-medical community, I have developed stand-alone versions.\n  The current package 'HTGM' builds upon my recent package 'GoMiner'.\n  The output of 'GoMiner' is a heatmap showing the relationship of a single list\n  of genes and the significant categories into which they map. 'High Throughput GoMiner'\n  ('HTGM') integrates the results of the individual 'GoMiner' analyses. The output of 'HTGM'\n  is a heatmap showing the relationship of the significant categories derived from each gene list.\n  The heatmap has only 2 axes, so the identity of the genes are unfortunately\n  \"integrated out of the equation.\" Because the graphic for the heatmap is implemented\n  in Scalable Vector Graphics (SVG) technology, it is relatively easy to hyperlink each\n  picture element to the relevant list of genes. By clicking on the desired picture\n  element, the user can recover the \"lost\" genes.",
    "version": "1.2",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HTGM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HTGM High Throughput 'GoMiner' Two papers published in the early 2000's (Zeeberg, B.R., Feng, W., Wang, G. et al. \n  (2003) <doi:10.1186/gb-2003-4-4-r28>) and (Zeeberg, B.R., Qin, H., Narashimhan, S., et al. (2005)\n  <doi:10.1186/1471-2105-6-168>) implement 'GoMiner' and 'High Throughput GoMiner'\n  ('HTGM') to map lists of genes to the Gene Ontology (GO) <https://geneontology.org>. Until recently,\n  these were hosted on a server at The National Cancer Institute (NCI). In order to continue\n  providing these services to the bio-medical community, I have developed stand-alone versions.\n  The current package 'HTGM' builds upon my recent package 'GoMiner'.\n  The output of 'GoMiner' is a heatmap showing the relationship of a single list\n  of genes and the significant categories into which they map. 'High Throughput GoMiner'\n  ('HTGM') integrates the results of the individual 'GoMiner' analyses. The output of 'HTGM'\n  is a heatmap showing the relationship of the significant categories derived from each gene list.\n  The heatmap has only 2 axes, so the identity of the genes are unfortunately\n  \"integrated out of the equation.\" Because the graphic for the heatmap is implemented\n  in Scalable Vector Graphics (SVG) technology, it is relatively easy to hyperlink each\n  picture element to the relevant list of genes. By clicking on the desired picture\n  element, the user can recover the \"lost\" genes.  "
  },
  {
    "id": 4037,
    "package_name": "HistDat",
    "title": "Summary Statistics for Histogram/Count Data",
    "description": "In some cases you will have data in a histogram format, where\n  you have a vector of all possible observations, and a vector of how many\n  times each observation appeared. You could expand this into a single 1D\n  vector, but this may not be advisable if the counts are extremely large.\n  'HistDat' allows for the calculation of summary statistics without the need\n  for expanding your data.",
    "version": "0.2.0",
    "maintainer": "Michael Milton <michael.r.milton@gmail.com>",
    "author": "Michael Milton",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HistDat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HistDat Summary Statistics for Histogram/Count Data In some cases you will have data in a histogram format, where\n  you have a vector of all possible observations, and a vector of how many\n  times each observation appeared. You could expand this into a single 1D\n  vector, but this may not be advisable if the counts are extremely large.\n  'HistDat' allows for the calculation of summary statistics without the need\n  for expanding your data.  "
  },
  {
    "id": 4055,
    "package_name": "HyRiM",
    "title": "Multicriteria Risk Management using Zero-Sum Games with\nVector-Valued Payoffs that are Probability Distributions",
    "description": "Construction and analysis of multivalued zero-sum matrix games over the abstract space of probability distributions, which describe the losses in each scenario of defense vs. attack action. The distributions can be compiled directly from expert opinions or other empirical data (insofar available). The package implements the methods put forth in the EU project HyRiM (Hybrid Risk Management for Utility Networks), FP7 EU Project Number 608090. The method has been published in Rass, S., K\u00f6nig, S., Schauer, S., 2016. Decisions with Uncertain Consequences-A Total Ordering on Loss-Distributions. PLoS ONE 11, e0168583. <doi:10.1371/journal.pone.0168583>, and applied for advanced persistent thread modeling in Rass, S., K\u00f6nig, S., Schauer, S., 2017. Defending Against Advanced Persistent Threats Using Game-Theory. PLoS ONE 12, e0168675. <doi:10.1371/journal.pone.0168675>. A volume covering the wider range of aspects of risk management, partially based on the theory implemented in the package is the book edited by S. Rass and S. Schauer, 2018. Game Theory for Security and Risk Management: From Theory to Practice. Springer, <doi:10.1007/978-3-319-75268-6>, ISBN 978-3-319-75267-9.",
    "version": "2.0.2",
    "maintainer": "\"Stefan Rass, on behalf of the Austrian Institute of Technology\" <stefan.rass@jku.at>",
    "author": "Stefan Rass, Sandra Koenig, Ali Alshawish",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HyRiM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HyRiM Multicriteria Risk Management using Zero-Sum Games with\nVector-Valued Payoffs that are Probability Distributions Construction and analysis of multivalued zero-sum matrix games over the abstract space of probability distributions, which describe the losses in each scenario of defense vs. attack action. The distributions can be compiled directly from expert opinions or other empirical data (insofar available). The package implements the methods put forth in the EU project HyRiM (Hybrid Risk Management for Utility Networks), FP7 EU Project Number 608090. The method has been published in Rass, S., K\u00f6nig, S., Schauer, S., 2016. Decisions with Uncertain Consequences-A Total Ordering on Loss-Distributions. PLoS ONE 11, e0168583. <doi:10.1371/journal.pone.0168583>, and applied for advanced persistent thread modeling in Rass, S., K\u00f6nig, S., Schauer, S., 2017. Defending Against Advanced Persistent Threats Using Game-Theory. PLoS ONE 12, e0168675. <doi:10.1371/journal.pone.0168675>. A volume covering the wider range of aspects of risk management, partially based on the theory implemented in the package is the book edited by S. Rass and S. Schauer, 2018. Game Theory for Security and Risk Management: From Theory to Practice. Springer, <doi:10.1007/978-3-319-75268-6>, ISBN 978-3-319-75267-9.  "
  },
  {
    "id": 4108,
    "package_name": "ICvectorfields",
    "title": "Vector Fields from Spatial Time Series of Population Abundance",
    "description": "Functions for converting time series of spatial abundance or density \n    data in raster format to vector fields of population movement using the digital \n    image correlation technique. More specifically, the functions in the package \n    compute cross-covariance using discrete fast Fourier transforms for computational \n    efficiency. Vectors in vector fields point in the direction of highest two \n    dimensional cross-covariance. The package has a novel implementation of the \n    digital image correlation algorithm that is designed to detect persistent \n    directional movement when image time series extend beyond a sequence of \n    two raster images. ",
    "version": "0.1.2",
    "maintainer": "Devin Goodsman <goodsman@ualberta.ca>",
    "author": "Devin Goodsman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1935-5779>)",
    "url": "",
    "bug_reports": "https://github.com/goodsman/ICvectorfields/issues",
    "repository": "https://cran.r-project.org/package=ICvectorfields",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICvectorfields Vector Fields from Spatial Time Series of Population Abundance Functions for converting time series of spatial abundance or density \n    data in raster format to vector fields of population movement using the digital \n    image correlation technique. More specifically, the functions in the package \n    compute cross-covariance using discrete fast Fourier transforms for computational \n    efficiency. Vectors in vector fields point in the direction of highest two \n    dimensional cross-covariance. The package has a novel implementation of the \n    digital image correlation algorithm that is designed to detect persistent \n    directional movement when image time series extend beyond a sequence of \n    two raster images.   "
  },
  {
    "id": 4117,
    "package_name": "IDPmisc",
    "title": "'Utilities of Institute of Data Analyses and Process Design\n(www.zhaw.ch/idp)'",
    "description": "Different high-level graphics functions for displaying large datasets, displaying circular data in a very flexible way, finding local maxima, brewing color ramps, drawing nice arrows, zooming 2D-plots, creating figures with differently colored margin and plot region.  In addition, the package contains auxiliary functions for data manipulation like omitting observations with irregular values or selecting data by logical vectors, which include NAs. Other functions are especially useful in spectroscopy and analyses of environmental data: robust baseline fitting, finding peaks in spectra, converting humidity measures.",
    "version": "1.1.21",
    "maintainer": "Christoph Hofer <christoph.hofer@zhaw.ch>",
    "author": "Christoph Hofer [cre],\n  Rene Locher [aut],\n  Andreas Ruckstuhl [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IDPmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IDPmisc 'Utilities of Institute of Data Analyses and Process Design\n(www.zhaw.ch/idp)' Different high-level graphics functions for displaying large datasets, displaying circular data in a very flexible way, finding local maxima, brewing color ramps, drawing nice arrows, zooming 2D-plots, creating figures with differently colored margin and plot region.  In addition, the package contains auxiliary functions for data manipulation like omitting observations with irregular values or selecting data by logical vectors, which include NAs. Other functions are especially useful in spectroscopy and analyses of environmental data: robust baseline fitting, finding peaks in spectra, converting humidity measures.  "
  },
  {
    "id": 4160,
    "package_name": "INLAtools",
    "title": "Functionalities for the 'INLA' Package",
    "description": "Contain code to work with a C struct, in short\n  cgeneric, to define a Gaussian Markov random (GMRF) model.\n  The cgeneric contain code to specify GMRF elements such as\n  the graph and the precision matrix, and also the initial and \n  prior for its parameters, useful for model inference. \n  It can be accessed from a C program and is the recommended \n  way to implement new GMRF models in the 'INLA' package \n  (<https://www.r-inla.org>).\n  The 'INLAtools' implement functions to evaluate \n  each one of the model specifications from R.\n  The implemented functionalities leverage the use \n  of 'cgeneric' models and provide a way to debug \n  the code as well to work with the prior for the \n  model parameters and to sample from it. \n  A very useful functionality is the Kronecker product method \n  that creates a new model from multiple cgeneric models.\n  It also works with the rgeneric, the R version of the \n  cgeneric intended to easy try implementation of new GMRF models.\n  The Kronecker between two cgeneric models was used in \n  Sterrantino et. al. (2024) <doi:10.1007/s10260-025-00788-y>,\n  and can be used to build the spatio-temporal intrinsic interaction \n  models for what the needed constraints are automatically set.",
    "version": "0.0.5",
    "maintainer": "Elias Teixeira Krainski <eliaskrainski@gmail.com>",
    "author": "Elias Teixeira Krainski [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-7063-2615>),\n  Finn Lindgren [aut] (ORCID: <https://orcid.org/0000-0002-5833-2011>),\n  Haavard Rue\u2019 [aut] (ORCID: <https://orcid.org/0000-0002-0222-1881>)",
    "url": "https://github.com/eliaskrainski/INLAtools",
    "bug_reports": "https://github.com/eliaskrainski/INLAtools/issues",
    "repository": "https://cran.r-project.org/package=INLAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "INLAtools Functionalities for the 'INLA' Package Contain code to work with a C struct, in short\n  cgeneric, to define a Gaussian Markov random (GMRF) model.\n  The cgeneric contain code to specify GMRF elements such as\n  the graph and the precision matrix, and also the initial and \n  prior for its parameters, useful for model inference. \n  It can be accessed from a C program and is the recommended \n  way to implement new GMRF models in the 'INLA' package \n  (<https://www.r-inla.org>).\n  The 'INLAtools' implement functions to evaluate \n  each one of the model specifications from R.\n  The implemented functionalities leverage the use \n  of 'cgeneric' models and provide a way to debug \n  the code as well to work with the prior for the \n  model parameters and to sample from it. \n  A very useful functionality is the Kronecker product method \n  that creates a new model from multiple cgeneric models.\n  It also works with the rgeneric, the R version of the \n  cgeneric intended to easy try implementation of new GMRF models.\n  The Kronecker between two cgeneric models was used in \n  Sterrantino et. al. (2024) <doi:10.1007/s10260-025-00788-y>,\n  and can be used to build the spatio-temporal intrinsic interaction \n  models for what the needed constraints are automatically set.  "
  },
  {
    "id": 4208,
    "package_name": "IVCor",
    "title": "A Robust Integrated Variance Correlation",
    "description": "A integrated variance correlation is proposed to measure the dependence between a categorical or continuous random variable and a continuous random variable or vector.\n    This package is designed to estimate the new correlation coefficient with parametric and nonparametric approaches. \n    Test of independence for different problems can also be implemented via the new correlation coefficient with this package.",
    "version": "0.1.0",
    "maintainer": "Han Pan <scott_pan@163.com>",
    "author": "Wei Xiong [aut],\n  Han Pan [aut, cre],\n  Hengjian Cui [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IVCor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IVCor A Robust Integrated Variance Correlation A integrated variance correlation is proposed to measure the dependence between a categorical or continuous random variable and a continuous random variable or vector.\n    This package is designed to estimate the new correlation coefficient with parametric and nonparametric approaches. \n    Test of independence for different problems can also be implemented via the new correlation coefficient with this package.  "
  },
  {
    "id": 4229,
    "package_name": "IncDTW",
    "title": "Incremental Calculation of Dynamic Time Warping",
    "description": "The Dynamic Time Warping (DTW) distance measure for time series allows non-linear alignments of time series to match  similar patterns in time series of different lengths and or different speeds. IncDTW is characterized by (1) the incremental calculation of DTW (reduces runtime complexity to a linear level for updating the DTW distance) - especially for life data streams or subsequence matching, (2) the vector based implementation of DTW which is faster because no matrices are allocated (reduces the space complexity from a quadratic to a linear level in the number of observations) - for all runtime intensive DTW computations, (3) the subsequence matching algorithm runDTW, that efficiently finds the k-NN to a query pattern in a long time series, and (4) C++ in the heart. For details about DTW see the original paper \"Dynamic programming algorithm optimization for spoken word recognition\" by Sakoe and Chiba (1978) <DOI:10.1109/TASSP.1978.1163055>. For details about this package, Dynamic Time Warping and Incremental Dynamic Time Warping please see \"IncDTW: An R Package for Incremental Calculation of Dynamic Time Warping\" by Leodolter et al. (2021) <doi:10.18637/jss.v099.i09>.",
    "version": "1.1.4.5",
    "maintainer": "Maximilian Leodolter <maximilian.leodolter@gmail.com>",
    "author": "Maximilian Leodolter [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IncDTW",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IncDTW Incremental Calculation of Dynamic Time Warping The Dynamic Time Warping (DTW) distance measure for time series allows non-linear alignments of time series to match  similar patterns in time series of different lengths and or different speeds. IncDTW is characterized by (1) the incremental calculation of DTW (reduces runtime complexity to a linear level for updating the DTW distance) - especially for life data streams or subsequence matching, (2) the vector based implementation of DTW which is faster because no matrices are allocated (reduces the space complexity from a quadratic to a linear level in the number of observations) - for all runtime intensive DTW computations, (3) the subsequence matching algorithm runDTW, that efficiently finds the k-NN to a query pattern in a long time series, and (4) C++ in the heart. For details about DTW see the original paper \"Dynamic programming algorithm optimization for spoken word recognition\" by Sakoe and Chiba (1978) <DOI:10.1109/TASSP.1978.1163055>. For details about this package, Dynamic Time Warping and Incremental Dynamic Time Warping please see \"IncDTW: An R Package for Incremental Calculation of Dynamic Time Warping\" by Leodolter et al. (2021) <doi:10.18637/jss.v099.i09>.  "
  },
  {
    "id": 4248,
    "package_name": "Information",
    "title": "Data Exploration with Information Theory (Weight-of-Evidence and\nInformation Value)",
    "description": "Performs exploratory data analysis and variable screening for\n    binary classification models using weight-of-evidence (WOE) and information\n    value (IV). In order to make the package as efficient as possible, aggregations\n    are done in data.table and creation of WOE vectors can be distributed across\n    multiple cores. The package also supports exploration for uplift models (NWOE\n    and NIV).",
    "version": "0.0.9",
    "maintainer": "Larsen Kim <kblarsen4@gmail.com>",
    "author": "Larsen Kim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Information",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Information Data Exploration with Information Theory (Weight-of-Evidence and\nInformation Value) Performs exploratory data analysis and variable screening for\n    binary classification models using weight-of-evidence (WOE) and information\n    value (IV). In order to make the package as efficient as possible, aggregations\n    are done in data.table and creation of WOE vectors can be distributed across\n    multiple cores. The package also supports exploration for uplift models (NWOE\n    and NIV).  "
  },
  {
    "id": 4274,
    "package_name": "Irescale",
    "title": "Calculate and Rectify Moran's I",
    "description": "Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>.",
    "version": "2.3.0",
    "maintainer": "Ivan Fuentes <jivfur@tamu.edu>",
    "author": "Ivan Fuentes, Thomas DeWitt, Thomas Ioerger, Michael Bishop",
    "url": "https://github.tamu.edu/jivfur/rectifiedI",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Irescale",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Irescale Calculate and Rectify Moran's I Provides a scaling method to obtain a standardized Moran's I measure. Moran's I is a measure for the spatial autocorrelation of a data set, it gives a measure of similarity between data and its surrounding. The range of this value must be [-1,1], but this does not happen in practice. This package scale the Moran's I value and map it into the theoretical range of [-1,1]. Once the Moran's I value is rescaled, it facilitates the comparison between projects, for instance, a researcher can calculate Moran's I in a city in China, with a sample size of n1 and area of interest a1. Another researcher runs a similar experiment in a city in Mexico with different sample size, n2, and an area of interest a2. Due to the differences between the conditions, it is not possible to compare Moran's I in a straightforward way. In this version of the package, the spatial autocorrelation Moran's I is calculated as proposed in Chen(2013) <arXiv:1606.03658>.  "
  },
  {
    "id": 4286,
    "package_name": "IsoriX",
    "title": "Isoscape Computation and Inference of Spatial Origins using\nMixed Models",
    "description": "Building isoscapes using mixed models and inferring the geographic\n  origin of samples based on their isotopic ratios. This package is essentially a\n  simplified interface to several other packages which implements a new\n  statistical framework based on mixed models. It uses 'spaMM' for fitting and\n  predicting isoscapes, and assigning an organism's origin depending on its\n  isotopic ratio. 'IsoriX' also relies heavily on the package 'rasterVis' for\n  plotting the maps produced with 'terra' using 'lattice'.",
    "version": "0.9.3",
    "maintainer": "Alexandre Courtiol <alexandre.courtiol@gmail.com>",
    "author": "Alexandre Courtiol [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0637-2959>),\n  Fran\u00e7ois Rousset [aut] (ORCID: <https://orcid.org/0000-0003-4670-0371>),\n  Marie-Sophie Rohwaeder [aut],\n  Stephanie Kramer-Schadt [aut] (ORCID:\n    <https://orcid.org/0000-0002-9269-4446>)",
    "url": "https://github.com/courtiol/IsoriX,\nhttps://bookdown.org/content/782",
    "bug_reports": "https://github.com/courtiol/IsoriX/issues",
    "repository": "https://cran.r-project.org/package=IsoriX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IsoriX Isoscape Computation and Inference of Spatial Origins using\nMixed Models Building isoscapes using mixed models and inferring the geographic\n  origin of samples based on their isotopic ratios. This package is essentially a\n  simplified interface to several other packages which implements a new\n  statistical framework based on mixed models. It uses 'spaMM' for fitting and\n  predicting isoscapes, and assigning an organism's origin depending on its\n  isotopic ratio. 'IsoriX' also relies heavily on the package 'rasterVis' for\n  plotting the maps produced with 'terra' using 'lattice'.  "
  },
  {
    "id": 4304,
    "package_name": "JMI",
    "title": "Jackknife Mutual Information",
    "description": "Computes the Jackknife Mutual Information (JMI) between two random vectors and provides the p-value for dependence tests. See Zeng, X., Xia, Y. and Tong, H. (2018) <doi:10.1073/pnas.1715593115>.",
    "version": "0.1.0",
    "maintainer": "Zeng Xianli <a0123862@u.nus.edu>",
    "author": "Zeng Xianli <a0123862@u.nus.edu>, Hang Weiqiang <e0010758@u.nus.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JMI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JMI Jackknife Mutual Information Computes the Jackknife Mutual Information (JMI) between two random vectors and provides the p-value for dependence tests. See Zeng, X., Xia, Y. and Tong, H. (2018) <doi:10.1073/pnas.1715593115>.  "
  },
  {
    "id": 4323,
    "package_name": "JacobiEigen",
    "title": "Classical Jacobi Eigenvalue Algorithm",
    "description": "Implements the classical Jacobi algorithm for the\n    eigenvalues and eigenvectors of a real symmetric matrix, both in \n    pure 'R' and in 'C++' using 'Rcpp'. Mainly as a programming example \n    for teaching purposes.",
    "version": "0.3-4",
    "maintainer": "Bill Venables <Bill.Venables@gmail.com>",
    "author": "Bill Venables",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JacobiEigen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JacobiEigen Classical Jacobi Eigenvalue Algorithm Implements the classical Jacobi algorithm for the\n    eigenvalues and eigenvectors of a real symmetric matrix, both in \n    pure 'R' and in 'C++' using 'Rcpp'. Mainly as a programming example \n    for teaching purposes.  "
  },
  {
    "id": 4356,
    "package_name": "KMLtoSHAPE",
    "title": "Preserving Attribute Values: Converting KML to Shapefile",
    "description": "The developed function is designed to facilitate the seamless conversion of KML (Keyhole Markup Language)\n             files to Shapefiles while preserving attribute values. It provides a straightforward interface for users\n             to effortlessly import KML data, extract relevant attributes, and export them into the widely compatible\n             Shapefile format. The package ensures accurate representation of spatial data while maintaining the integrity\n             of associated attribute information. For details see, Flores, G. (2021). <DOI:10.1007/978-3-030-63665-4_15>.\n             Whether for spatial analysis, visualization, or data interoperability, it simplifies the conversion process and \n             empowers users to seamlessly work with geospatial datasets.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nirmal Kumar [aut, cph],\n  Nobin Chandra Paul [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KMLtoSHAPE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KMLtoSHAPE Preserving Attribute Values: Converting KML to Shapefile The developed function is designed to facilitate the seamless conversion of KML (Keyhole Markup Language)\n             files to Shapefiles while preserving attribute values. It provides a straightforward interface for users\n             to effortlessly import KML data, extract relevant attributes, and export them into the widely compatible\n             Shapefile format. The package ensures accurate representation of spatial data while maintaining the integrity\n             of associated attribute information. For details see, Flores, G. (2021). <DOI:10.1007/978-3-030-63665-4_15>.\n             Whether for spatial analysis, visualization, or data interoperability, it simplifies the conversion process and \n             empowers users to seamlessly work with geospatial datasets.  "
  },
  {
    "id": 4386,
    "package_name": "Keng",
    "title": "Knock Errors Off Nice Guesses",
    "description": "Miscellaneous functions and data used in psychological research and teaching. Keng \n    currently has a built-in dataset depress, and could (1) scale a vector; (2) compute the cut-off \n    values of Pearson's r with known sample size; (3) test the significance and compute the post-hoc\n    power for Pearson's r with known sample size; (4) conduct a priori power analysis and plan the \n    sample size for Pearson's r; (5) compare lm()'s fitted outputs using R-squared, f_squared, \n    post-hoc power, and PRE (Proportional Reduction in Error, also called partial R-squared or \n    partial Eta-squared); (6) calculate PRE from partial correlation, Cohen's f, or f_squared; \n    (7) conduct a priori power analysis and plan the sample size for one or a set of predictors in \n    regression analysis; (8) conduct post-hoc power analysis for one or a set of predictors in \n    regression analysis with known sample size; (9) randomly pick numbers for Chinese Super Lotto\n    and Double Color Balls; (10) assess course objective achievement in Outcome-Based Education.",
    "version": "2025.10.8",
    "maintainer": "Qingyao Zhang <qingyaozhang@outlook.com>",
    "author": "Qingyao Zhang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6891-5982>)",
    "url": "https://github.com/qyaozh/Keng",
    "bug_reports": "https://github.com/qyaozh/Keng/issues",
    "repository": "https://cran.r-project.org/package=Keng",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Keng Knock Errors Off Nice Guesses Miscellaneous functions and data used in psychological research and teaching. Keng \n    currently has a built-in dataset depress, and could (1) scale a vector; (2) compute the cut-off \n    values of Pearson's r with known sample size; (3) test the significance and compute the post-hoc\n    power for Pearson's r with known sample size; (4) conduct a priori power analysis and plan the \n    sample size for Pearson's r; (5) compare lm()'s fitted outputs using R-squared, f_squared, \n    post-hoc power, and PRE (Proportional Reduction in Error, also called partial R-squared or \n    partial Eta-squared); (6) calculate PRE from partial correlation, Cohen's f, or f_squared; \n    (7) conduct a priori power analysis and plan the sample size for one or a set of predictors in \n    regression analysis; (8) conduct post-hoc power analysis for one or a set of predictors in \n    regression analysis with known sample size; (9) randomly pick numbers for Chinese Super Lotto\n    and Double Color Balls; (10) assess course objective achievement in Outcome-Based Education.  "
  },
  {
    "id": 4405,
    "package_name": "KoulMde",
    "title": "Koul's Minimum Distance Estimation in Regression and Image\nSegmentation Problems",
    "description": "Many methods are developed to deal with two major statistical problems: image segmentation \n    and nonparametric estimation in various regression models. Image segmentation is nowadays \n    gaining a lot of attention from various scientific subfields. Especially, image segmentation \n    has been popular in medical research such as magnetic resonance imaging (MRI) analysis. When \n    a patient suffers from some brain diseases such as dementia and Parkinson's disease, \n    those diseases can be easily diagnosed in brain MRI: the area affected by those diseases is \n    brightly expressed in MRI, which is called a white lesion. For the purpose of medical research,\n    locating and segment those white lesions in MRI is a critical issue; it can be done\n    manually. However, manual segmentation is very expensive in that it is error-prone and demands a huge\n    amount of time. Therefore, supervised machine learning has emerged as an alternative solution. \n    Despite its powerful performance in a classification problem such as hand-written digits, supervised \n    machine learning has not shown the same satisfactory result in MRI analysis. Setting aside all issues\n    of the supervised machine learning, it exposed a critical problem when employed for MRI analysis: it \n    requires time-consuming data labeling. Thus, there is a strong demand for an unsupervised approach, \n    and this package - based on Hira L. Koul (1986) <DOI:10.1214/aos/1176350059> - proposes an efficient\n    method for simple image segmentation - here, \"simple\" means that an image is black-and-white - which \n    can easily be applied to MRI analysis. This package includes a function GetSegImage(): when a black-and-white\n    image is given as an input, GetSegImage() separates an area of white pixels - which corresponds to \n    a white lesion in MRI - from the given image. For the second problem, consider linear regression model and autoregressive model of\n    order q where errors in the linear regression model and innovations in the\n    autoregression model are independent and symmetrically distributed. Hira L. Koul\n    (1986) <DOI:10.1214/aos/1176350059> proposed a nonparametric minimum distance\n    estimation method by minimizing L2-type distance between certain weighted\n    residual empirical processes. He also proposed a simpler version of the loss\n    function by using symmetry of the integrating measure in the distance. Kim\n    (2018) <DOI:10.1080/00949655.2017.1392527> proposed a fast computational method\n    which enables practitioners to compute the minimum distance estimator of the vector\n    of general multiple regression parameters for several integrating measures. This\n    package contains three functions: KoulLrMde(), KoulArMde(), and Koul2StageMde().\n    The former two provide minimum distance estimators for linear regression model\n    and autoregression model, respectively, where both are based on Koul's method.\n    These two functions take much less time for the computation than those based\n    on parametric minimum distance estimation methods. Koul2StageMde() provides\n    estimators for regression and autoregressive coefficients of linear regression\n    model with autoregressive errors through minimum distant method of two stages.\n    The new version is written in Rcpp and dramatically reduces computational time. ",
    "version": "3.2.1",
    "maintainer": "Jiwoong Kim <jwboys26@gmail.com>",
    "author": "Jiwoong Kim <jwboys26 at gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KoulMde",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KoulMde Koul's Minimum Distance Estimation in Regression and Image\nSegmentation Problems Many methods are developed to deal with two major statistical problems: image segmentation \n    and nonparametric estimation in various regression models. Image segmentation is nowadays \n    gaining a lot of attention from various scientific subfields. Especially, image segmentation \n    has been popular in medical research such as magnetic resonance imaging (MRI) analysis. When \n    a patient suffers from some brain diseases such as dementia and Parkinson's disease, \n    those diseases can be easily diagnosed in brain MRI: the area affected by those diseases is \n    brightly expressed in MRI, which is called a white lesion. For the purpose of medical research,\n    locating and segment those white lesions in MRI is a critical issue; it can be done\n    manually. However, manual segmentation is very expensive in that it is error-prone and demands a huge\n    amount of time. Therefore, supervised machine learning has emerged as an alternative solution. \n    Despite its powerful performance in a classification problem such as hand-written digits, supervised \n    machine learning has not shown the same satisfactory result in MRI analysis. Setting aside all issues\n    of the supervised machine learning, it exposed a critical problem when employed for MRI analysis: it \n    requires time-consuming data labeling. Thus, there is a strong demand for an unsupervised approach, \n    and this package - based on Hira L. Koul (1986) <DOI:10.1214/aos/1176350059> - proposes an efficient\n    method for simple image segmentation - here, \"simple\" means that an image is black-and-white - which \n    can easily be applied to MRI analysis. This package includes a function GetSegImage(): when a black-and-white\n    image is given as an input, GetSegImage() separates an area of white pixels - which corresponds to \n    a white lesion in MRI - from the given image. For the second problem, consider linear regression model and autoregressive model of\n    order q where errors in the linear regression model and innovations in the\n    autoregression model are independent and symmetrically distributed. Hira L. Koul\n    (1986) <DOI:10.1214/aos/1176350059> proposed a nonparametric minimum distance\n    estimation method by minimizing L2-type distance between certain weighted\n    residual empirical processes. He also proposed a simpler version of the loss\n    function by using symmetry of the integrating measure in the distance. Kim\n    (2018) <DOI:10.1080/00949655.2017.1392527> proposed a fast computational method\n    which enables practitioners to compute the minimum distance estimator of the vector\n    of general multiple regression parameters for several integrating measures. This\n    package contains three functions: KoulLrMde(), KoulArMde(), and Koul2StageMde().\n    The former two provide minimum distance estimators for linear regression model\n    and autoregression model, respectively, where both are based on Koul's method.\n    These two functions take much less time for the computation than those based\n    on parametric minimum distance estimation methods. Koul2StageMde() provides\n    estimators for regression and autoregressive coefficients of linear regression\n    model with autoregressive errors through minimum distant method of two stages.\n    The new version is written in Rcpp and dramatically reduces computational time.   "
  },
  {
    "id": 4409,
    "package_name": "KrigInv",
    "title": "Kriging-Based Inversion for Deterministic and Noisy Computer\nExperiments",
    "description": "Criteria and algorithms for sequentially estimating level sets of a multivariate numerical function, possibly observed with noise.",
    "version": "1.4.2",
    "maintainer": "Dario Azzimonti <dario.azzimonti@gmail.com>",
    "author": "Clement Chevalier [aut],\n  Dario Azzimonti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5080-3061>),\n  David Ginsbourger [aut] (ORCID:\n    <https://orcid.org/0000-0003-2724-2678>),\n  Victor Picheny [aut] (ORCID: <https://orcid.org/0000-0002-4948-5542>),\n  Yann Richet [ctb] (ORCID: <https://orcid.org/0000-0002-5677-8458>)",
    "url": "https://doi.org/10.1016/j.csda.2013.03.008",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KrigInv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KrigInv Kriging-Based Inversion for Deterministic and Noisy Computer\nExperiments Criteria and algorithms for sequentially estimating level sets of a multivariate numerical function, possibly observed with noise.  "
  },
  {
    "id": 4461,
    "package_name": "LINselect",
    "title": "Selection of Linear Estimators",
    "description": "Estimate the mean of a Gaussian vector, by choosing among a large collection of estimators,\n  following the method developed by Y. Baraud, C. Giraud and S. Huet (2014) <doi:10.1214/13-AIHP539>.\n  In particular it solves the problem of variable selection by choosing the best predictor among predictors emanating from different methods as lasso,\n  elastic-net, adaptive lasso, pls, randomForest. Moreover, it can be applied for choosing the tuning parameter in a Gauss-lasso procedure.",
    "version": "1.1.6",
    "maintainer": "Benjamin Auder <benjamin.auder@universite-paris-saclay.fr>",
    "author": "Yannick Baraud [aut],\n  Christophe Giraud [aut],\n  Sylvie Huet [aut],\n  Benjamin Auder [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LINselect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LINselect Selection of Linear Estimators Estimate the mean of a Gaussian vector, by choosing among a large collection of estimators,\n  following the method developed by Y. Baraud, C. Giraud and S. Huet (2014) <doi:10.1214/13-AIHP539>.\n  In particular it solves the problem of variable selection by choosing the best predictor among predictors emanating from different methods as lasso,\n  elastic-net, adaptive lasso, pls, randomForest. Moreover, it can be applied for choosing the tuning parameter in a Gauss-lasso procedure.  "
  },
  {
    "id": 4505,
    "package_name": "LSAfun",
    "title": "Applied Latent Semantic Analysis (LSA) Functions",
    "description": "Provides functions that allow for convenient working with vector space models of semantics/distributional semantic models/word embeddings. \n\tOriginally built for LSA models (hence the name), but can be used for all such vector-based models. \n\tFor actually building a vector semantic space, use the package 'lsa' or other specialized software. \n\tDownloadable semantic spaces can be found at <https://sites.google.com/site/fritzgntr/software-resources>.",
    "version": "0.8.1",
    "maintainer": "Fritz Guenther <fritz.guenther@uni-tuebingen.de>",
    "author": "Fritz Guenther [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LSAfun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSAfun Applied Latent Semantic Analysis (LSA) Functions Provides functions that allow for convenient working with vector space models of semantics/distributional semantic models/word embeddings. \n\tOriginally built for LSA models (hence the name), but can be used for all such vector-based models. \n\tFor actually building a vector semantic space, use the package 'lsa' or other specialized software. \n\tDownloadable semantic spaces can be found at <https://sites.google.com/site/fritzgntr/software-resources>.  "
  },
  {
    "id": 4510,
    "package_name": "LSDsensitivity",
    "title": "Sensitivity Analysis Tools for LSD Simulations",
    "description": "Tools for sensitivity analysis of LSD simulation models. Reads object-oriented data produced by LSD simulation models and performs screening and global sensitivity analysis (Sobol decomposition method, Saltelli et al. (2008) ISBN:9780470725177). A Kriging or polynomial meta-model (Kleijnen (2009) <doi:10.1016/j.ejor.2007.10.013>) is estimated using the simulation data to provide the data required by the Sobol decomposition. LSD (Laboratory for Simulation Development) is free software developed by Marco Valente and Marcelo C. Pereira (documentation and downloads available at <https://www.labsimdev.org/>).",
    "version": "1.3.0",
    "maintainer": "Marcelo C. Pereira <mcper@unicamp.br>",
    "author": "Marcelo C. Pereira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8069-2734>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LSDsensitivity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSDsensitivity Sensitivity Analysis Tools for LSD Simulations Tools for sensitivity analysis of LSD simulation models. Reads object-oriented data produced by LSD simulation models and performs screening and global sensitivity analysis (Sobol decomposition method, Saltelli et al. (2008) ISBN:9780470725177). A Kriging or polynomial meta-model (Kleijnen (2009) <doi:10.1016/j.ejor.2007.10.013>) is estimated using the simulation data to provide the data required by the Sobol decomposition. LSD (Laboratory for Simulation Development) is free software developed by Marco Valente and Marcelo C. Pereira (documentation and downloads available at <https://www.labsimdev.org/>).  "
  },
  {
    "id": 4521,
    "package_name": "LSVAR",
    "title": "Estimation of Low Rank Plus Sparse Structured Vector\nAuto-Regressive (VAR) Model",
    "description": "Implementations of estimation algorithm of low rank plus sparse structured VAR model by using Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). It relates to the algorithm in Sumanta, Li, and Michailidis (2019) <doi:10.1109/TSP.2018.2887401>.",
    "version": "1.2",
    "maintainer": "Peiliang Bai <baipl92@ufl.edu>",
    "author": "Peiliang Bai [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LSVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSVAR Estimation of Low Rank Plus Sparse Structured Vector\nAuto-Regressive (VAR) Model Implementations of estimation algorithm of low rank plus sparse structured VAR model by using Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). It relates to the algorithm in Sumanta, Li, and Michailidis (2019) <doi:10.1109/TSP.2018.2887401>.  "
  },
  {
    "id": 4523,
    "package_name": "LSX",
    "title": "Semi-Supervised Algorithm for Document Scaling",
    "description": "A word embeddings-based semi-supervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.\n    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).\n    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.",
    "version": "1.5.1",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph]",
    "url": "https://koheiw.github.io/LSX/",
    "bug_reports": "https://github.com/koheiw/LSX/issues",
    "repository": "https://cran.r-project.org/package=LSX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSX Semi-Supervised Algorithm for Document Scaling A word embeddings-based semi-supervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.\n    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).\n    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.  "
  },
  {
    "id": 4548,
    "package_name": "Largevars",
    "title": "Testing Large VARs for the Presence of Cointegration",
    "description": "Conducts a cointegration test for high-dimensional vector autoregressions (VARs) of order k based on the large N,T asymptotics of Bykhovskaya and Gorin, 2022 (<doi:10.48550/arXiv.2202.07150>). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy-1 point process. This package contains simulated quantiles of the first ten partial sums of the Airy-1 point process that are precise up to the first three digits.",
    "version": "1.0.3",
    "maintainer": "Eszter Kiss <ekiss2803@gmail.com>",
    "author": "Anna Bykhovskaya [aut],\n  Vadim Gorin [aut],\n  Eszter Kiss [cre, aut]",
    "url": "https://github.com/eszter-kiss/Largevars",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Largevars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Largevars Testing Large VARs for the Presence of Cointegration Conducts a cointegration test for high-dimensional vector autoregressions (VARs) of order k based on the large N,T asymptotics of Bykhovskaya and Gorin, 2022 (<doi:10.48550/arXiv.2202.07150>). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy-1 point process. This package contains simulated quantiles of the first ten partial sums of the Airy-1 point process that are precise up to the first three digits.  "
  },
  {
    "id": 4557,
    "package_name": "LatticeKrig",
    "title": "Multi-Resolution Kriging Based on Markov Random Fields",
    "description": "Methods for the interpolation of large spatial\n  datasets. This package uses a basis function approach that\n  provides a surface fitting method\n  that can approximate standard spatial data models.\n  Using a large number of basis functions allows for estimates that\n  can come close to interpolating the observations (a spatial model\n  with a small nugget variance.)  Moreover, the covariance model for\n  this method can approximate the Matern covariance family but also\n  allows for a multi-resolution model and supports efficient\n  computation of the profile likelihood for estimating covariance\n  parameters. This is accomplished through compactly supported basis\n  functions and a Markov random field model for the basis\n  coefficients. These features lead to sparse matrices for the\n  computations and this package makes of the R spam package for sparse\n  linear algebra.\n  An extension of this version over previous ones ( < 5.4 ) is the\n  support for different geometries besides a rectangular domain. The\n  Markov random field approach combined with a basis function\n  representation makes the implementation of different geometries\n  simple where only a few specific R functions need to be added with\n  most of the computation and evaluation done by generic routines that\n  have been tuned to be efficient.  One benefit of this package's\n  model/approach is the facility to do unconditional and conditional\n  simulation of the field for large numbers of arbitrary points. There\n  is also the flexibility for estimating non-stationary covariances\n  and also the case when the observations are a linear combination\n  (e.g. an integral) of the spatial process. Included are generic\n  methods for prediction, standard errors for prediction, plotting of\n  the estimated surface and conditional and unconditional simulation.\n  See the 'LatticeKrigRPackage' GitHub repository for a vignette of this\n  package.\n  Development of this package was supported in part by the National\n  Science Foundation  Grant 1417857 and the National Center for\n  Atmospheric Research. ",
    "version": "9.3.0",
    "maintainer": "Douglas Nychka <nychka@mines.edu>",
    "author": "Douglas Nychka [aut, cre],\n  Dorit Hammerling [aut],\n  Stephan Sain [aut],\n  Nathan Lenssen [aut],\n  Colette Smirniotis [aut],\n  Matthew Iverson [aut],\n  Antony Sikorski [aut]",
    "url": "https://www.r-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LatticeKrig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LatticeKrig Multi-Resolution Kriging Based on Markov Random Fields Methods for the interpolation of large spatial\n  datasets. This package uses a basis function approach that\n  provides a surface fitting method\n  that can approximate standard spatial data models.\n  Using a large number of basis functions allows for estimates that\n  can come close to interpolating the observations (a spatial model\n  with a small nugget variance.)  Moreover, the covariance model for\n  this method can approximate the Matern covariance family but also\n  allows for a multi-resolution model and supports efficient\n  computation of the profile likelihood for estimating covariance\n  parameters. This is accomplished through compactly supported basis\n  functions and a Markov random field model for the basis\n  coefficients. These features lead to sparse matrices for the\n  computations and this package makes of the R spam package for sparse\n  linear algebra.\n  An extension of this version over previous ones ( < 5.4 ) is the\n  support for different geometries besides a rectangular domain. The\n  Markov random field approach combined with a basis function\n  representation makes the implementation of different geometries\n  simple where only a few specific R functions need to be added with\n  most of the computation and evaluation done by generic routines that\n  have been tuned to be efficient.  One benefit of this package's\n  model/approach is the facility to do unconditional and conditional\n  simulation of the field for large numbers of arbitrary points. There\n  is also the flexibility for estimating non-stationary covariances\n  and also the case when the observations are a linear combination\n  (e.g. an integral) of the spatial process. Included are generic\n  methods for prediction, standard errors for prediction, plotting of\n  the estimated surface and conditional and unconditional simulation.\n  See the 'LatticeKrigRPackage' GitHub repository for a vignette of this\n  package.\n  Development of this package was supported in part by the National\n  Science Foundation  Grant 1417857 and the National Center for\n  Atmospheric Research.   "
  },
  {
    "id": 4572,
    "package_name": "LexFindR",
    "title": "Find Related Items and Lexical Dimensions in a Lexicon",
    "description": "Implements code to identify lexical competitors in a given list\n  of words. We include many of the standard competitor types used in spoken word\n  recognition research, such as functions to find cohorts, neighbors, and\n  rhymes, amongst many others. The package includes documentation for using a\n  variety of lexicon files, including those with form codes made up of multiple\n  letters (i.e., phoneme codes) and also basic orthographies. Importantly, the\n  code makes use of multiple CPU cores and vectorization when possible, making\n  it extremely fast and able to handle large lexicons. Additionally, the package\n  contains documentation for users to easily write new functions, allowing\n  researchers to examine other relationships within a lexicon. \n  Preprint: <https://osf.io/preprints/psyarxiv/8dyru/>. Open access: <doi:10.3758/s13428-021-01667-6>. \n  Citation: Li, Z., Crinnion, A.M. & Magnuson, J.S. (2021). \n  <doi:10.3758/s13428-021-01667-6>.",
    "version": "1.1.0",
    "maintainer": "ZhaoBin Li <li_zhaobin@icloud.com>",
    "author": "ZhaoBin Li [aut, cre],\n  Anne Marie Crinnion [aut],\n  James S. Magnuson [aut, cph]",
    "url": "https://github.com/maglab-uconn/LexFindR",
    "bug_reports": "https://github.com/maglab-uconn/LexFindR/issues",
    "repository": "https://cran.r-project.org/package=LexFindR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LexFindR Find Related Items and Lexical Dimensions in a Lexicon Implements code to identify lexical competitors in a given list\n  of words. We include many of the standard competitor types used in spoken word\n  recognition research, such as functions to find cohorts, neighbors, and\n  rhymes, amongst many others. The package includes documentation for using a\n  variety of lexicon files, including those with form codes made up of multiple\n  letters (i.e., phoneme codes) and also basic orthographies. Importantly, the\n  code makes use of multiple CPU cores and vectorization when possible, making\n  it extremely fast and able to handle large lexicons. Additionally, the package\n  contains documentation for users to easily write new functions, allowing\n  researchers to examine other relationships within a lexicon. \n  Preprint: <https://osf.io/preprints/psyarxiv/8dyru/>. Open access: <doi:10.3758/s13428-021-01667-6>. \n  Citation: Li, Z., Crinnion, A.M. & Magnuson, J.S. (2021). \n  <doi:10.3758/s13428-021-01667-6>.  "
  },
  {
    "id": 4575,
    "package_name": "LiblineaR",
    "title": "Linear Predictive Models Based on the LIBLINEAR C/C++ Library",
    "description": "A wrapper around the LIBLINEAR C/C++ library for machine\n        learning (available at\n        <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>). LIBLINEAR is\n        a simple library for solving large-scale regularized linear\n        classification and regression. It currently supports\n        L2-regularized classification (such as logistic regression,\n        L2-loss linear SVM and L1-loss linear SVM) as well as\n        L1-regularized classification (such as L2-loss linear SVM and\n        logistic regression) and L2-regularized support vector\n        regression (with L1- or L2-loss). The main features of\n        LiblineaR include multi-class classification (one-vs-the rest,\n        and Crammer & Singer method), cross validation for model\n        selection, probability estimates (logistic regression only) or\n        weights for unbalanced data. The estimation of the models is\n        particularly fast as compared to other libraries.",
    "version": "2.10-24",
    "maintainer": "Thibault Helleputte <thibault.helleputte@dnalytics.com>",
    "author": "Thibault Helleputte [cre, aut, cph],\n  J\u00e9r\u00f4me Paul [aut],\n  Pierre Gramme [aut]",
    "url": "<https://dnalytics.com/software/liblinear/>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LiblineaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LiblineaR Linear Predictive Models Based on the LIBLINEAR C/C++ Library A wrapper around the LIBLINEAR C/C++ library for machine\n        learning (available at\n        <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>). LIBLINEAR is\n        a simple library for solving large-scale regularized linear\n        classification and regression. It currently supports\n        L2-regularized classification (such as logistic regression,\n        L2-loss linear SVM and L1-loss linear SVM) as well as\n        L1-regularized classification (such as L2-loss linear SVM and\n        logistic regression) and L2-regularized support vector\n        regression (with L1- or L2-loss). The main features of\n        LiblineaR include multi-class classification (one-vs-the rest,\n        and Crammer & Singer method), cross validation for model\n        selection, probability estimates (logistic regression only) or\n        weights for unbalanced data. The estimation of the models is\n        particularly fast as compared to other libraries.  "
  },
  {
    "id": 4628,
    "package_name": "LowRankQP",
    "title": "Low Rank Quadratic Programming",
    "description": "Solves quadratic programming problems where the Hessian is represented as the product of two matrices. Thanks to Greg Hunt for helping getting this version back on CRAN. The methods in this package are described in: Ormerod, Wand and Koch (2008) \"Penalised spline support vector classifiers: computational issues\" <doi:10.1007/s00180-007-0102-8>.",
    "version": "1.0.6",
    "maintainer": "John T. Ormerod <john.ormerod@sydney.edu.au>",
    "author": "John T. Ormerod [aut, cre],\n  Matt P. Wand [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LowRankQP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LowRankQP Low Rank Quadratic Programming Solves quadratic programming problems where the Hessian is represented as the product of two matrices. Thanks to Greg Hunt for helping getting this version back on CRAN. The methods in this package are described in: Ormerod, Wand and Koch (2008) \"Penalised spline support vector classifiers: computational issues\" <doi:10.1007/s00180-007-0102-8>.  "
  },
  {
    "id": 4658,
    "package_name": "MARSGWR",
    "title": "A Hybrid Spatial Model for Capturing Spatially Varying\nRelationships Between Variables in the Data",
    "description": "It is a hybrid spatial model that combines the strength of two widely used regression models, MARS (Multivariate Adaptive Regression Splines) and\n             GWR (Geographically Weighted Regression) to provide an effective approach for predicting a response variable at unknown locations. The MARS model\n             is used in the first step of the development of a hybrid model to identify the most important predictor variables that assist in predicting the response\n             variable. For method details see, Friedman, J.H. (1991). <DOI:10.1214/aos/1176347963>.The GWR model is then used to predict the response variable at \n             testing locations based on these selected variables that account for spatial variations in the relationships between the variables. This hybrid model \n             can improve the accuracy of the predictions compared to using an individual model alone.This developed hybrid spatial model can be useful particularly in \n             cases where the relationship between the response variable and predictor variables is complex and non-linear, and varies across locations.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nobin Chandra Paul [aut, cre, cph],\n  Anil Rai [aut],\n  Ankur Biswas [aut],\n  Tauqueer Ahmad [aut],\n  Dhananjay D. Nangare [aut],\n  Bhaskar B. Gaikwad [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MARSGWR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MARSGWR A Hybrid Spatial Model for Capturing Spatially Varying\nRelationships Between Variables in the Data It is a hybrid spatial model that combines the strength of two widely used regression models, MARS (Multivariate Adaptive Regression Splines) and\n             GWR (Geographically Weighted Regression) to provide an effective approach for predicting a response variable at unknown locations. The MARS model\n             is used in the first step of the development of a hybrid model to identify the most important predictor variables that assist in predicting the response\n             variable. For method details see, Friedman, J.H. (1991). <DOI:10.1214/aos/1176347963>.The GWR model is then used to predict the response variable at \n             testing locations based on these selected variables that account for spatial variations in the relationships between the variables. This hybrid model \n             can improve the accuracy of the predictions compared to using an individual model alone.This developed hybrid spatial model can be useful particularly in \n             cases where the relationship between the response variable and predictor variables is complex and non-linear, and varies across locations.  "
  },
  {
    "id": 4659,
    "package_name": "MARSS",
    "title": "Multivariate Autoregressive State-Space Modeling",
    "description": "The MARSS package provides maximum-likelihood parameter\n    estimation for constrained and unconstrained linear multivariate autoregressive\n    state-space (MARSS) models, including partially deterministic models. MARSS models are a class \n    of dynamic linear model (DLM) and vector autoregressive model (VAR)\n    model. Fitting available via Expectation-Maximization (EM), BFGS (using optim), and 'TMB'\n    (using the 'marssTMB' companion package). Functions are provided for parametric and \n    innovations bootstrapping, Kalman filtering and smoothing, model selection criteria including\n    bootstrap AICb, confidences intervals via the Hessian approximation or bootstrapping, and all\n    conditional residual types. See the user guide for examples of dynamic factor analysis, \n    dynamic linear models, outlier and shock detection, and multivariate AR-p models.  \n    Online workshops (lectures, eBook, and computer labs) \n    at <https://atsa-es.github.io/>.",
    "version": "3.11.10",
    "maintainer": "Elizabeth Eli Holmes <eli.holmes@noaa.gov>",
    "author": "Elizabeth Eli Holmes [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9128-8393>),\n  Eric J. Ward [aut] (ORCID: <https://orcid.org/0000-0002-4359-0296>),\n  Mark D. Scheuerell [aut] (ORCID:\n    <https://orcid.org/0000-0002-8284-1254>),\n  Kellie Wills [aut]",
    "url": "https://atsa-es.github.io/MARSS/",
    "bug_reports": "https://github.com/atsa-es/MARSS/issues",
    "repository": "https://cran.r-project.org/package=MARSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MARSS Multivariate Autoregressive State-Space Modeling The MARSS package provides maximum-likelihood parameter\n    estimation for constrained and unconstrained linear multivariate autoregressive\n    state-space (MARSS) models, including partially deterministic models. MARSS models are a class \n    of dynamic linear model (DLM) and vector autoregressive model (VAR)\n    model. Fitting available via Expectation-Maximization (EM), BFGS (using optim), and 'TMB'\n    (using the 'marssTMB' companion package). Functions are provided for parametric and \n    innovations bootstrapping, Kalman filtering and smoothing, model selection criteria including\n    bootstrap AICb, confidences intervals via the Hessian approximation or bootstrapping, and all\n    conditional residual types. See the user guide for examples of dynamic factor analysis, \n    dynamic linear models, outlier and shock detection, and multivariate AR-p models.  \n    Online workshops (lectures, eBook, and computer labs) \n    at <https://atsa-es.github.io/>.  "
  },
  {
    "id": 4660,
    "package_name": "MARSSVRhybrid",
    "title": "MARS SVR Hybrid",
    "description": "Multivariate Adaptive Regression Spline (MARS) based Support Vector Regression (SVR) hybrid model is combined Machine learning hybrid approach which selects important variables using MARS and then fits SVR on the extracted important variables.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "author": "Pankaj Das [aut, cre],\n  Achal Lama [aut],\n  Girish Jha [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MARSSVRhybrid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MARSSVRhybrid MARS SVR Hybrid Multivariate Adaptive Regression Spline (MARS) based Support Vector Regression (SVR) hybrid model is combined Machine learning hybrid approach which selects important variables using MARS and then fits SVR on the extracted important variables.  "
  },
  {
    "id": 4678,
    "package_name": "MBMethPred",
    "title": "Medulloblastoma Subgroups Prediction",
    "description": "Utilizing a combination of machine learning models (Random Forest, Naive Bayes, K-Nearest Neighbor, Support Vector Machines, Extreme Gradient Boosting, and Linear Discriminant Analysis) and a deep Artificial Neural Network model, 'MBMethPred' can predict medulloblastoma subgroups, including wingless (WNT), sonic hedgehog (SHH), Group 3, and Group 4 from DNA methylation beta values. See Sharif Rahmani E, Lawarde A, Lingasamy P, Moreno SV, Salumets A and Modhukur V (2023), MBMethPred: a computational framework for the accurate classification of childhood medulloblastoma subgroups using data integration and AI-based approaches. Front. Genet. 14:1233657. <doi: 10.3389/fgene.2023.1233657> for more details.",
    "version": "0.1.4.4",
    "maintainer": "Edris Sharif Rahmani <rahmani.biotech@gmail.com>",
    "author": "Edris Sharif Rahmani [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0000-0002-7899-1663>),\n  Ankita Sunil Lawarde [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-7572-4431>),\n  Vijayachitra Modhukur [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-7123-9903>)",
    "url": "https://github.com/sharifrahmanie/MBMethPred",
    "bug_reports": "https://github.com/sharifrahmanie/MBMethPred/issues",
    "repository": "https://cran.r-project.org/package=MBMethPred",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MBMethPred Medulloblastoma Subgroups Prediction Utilizing a combination of machine learning models (Random Forest, Naive Bayes, K-Nearest Neighbor, Support Vector Machines, Extreme Gradient Boosting, and Linear Discriminant Analysis) and a deep Artificial Neural Network model, 'MBMethPred' can predict medulloblastoma subgroups, including wingless (WNT), sonic hedgehog (SHH), Group 3, and Group 4 from DNA methylation beta values. See Sharif Rahmani E, Lawarde A, Lingasamy P, Moreno SV, Salumets A and Modhukur V (2023), MBMethPred: a computational framework for the accurate classification of childhood medulloblastoma subgroups using data integration and AI-based approaches. Front. Genet. 14:1233657. <doi: 10.3389/fgene.2023.1233657> for more details.  "
  },
  {
    "id": 4703,
    "package_name": "MCPModGeneral",
    "title": "A Supplement to the 'DoseFinding' Package for the General Case",
    "description": "Analyzes non-normal data via the Multiple Comparison Procedures and Modeling approach (MCP-Mod). Many functions rely on the 'DoseFinding' package. This package makes it so the user does not need to provide or calculate the mu vector and S matrix. Instead, the user typically supplies the data in its raw form, and this package will calculate the needed objects and passes them into the 'DoseFinding' functions. If the user wishes to primarily use the functions provided in the 'DoseFinding' package, a singular function (prepareGen()) will provide mu and S. The package currently handles power analysis and the MCP-Mod procedure for negative binomial, Poisson, and binomial data. The MCP-Mod procedure can also be applied to survival data, but power analysis is not available.\n\tBretz, F., Pinheiro, J. C., and Branson, M. (2005) <doi:10.1111/j.1541-0420.2005.00344.x>.\n\tBuckland, S. T., Burnham, K. P. and Augustin, N. H. (1997) <doi:10.2307/2533961>.\n\tPinheiro, J. C., Bornkamp, B., Glimm, E. and Bretz, F. (2014) <doi:10.1002/sim.6052>.",
    "version": "0.1-3",
    "maintainer": "Ian Laga <ilaga25@gmail.com>",
    "author": "Ian Laga [aut, cre],\n  Francis Boateng [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MCPModGeneral",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MCPModGeneral A Supplement to the 'DoseFinding' Package for the General Case Analyzes non-normal data via the Multiple Comparison Procedures and Modeling approach (MCP-Mod). Many functions rely on the 'DoseFinding' package. This package makes it so the user does not need to provide or calculate the mu vector and S matrix. Instead, the user typically supplies the data in its raw form, and this package will calculate the needed objects and passes them into the 'DoseFinding' functions. If the user wishes to primarily use the functions provided in the 'DoseFinding' package, a singular function (prepareGen()) will provide mu and S. The package currently handles power analysis and the MCP-Mod procedure for negative binomial, Poisson, and binomial data. The MCP-Mod procedure can also be applied to survival data, but power analysis is not available.\n\tBretz, F., Pinheiro, J. C., and Branson, M. (2005) <doi:10.1111/j.1541-0420.2005.00344.x>.\n\tBuckland, S. T., Burnham, K. P. and Augustin, N. H. (1997) <doi:10.2307/2533961>.\n\tPinheiro, J. C., Bornkamp, B., Glimm, E. and Bretz, F. (2014) <doi:10.1002/sim.6052>.  "
  },
  {
    "id": 4751,
    "package_name": "MGMM",
    "title": "Missingness Aware Gaussian Mixture Models",
    "description": "Parameter estimation and classification for Gaussian Mixture Models (GMMs) in the presence of missing data. This package complements existing implementations by allowing for both missing elements in the input vectors and full (as opposed to strictly diagonal) covariance matrices. Estimation is performed using an expectation conditional maximization algorithm that accounts for missingness of both the cluster assignments and the vector components. The output includes the marginal cluster membership probabilities; the mean and covariance of each cluster; the posterior probabilities of cluster membership; and a completed version of the input data, with missing values imputed to their posterior expectations. For additional details, please see McCaw ZR, Julienne H, Aschard H. \"Fitting Gaussian mixture models on incomplete data.\" <doi:10.1186/s12859-022-04740-9>.",
    "version": "1.0.1.1",
    "maintainer": "Zachary McCaw <zmccaw@alumni.harvard.edu>",
    "author": "Zachary McCaw [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2006-9828>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MGMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MGMM Missingness Aware Gaussian Mixture Models Parameter estimation and classification for Gaussian Mixture Models (GMMs) in the presence of missing data. This package complements existing implementations by allowing for both missing elements in the input vectors and full (as opposed to strictly diagonal) covariance matrices. Estimation is performed using an expectation conditional maximization algorithm that accounts for missingness of both the cluster assignments and the vector components. The output includes the marginal cluster membership probabilities; the mean and covariance of each cluster; the posterior probabilities of cluster membership; and a completed version of the input data, with missing values imputed to their posterior expectations. For additional details, please see McCaw ZR, Julienne H, Aschard H. \"Fitting Gaussian mixture models on incomplete data.\" <doi:10.1186/s12859-022-04740-9>.  "
  },
  {
    "id": 4754,
    "package_name": "MGSDA",
    "title": "Multi-Group Sparse Discriminant Analysis",
    "description": "Implements Multi-Group Sparse Discriminant Analysis proposal of I.Gaynanova, J.Booth and M.Wells (2016), Simultaneous sparse estimation of canonical vectors in the p>>N setting, JASA <doi:10.1080/01621459.2015.1034318>.",
    "version": "1.6.1",
    "maintainer": "Irina Gaynanova <irinagn@umich.edu>",
    "author": "Irina Gaynanova",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MGSDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MGSDA Multi-Group Sparse Discriminant Analysis Implements Multi-Group Sparse Discriminant Analysis proposal of I.Gaynanova, J.Booth and M.Wells (2016), Simultaneous sparse estimation of canonical vectors in the p>>N setting, JASA <doi:10.1080/01621459.2015.1034318>.  "
  },
  {
    "id": 4823,
    "package_name": "MMLR",
    "title": "Fitting Markov-Modulated Linear Regression Models",
    "description": "A set of tools for fitting Markov-modulated linear regression, where responses Y(t) are time-additive, and model operates in the external environment, which is described as a continuous time Markov chain with finite state space.    \n    Model is proposed by Alexander Andronov (2012) <arXiv:1901.09600v1> and algorithm of parameters estimation is based on eigenvalues and eigenvectors decomposition.\n    Markov-switching regression models have the same idea of varying the regression parameters randomly in accordance with external environment. The difference is that for Markov-modulated linear regression model the external environment is described as a continuous-time homogeneous irreducible Markov chain with known parameters while switching models consider Markov chain as unobserved and estimation procedure involves estimation of transition matrix.\n    These models have significant differences in terms of the analytical approach.\n    Also, package provides a set of data simulation tools for Markov-modulated linear regression (for academical/research purposes).\n    Research project No. 1.1.1.2/VIAA/1/16/075.",
    "version": "0.2.0",
    "maintainer": "Nadezda Spiridovska <Spiridovska.N@tsi.lv>",
    "author": "Nadezda Spiridovska [aut, cre],\n  Diana Santalova [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MMLR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MMLR Fitting Markov-Modulated Linear Regression Models A set of tools for fitting Markov-modulated linear regression, where responses Y(t) are time-additive, and model operates in the external environment, which is described as a continuous time Markov chain with finite state space.    \n    Model is proposed by Alexander Andronov (2012) <arXiv:1901.09600v1> and algorithm of parameters estimation is based on eigenvalues and eigenvectors decomposition.\n    Markov-switching regression models have the same idea of varying the regression parameters randomly in accordance with external environment. The difference is that for Markov-modulated linear regression model the external environment is described as a continuous-time homogeneous irreducible Markov chain with known parameters while switching models consider Markov chain as unobserved and estimation procedure involves estimation of transition matrix.\n    These models have significant differences in terms of the analytical approach.\n    Also, package provides a set of data simulation tools for Markov-modulated linear regression (for academical/research purposes).\n    Research project No. 1.1.1.2/VIAA/1/16/075.  "
  },
  {
    "id": 4834,
    "package_name": "MNormTest",
    "title": "Multivariate Normal Hypothesis Testing",
    "description": "Hypothesis testing of the parameters of multivariate normal distributions, including the testing of a single mean vector, two mean vectors, multiple mean vectors, a single covariance matrix, multiple covariance matrices, a mean and a covariance matrix simultaneously, and the testing of independence of multivariate normal random vectors. Huixuan, Gao (2005, ISBN:9787301078587), \"Applied Multivariate Statistical Analysis\".",
    "version": "1.1.1",
    "maintainer": "Xifeng Zhang <cnxifeng9819@163.com>",
    "author": "Xifeng Zhang [aut, cre] (ORCID:\n    <https://orcid.org/0009-0001-8878-3753>)",
    "url": "https://github.com/Astringency/MNormTest,\nhttps://astringency.github.io/MNormTest/,\nhttps://CRAN.R-project.org/package=MNormTest",
    "bug_reports": "https://github.com/Astringency/MNormTest/issues",
    "repository": "https://cran.r-project.org/package=MNormTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MNormTest Multivariate Normal Hypothesis Testing Hypothesis testing of the parameters of multivariate normal distributions, including the testing of a single mean vector, two mean vectors, multiple mean vectors, a single covariance matrix, multiple covariance matrices, a mean and a covariance matrix simultaneously, and the testing of independence of multivariate normal random vectors. Huixuan, Gao (2005, ISBN:9787301078587), \"Applied Multivariate Statistical Analysis\".  "
  },
  {
    "id": 4846,
    "package_name": "MPCI",
    "title": "Multivariate Process Capability Indices (MPCI)",
    "description": "It performs the followings Multivariate Process Capability Indices: Shahriari et al. (1995) Multivariate Capability Vector, Taam et al. (1993) Multivariate Capability Index (MCpm), Pan and Lee (2010) proposal (NMCpm) and the followings based on Principal Component Analysis (PCA):Wang and Chen (1998), Xekalaki and Perakis (2002) and Wang (2005). Two datasets are included. ",
    "version": "1.0.7",
    "maintainer": "Edgar Santos-Fernandez <edgar.santosfdez@gmail.com>",
    "author": "Edgar Santos-Fernandez, Michele Scagliarini.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MPCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MPCI Multivariate Process Capability Indices (MPCI) It performs the followings Multivariate Process Capability Indices: Shahriari et al. (1995) Multivariate Capability Vector, Taam et al. (1993) Multivariate Capability Index (MCpm), Pan and Lee (2010) proposal (NMCpm) and the followings based on Principal Component Analysis (PCA):Wang and Chen (1998), Xekalaki and Perakis (2002) and Wang (2005). Two datasets are included.   "
  },
  {
    "id": 4853,
    "package_name": "MPSEM",
    "title": "Modelling Phylogenetic Signals using Eigenvector Maps",
    "description": "Computational tools to represent phylogenetic signals using adapted\n  eigenvector maps.",
    "version": "0.6-1",
    "maintainer": "Guillaume Gu\u00e9nard <guillaume.guenard@gmail.com>",
    "author": "Guillaume Gu\u00e9nard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0761-3072>),\n  Pierre Legendre [ctb] (ORCID: <https://orcid.org/0000-0002-3838-3305>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MPSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MPSEM Modelling Phylogenetic Signals using Eigenvector Maps Computational tools to represent phylogenetic signals using adapted\n  eigenvector maps.  "
  },
  {
    "id": 4859,
    "package_name": "MRAM",
    "title": "Multivariate Regression Association Measure",
    "description": "Implementations of an estimator for the multivariate regression association measure (MRAM) proposed in Shih and Chen (2025) <in revision> and its associated variable selection algorithm. The MRAM quantifies the predictability of a random vector Y from a random vector X given a random vector Z. It takes the maximum value 1 if and only if Y is almost surely a measurable function of X and Z, and the minimum value of 0 if Y is conditionally independent of X given Z. The MRAM generalizes the Kendall's tau copula correlation ratio proposed in Shih and Emura (2021) <doi:10.1016/j.jmva.2020.104708> by employing the spatial sign function. The estimator is based on the nearest neighbor method, and the associated variable selection algorithm is adapted from the feature ordering by conditional independence (FOCI) algorithm of Azadkia and Chatterjee (2021) <doi:10.1214/21-AOS2073>. For further details, see the paper Shih and Chen (2025) <in revision>.",
    "version": "0.2.1",
    "maintainer": "Jia-Han Shih <jhshih@math.nsysu.edu.tw>",
    "author": "Jia-Han Shih [aut, cre],\n  Yi-Hau Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MRAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MRAM Multivariate Regression Association Measure Implementations of an estimator for the multivariate regression association measure (MRAM) proposed in Shih and Chen (2025) <in revision> and its associated variable selection algorithm. The MRAM quantifies the predictability of a random vector Y from a random vector X given a random vector Z. It takes the maximum value 1 if and only if Y is almost surely a measurable function of X and Z, and the minimum value of 0 if Y is conditionally independent of X given Z. The MRAM generalizes the Kendall's tau copula correlation ratio proposed in Shih and Emura (2021) <doi:10.1016/j.jmva.2020.104708> by employing the spatial sign function. The estimator is based on the nearest neighbor method, and the associated variable selection algorithm is adapted from the feature ordering by conditional independence (FOCI) algorithm of Azadkia and Chatterjee (2021) <doi:10.1214/21-AOS2073>. For further details, see the paper Shih and Chen (2025) <in revision>.  "
  },
  {
    "id": 4913,
    "package_name": "MTS",
    "title": "All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models",
    "description": "Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model. ",
    "version": "1.2.1",
    "maintainer": "Ruey S. Tsay <ruey.tsay@chicagobooth.edu>",
    "author": "Ruey S. Tsay [aut, cre], David Wood [aut], Jon Lachmann [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MTS All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model.   "
  },
  {
    "id": 4923,
    "package_name": "MVNBayesian",
    "title": "Bayesian Analysis Framework for MVN (Mixture) Distribution",
    "description": "Tools of Bayesian analysis framework using the method\n  suggested by Berger (1985) <doi:10.1007/978-1-4757-4286-2> for\n  multivariate normal (MVN) distribution and multivariate normal\n  mixture (MixMVN) distribution:\n  a) calculating Bayesian posteriori of (Mix)MVN distribution;\n  b) generating random vectors of (Mix)MVN distribution;\n  c) Markov chain Monte Carlo (MCMC) for (Mix)MVN distribution.",
    "version": "0.0.8-11",
    "maintainer": "ZHANG Chen <447974102@qq.com>",
    "author": "ZHANG Chen",
    "url": "https://github.com/CubicZebra/MVNBayesian",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MVNBayesian",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MVNBayesian Bayesian Analysis Framework for MVN (Mixture) Distribution Tools of Bayesian analysis framework using the method\n  suggested by Berger (1985) <doi:10.1007/978-1-4757-4286-2> for\n  multivariate normal (MVN) distribution and multivariate normal\n  mixture (MixMVN) distribution:\n  a) calculating Bayesian posteriori of (Mix)MVN distribution;\n  b) generating random vectors of (Mix)MVN distribution;\n  c) Markov chain Monte Carlo (MCMC) for (Mix)MVN distribution.  "
  },
  {
    "id": 4963,
    "package_name": "MarginalMaxTest",
    "title": "Max-Type Test for Marginal Correlation with Bootstrap",
    "description": "Test the marginal correlation between a scalar response variable with a vector of explanatory variables \n    using the max-type test with bootstrap.\n    The test is based on the max-type statistic and its asymptotic distribution\n    under the null hypothesis of no marginal correlation.\n    The bootstrap procedure is used to approximate the null distribution of the test statistic.\n    The package provides a function for performing the test. For more technical details, refer to Zhang and Laber (2014) <doi:10.1080/01621459.2015.1106403>.",
    "version": "1.0.1",
    "maintainer": "Canyi Chen <cychen.stats@outlook.com>",
    "author": "Canyi Chen [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0673-5812>)",
    "url": "https://github.com/canyi-chen/MarginalMaxTest",
    "bug_reports": "https://github.com/canyi-chen/MarginalMaxTest/issues",
    "repository": "https://cran.r-project.org/package=MarginalMaxTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MarginalMaxTest Max-Type Test for Marginal Correlation with Bootstrap Test the marginal correlation between a scalar response variable with a vector of explanatory variables \n    using the max-type test with bootstrap.\n    The test is based on the max-type statistic and its asymptotic distribution\n    under the null hypothesis of no marginal correlation.\n    The bootstrap procedure is used to approximate the null distribution of the test statistic.\n    The package provides a function for performing the test. For more technical details, refer to Zhang and Laber (2014) <doi:10.1080/01621459.2015.1106403>.  "
  },
  {
    "id": 4980,
    "package_name": "MatrixExtra",
    "title": "Extra Methods for Sparse Matrices",
    "description": "Extends sparse matrix and vector classes from the 'Matrix' package by providing: \n  (a) Methods and operators that work natively on CSR formats (compressed sparse row, \n  a.k.a. 'RsparseMatrix') such as slicing/sub-setting, assignment, rbind(), \n  mathematical operators for CSR and COO such as addition (\"+\") or sqrt(), and methods such as diag(); \n  (b) Multi-threaded matrix multiplication and cross-product for many <sparse, dense> types, \n  including the 'float32' type from 'float'; \n  (c) Coercion methods between pairs of classes which are not present in 'Matrix', \n  such as 'dgCMatrix' -> 'ngRMatrix', as well as convenience conversion functions; \n  (d) Utility functions for sparse matrices such as sorting the indices or removing \n  zero-valued entries; \n  (e) Fast transposes that work by outputting in the opposite storage format;\n  (f) Faster replacements for many 'Matrix' methods for all sparse types, such as\n  slicing and elementwise multiplication.\n  (g) Convenience functions for sparse objects, such as 'mapSparse' or a shorter 'show' method.",
    "version": "0.1.15",
    "maintainer": "David Cortes <david.cortes.rivera@gmail.com>",
    "author": "David Cortes [aut, cre, cph],\n  Dmitry Selivanov [cph],\n  Thibaut Goetghebuer-Planchon [cph] (Copyright holder of included\n    robinmap library),\n  Martin Maechler [cph] (Copyright holder of 'Matrix' package from which\n    some code was taken),\n  Robert Gentleman [cph] (Copyright holder of mathematical functions used\n    by base R which were copied),\n  Ross Ihaka [cph] (Copyright holder of mathematical functions used by\n    base R which were copied)",
    "url": "https://github.com/david-cortes/MatrixExtra",
    "bug_reports": "https://github.com/david-cortes/MatrixExtra/issues",
    "repository": "https://cran.r-project.org/package=MatrixExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MatrixExtra Extra Methods for Sparse Matrices Extends sparse matrix and vector classes from the 'Matrix' package by providing: \n  (a) Methods and operators that work natively on CSR formats (compressed sparse row, \n  a.k.a. 'RsparseMatrix') such as slicing/sub-setting, assignment, rbind(), \n  mathematical operators for CSR and COO such as addition (\"+\") or sqrt(), and methods such as diag(); \n  (b) Multi-threaded matrix multiplication and cross-product for many <sparse, dense> types, \n  including the 'float32' type from 'float'; \n  (c) Coercion methods between pairs of classes which are not present in 'Matrix', \n  such as 'dgCMatrix' -> 'ngRMatrix', as well as convenience conversion functions; \n  (d) Utility functions for sparse matrices such as sorting the indices or removing \n  zero-valued entries; \n  (e) Fast transposes that work by outputting in the opposite storage format;\n  (f) Faster replacements for many 'Matrix' methods for all sparse types, such as\n  slicing and elementwise multiplication.\n  (g) Convenience functions for sparse objects, such as 'mapSparse' or a shorter 'show' method.  "
  },
  {
    "id": 4999,
    "package_name": "MedDietCalc",
    "title": "Multi Calculator to Compute Scores of Adherence to Mediterranean\nDiet",
    "description": "Multi Calculator of different scores to measure adherence to Mediterranean Diet, to compute them in nutriepidemiological data. Additionally, a sample dataset of this kind of data is provided, and some other minor tools useful in epidemiological studies.",
    "version": "0.1.1",
    "maintainer": "Miguel Menendez <miguelmo@gmail.com>",
    "author": "Miguel Menendez [aut, cre],\n  David Lora [ctb],\n  Agustin Gomez-Camara [dtc]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MedDietCalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MedDietCalc Multi Calculator to Compute Scores of Adherence to Mediterranean\nDiet Multi Calculator of different scores to measure adherence to Mediterranean Diet, to compute them in nutriepidemiological data. Additionally, a sample dataset of this kind of data is provided, and some other minor tools useful in epidemiological studies.  "
  },
  {
    "id": 5040,
    "package_name": "MexBrewer",
    "title": "Color Palettes Inspired by Works of Mexican Painters and\nMuralists",
    "description": "Color palettes inspired by the works of \n  Mexican painters and muralists. The package includes functions that\n  return vectors of colors and also functions to use color and fill\n  scales in 'ggplot2' visualizations.",
    "version": "0.0.2",
    "maintainer": "Antonio P\u00e1ez <paezha@mcmaster.ca>",
    "author": "Antonio P\u00e1ez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6912-9919>)",
    "url": "https://github.com/paezha/MexBrewer,\nhttps://paezha.github.io/MexBrewer/",
    "bug_reports": "https://github.com/paezha/MexBrewer/issues",
    "repository": "https://cran.r-project.org/package=MexBrewer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MexBrewer Color Palettes Inspired by Works of Mexican Painters and\nMuralists Color palettes inspired by the works of \n  Mexican painters and muralists. The package includes functions that\n  return vectors of colors and also functions to use color and fill\n  scales in 'ggplot2' visualizations.  "
  },
  {
    "id": 5092,
    "package_name": "MixtureMissing",
    "title": "Robust and Flexible Model-Based Clustering for Data Sets with\nMissing Values at Random",
    "description": "Implementations of various robust and flexible model-based clustering methods for data sets with missing values at random. \n    Two main models are: Multivariate Contaminated Normal Mixture (MCNM, Tong and Tortora, 2022, <doi:10.1007/s11634-021-00476-1>) and \n    Multivariate Generalized Hyperbolic Mixture (MGHM, Wei et al., 2019, <doi:10.1016/j.csda.2018.08.016>). Mixtures via some special or limiting\n    cases of the multivariate generalized hyperbolic distribution are also included: Normal-Inverse Gaussian, Symmetric Normal-Inverse Gaussian, \n    Skew-Cauchy, Cauchy, Skew-t, Student's t, Normal, Symmetric Generalized Hyperbolic, Hyperbolic Univariate Marginals, \n    Hyperbolic, and Symmetric Hyperbolic. Funding: This work was partially supported by the National Science foundation NSF Grant NO. 2209974.",
    "version": "3.0.5",
    "maintainer": "Hung Tong <hungtongmx@gmail.com>",
    "author": "Hung Tong [aut, cre],\n  Cristina Tortora [aut, ths, dgs]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MixtureMissing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MixtureMissing Robust and Flexible Model-Based Clustering for Data Sets with\nMissing Values at Random Implementations of various robust and flexible model-based clustering methods for data sets with missing values at random. \n    Two main models are: Multivariate Contaminated Normal Mixture (MCNM, Tong and Tortora, 2022, <doi:10.1007/s11634-021-00476-1>) and \n    Multivariate Generalized Hyperbolic Mixture (MGHM, Wei et al., 2019, <doi:10.1016/j.csda.2018.08.016>). Mixtures via some special or limiting\n    cases of the multivariate generalized hyperbolic distribution are also included: Normal-Inverse Gaussian, Symmetric Normal-Inverse Gaussian, \n    Skew-Cauchy, Cauchy, Skew-t, Student's t, Normal, Symmetric Generalized Hyperbolic, Hyperbolic Univariate Marginals, \n    Hyperbolic, and Symmetric Hyperbolic. Funding: This work was partially supported by the National Science foundation NSF Grant NO. 2209974.  "
  },
  {
    "id": 5102,
    "package_name": "ModTools",
    "title": "Building Regression and Classification Models",
    "description": "Consistent user interface to the most common regression and classification algorithms, such as random forest, neural networks, C5 trees and support vector machines, complemented with a handful of auxiliary functions, such as variable importance and a tuning function for the parameters.",
    "version": "0.9.13",
    "maintainer": "Andri Signorell <andri@signorell.net>",
    "author": "Andri Signorell [aut, cre],\n  Bernhard Compton [ctb],\n  Marcel Dettling [ctb],\n  Alexandre Hainard [ctb],\n  Max Kuhn [ctb],\n  Fr\u00e9d\u00e9rique Lisacek [ctb],\n  Michal Majka [ctb],\n  Markus M\u00fcller [ctb],\n  Dan Putler [ctb],\n  Jean-Charles Sanchez [ctb],\n  Natalia Tiberti [ctb],\n  Natacha Turck [ctb],\n  Jarek Tuszynski [ctb],\n  Robin Xavier [ctb],\n  Achim Zeileis [ctb]",
    "url": "https://andrisignorell.github.io/ModTools/,\nhttps://github.com/AndriSignorell/ModTools/",
    "bug_reports": "https://github.com/AndriSignorell/ModTools/issues",
    "repository": "https://cran.r-project.org/package=ModTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ModTools Building Regression and Classification Models Consistent user interface to the most common regression and classification algorithms, such as random forest, neural networks, C5 trees and support vector machines, complemented with a handful of auxiliary functions, such as variable importance and a tuning function for the parameters.  "
  },
  {
    "id": 5111,
    "package_name": "MomTrunc",
    "title": "Moments of Folded and Doubly Truncated Multivariate\nDistributions",
    "description": "It computes arbitrary products moments (mean vector and variance-covariance matrix), for some double truncated (and folded) multivariate distributions. These distributions belong to the family of selection elliptical distributions, which includes well known skewed distributions as the unified skew-t distribution (SUT) and its particular cases as the extended skew-t (EST), skew-t (ST) and the symmetric student-t (T) distribution. Analogous normal cases unified skew-normal (SUN), extended skew-normal (ESN), skew-normal (SN), and symmetric normal (N) are also included. Density, probabilities and random deviates are also offered for these members.",
    "version": "6.1",
    "maintainer": "Christian E. Galarza <cgalarza88@gmail.com>",
    "author": "Christian E. Galarza [aut, cre, trl] (ORCID:\n    <https://orcid.org/0000-0002-4818-6006>),\n  Raymond Kan [ctb] (ORCID: <https://orcid.org/0000-0002-0578-9974>),\n  Victor H. Lachos [aut, ths] (ORCID:\n    <https://orcid.org/0000-0002-7239-2459>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MomTrunc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MomTrunc Moments of Folded and Doubly Truncated Multivariate\nDistributions It computes arbitrary products moments (mean vector and variance-covariance matrix), for some double truncated (and folded) multivariate distributions. These distributions belong to the family of selection elliptical distributions, which includes well known skewed distributions as the unified skew-t distribution (SUT) and its particular cases as the extended skew-t (EST), skew-t (ST) and the symmetric student-t (T) distribution. Analogous normal cases unified skew-normal (SUN), extended skew-normal (ESN), skew-normal (SN), and symmetric normal (N) are also included. Density, probabilities and random deviates are also offered for these members.  "
  },
  {
    "id": 5141,
    "package_name": "MultNonParam",
    "title": "Multivariate Nonparametric Methods",
    "description": "A collection of multivariate nonparametric methods, selected in part to support an MS level course in nonparametric statistical methods. Methods include adjustments for multiple comparisons, implementation of multivariate Mann-Whitney-Wilcoxon testing, inversion of these tests to produce a confidence region, some permutation tests for linear models, and some algorithms for calculating exact probabilities associated with one- and two- stage testing involving Mann-Whitney-Wilcoxon statistics.  Supported by grant NSF DMS 1712839.  See Kolassa and Seifu (2013) <doi:10.1016/j.acra.2013.03.006>.",
    "version": "1.3.9",
    "maintainer": "John E. Kolassa <kolassa@stat.rutgers.edu>",
    "author": "John E. Kolassa [cre],\n  Stephane Jankowski [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultNonParam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultNonParam Multivariate Nonparametric Methods A collection of multivariate nonparametric methods, selected in part to support an MS level course in nonparametric statistical methods. Methods include adjustments for multiple comparisons, implementation of multivariate Mann-Whitney-Wilcoxon testing, inversion of these tests to produce a confidence region, some permutation tests for linear models, and some algorithms for calculating exact probabilities associated with one- and two- stage testing involving Mann-Whitney-Wilcoxon statistics.  Supported by grant NSF DMS 1712839.  See Kolassa and Seifu (2013) <doi:10.1016/j.acra.2013.03.006>.  "
  },
  {
    "id": 5149,
    "package_name": "MultiFit",
    "title": "Multiscale Fisher's Independence Test for Multivariate\nDependence",
    "description": "Test for independence of two random vectors, learn and report the dependency structure. For more information, see Gorsky, Shai and Li Ma, Multiscale Fisher's Independence Test for Multivariate Dependence, Biometrika, accepted, January 2022.",
    "version": "1.1.1",
    "maintainer": "S. Gorsky <sgorsky@umass.edu>",
    "author": "S. Gorsky, L. Ma",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiFit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiFit Multiscale Fisher's Independence Test for Multivariate\nDependence Test for independence of two random vectors, learn and report the dependency structure. For more information, see Gorsky, Shai and Li Ma, Multiscale Fisher's Independence Test for Multivariate Dependence, Biometrika, accepted, January 2022.  "
  },
  {
    "id": 5166,
    "package_name": "MultiStatM",
    "title": "Multivariate Statistical Methods",
    "description": "Algorithms to build set partitions and commutator matrices and their use in the \n    construction of multivariate d-Hermite polynomials;  estimation and derivation of theoretical  vector moments and vector\n    cumulants  of multivariate distributions; conversion formulae for multivariate moments and cumulants. Applications to\n    estimation and derivation of multivariate measures of skewness and kurtosis;  estimation and derivation of asymptotic \n    covariances for d-variate Hermite polynomials, multivariate moments and cumulants and measures of skewness and kurtosis.\n    The formulae implemented are discussed in Terdik (2021, ISBN:9783030813925), \"Multivariate Statistical Methods\".",
    "version": "2.0.0",
    "maintainer": "Emanuele Taufer <emanuele.taufer@unitn.it>",
    "author": "Gyorgy Terdik [aut],\n  Emanuele Taufer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiStatM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiStatM Multivariate Statistical Methods Algorithms to build set partitions and commutator matrices and their use in the \n    construction of multivariate d-Hermite polynomials;  estimation and derivation of theoretical  vector moments and vector\n    cumulants  of multivariate distributions; conversion formulae for multivariate moments and cumulants. Applications to\n    estimation and derivation of multivariate measures of skewness and kurtosis;  estimation and derivation of asymptotic \n    covariances for d-variate Hermite polynomials, multivariate moments and cumulants and measures of skewness and kurtosis.\n    The formulae implemented are discussed in Terdik (2021, ISBN:9783030813925), \"Multivariate Statistical Methods\".  "
  },
  {
    "id": 5174,
    "package_name": "MultiscaleDTM",
    "title": "Multi-Scale Geomorphometric Terrain Attributes",
    "description": "Calculates multi-scale geomorphometric terrain attributes from regularly gridded digital terrain models using a variable focal windows size (Ilich et al. (2023) <doi:10.1111/tgis.13067>).",
    "version": "1.0.1",
    "maintainer": "Alexander Ilich <ailich@usf.edu>",
    "author": "Alexander Ilich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1758-8499>),\n  Vincent Lecours [aut],\n  Benjamin Misiuk [aut],\n  Steven Murawski [aut]",
    "url": "https://ailich.github.io/MultiscaleDTM/,\nhttps://github.com/ailich/MultiscaleDTM",
    "bug_reports": "https://github.com/ailich/MultiscaleDTM/issues",
    "repository": "https://cran.r-project.org/package=MultiscaleDTM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiscaleDTM Multi-Scale Geomorphometric Terrain Attributes Calculates multi-scale geomorphometric terrain attributes from regularly gridded digital terrain models using a variable focal windows size (Ilich et al. (2023) <doi:10.1111/tgis.13067>).  "
  },
  {
    "id": 5186,
    "package_name": "NAC",
    "title": "Network-Adjusted Covariates for Community Detection",
    "description": "Incorporating node-level covariates for community detection has gained increasing attention these years. This package provides the function for implementing the novel community detection algorithm known as Network-Adjusted Covariates for Community Detection (NAC), which is designed to detect latent community structure in graphs with node-level information, i.e., covariates. This algorithm can handle models such as the degree-corrected stochastic block model (DCSBM) with covariates. NAC specifically addresses the discrepancy between the community structure inferred from the adjacency information and the community structure inferred from the covariates information. For more detailed information, please refer to the reference paper: Yaofang Hu and Wanjie Wang (2023) <arXiv:2306.15616>. In addition to NAC, this package includes several other existing community detection algorithms that are compared to NAC in the reference paper. These algorithms are Spectral Clustering On Ratios-of Eigenvectors (SCORE), network-based regularized spectral clustering (Net-based), covariate-based spectral clustering (Cov-based), covariate-assisted spectral clustering (CAclustering) and semidefinite programming (SDP). ",
    "version": "0.1.0",
    "maintainer": "Yaofang Hu <yaofangh@smu.edu>",
    "author": "Yaofang Hu [aut, cre],\n  Wanjie Wang [aut]",
    "url": "https://arxiv.org/abs/2306.15616",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NAC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NAC Network-Adjusted Covariates for Community Detection Incorporating node-level covariates for community detection has gained increasing attention these years. This package provides the function for implementing the novel community detection algorithm known as Network-Adjusted Covariates for Community Detection (NAC), which is designed to detect latent community structure in graphs with node-level information, i.e., covariates. This algorithm can handle models such as the degree-corrected stochastic block model (DCSBM) with covariates. NAC specifically addresses the discrepancy between the community structure inferred from the adjacency information and the community structure inferred from the covariates information. For more detailed information, please refer to the reference paper: Yaofang Hu and Wanjie Wang (2023) <arXiv:2306.15616>. In addition to NAC, this package includes several other existing community detection algorithms that are compared to NAC in the reference paper. These algorithms are Spectral Clustering On Ratios-of Eigenvectors (SCORE), network-based regularized spectral clustering (Net-based), covariate-based spectral clustering (Cov-based), covariate-assisted spectral clustering (CAclustering) and semidefinite programming (SDP).   "
  },
  {
    "id": 5286,
    "package_name": "NVAR",
    "title": "Nonlinear Vector Autoregression Models",
    "description": "Estimate nonlinear vector autoregression models (also known as the \n    next generation reservoir computing) for nonlinear dynamic systems. The \n    algorithm was described by Gauthier et al. (2021) <doi:10.1038/s41467-021-25801-2>.",
    "version": "0.1.0",
    "maintainer": "Jingmeng Cui <jingmeng.cui@outlook.com>",
    "author": "Jingmeng Cui [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3421-8457>)",
    "url": "https://github.com/Sciurus365/NVAR",
    "bug_reports": "https://github.com/Sciurus365/NVAR/issues",
    "repository": "https://cran.r-project.org/package=NVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NVAR Nonlinear Vector Autoregression Models Estimate nonlinear vector autoregression models (also known as the \n    next generation reservoir computing) for nonlinear dynamic systems. The \n    algorithm was described by Gauthier et al. (2021) <doi:10.1038/s41467-021-25801-2>.  "
  },
  {
    "id": 5311,
    "package_name": "NetMix",
    "title": "Dynamic Mixed-Membership Network Regression Model",
    "description": "Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,\n             incorporating node-level predictors of mixed-membership vectors, as well as \n             dyad-level predictors. For networks observed over time, the model defines a hidden\n             Markov process that allows the effects of node-level predictors to evolve in discrete,\n             historical periods. In addition, the package offers a variety of utilities for \n             exploring results of estimation, including tools for conducting posterior \n             predictive checks of goodness-of-fit and several plotting functions. The package \n             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic\n             Blockmodel Regression for Social Networks: Application to International Conflicts',\n             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.",
    "version": "0.2.0.3",
    "maintainer": "Santiago Olivella <olivella@unc.edu>",
    "author": "Santiago Olivella [aut, cre],\n  Adeline Lo [aut],\n  Tyler Pratt [aut],\n  Kosuke Imai [ctb]",
    "url": "",
    "bug_reports": "https://github.com/solivella/NetMix/issues",
    "repository": "https://cran.r-project.org/package=NetMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetMix Dynamic Mixed-Membership Network Regression Model Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,\n             incorporating node-level predictors of mixed-membership vectors, as well as \n             dyad-level predictors. For networks observed over time, the model defines a hidden\n             Markov process that allows the effects of node-level predictors to evolve in discrete,\n             historical periods. In addition, the package offers a variety of utilities for \n             exploring results of estimation, including tools for conducting posterior \n             predictive checks of goodness-of-fit and several plotting functions. The package \n             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic\n             Blockmodel Regression for Social Networks: Application to International Conflicts',\n             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.  "
  },
  {
    "id": 5317,
    "package_name": "NetVAR",
    "title": "Network Structures in VAR Models",
    "description": "Vector AutoRegressive (VAR) type models with tailored regularisation structures are provided to uncover network type structures in the data, such as influential time series (influencers). Currently the package implements the LISAR model from Zhang and Trimborn (2023) <doi:10.2139/ssrn.4619531>. The package automatically derives the required regularisation sequences and refines it during the estimation to provide the optimal model. The package allows for model optimisation under various loss functions such as Mean Squared Forecasting Error (MSFE), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). It provides a dedicated class, allowing for summary prints of the optimal model and a plotting function to conveniently analyse the optimal model via heatmaps.",
    "version": "0.1-2",
    "maintainer": "Simon Trimborn <trimborn.econometrics@gmail.com>",
    "author": "Simon Trimborn [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NetVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetVAR Network Structures in VAR Models Vector AutoRegressive (VAR) type models with tailored regularisation structures are provided to uncover network type structures in the data, such as influential time series (influencers). Currently the package implements the LISAR model from Zhang and Trimborn (2023) <doi:10.2139/ssrn.4619531>. The package automatically derives the required regularisation sequences and refines it during the estimation to provide the optimal model. The package allows for model optimisation under various loss functions such as Mean Squared Forecasting Error (MSFE), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). It provides a dedicated class, allowing for summary prints of the optimal model and a plotting function to conveniently analyse the optimal model via heatmaps.  "
  },
  {
    "id": 5342,
    "package_name": "Nmisc",
    "title": "Miscellaneous Functions Used at 'Numeract LLC'",
    "description": "Contains functions useful for debugging, set operations on vectors,\n    and 'UTC' date and time functionality. It adds a few vector manipulation \n    verbs to 'purrr' and 'dplyr' packages. It can also generate an R file to \n    install and update packages to simplify deployment into production. The \n    functions were developed at the data science firm 'Numeract LLC' and are \n    used in several packages and projects.",
    "version": "0.3.7",
    "maintainer": "Mike Badescu <mike.badescu@numeract.com>",
    "author": "Mike Badescu [aut, cre],\n  Ana-Maria Niculescu [aut],\n  Teodor Ciuraru [ctb],\n  Numeract LLC [cph]",
    "url": "https://github.com/numeract/Nmisc",
    "bug_reports": "https://github.com/numeract/Nmisc/issues",
    "repository": "https://cran.r-project.org/package=Nmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Nmisc Miscellaneous Functions Used at 'Numeract LLC' Contains functions useful for debugging, set operations on vectors,\n    and 'UTC' date and time functionality. It adds a few vector manipulation \n    verbs to 'purrr' and 'dplyr' packages. It can also generate an R file to \n    install and update packages to simplify deployment into production. The \n    functions were developed at the data science firm 'Numeract LLC' and are \n    used in several packages and projects.  "
  },
  {
    "id": 5434,
    "package_name": "OmicKriging",
    "title": "Poly-Omic Prediction of Complex TRaits",
    "description": "It provides functions to generate a correlation matrix\n    from a genetic dataset and to use this matrix to predict the phenotype of an\n    individual by using the phenotypes of the remaining individuals through\n    kriging. Kriging is a geostatistical method for optimal prediction or best\n    unbiased linear prediction. It consists of predicting the value of a\n    variable at an unobserved location as a weighted sum of the variable at\n    observed locations. Intuitively, it works as a reverse linear regression:\n    instead of computing correlation (univariate regression coefficients are\n    simply scaled correlation) between a dependent variable Y and independent\n    variables X, it uses known correlation between X and Y to predict Y.",
    "version": "1.4.0",
    "maintainer": "Hae Kyung Im <haky@uchicago.edu>",
    "author": "Hae Kyung Im, Heather E. Wheeler, Keston Aquino Michaels, Vassily\n    Trubetskoy",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OmicKriging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OmicKriging Poly-Omic Prediction of Complex TRaits It provides functions to generate a correlation matrix\n    from a genetic dataset and to use this matrix to predict the phenotype of an\n    individual by using the phenotypes of the remaining individuals through\n    kriging. Kriging is a geostatistical method for optimal prediction or best\n    unbiased linear prediction. It consists of predicting the value of a\n    variable at an unobserved location as a weighted sum of the variable at\n    observed locations. Intuitively, it works as a reverse linear regression:\n    instead of computing correlation (univariate regression coefficients are\n    simply scaled correlation) between a dependent variable Y and independent\n    variables X, it uses known correlation between X and Y to predict Y.  "
  },
  {
    "id": 5466,
    "package_name": "OpenLand",
    "title": "Quantitative Analysis and Visualization of LUCC",
    "description": "Tools for the analysis of land use and cover (LUC) time series. It \n    includes support for loading spatiotemporal raster data and synthesized \n    spatial plotting. Several LUC change (LUCC) metrics in regular or irregular \n    time intervals can be extracted and visualized through one- and multistep \n    sankey and chord diagrams. A complete intensity analysis according to \n    Aldwaik and Pontius (2012) <doi:10.1016/j.landurbplan.2012.02.010> is \n    implemented, including tools for the generation of standardized multilevel \n    output graphics.",
    "version": "1.0.3",
    "maintainer": "Reginal Exavier <reginalexavier@rocketmail.com>",
    "author": "Reginal Exavier [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5237-523X>),\n  Peter Zeilhofer [aut]",
    "url": "https://reginalexavier.github.io/OpenLand/,\nhttps://github.com/reginalexavier/OpenLand",
    "bug_reports": "https://github.com/reginalexavier/OpenLand/issues",
    "repository": "https://cran.r-project.org/package=OpenLand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpenLand Quantitative Analysis and Visualization of LUCC Tools for the analysis of land use and cover (LUC) time series. It \n    includes support for loading spatiotemporal raster data and synthesized \n    spatial plotting. Several LUC change (LUCC) metrics in regular or irregular \n    time intervals can be extracted and visualized through one- and multistep \n    sankey and chord diagrams. A complete intensity analysis according to \n    Aldwaik and Pontius (2012) <doi:10.1016/j.landurbplan.2012.02.010> is \n    implemented, including tools for the generation of standardized multilevel \n    output graphics.  "
  },
  {
    "id": 5474,
    "package_name": "OpenStreetMap",
    "title": "Access to Open Street Map Raster Images",
    "description": "Accesses high resolution raster maps using the OpenStreetMap\n    protocol. Dozens of road, satellite, and topographic map servers are directly\n    supported. Additionally raster maps\n    may be constructed using custom tile servers.  Maps can be\n    plotted using either base graphics, or ggplot2. This package is not affiliated\n    with the OpenStreetMap.org mapping project.",
    "version": "0.4.1",
    "maintainer": "Ian Fellows <ian@fellstat.com>",
    "author": "Ian Fellows [aut, cre],\n  Jan-Peter Stotz [aut]",
    "url": "https://github.com/ifellows/ROSM https://www.fellstat.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OpenStreetMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpenStreetMap Access to Open Street Map Raster Images Accesses high resolution raster maps using the OpenStreetMap\n    protocol. Dozens of road, satellite, and topographic map servers are directly\n    supported. Additionally raster maps\n    may be constructed using custom tile servers.  Maps can be\n    plotted using either base graphics, or ggplot2. This package is not affiliated\n    with the OpenStreetMap.org mapping project.  "
  },
  {
    "id": 5495,
    "package_name": "OrdMonReg",
    "title": "Compute least squares estimates of one bounded or two ordered\nisotonic regression curves",
    "description": "We consider the problem of estimating two isotonic regression curves g1* and g2* under the constraint that they are ordered, i.e. g1* <= g2*. Given two sets of n data points y_1, ..., y_n and z_1, ..., z_n that are observed at (the same) deterministic design points x_1, ..., x_n, the estimates are obtained by minimizing the Least Squares criterion L(a, b) = sum_{i=1}^n (y_i - a_i)^2 w1(x_i) + sum_{i=1}^n (z_i - b_i)^2 w2(x_i) over the class of pairs of vectors (a, b) such that a and b are isotonic and a_i <= b_i for all i = 1, ..., n. We offer two different approaches to compute the estimates: a projected subgradient algorithm where the projection is calculated using a PAVA as well as Dykstra's cyclical projection algorithm.",
    "version": "1.0.3",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "author": "Fadoua Balabdaoui, Kaspar Rufibach, Filippo Santambrogio",
    "url": "http://www.ceremade.dauphine.fr/~fadoua,\nhttp://www.kasparrufibach.ch,\nhttp://www.math.u-psud.fr/~santambr/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OrdMonReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OrdMonReg Compute least squares estimates of one bounded or two ordered\nisotonic regression curves We consider the problem of estimating two isotonic regression curves g1* and g2* under the constraint that they are ordered, i.e. g1* <= g2*. Given two sets of n data points y_1, ..., y_n and z_1, ..., z_n that are observed at (the same) deterministic design points x_1, ..., x_n, the estimates are obtained by minimizing the Least Squares criterion L(a, b) = sum_{i=1}^n (y_i - a_i)^2 w1(x_i) + sum_{i=1}^n (z_i - b_i)^2 w2(x_i) over the class of pairs of vectors (a, b) such that a and b are isotonic and a_i <= b_i for all i = 1, ..., n. We offer two different approaches to compute the estimates: a projected subgradient algorithm where the projection is calculated using a PAVA as well as Dykstra's cyclical projection algorithm.  "
  },
  {
    "id": 5506,
    "package_name": "OtsuSeg",
    "title": "Raster Thresholding Using Otsu\u00b4s Algorithm",
    "description": "Provides tools to process raster data and apply Otsu-based thresholding for burned area mapping and other image segmentation tasks. Implements the method described by Otsu (1979) <doi:10.1109/TSMC.1979.4310076>, a data-driven technique that determines an optimal threshold by maximizing the inter-class variance of pixel intensities. It includes validation functions to assess segmentation accuracy against reference data using standard accuracy metrics such as precision, recall, and F1-score.",
    "version": "0.1.0",
    "maintainer": "Olga Viedma <olga.viedma@uclm.es>",
    "author": "Hammadi, Achour [aut],\n  Olga Viedma [ctb, cre],\n  Zina Soltani [ctb],\n  Imene Habibi [ctb],\n  Wahbi Jaouadi [ctb]",
    "url": "https://github.com/olgaviedma/OtsuSeg",
    "bug_reports": "https://github.com/olgaviedma/OtsuSeg/issues",
    "repository": "https://cran.r-project.org/package=OtsuSeg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OtsuSeg Raster Thresholding Using Otsu\u00b4s Algorithm Provides tools to process raster data and apply Otsu-based thresholding for burned area mapping and other image segmentation tasks. Implements the method described by Otsu (1979) <doi:10.1109/TSMC.1979.4310076>, a data-driven technique that determines an optimal threshold by maximizing the inter-class variance of pixel intensities. It includes validation functions to assess segmentation accuracy against reference data using standard accuracy metrics such as precision, recall, and F1-score.  "
  },
  {
    "id": 5540,
    "package_name": "PBSmapping",
    "title": "Mapping Fisheries Data and Spatial Analysis Tools",
    "description": "This software has evolved from fisheries research conducted at the\n   Pacific Biological Station (PBS) in 'Nanaimo', British Columbia, Canada. It\n   extends the R language to include two-dimensional plotting features similar\n   to those commonly available in a Geographic Information System (GIS).\n   Embedded C code speeds algorithms from computational geometry, such as\n   finding polygons that contain specified point events or converting between\n   longitude-latitude and Universal Transverse Mercator (UTM) coordinates.\n   Additionally, we include 'C++' code developed by Angus Johnson for the\n   'Clipper' library, data for a global shoreline, and other data sets in the\n   public domain. Under the user's R library directory '.libPaths()',\n   specifically in './PBSmapping/doc', a complete user's guide is offered and\n   should be consulted to use package functions effectively.",
    "version": "2.74.1",
    "maintainer": "Rowan Haigh <rowan.haigh@dfo-mpo.gc.ca>",
    "author": "Jon T. Schnute [aut],\n  Nicholas Boers [aut],\n  Rowan Haigh [aut, cre],\n  Alex Couture-Beil [ctb],\n  Denis Chabot [ctb],\n  Chris Grandin [ctb],\n  Alan Murta [ctb],\n  Angus Johnson [ctb],\n  Paul Wessel [ctb],\n  Franklin Antonio [ctb],\n  Nicholas J. Lewin-Koh [ctb],\n  Roger Bivand [ctb],\n  Sean Anderson [ctb]",
    "url": "https://github.com/pbs-software/pbs-mapping,\nhttps://github.com/pbs-software/pbs-mapx,\nhttps://www.angusj.com/clipper2/Docs/Overview.htm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PBSmapping",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PBSmapping Mapping Fisheries Data and Spatial Analysis Tools This software has evolved from fisheries research conducted at the\n   Pacific Biological Station (PBS) in 'Nanaimo', British Columbia, Canada. It\n   extends the R language to include two-dimensional plotting features similar\n   to those commonly available in a Geographic Information System (GIS).\n   Embedded C code speeds algorithms from computational geometry, such as\n   finding polygons that contain specified point events or converting between\n   longitude-latitude and Universal Transverse Mercator (UTM) coordinates.\n   Additionally, we include 'C++' code developed by Angus Johnson for the\n   'Clipper' library, data for a global shoreline, and other data sets in the\n   public domain. Under the user's R library directory '.libPaths()',\n   specifically in './PBSmapping/doc', a complete user's guide is offered and\n   should be consulted to use package functions effectively.  "
  },
  {
    "id": 5548,
    "package_name": "PCBS",
    "title": "Principal Component BiSulfite",
    "description": "A system for fast, accurate, and flexible whole genome bisulfite sequencing (WGBS) data analysis of two-condition comparisons. Principal Component BiSulfite, 'PCBS', assigns methylated loci eigenvector values from the treatment-delineating principal component in lieu of running millions of pairwise statistical tests, which dramatically increases analysis flexibility and reduces computational requirements. Methods: <https://katlande.github.io/PCBS/articles/Differential_Methylation.html>. ",
    "version": "0.1.1",
    "maintainer": "Kathryn Lande <kathryn.lande@mail.mcgill.ca>",
    "author": "Kathryn Lande [aut, cre, cph]",
    "url": "https://github.com/katlande/PCBS",
    "bug_reports": "https://github.com/katlande/PCBS/issues",
    "repository": "https://cran.r-project.org/package=PCBS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCBS Principal Component BiSulfite A system for fast, accurate, and flexible whole genome bisulfite sequencing (WGBS) data analysis of two-condition comparisons. Principal Component BiSulfite, 'PCBS', assigns methylated loci eigenvector values from the treatment-delineating principal component in lieu of running millions of pairwise statistical tests, which dramatically increases analysis flexibility and reduces computational requirements. Methods: <https://katlande.github.io/PCBS/articles/Differential_Methylation.html>.   "
  },
  {
    "id": 5550,
    "package_name": "PCFAM",
    "title": "Computation of Ancestry Scores with Mixed Families and Unrelated\nIndividuals",
    "description": "We provide several algorithms to compute the genotype ancestry scores (such as eigenvector projections) in the case where highly correlated individuals are involved.",
    "version": "1.0",
    "maintainer": "Yi-Hui Zhou <yihui_zhou@ncsu.edu>",
    "author": "Yi-Hui Zhou",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PCFAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCFAM Computation of Ancestry Scores with Mixed Families and Unrelated\nIndividuals We provide several algorithms to compute the genotype ancestry scores (such as eigenvector projections) in the case where highly correlated individuals are involved.  "
  },
  {
    "id": 5586,
    "package_name": "PEtests",
    "title": "Power-Enhanced (PE) Tests for High-Dimensional Data",
    "description": "Two-sample power-enhanced mean tests, covariance tests, and simultaneous tests on mean vectors and covariance matrices for high-dimensional data. Methods of these PE tests are presented in \n    Yu, Li, and Xue (2022) <doi:10.1080/01621459.2022.2126781>; \n    Yu, Li, Xue, and Li (2022) <doi:10.1080/01621459.2022.2061354>.",
    "version": "0.1.0",
    "maintainer": "Xiufan Yu <xiufan.yu@nd.edu>",
    "author": "Xiufan Yu [aut, cre],\n  Danning Li [aut],\n  Lingzhou Xue [aut],\n  Runze Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PEtests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PEtests Power-Enhanced (PE) Tests for High-Dimensional Data Two-sample power-enhanced mean tests, covariance tests, and simultaneous tests on mean vectors and covariance matrices for high-dimensional data. Methods of these PE tests are presented in \n    Yu, Li, and Xue (2022) <doi:10.1080/01621459.2022.2126781>; \n    Yu, Li, Xue, and Li (2022) <doi:10.1080/01621459.2022.2061354>.  "
  },
  {
    "id": 5596,
    "package_name": "PHInfiniteEstimates",
    "title": "Tools for Inference in the Presence of a Monotone Likelihood",
    "description": "Proportional hazards estimation in the presence of a partially monotone likelihood has difficulties, in that finite estimators do not exist.  These difficulties are related to those arising from logistic and multinomial regression.  References for methods are given in the separate function documents.  Supported by grant NSF DMS 1712839.",
    "version": "2.9.5",
    "maintainer": "John E. Kolassa <kolassa@stat.rutgers.edu>",
    "author": "John E. Kolassa and Juan Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PHInfiniteEstimates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PHInfiniteEstimates Tools for Inference in the Presence of a Monotone Likelihood Proportional hazards estimation in the presence of a partially monotone likelihood has difficulties, in that finite estimators do not exist.  These difficulties are related to those arising from logistic and multinomial regression.  References for methods are given in the separate function documents.  Supported by grant NSF DMS 1712839.  "
  },
  {
    "id": 5671,
    "package_name": "PRIMAL",
    "title": "Parametric Simplex Method for Sparse Learning",
    "description": "Implements a unified framework of parametric simplex method for a variety of sparse learning problems (e.g., Dantzig selector (for linear regression), sparse quantile regression, sparse support vector machines, and compressive sensing) combined with efficient hyper-parameter selection strategies. The core algorithm is implemented in C++ with Eigen3 support for portable high performance linear algebra. For more details about parametric simplex method, see Haotian Pang (2017) <https://papers.nips.cc/paper/6623-parametric-simplex-method-for-sparse-learning.pdf>.",
    "version": "1.0.3",
    "maintainer": "Zichong Li <zichongli5@gmail.com>",
    "author": "Zichong Li [aut, cre],\n  Qianli Shen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PRIMAL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRIMAL Parametric Simplex Method for Sparse Learning Implements a unified framework of parametric simplex method for a variety of sparse learning problems (e.g., Dantzig selector (for linear regression), sparse quantile regression, sparse support vector machines, and compressive sensing) combined with efficient hyper-parameter selection strategies. The core algorithm is implemented in C++ with Eigen3 support for portable high performance linear algebra. For more details about parametric simplex method, see Haotian Pang (2017) <https://papers.nips.cc/paper/6623-parametric-simplex-method-for-sparse-learning.pdf>.  "
  },
  {
    "id": 5673,
    "package_name": "PRIMME",
    "title": "Eigenvalues and Singular Values and Vectors from Large Matrices",
    "description": "\n    R interface to 'PRIMME' <https://www.cs.wm.edu/~andreas/software/>, a C library for computing a few\n    eigenvalues and their corresponding eigenvectors of a real symmetric or complex\n    Hermitian matrix, or generalized Hermitian eigenproblem.  It can also compute\n    singular values and vectors of a square or rectangular matrix. 'PRIMME' finds\n    largest, smallest, or interior singular/eigenvalues and can use preconditioning\n    to accelerate convergence. General description of the methods are provided in the papers\n    Stathopoulos (2010, <doi:10.1145/1731022.1731031>) and Wu (2017, <doi:10.1137/16M1082214>).\n    See 'citation(\"PRIMME\")' for details.",
    "version": "3.2-6",
    "maintainer": "Eloy Romero <eloy@cs.wm.edu>",
    "author": "Eloy Romero [aut, cre],\n  Andreas Stathopoulos [aut],\n  Lingfei Wu [aut],\n  College of William & Mary [cph]",
    "url": "https://www.cs.wm.edu/~andreas/software/\nhttps://github.com/primme/primme",
    "bug_reports": "https://github.com/primme/primme/issues",
    "repository": "https://cran.r-project.org/package=PRIMME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRIMME Eigenvalues and Singular Values and Vectors from Large Matrices \n    R interface to 'PRIMME' <https://www.cs.wm.edu/~andreas/software/>, a C library for computing a few\n    eigenvalues and their corresponding eigenvectors of a real symmetric or complex\n    Hermitian matrix, or generalized Hermitian eigenproblem.  It can also compute\n    singular values and vectors of a square or rectangular matrix. 'PRIMME' finds\n    largest, smallest, or interior singular/eigenvalues and can use preconditioning\n    to accelerate convergence. General description of the methods are provided in the papers\n    Stathopoulos (2010, <doi:10.1145/1731022.1731031>) and Wu (2017, <doi:10.1137/16M1082214>).\n    See 'citation(\"PRIMME\")' for details.  "
  },
  {
    "id": 5690,
    "package_name": "PReMiuM",
    "title": "Dirichlet Process Bayesian Clustering, Profile Regression",
    "description": "Bayesian clustering using a Dirichlet process mixture model. This model is an alternative to regression models, non-parametrically linking a response vector to covariate data through cluster membership. The package allows Bernoulli, Binomial, Poisson, Normal, survival and categorical response, as well as Normal and discrete covariates. It also allows for fixed effects in the response model, where a spatial CAR (conditional autoregressive) term can be also included. Additionally, predictions may be made for the response, and missing values for the covariates are handled. Several samplers and label switching moves are implemented along with diagnostic tools to assess convergence. A number of R functions for post-processing of the output are also provided. In addition to fitting mixtures, it may additionally be of interest to determine which covariates actively drive the mixture components. This is implemented in the package as variable selection. The main reference for the package is Liverani, Hastie, Azizi, Papathomas and Richardson (2015) <doi:10.18637/jss.v064.i07>.",
    "version": "3.2.13",
    "maintainer": "Silvia Liverani <liveranis@gmail.com>",
    "author": "David I. Hastie, Silvia Liverani <liveranis@gmail.com> and Sylvia Richardson with contributions from Aurore J. Lavigne, Lucy Leigh, Lamiae Azizi, Xi Liu, Ruizhu Huang, Austin Gratton, Wei Jing",
    "url": "https://www.silvialiverani.com/software/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PReMiuM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PReMiuM Dirichlet Process Bayesian Clustering, Profile Regression Bayesian clustering using a Dirichlet process mixture model. This model is an alternative to regression models, non-parametrically linking a response vector to covariate data through cluster membership. The package allows Bernoulli, Binomial, Poisson, Normal, survival and categorical response, as well as Normal and discrete covariates. It also allows for fixed effects in the response model, where a spatial CAR (conditional autoregressive) term can be also included. Additionally, predictions may be made for the response, and missing values for the covariates are handled. Several samplers and label switching moves are implemented along with diagnostic tools to assess convergence. A number of R functions for post-processing of the output are also provided. In addition to fitting mixtures, it may additionally be of interest to determine which covariates actively drive the mixture components. This is implemented in the package as variable selection. The main reference for the package is Liverani, Hastie, Azizi, Papathomas and Richardson (2015) <doi:10.18637/jss.v064.i07>.  "
  },
  {
    "id": 5696,
    "package_name": "PSF",
    "title": "Forecasting of Univariate Time Series Using the Pattern\nSequence-Based Forecasting (PSF) Algorithm",
    "description": "Pattern Sequence Based Forecasting (PSF) takes univariate\n    time series data as input and assist to forecast its future values.\n    This algorithm forecasts the behavior of time series\n    based on similarity of pattern sequences. Initially, clustering is done with the\n    labeling of samples from database. The labels associated with samples are then\n    used for forecasting the future behaviour of time series data. The further\n    technical details and references regarding PSF are discussed in Vignette.",
    "version": "0.5",
    "maintainer": "Neeraj Bokde <neerajdhanraj@gmail.com>",
    "author": "Neeraj Bokde, Gualberto Asencio-Cortes and Francisco Martinez-Alvarez",
    "url": "https://www.neerajbokde.in/viggnette/2021-10-13-PSF/",
    "bug_reports": "https://github.com/neerajdhanraj/PSF/issues",
    "repository": "https://cran.r-project.org/package=PSF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PSF Forecasting of Univariate Time Series Using the Pattern\nSequence-Based Forecasting (PSF) Algorithm Pattern Sequence Based Forecasting (PSF) takes univariate\n    time series data as input and assist to forecast its future values.\n    This algorithm forecasts the behavior of time series\n    based on similarity of pattern sequences. Initially, clustering is done with the\n    labeling of samples from database. The labels associated with samples are then\n    used for forecasting the future behaviour of time series data. The further\n    technical details and references regarding PSF are discussed in Vignette.  "
  },
  {
    "id": 5728,
    "package_name": "PVR",
    "title": "Phylogenetic Eigenvectors Regression and Phylogentic\nSignal-Representation Curve",
    "description": "Estimates (and controls for) phylogenetic signal through phylogenetic eigenvectors regression (PVR) and phylogenetic signal-representation (PSR) curve, along with some plot utilities.",
    "version": "0.3",
    "maintainer": "Thiago Santos <thiago.santos@ufvjm.edu.br>",
    "author": "Thiago Santos",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PVR Phylogenetic Eigenvectors Regression and Phylogentic\nSignal-Representation Curve Estimates (and controls for) phylogenetic signal through phylogenetic eigenvectors regression (PVR) and phylogenetic signal-representation (PSR) curve, along with some plot utilities.  "
  },
  {
    "id": 5736,
    "package_name": "Pade",
    "title": "Pad\u00e9 Approximant Coefficients",
    "description": "Given a vector of Taylor series coefficients of sufficient length\n    as input, the function returns the numerator and denominator coefficients\n    for the Pad\u00e9 approximant of appropriate order (Baker, 1975)\n    <ISBN:9780120748556>.",
    "version": "1.0.8",
    "maintainer": "Avraham Adler <Avraham.Adler@gmail.com>",
    "author": "Avraham Adler [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3039-0703>)",
    "url": "https://github.com/aadler/Pade",
    "bug_reports": "https://github.com/aadler/Pade/issues",
    "repository": "https://cran.r-project.org/package=Pade",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Pade Pad\u00e9 Approximant Coefficients Given a vector of Taylor series coefficients of sufficient length\n    as input, the function returns the numerator and denominator coefficients\n    for the Pad\u00e9 approximant of appropriate order (Baker, 1975)\n    <ISBN:9780120748556>.  "
  },
  {
    "id": 5841,
    "package_name": "PlanetNICFI",
    "title": "Processing of the 'Planet NICFI' Satellite Imagery",
    "description": "It includes functions to download and process the 'Planet NICFI' (Norway's International Climate and Forest Initiative) Satellite Imagery utilizing the Planet Mosaics API <https://developers.planet.com/docs/basemaps/reference/#tag/Basemaps-and-Mosaics>. 'GDAL' (library for raster and vector geospatial data formats) and 'aria2c' (paralleled download utility) must be installed and configured in the user's Operating System.",
    "version": "1.0.5",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>),\n  Planet Labs Inc [cph]",
    "url": "https://github.com/mlampros/PlanetNICFI",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PlanetNICFI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PlanetNICFI Processing of the 'Planet NICFI' Satellite Imagery It includes functions to download and process the 'Planet NICFI' (Norway's International Climate and Forest Initiative) Satellite Imagery utilizing the Planet Mosaics API <https://developers.planet.com/docs/basemaps/reference/#tag/Basemaps-and-Mosaics>. 'GDAL' (library for raster and vector geospatial data formats) and 'aria2c' (paralleled download utility) must be installed and configured in the user's Operating System.  "
  },
  {
    "id": 5846,
    "package_name": "PlayerChart",
    "title": "Generate Pizza Chart: Player Stats 0-100",
    "description": "Create an interactive pizza chart visualizing a specific player's statistics across various attributes in a sports dataset. The chart is constructed based on input parameters: 'data', a dataframe containing player data for any sports; 'player_stats_col', a vector specifying the names of the columns from the dataframe that will be used to create slices in the pizza chart, with statistics ranging between 0 and 100; 'name_col', specifying the name of the column in the dataframe that contains the player names; and 'player_name', representing the specific player whose statistics will be visualized in the chart, serving as the chart title.",
    "version": "1.0.0",
    "maintainer": "Amal Panwar <panwar.amal1995@gmail.com>",
    "author": "Amal Panwar [aut, cre] (ORCID: <https://orcid.org/0009-0003-9046-9207>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PlayerChart",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PlayerChart Generate Pizza Chart: Player Stats 0-100 Create an interactive pizza chart visualizing a specific player's statistics across various attributes in a sports dataset. The chart is constructed based on input parameters: 'data', a dataframe containing player data for any sports; 'player_stats_col', a vector specifying the names of the columns from the dataframe that will be used to create slices in the pizza chart, with statistics ranging between 0 and 100; 'name_col', specifying the name of the column in the dataframe that contains the player names; and 'player_name', representing the specific player whose statistics will be visualized in the chart, serving as the chart title.  "
  },
  {
    "id": 5879,
    "package_name": "PolyTree",
    "title": "Estimate Causal Polytree from Data",
    "description": "Given a data matrix with rows representing data vectors and columns representing variables, produces a directed polytree for the underlying causal structure. Based on the algorithm developed in Chatterjee and Vidyasagar (2022) <arxiv:2209.07028>. The method is fully nonparametric, making no use of linearity assumptions, and especially useful when the number of variables is large.",
    "version": "0.0.1",
    "maintainer": "Sourav Chatterjee <souravc@stanford.edu>",
    "author": "Sourav Chatterjee [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4460-209X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PolyTree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PolyTree Estimate Causal Polytree from Data Given a data matrix with rows representing data vectors and columns representing variables, produces a directed polytree for the underlying causal structure. Based on the algorithm developed in Chatterjee and Vidyasagar (2022) <arxiv:2209.07028>. The method is fully nonparametric, making no use of linearity assumptions, and especially useful when the number of variables is large.  "
  },
  {
    "id": 5885,
    "package_name": "PoolTestR",
    "title": "Prevalence and Regression for Pool-Tested (Group-Tested) Data",
    "description": "An easy-to-use tool for working with presence/absence tests\n    on 'pooled' or 'grouped' samples. The primary application is for\n    estimating prevalence of a marker in a population based on the results\n    of tests on pooled specimens.  This sampling method is often employed\n    in surveillance of rare conditions in humans or animals (e.g.\n    molecular xenomonitoring). The package was initially conceived as an\n    R-based alternative to the molecular xenomonitoring software,\n    'PoolScreen' <https://sites.uab.edu/statgenetics/software/>. However,\n    it goes further, allowing for estimates of prevalence to be adjusted\n    for hierarchical sampling frames, and perform flexible mixed-effect\n    regression analyses (McLure et al. Environmental Modelling and\n    Software.  <DOI:10.1016/j.envsoft.2021.105158>). The package is\n    currently in early stages, however more features are planned or in the\n    works: e.g. adjustments for imperfect test specificity/sensitivity,\n    functions for helping with optimal experimental design, and functions\n    for spatial modelling.",
    "version": "0.2.0",
    "maintainer": "Angus McLure <angus.mclure@anu.edu.au>",
    "author": "Angus McLure [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2551-3059>),\n  Caitlin Cherryh [ctb] (ORCID: <https://orcid.org/0000-0001-6146-4376>)",
    "url": "https://github.com/AngusMcLure/PoolTestR",
    "bug_reports": "https://github.com/AngusMcLure/PoolTestR/issues",
    "repository": "https://cran.r-project.org/package=PoolTestR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PoolTestR Prevalence and Regression for Pool-Tested (Group-Tested) Data An easy-to-use tool for working with presence/absence tests\n    on 'pooled' or 'grouped' samples. The primary application is for\n    estimating prevalence of a marker in a population based on the results\n    of tests on pooled specimens.  This sampling method is often employed\n    in surveillance of rare conditions in humans or animals (e.g.\n    molecular xenomonitoring). The package was initially conceived as an\n    R-based alternative to the molecular xenomonitoring software,\n    'PoolScreen' <https://sites.uab.edu/statgenetics/software/>. However,\n    it goes further, allowing for estimates of prevalence to be adjusted\n    for hierarchical sampling frames, and perform flexible mixed-effect\n    regression analyses (McLure et al. Environmental Modelling and\n    Software.  <DOI:10.1016/j.envsoft.2021.105158>). The package is\n    currently in early stages, however more features are planned or in the\n    works: e.g. adjustments for imperfect test specificity/sensitivity,\n    functions for helping with optimal experimental design, and functions\n    for spatial modelling.  "
  },
  {
    "id": 5939,
    "package_name": "ProbitSpatial",
    "title": "Probit with Spatial Dependence, SAR, SEM and SARAR Models",
    "description": "Fast estimation of binomial spatial probit regression models with spatial autocorrelation for big datasets.",
    "version": "1.1",
    "maintainer": "Davide Martinetti <davide.martinetti@inrae.fr>",
    "author": "Davide Martinetti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2047-1793>),\n  Ghislain Geniaux [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ProbitSpatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProbitSpatial Probit with Spatial Dependence, SAR, SEM and SARAR Models Fast estimation of binomial spatial probit regression models with spatial autocorrelation for big datasets.  "
  },
  {
    "id": 5940,
    "package_name": "ProcData",
    "title": "Process Data Analysis",
    "description": "Provides tools for exploratory process data analysis. Process data refers to the data describing\n    participants' problem-solving processes in computer-based assessments. It is often recorded in computer\n    log files. This package provides functions to read, process, and write process data. It also implements\n    two feature extraction methods to compress the information stored in process data into standard \n    numerical vectors. This package also provides recurrent neural network based models that relate response processes \n    with other binary or scale variables of interest. The functions that involve training and evaluating neural networks \n    are wrappers of functions in 'keras'.",
    "version": "0.3.2",
    "maintainer": "Xueying Tang <xueyingtang1989@gmail.com>",
    "author": "Xueying Tang [aut, cre],\n  Susu Zhang [aut],\n  Zhi Wang [aut],\n  Jingchen Liu [aut],\n  Zhiliang Ying [aut]",
    "url": "",
    "bug_reports": "https://github.com/xytangtang/ProcData/issues",
    "repository": "https://cran.r-project.org/package=ProcData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProcData Process Data Analysis Provides tools for exploratory process data analysis. Process data refers to the data describing\n    participants' problem-solving processes in computer-based assessments. It is often recorded in computer\n    log files. This package provides functions to read, process, and write process data. It also implements\n    two feature extraction methods to compress the information stored in process data into standard \n    numerical vectors. This package also provides recurrent neural network based models that relate response processes \n    with other binary or scale variables of interest. The functions that involve training and evaluating neural networks \n    are wrappers of functions in 'keras'.  "
  },
  {
    "id": 5952,
    "package_name": "PropClust",
    "title": "Propensity Clustering and Decomposition",
    "description": "Implementation of propensity clustering and\n        decomposition as described in Ranola et al. (2013) <doi:10.1186/1752-0509-7-21>. \n        Propensity decomposition can be viewed on the\n        one hand as a generalization of the eigenvector-based\n        approximation of correlation networks, and on the other hand as\n        a generalization of random multigraph models and\n        conformity-based decompositions.",
    "version": "1.4-7",
    "maintainer": "Peter Langfelder <Peter.Langfelder@gmail.com>",
    "author": "John Michael O Ranola, Kenneth Lange, Steve Horvath, Peter Langfelder",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PropClust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PropClust Propensity Clustering and Decomposition Implementation of propensity clustering and\n        decomposition as described in Ranola et al. (2013) <doi:10.1186/1752-0509-7-21>. \n        Propensity decomposition can be viewed on the\n        one hand as a generalization of the eigenvector-based\n        approximation of correlation networks, and on the other hand as\n        a generalization of random multigraph models and\n        conformity-based decompositions.  "
  },
  {
    "id": 5960,
    "package_name": "PsychWordVec",
    "title": "Word Embedding Research Framework for Psychological Science",
    "description": "\n    An integrative toolbox of word embedding research that provides:\n    (1) a collection of 'pre-trained' static word vectors in the '.RData'\n    compressed format <https://psychbruce.github.io/WordVector_RData.pdf>;\n    (2) a group of functions to process, analyze, and visualize word vectors;\n    (3) a range of tests to examine conceptual associations, including\n    the Word Embedding Association Test <doi:10.1126/science.aal4230>\n    and the Relative Norm Distance <doi:10.1073/pnas.1720347115>,\n    with permutation test of significance; and\n    (4) a set of training methods to locally train (static) word vectors\n    from text corpora, including 'Word2Vec' <doi:10.48550/arXiv.1301.3781>,\n    'GloVe' <doi:10.3115/v1/D14-1162>, and 'FastText' <doi:10.48550/arXiv.1607.04606>.",
    "version": "2025.11",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/PsychWordVec/",
    "bug_reports": "https://github.com/psychbruce/PsychWordVec/issues",
    "repository": "https://cran.r-project.org/package=PsychWordVec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PsychWordVec Word Embedding Research Framework for Psychological Science \n    An integrative toolbox of word embedding research that provides:\n    (1) a collection of 'pre-trained' static word vectors in the '.RData'\n    compressed format <https://psychbruce.github.io/WordVector_RData.pdf>;\n    (2) a group of functions to process, analyze, and visualize word vectors;\n    (3) a range of tests to examine conceptual associations, including\n    the Word Embedding Association Test <doi:10.1126/science.aal4230>\n    and the Relative Norm Distance <doi:10.1073/pnas.1720347115>,\n    with permutation test of significance; and\n    (4) a set of training methods to locally train (static) word vectors\n    from text corpora, including 'Word2Vec' <doi:10.48550/arXiv.1301.3781>,\n    'GloVe' <doi:10.3115/v1/D14-1162>, and 'FastText' <doi:10.48550/arXiv.1607.04606>.  "
  },
  {
    "id": 5982,
    "package_name": "QBMS",
    "title": "Query the Breeding Management System(s)",
    "description": "This R package assists breeders in linking data systems with their analytic pipelines, \n    a crucial step in digitizing breeding processes. It supports querying and retrieving \n    phenotypic and genotypic data from systems like 'EBS' <https://ebs.excellenceinbreeding.org/>, \n    'BMS' <https://bmspro.io>, 'BreedBase' <https://breedbase.org>,\n    'GIGWA' <https://github.com/SouthGreenPlatform/Gigwa2> (using 'BrAPI' <https://brapi.org> calls),\n    , and 'Germinate' <https://germinateplatform.github.io/get-germinate/>. \n    Extra helper functions support environmental data sources, including \n    'TerraClimate' <https://www.climatologylab.org/terraclimate.html> and 'FAO' \n    'HWSDv2' <https://gaez.fao.org/pages/hwsd> soil database. ",
    "version": "2.0.0",
    "maintainer": "Khaled Al-Shamaa <k.el-shamaa@cgiar.org>",
    "author": "Khaled Al-Shamaa [aut, cre],\n  Mariano Omar Crimi [ctb],\n  Zakaria Kehel [ctb],\n  Johan Aparicio [ctb],\n  ICARDA [cph]",
    "url": "https://icarda.github.io/QBMS/",
    "bug_reports": "https://github.com/icarda/QBMS/issues",
    "repository": "https://cran.r-project.org/package=QBMS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QBMS Query the Breeding Management System(s) This R package assists breeders in linking data systems with their analytic pipelines, \n    a crucial step in digitizing breeding processes. It supports querying and retrieving \n    phenotypic and genotypic data from systems like 'EBS' <https://ebs.excellenceinbreeding.org/>, \n    'BMS' <https://bmspro.io>, 'BreedBase' <https://breedbase.org>,\n    'GIGWA' <https://github.com/SouthGreenPlatform/Gigwa2> (using 'BrAPI' <https://brapi.org> calls),\n    , and 'Germinate' <https://germinateplatform.github.io/get-germinate/>. \n    Extra helper functions support environmental data sources, including \n    'TerraClimate' <https://www.climatologylab.org/terraclimate.html> and 'FAO' \n    'HWSDv2' <https://gaez.fao.org/pages/hwsd> soil database.   "
  },
  {
    "id": 6002,
    "package_name": "QQreflimits",
    "title": "Reference Limits using QQ Methodology",
    "description": "A collection of routines for finding reference limits using, where \n    appropriate, QQ methodology.  All use a data vector X of cases from the \n    reference population. The default is to get the central 95% reference range \n    of the population, namely the 2.5 and 97.5 percentile, with optional \n    adjustment of the range.  Along with the reference limits, we want \n    confidence intervals which, for historical reasons, are typically at 90% \n    confidence.  A full analysis provides six numbers:\n      \u2013 the upper and the lower reference limits, and \n      - each of their confidence intervals.\n    For application details, see Hawkins and Esquivel (2024) \n    <doi:10.1093/jalm/jfad109>.",
    "version": "1.0.3",
    "maintainer": "Jessica J. Kraker <krakerjj@uwec.edu>",
    "author": "Douglas M. Hawkins [aut],\n  Jessica J. Kraker [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QQreflimits",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QQreflimits Reference Limits using QQ Methodology A collection of routines for finding reference limits using, where \n    appropriate, QQ methodology.  All use a data vector X of cases from the \n    reference population. The default is to get the central 95% reference range \n    of the population, namely the 2.5 and 97.5 percentile, with optional \n    adjustment of the range.  Along with the reference limits, we want \n    confidence intervals which, for historical reasons, are typically at 90% \n    confidence.  A full analysis provides six numbers:\n      \u2013 the upper and the lower reference limits, and \n      - each of their confidence intervals.\n    For application details, see Hawkins and Esquivel (2024) \n    <doi:10.1093/jalm/jfad109>.  "
  },
  {
    "id": 6005,
    "package_name": "QRAGadget",
    "title": "A 'Shiny' Gadget for Interactive 'QRA' Visualizations",
    "description": "Upload raster data and easily create interactive quantitative risk analysis 'QRA' visualizations. Select\n    from numerous color palettes, base-maps, and different configurations.",
    "version": "0.3.0",
    "maintainer": "Paul Govan <pgovan1@aggienetwork.com>",
    "author": "Paul Govan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1821-8492>)",
    "url": "https://github.com/paulgovan/qragadget,\nhttp://paulgovan.github.io/QRAGadget/",
    "bug_reports": "https://github.com/paulgovan/qragadget/issues",
    "repository": "https://cran.r-project.org/package=QRAGadget",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QRAGadget A 'Shiny' Gadget for Interactive 'QRA' Visualizations Upload raster data and easily create interactive quantitative risk analysis 'QRA' visualizations. Select\n    from numerous color palettes, base-maps, and different configurations.  "
  },
  {
    "id": 6018,
    "package_name": "QZ",
    "title": "Generalized Eigenvalues and QZ Decomposition",
    "description": "Generalized eigenvalues and eigenvectors\n        use QZ decomposition (generalized Schur decomposition).\n        The decomposition needs an N-by-N non-symmetric\n        matrix A or paired matrices (A,B) with eigenvalues reordering\n        mechanism. The decomposition functions are mainly based Fortran\n        subroutines in complex*16 and double precision of LAPACK\n        library (version 3.10.0 or later).",
    "version": "0.2-4",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "author": "Wei-Chen Chen [aut, cre],\n  LAPACK authors [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QZ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QZ Generalized Eigenvalues and QZ Decomposition Generalized eigenvalues and eigenvectors\n        use QZ decomposition (generalized Schur decomposition).\n        The decomposition needs an N-by-N non-symmetric\n        matrix A or paired matrices (A,B) with eigenvalues reordering\n        mechanism. The decomposition functions are mainly based Fortran\n        subroutines in complex*16 and double precision of LAPACK\n        library (version 3.10.0 or later).  "
  },
  {
    "id": 6087,
    "package_name": "RAGFlowChainR",
    "title": "Retrieval-Augmented Generation (RAG) Workflows in R with Local\nand Web Search",
    "description": "Enables Retrieval-Augmented Generation (RAG) workflows in R by combining\n    local vector search using 'DuckDB' with optional web search via the 'Tavily' API.\n    Supports 'OpenAI'- and 'Ollama'-compatible embedding models, full-text and 'HNSW'\n    (Hierarchical Navigable Small World) indexing, and modular large language model\n    (LLM) invocation. Designed for advanced question-answering, chat-based\n    applications, and production-ready AI pipelines. This package is the R\n    equivalent of the 'python' package 'RAGFlowChain' available at\n    <https://pypi.org/project/RAGFlowChain/>.",
    "version": "0.1.5",
    "maintainer": "Kwadwo Daddy Nyame Owusu Boakye <kwadwo.owusuboakye@outlook.com>",
    "author": "Kwadwo Daddy Nyame Owusu Boakye [aut, cre]",
    "url": "https://github.com/knowusuboaky/RAGFlowChainR,\nhttps://knowusuboaky.github.io/RAGFlowChainR/",
    "bug_reports": "https://github.com/knowusuboaky/RAGFlowChainR/issues",
    "repository": "https://cran.r-project.org/package=RAGFlowChainR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAGFlowChainR Retrieval-Augmented Generation (RAG) Workflows in R with Local\nand Web Search Enables Retrieval-Augmented Generation (RAG) workflows in R by combining\n    local vector search using 'DuckDB' with optional web search via the 'Tavily' API.\n    Supports 'OpenAI'- and 'Ollama'-compatible embedding models, full-text and 'HNSW'\n    (Hierarchical Navigable Small World) indexing, and modular large language model\n    (LLM) invocation. Designed for advanced question-answering, chat-based\n    applications, and production-ready AI pipelines. This package is the R\n    equivalent of the 'python' package 'RAGFlowChain' available at\n    <https://pypi.org/project/RAGFlowChain/>.  "
  },
  {
    "id": 6091,
    "package_name": "RAMpath",
    "title": "Structural Equation Modeling Using the Reticular Action Model\n(RAM) Notation",
    "description": "We rewrite of RAMpath software developed by John McArdle and Steven Boker as an R package. In addition to performing regular SEM analysis through the R package lavaan, RAMpath has unique features.  First, it can generate path diagrams according to a given model. Second, it can display path tracing rules through path diagrams and decompose total effects into their respective direct and indirect effects as well as decompose variance and covariance into individual bridges. Furthermore, RAMpath can fit dynamic system models automatically based on latent change scores and generate vector field plots based upon results obtained from a bivariate dynamic system. Starting version 0.4, RAMpath can conduct power analysis for both univariate and bivariate latent change score models.",
    "version": "0.5.1",
    "maintainer": "Zhiyong Zhang <zzhang4@nd.edu>",
    "author": "Zhiyong Zhang, Jack McArdle, Aki Hamagami, & Kevin Grimm",
    "url": "https://nd.psychstat.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RAMpath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAMpath Structural Equation Modeling Using the Reticular Action Model\n(RAM) Notation We rewrite of RAMpath software developed by John McArdle and Steven Boker as an R package. In addition to performing regular SEM analysis through the R package lavaan, RAMpath has unique features.  First, it can generate path diagrams according to a given model. Second, it can display path tracing rules through path diagrams and decompose total effects into their respective direct and indirect effects as well as decompose variance and covariance into individual bridges. Furthermore, RAMpath can fit dynamic system models automatically based on latent change scores and generate vector field plots based upon results obtained from a bivariate dynamic system. Starting version 0.4, RAMpath can conduct power analysis for both univariate and bivariate latent change score models.  "
  },
  {
    "id": 6209,
    "package_name": "RGENERATE",
    "title": "Tools to Generate Vector Time Series",
    "description": "A method 'generate()' is implemented in this package for the random\n    generation of vector time series according to models obtained by 'RMAWGEN',\n    'vars' or other packages.  This package was created to generalize the\n    algorithms of the 'RMAWGEN' package for the analysis and generation of any\n    environmental vector time series.",
    "version": "1.3.8",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-3508-5898>)",
    "url": "https://github.com/ecor/RGENERATE",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RGENERATE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RGENERATE Tools to Generate Vector Time Series A method 'generate()' is implemented in this package for the random\n    generation of vector time series according to models obtained by 'RMAWGEN',\n    'vars' or other packages.  This package was created to generalize the\n    algorithms of the 'RMAWGEN' package for the analysis and generation of any\n    environmental vector time series.  "
  },
  {
    "id": 6231,
    "package_name": "RHclust",
    "title": "Vector in Partition",
    "description": "Non-parametric clustering of joint pattern multi-genetic/epigenetic factors. This package contains functions designed to cluster subjects based on gene features including single nucleotide polymorphisms (SNPs), DNA methylation (CPG), gene expression (GE), and covariate data. The novel concept follows the general K-means (Hartigan and Wong (1979) <doi:10.2307/2346830> framework but uses weighted Euclidean distances across the gene features to cluster subjects. This approach is unique in that it attempts to capture all pairwise interactions in an effort to cluster based on their complex biological interactions.",
    "version": "2.0.0",
    "maintainer": "Joseph Handwerker <jkhndwrk@memphis.edu>",
    "author": "Joseph Handwerker [aut, cre],\n  Lauren Sobral [ctb],\n  Meredith Ray [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RHclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RHclust Vector in Partition Non-parametric clustering of joint pattern multi-genetic/epigenetic factors. This package contains functions designed to cluster subjects based on gene features including single nucleotide polymorphisms (SNPs), DNA methylation (CPG), gene expression (GE), and covariate data. The novel concept follows the general K-means (Hartigan and Wong (1979) <doi:10.2307/2346830> framework but uses weighted Euclidean distances across the gene features to cluster subjects. This approach is unique in that it attempts to capture all pairwise interactions in an effort to cluster based on their complex biological interactions.  "
  },
  {
    "id": 6234,
    "package_name": "RIA",
    "title": "Radiomics Image Analysis Toolbox for Medial Images",
    "description": "Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI,\n             nrrd and npy (numpy array) file formats.\n             RIA calculates first-order, gray level co-occurrence matrix, gray level run length matrix and\n             geometry-based statistics. Almost all calculations are done using vectorized formulas to\n             optimize run speeds. Calculation of several thousands of parameters only takes minutes\n             on a single core of a conventional PC. Detailed methodology has been published: Kolossvary\n             et al. Circ: Cardiovascular Imaging. 2017;10(12):e006843 <doi: 10.1161/CIRCIMAGING.117.006843>.",
    "version": "1.7.2",
    "maintainer": "Marton Kolossvary <marton.kolossvary@gmail.com>",
    "author": "Marton Kolossvary [aut, cre]",
    "url": "https://pubmed.ncbi.nlm.nih.gov/29233836/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RIA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RIA Radiomics Image Analysis Toolbox for Medial Images Radiomics image analysis toolbox for 2D and 3D radiological images. RIA supports DICOM, NIfTI,\n             nrrd and npy (numpy array) file formats.\n             RIA calculates first-order, gray level co-occurrence matrix, gray level run length matrix and\n             geometry-based statistics. Almost all calculations are done using vectorized formulas to\n             optimize run speeds. Calculation of several thousands of parameters only takes minutes\n             on a single core of a conventional PC. Detailed methodology has been published: Kolossvary\n             et al. Circ: Cardiovascular Imaging. 2017;10(12):e006843 <doi: 10.1161/CIRCIMAGING.117.006843>.  "
  },
  {
    "id": 6243,
    "package_name": "RImpact",
    "title": "Calculates Measures of Scholarly Impact",
    "description": "The metrics() function calculates measures of scholarly impact. \n    These include conventional measures, such as the number of publications\n    and the total citations to all publications, as well as modern and \n    robust metrics based on the vector of citations associated with each \n    publication, such as the h index and many of its variants or rivals.\n    These methods are described in Ruscio et al. (2012) \n    <DOI: 10.1080/15366367.2012.711147>.",
    "version": "1.0",
    "maintainer": "John Ruscio <ruscio@tcnj.edu>",
    "author": "John Ruscio",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RImpact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RImpact Calculates Measures of Scholarly Impact The metrics() function calculates measures of scholarly impact. \n    These include conventional measures, such as the number of publications\n    and the total citations to all publications, as well as modern and \n    robust metrics based on the vector of citations associated with each \n    publication, such as the h index and many of its variants or rivals.\n    These methods are described in Ruscio et al. (2012) \n    <DOI: 10.1080/15366367.2012.711147>.  "
  },
  {
    "id": 6250,
    "package_name": "RJSONIO",
    "title": "Serialize R Objects to JSON, JavaScript Object Notation",
    "description": "This is a package that allows conversion to and from \n  data in Javascript object notation (JSON) format.\n  This allows R objects to be inserted into Javascript/ECMAScript/ActionScript code\n  and allows R programmers to read and convert JSON content to R objects.\n  This is an alternative to rjson package. Originally, that was too slow for converting large R objects to JSON\n  and was not extensible.  rjson's performance is now similar to this package, and perhaps slightly faster in some cases.\n  This package uses methods and is readily extensible by defining methods for different classes, \n  vectorized operations, and C code and callbacks to R functions for deserializing JSON objects to R. \n  The two packages intentionally share the same basic interface. This package (RJSONIO) has many additional\n  options to allow customizing the generation and processing of JSON content.\n  This package uses libjson rather than implementing yet another JSON parser. The aim is to support\n  other general projects by building on their work, providing feedback and benefit from their ongoing development.",
    "version": "2.0.0",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0000-0001-9200-1016>),\n  CRAN Team [aut],\n  Duncan Temple Lang [aut] (ORCID:\n    <https://orcid.org/0000-0003-0159-1546>),\n  Jonathan Wallace [aut] (aka ninja9578, author of included libjson\n    sources)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RJSONIO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RJSONIO Serialize R Objects to JSON, JavaScript Object Notation This is a package that allows conversion to and from \n  data in Javascript object notation (JSON) format.\n  This allows R objects to be inserted into Javascript/ECMAScript/ActionScript code\n  and allows R programmers to read and convert JSON content to R objects.\n  This is an alternative to rjson package. Originally, that was too slow for converting large R objects to JSON\n  and was not extensible.  rjson's performance is now similar to this package, and perhaps slightly faster in some cases.\n  This package uses methods and is readily extensible by defining methods for different classes, \n  vectorized operations, and C code and callbacks to R functions for deserializing JSON objects to R. \n  The two packages intentionally share the same basic interface. This package (RJSONIO) has many additional\n  options to allow customizing the generation and processing of JSON content.\n  This package uses libjson rather than implementing yet another JSON parser. The aim is to support\n  other general projects by building on their work, providing feedback and benefit from their ongoing development.  "
  },
  {
    "id": 6274,
    "package_name": "RMAWGEN",
    "title": "Multi-Site Auto-Regressive Weather GENerator",
    "description": "S3 and S4 functions are implemented for spatial multi-site\n    stochastic generation of daily time series of temperature and\n    precipitation. These tools make use of Vector AutoRegressive models (VARs).\n    The weather generator model is then saved as an object and is calibrated by\n    daily instrumental \"Gaussianized\" time series through the 'vars' package\n    tools. Once obtained this model, it can it can be used for weather\n    generations and be adapted to work with several climatic monthly time\n    series.",
    "version": "1.3.9.3",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-3508-5898>),\n  Emanuele Eccel [aut] (ORCID: <https://orcid.org/0000-0003-3239-828X>),\n  Eike Luedeling [ctb] (ORCID: <https://orcid.org/0000-0002-7316-3631>)",
    "url": "https://ecor.github.io/RMAWGEN/,https://github.com/ecor/RMAWGEN,\nhttps://docs.google.com/file/d/0B66otCUk3Bv6V3RPbm1mUG4zVHc/edit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RMAWGEN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMAWGEN Multi-Site Auto-Regressive Weather GENerator S3 and S4 functions are implemented for spatial multi-site\n    stochastic generation of daily time series of temperature and\n    precipitation. These tools make use of Vector AutoRegressive models (VARs).\n    The weather generator model is then saved as an object and is calibrated by\n    daily instrumental \"Gaussianized\" time series through the 'vars' package\n    tools. Once obtained this model, it can it can be used for weather\n    generations and be adapted to work with several climatic monthly time\n    series.  "
  },
  {
    "id": 6292,
    "package_name": "RMVL",
    "title": "Mappable Vector Library for Handling Large Datasets",
    "description": "Mappable vector library provides convenient way to access large datasets. Use all of your data at once, with few limits. Memory mapped data can be shared between multiple R processes. Access speed depends on storage medium, so solid state drive is recommended, preferably with PCI Express (or M.2 nvme) interface or a fast network file system. The data is memory mapped into R and then accessed using usual R list and array subscription operators. Convenience functions are provided for merging, grouping and indexing large vectors and data.frames. The layout of underlying MVL files is optimized for large datasets. The vectors are stored to guarantee alignment for vector intrinsics after memory map. The package is built on top of libMVL, which can be used as a standalone C library. libMVL has simple C API making it easy to interchange datasets with outside programs. Large MVL datasets are distributed via Academic Torrents <https://academictorrents.com/collection/mvl-datasets>. ",
    "version": "1.1.0.2",
    "maintainer": "Vladimir Dergachev <support@altumrete.com>",
    "author": "Vladimir Dergachev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4708-6625>)",
    "url": "https://academictorrents.com/collection/mvl-datasets,\nhttps://github.com/volodya31415/RMVL,\nhttps://github.com/volodya31415/libMVL",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RMVL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMVL Mappable Vector Library for Handling Large Datasets Mappable vector library provides convenient way to access large datasets. Use all of your data at once, with few limits. Memory mapped data can be shared between multiple R processes. Access speed depends on storage medium, so solid state drive is recommended, preferably with PCI Express (or M.2 nvme) interface or a fast network file system. The data is memory mapped into R and then accessed using usual R list and array subscription operators. Convenience functions are provided for merging, grouping and indexing large vectors and data.frames. The layout of underlying MVL files is optimized for large datasets. The vectors are stored to guarantee alignment for vector intrinsics after memory map. The package is built on top of libMVL, which can be used as a standalone C library. libMVL has simple C API making it easy to interchange datasets with outside programs. Large MVL datasets are distributed via Academic Torrents <https://academictorrents.com/collection/mvl-datasets>.   "
  },
  {
    "id": 6348,
    "package_name": "RPEXE.RPEXT",
    "title": "Reduced Piecewise Exponential Estimate/Test Software",
    "description": "This reduced piecewise exponential survival software implements the likelihood ratio test and backward elimination procedure in Han, Schell, and Kim (2012 <doi:10.1080/19466315.2012.698945>, 2014 <doi:10.1002/sim.5915>), and Han et al. (2016 <doi:10.1111/biom.12590>). Inputs to the program can be either times when events/censoring occur or the vectors of total time on test and the number of events. Outputs of the programs are times and the corresponding p-values in the backward elimination. Details about the model and implementation are given in Han et al. 2014. This program can run in R version 3.2.2 and above.",
    "version": "0.0.2",
    "maintainer": "Gang Han <hangang.true@gmail.com>",
    "author": "Gang Han [aut, cre],\n  Yu Zhang [aut]",
    "url": "https://github.com/hangangtrue/RPEXE.RPEXT",
    "bug_reports": "https://github.com/hangangtrue/RPEXE.RPEXT/issues",
    "repository": "https://cran.r-project.org/package=RPEXE.RPEXT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPEXE.RPEXT Reduced Piecewise Exponential Estimate/Test Software This reduced piecewise exponential survival software implements the likelihood ratio test and backward elimination procedure in Han, Schell, and Kim (2012 <doi:10.1080/19466315.2012.698945>, 2014 <doi:10.1002/sim.5915>), and Han et al. (2016 <doi:10.1111/biom.12590>). Inputs to the program can be either times when events/censoring occur or the vectors of total time on test and the number of events. Outputs of the programs are times and the corresponding p-values in the backward elimination. Details about the model and implementation are given in Han et al. 2014. This program can run in R version 3.2.2 and above.  "
  },
  {
    "id": 6349,
    "package_name": "RPEnsemble",
    "title": "Random Projection Ensemble Classification",
    "description": "Implements the methodology of \"Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959--1035\". The random projection ensemble classifier is a general method for classification of high-dimensional data, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower-dimensional space. The random projections are divided into non-overlapping blocks, and within each block the projection yielding the smallest estimate of the test error is selected. The random projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections, with a data-driven voting threshold to determine the final assignment. ",
    "version": "0.5",
    "maintainer": "Timothy I. Cannings <timothy.cannings@ed.ac.uk>",
    "author": "Timothy I. Cannings and Richard J. Samworth",
    "url": "https://arxiv.org/abs/1504.04595,\nhttps://www.maths.ed.ac.uk/~tcannings/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RPEnsemble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPEnsemble Random Projection Ensemble Classification Implements the methodology of \"Cannings, T. I. and Samworth, R. J. (2017) Random-projection ensemble classification, J. Roy. Statist. Soc., Ser. B. (with discussion), 79, 959--1035\". The random projection ensemble classifier is a general method for classification of high-dimensional data, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors into a lower-dimensional space. The random projections are divided into non-overlapping blocks, and within each block the projection yielding the smallest estimate of the test error is selected. The random projection ensemble classifier then aggregates the results of applying the base classifier on the selected projections, with a data-driven voting threshold to determine the final assignment.   "
  },
  {
    "id": 6384,
    "package_name": "RRphylo",
    "title": "Phylogenetic Ridge Regression Methods for Comparative Studies",
    "description": "Functions for phylogenetic analysis (Castiglione et al., 2018 <doi:10.1111/2041-210X.12954>). The functions perform the estimation of phenotypic evolutionary rates, identification of phenotypic evolutionary rate shifts, quantification of direction and size of evolutionary change in multivariate traits, the computation of ontogenetic shape vectors and test for morphological convergence.",
    "version": "3.0.1",
    "maintainer": "Silvia Castiglione <silvia.castiglione@unina.it>",
    "author": "Pasquale Raia [aut],\n  Silvia Castiglione [aut, cre],\n  Carmela Serio [aut],\n  Giorgia Girardi [aut],\n  Alessandro Mondanaro [aut],\n  Marina Melchionna [aut],\n  Mirko Di Febbraro [aut],\n  Antonio Profico [aut],\n  Francesco Carotenuto [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RRphylo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RRphylo Phylogenetic Ridge Regression Methods for Comparative Studies Functions for phylogenetic analysis (Castiglione et al., 2018 <doi:10.1111/2041-210X.12954>). The functions perform the estimation of phenotypic evolutionary rates, identification of phenotypic evolutionary rate shifts, quantification of direction and size of evolutionary change in multivariate traits, the computation of ontogenetic shape vectors and test for morphological convergence.  "
  },
  {
    "id": 6389,
    "package_name": "RSBJson",
    "title": "Handle R Requests from R Service Bus Applications with JSON\nPayloads",
    "description": "Package to Handle R Requests from R Service Bus Applications with JSON Payloads in a generic way.\n    The incoming request is encoded as a string (character vector of length one) containing \n    the JSON file passed through by the client. ",
    "version": "1.1.2",
    "maintainer": "Tobias Verbeke <tobias.verbeke@openanalytics.eu>",
    "author": "Tobias Verbeke",
    "url": "https://www.rservicebus.io",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSBJson",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSBJson Handle R Requests from R Service Bus Applications with JSON\nPayloads Package to Handle R Requests from R Service Bus Applications with JSON Payloads in a generic way.\n    The incoming request is encoded as a string (character vector of length one) containing \n    the JSON file passed through by the client.   "
  },
  {
    "id": 6410,
    "package_name": "RSentiment",
    "title": "Analyse Sentiment of English Sentences",
    "description": "Analyses sentiment of a sentence in English and assigns score to it. It can classify sentences to the following categories of sentiments:- Positive, Negative, very Positive, very negative, \n              Neutral. For a vector of sentences, it counts the number of sentences in each\n              category of sentiment.In calculating the score, negation and various degrees\n              of adjectives are taken into consideration. It deals only with English sentences.",
    "version": "2.2.2",
    "maintainer": "Subhasree Bose <subhasree10.7@gmail.com>",
    "author": "Subhasree Bose <subhasree10.7@gmail.com> with contributons from Saptarsi Goswami.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSentiment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSentiment Analyse Sentiment of English Sentences Analyses sentiment of a sentence in English and assigns score to it. It can classify sentences to the following categories of sentiments:- Positive, Negative, very Positive, very negative, \n              Neutral. For a vector of sentences, it counts the number of sentences in each\n              category of sentiment.In calculating the score, negation and various degrees\n              of adjectives are taken into consideration. It deals only with English sentences.  "
  },
  {
    "id": 6415,
    "package_name": "RSpectra",
    "title": "Solvers for Large-Scale Eigenvalue and SVD Problems",
    "description": "R interface to the 'Spectra' library\n    <https://spectralib.org/> for large-scale eigenvalue and SVD\n    problems. It is typically used to compute a few\n    eigenvalues/vectors of an n by n matrix, e.g., the k largest eigenvalues,\n    which is usually more efficient than eigen() if k << n. This package\n    provides the 'eigs()' function that does the similar job as in 'Matlab',\n    'Octave', 'Python SciPy' and 'Julia'. It also provides the 'svds()' function\n    to calculate the largest k singular values and corresponding\n    singular vectors of a real matrix. The matrix to be computed on can be\n    dense, sparse, or in the form of an operator defined by the user.",
    "version": "0.16-2",
    "maintainer": "Yixuan Qiu <yixuan.qiu@cos.name>",
    "author": "Yixuan Qiu [aut, cre],\n  Jiali Mei [aut] (Function interface of matrix operation),\n  Gael Guennebaud [ctb] (Eigenvalue solvers from the 'Eigen' library),\n  Jitse Niesen [ctb] (Eigenvalue solvers from the 'Eigen' library)",
    "url": "https://github.com/yixuan/RSpectra",
    "bug_reports": "https://github.com/yixuan/RSpectra/issues",
    "repository": "https://cran.r-project.org/package=RSpectra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSpectra Solvers for Large-Scale Eigenvalue and SVD Problems R interface to the 'Spectra' library\n    <https://spectralib.org/> for large-scale eigenvalue and SVD\n    problems. It is typically used to compute a few\n    eigenvalues/vectors of an n by n matrix, e.g., the k largest eigenvalues,\n    which is usually more efficient than eigen() if k << n. This package\n    provides the 'eigs()' function that does the similar job as in 'Matlab',\n    'Octave', 'Python SciPy' and 'Julia'. It also provides the 'svds()' function\n    to calculate the largest k singular values and corresponding\n    singular vectors of a real matrix. The matrix to be computed on can be\n    dense, sparse, or in the form of an operator defined by the user.  "
  },
  {
    "id": 6416,
    "package_name": "RSpincalc",
    "title": "Conversion Between Attitude Representations of DCM, Euler\nAngles, Quaternions, and Euler Vectors",
    "description": "Conversion between attitude representations: DCM, Euler angles, Quaternions, and Euler vectors.\n    Plus conversion between 2 Euler angle set types (xyx, yzy, zxz, xzx, yxy, zyz, xyz, yzx, zxy, xzy, yxz, zyx).\n    Fully vectorized code, with warnings/errors for Euler angles (singularity, out of range, invalid angle order), \n    DCM (orthogonality, not proper, exceeded tolerance to unity determinant) and Euler vectors(not unity).\n    Also quaternion and other useful functions.\n    Based on SpinCalc by John Fuller and SpinConv by Paolo de Leva.",
    "version": "1.0.2",
    "maintainer": "Jose Gama <rxprtgama@gmail.com>",
    "author": "Jose Gama [aut, cre],\n  John Fuller [aut, cph],\n  Paolo Leva [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSpincalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSpincalc Conversion Between Attitude Representations of DCM, Euler\nAngles, Quaternions, and Euler Vectors Conversion between attitude representations: DCM, Euler angles, Quaternions, and Euler vectors.\n    Plus conversion between 2 Euler angle set types (xyx, yzy, zxz, xzx, yxy, zyz, xyz, yzx, zxy, xzy, yxz, zyx).\n    Fully vectorized code, with warnings/errors for Euler angles (singularity, out of range, invalid angle order), \n    DCM (orthogonality, not proper, exceeded tolerance to unity determinant) and Euler vectors(not unity).\n    Also quaternion and other useful functions.\n    Based on SpinCalc by John Fuller and SpinConv by Paolo de Leva.  "
  },
  {
    "id": 6450,
    "package_name": "RWmisc",
    "title": "Miscellaneous Spatial Functions",
    "description": "Contains convenience functions for working with spatial data across\n    multiple UTM zones, raster-vector operations common in the analysis of \n    conflict data, and converting degrees, minutes, and seconds latitude and\n    longitude coordinates to decimal degrees.",
    "version": "0.1.2",
    "maintainer": "Rob Williams <jayrobwilliams@gmail.com>",
    "author": "Rob Williams [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9259-3883>)",
    "url": "https://github.com/jayrobwilliams/RWmisc",
    "bug_reports": "https://github.com/jayrobwilliams/RWmisc/issues",
    "repository": "https://cran.r-project.org/package=RWmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RWmisc Miscellaneous Spatial Functions Contains convenience functions for working with spatial data across\n    multiple UTM zones, raster-vector operations common in the analysis of \n    conflict data, and converting degrees, minutes, and seconds latitude and\n    longitude coordinates to decimal degrees.  "
  },
  {
    "id": 6458,
    "package_name": "RaSEn",
    "title": "Random Subspace Ensemble Classification and Variable Screening",
    "description": "We propose a general ensemble classification framework, RaSE algorithm, for the sparse classification problem. In RaSE algorithm, for each weak learner, some random subspaces are generated and the optimal one is chosen to train the model on the basis of some criterion. To be adapted to the problem, a novel criterion, ratio information criterion (RIC) is put up with based on Kullback-Leibler divergence. Besides minimizing RIC, multiple criteria can be applied, for instance, minimizing extended Bayesian information criterion (eBIC), minimizing training error, minimizing the validation error, minimizing the cross-validation error, minimizing leave-one-out error. There are various choices of base classifier, for instance, linear discriminant analysis, quadratic discriminant analysis, k-nearest neighbour, logistic regression, decision trees, random forest, support vector machines. RaSE algorithm can also be applied to do feature ranking, providing us the importance of each feature based on the selected percentage in multiple subspaces. RaSE framework can be extended to the general prediction framework, including both classification and regression. We can use the selected percentages of variables for variable screening. The latest version added the variable screening function for both regression and classification problems. ",
    "version": "3.0.0",
    "maintainer": "Ye Tian <ye.t@columbia.edu>",
    "author": "Ye Tian [aut, cre] and Yang Feng [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RaSEn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RaSEn Random Subspace Ensemble Classification and Variable Screening We propose a general ensemble classification framework, RaSE algorithm, for the sparse classification problem. In RaSE algorithm, for each weak learner, some random subspaces are generated and the optimal one is chosen to train the model on the basis of some criterion. To be adapted to the problem, a novel criterion, ratio information criterion (RIC) is put up with based on Kullback-Leibler divergence. Besides minimizing RIC, multiple criteria can be applied, for instance, minimizing extended Bayesian information criterion (eBIC), minimizing training error, minimizing the validation error, minimizing the cross-validation error, minimizing leave-one-out error. There are various choices of base classifier, for instance, linear discriminant analysis, quadratic discriminant analysis, k-nearest neighbour, logistic regression, decision trees, random forest, support vector machines. RaSE algorithm can also be applied to do feature ranking, providing us the importance of each feature based on the selected percentage in multiple subspaces. RaSE framework can be extended to the general prediction framework, including both classification and regression. We can use the selected percentages of variables for variable screening. The latest version added the variable screening function for both regression and classification problems.   "
  },
  {
    "id": 6474,
    "package_name": "RandomProjectionTest",
    "title": "Two-Sample Test in High Dimensions using Random Projection",
    "description": "Performs the random projection test (Lopes et al., (2011) <doi:10.48550/arXiv.1108.2401>) for the one-sample and two-sample hypothesis testing problem for equality of means in the high dimensional setting. We are interested in detecting the mean vector in the one-sample problem or the difference between mean vectors in the two-sample problem.",
    "version": "0.1.4",
    "maintainer": "Juan Ortiz Author <juan.ortiz1alonso@gmail.com>",
    "author": "Juan Ortiz Author [aut, cre, cph, rev]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RandomProjectionTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RandomProjectionTest Two-Sample Test in High Dimensions using Random Projection Performs the random projection test (Lopes et al., (2011) <doi:10.48550/arXiv.1108.2401>) for the one-sample and two-sample hypothesis testing problem for equality of means in the high dimensional setting. We are interested in detecting the mean vector in the one-sample problem or the difference between mean vectors in the two-sample problem.  "
  },
  {
    "id": 6509,
    "package_name": "Rcatch22",
    "title": "Calculation of 22 Canonical Time-Series Characteristics",
    "description": "Calculate 22 summary statistics coded in C on time-series vectors to enable \n    pattern detection, classification, and regression applications in the \n    feature space as proposed by <doi:10.1007/s10618-019-00647-x>.",
    "version": "0.2.3",
    "maintainer": "Trent Henderson <then6675@uni.sydney.edu.au>",
    "author": "Trent Henderson [cre, aut],\n  Carl Lubba [ctb]",
    "url": "",
    "bug_reports": "https://github.com/hendersontrent/Rcatch22/issues/",
    "repository": "https://cran.r-project.org/package=Rcatch22",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rcatch22 Calculation of 22 Canonical Time-Series Characteristics Calculate 22 summary statistics coded in C on time-series vectors to enable \n    pattern detection, classification, and regression applications in the \n    feature space as proposed by <doi:10.1007/s10618-019-00647-x>.  "
  },
  {
    "id": 6511,
    "package_name": "RcensusPkg",
    "title": "Easily Access US Census Bureau Survey and Geographic Data",
    "description": "The key function 'get_vintage_data()' returns a dataframe and is \n    the window into the Census Bureau API requiring just a dataset name, \n    vintage(year), and vector of variable names for survey estimates/percentages. \n    Other functions assist in searching for available datasets, geographies, \n    group/variable concepts of interest.  Also provided are functions to access \n    and layer (via standard piping) displayable geometries for the US, states, \n    counties, blocks/tracts, roads, landmarks, places, and bodies of water. \n    Joining survey data with many of the geometry functions is built-in to \n    produce choropleth maps.",
    "version": "0.1.5",
    "maintainer": "Rick Dean <deanr3@bardstown.com>",
    "author": "Rick Dean [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0005-8086-8436>)",
    "url": "https://github.com/deandevl/RcensusPkg",
    "bug_reports": "https://github.com/deandevl/RcensusPkg/issues",
    "repository": "https://cran.r-project.org/package=RcensusPkg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcensusPkg Easily Access US Census Bureau Survey and Geographic Data The key function 'get_vintage_data()' returns a dataframe and is \n    the window into the Census Bureau API requiring just a dataset name, \n    vintage(year), and vector of variable names for survey estimates/percentages. \n    Other functions assist in searching for available datasets, geographies, \n    group/variable concepts of interest.  Also provided are functions to access \n    and layer (via standard piping) displayable geometries for the US, states, \n    counties, blocks/tracts, roads, landmarks, places, and bodies of water. \n    Joining survey data with many of the geometry functions is built-in to \n    produce choropleth maps.  "
  },
  {
    "id": 6551,
    "package_name": "RcppAlgos",
    "title": "High Performance Tools for Combinatorics and Computational\nMathematics",
    "description": "Provides optimized functions and flexible iterators implemented in\n    C++ for solving problems in combinatorics and computational mathematics.\n    Handles various combinatorial objects including combinations, permutations,\n    integer partitions and compositions, Cartesian products, unordered\n    Cartesian products, and partition of groups. Utilizes the RMatrix class\n    from 'RcppParallel' for thread safety. The combination and permutation\n    functions contain constraint parameters that allow for generation of all\n    results of a vector meeting specific criteria (e.g. finding all\n    combinations such that the sum is between two bounds). Capable of\n    ranking/unranking combinatorial objects efficiently (e.g. retrieve only the\n    nth lexicographical result) which sets up nicely for parallelization as\n    well as random sampling. Gmp support permits exploration where the total\n    number of results is large (e.g. comboSample(10000, 500, n = 4)).\n    Additionally, there are several high performance number theoretic\n    functions that are useful for problems common in computational mathematics.\n    Some of these functions make use of the fast integer division library\n    'libdivide'. The primeSieve function is based on the segmented sieve of\n    Eratosthenes implementation by Kim Walisch. It is also efficient for large\n    numbers by using the cache friendly improvements originally developed by\n    Tom\u00e1s Oliveira. Finally, there is a prime counting function that implements\n    Legendre's formula based on the work of Kim Walisch.",
    "version": "2.9.3",
    "maintainer": "Joseph Wood <jwood000@gmail.com>",
    "author": "Joseph Wood [aut, cre]",
    "url": "https://github.com/jwood000/RcppAlgos, https://gmplib.org/,\nhttps://github.com/kimwalisch/primesieve,\nhttps://libdivide.com,\nhttps://github.com/kimwalisch/primecount,\nhttps://ridiculousfish.com/,\nhttps://sweet.ua.pt/tos/software/prime_sieve.html",
    "bug_reports": "https://github.com/jwood000/RcppAlgos/issues",
    "repository": "https://cran.r-project.org/package=RcppAlgos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppAlgos High Performance Tools for Combinatorics and Computational\nMathematics Provides optimized functions and flexible iterators implemented in\n    C++ for solving problems in combinatorics and computational mathematics.\n    Handles various combinatorial objects including combinations, permutations,\n    integer partitions and compositions, Cartesian products, unordered\n    Cartesian products, and partition of groups. Utilizes the RMatrix class\n    from 'RcppParallel' for thread safety. The combination and permutation\n    functions contain constraint parameters that allow for generation of all\n    results of a vector meeting specific criteria (e.g. finding all\n    combinations such that the sum is between two bounds). Capable of\n    ranking/unranking combinatorial objects efficiently (e.g. retrieve only the\n    nth lexicographical result) which sets up nicely for parallelization as\n    well as random sampling. Gmp support permits exploration where the total\n    number of results is large (e.g. comboSample(10000, 500, n = 4)).\n    Additionally, there are several high performance number theoretic\n    functions that are useful for problems common in computational mathematics.\n    Some of these functions make use of the fast integer division library\n    'libdivide'. The primeSieve function is based on the segmented sieve of\n    Eratosthenes implementation by Kim Walisch. It is also efficient for large\n    numbers by using the cache friendly improvements originally developed by\n    Tom\u00e1s Oliveira. Finally, there is a prime counting function that implements\n    Legendre's formula based on the work of Kim Walisch.  "
  },
  {
    "id": 6553,
    "package_name": "RcppArray",
    "title": "'Rcpp' Meets 'C++' Arrays",
    "description": "Interoperability between 'Rcpp' and the 'C++11' array and tuple\n    types. Linking to this package allows fixed-length 'std::array' objects to\n    be converted to and from equivalent R vectors, and 'std::tuple' objects\n    converted to lists, via the as() and wrap() functions. There is also\n    experimental support for 'std::span' from 'C++20'.",
    "version": "0.3.0",
    "maintainer": "Jon Clayden <code@clayden.org>",
    "author": "Jon Clayden [cre, aut] (ORCID: <https://orcid.org/0000-0002-6608-0619>),\n  Dirk Eddelbuettel [aut],\n  Andrew Johnson [ctb] (ORCID: <https://orcid.org/0000-0001-7000-8065>)",
    "url": "https://github.com/jonclayden/RcppArray",
    "bug_reports": "https://github.com/jonclayden/RcppArray/issues",
    "repository": "https://cran.r-project.org/package=RcppArray",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppArray 'Rcpp' Meets 'C++' Arrays Interoperability between 'Rcpp' and the 'C++11' array and tuple\n    types. Linking to this package allows fixed-length 'std::array' objects to\n    be converted to and from equivalent R vectors, and 'std::tuple' objects\n    converted to lists, via the as() and wrap() functions. There is also\n    experimental support for 'std::span' from 'C++20'.  "
  },
  {
    "id": 6556,
    "package_name": "RcppBigIntAlgos",
    "title": "Factor Big Integers with the Parallel Quadratic Sieve",
    "description": "Features the multiple polynomial quadratic sieve (MPQS) algorithm\n    for factoring large integers and a vectorized factoring function that\n    returns the complete factorization of an integer. The MPQS is based off of\n    the seminal work of Carl Pomerance (1984) <doi:10.1007/3-540-39757-4_17>\n    along with the modification of multiple polynomials introduced by Peter\n    Montgomery and J. Davis as outlined by Robert D. Silverman (1987)\n    <doi:10.1090/S0025-5718-1987-0866119-8>. Utilizes the C library\n    GMP (GNU Multiple Precision Arithmetic). For smaller integers, a simple\n    Elliptic Curve algorithm is attempted followed by a constrained version of \n    Pollard's rho algorithm. The Pollard's rho algorithm is the same algorithm\n    used by the factorize function in the 'gmp' package.",
    "version": "1.1.0",
    "maintainer": "Joseph Wood <jwood000@gmail.com>",
    "author": "Joseph Wood [aut, cre],\n  Free Software Foundation, Inc. [cph],\n  Mike Tryczak [ctb]",
    "url": "https://github.com/jwood000/RcppBigIntAlgos, https://gmplib.org/,\nhttp://mathworld.wolfram.com/QuadraticSieve.html,\nhttps://micsymposium.org/mics_2011_proceedings/mics2011_submission_28.pdf,\nhttps://www.math.colostate.edu/~hulpke/lectures/m400c/quadsievex.pdf,\nhttps://blogs.msdn.microsoft.com/devdev/2006/06/19/factoring-large-numbers-with-quadratic-sieve/",
    "bug_reports": "https://github.com/jwood000/RcppBigIntAlgos/issues",
    "repository": "https://cran.r-project.org/package=RcppBigIntAlgos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppBigIntAlgos Factor Big Integers with the Parallel Quadratic Sieve Features the multiple polynomial quadratic sieve (MPQS) algorithm\n    for factoring large integers and a vectorized factoring function that\n    returns the complete factorization of an integer. The MPQS is based off of\n    the seminal work of Carl Pomerance (1984) <doi:10.1007/3-540-39757-4_17>\n    along with the modification of multiple polynomials introduced by Peter\n    Montgomery and J. Davis as outlined by Robert D. Silverman (1987)\n    <doi:10.1090/S0025-5718-1987-0866119-8>. Utilizes the C library\n    GMP (GNU Multiple Precision Arithmetic). For smaller integers, a simple\n    Elliptic Curve algorithm is attempted followed by a constrained version of \n    Pollard's rho algorithm. The Pollard's rho algorithm is the same algorithm\n    used by the factorize function in the 'gmp' package.  "
  },
  {
    "id": 6560,
    "package_name": "RcppCNPy",
    "title": "Read-Write Support for 'NumPy' Files via 'Rcpp'",
    "description": "The 'cnpy' library written by Carl Rogers provides read and write\n facilities for files created with (or for) the 'NumPy' extension for 'Python'.\n Vectors and matrices of numeric types can be read or written to and from\n files as well as compressed files. Support for integer files is available if\n the package has been built with as C++11 which should be the default on\n all platforms since the release of R 3.3.0.",
    "version": "0.2.14",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Wush Wu [aut] (ORCID: <https://orcid.org/0000-0001-5180-0567>),\n  Carl Rogers [aut] (Author of CNPy)",
    "url": "https://github.com/eddelbuettel/rcppcnpy,\nhttps://dirk.eddelbuettel.com/code/rcpp.cnpy.html",
    "bug_reports": "https://github.com/eddelbuettel/rcppcnpy/issues",
    "repository": "https://cran.r-project.org/package=RcppCNPy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppCNPy Read-Write Support for 'NumPy' Files via 'Rcpp' The 'cnpy' library written by Carl Rogers provides read and write\n facilities for files created with (or for) the 'NumPy' extension for 'Python'.\n Vectors and matrices of numeric types can be read or written to and from\n files as well as compressed files. Support for integer files is available if\n the package has been built with as C++11 which should be the default on\n all platforms since the release of R 3.3.0.  "
  },
  {
    "id": 6565,
    "package_name": "RcppColMetric",
    "title": "Efficient Column-Wise Metric Computation Against Common Vector",
    "description": "In data science, it is a common practice to compute a series of columns (e.g. features) against a common response vector. Various metrics are provided with efficient computation implemented with 'Rcpp'.",
    "version": "0.1.0",
    "maintainer": "Xiurui Zhu <zxr6@163.com>",
    "author": "Xiurui Zhu [aut, cre]",
    "url": "https://github.com/zhuxr11/RcppColMetric",
    "bug_reports": "https://github.com/zhuxr11/RcppColMetric/issues",
    "repository": "https://cran.r-project.org/package=RcppColMetric",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppColMetric Efficient Column-Wise Metric Computation Against Common Vector In data science, it is a common practice to compute a series of columns (e.g. features) against a common response vector. Various metrics are provided with efficient computation implemented with 'Rcpp'.  "
  },
  {
    "id": 6570,
    "package_name": "RcppDist",
    "title": "'Rcpp' Integration of Additional Probability Distributions",
    "description": "The 'Rcpp' package provides a C++ library to make it easier\n    to use C++ with R. R and 'Rcpp' provide functions for a variety of\n    statistical distributions. Several R packages make functions\n    available to R for additional statistical distributions. However,\n    to access these functions from C++ code, a costly call to the R\n    functions must be made. 'RcppDist' provides a header-only C++ library\n    with functions for additional statistical distributions that can be\n    called from C++ when writing code using 'Rcpp' or 'RcppArmadillo'.\n    Functions are available that return a 'NumericVector' as well as\n    doubles, and for multivariate or matrix distributions, 'Armadillo'\n    vectors and matrices. 'RcppDist' provides functions for the following\n    distributions: the four parameter beta distribution; the location-\n    scale t distribution; the truncated normal distribution; the\n    truncated t distribution; a truncated location-scale t distribution;\n    the triangle distribution; the multivariate normal distribution*;\n    the multivariate t distribution*; the Wishart distribution*; and\n    the inverse Wishart distribution*. Distributions marked with an\n    asterisk rely on 'RcppArmadillo'.",
    "version": "0.1.1.1",
    "maintainer": "JB Duck-Mayr <j.duckmayr@gmail.com>",
    "author": "JB Duck-Mayr [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2231-1294>)",
    "url": "https://github.com/duckmayr/RcppDist",
    "bug_reports": "https://github.com/duckmayr/RcppDist/issues",
    "repository": "https://cran.r-project.org/package=RcppDist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppDist 'Rcpp' Integration of Additional Probability Distributions The 'Rcpp' package provides a C++ library to make it easier\n    to use C++ with R. R and 'Rcpp' provide functions for a variety of\n    statistical distributions. Several R packages make functions\n    available to R for additional statistical distributions. However,\n    to access these functions from C++ code, a costly call to the R\n    functions must be made. 'RcppDist' provides a header-only C++ library\n    with functions for additional statistical distributions that can be\n    called from C++ when writing code using 'Rcpp' or 'RcppArmadillo'.\n    Functions are available that return a 'NumericVector' as well as\n    doubles, and for multivariate or matrix distributions, 'Armadillo'\n    vectors and matrices. 'RcppDist' provides functions for the following\n    distributions: the four parameter beta distribution; the location-\n    scale t distribution; the truncated normal distribution; the\n    truncated t distribution; a truncated location-scale t distribution;\n    the triangle distribution; the multivariate normal distribution*;\n    the multivariate t distribution*; the Wishart distribution*; and\n    the inverse Wishart distribution*. Distributions marked with an\n    asterisk rely on 'RcppArmadillo'.  "
  },
  {
    "id": 6572,
    "package_name": "RcppEigen",
    "title": "'Rcpp' Integration for the 'Eigen' Templated Linear Algebra\nLibrary",
    "description": "R and 'Eigen' integration using 'Rcpp'.\n 'Eigen' is a C++ template library for linear algebra: matrices, vectors,\n numerical solvers and related algorithms.  It supports dense and sparse\n matrices on integer, floating point and complex numbers, decompositions of\n such matrices, and solutions of linear systems. Its performance on many\n algorithms is comparable with some of the best implementations based on\n 'Lapack' and level-3 'BLAS'. The 'RcppEigen' package includes the header\n files from the 'Eigen' C++ template library. Thus users do not need to\n install 'Eigen' itself in order to use 'RcppEigen'. Since version 3.1.1,\n 'Eigen' is licensed under the Mozilla Public License (version 2); earlier\n version were licensed under the GNU LGPL version 3 or later. 'RcppEigen'\n (the 'Rcpp' bindings/bridge to 'Eigen') is licensed under the GNU GPL\n version 2 or later, as is the rest of 'Rcpp'.",
    "version": "0.3.4.0.2",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Doug Bates [aut] (ORCID: <https://orcid.org/0000-0001-8316-9503>),\n  Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Yixuan Qiu [aut] (ORCID: <https://orcid.org/0000-0003-0109-6692>),\n  Authors of Eigen [cph] (Authorship and copyright in included Eigen\n    library as detailed in inst/COPYRIGHTS)",
    "url": "https://github.com/RcppCore/RcppEigen,\nhttps://dirk.eddelbuettel.com/code/rcpp.eigen.html",
    "bug_reports": "https://github.com/RcppCore/RcppEigen/issues",
    "repository": "https://cran.r-project.org/package=RcppEigen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppEigen 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra\nLibrary R and 'Eigen' integration using 'Rcpp'.\n 'Eigen' is a C++ template library for linear algebra: matrices, vectors,\n numerical solvers and related algorithms.  It supports dense and sparse\n matrices on integer, floating point and complex numbers, decompositions of\n such matrices, and solutions of linear systems. Its performance on many\n algorithms is comparable with some of the best implementations based on\n 'Lapack' and level-3 'BLAS'. The 'RcppEigen' package includes the header\n files from the 'Eigen' C++ template library. Thus users do not need to\n install 'Eigen' itself in order to use 'RcppEigen'. Since version 3.1.1,\n 'Eigen' is licensed under the Mozilla Public License (version 2); earlier\n version were licensed under the GNU LGPL version 3 or later. 'RcppEigen'\n (the 'Rcpp' bindings/bridge to 'Eigen') is licensed under the GNU GPL\n version 2 or later, as is the rest of 'Rcpp'.  "
  },
  {
    "id": 6579,
    "package_name": "RcppGSL",
    "title": "'Rcpp' Integration for 'GNU GSL' Vectors and Matrices",
    "description": "'Rcpp' integration for 'GNU GSL' vectors and matrices\n The 'GNU Scientific Library' (or 'GSL') is a collection of numerical routines for\n scientific computing. It is particularly useful for C and C++ programs as it\n provides a standard C interface to a wide range of mathematical routines. There\n are over 1000 functions in total with an extensive test suite. The 'RcppGSL'\n package provides an easy-to-use interface between 'GSL' data structures and\n R using concepts from 'Rcpp' which is itself a package that eases the\n interfaces between R and C++. This package also serves as a prime example of\n how to build a package that uses 'Rcpp' to connect to another third-party\n library. The 'autoconf' script, 'inline' plugin and example package can all\n be used as a stanza to  write a similar package against another library.",
    "version": "0.3.13",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel and Romain Francois",
    "url": "https://github.com/eddelbuettel/rcppgsl,\nhttps://dirk.eddelbuettel.com/code/rcpp.gsl.html",
    "bug_reports": "https://github.com/eddelbuettel/rcppgsl/issues",
    "repository": "https://cran.r-project.org/package=RcppGSL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppGSL 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices 'Rcpp' integration for 'GNU GSL' vectors and matrices\n The 'GNU Scientific Library' (or 'GSL') is a collection of numerical routines for\n scientific computing. It is particularly useful for C and C++ programs as it\n provides a standard C interface to a wide range of mathematical routines. There\n are over 1000 functions in total with an extensive test suite. The 'RcppGSL'\n package provides an easy-to-use interface between 'GSL' data structures and\n R using concepts from 'Rcpp' which is itself a package that eases the\n interfaces between R and C++. This package also serves as a prime example of\n how to build a package that uses 'Rcpp' to connect to another third-party\n library. The 'autoconf' script, 'inline' plugin and example package can all\n be used as a stanza to  write a similar package against another library.  "
  },
  {
    "id": 6586,
    "package_name": "RcppLbfgsBlaze",
    "title": "'L-BFGS' Algorithm Based on 'Blaze' for 'R' and 'Rcpp'",
    "description": "The 'L-BFGS' algorithm is a popular optimization algorithm for unconstrained optimization problems. \n  'Blaze' is a high-performance 'C++' math library for dense and sparse arithmetic. \n  This package provides a simple interface to the 'L-BFGS' algorithm and allows users to optimize \n  their objective functions with 'Blaze' vectors and matrices in 'R' and 'Rcpp'.",
    "version": "0.1.0",
    "maintainer": "Ching-Chuan Chen <zw12356@gmail.com>",
    "author": "Ching-Chuan Chen [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0007-8273-3206>),\n  Zhepei Wang [aut] (LBFGS-Lite),\n  Naoaki Okazaki [aut] (liblbfgs)",
    "url": "https://github.com/ChingChuan-Chen/RcppLbfgsBlaze,\nhttps://github.com/ChingChuan-Chen/LBFGS-blaze,\nhttps://github.com/ZJU-FAST-Lab/LBFGS-Lite,\nhttps://bitbucket.org/blaze-lib/blaze/src/master/",
    "bug_reports": "https://github.com/Chingchuan-chen/RcppLbfgsBlaze/issues",
    "repository": "https://cran.r-project.org/package=RcppLbfgsBlaze",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppLbfgsBlaze 'L-BFGS' Algorithm Based on 'Blaze' for 'R' and 'Rcpp' The 'L-BFGS' algorithm is a popular optimization algorithm for unconstrained optimization problems. \n  'Blaze' is a high-performance 'C++' math library for dense and sparse arithmetic. \n  This package provides a simple interface to the 'L-BFGS' algorithm and allows users to optimize \n  their objective functions with 'Blaze' vectors and matrices in 'R' and 'Rcpp'.  "
  },
  {
    "id": 6606,
    "package_name": "RcppUUID",
    "title": "Generating Universally Unique Identificators",
    "description": "Using the efficient implementation in the Boost C++ library, functions are\n provided to generate vectors of 'Universally Unique Identifiers (UUID)' from R supporting\n random (version 4), name (version 5) and time (version 7) 'UUIDs'. The initial repository\n was at <https://gitlab.com/artemklevtsov/rcppuuid>.",
    "version": "1.2.0",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Artem Klevtsov [aut] (ORCID: <https://orcid.org/0000-0003-0492-6647>),\n  Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>)",
    "url": "https://github.com/eddelbuettel/rcppuuid",
    "bug_reports": "https://github.com/eddelbuettel/rcppuuid/issues",
    "repository": "https://cran.r-project.org/package=RcppUUID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppUUID Generating Universally Unique Identificators Using the efficient implementation in the Boost C++ library, functions are\n provided to generate vectors of 'Universally Unique Identifiers (UUID)' from R supporting\n random (version 4), name (version 5) and time (version 7) 'UUIDs'. The initial repository\n was at <https://gitlab.com/artemklevtsov/rcppuuid>.  "
  },
  {
    "id": 6617,
    "package_name": "Rdiagnosislist",
    "title": "Manipulate SNOMED CT Diagnosis Lists",
    "description": "Functions and methods for manipulating 'SNOMED CT'\n    concepts. The package contains functions for loading the 'SNOMED CT'\n    release into a convenient R environment, selecting 'SNOMED CT'\n    concepts using regular expressions, and navigating the 'SNOMED CT'\n    ontology. It provides the 'SNOMEDconcept' S3 class for a vector of\n    'SNOMED CT' concepts (stored as 64-bit integers) and the\n    'SNOMEDcodelist' S3 class for a table of concepts IDs with\n    descriptions. The package can be used to construct sets of\n    'SNOMED CT' concepts for research (<doi:10.1093/jamia/ocac158>).\n    For more information about 'SNOMED CT' visit\n    <https://www.snomed.org/>.",
    "version": "1.4.0",
    "maintainer": "Anoop D. Shah <anoop@doctors.org.uk>",
    "author": "Anoop D. Shah [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8907-5724>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rdiagnosislist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rdiagnosislist Manipulate SNOMED CT Diagnosis Lists Functions and methods for manipulating 'SNOMED CT'\n    concepts. The package contains functions for loading the 'SNOMED CT'\n    release into a convenient R environment, selecting 'SNOMED CT'\n    concepts using regular expressions, and navigating the 'SNOMED CT'\n    ontology. It provides the 'SNOMEDconcept' S3 class for a vector of\n    'SNOMED CT' concepts (stored as 64-bit integers) and the\n    'SNOMEDcodelist' S3 class for a table of concepts IDs with\n    descriptions. The package can be used to construct sets of\n    'SNOMED CT' concepts for research (<doi:10.1093/jamia/ocac158>).\n    For more information about 'SNOMED CT' visit\n    <https://www.snomed.org/>.  "
  },
  {
    "id": 6630,
    "package_name": "ReDirection",
    "title": "Predict Dominant Direction of Reactions of a Biochemical Network",
    "description": "Biologically relevant, yet mathematically sound constraints are used\n    to compute the propensity and thence infer the dominant direction of reactions\n    of a generic biochemical network. The reactions must be unique and their \n    number must exceed that of the reactants,i.e., reactions >= reactants + 2.  \n            'ReDirection', computes the null space of a user-defined stoichiometry\n    matrix. The spanning non-zero and unique reaction vectors (RVs) are \n    combinatorially summed to generate one or more subspaces recursively.  \n            Every reaction is represented as a sequence of identical components \n    across all RVs of a particular subspace. The terms are evaluated with \n    (biologically relevant bounds, linear maps, tests of convergence, descriptive\n    statistics, vector norms) and the terms are classified into forward-, reverse- \n    and equivalent-subsets. Since, these are mutually exclusive the probability \n    of occurrence is binary (all, 1; none, 0).\n            The combined propensity of a reaction is the p1-norm of the \n    sub-propensities, i.e., sum of the products of the probability and maximum\n    numeric value of a subset (least upper bound, greatest lower bound). This, \n    if strictly positive is the probable rate constant, is used to infer dominant\n    direction and annotate a reaction as \"Forward (f)\", \"Reverse (b)\" or \n    \"Equivalent (e)\".\n            The inherent computational complexity (NP-hard) per iteration suggests\n    that a suitable value for the number of reactions is around 20.\n            Three functions comprise ReDirection. These are check_matrix() and \n    reaction_vector() which are internal, and calculate_reaction_vector()\n            which is external.",
    "version": "1.0.1",
    "maintainer": "Siddhartha Kundu <siddhartha_kundu@aiims.edu>",
    "author": "Siddhartha Kundu <2021: Manuscript Under Preparation>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ReDirection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReDirection Predict Dominant Direction of Reactions of a Biochemical Network Biologically relevant, yet mathematically sound constraints are used\n    to compute the propensity and thence infer the dominant direction of reactions\n    of a generic biochemical network. The reactions must be unique and their \n    number must exceed that of the reactants,i.e., reactions >= reactants + 2.  \n            'ReDirection', computes the null space of a user-defined stoichiometry\n    matrix. The spanning non-zero and unique reaction vectors (RVs) are \n    combinatorially summed to generate one or more subspaces recursively.  \n            Every reaction is represented as a sequence of identical components \n    across all RVs of a particular subspace. The terms are evaluated with \n    (biologically relevant bounds, linear maps, tests of convergence, descriptive\n    statistics, vector norms) and the terms are classified into forward-, reverse- \n    and equivalent-subsets. Since, these are mutually exclusive the probability \n    of occurrence is binary (all, 1; none, 0).\n            The combined propensity of a reaction is the p1-norm of the \n    sub-propensities, i.e., sum of the products of the probability and maximum\n    numeric value of a subset (least upper bound, greatest lower bound). This, \n    if strictly positive is the probable rate constant, is used to infer dominant\n    direction and annotate a reaction as \"Forward (f)\", \"Reverse (b)\" or \n    \"Equivalent (e)\".\n            The inherent computational complexity (NP-hard) per iteration suggests\n    that a suitable value for the number of reactions is around 20.\n            Three functions comprise ReDirection. These are check_matrix() and \n    reaction_vector() which are internal, and calculate_reaction_vector()\n            which is external.  "
  },
  {
    "id": 6632,
    "package_name": "ReMFPCA",
    "title": "Regularized Multivariate Functional Principal Component Analysis",
    "description": "Methods and tools for implementing regularized multivariate functional principal component analysis ('ReMFPCA') for multivariate functional data whose variables might be observed over different dimensional domains. 'ReMFPCA' is an object-oriented interface leveraging the extensibility and scalability of R6. It employs a parameter vector to control the smoothness of each functional variable. By incorporating smoothness constraints as penalty terms within a regularized optimization framework, 'ReMFPCA' generates smooth multivariate functional principal components, offering a concise and interpretable representation of the data. For detailed information on the methods and techniques used in 'ReMFPCA', please refer to Haghbin et al. (2023) <doi:10.48550/arXiv.2306.13980>. ",
    "version": "2.0.0",
    "maintainer": "Hossein Haghbin <haghbin@pgu.ac.ir>",
    "author": "Hossein Haghbin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8416-2354>),\n  Yue Zhao [aut] (ORCID: <https://orcid.org/0009-0000-4561-9163>),\n  Mehdi Maadooliat [aut] (ORCID: <https://orcid.org/0000-0002-5408-2676>)",
    "url": "https://github.com/haghbinh/ReMFPCA",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ReMFPCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReMFPCA Regularized Multivariate Functional Principal Component Analysis Methods and tools for implementing regularized multivariate functional principal component analysis ('ReMFPCA') for multivariate functional data whose variables might be observed over different dimensional domains. 'ReMFPCA' is an object-oriented interface leveraging the extensibility and scalability of R6. It employs a parameter vector to control the smoothness of each functional variable. By incorporating smoothness constraints as penalty terms within a regularized optimization framework, 'ReMFPCA' generates smooth multivariate functional principal components, offering a concise and interpretable representation of the data. For detailed information on the methods and techniques used in 'ReMFPCA', please refer to Haghbin et al. (2023) <doi:10.48550/arXiv.2306.13980>.   "
  },
  {
    "id": 6636,
    "package_name": "ReadDIM",
    "title": "Read ESA SNAP Processed Raster Format in R",
    "description": "It helps you to read (.dim) images with CRS directly into R programming. One can import both Sentinel 1 and 2 images or any processed data with this software.",
    "version": "0.2.11",
    "maintainer": "Subhadip Datta <subhadipdatta007@gmail.com>",
    "author": "Subhadip Datta",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ReadDIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReadDIM Read ESA SNAP Processed Raster Format in R It helps you to read (.dim) images with CRS directly into R programming. One can import both Sentinel 1 and 2 images or any processed data with this software.  "
  },
  {
    "id": 6721,
    "package_name": "RiskMap",
    "title": "Geostatistical Modeling of Spatially Referenced Data",
    "description": "Geostatistical analysis of continuous and count data.\n  Implements stationary Gaussian processes with Mat\u00e9rn correlation for spatial prediction,\n  as described in Diggle and Giorgi (2019, ISBN: 978-1-138-06102-7).",
    "version": "1.0.0",
    "maintainer": "Emanuele Giorgi <e.giorgi@bham.ac.uk>",
    "author": "Emanuele Giorgi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0640-181X>),\n  Claudio Fronterre [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6723-9727>)",
    "url": "https://claudiofronterre.github.io/RiskMap/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RiskMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RiskMap Geostatistical Modeling of Spatially Referenced Data Geostatistical analysis of continuous and count data.\n  Implements stationary Gaussian processes with Mat\u00e9rn correlation for spatial prediction,\n  as described in Diggle and Giorgi (2019, ISBN: 978-1-138-06102-7).  "
  },
  {
    "id": 6734,
    "package_name": "Rlibkdv",
    "title": "A Versatile Kernel Density Visualization Library for Geospatial\nAnalytics (Heatmap)",
    "description": "Unlock the power of large-scale geospatial analysis, \n    quickly generate high-resolution kernel density visualizations, \n    supporting advanced analysis tasks such as bandwidth-tuning and spatiotemporal analysis. \n    Regardless of the size of your dataset, our library delivers efficient and accurate results.\n    Tsz Nam Chan, Leong Hou U, Byron Choi, Jianliang Xu, Reynold Cheng (2023) <doi:10.1145/3555041.3589401>.\n    Tsz Nam Chan, Rui Zang, Pak Lon Ip, Leong Hou U, Jianliang Xu (2023) <doi:10.1145/3555041.3589711>.\n    Tsz Nam Chan, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.1145/3514221.3517823>.\n    Tsz Nam Chan, Pak Lon Ip, Kaiyan Zhao, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.14778/3554821.3554855>.\n    Tsz Nam Chan, Pak Lon Ip, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.14778/3503585.3503591>.\n    Tsz Nam Chan, Pak Lon Ip, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.14778/3494124.3494135>.\n    Tsz Nam Chan, Pak Lon Ip, Leong Hou U, Weng Hou Tong, Shivansh Mittal, Ye Li, Reynold Cheng (2021) <doi:10.14778/3476311.3476312>.\n    Tsz Nam Chan, Zhe Li, Leong Hou U, Jianliang Xu, Reynold Cheng (2021) <doi:10.14778/3461535.3461540>.\n    Tsz Nam Chan, Reynold Cheng, Man Lung Yiu (2020) <doi:10.1145/3318464.3380561>.\n    Tsz Nam Chan, Leong Hou U, Reynold Cheng, Man Lung Yiu, Shivansh Mittal (2020) <doi:10.1109/TKDE.2020.3018376>.\n    Tsz Nam Chan, Man Lung Yiu, Leong Hou U (2019) <doi:10.1109/ICDE.2019.00055>.",
    "version": "1.1",
    "maintainer": "Bojian Zhu <bjzhu999@gmail.com>",
    "author": "Bojian Zhu [cre, aut],\n  Tsz Nam Chan [aut],\n  Leong Hou U [aut],\n  Dingming Wu [aut],\n  Jianliang Xu [aut],\n  LibKDV Group [cph]",
    "url": "https://github.com/bojianzhu/Rlibkdv",
    "bug_reports": "https://github.com/bojianzhu/Rlibkdv/issues",
    "repository": "https://cran.r-project.org/package=Rlibkdv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rlibkdv A Versatile Kernel Density Visualization Library for Geospatial\nAnalytics (Heatmap) Unlock the power of large-scale geospatial analysis, \n    quickly generate high-resolution kernel density visualizations, \n    supporting advanced analysis tasks such as bandwidth-tuning and spatiotemporal analysis. \n    Regardless of the size of your dataset, our library delivers efficient and accurate results.\n    Tsz Nam Chan, Leong Hou U, Byron Choi, Jianliang Xu, Reynold Cheng (2023) <doi:10.1145/3555041.3589401>.\n    Tsz Nam Chan, Rui Zang, Pak Lon Ip, Leong Hou U, Jianliang Xu (2023) <doi:10.1145/3555041.3589711>.\n    Tsz Nam Chan, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.1145/3514221.3517823>.\n    Tsz Nam Chan, Pak Lon Ip, Kaiyan Zhao, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.14778/3554821.3554855>.\n    Tsz Nam Chan, Pak Lon Ip, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.14778/3503585.3503591>.\n    Tsz Nam Chan, Pak Lon Ip, Leong Hou U, Byron Choi, Jianliang Xu (2022) <doi:10.14778/3494124.3494135>.\n    Tsz Nam Chan, Pak Lon Ip, Leong Hou U, Weng Hou Tong, Shivansh Mittal, Ye Li, Reynold Cheng (2021) <doi:10.14778/3476311.3476312>.\n    Tsz Nam Chan, Zhe Li, Leong Hou U, Jianliang Xu, Reynold Cheng (2021) <doi:10.14778/3461535.3461540>.\n    Tsz Nam Chan, Reynold Cheng, Man Lung Yiu (2020) <doi:10.1145/3318464.3380561>.\n    Tsz Nam Chan, Leong Hou U, Reynold Cheng, Man Lung Yiu, Shivansh Mittal (2020) <doi:10.1109/TKDE.2020.3018376>.\n    Tsz Nam Chan, Man Lung Yiu, Leong Hou U (2019) <doi:10.1109/ICDE.2019.00055>.  "
  },
  {
    "id": 6782,
    "package_name": "RobustPrediction",
    "title": "Robust Tuning and Training for Cross-Source Prediction",
    "description": "Provides robust parameter tuning and model training for predictive models applied across data sources where the data distribution varies slightly from source to source. This package implements three primary tuning methods: cross-validation-based internal tuning, external tuning, and the 'RobustTuneC' method. External tuning includes a conservative option where parameters are tuned internally on the training data and validating on an external dataset, providing a slightly pessimistic estimate. It supports Lasso, Ridge, Random Forest, Boosting, and Support Vector Machine classifiers. Currently, only binary classification is supported. The response variable must be the first column of the dataset and a factor with exactly two levels. The tuning methods are based on the paper by Nicole Ellenbach, Anne-Laure Boulesteix, Bernd Bischl, Kristian Unger, and Roman Hornung (2021) \"Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning\" <doi:10.1007/s00357-020-09368-z>.",
    "version": "0.1.7",
    "maintainer": "Yuting He <yutingh19@gmail.com>",
    "author": "Yuting He [aut, cre],\n  Nicole Ellenbach [ctb],\n  Roman Hornung [ctb]",
    "url": "https://github.com/Yuting-He/RobustPrediction",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobustPrediction",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobustPrediction Robust Tuning and Training for Cross-Source Prediction Provides robust parameter tuning and model training for predictive models applied across data sources where the data distribution varies slightly from source to source. This package implements three primary tuning methods: cross-validation-based internal tuning, external tuning, and the 'RobustTuneC' method. External tuning includes a conservative option where parameters are tuned internally on the training data and validating on an external dataset, providing a slightly pessimistic estimate. It supports Lasso, Ridge, Random Forest, Boosting, and Support Vector Machine classifiers. Currently, only binary classification is supported. The response variable must be the first column of the dataset and a factor with exactly two levels. The tuning methods are based on the paper by Nicole Ellenbach, Anne-Laure Boulesteix, Bernd Bischl, Kristian Unger, and Roman Hornung (2021) \"Improved Outcome Prediction Across Data Sources Through Robust Parameter Tuning\" <doi:10.1007/s00357-020-09368-z>.  "
  },
  {
    "id": 6816,
    "package_name": "Rsagacmd",
    "title": "Linking R with the Open-Source 'SAGA-GIS' Software",
    "description": "Provides an R scripting interface to the open-source 'SAGA-GIS' \n    (System for Automated Geoscientific Analyses Geographical Information\n    System) software. 'Rsagacmd' dynamically generates R functions for every\n    'SAGA-GIS' geoprocessing tool based on the user's currently installed\n    'SAGA-GIS' version. These functions are contained within an S3 object\n    and are accessed as a named list of libraries and tools. This structure\n    facilitates an easier scripting experience by organizing the large number\n    of 'SAGA-GIS' geoprocessing tools (>700) by their respective library.\n    Interactive scripting can fully take advantage of code autocompletion tools\n    (e.g. in 'RStudio'), allowing for each tools syntax to be quickly\n    recognized. Furthermore, the most common types of spatial data (via the\n    'terra', 'sp', and 'sf' packages) along with non-spatial data are\n    automatically passed from R to the 'SAGA-GIS' command line tool for\n    geoprocessing operations, and the results are loaded as the appropriate R\n    object. Outputs from individual 'SAGA-GIS' tools can also be chained using\n    pipes from the 'magrittr' and 'dplyr' packages to combine complex\n    geoprocessing operations together in a single statement. 'SAGA-GIS' is\n    available under a GPLv2 / LGPLv2 licence from\n    <https://sourceforge.net/projects/saga-gis/> including Windows x86/x64\n    and macOS binaries. SAGA-GIS is also included in Debian/Ubuntu default software\n    repositories. Rsagacmd has currently been tested on 'SAGA-GIS' versions\n    from 2.3.1 to 9.5.1 on Windows, Linux and macOS.",
    "version": "0.4.3",
    "maintainer": "Steven Pawley <dr.stevenpawley@gmail.com>",
    "author": "Steven Pawley [aut, cre]",
    "url": "https://stevenpawley.github.io/Rsagacmd/",
    "bug_reports": "https://github.com/stevenpawley/Rsagacmd/issues",
    "repository": "https://cran.r-project.org/package=Rsagacmd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rsagacmd Linking R with the Open-Source 'SAGA-GIS' Software Provides an R scripting interface to the open-source 'SAGA-GIS' \n    (System for Automated Geoscientific Analyses Geographical Information\n    System) software. 'Rsagacmd' dynamically generates R functions for every\n    'SAGA-GIS' geoprocessing tool based on the user's currently installed\n    'SAGA-GIS' version. These functions are contained within an S3 object\n    and are accessed as a named list of libraries and tools. This structure\n    facilitates an easier scripting experience by organizing the large number\n    of 'SAGA-GIS' geoprocessing tools (>700) by their respective library.\n    Interactive scripting can fully take advantage of code autocompletion tools\n    (e.g. in 'RStudio'), allowing for each tools syntax to be quickly\n    recognized. Furthermore, the most common types of spatial data (via the\n    'terra', 'sp', and 'sf' packages) along with non-spatial data are\n    automatically passed from R to the 'SAGA-GIS' command line tool for\n    geoprocessing operations, and the results are loaded as the appropriate R\n    object. Outputs from individual 'SAGA-GIS' tools can also be chained using\n    pipes from the 'magrittr' and 'dplyr' packages to combine complex\n    geoprocessing operations together in a single statement. 'SAGA-GIS' is\n    available under a GPLv2 / LGPLv2 licence from\n    <https://sourceforge.net/projects/saga-gis/> including Windows x86/x64\n    and macOS binaries. SAGA-GIS is also included in Debian/Ubuntu default software\n    repositories. Rsagacmd has currently been tested on 'SAGA-GIS' versions\n    from 2.3.1 to 9.5.1 on Windows, Linux and macOS.  "
  },
  {
    "id": 6837,
    "package_name": "Rtropical",
    "title": "Data Analysis Tools over Space of Phylogenetic Trees Using\nTropical Geometry",
    "description": "Process phylogenetic trees with tropical support vector machine and principal component analysis defined with tropical geometry. Details about tropical support vector machine are available in : Tang, X., Wang, H. & Yoshida, R. (2020) <arXiv:2003.00677>. Details about tropical principle component analysis are available in : Page, R., Yoshida, R. & Zhang L. (2020) <doi:10.1093/bioinformatics/btaa564> and Yoshida, R., Zhang, L. & Zhang, X. (2019) <doi:10.1007/s11538-018-0493-4>.",
    "version": "1.2.1",
    "maintainer": "Houjie Wang <wanghoujie6688@gmail.com>",
    "author": "Houjie Wang [aut, cre],\n  Kaizhang Wang [aut],\n  Grady Weyenberg [aut],\n  Xiaoxian Tang [aut],\n  Ruriko Yoshida [aut]",
    "url": "https://github.com/HoujieWang/Rtropical",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rtropical",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rtropical Data Analysis Tools over Space of Phylogenetic Trees Using\nTropical Geometry Process phylogenetic trees with tropical support vector machine and principal component analysis defined with tropical geometry. Details about tropical support vector machine are available in : Tang, X., Wang, H. & Yoshida, R. (2020) <arXiv:2003.00677>. Details about tropical principle component analysis are available in : Page, R., Yoshida, R. & Zhang L. (2020) <doi:10.1093/bioinformatics/btaa564> and Yoshida, R., Zhang, L. & Zhang, X. (2019) <doi:10.1007/s11538-018-0493-4>.  "
  },
  {
    "id": 6934,
    "package_name": "SDPDmod",
    "title": "Spatial Dynamic Panel Data Modeling",
    "description": "Spatial model calculation for static and dynamic panel data models, weights matrix creation and  Bayesian model comparison.\n  Bayesian model comparison methods were described by 'LeSage' (2014) <doi:10.1016/j.spasta.2014.02.002>.\n  The 'Lee'-'Yu' transformation approach is described in 'Yu', 'De Jong' and 'Lee' (2008) <doi:10.1016/j.jeconom.2008.08.002>, 'Lee' and 'Yu' (2010) <doi:10.1016/j.jeconom.2009.08.001> and 'Lee' and 'Yu' (2010) <doi:10.1017/S0266466609100099>.",
    "version": "0.0.7",
    "maintainer": "Rozeta Simonovska <simonovska.r@gmail.com>",
    "author": "Rozeta Simonovska [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/RozetaSimonovska/SDPDmod/issues/",
    "repository": "https://cran.r-project.org/package=SDPDmod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SDPDmod Spatial Dynamic Panel Data Modeling Spatial model calculation for static and dynamic panel data models, weights matrix creation and  Bayesian model comparison.\n  Bayesian model comparison methods were described by 'LeSage' (2014) <doi:10.1016/j.spasta.2014.02.002>.\n  The 'Lee'-'Yu' transformation approach is described in 'Yu', 'De Jong' and 'Lee' (2008) <doi:10.1016/j.jeconom.2008.08.002>, 'Lee' and 'Yu' (2010) <doi:10.1016/j.jeconom.2009.08.001> and 'Lee' and 'Yu' (2010) <doi:10.1017/S0266466609100099>.  "
  },
  {
    "id": 6941,
    "package_name": "SEAHORS",
    "title": "Spatial Exploration of ArcHaeological Objects in R Shiny",
    "description": "An R 'Shiny' application dedicated to the intra-site spatial analysis of piece-plotted archaeological remains, making the two and three-dimensional spatial exploration of archaeological data as user-friendly as possible.  Documentation about 'SEAHORS' is provided by the vignette included in this package and by the companion scientific paper: Royer, Discamps, Plutniak, Thomas (2023, PCI Archaeology, <doi:10.5281/zenodo.7674698>).",
    "version": "1.9.0",
    "maintainer": "Sebastien Plutniak <sebastien.plutniak@posteo.net>",
    "author": "Aurelien Royer [aut] (ORCID: <https://orcid.org/0000-0002-0139-8765>),\n  Sebastien Plutniak [cre] (ORCID:\n    <https://orcid.org/0000-0002-6674-3806>),\n  Emmanuel Discamps [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2464-0761>),\n  Marc Thomas [ctb] (ORCID: <https://orcid.org/0000-0002-8160-1910>)",
    "url": "https://github.com/AurelienRoyer/SEAHORS",
    "bug_reports": "https://github.com/AurelienRoyer/SEAHORS/issues",
    "repository": "https://cran.r-project.org/package=SEAHORS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SEAHORS Spatial Exploration of ArcHaeological Objects in R Shiny An R 'Shiny' application dedicated to the intra-site spatial analysis of piece-plotted archaeological remains, making the two and three-dimensional spatial exploration of archaeological data as user-friendly as possible.  Documentation about 'SEAHORS' is provided by the vignette included in this package and by the companion scientific paper: Royer, Discamps, Plutniak, Thomas (2023, PCI Archaeology, <doi:10.5281/zenodo.7674698>).  "
  },
  {
    "id": 6955,
    "package_name": "SESraster",
    "title": "Raster Randomization for Null Hypothesis Testing",
    "description": "Randomization of presence/absence species distribution raster\n    data with or without including spatial structure for calculating\n    standardized effect sizes and testing null hypothesis. The\n    randomization algorithms are based on classical algorithms for\n    matrices (Gotelli 2000, <doi:10.2307/177478>) implemented for raster\n    data.",
    "version": "0.7.1",
    "maintainer": "Neander Marcel Heming <neanderh@yahoo.com.br>",
    "author": "Neander Marcel Heming [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2461-5045>),\n  Fl\u00e1vio M. M. Mota [aut] (ORCID:\n    <https://orcid.org/0000-0002-0308-7151>),\n  Gabriela Alves-Ferreira [aut] (ORCID:\n    <https://orcid.org/0000-0001-5661-3381>)",
    "url": "https://CRAN.R-project.org/package=SESraster,\nhttps://github.com/HemingNM/SESraster,\nhttps://hemingnm.github.io/SESraster/",
    "bug_reports": "https://github.com/HemingNM/SESraster/issues",
    "repository": "https://cran.r-project.org/package=SESraster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SESraster Raster Randomization for Null Hypothesis Testing Randomization of presence/absence species distribution raster\n    data with or without including spatial structure for calculating\n    standardized effect sizes and testing null hypothesis. The\n    randomization algorithms are based on classical algorithms for\n    matrices (Gotelli 2000, <doi:10.2307/177478>) implemented for raster\n    data.  "
  },
  {
    "id": 6965,
    "package_name": "SGB",
    "title": "Simplicial Generalized Beta Regression",
    "description": "Main properties and regression procedures using a generalization of the Dirichlet distribution called Simplicial Generalized Beta distribution. It is a new distribution on the simplex (i.e. on the space of compositions or positive vectors with sum of components equal to 1). The Dirichlet distribution can be constructed from a random vector of independent Gamma variables divided by their sum. The SGB follows the same construction with generalized Gamma instead of Gamma variables. The Dirichlet exponents are supplemented by an overall shape parameter and a vector of scales. The scale vector is itself a composition and can be modeled with auxiliary variables through a log-ratio transformation. Graf, M. (2017, ISBN: 978-84-947240-0-8). See also the vignette enclosed in the package.",
    "version": "1.0.1.1",
    "maintainer": "Monique Graf <monique.p.n.graf@bluewin.ch>",
    "author": "Monique Graf ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SGB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SGB Simplicial Generalized Beta Regression Main properties and regression procedures using a generalization of the Dirichlet distribution called Simplicial Generalized Beta distribution. It is a new distribution on the simplex (i.e. on the space of compositions or positive vectors with sum of components equal to 1). The Dirichlet distribution can be constructed from a random vector of independent Gamma variables divided by their sum. The SGB follows the same construction with generalized Gamma instead of Gamma variables. The Dirichlet exponents are supplemented by an overall shape parameter and a vector of scales. The scale vector is itself a composition and can be modeled with auxiliary variables through a log-ratio transformation. Graf, M. (2017, ISBN: 978-84-947240-0-8). See also the vignette enclosed in the package.  "
  },
  {
    "id": 7003,
    "package_name": "SIT",
    "title": "Association Measurement Through Sliced Independence Test (SIT)",
    "description": "Computes the sit coefficient between two vectors x and y, \n  possibly all paired coefficients for a matrix. The reference for the methods implemented here is \n  Zhang, Yilin, Canyi Chen, and Liping Zhu. 2022. \"Sliced Independence Test.\" Statistica Sinica. <doi:10.5705/ss.202021.0203>. \n  This package incorporates the Galton peas example.",
    "version": "0.1.1",
    "maintainer": "Canyi Chen <cychen.stats@outlook.com>",
    "author": "Canyi Chen [aut, cre] (ORCID: <https://orcid.org/0000-0002-0673-5812>)",
    "url": "https://github.com/canyi-chen/SIT",
    "bug_reports": "https://github.com/canyi-chen/SIT/issues",
    "repository": "https://cran.r-project.org/package=SIT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIT Association Measurement Through Sliced Independence Test (SIT) Computes the sit coefficient between two vectors x and y, \n  possibly all paired coefficients for a matrix. The reference for the methods implemented here is \n  Zhang, Yilin, Canyi Chen, and Liping Zhu. 2022. \"Sliced Independence Test.\" Statistica Sinica. <doi:10.5705/ss.202021.0203>. \n  This package incorporates the Galton peas example.  "
  },
  {
    "id": 7004,
    "package_name": "SITH",
    "title": "A Spatial Model of Intra-Tumor Heterogeneity",
    "description": "Implements a three-dimensional stochastic model of cancer growth and mutation similar to the one described in Waclaw et al. (2015) <doi:10.1038/nature14971>. Allows for interactive 3D visualizations of the simulated tumor. Provides a comprehensive summary of the spatial distribution of mutants within the tumor. Contains functions which create synthetic sequencing datasets from the generated tumor. ",
    "version": "1.1.0",
    "maintainer": "Phillip B. Nicol <philnicol740@gmail.com>",
    "author": "Phillip B. Nicol",
    "url": "https://github.com/phillipnicol/SITH",
    "bug_reports": "https://github.com/phillipnicol/SITH/issues",
    "repository": "https://cran.r-project.org/package=SITH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SITH A Spatial Model of Intra-Tumor Heterogeneity Implements a three-dimensional stochastic model of cancer growth and mutation similar to the one described in Waclaw et al. (2015) <doi:10.1038/nature14971>. Allows for interactive 3D visualizations of the simulated tumor. Provides a comprehensive summary of the spatial distribution of mutants within the tumor. Contains functions which create synthetic sequencing datasets from the generated tumor.   "
  },
  {
    "id": 7032,
    "package_name": "SMFilter",
    "title": "Filtering Algorithms for the State Space Models on the Stiefel\nManifold",
    "description": "Provides the filtering algorithms for the state space models on the Stiefel manifold as well as the corresponding sampling algorithms for uniform, vector Langevin-Bingham and matrix Langevin-Bingham distributions on the Stiefel manifold.",
    "version": "1.0.3",
    "maintainer": "Yukai Yang <yukai.yang@statistik.uu.se>",
    "author": "Yukai Yang [aut, cre]",
    "url": "https://github.com/yukai-yang/SMFilter",
    "bug_reports": "https://github.com/yukai-yang/SMFilter/issues",
    "repository": "https://cran.r-project.org/package=SMFilter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SMFilter Filtering Algorithms for the State Space Models on the Stiefel\nManifold Provides the filtering algorithms for the state space models on the Stiefel manifold as well as the corresponding sampling algorithms for uniform, vector Langevin-Bingham and matrix Langevin-Bingham distributions on the Stiefel manifold.  "
  },
  {
    "id": 7050,
    "package_name": "SNFtool",
    "title": "Similarity Network Fusion",
    "description": "Similarity Network Fusion takes multiple views of a network and fuses them together to construct an overall status matrix. The input to our algorithm can be feature vectors, pairwise distances, or pairwise similarities. The learned status matrix can then be used for retrieval, clustering, and classification.",
    "version": "2.3.1",
    "maintainer": "Benjamin Brew <goldenberglab@gmail.com>",
    "author": "Bo Wang, Aziz Mezlini, Feyyaz Demir, Marc Fiume, Zhuowen Tu, Michael Brudno, Benjamin Haibe-Kains, Anna Goldenberg",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SNFtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SNFtool Similarity Network Fusion Similarity Network Fusion takes multiple views of a network and fuses them together to construct an overall status matrix. The input to our algorithm can be feature vectors, pairwise distances, or pairwise similarities. The learned status matrix can then be used for retrieval, clustering, and classification.  "
  },
  {
    "id": 7082,
    "package_name": "SPCAvRP",
    "title": "Sparse Principal Component Analysis via Random Projections\n(SPCAvRP)",
    "description": "Implements the SPCAvRP algorithm, developed and analysed in \"Sparse principal component analysis via random projections\" Gataric, M., Wang, T. and Samworth, R. J. (2018) <arXiv:1712.05630>. The algorithm is based on the aggregation of eigenvector information from carefully-selected random projections of the sample covariance matrix.",
    "version": "0.4",
    "maintainer": "Milana Gataric <m.gataric@statslab.cam.ac.uk>",
    "author": "Milana Gataric, Tengyao Wang and Richard J. Samworth",
    "url": "https://arxiv.org/abs/1712.05630",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPCAvRP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPCAvRP Sparse Principal Component Analysis via Random Projections\n(SPCAvRP) Implements the SPCAvRP algorithm, developed and analysed in \"Sparse principal component analysis via random projections\" Gataric, M., Wang, T. and Samworth, R. J. (2018) <arXiv:1712.05630>. The algorithm is based on the aggregation of eigenvector information from carefully-selected random projections of the sample covariance matrix.  "
  },
  {
    "id": 7087,
    "package_name": "SPEDInstabR",
    "title": "Estimation of the Relative Importance of Factors Affecting\nSpecies Distribution Based on Stability Concept",
    "description": "From output files obtained from the software 'ModestR', the relative contribution of factors to explain species distribution is depicted using several plots. A global geographic raster file for each environmental variable may be also obtained with the mean relative contribution, considering all species present in each raster cell, of the factor to explain species distribution. Finally, for each variable it is also possible to compare the frequencies of any variable obtained in the cells where the species is present with the frequencies of the same variable in the cells of the extent.",
    "version": "2.2",
    "maintainer": "Castor Guisande Gonzalez <castor@uvigo.es>",
    "author": "Castor Guisande Gonzalez",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPEDInstabR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPEDInstabR Estimation of the Relative Importance of Factors Affecting\nSpecies Distribution Based on Stability Concept From output files obtained from the software 'ModestR', the relative contribution of factors to explain species distribution is depicted using several plots. A global geographic raster file for each environmental variable may be also obtained with the mean relative contribution, considering all species present in each raster cell, of the factor to explain species distribution. Finally, for each variable it is also possible to compare the frequencies of any variable obtained in the cells where the species is present with the frequencies of the same variable in the cells of the extent.  "
  },
  {
    "id": 7089,
    "package_name": "SPEV",
    "title": "Unsmoothed and Smoothed Penalized PCA using Nesterov Smoothing",
    "description": "We provide functionality to implement penalized PCA with an option to smooth the objective function using Nesterov smoothing. Two functions are available to compute a user-specified number of eigenvectors. The function unsmoothed_penalized_EV() computes a penalized PCA without smoothing and has three parameters (the input matrix, the Lasso penalty, and the number of desired eigenvectors). The function smoothed_penalized_EV() computes a smoothed penalized PCA using the same parameters and additionally requires the specification of a smoothing parameter. Both functions return a matrix having the desired eigenvectors as columns.",
    "version": "1.0.0",
    "maintainer": "Rebecca Hurwitz <rebeccahurwitz@hsph.harvard.edu>",
    "author": "Rebecca Hurwitz [aut, cre],\n  Georg Hahn [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPEV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPEV Unsmoothed and Smoothed Penalized PCA using Nesterov Smoothing We provide functionality to implement penalized PCA with an option to smooth the objective function using Nesterov smoothing. Two functions are available to compute a user-specified number of eigenvectors. The function unsmoothed_penalized_EV() computes a penalized PCA without smoothing and has three parameters (the input matrix, the Lasso penalty, and the number of desired eigenvectors). The function smoothed_penalized_EV() computes a smoothed penalized PCA using the same parameters and additionally requires the specification of a smoothing parameter. Both functions return a matrix having the desired eigenvectors as columns.  "
  },
  {
    "id": 7097,
    "package_name": "SPREDA",
    "title": "Statistical Package for Reliability Data Analysis",
    "description": "The Statistical Package for REliability Data Analysis (SPREDA) implements recently-developed statistical methods for the analysis of reliability data. Modern technological developments, such as sensors and smart chips, allow us to dynamically track product/system usage as well as other environmental variables, such as temperature and humidity. We refer to these variables as dynamic covariates. The package contains functions for the analysis of time-to-event data with dynamic covariates and degradation data with dynamic covariates. The package also contains functions that can be used for analyzing time-to-event data with right censoring, and with left truncation and right censoring. Financial support from NSF and DuPont are acknowledged.  ",
    "version": "1.2",
    "maintainer": "Yili Hong <yilihong@vt.edu>",
    "author": "Yili Hong [aut, cre],\n  Yimeng Xie [aut],\n  Zhibing Xu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPREDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPREDA Statistical Package for Reliability Data Analysis The Statistical Package for REliability Data Analysis (SPREDA) implements recently-developed statistical methods for the analysis of reliability data. Modern technological developments, such as sensors and smart chips, allow us to dynamically track product/system usage as well as other environmental variables, such as temperature and humidity. We refer to these variables as dynamic covariates. The package contains functions for the analysis of time-to-event data with dynamic covariates and degradation data with dynamic covariates. The package also contains functions that can be used for analyzing time-to-event data with right censoring, and with left truncation and right censoring. Financial support from NSF and DuPont are acknowledged.    "
  },
  {
    "id": 7116,
    "package_name": "SRTtools",
    "title": "Adjust Srt File to Get Better Experience when Watching Movie",
    "description": "Srt file is a common subtitle format for videos, it contains subtitle and when the subtitle showed.\n    This package is for align time of srt file, and also change color, style and position of subtitle in videos,\n\tthe srt file will be read as a vector into R, and can be write into srt file after modified using this package.",
    "version": "1.2.0",
    "maintainer": "Jim Chen <jim71183@gmail.com>",
    "author": "Jim Chen [aut, cre]",
    "url": "https://github.com/ChiHangChen/SRTtools",
    "bug_reports": "https://github.com/ChiHangChen/SRTtools/issues",
    "repository": "https://cran.r-project.org/package=SRTtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SRTtools Adjust Srt File to Get Better Experience when Watching Movie Srt file is a common subtitle format for videos, it contains subtitle and when the subtitle showed.\n    This package is for align time of srt file, and also change color, style and position of subtitle in videos,\n\tthe srt file will be read as a vector into R, and can be write into srt file after modified using this package.  "
  },
  {
    "id": 7124,
    "package_name": "SSIMmap",
    "title": "The Structural Similarity Index Measure for Maps",
    "description": "Extends the classical SSIM method proposed by 'Wang', 'Bovik', 'Sheikh', and 'Simoncelli'(2004) <doi:10.1109/TIP.2003.819861>. \n  for irregular lattice-based maps and raster images.\n  The geographical SSIM method incorporates well-developed 'geographically weighted summary statistics'('Brunsdon', 'Fotheringham' and 'Charlton' 2002) <doi:10.1016/S0198-9715(01)00009-6> \n  with an adaptive bandwidth kernel function for irregular lattice-based maps.",
    "version": "0.1.1",
    "maintainer": "Hui Jeong (Hailyee) Ha <hha24@uwo.ca>",
    "author": "Hui Jeong (Hailyee) Ha [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5229-4528>),\n  Jed Long [aut] (ORCID: <https://orcid.org/0000-0003-3961-3085>)",
    "url": "https://github.com/Hailyee-Ha/SSIMmap",
    "bug_reports": "https://github.com/Hailyee-Ha/SSIMmap/issues",
    "repository": "https://cran.r-project.org/package=SSIMmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSIMmap The Structural Similarity Index Measure for Maps Extends the classical SSIM method proposed by 'Wang', 'Bovik', 'Sheikh', and 'Simoncelli'(2004) <doi:10.1109/TIP.2003.819861>. \n  for irregular lattice-based maps and raster images.\n  The geographical SSIM method incorporates well-developed 'geographically weighted summary statistics'('Brunsdon', 'Fotheringham' and 'Charlton' 2002) <doi:10.1016/S0198-9715(01)00009-6> \n  with an adaptive bandwidth kernel function for irregular lattice-based maps.  "
  },
  {
    "id": 7128,
    "package_name": "SSN2",
    "title": "Spatial Modeling on Stream Networks",
    "description": "Spatial statistical modeling and prediction for data on stream networks, including models based on in-stream distance (Ver Hoef, J.M. and Peterson, E.E., (2010) <DOI:10.1198/jasa.2009.ap08248>.) Models are created using moving average constructions. Spatial linear models, including explanatory variables, can be fit with (restricted) maximum likelihood.  Mapping and other graphical functions are included. ",
    "version": "0.4.0",
    "maintainer": "Michael Dumelle <Dumelle.Michael@epa.gov>",
    "author": "Michael Dumelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3393-5529>),\n  Jay M. Ver Hoef [aut],\n  Erin Peterson [aut],\n  Alan Pearse [ctb],\n  Dan Isaak [ctb],\n  Ryan A. Hill [ctb],\n  Marc Weber [ctb]",
    "url": "https://usepa.github.io/SSN2/",
    "bug_reports": "https://github.com/USEPA/SSN2/issues",
    "repository": "https://cran.r-project.org/package=SSN2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSN2 Spatial Modeling on Stream Networks Spatial statistical modeling and prediction for data on stream networks, including models based on in-stream distance (Ver Hoef, J.M. and Peterson, E.E., (2010) <DOI:10.1198/jasa.2009.ap08248>.) Models are created using moving average constructions. Spatial linear models, including explanatory variables, can be fit with (restricted) maximum likelihood.  Mapping and other graphical functions are included.   "
  },
  {
    "id": 7129,
    "package_name": "SSNbayes",
    "title": "Bayesian Spatio-Temporal Analysis in Stream Networks",
    "description": "Fits Bayesian spatio-temporal models and makes predictions on stream networks using the approach by Santos-Fernandez, Edgar, et al. (2022).\"Bayesian spatio-temporal models for stream networks\". <arXiv:2103.03538>. In these models, spatial dependence is captured using stream distance and flow connectivity, while temporal autocorrelation is modelled using vector autoregression methods. ",
    "version": "0.0.3",
    "maintainer": "Edgar Santos-Fernandez <santosfe@qut.edu.au>",
    "author": "Edgar Santos-Fernandez [aut, cre, cph]",
    "url": "https://github.com/EdgarSantos-Fernandez/SSNbayes",
    "bug_reports": "https://github.com/EdgarSantos-Fernandez/SSNbayes/issues",
    "repository": "https://cran.r-project.org/package=SSNbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSNbayes Bayesian Spatio-Temporal Analysis in Stream Networks Fits Bayesian spatio-temporal models and makes predictions on stream networks using the approach by Santos-Fernandez, Edgar, et al. (2022).\"Bayesian spatio-temporal models for stream networks\". <arXiv:2103.03538>. In these models, spatial dependence is captured using stream distance and flow connectivity, while temporal autocorrelation is modelled using vector autoregression methods.   "
  },
  {
    "id": 7131,
    "package_name": "SSOSVM",
    "title": "Stream Suitable Online Support Vector Machines",
    "description": "Soft-margin support vector machines (SVMs) are a common class of classification models. The training of SVMs usually requires that the data be available all at once in a single batch, however the Stochastic majorization-minimization (SMM) algorithm framework allows for the training of SVMs on streamed data instead Nguyen, Jones & McLachlan(2018)<doi:10.1007/s42081-018-0001-y>. This package utilizes the SMM framework to provide functions for training SVMs with hinge loss, squared-hinge loss, and logistic loss.",
    "version": "0.2.2",
    "maintainer": "Andrew Thomas Jones <andrewthomasjones@gmail.com>",
    "author": "Andrew Thomas Jones [aut, cre],\n  Hien Duy Nguyen [aut],\n  Geoffrey J. McLachlan [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SSOSVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSOSVM Stream Suitable Online Support Vector Machines Soft-margin support vector machines (SVMs) are a common class of classification models. The training of SVMs usually requires that the data be available all at once in a single batch, however the Stochastic majorization-minimization (SMM) algorithm framework allows for the training of SVMs on streamed data instead Nguyen, Jones & McLachlan(2018)<doi:10.1007/s42081-018-0001-y>. This package utilizes the SMM framework to provide functions for training SVMs with hinge loss, squared-hinge loss, and logistic loss.  "
  },
  {
    "id": 7146,
    "package_name": "STB",
    "title": "Simultaneous Tolerance Bounds",
    "description": "Provides an implementation of simultaneous tolerance bounds (STB), useful for checking whether a numeric vector fits to a hypothetical null-distribution or not.\n             Furthermore, there are functions for computing STB (bands, intervals) for random variates of linear mixed models fitted with package 'VCA'. All kinds of, possibly transformed \n             (studentized, standardized, Pearson-type transformed) random variates (residuals, random effects), can be assessed employing STB-methodology. ",
    "version": "0.6.6",
    "maintainer": "Andre Schuetzenmeister <andre.schuetzenmeister@roche.com>",
    "author": "Andre Schuetzenmeister [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=STB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "STB Simultaneous Tolerance Bounds Provides an implementation of simultaneous tolerance bounds (STB), useful for checking whether a numeric vector fits to a hypothetical null-distribution or not.\n             Furthermore, there are functions for computing STB (bands, intervals) for random variates of linear mixed models fitted with package 'VCA'. All kinds of, possibly transformed \n             (studentized, standardized, Pearson-type transformed) random variates (residuals, random effects), can be assessed employing STB-methodology.   "
  },
  {
    "id": 7160,
    "package_name": "SUMMER",
    "title": "Small-Area-Estimation Unit/Area Models and Methods for\nEstimation in R",
    "description": "Provides methods for spatial and spatio-temporal smoothing of demographic and health indicators using survey data, with particular focus on estimating and projecting under-five mortality rates, described in Mercer et al. (2015) <doi:10.1214/15-AOAS872>, Li et al. (2019) <doi:10.1371/journal.pone.0210645>, Wu et al. (DHS Spatial Analysis Reports No. 21, 2021), and Li et al. (2023) <doi:10.48550/arXiv.2007.05117>. ",
    "version": "2.0.0",
    "maintainer": "Zehang R Li <lizehang@gmail.com>",
    "author": "Zehang R Li [cre, aut],\n  Bryan D Martin [aut],\n  Yuan Hsiao [aut],\n  Jessica Godwin [aut],\n  John Paige [aut],\n  Peter Gao [aut],\n  Jon Wakefield [aut],\n  Samuel J Clark [aut],\n  Geir-Arne Fuglstad [aut],\n  Andrea Riebler [aut]",
    "url": "https://github.com/richardli/SUMMER,\nhttps://richardli.github.io/SUMMER/",
    "bug_reports": "https://github.com/richardli/SUMMER/issues",
    "repository": "https://cran.r-project.org/package=SUMMER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SUMMER Small-Area-Estimation Unit/Area Models and Methods for\nEstimation in R Provides methods for spatial and spatio-temporal smoothing of demographic and health indicators using survey data, with particular focus on estimating and projecting under-five mortality rates, described in Mercer et al. (2015) <doi:10.1214/15-AOAS872>, Li et al. (2019) <doi:10.1371/journal.pone.0210645>, Wu et al. (DHS Spatial Analysis Reports No. 21, 2021), and Li et al. (2023) <doi:10.48550/arXiv.2007.05117>.   "
  },
  {
    "id": 7169,
    "package_name": "SVMMaj",
    "title": "Implementation of the SVM-Maj Algorithm",
    "description": "\n  Implements the SVM-Maj algorithm to train data with support vector machine\n  <doi:10.1007/s11634-008-0020-9>.\n  This algorithm uses two efficient updates, one for linear kernel and one \n  for the nonlinear kernel.",
    "version": "0.2.9.3",
    "maintainer": "Hoksan Yip <hoksan@gmail.com>",
    "author": "Hoksan Yip [aut, cre],\n  Patrick J.F. Groenen [aut],\n  Georgi Nalbantov [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SVMMaj",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SVMMaj Implementation of the SVM-Maj Algorithm \n  Implements the SVM-Maj algorithm to train data with support vector machine\n  <doi:10.1007/s11634-008-0020-9>.\n  This algorithm uses two efficient updates, one for linear kernel and one \n  for the nonlinear kernel.  "
  },
  {
    "id": 7176,
    "package_name": "SailoR",
    "title": "An Extension of the Taylor Diagram to Two-Dimensional Vector\nData",
    "description": "A new diagram for the verification of vector variables (wind, current, etc) generated by multiple models against a set of observations is presented in this package. It has been designed as a generalization of the Taylor diagram to two dimensional quantities. It is based on the analysis of the two-dimensional structure of the mean squared error matrix between model and observations. The matrix is divided into the part corresponding to the relative rotation and the bias of the empirical orthogonal functions of the data. The full set of diagnostics produced by the analysis of the errors between model and observational vector datasets comprises the errors in the means, the analysis of the total variance of both datasets, the rotation matrix corresponding to the principal components in observation and model, the angle of rotation of model-derived empirical orthogonal functions respect to the ones from observations, the standard deviation of model and observations, the root mean squared error between both datasets and the squared two-dimensional correlation coefficient. See the output of function UVError() in this package.",
    "version": "1.2",
    "maintainer": "Santos J. Gonz\u00e1lez-Roj\u00ed <santosjose.gonzalez@ehu.eus>",
    "author": "Jon S\u00e1enz [aut, cph] (ORCID: <https://orcid.org/0000-0002-5920-7570>),\n  Sheila Carreno-Madinabeitia [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4625-6178>),\n  Santos J. Gonz\u00e1lez-Roj\u00ed [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4737-0984>),\n  Ganix Esnaola [ctb, cph] (ORCID:\n    <https://orcid.org/0000-0001-9058-043X>),\n  Gabriel Ibarra-Berastegi [ctb, cph] (ORCID:\n    <https://orcid.org/0000-0001-8681-3755>),\n  Alain Ulazia [ctb, cph] (ORCID:\n    <https://orcid.org/0000-0002-4124-2853>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SailoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SailoR An Extension of the Taylor Diagram to Two-Dimensional Vector\nData A new diagram for the verification of vector variables (wind, current, etc) generated by multiple models against a set of observations is presented in this package. It has been designed as a generalization of the Taylor diagram to two dimensional quantities. It is based on the analysis of the two-dimensional structure of the mean squared error matrix between model and observations. The matrix is divided into the part corresponding to the relative rotation and the bias of the empirical orthogonal functions of the data. The full set of diagnostics produced by the analysis of the errors between model and observational vector datasets comprises the errors in the means, the analysis of the total variance of both datasets, the rotation matrix corresponding to the principal components in observation and model, the angle of rotation of model-derived empirical orthogonal functions respect to the ones from observations, the standard deviation of model and observations, the root mean squared error between both datasets and the squared two-dimensional correlation coefficient. See the output of function UVError() in this package.  "
  },
  {
    "id": 7248,
    "package_name": "ShapePattern",
    "title": "Tools for Analyzing Shapes and Patterns",
    "description": "This is an evolving and growing collection of tools for the quantification, assessment, and comparison of shape and pattern. This collection provides tools for: (1) the spatial decomposition of planar shapes using 'ShrinkShape' to incrementally shrink shapes to extinction while computing area, perimeter, and number of parts at each iteration of shrinking; the spectra of results are returned in graphic and tabular formats (Remmel 2015) <doi:10.1111/cag.12222>, (2) simulating landscape patterns, (3) provision of tools for estimating composition and configuration parameters from a categorical (binary) landscape map (grid) and then simulates a selected number of statistically similar landscapes. Class-focused pattern metrics are computed for each simulated map to produce empirical distributions against which statistical comparisons can be made. The code permits the analysis of single maps or pairs of maps (Remmel and Fortin 2013) <doi:10.1007/s10980-013-9905-x>, (4) counting the number of each first-order pattern element and converting that information into both frequency and empirical probability vectors (Remmel 2020) <doi:10.3390/e22040420>, and (5) computing the porosity of raster patches <doi:10.3390/su10103413>. NOTE: This is a consolidation of existing packages ('PatternClass', 'ShapePattern') to begin warehousing all shape and pattern code in a common package. Additional utility tools for handling data are provided and this package will be added to as more tools are created, cleaned-up, and documented.  Note that all future developments will appear in this package and that 'PatternClass' will eventually be archived.",
    "version": "3.1.0",
    "maintainer": "Tarmo K. Remmel <remmelt@yorku.ca>",
    "author": "Tarmo K. Remmel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6251-876X>),\n  Marie-Josee Fortin [ctb],\n  Ferenc Csillag [ctb],\n  Sandor Kabos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ShapePattern",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShapePattern Tools for Analyzing Shapes and Patterns This is an evolving and growing collection of tools for the quantification, assessment, and comparison of shape and pattern. This collection provides tools for: (1) the spatial decomposition of planar shapes using 'ShrinkShape' to incrementally shrink shapes to extinction while computing area, perimeter, and number of parts at each iteration of shrinking; the spectra of results are returned in graphic and tabular formats (Remmel 2015) <doi:10.1111/cag.12222>, (2) simulating landscape patterns, (3) provision of tools for estimating composition and configuration parameters from a categorical (binary) landscape map (grid) and then simulates a selected number of statistically similar landscapes. Class-focused pattern metrics are computed for each simulated map to produce empirical distributions against which statistical comparisons can be made. The code permits the analysis of single maps or pairs of maps (Remmel and Fortin 2013) <doi:10.1007/s10980-013-9905-x>, (4) counting the number of each first-order pattern element and converting that information into both frequency and empirical probability vectors (Remmel 2020) <doi:10.3390/e22040420>, and (5) computing the porosity of raster patches <doi:10.3390/su10103413>. NOTE: This is a consolidation of existing packages ('PatternClass', 'ShapePattern') to begin warehousing all shape and pattern code in a common package. Additional utility tools for handling data are provided and this package will be added to as more tools are created, cleaned-up, and documented.  Note that all future developments will appear in this package and that 'PatternClass' will eventually be archived.  "
  },
  {
    "id": 7269,
    "package_name": "SiMRiv",
    "title": "Simulating Multistate Movements in River/Heterogeneous\nLandscapes",
    "description": "Provides functions to generate and analyze spatially-explicit individual-based multistate movements in rivers,\n  heterogeneous and homogeneous spaces. This is done by incorporating landscape bias on local behaviour, based on\n  resistance rasters. Although originally conceived and designed to simulate trajectories of species constrained to\n  linear habitats/dendritic ecological networks (e.g. river networks), the simulation algorithm is built to be\n  highly flexible and can be applied to any (aquatic, semi-aquatic or terrestrial) organism, independently on the\n  landscape in which it moves. Thus, the user will be able to use the package to simulate movements either in\n  homogeneous landscapes, heterogeneous landscapes (e.g. semi-aquatic animal moving mainly along rivers but also using\n  the matrix), or even in highly contrasted landscapes (e.g. fish in a river network). The algorithm and its input\n  parameters are the same for all cases, so that results are comparable. Simulated trajectories can then be used as\n  mechanistic null models (Potts & Lewis 2014, <DOI:10.1098/rspb.2014.0231>) to test a variety of 'Movement Ecology'\n  hypotheses (Nathan et al. 2008, <DOI:10.1073/pnas.0800375105>), including landscape effects (e.g. resources, \n  infrastructures) on animal movement and species site fidelity, or for predictive purposes (e.g. road mortality risk,\n  dispersal/connectivity). The package should be relevant to explore a broad spectrum of ecological phenomena, such as\n  those at the interface of animal behaviour, management, landscape and movement ecology, disease and invasive species\n  spread, and population dynamics.",
    "version": "1.0.7",
    "maintainer": "Miguel Porto <mpbertolo@gmail.com>",
    "author": "Lorenzo Quaglietta [aut],\n  Miguel Porto [aut, cre],\n  Erida Gjini [ctb]",
    "url": "https://www.r-project.org, https://github.com/miguel-porto/SiMRiv",
    "bug_reports": "https://github.com/miguel-porto/SiMRiv/issues",
    "repository": "https://cran.r-project.org/package=SiMRiv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SiMRiv Simulating Multistate Movements in River/Heterogeneous\nLandscapes Provides functions to generate and analyze spatially-explicit individual-based multistate movements in rivers,\n  heterogeneous and homogeneous spaces. This is done by incorporating landscape bias on local behaviour, based on\n  resistance rasters. Although originally conceived and designed to simulate trajectories of species constrained to\n  linear habitats/dendritic ecological networks (e.g. river networks), the simulation algorithm is built to be\n  highly flexible and can be applied to any (aquatic, semi-aquatic or terrestrial) organism, independently on the\n  landscape in which it moves. Thus, the user will be able to use the package to simulate movements either in\n  homogeneous landscapes, heterogeneous landscapes (e.g. semi-aquatic animal moving mainly along rivers but also using\n  the matrix), or even in highly contrasted landscapes (e.g. fish in a river network). The algorithm and its input\n  parameters are the same for all cases, so that results are comparable. Simulated trajectories can then be used as\n  mechanistic null models (Potts & Lewis 2014, <DOI:10.1098/rspb.2014.0231>) to test a variety of 'Movement Ecology'\n  hypotheses (Nathan et al. 2008, <DOI:10.1073/pnas.0800375105>), including landscape effects (e.g. resources, \n  infrastructures) on animal movement and species site fidelity, or for predictive purposes (e.g. road mortality risk,\n  dispersal/connectivity). The package should be relevant to explore a broad spectrum of ecological phenomena, such as\n  those at the interface of animal behaviour, management, landscape and movement ecology, disease and invasive species\n  spread, and population dynamics.  "
  },
  {
    "id": 7299,
    "package_name": "SimReg",
    "title": "Similarity Regression",
    "description": "Similarity regression,\n    evaluating the probability of association between sets of ontological terms\n    and binary response vector. A no-association model is compared with one in which\n    the log odds of a true response is linked to the semantic similarity\n    between terms and a latent characteristic ontological profile - 'Phenotype Similarity Regression for Identifying the Genetic Determinants of Rare Diseases', Greene et al 2016 <doi:10.1016/j.ajhg.2016.01.008>. ",
    "version": "3.4",
    "maintainer": "Daniel Greene <dg333@cam.ac.uk>",
    "author": "Daniel Greene",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimReg Similarity Regression Similarity regression,\n    evaluating the probability of association between sets of ontological terms\n    and binary response vector. A no-association model is compared with one in which\n    the log odds of a true response is linked to the semantic similarity\n    between terms and a latent characteristic ontological profile - 'Phenotype Similarity Regression for Identifying the Genetic Determinants of Rare Diseases', Greene et al 2016 <doi:10.1016/j.ajhg.2016.01.008>.   "
  },
  {
    "id": 7301,
    "package_name": "SimSeq",
    "title": "Nonparametric Simulation of RNA-Seq Data",
    "description": "RNA sequencing analysis methods are often derived by relying on hypothetical parametric models for read counts that are not likely to be precisely satisfied in practice. Methods are often tested by analyzing data that have been simulated according to the assumed model. This testing strategy can result in an overly optimistic view of the performance of an RNA-seq analysis method. We develop a data-based simulation algorithm for RNA-seq data. The vector of read counts simulated for a given experimental unit has a joint distribution that closely matches the distribution of a source RNA-seq dataset provided by the user. Users control the proportion of genes simulated to be differentially expressed (DE) and can provide a vector of weights to control the distribution of effect sizes. The algorithm requires a matrix of RNA-seq read counts with large sample sizes in at least two treatment groups. Many datasets are available that fit this standard.",
    "version": "1.4.0",
    "maintainer": "Samuel Benidt <sgbenidt@gmail.com>",
    "author": "Samuel Benidt",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimSeq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimSeq Nonparametric Simulation of RNA-Seq Data RNA sequencing analysis methods are often derived by relying on hypothetical parametric models for read counts that are not likely to be precisely satisfied in practice. Methods are often tested by analyzing data that have been simulated according to the assumed model. This testing strategy can result in an overly optimistic view of the performance of an RNA-seq analysis method. We develop a data-based simulation algorithm for RNA-seq data. The vector of read counts simulated for a given experimental unit has a joint distribution that closely matches the distribution of a source RNA-seq dataset provided by the user. Users control the proportion of genes simulated to be differentially expressed (DE) and can provide a vector of weights to control the distribution of effect sizes. The algorithm requires a matrix of RNA-seq read counts with large sample sizes in at least two treatment groups. Many datasets are available that fit this standard.  "
  },
  {
    "id": 7363,
    "package_name": "SpATS",
    "title": "Spatial Analysis of Field Trials with Splines",
    "description": "Analysis of field trial experiments by modelling spatial trends using two-dimensional Penalised spline (P-spline) models.",
    "version": "1.0-19",
    "maintainer": "Maria Xose Rodriguez-Alvarez <mxrodriguez@uvigo.es>",
    "author": "Maria Xose Rodriguez-Alvarez [aut, cre],\n  Martin Boer [aut],\n  Paul Eilers [aut],\n  Fred van Eeuwijk [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpATS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpATS Spatial Analysis of Field Trials with Splines Analysis of field trial experiments by modelling spatial trends using two-dimensional Penalised spline (P-spline) models.  "
  },
  {
    "id": 7369,
    "package_name": "SpaDES",
    "title": "Develop and Run Spatially Explicit Discrete Event Simulation\nModels",
    "description": "Metapackage for implementing a variety of event-based models, with\n    a focus on spatially explicit models. These include raster-based,\n    event-based, and agent-based models. The core simulation components\n    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;\n    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)\n    framework that facilitates modularity, and easily enables the user to\n    include additional functionality by running user-built simulation modules\n    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters\n    and other maps (via 'quickPlot'), and caching methods for reproducible\n    simulations (via 'reproducible'). Tools for running simulation experiments are\n    provided by 'SpaDES.experiment'. Additional functionality is provided by\n    the 'SpaDES.addins' and 'SpaDES.shiny' packages.",
    "version": "2.0.11",
    "maintainer": "Alex M Chubaty <achubaty@for-cast.ca>",
    "author": "Alex M Chubaty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7146-8135>),\n  Eliot J B McIntire [aut] (ORCID:\n    <https://orcid.org/0000-0002-6914-8316>),\n  Yong Luo [ctb],\n  Steve Cumming [ctb],\n  His Majesty the Queen in Right of Canada, as represented by the\n    Minister of Natural Resources Canada [cph]",
    "url": "https://spades.predictiveecology.org,\nhttps://github.com/PredictiveEcology/SpaDES",
    "bug_reports": "https://github.com/PredictiveEcology/SpaDES/issues",
    "repository": "https://cran.r-project.org/package=SpaDES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpaDES Develop and Run Spatially Explicit Discrete Event Simulation\nModels Metapackage for implementing a variety of event-based models, with\n    a focus on spatially explicit models. These include raster-based,\n    event-based, and agent-based models. The core simulation components\n    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;\n    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)\n    framework that facilitates modularity, and easily enables the user to\n    include additional functionality by running user-built simulation modules\n    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters\n    and other maps (via 'quickPlot'), and caching methods for reproducible\n    simulations (via 'reproducible'). Tools for running simulation experiments are\n    provided by 'SpaDES.experiment'. Additional functionality is provided by\n    the 'SpaDES.addins' and 'SpaDES.shiny' packages.  "
  },
  {
    "id": 7371,
    "package_name": "SpaDES.tools",
    "title": "Additional Tools for Developing Spatially Explicit Discrete\nEvent Simulation (SpaDES) Models",
    "description": "Provides GIS and map utilities, plus additional modeling\n    tools for developing cellular automata, dynamic raster models, and\n    agent based models in 'SpaDES'.  Included are various methods for\n    spatial spreading, spatial agents, GIS operations, random map\n    generation, and others.  See '?SpaDES.tools' for an categorized\n    overview of these additional tools.  The suggested package 'NLMR' can\n    be installed from the following repository:\n    (<https://PredictiveEcology.r-universe.dev>).",
    "version": "2.0.9",
    "maintainer": "Alex M Chubaty <achubaty@for-cast.ca>",
    "author": "Eliot J B McIntire [aut] (ORCID:\n    <https://orcid.org/0000-0002-6914-8316>),\n  Alex M Chubaty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7146-8135>),\n  Yong Luo [ctb],\n  Ceres Barros [ctb] (ORCID: <https://orcid.org/0000-0003-4036-977X>),\n  Steve Cumming [ctb],\n  Jean Marchal [ctb],\n  His Majesty the King in Right of Canada, as represented by the Minister\n    of Natural Resources Canada [cph]",
    "url": "https://spades-tools.predictiveecology.org,\nhttps://github.com/PredictiveEcology/SpaDES.tools",
    "bug_reports": "https://github.com/PredictiveEcology/SpaDES.tools/issues",
    "repository": "https://cran.r-project.org/package=SpaDES.tools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpaDES.tools Additional Tools for Developing Spatially Explicit Discrete\nEvent Simulation (SpaDES) Models Provides GIS and map utilities, plus additional modeling\n    tools for developing cellular automata, dynamic raster models, and\n    agent based models in 'SpaDES'.  Included are various methods for\n    spatial spreading, spatial agents, GIS operations, random map\n    generation, and others.  See '?SpaDES.tools' for an categorized\n    overview of these additional tools.  The suggested package 'NLMR' can\n    be installed from the following repository:\n    (<https://PredictiveEcology.r-universe.dev>).  "
  },
  {
    "id": 7385,
    "package_name": "SparseVFC",
    "title": "Sparse Vector Field Consensus for Vector Field Learning",
    "description": "The sparse vector field consensus \n\t\t(SparseVFC) algorithm (Ma et al., 2013 <doi:10.1016/j.patcog.2013.05.017>) for robust vector \n\t\tfield learning. Largely translated from the Matlab functions in <https://github.com/jiayi-ma/VFC>.",
    "version": "0.1.2",
    "maintainer": "Jingmeng Cui <jingmeng.cui@outlook.com>",
    "author": "Jingmeng Cui [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3421-8457>)",
    "url": "https://github.com/Sciurus365/SparseVFC",
    "bug_reports": "https://github.com/Sciurus365/SparseVFC/issues",
    "repository": "https://cran.r-project.org/package=SparseVFC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SparseVFC Sparse Vector Field Consensus for Vector Field Learning The sparse vector field consensus \n\t\t(SparseVFC) algorithm (Ma et al., 2013 <doi:10.1016/j.patcog.2013.05.017>) for robust vector \n\t\tfield learning. Largely translated from the Matlab functions in <https://github.com/jiayi-ma/VFC>.  "
  },
  {
    "id": 7387,
    "package_name": "SpatFD",
    "title": "Functional Geostatistics: Univariate and Multivariate Functional\nSpatial Prediction",
    "description": "Performance of functional kriging, cokriging, optimal sampling and simulation for spatial prediction of functional data. The framework of spatial prediction, optimal sampling and simulation are extended from scalar to functional data. 'SpatFD' is based on the Karhunen-Lo\u00e8ve expansion that allows to represent the observed functions in terms of its empirical functional principal components. Based on this approach, the functional auto-covariances and cross-covariances required for  spatial functional predictions and optimal sampling, are completely determined by the sum of the spatial auto-covariances and cross-covariances of the respective score components. The package provides new classes of data and functions for modeling spatial dependence structure among curves. The spatial prediction of curves at unsampled locations can be carried out using two types of predictors, and both of them report, the respective variances of the prediction error. In addition, there is a function for the determination of spatial locations sampling configuration that ensures minimum variance of spatial functional prediction. There are also two functions for plotting predicted curves at each location and mapping the surface at each time point, respectively. References Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s10260-015-0340-9>, Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s00477-016-1266-y>, Bohorquez M., Giraldo R. and Mateu J. (2021) <doi:10.1002/9781119387916>.",
    "version": "0.0.1",
    "maintainer": "Martha Patricia Bohorquez Casta\u00f1eda <mpbohorquezc@unal.edu.co>",
    "author": "Martha Patricia Bohorquez Casta\u00f1eda [aut, cre],\n  Diego Alejandro Sandoval Skinner [aut],\n  Angie Villamil [aut],\n  Samuel Hernando Sanchez Gutierrez [aut],\n  Nathaly Vergel Serrano [ctb],\n  Miguel Angel Munoz Layton [ctb],\n  Valeria Bejarano Salcedo [ctb],\n  Venus Celeste Puertas [ctb],\n  Ruben Dario Guevara Gonzalez [aut],\n  Joan Nicolas Castro Cortes [ctb],\n  Ramon Giraldo Henao [aut],\n  Jorge Mateu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatFD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatFD Functional Geostatistics: Univariate and Multivariate Functional\nSpatial Prediction Performance of functional kriging, cokriging, optimal sampling and simulation for spatial prediction of functional data. The framework of spatial prediction, optimal sampling and simulation are extended from scalar to functional data. 'SpatFD' is based on the Karhunen-Lo\u00e8ve expansion that allows to represent the observed functions in terms of its empirical functional principal components. Based on this approach, the functional auto-covariances and cross-covariances required for  spatial functional predictions and optimal sampling, are completely determined by the sum of the spatial auto-covariances and cross-covariances of the respective score components. The package provides new classes of data and functions for modeling spatial dependence structure among curves. The spatial prediction of curves at unsampled locations can be carried out using two types of predictors, and both of them report, the respective variances of the prediction error. In addition, there is a function for the determination of spatial locations sampling configuration that ensures minimum variance of spatial functional prediction. There are also two functions for plotting predicted curves at each location and mapping the surface at each time point, respectively. References Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s10260-015-0340-9>, Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s00477-016-1266-y>, Bohorquez M., Giraldo R. and Mateu J. (2021) <doi:10.1002/9781119387916>.  "
  },
  {
    "id": 7389,
    "package_name": "SpatGRID",
    "title": "Spatial Grid Generation from Longitude and Latitude List",
    "description": "The developed function is designed for the generation of spatial grids based on user-specified longitude and latitude coordinates. The function first validates the input longitude and latitude values,\n              ensuring they fall within the appropriate geographic ranges. It then creates a polygon from the coordinates and determines the appropriate Universal Transverse Mercator zone based on the provided \n              hemisphere and longitude values. Subsequently, transforming the input Shapefile to the Universal Transverse Mercator projection when necessary. Finally, a spatial grid is generated with the specified interval and saved as a Shapefile. \n              For method details see, Brus,D.J.(2022).<DOI:10.1201/9781003258940>. The function takes into account crucial parameters such as the hemisphere (north or south), desired grid interval, and the output Shapefile path.\n              The developed function is an efficient tool, simplifying the process of empty spatial grid generation for applications such as, geo-statistical analysis, digital soil mapping product generation, etc. Whether for environmental studies, \n              urban planning, or any other geo-spatial analysis, this package caters to the diverse needs of users working with spatial data, enhancing the accessibility and ease of spatial data processing and visualization.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nirmal Kumar [aut, cph],\n  Nobin Chandra Paul [aut, cre],\n  G.P. Obi Reddy [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatGRID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatGRID Spatial Grid Generation from Longitude and Latitude List The developed function is designed for the generation of spatial grids based on user-specified longitude and latitude coordinates. The function first validates the input longitude and latitude values,\n              ensuring they fall within the appropriate geographic ranges. It then creates a polygon from the coordinates and determines the appropriate Universal Transverse Mercator zone based on the provided \n              hemisphere and longitude values. Subsequently, transforming the input Shapefile to the Universal Transverse Mercator projection when necessary. Finally, a spatial grid is generated with the specified interval and saved as a Shapefile. \n              For method details see, Brus,D.J.(2022).<DOI:10.1201/9781003258940>. The function takes into account crucial parameters such as the hemisphere (north or south), desired grid interval, and the output Shapefile path.\n              The developed function is an efficient tool, simplifying the process of empty spatial grid generation for applications such as, geo-statistical analysis, digital soil mapping product generation, etc. Whether for environmental studies, \n              urban planning, or any other geo-spatial analysis, this package caters to the diverse needs of users working with spatial data, enhancing the accessibility and ease of spatial data processing and visualization.  "
  },
  {
    "id": 7406,
    "package_name": "SpatialTools",
    "title": "Tools for Spatial Data Analysis",
    "description": "Tools for spatial data analysis.  Emphasis on kriging.  Provides functions for prediction and simulation.  Intended to be relatively straightforward, fast, and flexible.",
    "version": "1.0.5",
    "maintainer": "Joshua French <joshua.french@ucdenver.edu>",
    "author": "Joshua French <joshua.french@ucdenver.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatialTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatialTools Tools for Spatial Data Analysis Tools for spatial data analysis.  Emphasis on kriging.  Provides functions for prediction and simulation.  Intended to be relatively straightforward, fast, and flexible.  "
  },
  {
    "id": 7434,
    "package_name": "Ssarkartrim",
    "title": "Trimmed-k Mean Estimator",
    "description": "Computes the trimmed-k mean by removing the k smallest and k largest values from a numeric vector. Created for STAT 5400 at the University of Iowa.",
    "version": "1.0.0",
    "maintainer": "Shouhardyo Sarkar <shouhardyo-sarkar@uiowa.edu>",
    "author": "Shouhardyo Sarkar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Ssarkartrim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Ssarkartrim Trimmed-k Mean Estimator Computes the trimmed-k mean by removing the k smallest and k largest values from a numeric vector. Created for STAT 5400 at the University of Iowa.  "
  },
  {
    "id": 7465,
    "package_name": "StepGWR",
    "title": "A Hybrid Spatial Model for Prediction and Capturing Spatial\nVariation in the Data",
    "description": "It is a hybrid spatial model that combines the variable selection capabilities of stepwise regression methods with the predictive power of the Geographically \n             Weighted Regression(GWR) model.The developed hybrid model follows a two-step approach where the stepwise variable selection method is applied first to identify \n             the subset of predictors that have the most significant impact on the response variable, and then a GWR model is fitted using those selected variables for spatial \n             prediction at test or unknown locations. For method details,see Leung, Y., Mei, C. L. and Zhang, W. X. (2000).<DOI:10.1068/a3162>.This hybrid spatial model aims to \n             improve the accuracy and interpretability of GWR predictions by selecting a subset of relevant variables through a stepwise selection process.This approach is particularly \n             useful for modeling spatially varying relationships and improving the accuracy of spatial predictions.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nobin Chandra Paul [aut, cre, cph],\n  Moumita Baishya [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StepGWR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StepGWR A Hybrid Spatial Model for Prediction and Capturing Spatial\nVariation in the Data It is a hybrid spatial model that combines the variable selection capabilities of stepwise regression methods with the predictive power of the Geographically \n             Weighted Regression(GWR) model.The developed hybrid model follows a two-step approach where the stepwise variable selection method is applied first to identify \n             the subset of predictors that have the most significant impact on the response variable, and then a GWR model is fitted using those selected variables for spatial \n             prediction at test or unknown locations. For method details,see Leung, Y., Mei, C. L. and Zhang, W. X. (2000).<DOI:10.1068/a3162>.This hybrid spatial model aims to \n             improve the accuracy and interpretability of GWR predictions by selecting a subset of relevant variables through a stepwise selection process.This approach is particularly \n             useful for modeling spatially varying relationships and improving the accuracy of spatial predictions.  "
  },
  {
    "id": 7486,
    "package_name": "StratigrapheR",
    "title": "Integrated Stratigraphy",
    "description": "Includes bases for litholog generation: graphical functions\n    based on R base graphics, interval management functions and svg importation \n    functions among others. Also include stereographic projection functions, \n    and other functions made to deal with large datasets while keeping options\n    to get into the details of the data.\n    When using for publication please cite \n    Sebastien Wouters, Anne-Christine Da Silva, Frederic Boulvain and \n    Xavier Devleeschouwer, 2021. The R Journal 13:2, 153-178.\n    The palaeomagnetism functions are based on:\n    Tauxe, L., 2010. Essentials of Paleomagnetism. University of California \n    Press. <https://earthref.org/MagIC/books/Tauxe/Essentials/>;\n    Allmendinger, R. W., Cardozo, N. C., and Fisher, D., 2013, Structural \n    Geology Algorithms: Vectors & Tensors: Cambridge, England, Cambridge\n    University Press, 289 pp.;\n    Cardozo, N., and Allmendinger, R. W., 2013, Spherical projections\n    with OSXStereonet: Computers & Geosciences, v. 51, no. 0, p. 193 - 205,\n    <doi: 10.1016/j.cageo.2012.07.021>.",
    "version": "1.3.1",
    "maintainer": "Sebastien Wouters <wouterseb@gmail.com>",
    "author": "Sebastien Wouters [aut, cre], Adam D. Smith [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StratigrapheR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StratigrapheR Integrated Stratigraphy Includes bases for litholog generation: graphical functions\n    based on R base graphics, interval management functions and svg importation \n    functions among others. Also include stereographic projection functions, \n    and other functions made to deal with large datasets while keeping options\n    to get into the details of the data.\n    When using for publication please cite \n    Sebastien Wouters, Anne-Christine Da Silva, Frederic Boulvain and \n    Xavier Devleeschouwer, 2021. The R Journal 13:2, 153-178.\n    The palaeomagnetism functions are based on:\n    Tauxe, L., 2010. Essentials of Paleomagnetism. University of California \n    Press. <https://earthref.org/MagIC/books/Tauxe/Essentials/>;\n    Allmendinger, R. W., Cardozo, N. C., and Fisher, D., 2013, Structural \n    Geology Algorithms: Vectors & Tensors: Cambridge, England, Cambridge\n    University Press, 289 pp.;\n    Cardozo, N., and Allmendinger, R. W., 2013, Spherical projections\n    with OSXStereonet: Computers & Geosciences, v. 51, no. 0, p. 193 - 205,\n    <doi: 10.1016/j.cageo.2012.07.021>.  "
  },
  {
    "id": 7517,
    "package_name": "SurfRough",
    "title": "Calculate Surface/Image Texture Indexes",
    "description": "Methods for the computation of surface/image texture indices using a geostatistical based approach (Trevisani et al. (2023) <doi:10.1016/j.geomorph.2023.108838>). It provides various functions for the computation of surface texture indices (e.g., omnidirectional roughness and roughness anisotropy), including the ones based on the robust MAD estimator. The kernels included in the software permit also to calculate the surface/image texture indices directly from the input surface  (i.e., without de-trending) using increments of order 2. It also provides the new radial roughness index (RRI), representing the improvement of the popular topographic roughness index (TRI). The framework can be easily extended with ad-hoc surface/image texture indices.",
    "version": "0.0.1.1",
    "maintainer": "Sebastiano Trevisani <strevisani@iuav.it>",
    "author": "Sebastiano Trevisani [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8436-7798>),\n  Ilich Alexander [ctb] (ORCID: <https://orcid.org/0000-0003-1758-8499>),\n  Zakharko Taras [ctb] (ORCID: <https://orcid.org/0000-0001-7601-8424>)",
    "url": "https://github.com/strevisani/SurfRough,\nhttps://doi.org/10.5281/zenodo.7132160",
    "bug_reports": "https://github.com/strevisani/SurfRough/issues",
    "repository": "https://cran.r-project.org/package=SurfRough",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SurfRough Calculate Surface/Image Texture Indexes Methods for the computation of surface/image texture indices using a geostatistical based approach (Trevisani et al. (2023) <doi:10.1016/j.geomorph.2023.108838>). It provides various functions for the computation of surface texture indices (e.g., omnidirectional roughness and roughness anisotropy), including the ones based on the robust MAD estimator. The kernels included in the software permit also to calculate the surface/image texture indices directly from the input surface  (i.e., without de-trending) using increments of order 2. It also provides the new radial roughness index (RRI), representing the improvement of the popular topographic roughness index (TRI). The framework can be easily extended with ad-hoc surface/image texture indices.  "
  },
  {
    "id": 7585,
    "package_name": "TCHazaRds",
    "title": "Tropical Cyclone (Hurricane, Typhoon) Spatial Hazard Modelling",
    "description": "Methods for generating modelled parametric Tropical Cyclone (TC) spatial hazard fields and time series output at point locations from TC tracks.  R's compatibility to simply use fast 'cpp' code via the 'Rcpp' package and the wide range spatial analysis tools via the 'terra' package makes it an attractive open source environment to study 'TCs'.  This package estimates TC vortex wind and pressure fields using parametric equations originally coded up in 'python' by 'TCRM' <https://github.com/GeoscienceAustralia/tcrm> and then coded up in 'Cuda' 'cpp' by 'TCwindgen' <https://github.com/CyprienBosserelle/TCwindgen>.",
    "version": "1.1.5",
    "maintainer": "Julian O'Grady <julian.ogrady@csiro.au>",
    "author": "Julian O'Grady [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3552-9193>)",
    "url": "https://github.com/AusClimateService/TCHazaRds",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TCHazaRds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TCHazaRds Tropical Cyclone (Hurricane, Typhoon) Spatial Hazard Modelling Methods for generating modelled parametric Tropical Cyclone (TC) spatial hazard fields and time series output at point locations from TC tracks.  R's compatibility to simply use fast 'cpp' code via the 'Rcpp' package and the wide range spatial analysis tools via the 'terra' package makes it an attractive open source environment to study 'TCs'.  This package estimates TC vortex wind and pressure fields using parametric equations originally coded up in 'python' by 'TCRM' <https://github.com/GeoscienceAustralia/tcrm> and then coded up in 'Cuda' 'cpp' by 'TCwindgen' <https://github.com/CyprienBosserelle/TCwindgen>.  "
  },
  {
    "id": 7593,
    "package_name": "TDAvec",
    "title": "Vector Summaries of Persistence Diagrams",
    "description": "Provides tools for computing various vector summaries of persistence diagrams studied in Topological Data Analysis. For improved computational efficiency, \n    all code for the vector summaries is written in 'C++' using the 'Rcpp' and 'RcppArmadillo' packages.",
    "version": "0.1.41",
    "maintainer": "Aleksei Luchinsky <aluchi@bgsu.edu>",
    "author": "Umar Islambekov [aut],\n  Aleksei Luchinsky [aut, cre]",
    "url": "https://github.com/uislambekov/TDAvec",
    "bug_reports": "https://github.com/uislambekov/TDAvec/issues",
    "repository": "https://cran.r-project.org/package=TDAvec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TDAvec Vector Summaries of Persistence Diagrams Provides tools for computing various vector summaries of persistence diagrams studied in Topological Data Analysis. For improved computational efficiency, \n    all code for the vector summaries is written in 'C++' using the 'Rcpp' and 'RcppArmadillo' packages.  "
  },
  {
    "id": 7614,
    "package_name": "THETASVM",
    "title": "Time Series Forecasting using THETA-SVM Hybrid Model",
    "description": "Testing, Implementation, and Forecasting of the THETA-SVM hybrid model. The THETA-SVM hybrid model combines the distinct strengths of the THETA model and the Support Vector Machine (SVM) model for time series forecasting.For method details see Bhattacharyya et al. (2022) <doi:10.1007/s11071-021-07099-3>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "author": "Fasila K. P. [aut, ctb],\n  Mrinmoy Ray [aut, cre],\n  Rajeev Ranjan Kumar [aut, ctb],\n  K. N. Singh [aut, ctb],\n  Amrender Kumar [aut, ctb],\n  Santosha Rathod [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=THETASVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "THETASVM Time Series Forecasting using THETA-SVM Hybrid Model Testing, Implementation, and Forecasting of the THETA-SVM hybrid model. The THETA-SVM hybrid model combines the distinct strengths of the THETA model and the Support Vector Machine (SVM) model for time series forecasting.For method details see Bhattacharyya et al. (2022) <doi:10.1007/s11071-021-07099-3>.  "
  },
  {
    "id": 7664,
    "package_name": "TSF",
    "title": "Two Stage Forecasting (TSF) for Long Memory Time Series in\nPresence of Structural Break",
    "description": "Forecasting of long memory time series in presence of structural break by using TSF algorithm by Papailias and Dias (2015) <doi:10.1016/j.ijforecast.2015.01.006>. ",
    "version": "0.1.1",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "author": "Sandipan Samanta, Ranjit Kumar Paul and Dipankar Mitra",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSF Two Stage Forecasting (TSF) for Long Memory Time Series in\nPresence of Structural Break Forecasting of long memory time series in presence of structural break by using TSF algorithm by Papailias and Dias (2015) <doi:10.1016/j.ijforecast.2015.01.006>.   "
  },
  {
    "id": 7665,
    "package_name": "TSGS",
    "title": "Trait Specific Gene Selection using SVM and GA",
    "description": "Obtaining relevant set of trait specific genes from gene expression data is important for clinical diagnosis of disease and discovery of disease mechanisms in plants and animals. This process involves identification of relevant genes and removal of redundant genes as much as possible from a whole gene set. This package returns the trait specific gene set from the high dimensional RNA-seq count data by applying combination of two conventional machine learning algorithms, support vector machine (SVM) and genetic algorithm (GA). GA is used to control and optimize the subset of genes sent to the SVM for classification and evaluation. Genetic algorithm uses repeated learning steps and cross validation over number of possible solution and selects the best. The algorithm selects the set of genes based on a fitness function that is obtained via support vector machines. Using SVM as the classifier performance and the genetic algorithm for feature selection, a set of trait specific gene set is obtained.",
    "version": "1.0",
    "maintainer": "Sudhir Srivastava <Sudhir.Srivastava@icar.gov.in>",
    "author": "Md. Samir Farooqi [aut],\n  K.K. Chaturvedi [aut],\n  D.C. Mishra [aut],\n  Sudhir Srivastava [cre, aut]",
    "url": "https://github.com/SudhirSrivastava/TSGS",
    "bug_reports": "https://github.com/SudhirSrivastava/TSGS/issues",
    "repository": "https://cran.r-project.org/package=TSGS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSGS Trait Specific Gene Selection using SVM and GA Obtaining relevant set of trait specific genes from gene expression data is important for clinical diagnosis of disease and discovery of disease mechanisms in plants and animals. This process involves identification of relevant genes and removal of redundant genes as much as possible from a whole gene set. This package returns the trait specific gene set from the high dimensional RNA-seq count data by applying combination of two conventional machine learning algorithms, support vector machine (SVM) and genetic algorithm (GA). GA is used to control and optimize the subset of genes sent to the SVM for classification and evaluation. Genetic algorithm uses repeated learning steps and cross validation over number of possible solution and selects the best. The algorithm selects the set of genes based on a fitness function that is obtained via support vector machines. Using SVM as the classifier performance and the genetic algorithm for feature selection, a set of trait specific gene set is obtained.  "
  },
  {
    "id": 7678,
    "package_name": "TSSVM",
    "title": "Time Series Forecasting using SVM Model",
    "description": "Implementation and forecasting univariate time series data using the Support Vector Machine model. Support Vector Machine is one of the prominent machine learning approach for non-linear time series forecasting. For method details see Kim, K. (2003) <doi:10.1016/S0925-2312(03)00372-2>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "author": "Mrinmoy Ray [aut, cre],\n  Samir Barman [aut, ctb],\n  Kanchan Sinha [aut, ctb],\n  K. N. Singh [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSSVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSSVM Time Series Forecasting using SVM Model Implementation and forecasting univariate time series data using the Support Vector Machine model. Support Vector Machine is one of the prominent machine learning approach for non-linear time series forecasting. For method details see Kim, K. (2003) <doi:10.1016/S0925-2312(03)00372-2>.  "
  },
  {
    "id": 7697,
    "package_name": "TVMM",
    "title": "Multivariate Tests for the Vector of Means",
    "description": "This is a statistical tool interactive that provides multivariate statistical tests that are more powerful than traditional Hotelling T2 test and LRT (likelihood ratio test) for the vector of normal mean populations with and without contamination and non-normal populations (Henrique J. P. Alves & Daniel F. Ferreira (2019) <DOI: 10.1080/03610918.2019.1693596>).",
    "version": "3.2.1",
    "maintainer": "Henrique Jose de Paula Alves <jpahenrique@gmail.com>",
    "author": "Henrique Jose de Paula Alves [aut, cre],\n  Ben Deivide de Oliveira Batista [ctb],\n  Daniel Furtado Ferreira [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TVMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TVMM Multivariate Tests for the Vector of Means This is a statistical tool interactive that provides multivariate statistical tests that are more powerful than traditional Hotelling T2 test and LRT (likelihood ratio test) for the vector of normal mean populations with and without contamination and non-normal populations (Henrique J. P. Alves & Daniel F. Ferreira (2019) <DOI: 10.1080/03610918.2019.1693596>).  "
  },
  {
    "id": 7777,
    "package_name": "Tivy",
    "title": "Toolkit for Investigation and Visualization of Young Anchovies",
    "description": "Specialized toolkit for processing biological and fisheries data from Peru's anchovy (Engraulis ringens) fishery. Provides functions to analyze fishing logbooks, calculate biological indicators (length-weight relationships, juvenile percentages), generate spatial fishing indicators, and visualize regulatory measures from Peru's Ministry of Production. Features automated data processing from multiple file formats, coordinate validation, spatial analysis of fishing zones, and tools for analyzing fishing closure announcements and regulatory compliance. Includes built-in datasets of Peruvian coastal coordinates and parallel lines for analyzing fishing activities within regulatory zones.",
    "version": "0.1.1",
    "maintainer": "Hans Ttito <kvttitos@gmail.com>",
    "author": "Hans Ttito [aut, cre] (ORCID: <https://orcid.org/0000-0003-3732-9419>)",
    "url": "https://github.com/HansTtito/Tivy",
    "bug_reports": "https://github.com/HansTtito/Tivy/issues",
    "repository": "https://cran.r-project.org/package=Tivy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Tivy Toolkit for Investigation and Visualization of Young Anchovies Specialized toolkit for processing biological and fisheries data from Peru's anchovy (Engraulis ringens) fishery. Provides functions to analyze fishing logbooks, calculate biological indicators (length-weight relationships, juvenile percentages), generate spatial fishing indicators, and visualize regulatory measures from Peru's Ministry of Production. Features automated data processing from multiple file formats, coordinate validation, spatial analysis of fishing zones, and tools for analyzing fishing closure announcements and regulatory compliance. Includes built-in datasets of Peruvian coastal coordinates and parallel lines for analyzing fishing activities within regulatory zones.  "
  },
  {
    "id": 7797,
    "package_name": "TransGraph",
    "title": "Transfer Graph Learning",
    "description": "Transfer learning, aiming to use auxiliary domains to help improve learning of the target domain of interest when multiple heterogeneous datasets are available, has been a hot topic in statistical machine learning. The recent transfer learning methods with statistical guarantees mainly focus on the overall parameter transfer for supervised models in the ideal case with the informative auxiliary domains with overall similarity. In contrast, transfer learning for unsupervised graph learning is in its infancy and largely follows the idea of overall parameter transfer as for supervised learning. \n             In this package, the transfer learning for several complex graphical models is implemented, including Tensor Gaussian graphical models, non-Gaussian directed acyclic graph (DAG), and Gaussian graphical mixture models. Notably, this package promotes local transfer at node-level and subgroup-level in DAG structural learning and Gaussian graphical mixture models, respectively, which are more flexible and robust than the existing overall parameter transfer. As by-products, transfer learning for undirected graphical model (precision matrix) via D-trace loss, transfer learning for mean vector estimation, and single non-Gaussian learning via topological layer method are also included in this package. \n             Moreover, the aggregation of auxiliary information is an important issue in transfer learning, and this package provides multiple user-friendly aggregation methods, including sample weighting, similarity weighting, and most informative selection.    \n             (Note: the transfer for tensor GGM has been temporarily removed in the current version as its dependent R package Tlasso has been archived. The historical version TransGraph_1.0.0.tar.gz can be downloaded at <https://cran.r-project.org/src/contrib/Archive/TransGraph/>)\n             Reference: \n             Ren, M., Zhen Y., and Wang J. (2024) <https://jmlr.org/papers/v25/22-1313.html> \"Transfer learning for tensor graphical models\".    \n             Ren, M., He X., and Wang J. (2023) <doi:10.48550/arXiv.2310.10239> \"Structural transfer learning of non-Gaussian DAG\".    \n             Zhao, R., He X., and Wang J. (2022) <https://jmlr.org/papers/v23/21-1173.html> \"Learning linear non-Gaussian directed acyclic graph with diverging number of nodes\".",
    "version": "1.1.0",
    "maintainer": "Mingyang Ren <renmingyang17@mails.ucas.ac.cn>",
    "author": "Mingyang Ren [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8061-9940>),\n  Ruixuan Zhao [aut],\n  Xin He [aut],\n  Junhui Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TransGraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TransGraph Transfer Graph Learning Transfer learning, aiming to use auxiliary domains to help improve learning of the target domain of interest when multiple heterogeneous datasets are available, has been a hot topic in statistical machine learning. The recent transfer learning methods with statistical guarantees mainly focus on the overall parameter transfer for supervised models in the ideal case with the informative auxiliary domains with overall similarity. In contrast, transfer learning for unsupervised graph learning is in its infancy and largely follows the idea of overall parameter transfer as for supervised learning. \n             In this package, the transfer learning for several complex graphical models is implemented, including Tensor Gaussian graphical models, non-Gaussian directed acyclic graph (DAG), and Gaussian graphical mixture models. Notably, this package promotes local transfer at node-level and subgroup-level in DAG structural learning and Gaussian graphical mixture models, respectively, which are more flexible and robust than the existing overall parameter transfer. As by-products, transfer learning for undirected graphical model (precision matrix) via D-trace loss, transfer learning for mean vector estimation, and single non-Gaussian learning via topological layer method are also included in this package. \n             Moreover, the aggregation of auxiliary information is an important issue in transfer learning, and this package provides multiple user-friendly aggregation methods, including sample weighting, similarity weighting, and most informative selection.    \n             (Note: the transfer for tensor GGM has been temporarily removed in the current version as its dependent R package Tlasso has been archived. The historical version TransGraph_1.0.0.tar.gz can be downloaded at <https://cran.r-project.org/src/contrib/Archive/TransGraph/>)\n             Reference: \n             Ren, M., Zhen Y., and Wang J. (2024) <https://jmlr.org/papers/v25/22-1313.html> \"Transfer learning for tensor graphical models\".    \n             Ren, M., He X., and Wang J. (2023) <doi:10.48550/arXiv.2310.10239> \"Structural transfer learning of non-Gaussian DAG\".    \n             Zhao, R., He X., and Wang J. (2022) <https://jmlr.org/papers/v23/21-1173.html> \"Learning linear non-Gaussian directed acyclic graph with diverging number of nodes\".  "
  },
  {
    "id": 7813,
    "package_name": "TreeOrderTests",
    "title": "Tests for Tree Ordered Alternatives in One-Way ANOVA",
    "description": "Implements a likelihood ratio test and two pairwise standardized mean difference tests for testing equality of means against tree ordered alternatives in one-way ANOVA. The null hypothesis assumes all group means are equal, while the alternative assumes the control mean is less than or equal to each treatment mean with at least one strict inequality. Inputs are a list of numeric vectors (groups) and a significance level; outputs include the test statistic, critical value, and decision. Methods described in \"Testing Against Tree Ordered Alternatives in One-way ANOVA\" <doi:10.48550/arXiv.2507.17229>.",
    "version": "0.1.0",
    "maintainer": "Subha Halder <sb.halder123456@gmail.com>",
    "author": "Subha Halder [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TreeOrderTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TreeOrderTests Tests for Tree Ordered Alternatives in One-Way ANOVA Implements a likelihood ratio test and two pairwise standardized mean difference tests for testing equality of means against tree ordered alternatives in one-way ANOVA. The null hypothesis assumes all group means are equal, while the alternative assumes the control mean is less than or equal to each treatment mean with at least one strict inequality. Inputs are a list of numeric vectors (groups) and a significance level; outputs include the test statistic, critical value, and decision. Methods described in \"Testing Against Tree Ordered Alternatives in One-way ANOVA\" <doi:10.48550/arXiv.2507.17229>.  "
  },
  {
    "id": 7834,
    "package_name": "TroublemakeR",
    "title": "Generates Spatial Problems in R for 'AMPL'",
    "description": "Provides methods for generating .dat files for use with the 'AMPL' \n    software using spatial data, particularly rasters. It includes support for \n    various spatial data formats and different problem types. By automating the \n    process of generating 'AMPL' datasets, this package can help streamline \n    optimization workflows and make it easier to solve complex optimization \n    problems. The methods implemented in this package are described in detail\n    in a publication by Fourer et al. (<doi:10.1287/mnsc.36.5.519>).",
    "version": "0.0.1",
    "maintainer": "Derek Corcoran <derek.corcoran.barrios@gmail.com>",
    "author": "Derek Corcoran [aut, cre]",
    "url": "https://github.com/Sustainscapes/TroublemakeR",
    "bug_reports": "https://github.com/Sustainscapes/TroublemakeR/issues",
    "repository": "https://cran.r-project.org/package=TroublemakeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TroublemakeR Generates Spatial Problems in R for 'AMPL' Provides methods for generating .dat files for use with the 'AMPL' \n    software using spatial data, particularly rasters. It includes support for \n    various spatial data formats and different problem types. By automating the \n    process of generating 'AMPL' datasets, this package can help streamline \n    optimization workflows and make it easier to solve complex optimization \n    problems. The methods implemented in this package are described in detail\n    in a publication by Fourer et al. (<doi:10.1287/mnsc.36.5.519>).  "
  },
  {
    "id": 7892,
    "package_name": "UniIsoRegression",
    "title": "Unimodal and Isotonic L1, L2 and Linf Regression",
    "description": "Perform L1 or L2 isotonic and unimodal regression on 1D weighted or unweighted input vector and isotonic regression on 2D weighted or unweighted input vector. It also performs L infinity isotonic and unimodal regression on 1D unweighted input vector. Reference: Quentin F. Stout (2008) <doi:10.1016/j.csda.2008.08.005>. Spouge, J., Wan, H. & Wilbur, W.(2003) <doi:10.1023/A:1023901806339>. Q.F. Stout (2013) <doi:10.1007/s00453-012-9628-4>.",
    "version": "0.0-0",
    "maintainer": "Zhipeng Xu <xzhipeng@umich.edu>",
    "author": "Zhipeng Xu <xzhipeng@umich.edu>, Chenkai Sun <sunchenk@umich.edu>, Aman Karunakaran <akarunak@umich.edu>, Quentin Stout<qstout@umich.edu>",
    "url": "https://github.com/xzp1995/UniIsoRegression",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UniIsoRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UniIsoRegression Unimodal and Isotonic L1, L2 and Linf Regression Perform L1 or L2 isotonic and unimodal regression on 1D weighted or unweighted input vector and isotonic regression on 2D weighted or unweighted input vector. It also performs L infinity isotonic and unimodal regression on 1D unweighted input vector. Reference: Quentin F. Stout (2008) <doi:10.1016/j.csda.2008.08.005>. Spouge, J., Wan, H. & Wilbur, W.(2003) <doi:10.1023/A:1023901806339>. Q.F. Stout (2013) <doi:10.1007/s00453-012-9628-4>.  "
  },
  {
    "id": 7911,
    "package_name": "VAR.etp",
    "title": "VAR Modelling: Estimation, Testing, and Prediction",
    "description": "A collection of the functions for estimation, hypothesis testing, prediction for stationary vector autoregressive models.",
    "version": "1.1",
    "maintainer": "Jae H. Kim <jaekim8080@gmail.com>",
    "author": "Jae. H. Kim",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VAR.etp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VAR.etp VAR Modelling: Estimation, Testing, and Prediction A collection of the functions for estimation, hypothesis testing, prediction for stationary vector autoregressive models.  "
  },
  {
    "id": 7912,
    "package_name": "VAR.spec",
    "title": "Allows Specifying a Bivariate VAR (Vector Autoregression) with\nDesired Spectral Characteristics",
    "description": "The spectral characteristics of a bivariate series (Marginal Spectra, Coherency- and Phase-Spectrum) determine whether there is a strong presence of short-, medium-, or long-term fluctuations (components of certain frequencies in the spectral representation of the series) in each one of them.  These are induced by strong peaks of the marginal spectra of each series at the corresponding frequencies. The spectral characteristics also determine how strongly these short-, medium-, or long-term fluctuations of the two series are correlated between the two series. Information on this is provided by the Coherency spectrum at the corresponding frequencies. Finally, certain fluctuations of the two series may be lagged to each other. Information on this is provided by the Phase spectrum at the corresponding frequencies. The idea in this package is to define a VAR (Vector autoregression) model with desired spectral characteristics by specifying a number of polynomials, required to define the VAR. See Ioannidis(2007) <doi:10.1016/j.jspi.2005.12.013>. These are specified via their roots, instead of via their coefficients. This is an idea borrowed from the Time Series Library of R. Dahlhaus, where it is used for defining ARMA models for univariate time series. This way, one may e.g. specify a VAR inducing a strong presence of long-term fluctuations in series 1 and in series 2, which are weakly correlated, but lagged by a number of time units to each other, while short-term fluctuations in series 1 and in series 2, are strongly present only in one of the two series, while they are strongly correlated to each other between the two series. Simulation from such models allows studying the behavior of data-analysis tools, such as estimation of the spectra, under different circumstances, as e.g. peaks in the spectra, generating bias, induced by leakage.",
    "version": "1.0",
    "maintainer": "Evangelos Ioannidis <eioannid@aueb.gr>",
    "author": "Evangelos Ioannidis [cre, aut, cph],\n  Panagiotis Papastamoulis [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VAR.spec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VAR.spec Allows Specifying a Bivariate VAR (Vector Autoregression) with\nDesired Spectral Characteristics The spectral characteristics of a bivariate series (Marginal Spectra, Coherency- and Phase-Spectrum) determine whether there is a strong presence of short-, medium-, or long-term fluctuations (components of certain frequencies in the spectral representation of the series) in each one of them.  These are induced by strong peaks of the marginal spectra of each series at the corresponding frequencies. The spectral characteristics also determine how strongly these short-, medium-, or long-term fluctuations of the two series are correlated between the two series. Information on this is provided by the Coherency spectrum at the corresponding frequencies. Finally, certain fluctuations of the two series may be lagged to each other. Information on this is provided by the Phase spectrum at the corresponding frequencies. The idea in this package is to define a VAR (Vector autoregression) model with desired spectral characteristics by specifying a number of polynomials, required to define the VAR. See Ioannidis(2007) <doi:10.1016/j.jspi.2005.12.013>. These are specified via their roots, instead of via their coefficients. This is an idea borrowed from the Time Series Library of R. Dahlhaus, where it is used for defining ARMA models for univariate time series. This way, one may e.g. specify a VAR inducing a strong presence of long-term fluctuations in series 1 and in series 2, which are weakly correlated, but lagged by a number of time units to each other, while short-term fluctuations in series 1 and in series 2, are strongly present only in one of the two series, while they are strongly correlated to each other between the two series. Simulation from such models allows studying the behavior of data-analysis tools, such as estimation of the spectra, under different circumstances, as e.g. peaks in the spectra, generating bias, induced by leakage.  "
  },
  {
    "id": 7914,
    "package_name": "VARcpDetectOnline",
    "title": "Sequential Change Point Detection for High-Dimensional VAR\nModels",
    "description": "Implements the algorithm introduced in Tian, Y., and Safikhani, A. (2024)\n    <doi:10.5705/ss.202024.0182>, \"Sequential Change Point Detection in High-dimensional \n    Vector Auto-regressive Models\". This package provides tools for detecting change points \n    in the transition matrices of VAR models, effectively identifying shifts in temporal \n    and cross-correlations within high-dimensional time series data.",
    "version": "0.2.0",
    "maintainer": "Yuhan Tian <yuhan.tian@ufl.edu>",
    "author": "Yuhan Tian [aut, cre],\n  Abolfazl Safikhani [aut]",
    "url": "https://github.com/Helloworld9293/VARcpDetectOnline",
    "bug_reports": "https://github.com/Helloworld9293/VARcpDetectOnline/issues",
    "repository": "https://cran.r-project.org/package=VARcpDetectOnline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VARcpDetectOnline Sequential Change Point Detection for High-Dimensional VAR\nModels Implements the algorithm introduced in Tian, Y., and Safikhani, A. (2024)\n    <doi:10.5705/ss.202024.0182>, \"Sequential Change Point Detection in High-dimensional \n    Vector Auto-regressive Models\". This package provides tools for detecting change points \n    in the transition matrices of VAR models, effectively identifying shifts in temporal \n    and cross-correlations within high-dimensional time series data.  "
  },
  {
    "id": 7915,
    "package_name": "VARshrink",
    "title": "Shrinkage Estimation Methods for Vector Autoregressive Models",
    "description": "\n    Vector autoregressive (VAR) model is a fundamental and effective approach\n    for multivariate time series analysis. Shrinkage estimation methods can be\n    applied to high-dimensional VAR models with dimensionality greater than\n    the number of observations, contrary to the standard ordinary least squares\n    method. This package is an integrative package delivering nonparametric,\n    parametric, and semiparametric methods in a unified and consistent manner,\n    such as the multivariate ridge regression in Golub, Heath, and Wahba (1979)\n    <doi:10.2307/1268518>, a James-Stein type nonparametric shrinkage method in\n    Opgen-Rhein and Strimmer (2007) <doi:10.1186/1471-2105-8-S2-S3>, and\n    Bayesian estimation methods using noninformative and informative priors\n    in Lee, Choi, and S.-H. Kim (2016) <doi:10.1016/j.csda.2016.03.007> and\n    Ni and Sun (2005) <doi:10.1198/073500104000000622>.",
    "version": "0.3.1",
    "maintainer": "Namgil Lee <namgil.lee@kangwon.ac.kr>",
    "author": "Namgil Lee [aut, cre] (ORCID: <https://orcid.org/0000-0003-0593-9028>),\n  Heon Young Yang [ctb],\n  Sung-Ho Kim [aut]",
    "url": "https://github.com/namgillee/VARshrink/",
    "bug_reports": "https://github.com/namgillee/VARshrink/issues/",
    "repository": "https://cran.r-project.org/package=VARshrink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VARshrink Shrinkage Estimation Methods for Vector Autoregressive Models \n    Vector autoregressive (VAR) model is a fundamental and effective approach\n    for multivariate time series analysis. Shrinkage estimation methods can be\n    applied to high-dimensional VAR models with dimensionality greater than\n    the number of observations, contrary to the standard ordinary least squares\n    method. This package is an integrative package delivering nonparametric,\n    parametric, and semiparametric methods in a unified and consistent manner,\n    such as the multivariate ridge regression in Golub, Heath, and Wahba (1979)\n    <doi:10.2307/1268518>, a James-Stein type nonparametric shrinkage method in\n    Opgen-Rhein and Strimmer (2007) <doi:10.1186/1471-2105-8-S2-S3>, and\n    Bayesian estimation methods using noninformative and informative priors\n    in Lee, Choi, and S.-H. Kim (2016) <doi:10.1016/j.csda.2016.03.007> and\n    Ni and Sun (2005) <doi:10.1198/073500104000000622>.  "
  },
  {
    "id": 7916,
    "package_name": "VARtests",
    "title": "Bootstrap Tests for Cointegration and Autocorrelation in VARs",
    "description": "Implements wild bootstrap tests for autocorrelation in Vector\n    Autoregressive (VAR) models based on Ahlgren and Catani (2016)\n    <doi:10.1007/s00362-016-0744-0>, a combined Lagrange Multiplier (LM)\n    test for Autoregressive Conditional Heteroskedasticity (ARCH) in VAR\n    models from Catani and Ahlgren (2016) <doi:10.1016/j.ecosta.2016.10.006>,\n    and bootstrap-based methods for determining the cointegration rank from\n    Cavaliere, Rahbek, and Taylor (2012) <doi:10.3982/ECTA9099> and\n    Cavaliere, Rahbek, and Taylor (2014) <doi:10.1080/07474938.2013.825175>.",
    "version": "2.0.7",
    "maintainer": "Markus Belfrage <markus.belfrage@gmail.com>",
    "author": "Markus Belfrage [aut, cre],\n  Paul Catani [ctb],\n  Niklas Ahlgren [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VARtests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VARtests Bootstrap Tests for Cointegration and Autocorrelation in VARs Implements wild bootstrap tests for autocorrelation in Vector\n    Autoregressive (VAR) models based on Ahlgren and Catani (2016)\n    <doi:10.1007/s00362-016-0744-0>, a combined Lagrange Multiplier (LM)\n    test for Autoregressive Conditional Heteroskedasticity (ARCH) in VAR\n    models from Catani and Ahlgren (2016) <doi:10.1016/j.ecosta.2016.10.006>,\n    and bootstrap-based methods for determining the cointegration rank from\n    Cavaliere, Rahbek, and Taylor (2012) <doi:10.3982/ECTA9099> and\n    Cavaliere, Rahbek, and Taylor (2014) <doi:10.1080/07474938.2013.825175>.  "
  },
  {
    "id": 7920,
    "package_name": "VBTree",
    "title": "Vector Binary Tree to Make Your Data Management More Efficient",
    "description": "Vector binary tree provides a new data structure, to\n make your data visiting and management more efficient. If the\n data has structured column names, it can read these names and\n factorize them through specific split pattern, then build the mappings\n within double list, vector binary tree, array and tensor mutually, through\n which the batched data processing is achievable easily. The methods of\n array and tensor are also applicable. Detailed methods are described in\n Chen Zhang et al. (2020) <doi:10.35566/isdsa2019c8>.",
    "version": "0.1.1",
    "maintainer": "Chen Zhang <chen.zhang_06sept@foxmail.com>",
    "author": "Chen Zhang [aut, cre, cph] (0009-0007-7689-5030)",
    "url": "https://github.com/CubicZebra/VBTree",
    "bug_reports": "https://github.com/CubicZebra/VBTree/issues",
    "repository": "https://cran.r-project.org/package=VBTree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VBTree Vector Binary Tree to Make Your Data Management More Efficient Vector binary tree provides a new data structure, to\n make your data visiting and management more efficient. If the\n data has structured column names, it can read these names and\n factorize them through specific split pattern, then build the mappings\n within double list, vector binary tree, array and tensor mutually, through\n which the batched data processing is achievable easily. The methods of\n array and tensor are also applicable. Detailed methods are described in\n Chen Zhang et al. (2020) <doi:10.35566/isdsa2019c8>.  "
  },
  {
    "id": 7936,
    "package_name": "VGAM",
    "title": "Vector Generalized Linear and Additive Models",
    "description": "An implementation of about 6 major classes of\n    statistical regression models. The central algorithm is\n    Fisher scoring and iterative reweighted least squares.\n    At the heart of this package are the vector generalized linear\n    and additive model (VGLM/VGAM) classes. VGLMs can be loosely\n    thought of as multivariate GLMs. VGAMs are data-driven\n    VGLMs that use smoothing. The book \"Vector Generalized\n    Linear and Additive Models: With an Implementation in R\"\n    (Yee, 2015) <DOI:10.1007/978-1-4939-2818-7> gives details of\n    the statistical framework and the package. Currently only\n    fixed-effects models are implemented. Many (100+) models and\n    distributions are estimated by maximum likelihood estimation\n    (MLE) or penalized MLE. The other classes are RR-VGLMs\n    (reduced-rank VGLMs), quadratic RR-VGLMs, doubly constrained\n    RR-VGLMs, quadratic RR-VGLMs, reduced-rank VGAMs,\n    RCIMs (row-column interaction models)---these classes perform\n    constrained and unconstrained quadratic ordination (CQO/UQO)\n    models in ecology, as well as constrained additive ordination\n    (CAO). Hauck-Donner effect detection is implemented.\n    Note that these functions are subject to change;\n    see the NEWS and ChangeLog files for latest changes.",
    "version": "1.1-14",
    "maintainer": "Thomas Yee <t.yee@auckland.ac.nz>",
    "author": "Thomas Yee [aut, cre] (ORCID: <https://orcid.org/0000-0002-9970-3907>),\n  Cleve Moler [ctb] (LINPACK routines in src)",
    "url": "https://www.stat.auckland.ac.nz/~yee/VGAM/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VGAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VGAM Vector Generalized Linear and Additive Models An implementation of about 6 major classes of\n    statistical regression models. The central algorithm is\n    Fisher scoring and iterative reweighted least squares.\n    At the heart of this package are the vector generalized linear\n    and additive model (VGLM/VGAM) classes. VGLMs can be loosely\n    thought of as multivariate GLMs. VGAMs are data-driven\n    VGLMs that use smoothing. The book \"Vector Generalized\n    Linear and Additive Models: With an Implementation in R\"\n    (Yee, 2015) <DOI:10.1007/978-1-4939-2818-7> gives details of\n    the statistical framework and the package. Currently only\n    fixed-effects models are implemented. Many (100+) models and\n    distributions are estimated by maximum likelihood estimation\n    (MLE) or penalized MLE. The other classes are RR-VGLMs\n    (reduced-rank VGLMs), quadratic RR-VGLMs, doubly constrained\n    RR-VGLMs, quadratic RR-VGLMs, reduced-rank VGAMs,\n    RCIMs (row-column interaction models)---these classes perform\n    constrained and unconstrained quadratic ordination (CQO/UQO)\n    models in ecology, as well as constrained additive ordination\n    (CAO). Hauck-Donner effect detection is implemented.\n    Note that these functions are subject to change;\n    see the NEWS and ChangeLog files for latest changes.  "
  },
  {
    "id": 7937,
    "package_name": "VGAMdata",
    "title": "Data Supporting the 'VGAM' Package",
    "description": "Mainly data sets to accompany the VGAM package and\n\tthe book \"Vector Generalized Linear and\n\tAdditive Models: With an Implementation in R\" (Yee, 2015)\n\t<DOI:10.1007/978-1-4939-2818-7>.\n\tThese are used to illustrate vector generalized\n\tlinear and additive models (VGLMs/VGAMs), and associated models\n\t(Reduced-Rank VGLMs, Quadratic RR-VGLMs, Row-Column\n\tInteraction Models, and constrained and unconstrained ordination\n\tmodels in ecology). This package now contains some\n\told VGAM family functions which have been replaced by newer\n\tones (often because they are now special cases).",
    "version": "1.1-13",
    "maintainer": "Thomas Yee <t.yee@auckland.ac.nz>",
    "author": "Thomas Yee [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9970-3907>),\n  James Gray [dtc]",
    "url": "https://www.stat.auckland.ac.nz/~yee/VGAMdata/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VGAMdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VGAMdata Data Supporting the 'VGAM' Package Mainly data sets to accompany the VGAM package and\n\tthe book \"Vector Generalized Linear and\n\tAdditive Models: With an Implementation in R\" (Yee, 2015)\n\t<DOI:10.1007/978-1-4939-2818-7>.\n\tThese are used to illustrate vector generalized\n\tlinear and additive models (VGLMs/VGAMs), and associated models\n\t(Reduced-Rank VGLMs, Quadratic RR-VGLMs, Row-Column\n\tInteraction Models, and constrained and unconstrained ordination\n\tmodels in ecology). This package now contains some\n\told VGAM family functions which have been replaced by newer\n\tones (often because they are now special cases).  "
  },
  {
    "id": 7967,
    "package_name": "VecDep",
    "title": "Measuring Copula-Based Dependence Between Random Vectors",
    "description": "Provides functions for estimation (parametric, semi-parametric and non-parametric)\n             of copula-based dependence coefficients between a finite collection of random vectors,\n             including phi-dependence measures and Bures-Wasserstein dependence measures. \n             An algorithm for agglomerative hierarchical variable clustering is also implemented.\n             Following the articles De Keyser & Gijbels (2024) <doi:10.1016/j.jmva.2024.105336>,\n             De Keyser & Gijbels (2024) <doi:10.1016/j.ijar.2023.109090>, and De Keyser & Gijbels (2024)\n             <doi:10.48550/arXiv.2404.07141>.",
    "version": "0.1.3",
    "maintainer": "Steven De Keyser <steven.dekeyser@kuleuven.be>",
    "author": "Steven De Keyser [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3469-8692>),\n  Ir\u00e8ne Gijbels [ctb] (ORCID: <https://orcid.org/0000-0002-4443-9803>)",
    "url": "https://github.com/StevenDeKeyser98/VecDep",
    "bug_reports": "https://github.com/StevenDeKeyser98/VecDep/issues",
    "repository": "https://cran.r-project.org/package=VecDep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VecDep Measuring Copula-Based Dependence Between Random Vectors Provides functions for estimation (parametric, semi-parametric and non-parametric)\n             of copula-based dependence coefficients between a finite collection of random vectors,\n             including phi-dependence measures and Bures-Wasserstein dependence measures. \n             An algorithm for agglomerative hierarchical variable clustering is also implemented.\n             Following the articles De Keyser & Gijbels (2024) <doi:10.1016/j.jmva.2024.105336>,\n             De Keyser & Gijbels (2024) <doi:10.1016/j.ijar.2023.109090>, and De Keyser & Gijbels (2024)\n             <doi:10.48550/arXiv.2404.07141>.  "
  },
  {
    "id": 7969,
    "package_name": "VectorCodeR",
    "title": "Easily Analyze Your Gait Patterns Using Vector Coding Technique",
    "description": "\n  Facilitate the analysis of inter-limb and intra-limb coordination in human movement. \n  It provides functions for calculating the phase angle between two segments, enabling researchers and practitioners to quantify the coordination patterns within and between limbs during various motor tasks.\n  Needham, R., Naemi, R., & Chockalingam, N. (2014) <doi:10.1016/j.jbiomech.2013.12.032>.\n  Needham, R., Naemi, R., & Chockalingam, N. (2015) <doi:10.1016/j.jbiomech.2015.07.023>.\n  Tepavac, D., & Field-Fote, E. C. (2001) <doi:10.1123/jab.17.3.259>.\n  Park, J.H., Lee, H., Cho, Js. et al. (2021) <doi:10.1038/s41598-020-80237-w>.",
    "version": "0.2.0",
    "maintainer": "zhexuan gu <22054513g@connect.polyu.hk>",
    "author": "pwh kwong [aut],\n  ananda sidarta [ctb],\n  zhexuan gu [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VectorCodeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VectorCodeR Easily Analyze Your Gait Patterns Using Vector Coding Technique \n  Facilitate the analysis of inter-limb and intra-limb coordination in human movement. \n  It provides functions for calculating the phase angle between two segments, enabling researchers and practitioners to quantify the coordination patterns within and between limbs during various motor tasks.\n  Needham, R., Naemi, R., & Chockalingam, N. (2014) <doi:10.1016/j.jbiomech.2013.12.032>.\n  Needham, R., Naemi, R., & Chockalingam, N. (2015) <doi:10.1016/j.jbiomech.2015.07.023>.\n  Tepavac, D., & Field-Fote, E. C. (2001) <doi:10.1123/jab.17.3.259>.\n  Park, J.H., Lee, H., Cho, Js. et al. (2021) <doi:10.1038/s41598-020-80237-w>.  "
  },
  {
    "id": 8009,
    "package_name": "WEGE",
    "title": "A Metric to Rank Locations for Biodiversity Conservation",
    "description": "Calculates the WEGE (Weighted Endemism including Global \n    Endangerment index) index for a particular area. Additionally it also \n    calculates rasters of KBA's (Key Biodiversity Area) criteria (A1a, A1b, A1e, \n    and B1), Weighted endemism (WE), the EDGE (Evolutionarily Distinct and\n    Globally Endangered) score, Evolutionary Distinctiveness (ED) and Extinction\n    risk (ER). Farooq, H., Azevedo, J., Belluardo F., Nanvonamuquitxo, C.,\n    Bennett, D., Moat, J., Soares, A., Faurby, S. & Antonelli, A. (2020)\n    <doi:10.1101/2020.01.17.910299>.",
    "version": "0.1.0",
    "maintainer": "Harith Farooq <harithmorgadinho@gmail.com>",
    "author": "Harith Farooq [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9031-2785>),\n  Josu\u00e9 Azevedo [aut],\n  Francesco Belluardo [aut],\n  Crist\u00f3v\u00e3o Nanvonamuquitxo [aut],\n  Dom Bennett [aut] (ORCID: <https://orcid.org/0000-0003-2722-1359>),\n  Jason Moat [aut],\n  Amadeu Soares [aut],\n  S\u00f8ren Faurby [aut],\n  Alexandre Antonelli [aut] (ORCID:\n    <https://orcid.org/0000-0003-1842-9297>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WEGE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WEGE A Metric to Rank Locations for Biodiversity Conservation Calculates the WEGE (Weighted Endemism including Global \n    Endangerment index) index for a particular area. Additionally it also \n    calculates rasters of KBA's (Key Biodiversity Area) criteria (A1a, A1b, A1e, \n    and B1), Weighted endemism (WE), the EDGE (Evolutionarily Distinct and\n    Globally Endangered) score, Evolutionary Distinctiveness (ED) and Extinction\n    risk (ER). Farooq, H., Azevedo, J., Belluardo F., Nanvonamuquitxo, C.,\n    Bennett, D., Moat, J., Soares, A., Faurby, S. & Antonelli, A. (2020)\n    <doi:10.1101/2020.01.17.910299>.  "
  },
  {
    "id": 8034,
    "package_name": "WaveSampling",
    "title": "Weakly Associated Vectors (WAVE) Sampling",
    "description": "Spatial data are generally auto-correlated, meaning that if two \n  units selected are close to each other, then it is likely that they share the\n  same properties. For this reason, when sampling in the population it is often\n  needed that the sample is well spread over space. A new method to draw a sample\n  from a population with spatial coordinates is proposed. This method is called\n  wave (Weakly Associated Vectors) sampling. It uses the less correlated vector\n  to a spatial weights matrix to update the inclusion probabilities vector\n  into a sample. For more details see Rapha\u00ebl Jauslin and Yves Till\u00e9 (2019) <doi:10.1007/s13253-020-00407-1>.",
    "version": "0.1.4",
    "maintainer": "Rapha\u00ebl Jauslin <raphael.jauslin@bfs.admin.ch>",
    "author": "Rapha\u00ebl Jauslin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1088-3356>),\n  Yves Till\u00e9 [aut] (ORCID: <https://orcid.org/0000-0003-0904-5523>)",
    "url": "https://github.com/RJauslin/WaveSampling",
    "bug_reports": "https://github.com/RJauslin/WaveSampling/issues",
    "repository": "https://cran.r-project.org/package=WaveSampling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WaveSampling Weakly Associated Vectors (WAVE) Sampling Spatial data are generally auto-correlated, meaning that if two \n  units selected are close to each other, then it is likely that they share the\n  same properties. For this reason, when sampling in the population it is often\n  needed that the sample is well spread over space. A new method to draw a sample\n  from a population with spatial coordinates is proposed. This method is called\n  wave (Weakly Associated Vectors) sampling. It uses the less correlated vector\n  to a spatial weights matrix to update the inclusion probabilities vector\n  into a sample. For more details see Rapha\u00ebl Jauslin and Yves Till\u00e9 (2019) <doi:10.1007/s13253-020-00407-1>.  "
  },
  {
    "id": 8043,
    "package_name": "WaveletML",
    "title": "Wavelet Decomposition Based Hybrid Machine Learning Models",
    "description": "Wavelet decomposes a series into multiple sub series called detailed and smooth components which helps to capture volatility at multi resolution level by various models. Two hybrid Machine Learning (ML) models (Artificial Neural Network and Support Vector Regression have been used) have been developed in combination with stochastic models, feature selection, and optimization algorithms for prediction of the data. The algorithms have been developed following Paul and Garai (2021)  <doi:10.1007/s00500-021-06087-4>. ",
    "version": "0.1.0",
    "maintainer": "Mr. Sandip Garai <sandipnicksandy@gmail.com>",
    "author": "Mr. Sandip Garai [aut, cre],\n  Dr. Ranjit Kumar Paul [aut],\n  Dr. Md Yeasin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WaveletML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WaveletML Wavelet Decomposition Based Hybrid Machine Learning Models Wavelet decomposes a series into multiple sub series called detailed and smooth components which helps to capture volatility at multi resolution level by various models. Two hybrid Machine Learning (ML) models (Artificial Neural Network and Support Vector Regression have been used) have been developed in combination with stochastic models, feature selection, and optimization algorithms for prediction of the data. The algorithms have been developed following Paul and Garai (2021)  <doi:10.1007/s00500-021-06087-4>.   "
  },
  {
    "id": 8046,
    "package_name": "WaveletSVR",
    "title": "Wavelet-SVR Hybrid Model for Time Series Forecasting",
    "description": "The main aim of this package is to combine the advantage of wavelet and support vector machine models for time series forecasting. This package also gives the accuracy measurements in terms of RMSE and MAPE. This package fits the hybrid Wavelet SVR model for time series forecasting The main aim of this package is to combine the advantage of wavelet and Support Vector Regression (SVR) models for time series forecasting. This package also gives the accuracy measurements in terms of Root Mean Square Error (RMSE) and Mean Absolute Prediction Error (MAPE). This package is based on the algorithm of Raimundo and Okamoto (2018) <DOI: 10.1109/INFOCT.2018.8356851>.",
    "version": "0.1.0",
    "maintainer": "Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "author": "Ranjit Kumar Paul [aut, cre],\n  Md Yeasin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WaveletSVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WaveletSVR Wavelet-SVR Hybrid Model for Time Series Forecasting The main aim of this package is to combine the advantage of wavelet and support vector machine models for time series forecasting. This package also gives the accuracy measurements in terms of RMSE and MAPE. This package fits the hybrid Wavelet SVR model for time series forecasting The main aim of this package is to combine the advantage of wavelet and Support Vector Regression (SVR) models for time series forecasting. This package also gives the accuracy measurements in terms of Root Mean Square Error (RMSE) and Mean Absolute Prediction Error (MAPE). This package is based on the algorithm of Raimundo and Okamoto (2018) <DOI: 10.1109/INFOCT.2018.8356851>.  "
  },
  {
    "id": 8063,
    "package_name": "WeightSVM",
    "title": "Subject Weighted Support Vector Machines",
    "description": "Functions for subject/instance weighted support vector machines (SVM). \n    It uses a modified version of 'libsvm' and is compatible with package 'e1071'. It also allows user defined kernel matrix.",
    "version": "1.7-16",
    "maintainer": "Tianchen Xu <tx2155@columbia.edu>",
    "author": "Tianchen Xu [aut, cre] (ORCID: <https://orcid.org/0000-0002-0102-7630>),\n  Chih-Chung Chang [ctb, cph] (libsvm C++-code),\n  Chih-Chen Lin [ctb, cph] (libsvm C++-code),\n  Ming-Wei Chang [ctb, cph] (libsvm C++-code),\n  Hsuan-Tien Lin [ctb, cph] (libsvm C++-code),\n  Ming-Hen Tsai [ctb, cph] (libsvm C++-code),\n  Chia-Hua Ho [ctb, cph] (libsvm C++-code),\n  Hsiang-Fu Yu [ctb, cph] (libsvm C++-code),\n  David Meyer [ctb],\n  Evgenia Dimitriadou [ctb],\n  Kurt Hornik [ctb],\n  Andreas Weingessel [ctb],\n  Friedrich Leisch [ctb]",
    "url": "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances",
    "bug_reports": "https://github.com/zjph602xtc/wsvm/issues",
    "repository": "https://cran.r-project.org/package=WeightSVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeightSVM Subject Weighted Support Vector Machines Functions for subject/instance weighted support vector machines (SVM). \n    It uses a modified version of 'libsvm' and is compatible with package 'e1071'. It also allows user defined kernel matrix.  "
  },
  {
    "id": 8172,
    "package_name": "abind",
    "title": "Combine Multidimensional Arrays",
    "description": "Combine multidimensional arrays into a single array.\n  This is a generalization of 'cbind' and 'rbind'.  Works with\n  vectors, matrices, and higher-dimensional arrays (aka tensors).\n  Also provides functions 'adrop', 'asub', and 'afill' for\n  manipulating, extracting and replacing data in arrays.",
    "version": "1.4-8",
    "maintainer": "Tony Plate <tplate@acm.org>",
    "author": "Tony Plate [aut, cre],\n  Richard Heiberger [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=abind",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "abind Combine Multidimensional Arrays Combine multidimensional arrays into a single array.\n  This is a generalization of 'cbind' and 'rbind'.  Works with\n  vectors, matrices, and higher-dimensional arrays (aka tensors).\n  Also provides functions 'adrop', 'asub', and 'afill' for\n  manipulating, extracting and replacing data in arrays.  "
  },
  {
    "id": 8201,
    "package_name": "acdcR",
    "title": "Agro-Climatic Data by County",
    "description": "The functions are designed to calculate the most widely-used county-level variables in \n  agricultural production or agricultural-climatic and weather analyses. To operate some functions \n  in this package needs download of the bulk PRISM raster. See the examples, testing versions and \n  more details from: <https://github.com/ysd2004/acdcR>.",
    "version": "1.0.0",
    "maintainer": "Seong D. Yun <seong.yun@msstate.edu>",
    "author": "Seong D. Yun [aut, cre]",
    "url": "https://github.com/ysd2004/acdcR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=acdcR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acdcR Agro-Climatic Data by County The functions are designed to calculate the most widely-used county-level variables in \n  agricultural production or agricultural-climatic and weather analyses. To operate some functions \n  in this package needs download of the bulk PRISM raster. See the examples, testing versions and \n  more details from: <https://github.com/ysd2004/acdcR>.  "
  },
  {
    "id": 8207,
    "package_name": "acfMPeriod",
    "title": "Robust Estimation of the ACF from the M-Periodogram",
    "description": "Non-robust and robust computations of the sample autocovariance (ACOVF) and sample autocorrelation functions (ACF) of univariate and multivariate processes. The methodology consists in reversing the diagonalization procedure involving the periodogram or the cross-periodogram and the Fourier transform vectors, and, thus, obtaining the ACOVF or the ACF as discussed in Fuller (1995) <doi:10.1002/9780470316917>. The robust version is obtained by fitting robust M-regressors to obtain the M-periodogram or M-cross-periodogram as discussed in Reisen et al. (2017) <doi:10.1016/j.jspi.2017.02.008>.",
    "version": "1.0.0",
    "maintainer": "Higor Cotta <cotta.higor@gmail.com>",
    "author": "Higor Cotta, Valderio Reisen, Pascal Bondon and C\u00e9line L\u00e9vy-Leduc",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=acfMPeriod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acfMPeriod Robust Estimation of the ACF from the M-Periodogram Non-robust and robust computations of the sample autocovariance (ACOVF) and sample autocorrelation functions (ACF) of univariate and multivariate processes. The methodology consists in reversing the diagonalization procedure involving the periodogram or the cross-periodogram and the Fourier transform vectors, and, thus, obtaining the ACOVF or the ACF as discussed in Fuller (1995) <doi:10.1002/9780470316917>. The robust version is obtained by fitting robust M-regressors to obtain the M-periodogram or M-cross-periodogram as discussed in Reisen et al. (2017) <doi:10.1016/j.jspi.2017.02.008>.  "
  },
  {
    "id": 8256,
    "package_name": "adaptivetau",
    "title": "Tau-Leaping Stochastic Simulation",
    "description": "Implements adaptive tau leaping to approximate the\n        trajectory of a continuous-time stochastic process as\n        described by Cao et al. (2007) The Journal of Chemical Physics\n        <doi:10.1063/1.2745299> (aka. the Gillespie stochastic\n        simulation algorithm).  This package is based upon work\n        supported by NSF DBI-0906041 and NIH K99-GM104158 to Philip\n        Johnson and NIH R01-AI049334 to Rustom Antia.",
    "version": "2.3-2",
    "maintainer": "Philip Johnson <plfj@umd.edu>",
    "author": "Philip Johnson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6087-7064>)",
    "url": "https://github.com/plfjohnson/adaptivetau",
    "bug_reports": "https://github.com/plfjohnson/adaptivetau/issues",
    "repository": "https://cran.r-project.org/package=adaptivetau",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adaptivetau Tau-Leaping Stochastic Simulation Implements adaptive tau leaping to approximate the\n        trajectory of a continuous-time stochastic process as\n        described by Cao et al. (2007) The Journal of Chemical Physics\n        <doi:10.1063/1.2745299> (aka. the Gillespie stochastic\n        simulation algorithm).  This package is based upon work\n        supported by NSF DBI-0906041 and NIH K99-GM104158 to Philip\n        Johnson and NIH R01-AI049334 to Rustom Antia.  "
  },
  {
    "id": 8270,
    "package_name": "addinsJoaoMelo",
    "title": "Addins Made of Joao Melo",
    "description": "Provide addins for 'RStudio'.\n  It currently contains 3 addins. The first to add a shortcut for the double pipe. The second is to add a shortcut for the same operator. And the third to simplify the creation of vectors from texts pasted from the computer transfer area.",
    "version": "0.1.0",
    "maintainer": "Joao Melo <pjoao266@gmail.com>",
    "author": "Joao Melo [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=addinsJoaoMelo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "addinsJoaoMelo Addins Made of Joao Melo Provide addins for 'RStudio'.\n  It currently contains 3 addins. The first to add a shortcut for the double pipe. The second is to add a shortcut for the same operator. And the third to simplify the creation of vectors from texts pasted from the computer transfer area.  "
  },
  {
    "id": 8283,
    "package_name": "adehabitatMA",
    "title": "Tools to Deal with Raster Maps",
    "description": "A collection of tools to deal with raster maps.",
    "version": "0.3.17",
    "maintainer": "Clement Calenge <clement.calenge@ofb.gouv.fr>",
    "author": "Clement Calenge [aut, cre],\n  Mathieu Basille [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=adehabitatMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adehabitatMA Tools to Deal with Raster Maps A collection of tools to deal with raster maps.  "
  },
  {
    "id": 8289,
    "package_name": "adespatial",
    "title": "Multivariate Multiscale Spatial Analysis",
    "description": "Tools for the multiscale spatial analysis of multivariate data.\n    Several methods are based on the use of a spatial weighting matrix and its\n    eigenvector decomposition (Moran's Eigenvectors Maps, MEM). \n    Several approaches are described in the review Dray et al (2012)\n    <doi:10.1890/11-1183.1>.",
    "version": "0.3-28",
    "maintainer": "Aur\u00e9lie Siberchicot <aurelie.siberchicot@univ-lyon1.fr>",
    "author": "St\u00e9phane Dray [aut] (ORCID: <https://orcid.org/0000-0003-0153-1105>),\n  David Bauman [ctb],\n  Guillaume Blanchet [ctb],\n  Daniel Borcard [ctb],\n  Sylvie Clappe [ctb],\n  Guillaume Guenard [ctb] (ORCID:\n    <https://orcid.org/0000-0003-0761-3072>),\n  Thibaut Jombart [ctb],\n  Guillaume Larocque [ctb],\n  Pierre Legendre [ctb] (ORCID: <https://orcid.org/0000-0002-3838-3305>),\n  Naima Madi [ctb],\n  H\u00e9l\u00e8ne H Wagner [ctb],\n  Aur\u00e9lie Siberchicot [ctb, cre] (ORCID:\n    <https://orcid.org/0000-0002-7638-8318>),\n  John Chambers [ctb] (Original author of two functions from package\n    SoDA)",
    "url": "https://github.com/adeverse/adespatial,\nhttp://adeverse.github.io/adespatial/",
    "bug_reports": "https://github.com/adeverse/adespatial/issues",
    "repository": "https://cran.r-project.org/package=adespatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adespatial Multivariate Multiscale Spatial Analysis Tools for the multiscale spatial analysis of multivariate data.\n    Several methods are based on the use of a spatial weighting matrix and its\n    eigenvector decomposition (Moran's Eigenvectors Maps, MEM). \n    Several approaches are described in the review Dray et al (2012)\n    <doi:10.1890/11-1183.1>.  "
  },
  {
    "id": 8293,
    "package_name": "adheRenceRX",
    "title": "Assess Medication Adherence from Pharmaceutical Claims Data",
    "description": "A (mildly) opinionated set of functions to help assess medication adherence for researchers working with medication claims data.\n    Medication adherence analyses have several complex steps that are often convoluted and can be time-intensive. The focus is to create a \n    set of functions using \"tidy principles\" geared towards transparency, speed, and flexibility while working with adherence metrics. All functions perform exactly one task \n    with an intuitive name so that a researcher can handle details (often achieved with vectorized solutions) while we handle non-vectorized tasks common to most \n    adherence calculations such as adjusting fill dates and determining episodes of care. The methodologies in referenced in this package come from\n    Canfield SL, et al (2019) \"Navigating the Wild West of Medication Adherence Reporting in Specialty Pharmacy\" <doi:10.18553/jmcp.2019.25.10.1073>.",
    "version": "1.0.0",
    "maintainer": "Brennan Beal <brennanbeal@gmail.com>",
    "author": "Brennan Beal [aut, cre]",
    "url": "https://github.com/btbeal/adheRenceRX",
    "bug_reports": "https://github.com/btbeal/adheRenceRX/issues",
    "repository": "https://cran.r-project.org/package=adheRenceRX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adheRenceRX Assess Medication Adherence from Pharmaceutical Claims Data A (mildly) opinionated set of functions to help assess medication adherence for researchers working with medication claims data.\n    Medication adherence analyses have several complex steps that are often convoluted and can be time-intensive. The focus is to create a \n    set of functions using \"tidy principles\" geared towards transparency, speed, and flexibility while working with adherence metrics. All functions perform exactly one task \n    with an intuitive name so that a researcher can handle details (often achieved with vectorized solutions) while we handle non-vectorized tasks common to most \n    adherence calculations such as adjusting fill dates and determining episodes of care. The methodologies in referenced in this package come from\n    Canfield SL, et al (2019) \"Navigating the Wild West of Medication Adherence Reporting in Specialty Pharmacy\" <doi:10.18553/jmcp.2019.25.10.1073>.  "
  },
  {
    "id": 8297,
    "package_name": "adjSURVCI",
    "title": "Parameter and Adjusted Probability Estimation for Right-Censored\nData",
    "description": "Functions in this package fit a stratified Cox proportional hazards and a proportional subdistribution hazards model by extending Zhang et al., (2007) <doi: 10.1016/j.cmpb.2007.07.010> \n  and Zhang et al., (2011) <doi: 10.1016/j.cmpb.2010.07.005> respectively to clustered right-censored data. The functions also provide the estimates of the cumulative baseline hazard along with their standard errors. Furthermore, the adjusted survival and cumulative incidence probabilities are also provided along with their standard errors. Finally, the estimate of cumulative incidence and survival probabilities given a vector of covariates along with their standard errors are also provided.",
    "version": "1.0",
    "maintainer": "Manoj Khanal <themanoj2008@gmail.com>",
    "author": "Manoj Khanal [aut, cre],\n  Kwang Woo Ahn [aut, ths],\n  Soyoung Kim [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=adjSURVCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adjSURVCI Parameter and Adjusted Probability Estimation for Right-Censored\nData Functions in this package fit a stratified Cox proportional hazards and a proportional subdistribution hazards model by extending Zhang et al., (2007) <doi: 10.1016/j.cmpb.2007.07.010> \n  and Zhang et al., (2011) <doi: 10.1016/j.cmpb.2010.07.005> respectively to clustered right-censored data. The functions also provide the estimates of the cumulative baseline hazard along with their standard errors. Furthermore, the adjusted survival and cumulative incidence probabilities are also provided along with their standard errors. Finally, the estimate of cumulative incidence and survival probabilities given a vector of covariates along with their standard errors are also provided.  "
  },
  {
    "id": 8312,
    "package_name": "admisc",
    "title": "Adrian Dusa's Miscellaneous",
    "description": "Contains functions used across packages 'DDIwR', 'QCA' and 'venn'.\n    Interprets and translates, factorizes and negates SOP - Sum of Products\n    expressions, for both binary and multi-value crisp sets, and extracts\n    information (set names, set values) from those expressions. Other functions\n    perform various other checks if possibly numeric (even if all numbers reside\n    in a character vector) and coerce to numeric, or check if the numbers are\n    whole. It also offers, among many others, a highly versatile recoding\n    routine and some more flexible alternatives to the base functions 'with()'\n    and 'within()'.\n    SOP simplification functions in this package use related minimization from\n    package 'QCA', which is recommended to be installed despite not being listed\n    in the Imports field, due to circular dependency issues.",
    "version": "0.39",
    "maintainer": "Adrian Dusa <dusa.adrian@unibuc.ro>",
    "author": "Adrian Dusa [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3525-9253>)",
    "url": "https://github.com/dusadrian/admisc",
    "bug_reports": "https://github.com/dusadrian/admisc/issues",
    "repository": "https://cran.r-project.org/package=admisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admisc Adrian Dusa's Miscellaneous Contains functions used across packages 'DDIwR', 'QCA' and 'venn'.\n    Interprets and translates, factorizes and negates SOP - Sum of Products\n    expressions, for both binary and multi-value crisp sets, and extracts\n    information (set names, set values) from those expressions. Other functions\n    perform various other checks if possibly numeric (even if all numbers reside\n    in a character vector) and coerce to numeric, or check if the numbers are\n    whole. It also offers, among many others, a highly versatile recoding\n    routine and some more flexible alternatives to the base functions 'with()'\n    and 'within()'.\n    SOP simplification functions in this package use related minimization from\n    package 'QCA', which is recommended to be installed despite not being listed\n    in the Imports field, due to circular dependency issues.  "
  },
  {
    "id": 8342,
    "package_name": "affinity",
    "title": "Raster Georeferencing, Grid Affine Transforms, Cell Abstraction",
    "description": "Tools for raster georeferencing, grid affine transforms, and general raster logic. \n These functions provide converters between raster specifications, world vector, geotransform, \n 'RasterIO' window, and 'RasterIO window' in 'sf' package list format. There are functions to offset\n a matrix by padding any of four corners (useful for vectorizing neighbourhood operations), and\n helper functions to harvesting user clicks on a graphics device to use for simple georeferencing\n of images.  Methods used are available from <https://en.wikipedia.org/wiki/World_file> and\n <https://gdal.org/user/raster_data_model.html>. ",
    "version": "0.2.5",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre]",
    "url": "https://github.com/hypertidy/affinity",
    "bug_reports": "https://github.com/hypertidy/affinity/issues",
    "repository": "https://cran.r-project.org/package=affinity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "affinity Raster Georeferencing, Grid Affine Transforms, Cell Abstraction Tools for raster georeferencing, grid affine transforms, and general raster logic. \n These functions provide converters between raster specifications, world vector, geotransform, \n 'RasterIO' window, and 'RasterIO window' in 'sf' package list format. There are functions to offset\n a matrix by padding any of four corners (useful for vectorizing neighbourhood operations), and\n helper functions to harvesting user clicks on a graphics device to use for simple georeferencing\n of images.  Methods used are available from <https://en.wikipedia.org/wiki/World_file> and\n <https://gdal.org/user/raster_data_model.html>.   "
  },
  {
    "id": 8358,
    "package_name": "ageg",
    "title": "Age Grouping Functions",
    "description": "Pair of simple convenience functions to convert a vector of birth dates to age and age distributions. These functions may be helpful when related age and custom age distributions are desired given a vector of birth dates.",
    "version": "1.0.0",
    "maintainer": "Austin Anders <nobilisvenator@hotmail.com>",
    "author": "Austin Anders [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ageg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ageg Age Grouping Functions Pair of simple convenience functions to convert a vector of birth dates to age and age distributions. These functions may be helpful when related age and custom age distributions are desired given a vector of birth dates.  "
  },
  {
    "id": 8377,
    "package_name": "agriwater",
    "title": "Evapotranspiration and Energy Fluxes Spatial Analysis",
    "description": "Spatial modeling of energy balance and actual \n    evapotranspiration using satellite images and meteorological data. \n    Options of satellite are: Landsat-8 (with and without thermal bands), \n    Sentinel-2 and MODIS. Respectively spatial resolutions are 30, 100, \n    10 and 250 meters. User can use data from a single meteorological \n    station or a grid of meteorological stations (using any spatial \n    interpolation method). Silva, Teixeira, and Manzione (2019) <doi:10.1016/j.envsoft.2019.104497>.",
    "version": "1.0.2",
    "maintainer": "Cesar de Oliveira Ferreira Silva <cesaroliveira.f.silva@gmail.com>",
    "author": "Cesar de Oliveira Ferreira Silva [aut, cre],\n  Antonio Heriberto de Castro Teixeira [ctb],\n  Rodrigo Lilla Manzione [aut]",
    "url": "",
    "bug_reports": "https://github.com/cesarofs/agriwater/issues",
    "repository": "https://cran.r-project.org/package=agriwater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "agriwater Evapotranspiration and Energy Fluxes Spatial Analysis Spatial modeling of energy balance and actual \n    evapotranspiration using satellite images and meteorological data. \n    Options of satellite are: Landsat-8 (with and without thermal bands), \n    Sentinel-2 and MODIS. Respectively spatial resolutions are 30, 100, \n    10 and 250 meters. User can use data from a single meteorological \n    station or a grid of meteorological stations (using any spatial \n    interpolation method). Silva, Teixeira, and Manzione (2019) <doi:10.1016/j.envsoft.2019.104497>.  "
  },
  {
    "id": 8481,
    "package_name": "ammistability",
    "title": "Additive Main Effects and Multiplicative Interaction Model\nStability Parameters",
    "description": "Computes various stability parameters from Additive Main Effects\n    and Multiplicative Interaction (AMMI) analysis results such as  Modified\n    AMMI Stability Value (MASV), Sums of the Absolute Value of the Interaction\n    Principal Component Scores (SIPC), Sum Across Environments of\n    Genotype-Environment Interaction Modelled by AMMI (AMGE), Sum Across \n    Environments of Absolute Value of Genotype-Environment Interaction Modelled\n    by AMMI (AV_(AMGE)), AMMI Stability Index (ASI), Modified ASI (MASI), AMMI\n    Based Stability Parameter (ASTAB), Annicchiarico's D Parameter (DA), Zhang's\n    D Parameter (DZ), Averages of the Squared Eigenvector Values (EV), Stability\n    Measure Based on Fitted AMMI Model (FA), Absolute Value of the Relative\n    Contribution of IPCs to the Interaction (Za). Further calculates the\n    Simultaneous Selection Index for Yield and Stability from the computed\n    stability parameters. See the vignette for complete list of citations for\n    the methods implemented.",
    "version": "0.1.4",
    "maintainer": "B. C. Ajay <ajaygpb@yahoo.co.in>",
    "author": "B. C. Ajay [aut, cre] (ORCID: <https://orcid.org/0000-0001-7222-8483>),\n  J. Aravind [aut] (ORCID: <https://orcid.org/0000-0002-4791-442X>),\n  R. Abdul Fiyaz [aut] (ORCID: <https://orcid.org/0000-0001-6261-7071>),\n  ICAR [cph] (url: https://www.icar.org.in)",
    "url": "https://github.com/ajaygpb/ammistability/\nhttps://CRAN.R-project.org/package=ammistability\nhttps://ajaygpb.github.io/ammistability/\nhttps://doi.org/10.5281/zenodo.1344756",
    "bug_reports": "https://github.com/ajaygpb/ammistability/issues",
    "repository": "https://cran.r-project.org/package=ammistability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ammistability Additive Main Effects and Multiplicative Interaction Model\nStability Parameters Computes various stability parameters from Additive Main Effects\n    and Multiplicative Interaction (AMMI) analysis results such as  Modified\n    AMMI Stability Value (MASV), Sums of the Absolute Value of the Interaction\n    Principal Component Scores (SIPC), Sum Across Environments of\n    Genotype-Environment Interaction Modelled by AMMI (AMGE), Sum Across \n    Environments of Absolute Value of Genotype-Environment Interaction Modelled\n    by AMMI (AV_(AMGE)), AMMI Stability Index (ASI), Modified ASI (MASI), AMMI\n    Based Stability Parameter (ASTAB), Annicchiarico's D Parameter (DA), Zhang's\n    D Parameter (DZ), Averages of the Squared Eigenvector Values (EV), Stability\n    Measure Based on Fitted AMMI Model (FA), Absolute Value of the Relative\n    Contribution of IPCs to the Interaction (Za). Further calculates the\n    Simultaneous Selection Index for Yield and Stability from the computed\n    stability parameters. See the vignette for complete list of citations for\n    the methods implemented.  "
  },
  {
    "id": 8484,
    "package_name": "ampir",
    "title": "Predict Antimicrobial Peptides",
    "description": "A toolkit to predict antimicrobial peptides from protein sequences on a genome-wide scale.\n    It incorporates two support vector machine models (\"precursor\" and \"mature\") trained on publicly available antimicrobial peptide data using calculated\n    physico-chemical and compositional sequence properties described in Meher et al. (2017) <doi:10.1038/srep42362>.\n    In order to support genome-wide analyses, these models are designed to accept any type of protein as input\n    and calculation of compositional properties has been optimised for high-throughput use. For best results it is important to select the model that accurately \n    represents your sequence type: for full length proteins, it is recommended to use the default \"precursor\" model. The alternative, \"mature\", model is best suited\n    for mature peptide sequences that represent the final antimicrobial peptide sequence after post-translational processing. For details see Fingerhut et al. (2020) <doi:10.1093/bioinformatics/btaa653>.\n    The 'ampir' package is also available via a Shiny based GUI at <https://ampir.marine-omics.net/>.",
    "version": "1.1.0",
    "maintainer": "Legana Fingerhut <legana.fingerhut@my.jcu.edu.au>",
    "author": "Legana Fingerhut [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2482-5336>),\n  Ira Cooke [aut] (ORCID: <https://orcid.org/0000-0001-6520-1397>),\n  Jinlong Zhang [ctb] (R/read_faa.R),\n  Nan Xiao [ctb] (R/calc_pseudo_comp.R)",
    "url": "https://github.com/Legana/ampir",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ampir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ampir Predict Antimicrobial Peptides A toolkit to predict antimicrobial peptides from protein sequences on a genome-wide scale.\n    It incorporates two support vector machine models (\"precursor\" and \"mature\") trained on publicly available antimicrobial peptide data using calculated\n    physico-chemical and compositional sequence properties described in Meher et al. (2017) <doi:10.1038/srep42362>.\n    In order to support genome-wide analyses, these models are designed to accept any type of protein as input\n    and calculation of compositional properties has been optimised for high-throughput use. For best results it is important to select the model that accurately \n    represents your sequence type: for full length proteins, it is recommended to use the default \"precursor\" model. The alternative, \"mature\", model is best suited\n    for mature peptide sequences that represent the final antimicrobial peptide sequence after post-translational processing. For details see Fingerhut et al. (2020) <doi:10.1093/bioinformatics/btaa653>.\n    The 'ampir' package is also available via a Shiny based GUI at <https://ampir.marine-omics.net/>.  "
  },
  {
    "id": 8489,
    "package_name": "anMC",
    "title": "Compute High Dimensional Orthant Probabilities",
    "description": "Computationally efficient method to estimate orthant probabilities of high-dimensional Gaussian vectors. Further implements a function to compute conservative estimates of excursion sets under Gaussian random field priors. ",
    "version": "0.2.5",
    "maintainer": "Dario Azzimonti <dario.azzimonti@gmail.com>",
    "author": "Dario Azzimonti [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5080-3061>)",
    "url": "https://doi.org/10.1080/10618600.2017.1360781",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=anMC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "anMC Compute High Dimensional Orthant Probabilities Computationally efficient method to estimate orthant probabilities of high-dimensional Gaussian vectors. Further implements a function to compute conservative estimates of excursion sets under Gaussian random field priors.   "
  },
  {
    "id": 8550,
    "package_name": "apercu",
    "title": "Quick Look at your Data",
    "description": "The goal is to print an \"aper\u00e7u\", a short view of a vector, a\n    matrix, a data.frame, a list or an array. By default, it prints the first 5\n    elements of each dimension. By default, the number of columns is equal to\n    the number of lines. If you want to control the selection of the elements,\n    you can pass a list, with each element being a vector giving the selection\n    for each dimension.",
    "version": "0.2.5",
    "maintainer": "Aurelien Chateigner <aurelien.chateigner@gmail.com>",
    "author": "Aurelien Chateigner <aurelien.chateigner@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=apercu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apercu Quick Look at your Data The goal is to print an \"aper\u00e7u\", a short view of a vector, a\n    matrix, a data.frame, a list or an array. By default, it prints the first 5\n    elements of each dimension. By default, the number of columns is equal to\n    the number of lines. If you want to control the selection of the elements,\n    you can pass a list, with each element being a vector giving the selection\n    for each dimension.  "
  },
  {
    "id": 8563,
    "package_name": "apng",
    "title": "Convert Png Files into Animated Png",
    "description": "Convert several png files into an animated png file.\n  This package exports only a single function `apng'. Call the\n  apng function with a vector of file names (which should be\n  png files) to convert them to a single animated png file.",
    "version": "1.1",
    "maintainer": "Quinten Stokkink <q.a.stokkink@tudelft.nl>",
    "author": "Quinten Stokkink",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=apng",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apng Convert Png Files into Animated Png Convert several png files into an animated png file.\n  This package exports only a single function `apng'. Call the\n  apng function with a vector of file names (which should be\n  png files) to convert them to a single animated png file.  "
  },
  {
    "id": 8593,
    "package_name": "arcgislayers",
    "title": "Harness ArcGIS Data Services",
    "description": "Enables users of 'ArcGIS Enterprise', 'ArcGIS Online', or\n    'ArcGIS Platform' to read, write, publish, or manage vector and raster\n    data via ArcGIS location services REST API endpoints\n    <https://developers.arcgis.com/rest/>.",
    "version": "0.5.2",
    "maintainer": "Josiah Parry <josiah.parry@gmail.com>",
    "author": "Josiah Parry [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9910-865X>),\n  Eli Pousson [ctb] (ORCID: <https://orcid.org/0000-0001-8280-1706>),\n  Kenneth Vernon [ctb] (ORCID: <https://orcid.org/0000-0003-0098-5092>),\n  Martha Bass [ctb] (ORCID: <https://orcid.org/0009-0004-0268-5426>),\n  Antony Barja [ctb] (ORCID: <https://orcid.org/0000-0001-5921-2858>)",
    "url": "https://developers.arcgis.com/r-bridge,\nhttps://github.com/R-ArcGIS/arcgislayers",
    "bug_reports": "https://github.com/R-ArcGIS/arcgislayers/issues",
    "repository": "https://cran.r-project.org/package=arcgislayers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arcgislayers Harness ArcGIS Data Services Enables users of 'ArcGIS Enterprise', 'ArcGIS Online', or\n    'ArcGIS Platform' to read, write, publish, or manage vector and raster\n    data via ArcGIS location services REST API endpoints\n    <https://developers.arcgis.com/rest/>.  "
  },
  {
    "id": 8598,
    "package_name": "archeofrag",
    "title": "Spatial Analysis in Archaeology from Refitting Fragments",
    "description": "Methods to analyse spatial units in archaeology from the relationships between refitting fragmented objects scattered in these units (e.g. stratigraphic layers). Graphs are used to model archaeological observations. The package is mainly based on the 'igraph' package for graph analysis. Functions can: 1) create, manipulate, visualise, and simulate fragmentation graphs, 2) measure the cohesion and admixture of archaeological spatial units, and 3) characterise the topology of a specific set of refitting relationships. A series of published empirical datasets is included. Documentation about 'archeofrag' is provided by a vignette and by the accompanying scientific papers: Plutniak (2021, Journal of Archaeological Science, <doi:10.1016/j.jas.2021.105501>) and Plutniak (2022, Journal of Open Source Software, <doi:10.21105/joss.04335>). This package is complemented by the 'archeofrag.gui' R package, a companion GUI application available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.",
    "version": "1.2.3",
    "maintainer": "Sebastien Plutniak <sebastien.plutniak@posteo.net>",
    "author": "Sebastien Plutniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6674-3806>)",
    "url": "https://github.com/sebastien-plutniak/archeofrag",
    "bug_reports": "https://github.com/sebastien-plutniak/archeofrag/issues",
    "repository": "https://cran.r-project.org/package=archeofrag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "archeofrag Spatial Analysis in Archaeology from Refitting Fragments Methods to analyse spatial units in archaeology from the relationships between refitting fragmented objects scattered in these units (e.g. stratigraphic layers). Graphs are used to model archaeological observations. The package is mainly based on the 'igraph' package for graph analysis. Functions can: 1) create, manipulate, visualise, and simulate fragmentation graphs, 2) measure the cohesion and admixture of archaeological spatial units, and 3) characterise the topology of a specific set of refitting relationships. A series of published empirical datasets is included. Documentation about 'archeofrag' is provided by a vignette and by the accompanying scientific papers: Plutniak (2021, Journal of Archaeological Science, <doi:10.1016/j.jas.2021.105501>) and Plutniak (2022, Journal of Open Source Software, <doi:10.21105/joss.04335>). This package is complemented by the 'archeofrag.gui' R package, a companion GUI application available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.  "
  },
  {
    "id": 8599,
    "package_name": "archeofrag.gui",
    "title": "Spatial Analysis in Archaeology from Refitting Fragments (GUI)",
    "description": "A 'Shiny' application to access the functionalities and datasets of the 'archeofrag' package for spatial analysis in archaeology from refitting data. Quick and seamless exploration of archaeological refitting datasets, focusing on physical refits only. Features include: built-in documentation and convenient workflow, plot generation and exports, exploration of spatial units merging solutions, simulation of archaeological site formation processes, support for parallel computing, R code generation to re-execute simulations and ensure reproducibility, code generation for the 'openMOLE' model exploration software. A demonstration of the app is available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.",
    "version": "1.1.0",
    "maintainer": "Sebastien Plutniak <sebastien.plutniak@posteo.net>",
    "author": "Sebastien Plutniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6674-3806>)",
    "url": "https://github.com/sebastien-plutniak/archeofrag.gui",
    "bug_reports": "https://github.com/sebastien-plutniak/archeofrag.gui/issues",
    "repository": "https://cran.r-project.org/package=archeofrag.gui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "archeofrag.gui Spatial Analysis in Archaeology from Refitting Fragments (GUI) A 'Shiny' application to access the functionalities and datasets of the 'archeofrag' package for spatial analysis in archaeology from refitting data. Quick and seamless exploration of archaeological refitting datasets, focusing on physical refits only. Features include: built-in documentation and convenient workflow, plot generation and exports, exploration of spatial units merging solutions, simulation of archaeological site formation processes, support for parallel computing, R code generation to re-execute simulations and ensure reproducibility, code generation for the 'openMOLE' model exploration software. A demonstration of the app is available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.  "
  },
  {
    "id": 8612,
    "package_name": "areabiplot",
    "title": "Area Biplot",
    "description": "Considering an (n x m) data matrix X, this package is based on the method proposed \n             by Gower, Groener, and Velden (2010) <doi:10.1198/jcgs.2010.07134>, and \n             utilize the  resulting matrices from the extended version of the NIPALS decomposition \n             to determine n triangles whose areas are used  to  visually  estimate the elements of\n             a specific column of X. After a 90-degree rotation of the sample points, the triangles\n             are drawn regarding the following points: 1.the origin of the axes; 2.the sample points;\n             3. the vector endpoint representing some variable.",
    "version": "1.0.0",
    "maintainer": "Alberto Silva <albertos@ua.pt>",
    "author": "Alberto Silva [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3496-6802>),\n  Adelaide Freitas [aut] (ORCID: <https://orcid.org/0000-0002-4685-1615>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=areabiplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "areabiplot Area Biplot Considering an (n x m) data matrix X, this package is based on the method proposed \n             by Gower, Groener, and Velden (2010) <doi:10.1198/jcgs.2010.07134>, and \n             utilize the  resulting matrices from the extended version of the NIPALS decomposition \n             to determine n triangles whose areas are used  to  visually  estimate the elements of\n             a specific column of X. After a 90-degree rotation of the sample points, the triangles\n             are drawn regarding the following points: 1.the origin of the axes; 2.the sample points;\n             3. the vector endpoint representing some variable.  "
  },
  {
    "id": 8615,
    "package_name": "areaplot",
    "title": "Plot Stacked Areas and Confidence Bands as Filled Polygons",
    "description": "Plot stacked areas and confidence bands as filled polygons, or add\n  polygons to existing plots. A variety of input formats are supported,\n  including vectors, matrices, data frames, formulas, etc.",
    "version": "2.1.3",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Arni Magnusson [aut, cre]",
    "url": "https://github.com/arni-magnusson/areaplot",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=areaplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "areaplot Plot Stacked Areas and Confidence Bands as Filled Polygons Plot stacked areas and confidence bands as filled polygons, or add\n  polygons to existing plots. A variety of input formats are supported,\n  including vectors, matrices, data frames, formulas, etc.  "
  },
  {
    "id": 8643,
    "package_name": "arrApply",
    "title": "Apply a Function to a Margin of an Array",
    "description": "High performance variant of apply() for a fixed set of functions.\n  Considerable speedup of this implementation is a trade-off for universality: user defined\n  functions cannot be used with this package. However, about 20 most currently employed\n  functions are available for usage. They can be divided in three types:\n  reducing functions (like mean(), sum() etc., giving a scalar when applied to a vector),\n  mapping function (like normalise(), cumsum() etc., giving a vector of the same length\n  as the input vector) and finally, vector reducing function (like diff() which produces\n  result vector of a length different from the length of input vector).\n  Optional or mandatory additional arguments required by some functions\n  (e.g. norm type for norm()) can be\n  passed as named arguments in '...'.",
    "version": "2.2.1",
    "maintainer": "Serguei Sokol <sokol@insa-toulouse.fr>",
    "author": "Serguei Sokol [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=arrApply",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arrApply Apply a Function to a Margin of an Array High performance variant of apply() for a fixed set of functions.\n  Considerable speedup of this implementation is a trade-off for universality: user defined\n  functions cannot be used with this package. However, about 20 most currently employed\n  functions are available for usage. They can be divided in three types:\n  reducing functions (like mean(), sum() etc., giving a scalar when applied to a vector),\n  mapping function (like normalise(), cumsum() etc., giving a vector of the same length\n  as the input vector) and finally, vector reducing function (like diff() which produces\n  result vector of a length different from the length of input vector).\n  Optional or mandatory additional arguments required by some functions\n  (e.g. norm type for norm()) can be\n  passed as named arguments in '...'.  "
  },
  {
    "id": 8678,
    "package_name": "aspace",
    "title": "Functions for Estimating Centrographic Statistics",
    "description": "A collection of functions for computing centrographic\n        statistics (e.g., standard distance, standard deviation\n        ellipse, standard deviation box) for observations taken at\n        point locations. Separate plotting functions have been\n        developed for each measure. Users interested in writing results\n        to ESRI shapefiles can do so by using results from 'aspace'\n        functions as inputs to the convert.to.shapefile() and\n        write.shapefile() functions in the 'shapefiles' library. We intend to\n        provide 'terra' integration for geographic data in a future release.\n        The 'aspace' package was originally conceived to aid in the analysis of\n        spatial patterns of travel behaviour (see Buliung and Remmel 2008\n        <doi:10.1007/s10109-008-0063-7>).",
    "version": "4.1.2",
    "maintainer": "Tarmo K. Remmel <remmelt@yorku.ca>",
    "author": "Tarmo K. Remmel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6251-876X>),\n  Randy Bui [aut],\n  Ron N. Buliung [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aspace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aspace Functions for Estimating Centrographic Statistics A collection of functions for computing centrographic\n        statistics (e.g., standard distance, standard deviation\n        ellipse, standard deviation box) for observations taken at\n        point locations. Separate plotting functions have been\n        developed for each measure. Users interested in writing results\n        to ESRI shapefiles can do so by using results from 'aspace'\n        functions as inputs to the convert.to.shapefile() and\n        write.shapefile() functions in the 'shapefiles' library. We intend to\n        provide 'terra' integration for geographic data in a future release.\n        The 'aspace' package was originally conceived to aid in the analysis of\n        spatial patterns of travel behaviour (see Buliung and Remmel 2008\n        <doi:10.1007/s10109-008-0063-7>).  "
  },
  {
    "id": 8681,
    "package_name": "asremlPlus",
    "title": "Augments 'ASReml-R' in Fitting Mixed Models and Packages\nGenerally in Exploring Prediction Differences",
    "description": "Assists in automating the selection of terms to include in mixed models when  \n  'asreml' is used to fit the models. Procedures are available for choosing models that \n  conform to the hierarchy or marginality principle, for fitting and choosing between \n  two-dimensional spatial models using correlation, natural cubic smoothing spline and \n  P-spline models. A history of the fitting of a sequence of models is kept in a data frame. \n  Also used to compute functions and contrasts of, to investigate differences between and \n  to plot predictions obtained using any model fitting function. The content  falls into \n  the following natural groupings: (i) Data, (ii) Model modification functions, (iii) Model \n  selection and description functions, (iv) Model diagnostics and simulation functions, \n  (v) Prediction production and presentation functions, (vi) Response transformation \n  functions, (vii) Object manipulation functions, and (viii) Miscellaneous functions \n  (for further details see 'asremlPlus-package' in help). The 'asreml' package provides a \n  computationally efficient algorithm for fitting a wide range of linear mixed models using \n  Residual Maximum Likelihood. It is a commercial package and a license for it can be \n  purchased from 'VSNi' <https://vsni.co.uk/> as 'asreml-R', who will supply a zip file \n  for local installation/updating (see <https://asreml.kb.vsni.co.uk/>). It is not needed \n  for functions that are methods for 'alldiffs'  and 'data.frame' objects. The package \n  'asremPlus' can also be installed from <http://chris.brien.name/rpackages/>.",
    "version": "4.4.55",
    "maintainer": "Chris Brien <chris.brien@adelaide.edu.au>",
    "author": "Chris Brien [aut, cre] (ORCID: <https://orcid.org/0000-0003-0581-1817>)",
    "url": "http://chris.brien.name",
    "bug_reports": "https://github.com/briencj/asremlPlus/issues",
    "repository": "https://cran.r-project.org/package=asremlPlus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asremlPlus Augments 'ASReml-R' in Fitting Mixed Models and Packages\nGenerally in Exploring Prediction Differences Assists in automating the selection of terms to include in mixed models when  \n  'asreml' is used to fit the models. Procedures are available for choosing models that \n  conform to the hierarchy or marginality principle, for fitting and choosing between \n  two-dimensional spatial models using correlation, natural cubic smoothing spline and \n  P-spline models. A history of the fitting of a sequence of models is kept in a data frame. \n  Also used to compute functions and contrasts of, to investigate differences between and \n  to plot predictions obtained using any model fitting function. The content  falls into \n  the following natural groupings: (i) Data, (ii) Model modification functions, (iii) Model \n  selection and description functions, (iv) Model diagnostics and simulation functions, \n  (v) Prediction production and presentation functions, (vi) Response transformation \n  functions, (vii) Object manipulation functions, and (viii) Miscellaneous functions \n  (for further details see 'asremlPlus-package' in help). The 'asreml' package provides a \n  computationally efficient algorithm for fitting a wide range of linear mixed models using \n  Residual Maximum Likelihood. It is a commercial package and a license for it can be \n  purchased from 'VSNi' <https://vsni.co.uk/> as 'asreml-R', who will supply a zip file \n  for local installation/updating (see <https://asreml.kb.vsni.co.uk/>). It is not needed \n  for functions that are methods for 'alldiffs'  and 'data.frame' objects. The package \n  'asremPlus' can also be installed from <http://chris.brien.name/rpackages/>.  "
  },
  {
    "id": 8693,
    "package_name": "asteRisk",
    "title": "Computation of Satellite Position",
    "description": "Provides basic functionalities to calculate the position of\n    satellites given a known state vector. The package includes implementations\n    of the SGP4 and SDP4 simplified perturbation models to propagate orbital\n    state vectors, as well as utilities to read TLE files and convert coordinates\n    between different frames of reference. Several of the functionalities of the\n    package (including the high-precision numerical orbit propagator) require\n    the coefficients and data included in the 'asteRiskData' package, available\n    in a 'drat' repository. To install this data package, run \n    'install.packages(\"asteRiskData\", repos=\"https://rafael-ayala.github.io/drat/\")'.\n    Felix R. Hoots, Ronald L. Roehrich and T.S. Kelso (1988) <https://celestrak.org/NORAD/documentation/spacetrk.pdf>.\n    David Vallado, Paul Crawford, Richard Hujsak and T.S. Kelso (2012) <doi:10.2514/6.2006-6753>.\n    Felix R. Hoots, Paul W. Schumacher Jr. and Robert A. Glover (2014) <doi:10.2514/1.9161>.",
    "version": "1.4.5",
    "maintainer": "Rafael Ayala <rafaelayalahernandez@gmail.com>",
    "author": "Rafael Ayala [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9332-4623>),\n  Daniel Ayala [aut] (ORCID: <https://orcid.org/0000-0003-2095-1009>),\n  David Ruiz [aut] (ORCID: <https://orcid.org/0000-0003-4460-5493>),\n  Pablo Hernandez [aut] (ORCID: <https://orcid.org/0009-0000-9279-6744>),\n  Lara Selles Vidal [aut] (ORCID:\n    <https://orcid.org/0000-0003-2537-6824>)",
    "url": "",
    "bug_reports": "https://github.com/Rafael-Ayala/asteRisk/issues",
    "repository": "https://cran.r-project.org/package=asteRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asteRisk Computation of Satellite Position Provides basic functionalities to calculate the position of\n    satellites given a known state vector. The package includes implementations\n    of the SGP4 and SDP4 simplified perturbation models to propagate orbital\n    state vectors, as well as utilities to read TLE files and convert coordinates\n    between different frames of reference. Several of the functionalities of the\n    package (including the high-precision numerical orbit propagator) require\n    the coefficients and data included in the 'asteRiskData' package, available\n    in a 'drat' repository. To install this data package, run \n    'install.packages(\"asteRiskData\", repos=\"https://rafael-ayala.github.io/drat/\")'.\n    Felix R. Hoots, Ronald L. Roehrich and T.S. Kelso (1988) <https://celestrak.org/NORAD/documentation/spacetrk.pdf>.\n    David Vallado, Paul Crawford, Richard Hujsak and T.S. Kelso (2012) <doi:10.2514/6.2006-6753>.\n    Felix R. Hoots, Paul W. Schumacher Jr. and Robert A. Glover (2014) <doi:10.2514/1.9161>.  "
  },
  {
    "id": 8694,
    "package_name": "aster",
    "title": "Aster Models",
    "description": "Aster models (Geyer, Wagenius, and Shaw, 2007,\n    <doi:10.1093/biomet/asm030>; Shaw, Geyer, Wagenius, Hangelbroek, and\n    Etterson, 2008, <doi:10.1086/588063>; Geyer, Ridley, Latta, Etterson,\n    and Shaw, 2013, <doi:10.1214/13-AOAS653>) are exponential family\n    regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, life table analysis,\n    zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    There are also random effects versions of these models.",
    "version": "1.3-7",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "author": "Charles J. Geyer [aut, cre]",
    "url": "https://www.stat.umn.edu/geyer/aster/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aster Aster Models Aster models (Geyer, Wagenius, and Shaw, 2007,\n    <doi:10.1093/biomet/asm030>; Shaw, Geyer, Wagenius, Hangelbroek, and\n    Etterson, 2008, <doi:10.1086/588063>; Geyer, Ridley, Latta, Etterson,\n    and Shaw, 2013, <doi:10.1214/13-AOAS653>) are exponential family\n    regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, life table analysis,\n    zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    There are also random effects versions of these models.  "
  },
  {
    "id": 8695,
    "package_name": "aster2",
    "title": "Aster Models",
    "description": "Aster models are exponential family regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    Unlike the aster package, this package does dependence groups (nodes of\n    the graph need not be conditionally independent given their predecessor\n    node), including multinomial and two-parameter normal as families.  Thus\n    this package also generalizes mark-capture-recapture analysis.",
    "version": "0.3-2",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "author": "Charles J. Geyer [aut, cre]",
    "url": "https://www.stat.umn.edu/geyer/aster/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aster2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aster2 Aster Models Aster models are exponential family regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    Unlike the aster package, this package does dependence groups (nodes of\n    the graph need not be conditionally independent given their predecessor\n    node), including multinomial and two-parameter normal as families.  Thus\n    this package also generalizes mark-capture-recapture analysis.  "
  },
  {
    "id": 8702,
    "package_name": "asymmetry",
    "title": "Multidimensional Scaling of Asymmetric Proximities",
    "description": "Multidimensional scaling models and methods for the visualization and analysis of asymmetric proximity data. An asymmetric data matrix has the same number of rows and columns, and these rows and columns refer to the same set of objects. At least some elements in the upper-triangle are different from the corresponding elements in the lower triangle. An example of an asymmetric matrix is a  student migration table, where the rows correspond to the countries of origin of the students and the columns to the destination countries. This package provides algorithms for three multidimensional scaling models, the slide-vector model, a scaling model with unique dimensions and the asymscal model.Furthermore, some other procedures, such as a heat map for skew-symmetric data, and the decomposition of asymmetry are also provided for the exploratory analysis of asymmetric tables.",
    "version": "2.0.5",
    "maintainer": "Berrie Zielman <berrie.zielman@gmail.com>",
    "author": "Berrie Zielman [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=asymmetry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asymmetry Multidimensional Scaling of Asymmetric Proximities Multidimensional scaling models and methods for the visualization and analysis of asymmetric proximity data. An asymmetric data matrix has the same number of rows and columns, and these rows and columns refer to the same set of objects. At least some elements in the upper-triangle are different from the corresponding elements in the lower triangle. An example of an asymmetric matrix is a  student migration table, where the rows correspond to the countries of origin of the students and the columns to the destination countries. This package provides algorithms for three multidimensional scaling models, the slide-vector model, a scaling model with unique dimensions and the asymscal model.Furthermore, some other procedures, such as a heat map for skew-symmetric data, and the decomposition of asymmetry are also provided for the exploratory analysis of asymmetric tables.  "
  },
  {
    "id": 8710,
    "package_name": "atakrig",
    "title": "Area-to-Area Kriging",
    "description": "Point-scale variogram deconvolution from irregular/regular spatial support according to Goovaerts, P., (2008) <doi: 10.1007/s11004-007-9129-1>; ordinary area-to-area (co)Kriging and area-to-point (co)Kriging.",
    "version": "0.9.8.1",
    "maintainer": "Maogui Hu <humg@lreis.ac.cn>",
    "author": "Maogui Hu [aut, cre],\n  Yanwei Huang [ctb],\n  Roger Bivand [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=atakrig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "atakrig Area-to-Area Kriging Point-scale variogram deconvolution from irregular/regular spatial support according to Goovaerts, P., (2008) <doi: 10.1007/s11004-007-9129-1>; ordinary area-to-area (co)Kriging and area-to-point (co)Kriging.  "
  },
  {
    "id": 8742,
    "package_name": "autoFRK",
    "title": "Automatic Fixed Rank Kriging",
    "description": "Automatic fixed rank kriging for (irregularly located)\n    spatial data using a class of basis functions with multi-resolution features\n    and ordered in terms of their resolutions. The model parameters are estimated\n    by maximum likelihood (ML) and the number of basis functions is determined\n    by Akaike's information criterion (AIC). For spatial data with either one\n    realization or independent replicates, the ML estimates and AIC are efficiently\n    computed using their closed-form expressions when no missing value occurs. Details \n    regarding the basis function construction, parameter estimation, and AIC calculation  \n    can be found in Tzeng and Huang (2018) <doi:10.1080/00401706.2017.1345701>. For\n    data with missing values, the ML estimates are obtained using the expectation-\n    maximization algorithm. Apart from the number of basis functions, there are\n    no other tuning parameters, making the method fully automatic. Users can also\n    include a stationary structure in the spatial covariance, which utilizes\n    'LatticeKrig' package.",
    "version": "1.4.3",
    "maintainer": "ShengLi Tzeng <slt.cmu@gmail.com>",
    "author": "ShengLi Tzeng [aut, cre], Hsin-Cheng Huang [aut], Wen-Ting Wang [ctb], Douglas Nychka [ctb], Colin Gillespie [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=autoFRK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "autoFRK Automatic Fixed Rank Kriging Automatic fixed rank kriging for (irregularly located)\n    spatial data using a class of basis functions with multi-resolution features\n    and ordered in terms of their resolutions. The model parameters are estimated\n    by maximum likelihood (ML) and the number of basis functions is determined\n    by Akaike's information criterion (AIC). For spatial data with either one\n    realization or independent replicates, the ML estimates and AIC are efficiently\n    computed using their closed-form expressions when no missing value occurs. Details \n    regarding the basis function construction, parameter estimation, and AIC calculation  \n    can be found in Tzeng and Huang (2018) <doi:10.1080/00401706.2017.1345701>. For\n    data with missing values, the ML estimates are obtained using the expectation-\n    maximization algorithm. Apart from the number of basis functions, there are\n    no other tuning parameters, making the method fully automatic. Users can also\n    include a stationary structure in the spatial covariance, which utilizes\n    'LatticeKrig' package.  "
  },
  {
    "id": 8771,
    "package_name": "auxvecLASSO",
    "title": "LASSO Auxiliary Variable Selection and Auxiliary Vector\nDiagnostics",
    "description": "Provides tools for assessing and selecting auxiliary variables using LASSO. \n    The package includes functions for variable selection and diagnostics, facilitating \n    survey calibration analysis with emphasis on robust auxiliary vector selection. For \n    more details see Tibshirani (1996) <doi:10.1111/j.2517-6161.1996.tb02080.x> and \n    Caughrey and Hartman (2017) <doi:10.2139/ssrn.3494436>.",
    "version": "0.2.0",
    "maintainer": "Gustaf Andersson <gustafanderssons@gmail.com>",
    "author": "Gustaf Andersson [aut, cre, cph]",
    "url": "https://github.com/gustafanderssons/auxvecLASSO-R-Package",
    "bug_reports": "https://github.com/gustafanderssons/auxvecLASSO-R-Package/issues",
    "repository": "https://cran.r-project.org/package=auxvecLASSO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "auxvecLASSO LASSO Auxiliary Variable Selection and Auxiliary Vector\nDiagnostics Provides tools for assessing and selecting auxiliary variables using LASSO. \n    The package includes functions for variable selection and diagnostics, facilitating \n    survey calibration analysis with emphasis on robust auxiliary vector selection. For \n    more details see Tibshirani (1996) <doi:10.1111/j.2517-6161.1996.tb02080.x> and \n    Caughrey and Hartman (2017) <doi:10.2139/ssrn.3494436>.  "
  },
  {
    "id": 8804,
    "package_name": "b64",
    "title": "Fast and Vectorized Base 64 Engine",
    "description": "Provides a fast, lightweight, and vectorized base 64 engine\n    to encode and decode character and raw vectors as well as files stored\n    on disk. Common base 64 alphabets are supported out of the box\n    including the standard, URL-safe, bcrypt, crypt, 'BinHex', and\n    IMAP-modified UTF-7 alphabets. Custom engines can be created to\n    support unique base 64 encoding and decoding needs.",
    "version": "0.1.7",
    "maintainer": "Josiah Parry <josiah.parry@gmail.com>",
    "author": "Josiah Parry [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9910-865X>),\n  Etienne Bacher [ctb] (ORCID: <https://orcid.org/0000-0002-9271-5075>)",
    "url": "https://extendr.github.io/b64/, https://github.com/extendr/b64",
    "bug_reports": "https://github.com/extendr/b64/issues",
    "repository": "https://cran.r-project.org/package=b64",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "b64 Fast and Vectorized Base 64 Engine Provides a fast, lightweight, and vectorized base 64 engine\n    to encode and decode character and raw vectors as well as files stored\n    on disk. Common base 64 alphabets are supported out of the box\n    including the standard, URL-safe, bcrypt, crypt, 'BinHex', and\n    IMAP-modified UTF-7 alphabets. Custom engines can be created to\n    support unique base 64 encoding and decoding needs.  "
  },
  {
    "id": 8858,
    "package_name": "baorista",
    "title": "Bayesian Aoristic Analyses",
    "description": "Provides an alternative approach to aoristic analyses for archaeological datasets by fitting Bayesian parametric growth models and non-parametric random-walk Intrinsic Conditional Autoregressive (ICAR) models on time frequency data (Crema (2024)<doi:10.1111/arcm.12984>). It handles event typo-chronology based timespans defined by start/end date as well as more complex user-provided vector of probabilities.  ",
    "version": "0.2.1",
    "maintainer": "Enrico Crema <enrico.crema@gmail.com>",
    "author": "Enrico Crema [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6727-5138>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=baorista",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "baorista Bayesian Aoristic Analyses Provides an alternative approach to aoristic analyses for archaeological datasets by fitting Bayesian parametric growth models and non-parametric random-walk Intrinsic Conditional Autoregressive (ICAR) models on time frequency data (Crema (2024)<doi:10.1111/arcm.12984>). It handles event typo-chronology based timespans defined by start/end date as well as more complex user-provided vector of probabilities.    "
  },
  {
    "id": 8861,
    "package_name": "bark",
    "title": "Bayesian Additive Regression Kernels",
    "description": "Bayesian Additive Regression Kernels (BARK) provides\n\tan implementation for non-parametric function estimation using Levy\n\tRandom Field priors for functions that may be represented as a\n\tsum of additive multivariate kernels.  Kernels are located at\n\tevery data point as in Support Vector Machines, however, coefficients \n\tmay be heavily shrunk to zero under the Cauchy process prior, or even, \n\tset to zero.  The number of active features is controlled by priors on\n\tprecision parameters within the kernels, permitting feature selection. For \n\tmore details see Ouyang, Z (2008) \"Bayesian Additive Regression Kernels\",\n\tDuke University. PhD dissertation, Chapter 3 and Wolpert, R. L, Clyde, M.A, \n\tand Tu, C. (2011) \"Stochastic Expansions with Continuous Dictionaries Levy\n\tAdaptive Regression Kernels, Annals of Statistics Vol (39) pages 1916-1962 \n\t<doi:10.1214/11-AOS889>.",
    "version": "1.0.5",
    "maintainer": "Merlise Clyde <clyde@duke.edu>",
    "author": "Merlise Clyde [aut, cre, ths] (ORCID=0000-0002-3595-1872),\n  Zhi Ouyang [aut],\n  Robert Wolpert [ctb, ths]",
    "url": "https://www.R-project.org, https://github.com/merliseclyde/bark",
    "bug_reports": "https://github.com/merliseclyde/bark/issues",
    "repository": "https://cran.r-project.org/package=bark",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bark Bayesian Additive Regression Kernels Bayesian Additive Regression Kernels (BARK) provides\n\tan implementation for non-parametric function estimation using Levy\n\tRandom Field priors for functions that may be represented as a\n\tsum of additive multivariate kernels.  Kernels are located at\n\tevery data point as in Support Vector Machines, however, coefficients \n\tmay be heavily shrunk to zero under the Cauchy process prior, or even, \n\tset to zero.  The number of active features is controlled by priors on\n\tprecision parameters within the kernels, permitting feature selection. For \n\tmore details see Ouyang, Z (2008) \"Bayesian Additive Regression Kernels\",\n\tDuke University. PhD dissertation, Chapter 3 and Wolpert, R. L, Clyde, M.A, \n\tand Tu, C. (2011) \"Stochastic Expansions with Continuous Dictionaries Levy\n\tAdaptive Regression Kernels, Annals of Statistics Vol (39) pages 1916-1962 \n\t<doi:10.1214/11-AOS889>.  "
  },
  {
    "id": 8862,
    "package_name": "barrel",
    "title": "Covariance-Based Ellipses and Annotation Tools for Ordination\nPlots",
    "description": "Provides tools to visualize ordination results in 'R' by adding covariance-based ellipses, centroids, vectors, and confidence regions to plots created with 'ggplot2'. \n    The package extends the 'vegan' framework and supports Principal Component Analysis (PCA), Redundancy Analysis (RDA), and Non-metric Multidimensional Scaling (NMDS). \n    Ellipses can represent either group dispersion (standard deviation, SD) or centroid precision (standard error, SE), following Wang et al. (2015) <doi:10.1371/journal.pone.0118537>. \n    Robust estimators of covariance are implemented, including the Minimum Covariance Determinant (MCD) method of Hubert et al. (2018) <doi:10.1002/wics.1421>. \n    This approach reduces the influence of outliers. \n    barrel is particularly useful for multivariate ecological datasets, promoting reproducible, publication-quality ordination graphics with minimal effort.",
    "version": "0.1.0",
    "maintainer": "Diego Barranco-Elena <diego.barranco@udl.cat>",
    "author": "Diego Barranco-Elena [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=barrel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "barrel Covariance-Based Ellipses and Annotation Tools for Ordination\nPlots Provides tools to visualize ordination results in 'R' by adding covariance-based ellipses, centroids, vectors, and confidence regions to plots created with 'ggplot2'. \n    The package extends the 'vegan' framework and supports Principal Component Analysis (PCA), Redundancy Analysis (RDA), and Non-metric Multidimensional Scaling (NMDS). \n    Ellipses can represent either group dispersion (standard deviation, SD) or centroid precision (standard error, SE), following Wang et al. (2015) <doi:10.1371/journal.pone.0118537>. \n    Robust estimators of covariance are implemented, including the Minimum Covariance Determinant (MCD) method of Hubert et al. (2018) <doi:10.1002/wics.1421>. \n    This approach reduces the influence of outliers. \n    barrel is particularly useful for multivariate ecological datasets, promoting reproducible, publication-quality ordination graphics with minimal effort.  "
  },
  {
    "id": 8863,
    "package_name": "barrks",
    "title": "Calculate Bark Beetle Phenology Using Different Models",
    "description": "Calculate the bark beetle phenology based on raster data or\n    point-related data. There are multiple models implemented for two bark\n    beetle species. The models can be customized and their submodels (onset of\n    infestation, beetle development, diapause initiation, mortality) can be\n    combined. The following models are available in the package:\n    PHENIPS-Clim (first-time release in this package),\n    PHENIPS (Baier et al. 2007) <doi:10.1016/j.foreco.2007.05.020>,\n    RITY (Ogris et al. 2019) <doi:10.1016/j.ecolmodel.2019.108775>,\n    CHAPY (Ogris et al. 2020) <doi:10.1016/j.ecolmodel.2020.109137>,\n    BSO (Jakoby et al. 2019) <doi:10.1111/gcb.14766>,\n    Lange et al. (2008) <doi:10.1007/978-3-540-85081-6_32>,\n    J\u00f6nsson et al. (2011) <doi:10.1007/s10584-011-0038-4>.\n    The package may be expanded by models for other bark beetle species in the\n    future.",
    "version": "1.1.2",
    "maintainer": "Jakob Jentschke <jakob.jentschke@forst.bwl.de>",
    "author": "Jakob Jentschke [aut, cre],\n  FVA BW, Abt. Waldschutz [cph, fnd]",
    "url": "https://jjentschke.github.io/barrks/,\nhttps://github.com/jjentschke/barrks/",
    "bug_reports": "https://github.com/jjentschke/barrks/issues/",
    "repository": "https://cran.r-project.org/package=barrks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "barrks Calculate Bark Beetle Phenology Using Different Models Calculate the bark beetle phenology based on raster data or\n    point-related data. There are multiple models implemented for two bark\n    beetle species. The models can be customized and their submodels (onset of\n    infestation, beetle development, diapause initiation, mortality) can be\n    combined. The following models are available in the package:\n    PHENIPS-Clim (first-time release in this package),\n    PHENIPS (Baier et al. 2007) <doi:10.1016/j.foreco.2007.05.020>,\n    RITY (Ogris et al. 2019) <doi:10.1016/j.ecolmodel.2019.108775>,\n    CHAPY (Ogris et al. 2020) <doi:10.1016/j.ecolmodel.2020.109137>,\n    BSO (Jakoby et al. 2019) <doi:10.1111/gcb.14766>,\n    Lange et al. (2008) <doi:10.1007/978-3-540-85081-6_32>,\n    J\u00f6nsson et al. (2011) <doi:10.1007/s10584-011-0038-4>.\n    The package may be expanded by models for other bark beetle species in the\n    future.  "
  },
  {
    "id": 8930,
    "package_name": "bayesWatch",
    "title": "Bayesian Change-Point Detection for Process Monitoring with\nFault Detection",
    "description": "Bayes Watch fits an array of Gaussian Graphical Mixture Models to groupings of homogeneous data in time, called regimes, which are modeled as the observed states of a Markov process with unknown transition probabilities.  In doing so, Bayes Watch defines a posterior distribution on a vector of regime assignments, which gives meaningful expressions on the probability of every possible change-point.  Bayes Watch also allows for an effective and efficient fault detection system that assesses what features in the data where the most responsible for a given change-point.  For further details, see: Alexander C. Murph et al. (2023) <doi:10.48550/arXiv.2310.02940>.",
    "version": "0.1.4",
    "maintainer": "Alexander C. Murph <murph290@gmail.com>",
    "author": "Alexander C. Murph [aut, cre],\n  Reza Mohammadi [ctb, cph],\n  Alex Lenkoski [ctb, cph],\n  Andrew Johnson [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bayesWatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayesWatch Bayesian Change-Point Detection for Process Monitoring with\nFault Detection Bayes Watch fits an array of Gaussian Graphical Mixture Models to groupings of homogeneous data in time, called regimes, which are modeled as the observed states of a Markov process with unknown transition probabilities.  In doing so, Bayes Watch defines a posterior distribution on a vector of regime assignments, which gives meaningful expressions on the probability of every possible change-point.  Bayes Watch also allows for an effective and efficient fault detection system that assesses what features in the data where the most responsible for a given change-point.  For further details, see: Alexander C. Murph et al. (2023) <doi:10.48550/arXiv.2310.02940>.  "
  },
  {
    "id": 8944,
    "package_name": "bayesianVARs",
    "title": "MCMC Estimation of Bayesian Vectorautoregressions",
    "description": "Efficient Markov Chain Monte Carlo (MCMC) algorithms for the\n    fully Bayesian estimation of vectorautoregressions (VARs) featuring\n    stochastic volatility (SV). Implements state-of-the-art shrinkage\n    priors following Gruber & Kastner (2023) <doi:10.48550/arXiv.2206.04902>.\n    Efficient equation-per-equation estimation following Kastner & Huber\n    (2020) <doi:10.1002/for.2680> and Carrerio et al. (2021)\n    <doi:10.1016/j.jeconom.2021.11.010>.",
    "version": "0.1.5",
    "maintainer": "Luis Gruber <Luis.Gruber@aau.at>",
    "author": "Luis Gruber [cph, aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2399-738X>),\n  Gregor Kastner [ctb] (ORCID: <https://orcid.org/0000-0002-8237-8271>)",
    "url": "https://github.com/luisgruber/bayesianVARs,\nhttps://luisgruber.github.io/bayesianVARs/",
    "bug_reports": "https://github.com/luisgruber/bayesianVARs/issues",
    "repository": "https://cran.r-project.org/package=bayesianVARs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayesianVARs MCMC Estimation of Bayesian Vectorautoregressions Efficient Markov Chain Monte Carlo (MCMC) algorithms for the\n    fully Bayesian estimation of vectorautoregressions (VARs) featuring\n    stochastic volatility (SV). Implements state-of-the-art shrinkage\n    priors following Gruber & Kastner (2023) <doi:10.48550/arXiv.2206.04902>.\n    Efficient equation-per-equation estimation following Kastner & Huber\n    (2020) <doi:10.1002/for.2680> and Carrerio et al. (2021)\n    <doi:10.1016/j.jeconom.2021.11.010>.  "
  },
  {
    "id": 9009,
    "package_name": "bdsvd",
    "title": "Block Structure Detection Using Singular Vectors",
    "description": "Performs block diagonal covariance matrix detection using singular vectors (BD-SVD), which can be extended to hierarchical variable clustering (HC-SVD). The methods are described in Bauer (2024) <doi:10.1080/10618600.2024.2422985> and Bauer (202X) <doi:10.48550/arXiv.2308.06820>.",
    "version": "0.2.1",
    "maintainer": "Jan O. Bauer <j.bauer@vu.nl>",
    "author": "Jan O. Bauer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7123-4507>),\n  Ron Holzapfel [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bdsvd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bdsvd Block Structure Detection Using Singular Vectors Performs block diagonal covariance matrix detection using singular vectors (BD-SVD), which can be extended to hierarchical variable clustering (HC-SVD). The methods are described in Bauer (2024) <doi:10.1080/10618600.2024.2422985> and Bauer (202X) <doi:10.48550/arXiv.2308.06820>.  "
  },
  {
    "id": 9025,
    "package_name": "beezdiscounting",
    "title": "Behavioral Economic Easy Discounting",
    "description": "Facilitates some of the analyses performed in studies of\n    behavioral economic discounting. The package supports scoring of the 27-Item Monetary Choice\n    Questionnaire (see Kaplan et al., 2016; <doi:10.1007/s40614-016-0070-9>), calculating k\n    values (Mazur's simple hyperbolic and exponential) using nonlinear regression, calculating\n    various Area Under the Curve (AUC) measures, plotting regression curves for both fit-to-group and\n    two-stage approaches, checking for unsystematic discounting\n    (Johnson & Bickel, 2008; <doi:10.1037/1064-1297.16.3.264>) and scoring of the\n    minute discounting task (see Koffarnus & Bickel, 2014; <doi:10.1037/a0035973>) using the\n    Qualtrics 5-trial discounting template (see the Qualtrics Minute Discounting User Guide;\n    <doi:10.13140/RG.2.2.26495.79527>), which is also available as a .qsf file in this package.",
    "version": "0.3.2",
    "maintainer": "Brent A. Kaplan <bkaplan.ku@gmail.com>",
    "author": "Brent A. Kaplan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3758-6776>)",
    "url": "https://github.com/brentkaplan/beezdiscounting",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=beezdiscounting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "beezdiscounting Behavioral Economic Easy Discounting Facilitates some of the analyses performed in studies of\n    behavioral economic discounting. The package supports scoring of the 27-Item Monetary Choice\n    Questionnaire (see Kaplan et al., 2016; <doi:10.1007/s40614-016-0070-9>), calculating k\n    values (Mazur's simple hyperbolic and exponential) using nonlinear regression, calculating\n    various Area Under the Curve (AUC) measures, plotting regression curves for both fit-to-group and\n    two-stage approaches, checking for unsystematic discounting\n    (Johnson & Bickel, 2008; <doi:10.1037/1064-1297.16.3.264>) and scoring of the\n    minute discounting task (see Koffarnus & Bickel, 2014; <doi:10.1037/a0035973>) using the\n    Qualtrics 5-trial discounting template (see the Qualtrics Minute Discounting User Guide;\n    <doi:10.13140/RG.2.2.26495.79527>), which is also available as a .qsf file in this package.  "
  },
  {
    "id": 9043,
    "package_name": "bestNormalize",
    "title": "Normalizing Transformation Functions",
    "description": "Estimate a suite of normalizing transformations, including \n    a new adaptation of a technique based on ranks which can guarantee \n    normally distributed transformed data if there are no ties: ordered \n    quantile normalization (ORQ). ORQ normalization combines a rank-mapping\n    approach with a shifted logit approximation that allows\n    the transformation to work on data outside the original domain. It is \n    also able to handle new data within the original domain via linear \n    interpolation. The package is built to estimate the best normalizing \n    transformation for a vector consistently and accurately. It implements \n    the Box-Cox transformation, the Yeo-Johnson transformation, three types \n    of Lambert WxF transformations, and the ordered quantile normalization \n    transformation. It estimates the normalization efficacy of other\n    commonly used transformations, and it allows users to specify \n    custom transformations or normalization statistics. Finally, functionality\n    can be integrated into a machine learning workflow via recipes. ",
    "version": "1.9.2",
    "maintainer": "Ryan A Peterson <ryan-peterson@uiowa.edu>",
    "author": "Ryan A Peterson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4650-5798>)",
    "url": "https://petersonr.github.io/bestNormalize/,\nhttps://github.com/petersonR/bestNormalize",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bestNormalize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bestNormalize Normalizing Transformation Functions Estimate a suite of normalizing transformations, including \n    a new adaptation of a technique based on ranks which can guarantee \n    normally distributed transformed data if there are no ties: ordered \n    quantile normalization (ORQ). ORQ normalization combines a rank-mapping\n    approach with a shifted logit approximation that allows\n    the transformation to work on data outside the original domain. It is \n    also able to handle new data within the original domain via linear \n    interpolation. The package is built to estimate the best normalizing \n    transformation for a vector consistently and accurately. It implements \n    the Box-Cox transformation, the Yeo-Johnson transformation, three types \n    of Lambert WxF transformations, and the ordered quantile normalization \n    transformation. It estimates the normalization efficacy of other\n    commonly used transformations, and it allows users to specify \n    custom transformations or normalization statistics. Finally, functionality\n    can be integrated into a machine learning workflow via recipes.   "
  },
  {
    "id": 9124,
    "package_name": "bignum",
    "title": "Arbitrary-Precision Integer and Floating-Point Mathematics",
    "description": "Classes for storing and manipulating arbitrary-precision\n    integer vectors and high-precision floating-point vectors. These\n    extend the range and precision of the 'integer' and 'double' data\n    types found in R. This package utilizes the 'Boost.Multiprecision' C++\n    library. It is specifically designed to work well with the 'tidyverse'\n    collection of R packages.",
    "version": "0.3.2",
    "maintainer": "David Hall <david.hall.physics@gmail.com>",
    "author": "David Hall [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2193-0480>)",
    "url": "https://davidchall.github.io/bignum/,\nhttps://github.com/davidchall/bignum",
    "bug_reports": "https://github.com/davidchall/bignum/issues",
    "repository": "https://cran.r-project.org/package=bignum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bignum Arbitrary-Precision Integer and Floating-Point Mathematics Classes for storing and manipulating arbitrary-precision\n    integer vectors and high-precision floating-point vectors. These\n    extend the range and precision of the 'integer' and 'double' data\n    types found in R. This package utilizes the 'Boost.Multiprecision' C++\n    library. It is specifically designed to work well with the 'tidyverse'\n    collection of R packages.  "
  },
  {
    "id": 9129,
    "package_name": "bigsimr",
    "title": "Fast Generation of High-Dimensional Random Vectors",
    "description": "Simulate multivariate data with arbitrary marginal distributions.\n  'bigsimr' is a package for simulating high-dimensional multivariate data with \n  a target correlation and arbitrary marginal distributions via Gaussian copula. \n  It utilizes the Julia package 'Bigsimr.jl' for its core routines.",
    "version": "0.12.0",
    "maintainer": "Alex Knudson <alexk.706@gmail.com>",
    "author": "Alex Knudson [aut, cre],\n  Grant Schissler [aut],\n  Duc Tran [ctb]",
    "url": "https://github.com/SchisslerGroup/r-bigsimr",
    "bug_reports": "https://github.com/SchisslerGroup/Bigsimr.jl/issues",
    "repository": "https://cran.r-project.org/package=bigsimr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigsimr Fast Generation of High-Dimensional Random Vectors Simulate multivariate data with arbitrary marginal distributions.\n  'bigsimr' is a package for simulating high-dimensional multivariate data with \n  a target correlation and arbitrary marginal distributions via Gaussian copula. \n  It utilizes the Julia package 'Bigsimr.jl' for its core routines.  "
  },
  {
    "id": 9137,
    "package_name": "bigtime",
    "title": "Sparse Estimation of Large Time Series Models",
    "description": "Estimation of large Vector AutoRegressive (VAR), Vector AutoRegressive with Exogenous Variables X (VARX) and Vector AutoRegressive Moving Average (VARMA) Models with Structured Lasso Penalties, see Nicholson, Wilms, Bien and Matteson (2020) <https://jmlr.org/papers/v21/19-777.html> and Wilms, Basu, Bien and Matteson (2021) <doi:10.1080/01621459.2021.1942013>.",
    "version": "0.2.3",
    "maintainer": "Ines Wilms <i.wilms@maastrichtuniversity.nl>",
    "author": "Ines Wilms [cre, aut],\n  David S. Matteson [aut],\n  Jacob Bien [aut],\n  Sumanta Basu [aut],\n  Will Nicholson [aut],\n  Enrico Wegner [aut]",
    "url": "https://github.com/ineswilms/bigtime",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bigtime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigtime Sparse Estimation of Large Time Series Models Estimation of large Vector AutoRegressive (VAR), Vector AutoRegressive with Exogenous Variables X (VARX) and Vector AutoRegressive Moving Average (VARMA) Models with Structured Lasso Penalties, see Nicholson, Wilms, Bien and Matteson (2020) <https://jmlr.org/papers/v21/19-777.html> and Wilms, Basu, Bien and Matteson (2021) <doi:10.1080/01621459.2021.1942013>.  "
  },
  {
    "id": 9163,
    "package_name": "binomCI",
    "title": "Confidence Intervals for a Binomial Proportion",
    "description": "Twelve confidence intervals for one binomial proportion or a vector of binomial proportions are computed. The confidence intervals are: Jeffreys,  Wald, Wald corrected, Wald, Blyth and Still,  Agresti and Coull, Wilson, Score, Score corrected, Wald logit, Wald logit corrected, Arcsine and Exact binomial. References include, among others: Vollset, S. E. (1993). \"Confidence intervals for a binomial proportion\". Statistics in Medicine, 12(9): 809-824. <doi:10.1002/sim.4780120902>. ",
    "version": "1.2",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=binomCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binomCI Confidence Intervals for a Binomial Proportion Twelve confidence intervals for one binomial proportion or a vector of binomial proportions are computed. The confidence intervals are: Jeffreys,  Wald, Wald corrected, Wald, Blyth and Still,  Agresti and Coull, Wilson, Score, Score corrected, Wald logit, Wald logit corrected, Arcsine and Exact binomial. References include, among others: Vollset, S. E. (1993). \"Confidence intervals for a binomial proportion\". Statistics in Medicine, 12(9): 809-824. <doi:10.1002/sim.4780120902>.   "
  },
  {
    "id": 9182,
    "package_name": "bioclim",
    "title": "Bioclimatic Analysis and Classification",
    "description": "Using numeric or raster data, this package contains functions to \n    calculate: complete water balance, bioclimatic balance, bioclimatic \n    intensities, reports for individual locations, multi-layered rasters for\n    spatial analysis.",
    "version": "0.4.0",
    "maintainer": "Roberto Serrano-Notivoli <roberto.serrano@unizar.es>",
    "author": "Roberto Serrano-Notivoli",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bioclim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bioclim Bioclimatic Analysis and Classification Using numeric or raster data, this package contains functions to \n    calculate: complete water balance, bioclimatic balance, bioclimatic \n    intensities, reports for individual locations, multi-layered rasters for\n    spatial analysis.  "
  },
  {
    "id": 9191,
    "package_name": "biologicalActivityIndices",
    "title": "Biological Activity Indices",
    "description": "Ecological alteration of degraded lands can improve their sustainability by addition of large amount of biomass to soil resulting in improved soil health. Soil biological parameters (such as carbon, nitrogen and phosphorus cycling enzyme activity) are reactive to minute variations in soils [Ghosh et al. (2021) <doi:10.1016/j.ecoleng.2021.106176> ]. Hence, biological activity index combining Urease, Alkaline Phosphatase, Dehydrogenase (DHA) & Beta-Glucosidase activity will assist in detecting early changes in restored land use systems [Patidar et al. (2023) <doi:10.3389/fsufs.2023.1230156>]. This package helps to calculate Biological Activity Index (BAI) based on vectors of Land Use System/treatment and control/reference Land Use System containing four values of Urease, Alkaline Phosphatase, DHA & Beta-Glucosidase. (DHA), urease (URE), fluorescein diacetate hydrolysis (FDA) and alkaline phosphatase (ALP) activities are measured in soil samples using triphenyl tetrazolium chloride, urea, fluorescein diacetate and p-nitro phenyl-phosphate as substrates, respectively.",
    "version": "0.1.0",
    "maintainer": "Tanuj Misra <tanujmisra102@gmail.com>",
    "author": "Avijit Ghosh [aut],\n  Tanuj Misra [aut, cre],\n  Amit Kumar Singh [aut],\n  Arun Shukla [aut],\n  Saurav Singla [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biologicalActivityIndices",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biologicalActivityIndices Biological Activity Indices Ecological alteration of degraded lands can improve their sustainability by addition of large amount of biomass to soil resulting in improved soil health. Soil biological parameters (such as carbon, nitrogen and phosphorus cycling enzyme activity) are reactive to minute variations in soils [Ghosh et al. (2021) <doi:10.1016/j.ecoleng.2021.106176> ]. Hence, biological activity index combining Urease, Alkaline Phosphatase, Dehydrogenase (DHA) & Beta-Glucosidase activity will assist in detecting early changes in restored land use systems [Patidar et al. (2023) <doi:10.3389/fsufs.2023.1230156>]. This package helps to calculate Biological Activity Index (BAI) based on vectors of Land Use System/treatment and control/reference Land Use System containing four values of Urease, Alkaline Phosphatase, DHA & Beta-Glucosidase. (DHA), urease (URE), fluorescein diacetate hydrolysis (FDA) and alkaline phosphatase (ALP) activities are measured in soil samples using triphenyl tetrazolium chloride, urea, fluorescein diacetate and p-nitro phenyl-phosphate as substrates, respectively.  "
  },
  {
    "id": 9222,
    "package_name": "bispdep",
    "title": "Statistical Tools for Bivariate Spatial Dependence Analysis",
    "description": "A collection of functions to test spatial autocorrelation between variables, including Moran I, Geary C and Getis G together with scatter plots, functions for mapping and identifying clusters and outliers, functions associated with the moments of the previous statistics that will allow testing whether there is bivariate spatial autocorrelation, and a function that allows identifying (visualizing neighbours) on the map, the neighbors of any region once the scheme of the spatial weights matrix has been established.",
    "version": "1.0-2",
    "maintainer": "Carlos Melo <cmelo@udistrital.edu.co>",
    "author": "Carlos Melo [aut, cre] (ORCID: <https://orcid.org/0000-0002-5598-1913>),\n  Oscar Melo [ctb] (ORCID: <https://orcid.org/0000-0002-0296-4511>),\n  Sandra Melo [ctb] (ORCID: <https://orcid.org/0000-0002-4875-7657>)",
    "url": "https://github.com/carlosm77/bispdep",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bispdep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bispdep Statistical Tools for Bivariate Spatial Dependence Analysis A collection of functions to test spatial autocorrelation between variables, including Moran I, Geary C and Getis G together with scatter plots, functions for mapping and identifying clusters and outliers, functions associated with the moments of the previous statistics that will allow testing whether there is bivariate spatial autocorrelation, and a function that allows identifying (visualizing neighbours) on the map, the neighbors of any region once the scheme of the spatial weights matrix has been established.  "
  },
  {
    "id": 9226,
    "package_name": "bitops",
    "title": "Bitwise Operations",
    "description": "Functions for bitwise operations on integer vectors.",
    "version": "1.0-9",
    "maintainer": "Martin Maechler <maechler@stat.math.ethz.ch>",
    "author": "Steve Dutky [aut] (S original; then (after MM's port) revised and\n    modified),\n  Martin Maechler [cre, aut] (Initial R port; tweaks, ORCID:\n    <https://orcid.org/0000-0002-8685-9910>)",
    "url": "https://github.com/mmaechler/R-bitops",
    "bug_reports": "https://github.com/mmaechler/R-bitops/issues",
    "repository": "https://cran.r-project.org/package=bitops",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bitops Bitwise Operations Functions for bitwise operations on integer vectors.  "
  },
  {
    "id": 9227,
    "package_name": "bitstreamio",
    "title": "Read and Write Bits from Files, Connections and Raw Vectors",
    "description": "Bit-level reading and writing are necessary when dealing with \n  many file formats e.g. compressed data and binary files.  Currently, R \n  connections are manipulated at the byte level.  This package wraps existing\n  connections and raw vectors so that it is possible to read bits, bit sequences,\n  unaligned bytes and low-bit representations of integers.",
    "version": "0.1.0",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph]",
    "url": "https://github.com/coolbutuseless/bitstreamio",
    "bug_reports": "https://github.com/coolbutuseless/bitstreamio/issues",
    "repository": "https://cran.r-project.org/package=bitstreamio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bitstreamio Read and Write Bits from Files, Connections and Raw Vectors Bit-level reading and writing are necessary when dealing with \n  many file formats e.g. compressed data and binary files.  Currently, R \n  connections are manipulated at the byte level.  This package wraps existing\n  connections and raw vectors so that it is possible to read bits, bit sequences,\n  unaligned bytes and low-bit representations of integers.  "
  },
  {
    "id": 9235,
    "package_name": "biwt",
    "title": "Compute the Biweight Mean Vector and Covariance & Correlation\nMatrice",
    "description": "Compute multivariate location, scale, and correlation\n        estimates based on Tukey's biweight M-estimator.",
    "version": "1.0.1",
    "maintainer": "Jo Hardin <jo.hardin@pomona.edu>",
    "author": "Jo Hardin <jo.hardin@pomona.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biwt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biwt Compute the Biweight Mean Vector and Covariance & Correlation\nMatrice Compute multivariate location, scale, and correlation\n        estimates based on Tukey's biweight M-estimator.  "
  },
  {
    "id": 9237,
    "package_name": "bizicount",
    "title": "Bivariate Zero-Inflated Count Models Using Copulas",
    "description": "Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Ne\u0161lehov\u00e1 (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.",
    "version": "1.3.4",
    "maintainer": "John Niehaus <jniehaus2257@gmail.com>",
    "author": "John Niehaus [aut, cre]",
    "url": "https://github.com/jmniehaus/bizicount",
    "bug_reports": "https://github.com/jmniehaus/bizicount/issues",
    "repository": "https://cran.r-project.org/package=bizicount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bizicount Bivariate Zero-Inflated Count Models Using Copulas Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Ne\u0161lehov\u00e1 (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.  "
  },
  {
    "id": 9240,
    "package_name": "blackbox",
    "title": "Black Box Optimization and Exploration of Parameter Space",
    "description": "Performs prediction of a response function from simulated response values, allowing black-box optimization of functions estimated with some error. Includes a simple user interface for such applications, as well as more specialized functions designed to be called by the Migraine software (Rousset and Leblois, 2012 <doi:10.1093/molbev/MSR262>; Leblois et al., 2014 <doi:10.1093/molbev/msu212>; and see URL). The latter functions are used for prediction of likelihood surfaces and implied likelihood ratio confidence intervals, and for exploration of predictor space of the surface. Prediction of the response is based on ordinary Kriging (with residual error) of the input. Estimation of smoothing parameters is performed by generalized cross-validation.",
    "version": "1.1.46",
    "maintainer": "Fran\u00e7ois Rousset <francois.rousset@umontpellier.fr>",
    "author": "Fran\u00e7ois Rousset [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4670-0371>),\n  Rapha\u00ebl Leblois [ctb] (ORCID: <https://orcid.org/0000-0002-3051-4497>)",
    "url": "https://kimura.univ-montp2.fr/~rousset/Migraine.htm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=blackbox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blackbox Black Box Optimization and Exploration of Parameter Space Performs prediction of a response function from simulated response values, allowing black-box optimization of functions estimated with some error. Includes a simple user interface for such applications, as well as more specialized functions designed to be called by the Migraine software (Rousset and Leblois, 2012 <doi:10.1093/molbev/MSR262>; Leblois et al., 2014 <doi:10.1093/molbev/msu212>; and see URL). The latter functions are used for prediction of likelihood surfaces and implied likelihood ratio confidence intervals, and for exploration of predictor space of the surface. Prediction of the response is based on ordinary Kriging (with residual error) of the input. Estimation of smoothing parameters is performed by generalized cross-validation.  "
  },
  {
    "id": 9258,
    "package_name": "blockCV",
    "title": "Spatial and Environmental Blocking for K-Fold and LOO\nCross-Validation",
    "description": "Creating spatially or environmentally separated folds for cross-validation to provide a robust error estimation in spatially structured environments; Investigating and visualising the effective range of spatial autocorrelation in continuous raster covariates and point samples to find an initial realistic distance band to separate training and testing datasets spatially described in Valavi, R. et al. (2019) <doi:10.1111/2041-210X.13107>.",
    "version": "3.2-0",
    "maintainer": "Roozbeh Valavi <valavi.r@gmail.com>",
    "author": "Roozbeh Valavi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2495-5277>),\n  Jane Elith [aut],\n  Jos\u00e9 Lahoz-Monfort [aut],\n  Ian Flint [aut],\n  Gurutzeta Guillera-Arroita [aut]",
    "url": "https://github.com/rvalavi/blockCV",
    "bug_reports": "https://github.com/rvalavi/blockCV/issues",
    "repository": "https://cran.r-project.org/package=blockCV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blockCV Spatial and Environmental Blocking for K-Fold and LOO\nCross-Validation Creating spatially or environmentally separated folds for cross-validation to provide a robust error estimation in spatially structured environments; Investigating and visualising the effective range of spatial autocorrelation in continuous raster covariates and point samples to find an initial realistic distance band to separate training and testing datasets spatially described in Valavi, R. et al. (2019) <doi:10.1111/2041-210X.13107>.  "
  },
  {
    "id": 9263,
    "package_name": "blocking",
    "title": "Various Blocking Methods for Entity Resolution",
    "description": "The goal of 'blocking' is to provide blocking methods for record linkage and deduplication using approximate nearest neighbour (ANN) algorithms and graph techniques. It supports multiple ANN implementations via 'rnndescent', 'RcppHNSW', 'RcppAnnoy', and 'mlpack' packages, and provides integration with the 'reclin2' package. The package generates shingles from character strings and similarity vectors for record comparison, and includes evaluation metrics for assessing blocking performance including false positive rate (FPR) and false negative rate (FNR) estimates. For details see: Papadakis et al. (2020) <doi:10.1145/3377455>, Steorts et al. (2014) <doi:10.1007/978-3-319-11257-2_20>, Dasylva and Goussanou (2021) <https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X202100200002>, Dasylva and Goussanou (2022) <doi:10.1007/s42081-022-00153-3>.",
    "version": "1.0.1",
    "maintainer": "Maciej Ber\u0119sewicz <maciej.beresewicz@ue.poznan.pl>",
    "author": "Maciej Ber\u0119sewicz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8281-4301>),\n  Adam Struzik [aut, ctr]",
    "url": "https://github.com/ncn-foreigners/blocking,\nhttps://ncn-foreigners.ue.poznan.pl/blocking/",
    "bug_reports": "https://github.com/ncn-foreigners/blocking/issues",
    "repository": "https://cran.r-project.org/package=blocking",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blocking Various Blocking Methods for Entity Resolution The goal of 'blocking' is to provide blocking methods for record linkage and deduplication using approximate nearest neighbour (ANN) algorithms and graph techniques. It supports multiple ANN implementations via 'rnndescent', 'RcppHNSW', 'RcppAnnoy', and 'mlpack' packages, and provides integration with the 'reclin2' package. The package generates shingles from character strings and similarity vectors for record comparison, and includes evaluation metrics for assessing blocking performance including false positive rate (FPR) and false negative rate (FNR) estimates. For details see: Papadakis et al. (2020) <doi:10.1145/3377455>, Steorts et al. (2014) <doi:10.1007/978-3-319-11257-2_20>, Dasylva and Goussanou (2021) <https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X202100200002>, Dasylva and Goussanou (2022) <doi:10.1007/s42081-022-00153-3>.  "
  },
  {
    "id": 9265,
    "package_name": "blockmatrix",
    "title": "blockmatrix: Tools to solve algebraic systems with partitioned\nmatrices",
    "description": "Some elementary matrix algebra tools are implemented to manage\n    block matrices or partitioned matrix, i.e. \"matrix of matrices\"\n    (http://en.wikipedia.org/wiki/Block_matrix). The block matrix is here\n    defined as a new S3 object. In this package, some methods for \"matrix\"\n    object are rewritten for \"blockmatrix\" object. New methods are implemented.\n    This package was created to solve equation systems with block matrices for\n    the analysis of environmental vector time series .\n    Bugs/comments/questions/collaboration of any kind are warmly welcomed.",
    "version": "1.0",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano",
    "url": "http://cri.gmpf.eu/Research/Sustainable-Agro-Ecosystems-and-Bioresources/Dynamics-in-the-agro-ecosystems/people/Emanuele-Cordano",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=blockmatrix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blockmatrix blockmatrix: Tools to solve algebraic systems with partitioned\nmatrices Some elementary matrix algebra tools are implemented to manage\n    block matrices or partitioned matrix, i.e. \"matrix of matrices\"\n    (http://en.wikipedia.org/wiki/Block_matrix). The block matrix is here\n    defined as a new S3 object. In this package, some methods for \"matrix\"\n    object are rewritten for \"blockmatrix\" object. New methods are implemented.\n    This package was created to solve equation systems with block matrices for\n    the analysis of environmental vector time series .\n    Bugs/comments/questions/collaboration of any kind are warmly welcomed.  "
  },
  {
    "id": 9274,
    "package_name": "blox",
    "title": "Block Diagonal Matrix Approximation",
    "description": "Finds the best block diagonal matrix approximation of a symmetric matrix. This can be exploited for divisive hierarchical clustering using singular vectors, named HC-SVD. The method is described in Bauer (202Xa) <doi:10.48550/arXiv.2308.06820>.",
    "version": "0.0.1",
    "maintainer": "Jan O. Bauer <j.bauer@vu.nl>",
    "author": "Jan O. Bauer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7123-4507>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=blox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blox Block Diagonal Matrix Approximation Finds the best block diagonal matrix approximation of a symmetric matrix. This can be exploited for divisive hierarchical clustering using singular vectors, named HC-SVD. The method is described in Bauer (202Xa) <doi:10.48550/arXiv.2308.06820>.  "
  },
  {
    "id": 9387,
    "package_name": "bracer",
    "title": "Brace Expansions",
    "description": "Performs brace expansions on strings.  Made popular by Unix shells, brace expansion allows users to concisely generate certain character vectors by taking a single string and (recursively) expanding the comma-separated lists and double-period-separated integer and character sequences enclosed within braces in that string.  The double-period-separated numeric integer expansion also supports padding the resulting numbers with zeros.",
    "version": "1.2.2",
    "maintainer": "Trevor L Davis <trevor.l.davis@gmail.com>",
    "author": "Trevor L Davis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6341-4639>),\n  Jon Schlinkert [aut] (Author of the 'braces' Javascript library)",
    "url": "https://trevorldavis.com/R/bracer/,\nhttps://github.com/trevorld/bracer",
    "bug_reports": "https://github.com/trevorld/bracer/issues",
    "repository": "https://cran.r-project.org/package=bracer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bracer Brace Expansions Performs brace expansions on strings.  Made popular by Unix shells, brace expansion allows users to concisely generate certain character vectors by taking a single string and (recursively) expanding the comma-separated lists and double-period-separated integer and character sequences enclosed within braces in that string.  The double-period-separated numeric integer expansion also supports padding the resulting numbers with zeros.  "
  },
  {
    "id": 9404,
    "package_name": "brclimr",
    "title": "Fetch Zonal Statistics of Weather Indicators for Brazilian\nMunicipalities",
    "description": "Fetches zonal statistics from weather indicators that were calculated for each municipality in Brazil using data from the BR-DWGD and TerraClimate projects.\n    Zonal statistics such as mean, maximum, minimum, standard deviation, and sum were computed by taking into account the data cells that intersect the boundaries of each municipality and stored in Parquet files. This procedure was carried out for all Brazilian municipalities, and for all available dates, for every indicator available in the weather products (BR-DWGD and TerraClimate projects). This package queries on-line the already calculated statistics on the Parquet files and returns easy-to-use data.frames.",
    "version": "0.2.0",
    "maintainer": "Raphael Saldanha <raphael.de-freitas-saldanha@inria.fr>",
    "author": "Raphael Saldanha [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0652-8466>),\n  Reza Akbarinia [aut],\n  Patrick Valduriez [aut],\n  Marcel Pedroso [aut],\n  Victor Ribeiro [aut],\n  Carlos Cardoso [aut],\n  Eduardo Pena [aut],\n  Fabio Porto [aut]",
    "url": "https://rfsaldanha.github.io/brclimr/,\nhttps://github.com/rfsaldanha/brclimr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=brclimr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brclimr Fetch Zonal Statistics of Weather Indicators for Brazilian\nMunicipalities Fetches zonal statistics from weather indicators that were calculated for each municipality in Brazil using data from the BR-DWGD and TerraClimate projects.\n    Zonal statistics such as mean, maximum, minimum, standard deviation, and sum were computed by taking into account the data cells that intersect the boundaries of each municipality and stored in Parquet files. This procedure was carried out for all Brazilian municipalities, and for all available dates, for every indicator available in the weather products (BR-DWGD and TerraClimate projects). This package queries on-line the already calculated statistics on the Parquet files and returns easy-to-use data.frames.  "
  },
  {
    "id": 9448,
    "package_name": "bsearchtools",
    "title": "Binary Search Tools",
    "description": "Exposes the binary search functions of the C++ standard library (std::lower_bound, std::upper_bound) plus other convenience functions, allowing faster lookups on sorted vectors.",
    "version": "0.0.61",
    "maintainer": "Marco Giuliano <mgiuliano.mail@gmail.com>",
    "author": "Marco Giuliano",
    "url": "https://github.com/digEmAll/bsearchtools",
    "bug_reports": "https://github.com/digEmAll/bsearchtools/issues",
    "repository": "https://cran.r-project.org/package=bsearchtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsearchtools Binary Search Tools Exposes the binary search functions of the C++ standard library (std::lower_bound, std::upper_bound) plus other convenience functions, allowing faster lookups on sorted vectors.  "
  },
  {
    "id": 9456,
    "package_name": "bspline",
    "title": "B-Spline Interpolation and Regression",
    "description": "Build and use B-splines for interpolation and regression.\n  In case of regression, equality constraints as well as monotonicity\n  and/or positivity of B-spline weights can be imposed. Moreover, \n  knot positions can be on regular grid or be part of \n  optimized parameters too (in addition to the spline weights).\n  For this end, 'bspline' is able to calculate\n  Jacobian of basis vectors as function of knot positions. User is provided with \n  functions calculating spline values at arbitrary points. These \n  functions can be differentiated and integrated to obtain B-splines calculating \n  derivatives/integrals at any point. B-splines of this package can \n  simultaneously operate on a series of curves sharing the same set of \n  knots. 'bspline' is written with concern about computing \n  performance that's why the basis and Jacobian calculation is implemented in C++.\n  The rest is implemented in R but without notable impact on computing speed.",
    "version": "2.5.0",
    "maintainer": "Serguei Sokol <sokol@insa-toulouse.fr>",
    "author": "Serguei Sokol [aut, cre]",
    "url": "https://github.com/MathsCell/bspline",
    "bug_reports": "https://github.com/MathsCell/bspline/issues",
    "repository": "https://cran.r-project.org/package=bspline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bspline B-Spline Interpolation and Regression Build and use B-splines for interpolation and regression.\n  In case of regression, equality constraints as well as monotonicity\n  and/or positivity of B-spline weights can be imposed. Moreover, \n  knot positions can be on regular grid or be part of \n  optimized parameters too (in addition to the spline weights).\n  For this end, 'bspline' is able to calculate\n  Jacobian of basis vectors as function of knot positions. User is provided with \n  functions calculating spline values at arbitrary points. These \n  functions can be differentiated and integrated to obtain B-splines calculating \n  derivatives/integrals at any point. B-splines of this package can \n  simultaneously operate on a series of curves sharing the same set of \n  knots. 'bspline' is written with concern about computing \n  performance that's why the basis and Jacobian calculation is implemented in C++.\n  The rest is implemented in R but without notable impact on computing speed.  "
  },
  {
    "id": 9466,
    "package_name": "bsub",
    "title": "Submitter and Monitor of the 'LSF Cluster'",
    "description": "It submits R code/R scripts/shell commands to 'LSF cluster' \n  (<https://en.wikipedia.org/wiki/Platform_LSF>, the 'bsub' system) without \n  leaving R. There is also an interactive 'shiny' application for monitoring job status.",
    "version": "2.0.6",
    "maintainer": "Zuguang Gu <z.gu@dkfz.de>",
    "author": "Zuguang Gu [aut, cre] (ORCID: <https://orcid.org/0000-0002-7395-8709>)",
    "url": "https://github.com/jokergoo/bsub",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bsub",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsub Submitter and Monitor of the 'LSF Cluster' It submits R code/R scripts/shell commands to 'LSF cluster' \n  (<https://en.wikipedia.org/wiki/Platform_LSF>, the 'bsub' system) without \n  leaving R. There is also an interactive 'shiny' application for monitoring job status.  "
  },
  {
    "id": 9467,
    "package_name": "bsvarSIGNs",
    "title": "Bayesian SVARs with Sign, Zero, and Narrative Restrictions",
    "description": "Implements state-of-the-art algorithms for the Bayesian analysis of Structural Vector Autoregressions (SVARs) identified by sign, zero, and narrative restrictions. The core model is based on a flexible Vector Autoregression with estimated hyper-parameters of the Minnesota prior and the dummy observation priors as in Giannone, Lenza, Primiceri (2015) <doi:10.1162/REST_a_00483>. The sign restrictions are implemented employing the methods proposed by Rubio-Ram\u00edrez, Waggoner & Zha (2010) <doi:10.1111/j.1467-937X.2009.00578.x>, while identification through sign and zero restrictions follows the approach developed by Arias, Rubio-Ram\u00edrez, & Waggoner (2018) <doi:10.3982/ECTA14468>. Furthermore, our tool provides algorithms for identification via sign and narrative restrictions, in line with the methods introduced by Antol\u00edn-D\u00edaz and Rubio-Ram\u00edrez (2018) <doi:10.1257/aer.20161852>. Users can also estimate a model with sign, zero, and narrative restrictions imposed at once. The package facilitates predictive and structural analyses using impulse responses, forecast error variance and historical decompositions, forecasting and conditional forecasting, as well as analyses of structural shocks and fitted values. All this is complemented by colourful plots, user-friendly summary functions, and comprehensive documentation including the vignette by Wang & Wo\u017aniak (2024) <doi:10.48550/arXiv.2501.16711>. The 'bsvarSIGNs' package is aligned regarding objects, workflows, and code structure with the R package 'bsvars' by Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars>, and they constitute an integrated toolset. It was granted the Di Cook Open-Source Statistical Software Award by the Statistical Society of Australia in 2024.",
    "version": "2.0",
    "maintainer": "Xiaolei Wang <adamwang15@gmail.com>",
    "author": "Xiaolei Wang [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-6192-9061>),\n  Tomasz Wo\u017aniak [aut] (ORCID: <https://orcid.org/0000-0003-2212-2378>)",
    "url": "https://bsvars.org/bsvarSIGNs/",
    "bug_reports": "https://github.com/bsvars/bsvarSIGNs/issues",
    "repository": "https://cran.r-project.org/package=bsvarSIGNs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsvarSIGNs Bayesian SVARs with Sign, Zero, and Narrative Restrictions Implements state-of-the-art algorithms for the Bayesian analysis of Structural Vector Autoregressions (SVARs) identified by sign, zero, and narrative restrictions. The core model is based on a flexible Vector Autoregression with estimated hyper-parameters of the Minnesota prior and the dummy observation priors as in Giannone, Lenza, Primiceri (2015) <doi:10.1162/REST_a_00483>. The sign restrictions are implemented employing the methods proposed by Rubio-Ram\u00edrez, Waggoner & Zha (2010) <doi:10.1111/j.1467-937X.2009.00578.x>, while identification through sign and zero restrictions follows the approach developed by Arias, Rubio-Ram\u00edrez, & Waggoner (2018) <doi:10.3982/ECTA14468>. Furthermore, our tool provides algorithms for identification via sign and narrative restrictions, in line with the methods introduced by Antol\u00edn-D\u00edaz and Rubio-Ram\u00edrez (2018) <doi:10.1257/aer.20161852>. Users can also estimate a model with sign, zero, and narrative restrictions imposed at once. The package facilitates predictive and structural analyses using impulse responses, forecast error variance and historical decompositions, forecasting and conditional forecasting, as well as analyses of structural shocks and fitted values. All this is complemented by colourful plots, user-friendly summary functions, and comprehensive documentation including the vignette by Wang & Wo\u017aniak (2024) <doi:10.48550/arXiv.2501.16711>. The 'bsvarSIGNs' package is aligned regarding objects, workflows, and code structure with the R package 'bsvars' by Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars>, and they constitute an integrated toolset. It was granted the Di Cook Open-Source Statistical Software Award by the Statistical Society of Australia in 2024.  "
  },
  {
    "id": 9468,
    "package_name": "bsvars",
    "title": "Bayesian Estimation of Structural Vector Autoregressive Models",
    "description": "Provides fast and efficient procedures for Bayesian analysis of Structural Vector Autoregressions. This package estimates a wide range of models, including homo-, heteroskedastic, and non-normal specifications. Structural models can be identified by adjustable exclusion restrictions, time-varying volatility, or non-normality. They all include a flexible three-level equation-specific local-global hierarchical prior distribution for the estimated level of shrinkage for autoregressive and structural parameters. Additionally, the package facilitates predictive and structural analyses such as impulse responses, forecast error variance and historical decompositions, forecasting, verification of heteroskedasticity, non-normality, and hypotheses on autoregressive parameters, as well as analyses of structural shocks, volatilities, and fitted values. Beautiful plots, informative summary functions, and extensive documentation including the vignette by Wo\u017aniak (2024) <doi:10.48550/arXiv.2410.15090> complement all this. The implemented techniques align closely with those presented in L\u00fctkepohl, Shang, Uzeda, & Wo\u017aniak (2024) <doi:10.48550/arXiv.2404.11057>, L\u00fctkepohl & Wo\u017aniak (2020) <doi:10.1016/j.jedc.2020.103862>, and Song & Wo\u017aniak (2021) <doi:10.1093/acrefore/9780190625979.013.174>. The 'bsvars' package is aligned regarding objects, workflows, and code structure with the R package 'bsvarSIGNs' by Wang & Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they constitute an integrated toolset.",
    "version": "3.2",
    "maintainer": "Tomasz Wo\u017aniak <wozniak.tom@pm.me>",
    "author": "Tomasz Wo\u017aniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2212-2378>)",
    "url": "https://bsvars.org/bsvars/",
    "bug_reports": "https://github.com/bsvars/bsvars/issues",
    "repository": "https://cran.r-project.org/package=bsvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsvars Bayesian Estimation of Structural Vector Autoregressive Models Provides fast and efficient procedures for Bayesian analysis of Structural Vector Autoregressions. This package estimates a wide range of models, including homo-, heteroskedastic, and non-normal specifications. Structural models can be identified by adjustable exclusion restrictions, time-varying volatility, or non-normality. They all include a flexible three-level equation-specific local-global hierarchical prior distribution for the estimated level of shrinkage for autoregressive and structural parameters. Additionally, the package facilitates predictive and structural analyses such as impulse responses, forecast error variance and historical decompositions, forecasting, verification of heteroskedasticity, non-normality, and hypotheses on autoregressive parameters, as well as analyses of structural shocks, volatilities, and fitted values. Beautiful plots, informative summary functions, and extensive documentation including the vignette by Wo\u017aniak (2024) <doi:10.48550/arXiv.2410.15090> complement all this. The implemented techniques align closely with those presented in L\u00fctkepohl, Shang, Uzeda, & Wo\u017aniak (2024) <doi:10.48550/arXiv.2404.11057>, L\u00fctkepohl & Wo\u017aniak (2020) <doi:10.1016/j.jedc.2020.103862>, and Song & Wo\u017aniak (2021) <doi:10.1093/acrefore/9780190625979.013.174>. The 'bsvars' package is aligned regarding objects, workflows, and code structure with the R package 'bsvarSIGNs' by Wang & Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they constitute an integrated toolset.  "
  },
  {
    "id": 9477,
    "package_name": "budgetIVr",
    "title": "Partial Identification of Causal Effects with Mostly Invalid\nInstruments",
    "description": "A tuneable and interpretable method for relaxing \n\tthe instrumental variables (IV) assumptions to infer treatment effects in the presence\n\tof unobserved confounding.\n\tFor a treatment-associated covariate to be a valid IV, it must be (a) unconfounded with the outcome\n\tand (b) have a causal effect on the outcome that is exclusively mediated by the exposure. \n\tThere is no general test of the validity of these IV assumptions for any particular pre-treatment \n\tcovariate. \n\tHowever, if different pre-treatment covariates give differing causal effect estimates \n\twhen treated as IVs, then we know at least some of the covariates violate these assumptions. \n\t'budgetIVr' exploits this fact by taking as input a minimum budget of pre-treatment covariates assumed\n\tto be valid IVs and idenfiying the set of causal effects that are consistent with the user's data and budget assumption.\n\tThe following generalizations of this principle can be used in this package:\n\t(1) a vector of multiple budgets can be assigned alongside corresponding thresholds that model degrees of IV invalidity; \n\t(2) budgets and thresholds can be chosen using specialist knowledge or varied in a principled sensitivity analysis;\n\t(3) treatment effects can be nonlinear and/or depend on multiple exposures (at a computational cost).\n\tThe methods in this package require only summary statistics. \n\tConfidence sets are constructed under the \"no measurement error\" (NOME) assumption from the Mendelian randomization literature.\n\tFor further methodological details, please refer to Penn et al. (2024) <doi:10.48550/arXiv.2411.06913>.",
    "version": "0.1.2",
    "maintainer": "Jordan Penn <jordan.penn5841@gmail.com>",
    "author": "Jordan Penn [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0002-3572-1724>)",
    "url": "https://github.com/jpenn2023/budgetIVr",
    "bug_reports": "https://github.com/jpenn2023/budgetIVr/issues",
    "repository": "https://cran.r-project.org/package=budgetIVr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "budgetIVr Partial Identification of Causal Effects with Mostly Invalid\nInstruments A tuneable and interpretable method for relaxing \n\tthe instrumental variables (IV) assumptions to infer treatment effects in the presence\n\tof unobserved confounding.\n\tFor a treatment-associated covariate to be a valid IV, it must be (a) unconfounded with the outcome\n\tand (b) have a causal effect on the outcome that is exclusively mediated by the exposure. \n\tThere is no general test of the validity of these IV assumptions for any particular pre-treatment \n\tcovariate. \n\tHowever, if different pre-treatment covariates give differing causal effect estimates \n\twhen treated as IVs, then we know at least some of the covariates violate these assumptions. \n\t'budgetIVr' exploits this fact by taking as input a minimum budget of pre-treatment covariates assumed\n\tto be valid IVs and idenfiying the set of causal effects that are consistent with the user's data and budget assumption.\n\tThe following generalizations of this principle can be used in this package:\n\t(1) a vector of multiple budgets can be assigned alongside corresponding thresholds that model degrees of IV invalidity; \n\t(2) budgets and thresholds can be chosen using specialist knowledge or varied in a principled sensitivity analysis;\n\t(3) treatment effects can be nonlinear and/or depend on multiple exposures (at a computational cost).\n\tThe methods in this package require only summary statistics. \n\tConfidence sets are constructed under the \"no measurement error\" (NOME) assumption from the Mendelian randomization literature.\n\tFor further methodological details, please refer to Penn et al. (2024) <doi:10.48550/arXiv.2411.06913>.  "
  },
  {
    "id": 9504,
    "package_name": "bvarsv",
    "title": "Bayesian Analysis of a Vector Autoregressive Model with\nStochastic Volatility and Time-Varying Parameters",
    "description": "R/C++ implementation of the model proposed by Primiceri (\"Time Varying Structural Vector Autoregressions and Monetary Policy\", Review of Economic Studies, 2005), with functionality for computing posterior predictive distributions and impulse responses.",
    "version": "1.1",
    "maintainer": "Fabian Krueger <Fabian.Krueger83@gmail.com>",
    "author": "Fabian Krueger",
    "url": "https://sites.google.com/site/fk83research/code",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bvarsv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bvarsv Bayesian Analysis of a Vector Autoregressive Model with\nStochastic Volatility and Time-Varying Parameters R/C++ implementation of the model proposed by Primiceri (\"Time Varying Structural Vector Autoregressions and Monetary Policy\", Review of Economic Studies, 2005), with functionality for computing posterior predictive distributions and impulse responses.  "
  },
  {
    "id": 9505,
    "package_name": "bvartools",
    "title": "Bayesian Inference of Vector Autoregressive and Error Correction\nModels",
    "description": "Assists in the set-up of algorithms for Bayesian inference of vector autoregressive (VAR) and error correction (VEC) models. Functions for posterior simulation, forecasting, impulse response analysis and forecast error variance decomposition are largely based on the introductory texts of Chan, Koop, Poirier and Tobias (2019, ISBN: 9781108437493), Koop and Korobilis (2010) <doi:10.1561/0800000013> and Luetkepohl (2006, ISBN: 9783540262398).",
    "version": "0.2.4",
    "maintainer": "Franz X. Mohr <franz.x.mohr@outlook.com>",
    "author": "Franz X. Mohr [aut, cre] (ORCiD: 0009-0003-8890-7781)",
    "url": "https://github.com/franzmohr/bvartools",
    "bug_reports": "https://github.com/franzmohr/bvartools/issues",
    "repository": "https://cran.r-project.org/package=bvartools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bvartools Bayesian Inference of Vector Autoregressive and Error Correction\nModels Assists in the set-up of algorithms for Bayesian inference of vector autoregressive (VAR) and error correction (VEC) models. Functions for posterior simulation, forecasting, impulse response analysis and forecast error variance decomposition are largely based on the introductory texts of Chan, Koop, Poirier and Tobias (2019, ISBN: 9781108437493), Koop and Korobilis (2010) <doi:10.1561/0800000013> and Luetkepohl (2006, ISBN: 9783540262398).  "
  },
  {
    "id": 9506,
    "package_name": "bvhar",
    "title": "Bayesian Vector Heterogeneous Autoregressive Modeling",
    "description": "Tools to model and forecast multivariate time series\n    including Bayesian Vector heterogeneous autoregressive (VHAR) model\n    by Kim & Baek (2023) (<doi:10.1080/00949655.2023.2281644>).\n    'bvhar' can model Vector Autoregressive (VAR), VHAR, Bayesian VAR (BVAR), and Bayesian VHAR (BVHAR) models.",
    "version": "2.3.0",
    "maintainer": "Young Geun Kim <ygeunkimstat@gmail.com>",
    "author": "Young Geun Kim [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8651-1167>),\n  Changryong Baek [ctb]",
    "url": "https://ygeunkim.github.io/package/bvhar/,\nhttps://github.com/ygeunkim/bvhar",
    "bug_reports": "https://github.com/ygeunkim/bvhar/issues",
    "repository": "https://cran.r-project.org/package=bvhar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bvhar Bayesian Vector Heterogeneous Autoregressive Modeling Tools to model and forecast multivariate time series\n    including Bayesian Vector heterogeneous autoregressive (VHAR) model\n    by Kim & Baek (2023) (<doi:10.1080/00949655.2023.2281644>).\n    'bvhar' can model Vector Autoregressive (VAR), VHAR, Bayesian VAR (BVAR), and Bayesian VHAR (BVHAR) models.  "
  },
  {
    "id": 9553,
    "package_name": "calibrate",
    "title": "Calibration of Scatterplot and Biplot Axes",
    "description": "Package for drawing calibrated scales with tick marks on (non-orthogonal) \n             variable vectors in scatterplots and biplots. Also provides some functions for biplot creation and\n\t     for multivariate analysis such as principal coordinate analysis.",
    "version": "1.7.7",
    "maintainer": "Jan Graffelman <jan.graffelman@upc.edu>",
    "author": "Jan Graffelman <jan.graffelman@upc.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=calibrate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "calibrate Calibration of Scatterplot and Biplot Axes Package for drawing calibrated scales with tick marks on (non-orthogonal) \n             variable vectors in scatterplots and biplots. Also provides some functions for biplot creation and\n\t     for multivariate analysis such as principal coordinate analysis.  "
  },
  {
    "id": 9580,
    "package_name": "candisc",
    "title": "Visualizing Generalized Canonical Discriminant and Canonical\nCorrelation Analysis",
    "description": "Functions for computing and visualizing \n\tgeneralized canonical discriminant analyses and canonical correlation analysis\n\tfor a multivariate linear model.\n\tTraditional canonical discriminant analysis is restricted to a one-way 'MANOVA'\n\tdesign and is equivalent to canonical correlation analysis between a set of quantitative\n\tresponse variables and a set of dummy variables coded from the factor variable.\n\tThe 'candisc' package generalizes this to higher-way 'MANOVA' designs\n\tfor all factors in a multivariate linear model,\n\tcomputing canonical scores and vectors for each term. The graphic functions provide low-rank (1D, 2D, 3D) \n\tvisualizations of terms in an 'mlm' via the 'plot.candisc' and 'heplot.candisc' methods. Related plots are\n\tnow provided for canonical correlation analysis when all predictors are quantitative. Methods for\n\tlinear discriminant analysis are now included.",
    "version": "1.1.0",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>),\n  John Fox [aut] (ORCID: <https://orcid.org/0000-0002-1196-8012>)",
    "url": "https://github.com/friendly/candisc/,\nhttps://friendly.github.io/candisc/",
    "bug_reports": "https://github.com/friendly/candisc/issues",
    "repository": "https://cran.r-project.org/package=candisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "candisc Visualizing Generalized Canonical Discriminant and Canonical\nCorrelation Analysis Functions for computing and visualizing \n\tgeneralized canonical discriminant analyses and canonical correlation analysis\n\tfor a multivariate linear model.\n\tTraditional canonical discriminant analysis is restricted to a one-way 'MANOVA'\n\tdesign and is equivalent to canonical correlation analysis between a set of quantitative\n\tresponse variables and a set of dummy variables coded from the factor variable.\n\tThe 'candisc' package generalizes this to higher-way 'MANOVA' designs\n\tfor all factors in a multivariate linear model,\n\tcomputing canonical scores and vectors for each term. The graphic functions provide low-rank (1D, 2D, 3D) \n\tvisualizations of terms in an 'mlm' via the 'plot.candisc' and 'heplot.candisc' methods. Related plots are\n\tnow provided for canonical correlation analysis when all predictors are quantitative. Methods for\n\tlinear discriminant analysis are now included.  "
  },
  {
    "id": 9582,
    "package_name": "cansim",
    "title": "Accessing Statistics Canada Data Table and Vectors",
    "description": "Searches for, accesses, and retrieves Statistics Canada data \n    tables, as well as individual vectors, as tidy data frames.\n    This package enriches the tables with metadata, deals\n    with encoding issues, allows for bilingual English or French language data retrieval, and bundles\n    convenience functions to make it easier to work with retrieved table data. For more efficient data\n    access the package allows for caching data in a local database and database level filtering, data\n    manipulation and summarizing.",
    "version": "0.4.4",
    "maintainer": "Jens von Bergmann <jens@mountainmath.ca>",
    "author": "Jens von Bergmann [aut, cre],\n  Dmitry Shkolnik [aut]",
    "url": "https://github.com/mountainMath/cansim,\nhttps://mountainmath.github.io/cansim/,\nhttps://www.statcan.gc.ca/",
    "bug_reports": "https://github.com/mountainMath/cansim/issues",
    "repository": "https://cran.r-project.org/package=cansim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cansim Accessing Statistics Canada Data Table and Vectors Searches for, accesses, and retrieves Statistics Canada data \n    tables, as well as individual vectors, as tidy data frames.\n    This package enriches the tables with metadata, deals\n    with encoding issues, allows for bilingual English or French language data retrieval, and bundles\n    convenience functions to make it easier to work with retrieved table data. For more efficient data\n    access the package allows for caching data in a local database and database level filtering, data\n    manipulation and summarizing.  "
  },
  {
    "id": 9619,
    "package_name": "caroline",
    "title": "A Collection of Database, Data Structure, Visualization, and\nUtility Functions for R",
    "description": "The caroline R library contains dozens of functions useful\n for: database migration (dbWriteTable2), database style joins &\n aggregation (nerge, groupBy, & bestBy), data structure\n conversion (nv, tab2df), legend table making (sstable & leghead),\n automatic legend positioning for scatter and box plots (),  \n plot annotation (labsegs & mvlabs), data visualization \n (pies, sparge, confound.grid & raPlot), character string manipulation (m & pad),\n file I/O (write.delim), batch scripting, data exploration, and more.\n The package's greatest contributions lie in the database style merge, \n aggregation and interface functions as well as in it's extensive\n use and propagation of row, column and vector names in most functions.",
    "version": "0.9.9",
    "maintainer": "David Schruth <code@anthropoidea.org>",
    "author": "David Schruth [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=caroline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "caroline A Collection of Database, Data Structure, Visualization, and\nUtility Functions for R The caroline R library contains dozens of functions useful\n for: database migration (dbWriteTable2), database style joins &\n aggregation (nerge, groupBy, & bestBy), data structure\n conversion (nv, tab2df), legend table making (sstable & leghead),\n automatic legend positioning for scatter and box plots (),  \n plot annotation (labsegs & mvlabs), data visualization \n (pies, sparge, confound.grid & raPlot), character string manipulation (m & pad),\n file I/O (write.delim), batch scripting, data exploration, and more.\n The package's greatest contributions lie in the database style merge, \n aggregation and interface functions as well as in it's extensive\n use and propagation of row, column and vector names in most functions.  "
  },
  {
    "id": 9647,
    "package_name": "catch",
    "title": "Covariate-Adjusted Tensor Classification in High-Dimensions",
    "description": "Performs classification and variable selection on high-dimensional tensors (multi-dimensional arrays) after adjusting for additional covariates (scalar or vectors) as CATCH model in Pan, Mai and Zhang (2018) <arXiv:1805.04421>. The low-dimensional covariates and the high-dimensional tensors are jointly modeled to predict a categorical outcome in a multi-class discriminant analysis setting. The Covariate-Adjusted Tensor Classification in High-dimensions (CATCH) model is fitted in two steps: (1) adjust for the covariates within each class; and (2) penalized estimation with the adjusted tensor using a cyclic block coordinate descent algorithm. The package can provide a solution path for tuning parameter in the penalized estimation step. Special case of the CATCH model includes linear discriminant analysis model and matrix (or tensor) discriminant analysis without covariates.",
    "version": "1.0.1",
    "maintainer": "Yuqing Pan <yuqing.pan@stat.fsu.edu>",
    "author": "Yuqing Pan <yuqing.pan@stat.fsu.edu>,\n\tQing Mai <mai@stat.fsu.edu>,\n\tXin Zhang <henry@stat.fsu.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catch Covariate-Adjusted Tensor Classification in High-Dimensions Performs classification and variable selection on high-dimensional tensors (multi-dimensional arrays) after adjusting for additional covariates (scalar or vectors) as CATCH model in Pan, Mai and Zhang (2018) <arXiv:1805.04421>. The low-dimensional covariates and the high-dimensional tensors are jointly modeled to predict a categorical outcome in a multi-class discriminant analysis setting. The Covariate-Adjusted Tensor Classification in High-dimensions (CATCH) model is fitted in two steps: (1) adjust for the covariates within each class; and (2) penalized estimation with the adjusted tensor using a cyclic block coordinate descent algorithm. The package can provide a solution path for tuning parameter in the penalized estimation step. Special case of the CATCH model includes linear discriminant analysis model and matrix (or tensor) discriminant analysis without covariates.  "
  },
  {
    "id": 9648,
    "package_name": "catcont",
    "title": "Test, Identify, Select and Mutate Categorical or Continuous\nValues",
    "description": "Methods and utilities for testing, identifying, selecting and  \n    mutating objects as categorical or continous types. These functions work on both \n    atomic vectors as well as recursive objects: data.frames, data.tables, \n    tibbles, lists, etc.. ",
    "version": "0.5.0",
    "maintainer": "Christopher Brown <chris.brown@decisionpatterns.com>",
    "author": "Christopher Brown [aut, cre],\n  Decision Patterns [cph]",
    "url": "https://github.com/decisionpatterns/catcont\nhttp://www.decisionpatterns.com",
    "bug_reports": "https://github.com/decisionpatterns/catcont/issues",
    "repository": "https://cran.r-project.org/package=catcont",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catcont Test, Identify, Select and Mutate Categorical or Continuous\nValues Methods and utilities for testing, identifying, selecting and  \n    mutating objects as categorical or continous types. These functions work on both \n    atomic vectors as well as recursive objects: data.frames, data.tables, \n    tibbles, lists, etc..   "
  },
  {
    "id": 9660,
    "package_name": "catseyes",
    "title": "Create Catseye Plots Illustrating the Normal Distribution of the\nMeans",
    "description": "Provides the tools to produce catseye plots, principally\n    by catseyesplot() function which calls R's standard plot() function internally, or alternatively\n    by the catseyes() function to overlay the catseye plot onto an existing\n    R plot window. Catseye plots illustrate the normal distribution of the mean (picture a \n    normal bell curve reflected over its base and rotated 90 degrees), with a shaded confidence\n    interval; they are an intuitive way of illustrating and comparing normally distributed estimates,\n    and are arguably a superior alternative to standard confidence intervals, since they show the full\n    distribution rather than fixed quantile bounds. The catseyesplot and catseyes functions require\n    pre-calculated means and standard errors (or standard deviations), provided as numeric vectors;\n    this allows the flexibility of obtaining this information from a variety of sources, such as\n    direct calculation or prediction from a model.  Catseye plots, as illustrations of the\n    normal distribution of the means, are described in Cumming (2013 & 2014).\n    Cumming, G. (2013). The new statistics: Why and how. Psychological Science, 27, 7-29. <doi:10.1177/0956797613504966> pmid:24220629.",
    "version": "0.2.5",
    "maintainer": "Clark Andersen <crandersen@mdanderson.org>",
    "author": "Clark Andersen [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catseyes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catseyes Create Catseye Plots Illustrating the Normal Distribution of the\nMeans Provides the tools to produce catseye plots, principally\n    by catseyesplot() function which calls R's standard plot() function internally, or alternatively\n    by the catseyes() function to overlay the catseye plot onto an existing\n    R plot window. Catseye plots illustrate the normal distribution of the mean (picture a \n    normal bell curve reflected over its base and rotated 90 degrees), with a shaded confidence\n    interval; they are an intuitive way of illustrating and comparing normally distributed estimates,\n    and are arguably a superior alternative to standard confidence intervals, since they show the full\n    distribution rather than fixed quantile bounds. The catseyesplot and catseyes functions require\n    pre-calculated means and standard errors (or standard deviations), provided as numeric vectors;\n    this allows the flexibility of obtaining this information from a variety of sources, such as\n    direct calculation or prediction from a model.  Catseye plots, as illustrations of the\n    normal distribution of the means, are described in Cumming (2013 & 2014).\n    Cumming, G. (2013). The new statistics: Why and how. Psychological Science, 27, 7-29. <doi:10.1177/0956797613504966> pmid:24220629.  "
  },
  {
    "id": 9719,
    "package_name": "ccss",
    "title": "Cluster Circular Systematic Sampling",
    "description": "Draws systematic samples from a population that follows linear trend. The function returns a matrix comprising of the required samples as its column vectors. The samples produced are highly efficient and the inter sampling variance is minimum. The scheme will be useful in various field like Bioinformatics where the samples are expensive and must be precise in reflecting the population by possessing least sampling variance.",
    "version": "1.0",
    "maintainer": "Abhibhav Sharma <mail.abhibhav@gmail.com>",
    "author": "Abhibhav Sharma",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ccss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ccss Cluster Circular Systematic Sampling Draws systematic samples from a population that follows linear trend. The function returns a matrix comprising of the required samples as its column vectors. The samples produced are highly efficient and the inter sampling variance is minimum. The scheme will be useful in various field like Bioinformatics where the samples are expensive and must be precise in reflecting the population by possessing least sampling variance.  "
  },
  {
    "id": 9723,
    "package_name": "cdata",
    "title": "Fluid Data Transformations",
    "description": "Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. \n    The methodology is describe in 'Zumel', 2018, \"Fluid data reshaping with 'cdata'\", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .\n    This package introduces the idea of explicit control table specification of data transforms.\n    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.",
    "version": "1.2.1",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Nina Zumel [aut],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/cdata/,\nhttps://winvector.github.io/cdata/",
    "bug_reports": "https://github.com/WinVector/cdata/issues",
    "repository": "https://cran.r-project.org/package=cdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdata Fluid Data Transformations Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. \n    The methodology is describe in 'Zumel', 2018, \"Fluid data reshaping with 'cdata'\", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .\n    This package introduces the idea of explicit control table specification of data transforms.\n    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.  "
  },
  {
    "id": 9753,
    "package_name": "cencrne",
    "title": "Consistent Estimation of the Number of Communities via\nRegularized Network Embedding",
    "description": "The network analysis plays an important role in numerous application domains including biomedicine. \n             Estimation of the number of communities is a fundamental and critical issue in network analysis. Most existing studies assume that the number of communities is known a priori, or lack of rigorous theoretical guarantee on the estimation consistency. This method proposes a regularized network embedding model to simultaneously estimate the community structure and the number of communities in a unified formulation. \n\t           The proposed model equips network embedding with a novel composite regularization term, which pushes the embedding vector towards its center and collapses similar community centers with each other. A rigorous theoretical analysis is conducted, establishing asymptotic consistency in terms of community detection and estimation of the number of communities. \n\t           Reference: \n             Ren, M., Zhang S. and Wang J. (2022). \"Consistent Estimation of the Number of Communities via Regularized Network Embedding\". Biometrics, <doi:10.1111/biom.13815>.",
    "version": "1.0.0",
    "maintainer": "Mingyang Ren <renmingyang17@mails.ucas.ac.cn>",
    "author": "Mingyang Ren [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8061-9940>),\n  Sanguo Zhang [aut],\n  Junhui Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cencrne",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cencrne Consistent Estimation of the Number of Communities via\nRegularized Network Embedding The network analysis plays an important role in numerous application domains including biomedicine. \n             Estimation of the number of communities is a fundamental and critical issue in network analysis. Most existing studies assume that the number of communities is known a priori, or lack of rigorous theoretical guarantee on the estimation consistency. This method proposes a regularized network embedding model to simultaneously estimate the community structure and the number of communities in a unified formulation. \n\t           The proposed model equips network embedding with a novel composite regularization term, which pushes the embedding vector towards its center and collapses similar community centers with each other. A rigorous theoretical analysis is conducted, establishing asymptotic consistency in terms of community detection and estimation of the number of communities. \n\t           Reference: \n             Ren, M., Zhang S. and Wang J. (2022). \"Consistent Estimation of the Number of Communities via Regularized Network Embedding\". Biometrics, <doi:10.1111/biom.13815>.  "
  },
  {
    "id": 9762,
    "package_name": "centerline",
    "title": "Extract Centerline from Closed Polygons",
    "description": "Generates skeletons of closed 2D polygons using Voronoi diagrams. \n            It provides methods for 'sf', 'terra', and 'geos' objects to \n            compute polygon centerlines based on the generated skeletons.\n            Voronoi, G. (1908) <doi:10.1515/crll.1908.134.198>.",
    "version": "0.2.4",
    "maintainer": "Anatoly Tsyplenkov <atsyplenkov@fastmail.com>",
    "author": "Anatoly Tsyplenkov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4144-8402>)",
    "url": "https://centerline.anatolii.nz,\nhttps://github.com/atsyplenkov/centerline",
    "bug_reports": "https://github.com/atsyplenkov/centerline/issues",
    "repository": "https://cran.r-project.org/package=centerline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "centerline Extract Centerline from Closed Polygons Generates skeletons of closed 2D polygons using Voronoi diagrams. \n            It provides methods for 'sf', 'terra', and 'geos' objects to \n            compute polygon centerlines based on the generated skeletons.\n            Voronoi, G. (1908) <doi:10.1515/crll.1908.134.198>.  "
  },
  {
    "id": 9772,
    "package_name": "ceramic",
    "title": "Download Online Imagery Tiles",
    "description": "Download imagery tiles to a standard cache and load the data into raster objects. \n Facilities for 'AWS' terrain <https://registry.opendata.aws/terrain-tiles/> terrain and 'Mapbox' \n <https://www.mapbox.com/> servers are provided. ",
    "version": "0.9.5",
    "maintainer": "Michael Sumner <mdsumner@gmail.com>",
    "author": "Michael Sumner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2471-7511>),\n  Miles McBain [ctb] (ORCID: <https://orcid.org/0000-0003-2865-2548>),\n  Ben Raymond [ctb] (regex wizardry)",
    "url": "https://hypertidy.github.io/ceramic/",
    "bug_reports": "https://github.com/hypertidy/ceramic/issues",
    "repository": "https://cran.r-project.org/package=ceramic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ceramic Download Online Imagery Tiles Download imagery tiles to a standard cache and load the data into raster objects. \n Facilities for 'AWS' terrain <https://registry.opendata.aws/terrain-tiles/> terrain and 'Mapbox' \n <https://www.mapbox.com/> servers are provided.   "
  },
  {
    "id": 9782,
    "package_name": "cffdrs",
    "title": "Canadian Forest Fire Danger Rating System",
    "description": "This project provides a group of new functions to calculate\n    the outputs of the two main components of the Canadian Forest Fire\n    Danger Rating System (CFFDRS) Van Wagner and Pickett (1985)\n    <https://ostrnrcan-dostrncan.canada.ca/entities/publication/29706108-2891-4e5d-a59a-a77c96bc507c>) at various time\n    scales: the Fire Weather Index (FWI) System Wan Wagner (1985)\n    <https://ostrnrcan-dostrncan.canada.ca/entities/publication/d96e56aa-e836-4394-ba29-3afe91c3aa6c> and the Fire Behaviour\n    Prediction (FBP) System Forestry Canada Fire Danger Group (1992)\n    <https://cfs.nrcan.gc.ca/pubwarehouse/pdfs/10068.pdf>. Some functions\n    have two versions, table and raster based.",
    "version": "1.9.2",
    "maintainer": "Brett Moore <Brett.Moore@nrcan-rncan.gc.ca>",
    "author": "Xianli Wang [aut],\n  Alan Cantin [aut],\n  Marc-Andr\u00e9 Parisien [aut],\n  Mike Wotton [aut],\n  Kerry Anderson [aut],\n  Brett Moore [cre, aut],\n  Tom Schiks [aut],\n  Mike Flannigan [aut]",
    "url": "https://github.com/cffdrs/cffdrs_r",
    "bug_reports": "https://github.com/cffdrs/cffdrs_r/issues/new/choose",
    "repository": "https://cran.r-project.org/package=cffdrs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cffdrs Canadian Forest Fire Danger Rating System This project provides a group of new functions to calculate\n    the outputs of the two main components of the Canadian Forest Fire\n    Danger Rating System (CFFDRS) Van Wagner and Pickett (1985)\n    <https://ostrnrcan-dostrncan.canada.ca/entities/publication/29706108-2891-4e5d-a59a-a77c96bc507c>) at various time\n    scales: the Fire Weather Index (FWI) System Wan Wagner (1985)\n    <https://ostrnrcan-dostrncan.canada.ca/entities/publication/d96e56aa-e836-4394-ba29-3afe91c3aa6c> and the Fire Behaviour\n    Prediction (FBP) System Forestry Canada Fire Danger Group (1992)\n    <https://cfs.nrcan.gc.ca/pubwarehouse/pdfs/10068.pdf>. Some functions\n    have two versions, table and raster based.  "
  },
  {
    "id": 9808,
    "package_name": "changepoints",
    "title": "A Collection of Change-Point Detection Methods",
    "description": "Performs a series of offline and/or online change-point detection algorithms for 1) univariate mean: <doi:10.1214/20-EJS1710>, <arXiv:2006.03283>; 2) univariate polynomials: <doi:10.1214/21-EJS1963>; 3) univariate and multivariate nonparametric settings: <doi:10.1214/21-EJS1809>, <doi:10.1109/TIT.2021.3130330>; 4) high-dimensional covariances: <doi:10.3150/20-BEJ1249>; 5) high-dimensional networks with and without missing values: <doi:10.1214/20-AOS1953>, <arXiv:2101.05477>, <arXiv:2110.06450>; 6) high-dimensional linear regression models: <arXiv:2010.10410>, <arXiv:2207.12453>; 7) high-dimensional vector autoregressive models: <arXiv:1909.06359>; 8) high-dimensional self exciting point processes: <arXiv:2006.03572>; 9) dependent dynamic nonparametric random dot product graphs: <arXiv:1911.07494>; 10) univariate mean against adversarial attacks: <arXiv:2105.10417>.",
    "version": "1.1.0",
    "maintainer": "Haotian Xu <haotian.xu@uclouvain.be>",
    "author": "Haotian Xu [aut, cre],\n  Oscar Padilla [aut],\n  Daren Wang [aut],\n  Mengchu Li [aut],\n  Qin Wen [ctb]",
    "url": "https://github.com/HaotianXu/changepoints",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=changepoints",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "changepoints A Collection of Change-Point Detection Methods Performs a series of offline and/or online change-point detection algorithms for 1) univariate mean: <doi:10.1214/20-EJS1710>, <arXiv:2006.03283>; 2) univariate polynomials: <doi:10.1214/21-EJS1963>; 3) univariate and multivariate nonparametric settings: <doi:10.1214/21-EJS1809>, <doi:10.1109/TIT.2021.3130330>; 4) high-dimensional covariances: <doi:10.3150/20-BEJ1249>; 5) high-dimensional networks with and without missing values: <doi:10.1214/20-AOS1953>, <arXiv:2101.05477>, <arXiv:2110.06450>; 6) high-dimensional linear regression models: <arXiv:2010.10410>, <arXiv:2207.12453>; 7) high-dimensional vector autoregressive models: <arXiv:1909.06359>; 8) high-dimensional self exciting point processes: <arXiv:2006.03572>; 9) dependent dynamic nonparametric random dot product graphs: <arXiv:1911.07494>; 10) univariate mean against adversarial attacks: <arXiv:2105.10417>.  "
  },
  {
    "id": 9813,
    "package_name": "charcuterie",
    "title": "Handle Strings as Vectors of Characters",
    "description": "Creates a new chars class which looks like a string but is actually \n    a vector of individual characters, making 'strings' iterable. This class \n    enables vector operations on 'strings' such as reverse, sort, head, and set \n    operations.",
    "version": "0.0.6",
    "maintainer": "Jonathan Carroll <rpkg@jcarroll.com.au>",
    "author": "Jonathan Carroll [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1404-5264>)",
    "url": "https://github.com/jonocarroll/charcuterie,\nhttps://jonocarroll.github.io/charcuterie/",
    "bug_reports": "https://github.com/jonocarroll/charcuterie/issues",
    "repository": "https://cran.r-project.org/package=charcuterie",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "charcuterie Handle Strings as Vectors of Characters Creates a new chars class which looks like a string but is actually \n    a vector of individual characters, making 'strings' iterable. This class \n    enables vector operations on 'strings' such as reverse, sort, head, and set \n    operations.  "
  },
  {
    "id": 9849,
    "package_name": "chessboard",
    "title": "Create Network Connections Based on Chess Moves",
    "description": "Provides functions to work with directed (asymmetric) and \n    undirected (symmetric) spatial networks. It makes the creation of \n    connectivity matrices easier, i.e. a binary matrix of dimension n x n, where \n    n is the number of nodes (sampling units) indicating the presence (1) or  \n    the absence (0) of an edge (link) between pairs of nodes. Different network\n    objects can be produced by 'chessboard': node list, neighbor list, edge \n    list, connectivity matrix. It can also produce objects that will be used \n    later in Moran's Eigenvector Maps (Dray et al. (2006) <doi:10.1016/j.ecolmodel.2006.02.015>)\n    and Asymetric Eigenvector Maps (Blanchet et al. (2008) <doi:10.1016/j.ecolmodel.2008.04.001>), \n    methods available in the package 'adespatial' (Dray et al. (2023) \n    <https://CRAN.R-project.org/package=adespatial>). This work is part of the \n    FRB-CESAB working group Bridge \n    <https://www.fondationbiodiversite.fr/en/the-frb-in-action/programs-and-projects/le-cesab/bridge/>.",
    "version": "0.1",
    "maintainer": "Nicolas Casajus <nicolas.casajus@fondationbiodiversite.fr>",
    "author": "Nicolas Casajus [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5537-5294>),\n  Erica Rievrs Borges [aut] (ORCID:\n    <https://orcid.org/0000-0001-7751-6265>),\n  Eric Tabacchi [aut] (ORCID: <https://orcid.org/0000-0001-7729-4439>),\n  Guillaume Fried [aut] (ORCID: <https://orcid.org/0000-0002-3653-195X>),\n  Nicolas Mouquet [aut] (ORCID: <https://orcid.org/0000-0003-1840-6984>)",
    "url": "https://github.com/frbcesab/chessboard",
    "bug_reports": "https://github.com/frbcesab/chessboard/issues",
    "repository": "https://cran.r-project.org/package=chessboard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chessboard Create Network Connections Based on Chess Moves Provides functions to work with directed (asymmetric) and \n    undirected (symmetric) spatial networks. It makes the creation of \n    connectivity matrices easier, i.e. a binary matrix of dimension n x n, where \n    n is the number of nodes (sampling units) indicating the presence (1) or  \n    the absence (0) of an edge (link) between pairs of nodes. Different network\n    objects can be produced by 'chessboard': node list, neighbor list, edge \n    list, connectivity matrix. It can also produce objects that will be used \n    later in Moran's Eigenvector Maps (Dray et al. (2006) <doi:10.1016/j.ecolmodel.2006.02.015>)\n    and Asymetric Eigenvector Maps (Blanchet et al. (2008) <doi:10.1016/j.ecolmodel.2008.04.001>), \n    methods available in the package 'adespatial' (Dray et al. (2023) \n    <https://CRAN.R-project.org/package=adespatial>). This work is part of the \n    FRB-CESAB working group Bridge \n    <https://www.fondationbiodiversite.fr/en/the-frb-in-action/programs-and-projects/le-cesab/bridge/>.  "
  },
  {
    "id": 9899,
    "package_name": "cif",
    "title": "Cointegrated ICU Forecasting",
    "description": "Set of forecasting tools to predict ICU beds using a Vector Error Correction model with a single cointegrating vector. Method described in  Berta, P. Lovaglio, P.G. Paruolo, P. Verzillo, S., 2020. \"Real Time Forecasting of Covid-19 Intensive Care Units demand\" Health, Econometrics and Data Group (HEDG) Working Papers 20/16, HEDG, Department of Economics, University of York, <https://www.york.ac.uk/media/economics/documents/hedg/workingpapers/2020/2016.pdf>. ",
    "version": "0.1.1",
    "maintainer": "Paolo Paruolo <Paolo.PARUOLO@ec.europa.eu>",
    "author": "Paolo Berta [aut],\n  Paolo Paruolo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3982-4889>),\n  Stefano Verzillo [ctb],\n  Pietro Giorgio Lovaglio [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cif Cointegrated ICU Forecasting Set of forecasting tools to predict ICU beds using a Vector Error Correction model with a single cointegrating vector. Method described in  Berta, P. Lovaglio, P.G. Paruolo, P. Verzillo, S., 2020. \"Real Time Forecasting of Covid-19 Intensive Care Units demand\" Health, Econometrics and Data Group (HEDG) Working Papers 20/16, HEDG, Department of Economics, University of York, <https://www.york.ac.uk/media/economics/documents/hedg/workingpapers/2020/2016.pdf>.   "
  },
  {
    "id": 9929,
    "package_name": "ciuupi",
    "title": "Confidence Intervals Utilizing Uncertain Prior Information",
    "description": "Computes a confidence interval for a specified linear combination of the \n    regression parameters in a linear regression model with iid normal errors\n    with known variance when there is uncertain prior information that a distinct\n    specified linear combination of the regression parameters takes a given \n    value.  This confidence interval, found by numerical nonlinear constrained \n    optimization, has the required minimum coverage and utilizes this uncertain \n    prior information through desirable expected length properties.\n    This confidence interval has the following three practical applications. \n    Firstly, if the error variance has been accurately estimated from previous \n    data then it may be treated as being effectively known. Secondly, for \n    sufficiently large (dimension of the response vector) minus (dimension of \n    regression parameter vector), greater than or equal to 30 (say),\n    if we replace the assumed known value of the error variance by its usual \n    estimator in the formula for the confidence interval then the resulting \n    interval has, to a very good approximation, the same coverage probability \n    and expected length properties as when the error variance is known. Thirdly,\n    some more complicated models can be approximated by the linear regression \n    model with error variance known when certain unknown parameters are replaced \n    by estimates. This confidence interval is described in \n    Mainzer, R. and Kabaila, P. \n    (2019) <doi:10.32614/RJ-2019-026>, and is a member of the family of \n    confidence intervals proposed by Kabaila, P. and Giri, K. (2009) \n    <doi:10.1016/j.jspi.2009.03.018>.",
    "version": "1.2.3",
    "maintainer": "Paul Kabaila <P.Kabaila@latrobe.edu.au>",
    "author": "Paul Kabaila [aut, cre],\n  Rheanna Mainzer [aut],\n  Ayesha Perera [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ciuupi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ciuupi Confidence Intervals Utilizing Uncertain Prior Information Computes a confidence interval for a specified linear combination of the \n    regression parameters in a linear regression model with iid normal errors\n    with known variance when there is uncertain prior information that a distinct\n    specified linear combination of the regression parameters takes a given \n    value.  This confidence interval, found by numerical nonlinear constrained \n    optimization, has the required minimum coverage and utilizes this uncertain \n    prior information through desirable expected length properties.\n    This confidence interval has the following three practical applications. \n    Firstly, if the error variance has been accurately estimated from previous \n    data then it may be treated as being effectively known. Secondly, for \n    sufficiently large (dimension of the response vector) minus (dimension of \n    regression parameter vector), greater than or equal to 30 (say),\n    if we replace the assumed known value of the error variance by its usual \n    estimator in the formula for the confidence interval then the resulting \n    interval has, to a very good approximation, the same coverage probability \n    and expected length properties as when the error variance is known. Thirdly,\n    some more complicated models can be approximated by the linear regression \n    model with error variance known when certain unknown parameters are replaced \n    by estimates. This confidence interval is described in \n    Mainzer, R. and Kabaila, P. \n    (2019) <doi:10.32614/RJ-2019-026>, and is a member of the family of \n    confidence intervals proposed by Kabaila, P. and Giri, K. (2009) \n    <doi:10.1016/j.jspi.2009.03.018>.  "
  },
  {
    "id": 9946,
    "package_name": "class",
    "title": "Functions for Classification",
    "description": "Various functions for classification, including k-nearest\n  neighbour, Learning Vector Quantization and Self-Organizing Maps.",
    "version": "7.3-23",
    "maintainer": "Brian Ripley <Brian.Ripley@R-project.org>",
    "author": "Brian Ripley [aut, cre, cph],\n  William Venables [cph]",
    "url": "http://www.stats.ox.ac.uk/pub/MASS4/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=class",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "class Functions for Classification Various functions for classification, including k-nearest\n  neighbour, Learning Vector Quantization and Self-Organizing Maps.  "
  },
  {
    "id": 9950,
    "package_name": "classmap",
    "title": "Visualizing Classification Results",
    "description": "Tools to visualize the results of a classification or a regression.\n    The graphical displays include stacked plots, silhouette plots, quasi residual plots, class maps, predictions plots, and predictions correlation plots.\n    Implements the techniques described and illustrated in Raymaekers J., Rousseeuw P.J., Hubert M. (2022). Class maps for visualizing classification results. \\emph{Technometrics}, 64(2), 151\u2013165. \\doi{10.1080/00401706.2021.1927849}\n (open access), Raymaekers J., Rousseeuw P.J.(2022). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. \\emph{Journal of Computational and Graphical Statistics}, 31(4), 1332\u20131343. \\doi{10.1080/10618600.2022.2050249}, and Rousseeuw, P.J. (2025). Explainable Linear and Generalized Linear Models by the Predictions Plot. <doi:10.48550/arXiv.2412.16980> (open access).\n Examples can be found in the vignettes:\n    \"Discriminant_analysis_examples\",\"K_nearest_neighbors_examples\",\n    \"Support_vector_machine_examples\", \"Rpart_examples\", \"Random_forest_examples\",\n    \"Neural_net_examples\", and \"predsplot_examples\".",
    "version": "1.2.6",
    "maintainer": "Jakob Raymaekers <jakob.raymaekers@kuleuven.be>",
    "author": "Jakob Raymaekers [aut, cre],\n  Peter Rousseeuw [aut]",
    "url": "https://doi.org/10.1080/00401706.2021.1927849,\nhttps://doi.org/10.1080/10618600.2022.2050249,\nhttps://arxiv.org/abs/2412.16980",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=classmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "classmap Visualizing Classification Results Tools to visualize the results of a classification or a regression.\n    The graphical displays include stacked plots, silhouette plots, quasi residual plots, class maps, predictions plots, and predictions correlation plots.\n    Implements the techniques described and illustrated in Raymaekers J., Rousseeuw P.J., Hubert M. (2022). Class maps for visualizing classification results. \\emph{Technometrics}, 64(2), 151\u2013165. \\doi{10.1080/00401706.2021.1927849}\n (open access), Raymaekers J., Rousseeuw P.J.(2022). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. \\emph{Journal of Computational and Graphical Statistics}, 31(4), 1332\u20131343. \\doi{10.1080/10618600.2022.2050249}, and Rousseeuw, P.J. (2025). Explainable Linear and Generalized Linear Models by the Predictions Plot. <doi:10.48550/arXiv.2412.16980> (open access).\n Examples can be found in the vignettes:\n    \"Discriminant_analysis_examples\",\"K_nearest_neighbors_examples\",\n    \"Support_vector_machine_examples\", \"Rpart_examples\", \"Random_forest_examples\",\n    \"Neural_net_examples\", and \"predsplot_examples\".  "
  },
  {
    "id": 9977,
    "package_name": "climateStability",
    "title": "Estimating Climate Stability from Climate Model Data",
    "description": "Climate stability measures are not formalized in the literature and\n  tools for generating stability metrics from existing data are nascent.\n  This package provides tools for calculating climate stability from raster data\n  encapsulating climate change as a series of time slices. The methods follow\n  Owens and Guralnick <doi:10.17161/bi.v14i0.9786> Biodiversity Informatics.",
    "version": "0.1.4",
    "maintainer": "Hannah Owens <hannah.owens@gmail.com>",
    "author": "Hannah Owens [aut, cre]",
    "url": "https://github.com/hannahlowens/climateStability",
    "bug_reports": "https://github.com/hannahlowens/climateStability/issues",
    "repository": "https://cran.r-project.org/package=climateStability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "climateStability Estimating Climate Stability from Climate Model Data Climate stability measures are not formalized in the literature and\n  tools for generating stability metrics from existing data are nascent.\n  This package provides tools for calculating climate stability from raster data\n  encapsulating climate change as a series of time slices. The methods follow\n  Owens and Guralnick <doi:10.17161/bi.v14i0.9786> Biodiversity Informatics.  "
  },
  {
    "id": 10016,
    "package_name": "clptheory",
    "title": "Compute Price of Production and Labor Values",
    "description": "Computes the uniform rate of profit, the vector of price of \n    production and the vector of labor values; and also compute measures of deviation \n    between relative prices of production and relative values.  \n    <https://scholarworks.umass.edu/econ_workingpaper/347/>. You provide the  \n    input-output data and 'clptheory' does the calculations for you.",
    "version": "0.1.0",
    "maintainer": "Deepankar Basu <dbasu@umass.edu>",
    "author": "Deepankar Basu [aut, cre, cph]",
    "url": "https://github.com/dbasu-umass/clptheory/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clptheory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clptheory Compute Price of Production and Labor Values Computes the uniform rate of profit, the vector of price of \n    production and the vector of labor values; and also compute measures of deviation \n    between relative prices of production and relative values.  \n    <https://scholarworks.umass.edu/econ_workingpaper/347/>. You provide the  \n    input-output data and 'clptheory' does the calculations for you.  "
  },
  {
    "id": 10059,
    "package_name": "clustringr",
    "title": "Cluster Strings by Edit-Distance",
    "description": "Returns an edit-distance based clusterization of an input vector of strings.\n    Each cluster will contain a set of strings w/ small mutual edit-distance\n    (e.g., Levenshtein, optimum-sequence-alignment, Damerau-Levenshtein), as computed by\n    stringdist::stringdist(). The set of all mutual edit-distances is then used by\n    graph algorithms (from package 'igraph') to single out subsets of high connectivity.",
    "version": "1.0",
    "maintainer": "Dan S. Reznik <dreznik@gmail.com>",
    "author": "Dan S. Reznik",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clustringr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clustringr Cluster Strings by Edit-Distance Returns an edit-distance based clusterization of an input vector of strings.\n    Each cluster will contain a set of strings w/ small mutual edit-distance\n    (e.g., Levenshtein, optimum-sequence-alignment, Damerau-Levenshtein), as computed by\n    stringdist::stringdist(). The set of all mutual edit-distances is then used by\n    graph algorithms (from package 'igraph') to single out subsets of high connectivity.  "
  },
  {
    "id": 10079,
    "package_name": "cmocean",
    "title": "Beautiful Colour Maps for Oceanography",
    "description": "Perceptually uniform palettes for commonly used\n\tvariables in oceanography as functions taking an integer\n\tand producing character vectors of colours.\n\tSee Thyng, K.M., Greene, C.A., Hetland, R.D., Zimmerle, H.M.\n\tand S.F. DiMarco (2016) <doi:10.5670/oceanog.2016.66> for\n\tthe guidelines adhered to when creating the palettes.",
    "version": "0.3-2",
    "maintainer": "Ivan Krylov <ikrylov@disroot.org>",
    "author": "Kristen Thyng [aut] (ORCID: <https://orcid.org/0000-0002-8746-614X>),\n  Clark Richards [ctb] (ORCID: <https://orcid.org/0000-0002-7833-206X>),\n  Ilja Kocken [ctb] (ORCID: <https://orcid.org/0000-0003-2196-8718>),\n  Ivan Krylov [cre] (ORCID: <https://orcid.org/0000-0002-0172-3812>)",
    "url": "https://matplotlib.org/cmocean/",
    "bug_reports": "https://github.com/aitap/cmocean/issues",
    "repository": "https://cran.r-project.org/package=cmocean",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmocean Beautiful Colour Maps for Oceanography Perceptually uniform palettes for commonly used\n\tvariables in oceanography as functions taking an integer\n\tand producing character vectors of colours.\n\tSee Thyng, K.M., Greene, C.A., Hetland, R.D., Zimmerle, H.M.\n\tand S.F. DiMarco (2016) <doi:10.5670/oceanog.2016.66> for\n\tthe guidelines adhered to when creating the palettes.  "
  },
  {
    "id": 10130,
    "package_name": "codelist",
    "title": "Working with Code Lists",
    "description": "Functions for working with code lists and vectors with codes. These\n    are an alternative for factor that keep track of both the codes and labels.\n    Methods allow for transforming between codes and labels. Also supports\n    hierarchical code lists.",
    "version": "0.1.0",
    "maintainer": "Jan van der Laan <r@eoos.dds.nl>",
    "author": "Jan van der Laan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0693-1514>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=codelist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "codelist Working with Code Lists Functions for working with code lists and vectors with codes. These\n    are an alternative for factor that keep track of both the codes and labels.\n    Methods allow for transforming between codes and labels. Also supports\n    hierarchical code lists.  "
  },
  {
    "id": 10133,
    "package_name": "codep",
    "title": "Multiscale Codependence Analysis",
    "description": "Computation of Multiscale Codependence Analysis and spatial eigenvector maps.",
    "version": "1.2-4",
    "maintainer": "Guillaume Gu\u00e9nard <guillaume.guenard@umontreal.ca>",
    "author": "Guillaume Gu\u00e9nard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0761-3072>),\n  Pierre Legendre [ctb] (ORCID: <https://orcid.org/0000-0002-3838-3305>),\n  Bertrand Pages [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=codep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "codep Multiscale Codependence Analysis Computation of Multiscale Codependence Analysis and spatial eigenvector maps.  "
  },
  {
    "id": 10166,
    "package_name": "collapse",
    "title": "Advanced and Fast Data Transformation",
    "description": "A large C/C++-based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic, robust, and \n    programmer friendly. Core functionality includes a rich set of S3 generic grouped \n    and weighted statistical functions for vectors, matrices and data frames, which \n    provide efficient low-level vectorizations, OpenMP multithreading, and skip missing \n    values by default. These are integrated with fast grouping and ordering algorithms \n    (also callable from C), and efficient data manipulation functions. The package also \n    provides a flexible and rigorous approach to time series and panel data in R, fast \n    functions for data transformation and common statistical procedures, detailed \n    (grouped, weighted) summary statistics, powerful tools to work with nested data, \n    fast data object conversions, functions for memory efficient R programming, and \n    helpers to effectively deal with variable labels, attributes, and missing data. It \n    seamlessly supports base R objects/classes as well as 'units', 'integer64', 'xts'/\n    'zoo', 'tibble', 'grouped_df', 'data.table', 'sf', and 'pseries'/'pdata.frame'.",
    "version": "2.1.5",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6212-5229>),\n  Matt Dowle [ctb],\n  Arun Srinivasan [ctb],\n  Morgan Jacob [ctb],\n  Dirk Eddelbuettel [ctb],\n  Laurent Berge [ctb],\n  Kevin Tappe [ctb],\n  Alina Cherkas [ctb],\n  R Core Team and contributors worldwide [ctb],\n  Martyn Plummer [cph],\n  1999-2016 The R Core Team [cph]",
    "url": "https://sebkrantz.github.io/collapse/,\nhttps://github.com/SebKrantz/collapse",
    "bug_reports": "https://github.com/SebKrantz/collapse/issues",
    "repository": "https://cran.r-project.org/package=collapse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "collapse Advanced and Fast Data Transformation A large C/C++-based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic, robust, and \n    programmer friendly. Core functionality includes a rich set of S3 generic grouped \n    and weighted statistical functions for vectors, matrices and data frames, which \n    provide efficient low-level vectorizations, OpenMP multithreading, and skip missing \n    values by default. These are integrated with fast grouping and ordering algorithms \n    (also callable from C), and efficient data manipulation functions. The package also \n    provides a flexible and rigorous approach to time series and panel data in R, fast \n    functions for data transformation and common statistical procedures, detailed \n    (grouped, weighted) summary statistics, powerful tools to work with nested data, \n    fast data object conversions, functions for memory efficient R programming, and \n    helpers to effectively deal with variable labels, attributes, and missing data. It \n    seamlessly supports base R objects/classes as well as 'units', 'integer64', 'xts'/\n    'zoo', 'tibble', 'grouped_df', 'data.table', 'sf', and 'pseries'/'pdata.frame'.  "
  },
  {
    "id": 10191,
    "package_name": "colorfast",
    "title": "Fast Conversion of R Colors to Color Component Values and Native\nPacked Integer Format",
    "description": "Color values in R are often represented as strings of hexadecimal\n    colors or named colors.  This package offers fast conversion of \n    these color representations to either an array of red/green/blue/alpha values\n    or to the packed integer format used in native raster objects.  Functions\n    for conversion are also exported at the 'C' level for use in other packages.\n    This fast conversion\n    of colors is implemented using an order-preserving minimal perfect hash\n    derived from Majewski et al (1996) \"A Family of Perfect Hashing Methods\" \n    <doi:10.1093/comjnl/39.6.547>.",
    "version": "1.0.1",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph]",
    "url": "https://github.com/coolbutuseless/colorfast",
    "bug_reports": "https://github.com/coolbutuseless/colorfast/issues",
    "repository": "https://cran.r-project.org/package=colorfast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colorfast Fast Conversion of R Colors to Color Component Values and Native\nPacked Integer Format Color values in R are often represented as strings of hexadecimal\n    colors or named colors.  This package offers fast conversion of \n    these color representations to either an array of red/green/blue/alpha values\n    or to the packed integer format used in native raster objects.  Functions\n    for conversion are also exported at the 'C' level for use in other packages.\n    This fast conversion\n    of colors is implemented using an order-preserving minimal perfect hash\n    derived from Majewski et al (1996) \"A Family of Perfect Hashing Methods\" \n    <doi:10.1093/comjnl/39.6.547>.  "
  },
  {
    "id": 10195,
    "package_name": "colorist",
    "title": "Coloring Wildlife Distributions in Space-Time",
    "description": "Color and visualize wildlife distributions in\n    space-time using raster data. In addition to enabling display of\n    sequential change in distributions through the use of small multiples,\n    'colorist' provides functions for extracting several features of\n    interest from a sequence of distributions and for visualizing those\n    features using HCL (hue-chroma-luminance) color palettes. Resulting\n    maps allow for \"fair\" visual comparison of intensity values (e.g.,\n    occurrence, abundance, or density) across space and time and can be\n    used to address questions about where, when, and how consistently a\n    species, group, or individual is likely to be found.",
    "version": "0.1.3",
    "maintainer": "Matthew Strimas-Mackey <mes335@cornell.edu>",
    "author": "Justin Schuetz [aut] (ORCID: <https://orcid.org/0000-0002-6163-538X>),\n  Matthew Strimas-Mackey [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8929-7776>),\n  Tom Auer [aut] (ORCID: <https://orcid.org/0000-0001-8619-7147>)",
    "url": "https://github.com/mstrimas/colorist",
    "bug_reports": "https://github.com/mstrimas/colorist/issues",
    "repository": "https://cran.r-project.org/package=colorist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colorist Coloring Wildlife Distributions in Space-Time Color and visualize wildlife distributions in\n    space-time using raster data. In addition to enabling display of\n    sequential change in distributions through the use of small multiples,\n    'colorist' provides functions for extracting several features of\n    interest from a sequence of distributions and for visualizing those\n    features using HCL (hue-chroma-luminance) color palettes. Resulting\n    maps allow for \"fair\" visual comparison of intensity values (e.g.,\n    occurrence, abundance, or density) across space and time and can be\n    used to address questions about where, when, and how consistently a\n    species, group, or individual is likely to be found.  "
  },
  {
    "id": 10213,
    "package_name": "comat",
    "title": "Creates Co-Occurrence Matrices of Spatial Data",
    "description": "Builds co-occurrence matrices based on spatial raster data.\n    It includes creation of weighted co-occurrence matrices (wecoma) and \n    integrated co-occurrence matrices \n    (incoma; Vadivel et al. (2007) <doi:10.1016/j.patrec.2007.01.004>).",
    "version": "0.9.6",
    "maintainer": "Jakub Nowosad <nowosad.jakub@gmail.com>",
    "author": "Jakub Nowosad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1057-3721>),\n  Maximillian H.K. Hesselbarth [ctb] (Co-author of underlying C++ code\n    for get_class_index_map(), get_unique_values(), and rcpp_get_coma()\n    functions),\n  Marco Sciaini [ctb] (Co-author of underlying C++ code for\n    get_class_index_map(), get_unique_values(), and rcpp_get_coma()\n    functions),\n  Sebastian Hanss [ctb] (Co-author of underlying C++ code for\n    get_class_index_map(), get_unique_values(), and rcpp_get_coma()\n    functions)",
    "url": "https://jakubnowosad.com/comat/",
    "bug_reports": "https://github.com/Nowosad/comat/issues",
    "repository": "https://cran.r-project.org/package=comat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "comat Creates Co-Occurrence Matrices of Spatial Data Builds co-occurrence matrices based on spatial raster data.\n    It includes creation of weighted co-occurrence matrices (wecoma) and \n    integrated co-occurrence matrices \n    (incoma; Vadivel et al. (2007) <doi:10.1016/j.patrec.2007.01.004>).  "
  },
  {
    "id": 10238,
    "package_name": "comparator",
    "title": "Comparison Functions for Clustering and Record Linkage",
    "description": "Implements functions for comparing strings, sequences and \n    numeric vectors for clustering and record linkage applications. \n    Supported comparison functions include: generalized edit distances \n    for comparing sequences/strings, Monge-Elkan similarity for fuzzy \n    comparison of token sets, and L-p distances for comparing numeric \n    vectors. Where possible, comparison functions are implemented in \n    C/C++ to ensure good performance.",
    "version": "0.1.4",
    "maintainer": "Neil Marchant <ngmarchant@gmail.com>",
    "author": "Neil Marchant [aut, cre]",
    "url": "https://github.com/ngmarchant/comparator",
    "bug_reports": "https://github.com/ngmarchant/comparator/issues",
    "repository": "https://cran.r-project.org/package=comparator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "comparator Comparison Functions for Clustering and Record Linkage Implements functions for comparing strings, sequences and \n    numeric vectors for clustering and record linkage applications. \n    Supported comparison functions include: generalized edit distances \n    for comparing sequences/strings, Monge-Elkan similarity for fuzzy \n    comparison of token sets, and L-p distances for comparing numeric \n    vectors. Where possible, comparison functions are implemented in \n    C/C++ to ensure good performance.  "
  },
  {
    "id": 10334,
    "package_name": "consolechoice",
    "title": "An Easy and Quick Way to Loop a Character Vector as a Menu in\nthe Console",
    "description": "A fast way to loop a character vector or file names as a menu in the console for the\n  user to choose an option.",
    "version": "1.1.1",
    "maintainer": "John Piper <john.piper.using.r@gmail.com>",
    "author": "John Piper [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=consolechoice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "consolechoice An Easy and Quick Way to Loop a Character Vector as a Menu in\nthe Console A fast way to loop a character vector or file names as a menu in the console for the\n  user to choose an option.  "
  },
  {
    "id": 10340,
    "package_name": "constrainedKriging",
    "title": "Constrained, Covariance-Matching Constrained and Universal Point\nor Block Kriging",
    "description": "Provides functions for efficient computation of non-linear spatial predictions with local change of support (Hofer, C. and Papritz, A. (2011) \"constrainedKriging: An R-package for customary, constrained and covariance-matching constrained point or block kriging\" <doi:10.1016/j.cageo.2011.02.009>).  This package supplies functions for two-dimensional spatial interpolation by constrained (Cressie, N. (1993) \"Aggregation in geostatistical problems\" <doi:10.1007/978-94-011-1739-5_3>), covariance-matching constrained (Aldworth, J. and Cressie, N. (2003) \"Prediction of nonlinear spatial functionals\" <doi:10.1016/S0378-3758(02)00321-X>) and universal (external drift) Kriging for points or blocks of any shape from data with a non-stationary mean function and an isotropic weakly stationary covariance function.  The linear spatial interpolation methods, constrained and covariance-matching constrained Kriging, provide approximately unbiased prediction for non-linear target values under change of support.  This package extends the range of tools for spatial predictions available in R and provides an alternative to conditional simulation for non-linear spatial prediction problems with local change of support.",
    "version": "0.2-11",
    "maintainer": "Andreas Papritz <papritz@retired.ethz.ch>",
    "author": "Christoph Hofer [aut],\n  Andreas Papritz [ctb, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=constrainedKriging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "constrainedKriging Constrained, Covariance-Matching Constrained and Universal Point\nor Block Kriging Provides functions for efficient computation of non-linear spatial predictions with local change of support (Hofer, C. and Papritz, A. (2011) \"constrainedKriging: An R-package for customary, constrained and covariance-matching constrained point or block kriging\" <doi:10.1016/j.cageo.2011.02.009>).  This package supplies functions for two-dimensional spatial interpolation by constrained (Cressie, N. (1993) \"Aggregation in geostatistical problems\" <doi:10.1007/978-94-011-1739-5_3>), covariance-matching constrained (Aldworth, J. and Cressie, N. (2003) \"Prediction of nonlinear spatial functionals\" <doi:10.1016/S0378-3758(02)00321-X>) and universal (external drift) Kriging for points or blocks of any shape from data with a non-stationary mean function and an isotropic weakly stationary covariance function.  The linear spatial interpolation methods, constrained and covariance-matching constrained Kriging, provide approximately unbiased prediction for non-linear target values under change of support.  This package extends the range of tools for spatial predictions available in R and provides an alternative to conditional simulation for non-linear spatial prediction problems with local change of support.  "
  },
  {
    "id": 10373,
    "package_name": "convoSPAT",
    "title": "Convolution-Based Nonstationary Spatial Modeling",
    "description": "Fits convolution-based nonstationary\n    Gaussian process models to point-referenced spatial data. The nonstationary\n    covariance function allows the user to specify the underlying correlation\n    structure and which spatial dependence parameters should be allowed to\n    vary over space: the anisotropy, nugget variance, and process variance.\n    The parameters are estimated via maximum likelihood, using a local\n    likelihood approach. Also provided are functions to fit stationary spatial\n    models for comparison, calculate the Kriging predictor and standard errors,\n    and create various plots to visualize nonstationarity.",
    "version": "1.2.7",
    "maintainer": "Mark D. Risser <markdrisser@gmail.com>",
    "author": "Mark D. Risser [aut, cre]",
    "url": "http://github.com/markdrisser/convoSPAT",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=convoSPAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "convoSPAT Convolution-Based Nonstationary Spatial Modeling Fits convolution-based nonstationary\n    Gaussian process models to point-referenced spatial data. The nonstationary\n    covariance function allows the user to specify the underlying correlation\n    structure and which spatial dependence parameters should be allowed to\n    vary over space: the anisotropy, nugget variance, and process variance.\n    The parameters are estimated via maximum likelihood, using a local\n    likelihood approach. Also provided are functions to fit stationary spatial\n    models for comparison, calculate the Kriging predictor and standard errors,\n    and create various plots to visualize nonstationarity.  "
  },
  {
    "id": 10377,
    "package_name": "cooltools",
    "title": "Practical Tools for Scientific Computations and Visualizations",
    "description": "Collection of routines for efficient scientific computations in physics and astrophysics. These routines include utility functions, numerical computation tools, as well as visualisation tools. They can be used, for example, for generating random numbers from spherical and custom distributions, information and entropy analysis, special Fourier transforms, two-point correlation estimation (e.g. as in Landy & Szalay (1993) <doi:10.1086/172900>), binning & gridding of point sets, 2D interpolation, Monte Carlo integration, vector arithmetic and coordinate transformations. Also included is a non-exhaustive list of important constants and cosmological conversion functions. The graphics routines can be used to produce and export publication-ready scientific plots and movies, e.g. as used in Obreschkow et al. (2020, MNRAS Vol 493, Issue 3, Pages 4551\u20134569). These routines include special color scales, projection functions, and bitmap handling routines.",
    "version": "2.18",
    "maintainer": "Danail Obreschkow <danail.obreschkow@gmail.com>",
    "author": "Danail Obreschkow [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1527-0762>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cooltools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cooltools Practical Tools for Scientific Computations and Visualizations Collection of routines for efficient scientific computations in physics and astrophysics. These routines include utility functions, numerical computation tools, as well as visualisation tools. They can be used, for example, for generating random numbers from spherical and custom distributions, information and entropy analysis, special Fourier transforms, two-point correlation estimation (e.g. as in Landy & Szalay (1993) <doi:10.1086/172900>), binning & gridding of point sets, 2D interpolation, Monte Carlo integration, vector arithmetic and coordinate transformations. Also included is a non-exhaustive list of important constants and cosmological conversion functions. The graphics routines can be used to produce and export publication-ready scientific plots and movies, e.g. as used in Obreschkow et al. (2020, MNRAS Vol 493, Issue 3, Pages 4551\u20134569). These routines include special color scales, projection functions, and bitmap handling routines.  "
  },
  {
    "id": 10387,
    "package_name": "copulaSim",
    "title": "Virtual Patient Simulation by Copula Invariance Property",
    "description": "To optimize clinical trial designs and data analysis methods consistently through trial simulation, we need to simulate multivariate mixed-type virtual patient data independent of designs and analysis methods under evaluation. To make the outcome of optimization more realistic, relevant empirical patient level data should be utilized when it\u2019s available. However, a few problems arise in simulating trials based on small empirical data, where the underlying marginal distributions and their dependence structure cannot be understood or verified thoroughly due to the limited sample size. To resolve this issue, we use the copula invariance property, which can generate the joint distribution without making a strong parametric assumption. The function copula.sim can generate virtual patient data with optional data validation methods that are based on energy distance and ball divergence measurement. The function compare.copula.sim can conduct comparison of marginal mean and covariance of simulated data. To simulate patient-level data from a hypothetical treatment arm that would perform differently from the observed data, the function new.arm.copula.sim can be used to generate new multivariate data with the same dependence structure of the original data but with a shifted mean vector.",
    "version": "0.0.1",
    "maintainer": "Pei-Shan Yen <peishan0824@gmail.com>",
    "author": "Pei-Shan Yen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7386-0552>),\n  Xuemin Gu [ctb],\n  Jenny Jiao [ctb],\n  Jane Zhang [ctb]",
    "url": "https://github.com/psyen0824/copulaSim",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=copulaSim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "copulaSim Virtual Patient Simulation by Copula Invariance Property To optimize clinical trial designs and data analysis methods consistently through trial simulation, we need to simulate multivariate mixed-type virtual patient data independent of designs and analysis methods under evaluation. To make the outcome of optimization more realistic, relevant empirical patient level data should be utilized when it\u2019s available. However, a few problems arise in simulating trials based on small empirical data, where the underlying marginal distributions and their dependence structure cannot be understood or verified thoroughly due to the limited sample size. To resolve this issue, we use the copula invariance property, which can generate the joint distribution without making a strong parametric assumption. The function copula.sim can generate virtual patient data with optional data validation methods that are based on energy distance and ball divergence measurement. The function compare.copula.sim can conduct comparison of marginal mean and covariance of simulated data. To simulate patient-level data from a hypothetical treatment arm that would perform differently from the observed data, the function new.arm.copula.sim can be used to generate new multivariate data with the same dependence structure of the original data but with a shifted mean vector.  "
  },
  {
    "id": 10425,
    "package_name": "correlbinom",
    "title": "Correlated Binomial Probabilities",
    "description": "Calculates the probabilities of k successes given n trials of a binomial random variable with non-negative correlation across trials. The function takes as inputs the scalar values the level of correlation or association between trials, the success probability, the number of trials, an optional input specifying the number of bits of precision used in the calculation, and an optional input specifying whether the calculation approach to be used is from Witt (2014) <doi:10.1080/03610926.2012.725148> or from Kuk (2004) <doi:10.1046/j.1467-9876.2003.05369.x>. The output is a (trials+1)-dimensional vector containing the likelihoods of 0, 1, ..., trials successes.",
    "version": "0.0.1",
    "maintainer": "Gary Witt <gary.witt@temple.edu>",
    "author": "Chris Rohlfs [aut], Gary Witt [aut,cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=correlbinom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "correlbinom Correlated Binomial Probabilities Calculates the probabilities of k successes given n trials of a binomial random variable with non-negative correlation across trials. The function takes as inputs the scalar values the level of correlation or association between trials, the success probability, the number of trials, an optional input specifying the number of bits of precision used in the calculation, and an optional input specifying whether the calculation approach to be used is from Witt (2014) <doi:10.1080/03610926.2012.725148> or from Kuk (2004) <doi:10.1046/j.1467-9876.2003.05369.x>. The output is a (trials+1)-dimensional vector containing the likelihoods of 0, 1, ..., trials successes.  "
  },
  {
    "id": 10473,
    "package_name": "covercorr",
    "title": "Coverage Correlation Coefficient and Testing for Independence",
    "description": "Computes the coverage correlation coefficient introduced in <doi:10.48550/arXiv.2508.06402> , a statistical measure that quantifies dependence between two random vectors by computing the union volume of data-centered hypercubes in a uniform space.",
    "version": "1.0.0",
    "maintainer": "Tengyao Wang <t.wang59@lse.ac.uk>",
    "author": "Tengyao Wang [aut, cre],\n  Mona Azadkia [aut, ctb],\n  Xuzhi Yang [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=covercorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covercorr Coverage Correlation Coefficient and Testing for Independence Computes the coverage correlation coefficient introduced in <doi:10.48550/arXiv.2508.06402> , a statistical measure that quantifies dependence between two random vectors by computing the union volume of data-centered hypercubes in a uniform space.  "
  },
  {
    "id": 10507,
    "package_name": "cp4p",
    "title": "Calibration Plot for Proteomics",
    "description": "Functions to check whether a vector of p-values respects the assumptions of FDR (false discovery rate) control procedures and to compute adjusted p-values.",
    "version": "0.3.6",
    "maintainer": "Quentin Giai Gianetto <quentin2g@yahoo.fr>",
    "author": "Quentin Giai Gianetto, Florence Combes, Claire Ramus, Christophe Bruley, Yohann Cout\u00e9, Thomas Burger",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cp4p",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cp4p Calibration Plot for Proteomics Functions to check whether a vector of p-values respects the assumptions of FDR (false discovery rate) control procedures and to compute adjusted p-values.  "
  },
  {
    "id": 10527,
    "package_name": "cppcontainers",
    "title": "'C++' Standard Template Library Containers",
    "description": "Use 'C++' Standard Template Library containers interactively in R. Includes sets, unordered sets, multisets, unordered multisets, maps, \n  unordered maps, multimaps, unordered multimaps, stacks, queues, priority queues, vectors, deques, forward lists, and lists.",
    "version": "1.0.5",
    "maintainer": "Christian D\u00fcben <cdueben.ml+cran@proton.me>",
    "author": "Christian D\u00fcben [aut, cre]",
    "url": "https://github.com/cdueben/cppcontainers",
    "bug_reports": "https://github.com/cdueben/cppcontainers/issues",
    "repository": "https://cran.r-project.org/package=cppcontainers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cppcontainers 'C++' Standard Template Library Containers Use 'C++' Standard Template Library containers interactively in R. Includes sets, unordered sets, multisets, unordered multisets, maps, \n  unordered maps, multimaps, unordered multimaps, stacks, queues, priority queues, vectors, deques, forward lists, and lists.  "
  },
  {
    "id": 10528,
    "package_name": "cppdoubles",
    "title": "Fast Relative Comparisons of Floating Point Numbers in 'C++'",
    "description": "Compare double-precision floating point vectors using\n    relative differences. All equality operations are calculated using\n    'cpp11'.",
    "version": "0.4.0",
    "maintainer": "Nick Christofides <nick.christofides.r@gmail.com>",
    "author": "Nick Christofides [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9743-7342>)",
    "url": "",
    "bug_reports": "https://github.com/NicChr/cppdoubles/issues",
    "repository": "https://cran.r-project.org/package=cppdoubles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cppdoubles Fast Relative Comparisons of Floating Point Numbers in 'C++' Compare double-precision floating point vectors using\n    relative differences. All equality operations are calculated using\n    'cpp11'.  "
  },
  {
    "id": 10552,
    "package_name": "createLogicalPCM",
    "title": "Create Logical Pairwise Comparison Matrix for the Analytic\nHierarchy Process",
    "description": "Create Pairwise Comparison Matrices for use in the Analytic Hierarchy Process. The Pairwise Comparison Matrix created will be a logical matrix, which unlike a random comparison matrix, is similar to what a rational decision maker would create on the basis of a preference vector for the alternatives considered.",
    "version": "0.1.0",
    "maintainer": "Amarnath Bose <amarnath.bose@gmail.com>",
    "author": "Amarnath Bose [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=createLogicalPCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "createLogicalPCM Create Logical Pairwise Comparison Matrix for the Analytic\nHierarchy Process Create Pairwise Comparison Matrices for use in the Analytic Hierarchy Process. The Pairwise Comparison Matrix created will be a logical matrix, which unlike a random comparison matrix, is similar to what a rational decision maker would create on the basis of a preference vector for the alternatives considered.  "
  },
  {
    "id": 10574,
    "package_name": "crmReg",
    "title": "Cellwise Robust M-Regression and SPADIMO",
    "description": "Method for fitting a cellwise robust linear M-regression model (CRM, Filzmoser et al. (2020) <DOI:10.1016/j.csda.2020.106944>) that yields both a map of cellwise outliers consistent with the linear model, and a vector of regression coefficients that is robust against vertical outliers and leverage points. As a by-product, the method yields an imputed data set that contains estimates of what the values in cellwise outliers would need to amount to if they had fit the model. The package also provides diagnostic tools for analyzing casewise and cellwise outliers using sparse directions of maximal outlyingness (SPADIMO, Debruyne et al. (2019) <DOI:10.1007/s11222-018-9831-5>).",
    "version": "1.0.4",
    "maintainer": "Sebastiaan Hoppner <sebastiaan.hoppner@gmail.com>",
    "author": "Peter Filzmoser [aut],\n  Sebastiaan Hoppner [aut, cre],\n  Irene Ortner [aut],\n  Sven Serneels [aut],\n  Tim Verdonck [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=crmReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crmReg Cellwise Robust M-Regression and SPADIMO Method for fitting a cellwise robust linear M-regression model (CRM, Filzmoser et al. (2020) <DOI:10.1016/j.csda.2020.106944>) that yields both a map of cellwise outliers consistent with the linear model, and a vector of regression coefficients that is robust against vertical outliers and leverage points. As a by-product, the method yields an imputed data set that contains estimates of what the values in cellwise outliers would need to amount to if they had fit the model. The package also provides diagnostic tools for analyzing casewise and cellwise outliers using sparse directions of maximal outlyingness (SPADIMO, Debruyne et al. (2019) <DOI:10.1007/s11222-018-9831-5>).  "
  },
  {
    "id": 10583,
    "package_name": "cropDemand",
    "title": "Spatial Crop Water Demand for Brazil",
    "description": "Estimation of crop water demand can be processed via this package. As example, the data  from 'TerraClimate' dataset (<https://www.climatologylab.org/terraclimate.html>) calibrated with automatic weather stations of National Meteorological Institute of Brazil is available in a coarse spatial resolution to do the crop water demand. However, the user have also the option to download the variables directly from 'TerraClimate' repository with the download.terraclimate function  and access the original 'TerraClimate' products. If the user believes that is necessary calibrate the variables, there is another function to do it. Lastly, the estimation of the crop water demand present in this package can be run for all the Brazilian territory with 'TerraClimate' dataset. ",
    "version": "1.0.3",
    "maintainer": "Roberto Filgueiras <betofilgueiras@gmail.com>",
    "author": "Roberto Filgueiras [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0186-8907>),\n  Luan P. Venancio [aut] (ORCID: <https://orcid.org/0000-0002-5544-8588>),\n  Catariny C. Aleman [aut] (ORCID:\n    <https://orcid.org/0000-0002-3894-3077>),\n  Fernando F. da Cunha [aut] (ORCID:\n    <https://orcid.org/0000-0002-1671-1021>)",
    "url": "",
    "bug_reports": "https://github.com/FilgueirasR/cropDemand/issues",
    "repository": "https://cran.r-project.org/package=cropDemand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cropDemand Spatial Crop Water Demand for Brazil Estimation of crop water demand can be processed via this package. As example, the data  from 'TerraClimate' dataset (<https://www.climatologylab.org/terraclimate.html>) calibrated with automatic weather stations of National Meteorological Institute of Brazil is available in a coarse spatial resolution to do the crop water demand. However, the user have also the option to download the variables directly from 'TerraClimate' repository with the download.terraclimate function  and access the original 'TerraClimate' products. If the user believes that is necessary calibrate the variables, there is another function to do it. Lastly, the estimation of the crop water demand present in this package can be run for all the Brazilian territory with 'TerraClimate' dataset.   "
  },
  {
    "id": 10584,
    "package_name": "cropZoning",
    "title": "Climate Crop Zoning Based in Air Temperature for Brazil",
    "description": "Climate crop zoning based in minimum and maximum air temperature. The data used in the package are from 'TerraClimate' dataset (<https://www.climatologylab.org/terraclimate.html>), but, it have been calibrated with automatic weather stations  of National Meteorological Institute of Brazil.  The climate crop zoning of this package can be run for all the Brazilian territory.",
    "version": "1.0.3",
    "maintainer": "Roberto Filgueiras <betofilgueiras@gmail.com>",
    "author": "Roberto Filgueiras [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0186-8907>),\n  Luan P. Venancio [aut] (ORCID: <https://orcid.org/0000-0002-5544-8588>),\n  Catariny C. Aleman [aut] (ORCID:\n    <https://orcid.org/0000-0002-3894-3077>),\n  Fernando F. da Cunha [aut] (ORCID:\n    <https://orcid.org/0000-0002-1671-1021>)",
    "url": "",
    "bug_reports": "https://github.com/FilgueirasR/cropZoning/issues",
    "repository": "https://cran.r-project.org/package=cropZoning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cropZoning Climate Crop Zoning Based in Air Temperature for Brazil Climate crop zoning based in minimum and maximum air temperature. The data used in the package are from 'TerraClimate' dataset (<https://www.climatologylab.org/terraclimate.html>), but, it have been calibrated with automatic weather stations  of National Meteorological Institute of Brazil.  The climate crop zoning of this package can be run for all the Brazilian territory.  "
  },
  {
    "id": 10585,
    "package_name": "cropcircles",
    "title": "Crops an Image to a Circle",
    "description": "Images are cropped to a circle with a transparent background. The function takes a\n  vector of images, either local or from a link, and circle crops the image. Paths to the\n  cropped image are returned for plotting with 'ggplot2'. Also includes cropping to a hexagon,\n  heart, parallelogram, and square.",
    "version": "0.2.4",
    "maintainer": "Daniel Oehm <danieloehm@gmail.com>",
    "author": "Daniel Oehm [aut, cre]",
    "url": "https://github.com/doehm/cropcircles",
    "bug_reports": "https://github.com/doehm/cropcircles/issues",
    "repository": "https://cran.r-project.org/package=cropcircles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cropcircles Crops an Image to a Circle Images are cropped to a circle with a transparent background. The function takes a\n  vector of images, either local or from a link, and circle crops the image. Paths to the\n  cropped image are returned for plotting with 'ggplot2'. Also includes cropping to a hexagon,\n  heart, parallelogram, and square.  "
  },
  {
    "id": 10636,
    "package_name": "csn",
    "title": "Closed Skew-Normal Distribution",
    "description": "Provides functions for computing the density\n    and the log-likelihood function of closed-skew normal variates,\n    and for generating random vectors sampled from this distribution.\n    See Gonzalez-Farias, G., Dominguez-Molina, J., and Gupta, A. (2004).\n    The closed skew normal distribution, \n    Skew-elliptical distributions and their applications: a journey beyond normality,\n    Chapman and Hall/CRC, Boca Raton, FL, pp. 25-42.",
    "version": "1.1.3",
    "maintainer": "Dmitry Pavlyuk <Dmitry.V.Pavlyuk@gmail.com>",
    "author": "Dmitry Pavlyuk, Eugene Girtcius",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csn Closed Skew-Normal Distribution Provides functions for computing the density\n    and the log-likelihood function of closed-skew normal variates,\n    and for generating random vectors sampled from this distribution.\n    See Gonzalez-Farias, G., Dominguez-Molina, J., and Gupta, A. (2004).\n    The closed skew normal distribution, \n    Skew-elliptical distributions and their applications: a journey beyond normality,\n    Chapman and Hall/CRC, Boca Raton, FL, pp. 25-42.  "
  },
  {
    "id": 10662,
    "package_name": "ctmcmove",
    "title": "Modeling Animal Movement with Continuous-Time Discrete-Space\nMarkov Chains",
    "description": "Software to facilitates taking movement data in xyt format and pairing it with raster covariates within a continuous time Markov chain (CTMC) framework.  As described in Hanks et al. (2015) <DOI:10.1214/14-AOAS803> , this allows flexible modeling of movement in response to covariates (or covariate gradients) with model fitting possible within a Poisson GLM framework. ",
    "version": "1.2.10",
    "maintainer": "Ephraim Hanks <hanks@psu.edu>",
    "author": "Ephraim Hanks [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ctmcmove",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctmcmove Modeling Animal Movement with Continuous-Time Discrete-Space\nMarkov Chains Software to facilitates taking movement data in xyt format and pairing it with raster covariates within a continuous time Markov chain (CTMC) framework.  As described in Hanks et al. (2015) <DOI:10.1214/14-AOAS803> , this allows flexible modeling of movement in response to covariates (or covariate gradients) with model fitting possible within a Poisson GLM framework.   "
  },
  {
    "id": 10676,
    "package_name": "ctypesio",
    "title": "Read and Write Standard 'C' Types from Files, Connections and\nRaw Vectors",
    "description": "Interacting with binary files can be difficult because R's types\n    are a subset of what is generally supported by 'C'.  This package provides a \n    suite of functions for reading and writing binary data (with files, connections, \n    and raw vectors) using 'C' type descriptions.  These functions convert data\n    between 'C' types and R types while checking for values outside the \n    type limits, 'NA' values, etc.",
    "version": "0.1.3",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph],\n  Anne Fu [ctb] (Better UTF-8 support, ORCID:\n    <https://orcid.org/0000-0002-9025-6071>)",
    "url": "https://github.com/coolbutuseless/ctypesio",
    "bug_reports": "https://github.com/coolbutuseless/ctypesio/issues",
    "repository": "https://cran.r-project.org/package=ctypesio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctypesio Read and Write Standard 'C' Types from Files, Connections and\nRaw Vectors Interacting with binary files can be difficult because R's types\n    are a subset of what is generally supported by 'C'.  This package provides a \n    suite of functions for reading and writing binary data (with files, connections, \n    and raw vectors) using 'C' type descriptions.  These functions convert data\n    between 'C' types and R types while checking for values outside the \n    type limits, 'NA' values, etc.  "
  },
  {
    "id": 10678,
    "package_name": "cuadramelo",
    "title": "Matrix Balancing and Rounding",
    "description": "Balancing and rounding matrices subject to restrictions. Adjustment of matrices so that columns and rows add up to given vectors, rounding of a matrix while keeping the column and/or row totals, performing these by blocks...",
    "version": "1.0.0",
    "maintainer": "Miguel Serrano <miguel.serrano.martin@ine.es>",
    "author": "Miguel Serrano [aut, cre]",
    "url": "https://mserrano-ine.github.io/cuadramelo/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cuadramelo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cuadramelo Matrix Balancing and Rounding Balancing and rounding matrices subject to restrictions. Adjustment of matrices so that columns and rows add up to given vectors, rounding of a matrix while keeping the column and/or row totals, performing these by blocks...  "
  },
  {
    "id": 10680,
    "package_name": "cubature",
    "title": "Adaptive Multivariate Integration over Hypercubes",
    "description": "R wrappers around the cubature C library of Steven\n    G. Johnson for adaptive multivariate integration over hypercubes\n    and the Cuba C library of Thomas Hahn for deterministic and\n    Monte Carlo integration. Scalar and vector interfaces for \n    cubature and Cuba routines are provided; the vector interfaces\n    are highly recommended as demonstrated in the package\n    vignette.",
    "version": "2.1.4-1",
    "maintainer": "Balasubramanian Narasimhan <naras@stat.stanford.edu>",
    "author": "Balasubramanian Narasimhan [aut, cre],\n  Manuel Koller [ctb],\n  Dirk Eddelbuettel [ctb],\n  Steven G. Johnson [aut],\n  Thomas Hahn [aut],\n  Annie Bouvier [aut],\n  Ki\u00ean Ki\u00eau [aut],\n  Simen Gaure [ctb]",
    "url": "https://bnaras.github.io/cubature/",
    "bug_reports": "https://github.com/bnaras/cubature/issues",
    "repository": "https://cran.r-project.org/package=cubature",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cubature Adaptive Multivariate Integration over Hypercubes R wrappers around the cubature C library of Steven\n    G. Johnson for adaptive multivariate integration over hypercubes\n    and the Cuba C library of Thomas Hahn for deterministic and\n    Monte Carlo integration. Scalar and vector interfaces for \n    cubature and Cuba routines are provided; the vector interfaces\n    are highly recommended as demonstrated in the package\n    vignette.  "
  },
  {
    "id": 10681,
    "package_name": "cubble",
    "title": "A Vector Spatio-Temporal Data Structure for Data Analysis",
    "description": "A spatiotemperal data object in a relational data structure to separate the recording of time variant/ invariant variables. See the Journal of Statistical Software reference: <doi:10.18637/jss.v110.i07>.",
    "version": "1.0.0",
    "maintainer": "H. Sherry Zhang <huizezhangsh@gmail.com>",
    "author": "H. Sherry Zhang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7122-1463>),\n  Dianne Cook [aut] (ORCID: <https://orcid.org/0000-0002-3813-7155>),\n  Ursula Laa [aut] (ORCID: <https://orcid.org/0000-0002-0249-6439>),\n  Nicolas Langren\u00e9 [aut] (ORCID: <https://orcid.org/0000-0001-7601-4618>),\n  Patricia Men\u00e9ndez [aut] (ORCID:\n    <https://orcid.org/0000-0003-0701-6315>)",
    "url": "https://github.com/huizezhang-sherry/cubble,\nhttps://huizezhang-sherry.github.io/cubble/",
    "bug_reports": "https://github.com/huizezhang-sherry/cubble/issues",
    "repository": "https://cran.r-project.org/package=cubble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cubble A Vector Spatio-Temporal Data Structure for Data Analysis A spatiotemperal data object in a relational data structure to separate the recording of time variant/ invariant variables. See the Journal of Statistical Software reference: <doi:10.18637/jss.v110.i07>.  "
  },
  {
    "id": 10683,
    "package_name": "cubeview",
    "title": "View 3D Raster Cubes Interactively",
    "description": "Creates a 3D data cube view of a RasterStack/Brick, typically a \n    collection/array of RasterLayers (along z-axis) with the same geographical \n    extent (x and y dimensions) and resolution, provided by package 'raster'. \n    Slices through each dimension (x/y/z), freely adjustable in location, \n    are mapped to the visible sides of the cube. The cube can be freely rotated. \n    Zooming and panning can be used to focus on different areas of the cube.",
    "version": "0.4.1",
    "maintainer": "Tim Appelhans <tim.appelhans@gmail.com>",
    "author": "Tim Appelhans [cre, aut],\n  Stefan Woellauer [aut],\n  three.js authors [ctb, cph] (three.js library)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cubeview",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cubeview View 3D Raster Cubes Interactively Creates a 3D data cube view of a RasterStack/Brick, typically a \n    collection/array of RasterLayers (along z-axis) with the same geographical \n    extent (x and y dimensions) and resolution, provided by package 'raster'. \n    Slices through each dimension (x/y/z), freely adjustable in location, \n    are mapped to the visible sides of the cube. The cube can be freely rotated. \n    Zooming and panning can be used to focus on different areas of the cube.  "
  },
  {
    "id": 10691,
    "package_name": "cumulcalib",
    "title": "Cumulative Calibration Assessment for Prediction Models",
    "description": "Tools for visualization of, and inference on, the calibration of prediction models on the cumulative domain. This provides a method for evaluating calibration of risk prediction models without having to group the data or use tuning parameters (e.g., loess bandwidth). This package implements the methodology described in Sadatsafavi and Patkau (2024) <doi:10.1002/sim.10138>. The core of the package is cumulcalib(), which takes in vectors of binary responses and predicted risks. The plot() and summary() methods are implemented for the results returned by cumulcalib().",
    "version": "0.0.1",
    "maintainer": "Mohsen Sadatsafavi <mohsen.sadatsafavi@ubc.ca>",
    "author": "Mohsen Sadatsafavi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0419-7862>)",
    "url": "https://github.com/resplab/cumulcalib",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cumulcalib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cumulcalib Cumulative Calibration Assessment for Prediction Models Tools for visualization of, and inference on, the calibration of prediction models on the cumulative domain. This provides a method for evaluating calibration of risk prediction models without having to group the data or use tuning parameters (e.g., loess bandwidth). This package implements the methodology described in Sadatsafavi and Patkau (2024) <doi:10.1002/sim.10138>. The core of the package is cumulcalib(), which takes in vectors of binary responses and predicted risks. The plot() and summary() methods are implemented for the results returned by cumulcalib().  "
  },
  {
    "id": 10714,
    "package_name": "cusumcharter",
    "title": "Easier CUSUM Control Charts",
    "description": "Create CUSUM (cumulative sum) statistics from a vector or dataframe.\n    Also create single or faceted CUSUM control charts, with or without control limits.\n    Accepts vector, dataframe, tibble or data.table inputs.",
    "version": "0.1.0",
    "maintainer": "John MacKintosh <johnmackintosh.jm@gmail.com>",
    "author": "John MacKintosh [aut, cre]",
    "url": "https://github.com/johnmackintosh/cusumcharter,\nhttps://johnmackintosh.github.io/cusumcharter/",
    "bug_reports": "https://github.com/johnmackintosh/cusumcharter/issues",
    "repository": "https://cran.r-project.org/package=cusumcharter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cusumcharter Easier CUSUM Control Charts Create CUSUM (cumulative sum) statistics from a vector or dataframe.\n    Also create single or faceted CUSUM control charts, with or without control limits.\n    Accepts vector, dataframe, tibble or data.table inputs.  "
  },
  {
    "id": 10786,
    "package_name": "dagirlite",
    "title": "Spatial Vector Data for Danmarks Administrative Geografiske\nInddeling DAGI",
    "description": "Compressed spatial vector data originally from <https://dawadocs.dataforsyningen.dk/> saved as Simple \n    Features, SF, objects with data on population, age and gender from Statistics Denmark <https://www.dst.dk/da/>.",
    "version": "0.1.0",
    "maintainer": "Lars Boerty <lars.borty@gmail.com>",
    "author": "Lars Boerty [aut, cre] (ORCID: <https://orcid.org/0000-0002-3715-8528>),\n  Haematology Research Unit - Aalborg [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dagirlite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dagirlite Spatial Vector Data for Danmarks Administrative Geografiske\nInddeling DAGI Compressed spatial vector data originally from <https://dawadocs.dataforsyningen.dk/> saved as Simple \n    Features, SF, objects with data on population, age and gender from Statistics Denmark <https://www.dst.dk/da/>.  "
  },
  {
    "id": 10842,
    "package_name": "datapasta",
    "title": "R Tools for Data Copy-Pasta",
    "description": "RStudio addins and R functions that make copy-pasting vectors and tables to text painless.",
    "version": "3.1.0",
    "maintainer": "Miles McBain <miles.mcbain@gmail.com>",
    "author": "Miles McBain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2865-2548>),\n  Jonathan Carroll [aut] (ORCID: <https://orcid.org/0000-0002-1404-5264>),\n  Mark Dulhunty [ctb],\n  Andrew Collier [ctb],\n  Sharla Gelfand [aut],\n  Suthira Owlarn [aut] (ORCID: <https://orcid.org/0000-0002-3258-1415>),\n  Garrick Aden-Buie [aut] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://github.com/milesmcbain/datapasta",
    "bug_reports": "https://github.com/milesmcbain/datapasta/issues",
    "repository": "https://cran.r-project.org/package=datapasta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datapasta R Tools for Data Copy-Pasta RStudio addins and R functions that make copy-pasting vectors and tables to text painless.  "
  },
  {
    "id": 10872,
    "package_name": "dauphin",
    "title": "Compact Standard for Australian Phone Numbers",
    "description": "Phone numbers are often represented as strings because there\n    is no obvious and suitable native representation for them.  This leads\n    to high memory use and a lack of standard representation.  The package\n    provides integer representation of Australian phone numbers with optional\n    raw vector calling code. The package name is an extension of 'au' and 'ph'.",
    "version": "0.3.2",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dauphin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dauphin Compact Standard for Australian Phone Numbers Phone numbers are often represented as strings because there\n    is no obvious and suitable native representation for them.  This leads\n    to high memory use and a lack of standard representation.  The package\n    provides integer representation of Australian phone numbers with optional\n    raw vector calling code. The package name is an extension of 'au' and 'ph'.  "
  },
  {
    "id": 10882,
    "package_name": "dbcsp",
    "title": "Distance-Based Common Spatial Patterns",
    "description": "A way to apply Distance-Based Common Spatial Patterns\n    (DB-CSP) techniques in different fields, both classical Common Spatial\n    Patterns (CSP) as well as DB-CSP. The method is composed of two\n    phases: applying the DB-CSP algorithm and performing a classification.\n    The main idea behind the CSP is to use a linear transform to project\n    data into low-dimensional subspace with a projection matrix, in such a\n    way that each row consists of weights for signals. This transformation\n    maximizes the variance of two-class signal matrices.The dbcsp object\n    is created to compute the projection vectors. For exploratory and\n    descriptive purpose, plot and boxplot functions can be used. Functions\n    train, predict and selectQ are implemented for the classification\n    step.",
    "version": "0.0.2.2",
    "maintainer": "Itsaso Rodr\u00edguez-Moreno <itsaso.rodriguez@ehu.eus>",
    "author": "Itziar Irigoien [aut],\n  Concepci\u00f3n Arenas [aut],\n  Itsaso Rodr\u00edguez-Moreno [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dbcsp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbcsp Distance-Based Common Spatial Patterns A way to apply Distance-Based Common Spatial Patterns\n    (DB-CSP) techniques in different fields, both classical Common Spatial\n    Patterns (CSP) as well as DB-CSP. The method is composed of two\n    phases: applying the DB-CSP algorithm and performing a classification.\n    The main idea behind the CSP is to use a linear transform to project\n    data into low-dimensional subspace with a projection matrix, in such a\n    way that each row consists of weights for signals. This transformation\n    maximizes the variance of two-class signal matrices.The dbcsp object\n    is created to compute the projection vectors. For exploratory and\n    descriptive purpose, plot and boxplot functions can be used. Functions\n    train, predict and selectQ are implemented for the classification\n    step.  "
  },
  {
    "id": 10914,
    "package_name": "dcsvm",
    "title": "Density Convoluted Support Vector Machines",
    "description": "Implements an efficient algorithm for solving sparse-penalized support vector machines with kernel density convolution. This package is designed for high-dimensional classification tasks, supporting lasso (L1) and elastic-net penalties for sparse feature selection and providing options for tuning kernel bandwidth and penalty weights. The 'dcsvm' is applicable to fields such as bioinformatics, image analysis, and text classification, where high-dimensional data commonly arise. Learn more about the methodology and algorithm at Wang, Zhou, Gu, and Zou (2023) <doi:10.1109/TIT.2022.3222767>.",
    "version": "0.0.1",
    "maintainer": "Boxiang Wang <boxiang-wang@uiowa.edu>",
    "author": "Boxiang Wang [aut, cre],\n  Le Zhou [aut],\n  Yuwen Gu [aut],\n  Hui Zou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dcsvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dcsvm Density Convoluted Support Vector Machines Implements an efficient algorithm for solving sparse-penalized support vector machines with kernel density convolution. This package is designed for high-dimensional classification tasks, supporting lasso (L1) and elastic-net penalties for sparse feature selection and providing options for tuning kernel bandwidth and penalty weights. The 'dcsvm' is applicable to fields such as bioinformatics, image analysis, and text classification, where high-dimensional data commonly arise. Learn more about the methodology and algorithm at Wang, Zhou, Gu, and Zou (2023) <doi:10.1109/TIT.2022.3222767>.  "
  },
  {
    "id": 10926,
    "package_name": "ddplot",
    "title": "Create D3 Based SVG Graphics",
    "description": "Create 'D3' based 'SVG' ('Scalable Vector Graphics') graphics using a simple 'R' API. \n    The package aims to simplify\n    the creation of many 'SVG' plot types using a straightforward 'R' API.  \n    The package relies on the 'r2d3' 'R' package and the 'D3' 'JavaScript' library. \n    See <https://rstudio.github.io/r2d3/> and <https://d3js.org/> respectively.",
    "version": "0.0.2",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden [aut, cre],\n  June Choe [ctb, cph],\n  Mike Bostock [ctb, cph] (D3.js developer),\n  RStudio [ctb, cph] (developers of the r2d3 package)",
    "url": "https://github.com/feddelegrand7/ddplot",
    "bug_reports": "https://github.com/feddelegrand7/ddplot/issues",
    "repository": "https://cran.r-project.org/package=ddplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ddplot Create D3 Based SVG Graphics Create 'D3' based 'SVG' ('Scalable Vector Graphics') graphics using a simple 'R' API. \n    The package aims to simplify\n    the creation of many 'SVG' plot types using a straightforward 'R' API.  \n    The package relies on the 'r2d3' 'R' package and the 'D3' 'JavaScript' library. \n    See <https://rstudio.github.io/r2d3/> and <https://d3js.org/> respectively.  "
  },
  {
    "id": 10940,
    "package_name": "debkeepr",
    "title": "Analysis of Non-Decimal Currencies and Double-Entry Bookkeeping",
    "description": "Analysis of historical non-decimal currencies and value\n    systems that use tripartite or tetrapartite systems such as pounds,\n    shillings, and pence. It introduces new vector classes to represent\n    non-decimal currencies, making them compatible with numeric classes,\n    and provides functions to work with these classes in data frames in\n    the context of double-entry bookkeeping.",
    "version": "0.1.1",
    "maintainer": "Jesse Sadler <jrsadler@icloud.com>",
    "author": "Jesse Sadler [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6081-9681>)",
    "url": "https://github.com/jessesadler/debkeepr,\nhttps://jessesadler.github.io/debkeepr/",
    "bug_reports": "https://github.com/jessesadler/debkeepr/issues",
    "repository": "https://cran.r-project.org/package=debkeepr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "debkeepr Analysis of Non-Decimal Currencies and Double-Entry Bookkeeping Analysis of historical non-decimal currencies and value\n    systems that use tripartite or tetrapartite systems such as pounds,\n    shillings, and pence. It introduces new vector classes to represent\n    non-decimal currencies, making them compatible with numeric classes,\n    and provides functions to work with these classes in data frames in\n    the context of double-entry bookkeeping.  "
  },
  {
    "id": 10950,
    "package_name": "decomposedPSF",
    "title": "Time Series Prediction with PSF and Decomposition Methods (EMD\nand EEMD)",
    "description": "Predict future values with hybrid combinations of Pattern Sequence based\n        Forecasting (PSF), Autoregressive Integrated Moving Average (ARIMA), Empirical Mode\n        Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD) methods based\n        hybrid methods.",
    "version": "0.2",
    "maintainer": "Neeraj Bokde <neerajdhanraj@gmail.com>",
    "author": "Neeraj Bokde",
    "url": "https://www.neerajbokde.in/software/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=decomposedPSF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "decomposedPSF Time Series Prediction with PSF and Decomposition Methods (EMD\nand EEMD) Predict future values with hybrid combinations of Pattern Sequence based\n        Forecasting (PSF), Autoregressive Integrated Moving Average (ARIMA), Empirical Mode\n        Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD) methods based\n        hybrid methods.  "
  },
  {
    "id": 10959,
    "package_name": "deduped",
    "title": "Making \"Deduplicated\" Functions",
    "description": "Contains one main function deduped() which speeds up slow,\n    vectorized functions by only performing computations on the unique values\n    of the input and expanding the results at the end.",
    "version": "0.3.0",
    "maintainer": "Or Gadish <orgadish@gmail.com>",
    "author": "Or Gadish [aut, cre, cph]",
    "url": "https://github.com/orgadish/deduped",
    "bug_reports": "https://github.com/orgadish/deduped/issues",
    "repository": "https://cran.r-project.org/package=deduped",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deduped Making \"Deduplicated\" Functions Contains one main function deduped() which speeds up slow,\n    vectorized functions by only performing computations on the unique values\n    of the input and expanding the results at the end.  "
  },
  {
    "id": 10971,
    "package_name": "deepspat",
    "title": "Deep Compositional Spatial Models",
    "description": "Deep compositional spatial models are standard spatial covariance\n             models coupled with an injective warping function of the spatial \n             domain. The warping function is constructed through a composition \n             of multiple elemental injective functions in a deep-learning \n             framework. The package implements two cases for the univariate setting; first,\n\t     when these warping functions are known up to some weights that\n\t     need to be estimated, and, second, when the weights in each layer are random.\n\t     In the multivariate setting only the former case is available.\n\t     Estimation and inference is done using `tensorflow`, which makes use of \n             graphics processing units. \n             For more details see Zammit-Mangion et al. (2022) <doi:10.1080/01621459.2021.1887741>,\n             Vu et al. (2022) <doi:10.5705/ss.202020.0156>,\n             Vu et al. (2023) <doi:10.1016/j.spasta.2023.100742>, and\n             Shao et al. (2025) <doi:10.48550/arXiv.2505.12548>.",
    "version": "0.3.1",
    "maintainer": "Quan Vu <quanvustats@gmail.com>",
    "author": "Andrew Zammit-Mangion [aut],\n  Quan Vu [aut, cre],\n  Xuanjie Shao [aut]",
    "url": "https://github.com/andrewzm/deepspat",
    "bug_reports": "https://github.com/andrewzm/deepspat/issues",
    "repository": "https://cran.r-project.org/package=deepspat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deepspat Deep Compositional Spatial Models Deep compositional spatial models are standard spatial covariance\n             models coupled with an injective warping function of the spatial \n             domain. The warping function is constructed through a composition \n             of multiple elemental injective functions in a deep-learning \n             framework. The package implements two cases for the univariate setting; first,\n\t     when these warping functions are known up to some weights that\n\t     need to be estimated, and, second, when the weights in each layer are random.\n\t     In the multivariate setting only the former case is available.\n\t     Estimation and inference is done using `tensorflow`, which makes use of \n             graphics processing units. \n             For more details see Zammit-Mangion et al. (2022) <doi:10.1080/01621459.2021.1887741>,\n             Vu et al. (2022) <doi:10.5705/ss.202020.0156>,\n             Vu et al. (2023) <doi:10.1016/j.spasta.2023.100742>, and\n             Shao et al. (2025) <doi:10.48550/arXiv.2505.12548>.  "
  },
  {
    "id": 10979,
    "package_name": "deforestable",
    "title": "Classify RGB Images into Forest or Non-Forest",
    "description": "Implements two out-of box classifiers presented in <doi:10.1002/env.2848> for \n    distinguishing forest and non-forest terrain images. Under these algorithms, there are \n    frequentist approaches: one parametric, using stable distributions, and another one- \n    non-parametric, using the squared Mahalanobis distance. The package also contains functions for \n    data handling and building of new classifiers as well as some test data set.  ",
    "version": "3.1.2",
    "maintainer": "Dmitry Otryakhin <d.otryakhin.acad@protonmail.ch>",
    "author": "Jesper Muren [aut] (ORCID: <https://orcid.org/0000-0002-9208-5325>),\n  Dmitry Otryakhin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4700-7221>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=deforestable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deforestable Classify RGB Images into Forest or Non-Forest Implements two out-of box classifiers presented in <doi:10.1002/env.2848> for \n    distinguishing forest and non-forest terrain images. Under these algorithms, there are \n    frequentist approaches: one parametric, using stable distributions, and another one- \n    non-parametric, using the squared Mahalanobis distance. The package also contains functions for \n    data handling and building of new classifiers as well as some test data set.    "
  },
  {
    "id": 10985,
    "package_name": "degross",
    "title": "Density Estimation from GROuped Summary Statistics",
    "description": "Estimation of a density from grouped (tabulated) summary statistics evaluated in each of the big bins (or classes) partitioning the support of the variable. These statistics include class frequencies and central moments of order one up to four. The log-density is modelled using a linear combination of penalised B-splines. The multinomial log-likelihood involving the frequencies adds up to a roughness penalty based on the differences in the coefficients of neighbouring B-splines and the log of a root-n approximation of the sampling density of the observed vector of central moments in each class. The so-obtained penalized log-likelihood is maximized using the EM algorithm to get an estimate of the spline parameters and, consequently, of the variable density and related quantities such as quantiles, see Lambert, P. (2021) <arXiv:2107.03883> for details.",
    "version": "0.9.0",
    "maintainer": "Philippe Lambert <p.lambert@uliege.be>",
    "author": "Philippe Lambert [aut, cre] (Universit\u00e9 de Li\u00e8ge / Universit\u00e9\n    catholique de Louvain (Belgium))",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=degross",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "degross Density Estimation from GROuped Summary Statistics Estimation of a density from grouped (tabulated) summary statistics evaluated in each of the big bins (or classes) partitioning the support of the variable. These statistics include class frequencies and central moments of order one up to four. The log-density is modelled using a linear combination of penalised B-splines. The multinomial log-likelihood involving the frequencies adds up to a roughness penalty based on the differences in the coefficients of neighbouring B-splines and the log of a root-n approximation of the sampling density of the observed vector of central moments in each class. The so-obtained penalized log-likelihood is maximized using the EM algorithm to get an estimate of the spline parameters and, consequently, of the variable density and related quantities such as quantiles, see Lambert, P. (2021) <arXiv:2107.03883> for details.  "
  },
  {
    "id": 11106,
    "package_name": "dgumbel",
    "title": "The Gumbel Distribution Functions and Gradients",
    "description": "Gumbel distribution functions (De Haan L. (2007)\n    <doi:10.1007/0-387-34471-3>) implemented with the techniques of automatic\n    differentiation (Griewank A. (2008) <isbn:978-0-89871-659-7>).\n    With this tool, a user should be able to quickly model extreme\n    events for which the Gumbel distribution is the domain of attraction.\n    The package makes available the density function, the distribution\n    function the quantile function and a random generating function. In\n    addition, it supports gradient functions. The package combines 'Adept'\n    (C++ templated automatic differentiation) (Hogan R. (2017)\n    <doi:10.5281/zenodo.1004730>) and 'Eigen' (templated matrix-vector\n    library) for fast computations of both objective functions and exact\n    gradients. It relies on 'RcppEigen' for easy access to 'Eigen' and\n    bindings to R.",
    "version": "1.0.1",
    "maintainer": "Berent \u00c5nund Str\u00f8mnes Lunde <lundeberent@gmail.com>",
    "author": "Berent \u00c5nund Str\u00f8mnes Lunde [aut, cre, cph],\n  Robin Hogan [ctb] (Author of included Adept library),\n  The University of Reading [cph] (Copyright holder of included Adept\n    library)",
    "url": "https://github.com/blunde1/dgumbel",
    "bug_reports": "https://github.com/blunde1/dgumbel/issues",
    "repository": "https://cran.r-project.org/package=dgumbel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dgumbel The Gumbel Distribution Functions and Gradients Gumbel distribution functions (De Haan L. (2007)\n    <doi:10.1007/0-387-34471-3>) implemented with the techniques of automatic\n    differentiation (Griewank A. (2008) <isbn:978-0-89871-659-7>).\n    With this tool, a user should be able to quickly model extreme\n    events for which the Gumbel distribution is the domain of attraction.\n    The package makes available the density function, the distribution\n    function the quantile function and a random generating function. In\n    addition, it supports gradient functions. The package combines 'Adept'\n    (C++ templated automatic differentiation) (Hogan R. (2017)\n    <doi:10.5281/zenodo.1004730>) and 'Eigen' (templated matrix-vector\n    library) for fast computations of both objective functions and exact\n    gradients. It relies on 'RcppEigen' for easy access to 'Eigen' and\n    bindings to R.  "
  },
  {
    "id": 11166,
    "package_name": "dint",
    "title": "A Toolkit for Year-Quarter, Year-Month and Year-Isoweek Dates",
    "description": "S3 classes and methods to create and work\n    with year-quarter, year-month and year-isoweek vectors. Basic\n    arithmetic operations (such as adding and subtracting) are supported,\n    as well as formatting and converting to and from standard R date\n    types.",
    "version": "2.1.5",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>)",
    "url": "https://github.com/s-fleck/dint",
    "bug_reports": "https://github.com/s-fleck/dint/issues",
    "repository": "https://cran.r-project.org/package=dint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dint A Toolkit for Year-Quarter, Year-Month and Year-Isoweek Dates S3 classes and methods to create and work\n    with year-quarter, year-month and year-isoweek vectors. Basic\n    arithmetic operations (such as adding and subtracting) are supported,\n    as well as formatting and converting to and from standard R date\n    types.  "
  },
  {
    "id": 11204,
    "package_name": "disordR",
    "title": "Non-Ordered Vectors",
    "description": "Functionality for manipulating values of associative\n  maps.  The package is a dependency for mvp-type\n  packages that use the STL map class: it traps\n  plausible idiom that is ill-defined (implementation-specific) and\n  returns an informative error, rather than returning a possibly\n  incorrect result.  To cite the package in publications please use\n  Hankin (2022) <doi:10.48550/ARXIV.2210.03856>.",
    "version": "0.9-8-5",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "author": "Robin K. S. Hankin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5982-0415>)",
    "url": "https://github.com/RobinHankin/disordR,\nhttps://robinhankin.github.io/disordR/",
    "bug_reports": "https://github.com/RobinHankin/disordR/issues",
    "repository": "https://cran.r-project.org/package=disordR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "disordR Non-Ordered Vectors Functionality for manipulating values of associative\n  maps.  The package is a dependency for mvp-type\n  packages that use the STL map class: it traps\n  plausible idiom that is ill-defined (implementation-specific) and\n  returns an informative error, rather than returning a possibly\n  incorrect result.  To cite the package in publications please use\n  Hankin (2022) <doi:10.48550/ARXIV.2210.03856>.  "
  },
  {
    "id": 11207,
    "package_name": "dispeRse",
    "title": "Simulation of Demic Diffusion with Environmental Constraints",
    "description": "Simulates demic diffusion building on models previously developed\n\tfor the expansion of Neolithic and other food-producing economies during\n\tthe Holocene (Fort et al. (2012) <doi:10.7183/0002-7316.77.2.203>, Souza et al.\n\t(2021) <doi:10.1098/rsif.2021.0499>). Growth and emigration are modelled as\n\tdensity-dependent processes using logistic growth and an asymptotic threshold\n\tmodel. Environmental and terrain layers, which can change over time, affect\n\tcarrying capacity, growth and mobility. Multiple centres of origin with\n\ttheir respective starting times can be specified.",
    "version": "1.1",
    "maintainer": "Jonas Gregorio de Souza <jonas.gregorio@gmail.com>",
    "author": "Jonas Gregorio de Souza [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6032-4443>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dispeRse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dispeRse Simulation of Demic Diffusion with Environmental Constraints Simulates demic diffusion building on models previously developed\n\tfor the expansion of Neolithic and other food-producing economies during\n\tthe Holocene (Fort et al. (2012) <doi:10.7183/0002-7316.77.2.203>, Souza et al.\n\t(2021) <doi:10.1098/rsif.2021.0499>). Growth and emigration are modelled as\n\tdensity-dependent processes using logistic growth and an asymptotic threshold\n\tmodel. Environmental and terrain layers, which can change over time, affect\n\tcarrying capacity, growth and mobility. Multiple centres of origin with\n\ttheir respective starting times can be specified.  "
  },
  {
    "id": 11220,
    "package_name": "distanceto",
    "title": "Calculate Distance to Features",
    "description": "Calculates distances from point locations to features.\n    The usual approach for eg. resource selection function analyses is to\n    generate a complete distance to features surface then sample it with your \n    observed and random points. Since these raster based approaches can be\n    pretty costly with large areas, and often lead to memory issues in R, \n    the distanceto package opts to compute these distances using\n    efficient, vector based approaches. As a helper, there's a decidedly \n    low-res raster based approach for visually inspecting your region's \n    distance surface. But the workhorse is distance_to.",
    "version": "0.0.3",
    "maintainer": "Alec L. Robitaille <robit.alec@gmail.com>",
    "author": "Alec L. Robitaille [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4706-1762>)",
    "url": "https://github.com/robitalec/distance-to,\nhttps://robitalec.github.io/distance-to/",
    "bug_reports": "https://github.com/robitalec/distance-to/issues",
    "repository": "https://cran.r-project.org/package=distanceto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distanceto Calculate Distance to Features Calculates distances from point locations to features.\n    The usual approach for eg. resource selection function analyses is to\n    generate a complete distance to features surface then sample it with your \n    observed and random points. Since these raster based approaches can be\n    pretty costly with large areas, and often lead to memory issues in R, \n    the distanceto package opts to compute these distances using\n    efficient, vector based approaches. As a helper, there's a decidedly \n    low-res raster based approach for visually inspecting your region's \n    distance surface. But the workhorse is distance_to.  "
  },
  {
    "id": 11234,
    "package_name": "distributional",
    "title": "Vectorised Probability Distributions",
    "description": "Vectorised distribution objects with tools for manipulating, \n    visualising, and using probability distributions. Designed to allow model\n    prediction outputs to return distributions rather than their parameters, \n    allowing users to directly interact with predictive distributions in a\n    data-oriented workflow. In addition to providing generic replacements for\n    p/d/q/r functions, other useful statistics can be computed including means,\n    variances, intervals, and highest density regions.",
    "version": "0.5.0",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "author": "Mitchell O'Hara-Wild [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6729-7695>),\n  Matthew Kay [aut] (ORCID: <https://orcid.org/0000-0001-9446-0419>),\n  Alex Hayes [aut] (ORCID: <https://orcid.org/0000-0002-4985-5160>),\n  Rob Hyndman [aut] (ORCID: <https://orcid.org/0000-0002-2140-5352>),\n  Earo Wang [ctb] (ORCID: <https://orcid.org/0000-0001-6448-5260>),\n  Vencislav Popov [ctb] (ORCID: <https://orcid.org/0000-0002-8073-4199>)",
    "url": "https://pkg.mitchelloharawild.com/distributional/,\nhttps://github.com/mitchelloharawild/distributional",
    "bug_reports": "https://github.com/mitchelloharawild/distributional/issues",
    "repository": "https://cran.r-project.org/package=distributional",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distributional Vectorised Probability Distributions Vectorised distribution objects with tools for manipulating, \n    visualising, and using probability distributions. Designed to allow model\n    prediction outputs to return distributions rather than their parameters, \n    allowing users to directly interact with predictive distributions in a\n    data-oriented workflow. In addition to providing generic replacements for\n    p/d/q/r functions, other useful statistics can be computed including means,\n    variances, intervals, and highest density regions.  "
  },
  {
    "id": 11253,
    "package_name": "divraster",
    "title": "Diversity Metrics Calculations for Rasterized Data",
    "description": "Alpha and beta diversity for taxonomic (TD), functional (FD),\n    and phylogenetic (PD) dimensions based on rasters. Spatial and\n    temporal beta diversity can be partitioned into replacement and\n    richness difference components. It also calculates standardized effect\n    size for FD and PD alpha diversity and the average individual traits\n    across multilayer rasters. The layers of the raster represent species,\n    while the cells represent communities. Methods details can be found at\n    Cardoso et al. 2022 <https://CRAN.R-project.org/package=BAT> and\n    Heming et al. 2023 <https://CRAN.R-project.org/package=SESraster>.",
    "version": "1.2.1",
    "maintainer": "Fl\u00e1vio M. M. Mota <flaviomoc@gmail.com>",
    "author": "Fl\u00e1vio M. M. Mota [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0308-7151>),\n  Neander Marcel Heming [aut] (ORCID:\n    <https://orcid.org/0000-0003-2461-5045>),\n  Gabriela Alves-Ferreira [aut] (ORCID:\n    <https://orcid.org/0000-0001-5661-3381>)",
    "url": "https://github.com/flaviomoc/divraster,\nhttps://flaviomoc.github.io/divraster/",
    "bug_reports": "https://github.com/flaviomoc/divraster/issues",
    "repository": "https://cran.r-project.org/package=divraster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "divraster Diversity Metrics Calculations for Rasterized Data Alpha and beta diversity for taxonomic (TD), functional (FD),\n    and phylogenetic (PD) dimensions based on rasters. Spatial and\n    temporal beta diversity can be partitioned into replacement and\n    richness difference components. It also calculates standardized effect\n    size for FD and PD alpha diversity and the average individual traits\n    across multilayer rasters. The layers of the raster represent species,\n    while the cells represent communities. Methods details can be found at\n    Cardoso et al. 2022 <https://CRAN.R-project.org/package=BAT> and\n    Heming et al. 2023 <https://CRAN.R-project.org/package=SESraster>.  "
  },
  {
    "id": 11297,
    "package_name": "dobin",
    "title": "Dimension Reduction for Outlier Detection",
    "description": "A dimension reduction technique for outlier detection. DOBIN: a Distance \n    based Outlier BasIs using Neighbours, constructs a set of basis vectors for outlier \n    detection. This is not an outlier detection method; rather it is a pre-processing \n    method for outlier detection. It brings outliers to the fore-front using fewer basis \n    vectors (Kandanaarachchi, Hyndman 2020) <doi:10.1080/10618600.2020.1807353>. ",
    "version": "1.0.4",
    "maintainer": "Sevvandi Kandanaarachchi <sevvandik@gmail.com>",
    "author": "Sevvandi Kandanaarachchi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0337-0395>)",
    "url": "https://sevvandi.github.io/dobin/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dobin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dobin Dimension Reduction for Outlier Detection A dimension reduction technique for outlier detection. DOBIN: a Distance \n    based Outlier BasIs using Neighbours, constructs a set of basis vectors for outlier \n    detection. This is not an outlier detection method; rather it is a pre-processing \n    method for outlier detection. It brings outliers to the fore-front using fewer basis \n    vectors (Kandanaarachchi, Hyndman 2020) <doi:10.1080/10618600.2020.1807353>.   "
  },
  {
    "id": 11301,
    "package_name": "doc2vec",
    "title": "Distributed Representations of Sentences, Documents and Topics",
    "description": "Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,\n    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. \n    The techniques in the package are detailed in the paper \"Distributed Representations of Sentences and Documents\" by Mikolov et al. (2014), available at <doi:10.48550/arXiv.1405.4053>.\n    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. \n    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.\n    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps\n    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm \n    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense\n    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the \n    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can \n    be found which are representative of the topic.\n    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at <doi:10.48550/arXiv.2008.09470>. ",
    "version": "0.2.2",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  hiyijian [ctb, cph] (Code in src/doc2vec)",
    "url": "https://github.com/bnosac/doc2vec",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=doc2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doc2vec Distributed Representations of Sentences, Documents and Topics Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,\n    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. \n    The techniques in the package are detailed in the paper \"Distributed Representations of Sentences and Documents\" by Mikolov et al. (2014), available at <doi:10.48550/arXiv.1405.4053>.\n    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. \n    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.\n    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps\n    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm \n    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense\n    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the \n    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can \n    be found which are representative of the topic.\n    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at <doi:10.48550/arXiv.2008.09470>.   "
  },
  {
    "id": 11310,
    "package_name": "docore",
    "title": "Utility Functions for Scientific Coding",
    "description": "Basic routines used in scientific coding, such as timing routines, vector/array handing functions and I/O support routines.",
    "version": "1.0",
    "maintainer": "Danail Obreschkow <danail.obreschkow@gmail.com>",
    "author": "Danail Obreschkow",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=docore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "docore Utility Functions for Scientific Coding Basic routines used in scientific coding, such as timing routines, vector/array handing functions and I/O support routines.  "
  },
  {
    "id": 11334,
    "package_name": "dotCall64",
    "title": "Enhanced Foreign Function Interface Supporting Long Vectors",
    "description": "Provides .C64(), which is an enhanced version of .C()\n    and .Fortran() from the foreign function interface. .C64() supports long\n    vectors, arguments of type 64-bit integer, and provides a mechanism to\n    avoid unnecessary copies of read-only and write-only arguments. This\n    makes it a convenient and fast interface to C/C++ and Fortran code.",
    "version": "1.2",
    "maintainer": "Reinhard Furrer <reinhard.furrer@uzh.ch>",
    "author": "Kaspar Moesinger [aut],\n  Florian Gerber [aut] (ORCID: <https://orcid.org/0000-0001-8545-5263>),\n  Reinhard Furrer [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-6319-2332>)",
    "url": "https://git.math.uzh.ch/reinhard.furrer/dotCall64",
    "bug_reports": "https://git.math.uzh.ch/reinhard.furrer/dotCall64/-/issues",
    "repository": "https://cran.r-project.org/package=dotCall64",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dotCall64 Enhanced Foreign Function Interface Supporting Long Vectors Provides .C64(), which is an enhanced version of .C()\n    and .Fortran() from the foreign function interface. .C64() supports long\n    vectors, arguments of type 64-bit integer, and provides a mechanism to\n    avoid unnecessary copies of read-only and write-only arguments. This\n    makes it a convenient and fast interface to C/C++ and Fortran code.  "
  },
  {
    "id": 11420,
    "package_name": "dsmisc",
    "title": "Data Science Box of Pandora Miscellaneous",
    "description": "Tool collection for common and not so common data science use cases. This includes \n    custom made algorithms for data management as well as value calculations that are hard to find \n    elsewhere because of their specificity but would be a waste to get lost nonetheless. \n    Currently available functionality: find\n    sub-graphs in an edge list data.frame, find mode or modes in a vector of values, extract \n    (a) specific regular expression group(s), generate ISO time stamps that play well with \n    file names, or generate URL parameter lists by expanding value combinations. ",
    "version": "0.3.3",
    "maintainer": "Peter Meissner <retep.meissner@gmail.com>",
    "author": "Peter Meissner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dsmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dsmisc Data Science Box of Pandora Miscellaneous Tool collection for common and not so common data science use cases. This includes \n    custom made algorithms for data management as well as value calculations that are hard to find \n    elsewhere because of their specificity but would be a waste to get lost nonetheless. \n    Currently available functionality: find\n    sub-graphs in an edge list data.frame, find mode or modes in a vector of values, extract \n    (a) specific regular expression group(s), generate ISO time stamps that play well with \n    file names, or generate URL parameter lists by expanding value combinations.   "
  },
  {
    "id": 11444,
    "package_name": "dttr2",
    "title": "Manipulate Date, POSIXct and hms Vectors",
    "description": "Manipulates date ('Date'), date time ('POSIXct') and time\n    ('hms') vectors.  Date/times are considered discrete and are floored\n    whenever encountered.  Times are wrapped and time zones are maintained\n    unless explicitly altered by the user.",
    "version": "0.5.2",
    "maintainer": "Ayla Pearson <ayla@poissonconsulting.ca>",
    "author": "Joe Thorley [aut] (ORCID: <https://orcid.org/0000-0002-7683-4592>),\n  Ayla Pearson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7388-1222>),\n  Poisson Consulting [cph, fnd]",
    "url": "https://github.com/poissonconsulting/dttr2,\nhttps://poissonconsulting.github.io/dttr2/",
    "bug_reports": "https://github.com/poissonconsulting/dttr2/issues",
    "repository": "https://cran.r-project.org/package=dttr2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dttr2 Manipulate Date, POSIXct and hms Vectors Manipulates date ('Date'), date time ('POSIXct') and time\n    ('hms') vectors.  Date/times are considered discrete and are floored\n    whenever encountered.  Times are wrapped and time zones are maintained\n    unless explicitly altered by the user.  "
  },
  {
    "id": 11458,
    "package_name": "dummy",
    "title": "Automatic Creation of Dummies with Support for Predictive\nModeling",
    "description": "Efficiently create dummies of all factors and character vectors in a data frame. Support is included for learning the categories on one data set (e.g., a training set) and deploying them on another (e.g., a test set).",
    "version": "0.1.3",
    "maintainer": "Michel Ballings <michel.ballings@GMail.com>",
    "author": "Michel Ballings and Dirk Van den Poel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dummy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dummy Automatic Creation of Dummies with Support for Predictive\nModeling Efficiently create dummies of all factors and character vectors in a data frame. Support is included for learning the categories on one data set (e.g., a training set) and deploying them on another (e.g., a test set).  "
  },
  {
    "id": 11468,
    "package_name": "dvqcc",
    "title": "Dynamic VAR - Based Control Charts for Batch Process Monitoring",
    "description": "A set of control charts for batch processes based on the VAR model. The package contains the implementation of T2.var and W.var control charts based on VAR model coefficients using the couple vectors theory. In each time-instant the VAR coefficients are estimated from a historical in-control dataset and a decision rule is made for online classifying of a new batch data. Those charts allow efficient online monitoring since the very first time-instant. The offline version is available too. In order to evaluate the chart's performance, this package contains functions to generate batch data for offline and online monitoring.See in Danilo Marcondes Filho and Marcio Valk (2020) <doi:10.1016/j.ejor.2019.12.038>. ",
    "version": "0.1.0",
    "maintainer": "Danilo Marcondes Filho <marcondes.filho@ufrgs.br>",
    "author": "M\u00e1rcio Valk [aut],\n  Danilo Marcondes Filho [aut, cre],\n  Gabriela Cybis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dvqcc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dvqcc Dynamic VAR - Based Control Charts for Batch Process Monitoring A set of control charts for batch processes based on the VAR model. The package contains the implementation of T2.var and W.var control charts based on VAR model coefficients using the couple vectors theory. In each time-instant the VAR coefficients are estimated from a historical in-control dataset and a decision rule is made for online classifying of a new batch data. Those charts allow efficient online monitoring since the very first time-instant. The offline version is available too. In order to evaluate the chart's performance, this package contains functions to generate batch data for offline and online monitoring.See in Danilo Marcondes Filho and Marcio Valk (2020) <doi:10.1016/j.ejor.2019.12.038>.   "
  },
  {
    "id": 11507,
    "package_name": "e1071",
    "title": "Misc Functions of the Department of Statistics, Probability\nTheory Group (Formerly: E1071), TU Wien",
    "description": "Functions for latent class analysis, short time Fourier\n\t     transform, fuzzy clustering, support vector machines,\n\t     shortest path computation, bagged clustering, naive Bayes\n\t     classifier, generalized k-nearest neighbour ...",
    "version": "1.7-16",
    "maintainer": "David Meyer <David.Meyer@R-project.org>",
    "author": "David Meyer [aut, cre] (ORCID: <https://orcid.org/0000-0002-5196-3048>),\n  Evgenia Dimitriadou [aut, cph],\n  Kurt Hornik [aut] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  Andreas Weingessel [aut],\n  Friedrich Leisch [aut],\n  Chih-Chung Chang [ctb, cph] (libsvm C++-code),\n  Chih-Chen Lin [ctb, cph] (libsvm C++-code)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=e1071",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "e1071 Misc Functions of the Department of Statistics, Probability\nTheory Group (Formerly: E1071), TU Wien Functions for latent class analysis, short time Fourier\n\t     transform, fuzzy clustering, support vector machines,\n\t     shortest path computation, bagged clustering, naive Bayes\n\t     classifier, generalized k-nearest neighbour ...  "
  },
  {
    "id": 11523,
    "package_name": "eList",
    "title": "List Comprehension and Tools",
    "description": "\n    Create list comprehensions (and other types of comprehension) similar to those in\n    'python', 'haskell', and other languages. List comprehension in 'R' converts a \n    regular for() loop into a vectorized lapply() function. Support for looping \n    with multiple variables, parallelization, and across non-standard objects included. Package \n    also contains a variety of functions to help with list comprehension.",
    "version": "0.2.0",
    "maintainer": "Chris Mann <cmann3@unl.edu>",
    "author": "Chris Mann <cmann3@unl.edu>",
    "url": "",
    "bug_reports": "https://github.com/cmann3/eList/issues",
    "repository": "https://cran.r-project.org/package=eList",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eList List Comprehension and Tools \n    Create list comprehensions (and other types of comprehension) similar to those in\n    'python', 'haskell', and other languages. List comprehension in 'R' converts a \n    regular for() loop into a vectorized lapply() function. Support for looping \n    with multiple variables, parallelization, and across non-standard objects included. Package \n    also contains a variety of functions to help with list comprehension.  "
  },
  {
    "id": 11535,
    "package_name": "earthdatalogin",
    "title": "NASA 'EarthData' Access Utilities",
    "description": "Providing easy, portable access to NASA 'EarthData' products\n  through the use of bearer tokens. Much of NASA's public data catalogs hosted\n  and maintained by its 12 Distributed Active Archive Centers ('DAACs') are\n  now made available on the Amazon Web Services 'S3' storage.  However, \n  accessing this data through the standard 'S3' API is restricted to only to \n  compute resources running inside 'us-west-2' Data Center in Portland, Oregon,\n  which allows NASA to avoid being charged data egress rates. This package\n  provides public access to the data from any networked device by using the \n  'EarthData' login application programming interface (API),\n  <https://www.earthdata.nasa.gov/data/earthdata-login>,\n  providing convenient authentication and access to cloud-hosted NASA 'EarthData'\n  products. This makes access to a wide range of earth observation data from \n  any location straight forward and compatible with R packages that are \n  widely used with cloud native earth observation data (such as 'terra',\n  'sf', etc.)",
    "version": "0.0.3",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Luis L\u00f3pez [aut] (ORCID: <https://orcid.org/0000-0003-4896-3263>),\n  Yuvi Panda [aut],\n  Bri Lind [aut] (ORCID: <https://orcid.org/0000-0002-5306-9963>),\n  Andy Teucher [ctb] (ORCID: <https://orcid.org/0000-0002-7840-692X>),\n  Openscapes [fnd]",
    "url": "https://boettiger-lab.github.io/earthdatalogin/,\nhttps://github.com/boettiger-lab/earthdatalogin",
    "bug_reports": "https://github.com/boettiger-lab/earthdatalogin/issues",
    "repository": "https://cran.r-project.org/package=earthdatalogin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "earthdatalogin NASA 'EarthData' Access Utilities Providing easy, portable access to NASA 'EarthData' products\n  through the use of bearer tokens. Much of NASA's public data catalogs hosted\n  and maintained by its 12 Distributed Active Archive Centers ('DAACs') are\n  now made available on the Amazon Web Services 'S3' storage.  However, \n  accessing this data through the standard 'S3' API is restricted to only to \n  compute resources running inside 'us-west-2' Data Center in Portland, Oregon,\n  which allows NASA to avoid being charged data egress rates. This package\n  provides public access to the data from any networked device by using the \n  'EarthData' login application programming interface (API),\n  <https://www.earthdata.nasa.gov/data/earthdata-login>,\n  providing convenient authentication and access to cloud-hosted NASA 'EarthData'\n  products. This makes access to a wide range of earth observation data from \n  any location straight forward and compatible with R packages that are \n  widely used with cloud native earth observation data (such as 'terra',\n  'sf', etc.)  "
  },
  {
    "id": 11560,
    "package_name": "easylabel",
    "title": "Interactive Scatter Plot and Volcano Plot Labels",
    "description": "Interactive labelling of scatter plots, volcano plots and \n    Manhattan plots using a 'shiny' and 'plotly' interface. Users can hover \n    over points to see where specific points are located and click points \n    on/off to easily label them. Labels can be dragged around the plot to place \n    them optimally. Plots can be exported directly to PDF for publication. For \n    plots with large numbers of points, points can optionally be rasterized as a \n    bitmap, while all other elements (axes, text, labels & lines) are preserved \n    as vector objects. This can dramatically reduce file size for plots with \n    millions of points such as Manhattan plots, and is ideal for publication.",
    "version": "0.3.3",
    "maintainer": "Myles Lewis <myles.lewis@qmul.ac.uk>",
    "author": "Myles Lewis [aut, cre] (ORCID: <https://orcid.org/0000-0001-9365-5345>),\n  Katriona Goldmann [aut] (ORCID:\n    <https://orcid.org/0000-0002-9073-6323>),\n  Cankut Cubuk [ctb] (ORCID: <https://orcid.org/0000-0003-4646-0849>)",
    "url": "https://github.com/myles-lewis/easylabel",
    "bug_reports": "https://github.com/myles-lewis/easylabel/issues",
    "repository": "https://cran.r-project.org/package=easylabel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easylabel Interactive Scatter Plot and Volcano Plot Labels Interactive labelling of scatter plots, volcano plots and \n    Manhattan plots using a 'shiny' and 'plotly' interface. Users can hover \n    over points to see where specific points are located and click points \n    on/off to easily label them. Labels can be dragged around the plot to place \n    them optimally. Plots can be exported directly to PDF for publication. For \n    plots with large numbers of points, points can optionally be rasterized as a \n    bitmap, while all other elements (axes, text, labels & lines) are preserved \n    as vector objects. This can dramatically reduce file size for plots with \n    millions of points such as Manhattan plots, and is ideal for publication.  "
  },
  {
    "id": 11573,
    "package_name": "eatTools",
    "title": "Miscellaneous Functions for the Analysis of Educational\nAssessments",
    "description": "\n   Miscellaneous functions for data cleaning and data analysis of educational assessments. Includes functions for descriptive \n   analyses, character vector manipulations and weighted statistics. Mainly a lightweight dependency for the packages 'eatRep', \n   'eatGADS', 'eatPrep' and 'eatModel' (which will be subsequently submitted to 'CRAN').\n   The function for defining (weighted) contrasts in weighted effect coding refers to\n   te Grotenhuis et al. (2017) <doi:10.1007/s00038-016-0901-1>.\n   Functions for weighted statistics refer to\n   Wolter (2007) <doi:10.1007/978-0-387-35099-8>.",
    "version": "0.7.9",
    "maintainer": "Sebastian Weirich <sebastian.weirich@iqb.hu-berlin.de>",
    "author": "Sebastian Weirich [aut, cre],\n  Martin Hecht [aut],\n  Karoline Sachse [aut],\n  Benjamin Becker [aut],\n  Nicole Mahler [aut],\n  Edna Grewers [ctb]",
    "url": "https://github.com/weirichs/eatTools,\nhttps://weirichs.github.io/eatTools/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eatTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eatTools Miscellaneous Functions for the Analysis of Educational\nAssessments \n   Miscellaneous functions for data cleaning and data analysis of educational assessments. Includes functions for descriptive \n   analyses, character vector manipulations and weighted statistics. Mainly a lightweight dependency for the packages 'eatRep', \n   'eatGADS', 'eatPrep' and 'eatModel' (which will be subsequently submitted to 'CRAN').\n   The function for defining (weighted) contrasts in weighted effect coding refers to\n   te Grotenhuis et al. (2017) <doi:10.1007/s00038-016-0901-1>.\n   Functions for weighted statistics refer to\n   Wolter (2007) <doi:10.1007/978-0-387-35099-8>.  "
  },
  {
    "id": 11654,
    "package_name": "eddington",
    "title": "Compute a Cyclist's Eddington Number",
    "description": "Compute a cyclist's Eddington number, including efficiently\n    computing cumulative E over a vector. A cyclist's Eddington number\n    <https://en.wikipedia.org/wiki/Arthur_Eddington#Eddington_number_for_cycling>\n    is the maximum number satisfying the condition such that a cyclist has\n    ridden E miles or greater on E distinct days. The algorithm in this package\n    is an improvement over the conventional approach because both summary\n    statistics and cumulative statistics can be computed in linear time, since\n    it does not require initial sorting of the data. These functions may also be\n    used for computing h-indices for authors, a metric described by Hirsch (2005)\n    <doi:10.1073/pnas.0507655102>. Both are specific applications of computing\n    the side length of a Durfee square <https://en.wikipedia.org/wiki/Durfee_square>.",
    "version": "4.2.0",
    "maintainer": "Paul Egeler <paulegeler@gmail.com>",
    "author": "Paul Egeler [aut, cre],\n  Tashi Reigle [ctb]",
    "url": "https://github.com/pegeler/eddington2",
    "bug_reports": "https://github.com/pegeler/eddington2/issues",
    "repository": "https://cran.r-project.org/package=eddington",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eddington Compute a Cyclist's Eddington Number Compute a cyclist's Eddington number, including efficiently\n    computing cumulative E over a vector. A cyclist's Eddington number\n    <https://en.wikipedia.org/wiki/Arthur_Eddington#Eddington_number_for_cycling>\n    is the maximum number satisfying the condition such that a cyclist has\n    ridden E miles or greater on E distinct days. The algorithm in this package\n    is an improvement over the conventional approach because both summary\n    statistics and cumulative statistics can be computed in linear time, since\n    it does not require initial sorting of the data. These functions may also be\n    used for computing h-indices for authors, a metric described by Hirsch (2005)\n    <doi:10.1073/pnas.0507655102>. Both are specific applications of computing\n    the side length of a Durfee square <https://en.wikipedia.org/wiki/Durfee_square>.  "
  },
  {
    "id": 11737,
    "package_name": "elevatr",
    "title": "Access Elevation Data from Various APIs",
    "description": "Several web services are available that provide access to elevation\n             data. This package provides access to many of those services and \n             returns elevation data either as an 'sf' simple features object \n             from point elevation services or as a 'raster' object from raster \n             elevation services. In future versions, 'elevatr' will drop \n             support for 'raster' and will instead return 'terra' objects. \n             Currently, the package supports access to the Amazon Web Services \n             Terrain Tiles <https://registry.opendata.aws/terrain-tiles/>, \n             the Open Topography Global Datasets \n             API <https://opentopography.org/developers/>, and the USGS \n             Elevation Point Query Service <https://apps.nationalmap.gov/epqs/>.",
    "version": "0.99.1",
    "maintainer": "Jeffrey Hollister <hollister.jeff@epa.gov>",
    "author": "Jeffrey Hollister [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9254-9740>),\n  Tarak Shah [ctb],\n  Jakub Nowosad [ctb] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\n  Alec L. Robitaille [ctb] (ORCID:\n    <https://orcid.org/0000-0002-4706-1762>),\n  Marcus W. Beck [rev] (ORCID: <https://orcid.org/0000-0002-4996-0059>),\n  Mike Johnson [ctb] (ORCID: <https://orcid.org/0000-0002-5288-8350>)",
    "url": "https://github.com/usepa/elevatr/",
    "bug_reports": "https://github.com/usepa/elevatr/issues/",
    "repository": "https://cran.r-project.org/package=elevatr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elevatr Access Elevation Data from Various APIs Several web services are available that provide access to elevation\n             data. This package provides access to many of those services and \n             returns elevation data either as an 'sf' simple features object \n             from point elevation services or as a 'raster' object from raster \n             elevation services. In future versions, 'elevatr' will drop \n             support for 'raster' and will instead return 'terra' objects. \n             Currently, the package supports access to the Amazon Web Services \n             Terrain Tiles <https://registry.opendata.aws/terrain-tiles/>, \n             the Open Topography Global Datasets \n             API <https://opentopography.org/developers/>, and the USGS \n             Elevation Point Query Service <https://apps.nationalmap.gov/epqs/>.  "
  },
  {
    "id": 11784,
    "package_name": "encode",
    "title": "Represent Ordered Lists and Pairs as Strings",
    "description": "Interconverts between ordered lists and compact string notation.  \n Useful for capturing code lists, and pair-wise codes and decodes, for text storage.\n Analogous to factor levels and labels. Generics encode() and decode()\n perform interconversion, while codes() and decodes() extract components of an encoding.\n The function encoded() checks whether something is interpretable as an encoding.\n If a vector has an encoded 'guide' attribute, as_factor() uses it to coerce to factor.",
    "version": "0.3.6",
    "maintainer": "Tim Bergsma <bergsmat@gmail.com>",
    "author": "Tim Bergsma",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=encode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "encode Represent Ordered Lists and Pairs as Strings Interconverts between ordered lists and compact string notation.  \n Useful for capturing code lists, and pair-wise codes and decodes, for text storage.\n Analogous to factor levels and labels. Generics encode() and decode()\n perform interconversion, while codes() and decodes() extract components of an encoding.\n The function encoded() checks whether something is interpretable as an encoding.\n If a vector has an encoded 'guide' attribute, as_factor() uses it to coerce to factor.  "
  },
  {
    "id": 11801,
    "package_name": "enmSdmX",
    "title": "Species Distribution Modeling and Ecological Niche Modeling",
    "description": "Implements species distribution modeling and ecological niche\n\tmodeling, including: bias correction, spatial cross-validation, model\n\tevaluation, raster interpolation, biotic \"velocity\" (speed and\n\tdirection of movement of a \"mass\" represented by a raster), interpolating\n\tacross a time series of rasters, and use of spatially imprecise records.\n\tThe heart of the package is a set of \"training\" functions which\n\tautomatically optimize model complexity based number of available\n\toccurrences. These algorithms include MaxEnt, MaxNet, boosted regression\n\ttrees/gradient boosting machines, generalized additive models,\n\tgeneralized linear models, natural splines, and random forests. To enhance\n\tinteroperability with other modeling packages, no new classes are created.\n\tThe package works with 'PROJ6' geodetic objects and coordinate reference\n\tsystems.",
    "version": "1.2.12",
    "maintainer": "Adam B. Smith <adam.smith@mobot.org>",
    "author": "Adam B. Smith [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-6420-1659>)",
    "url": "https://github.com/adamlilith/enmSdmX",
    "bug_reports": "https://github.com/adamlilith/enmSdmX/issues",
    "repository": "https://cran.r-project.org/package=enmSdmX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "enmSdmX Species Distribution Modeling and Ecological Niche Modeling Implements species distribution modeling and ecological niche\n\tmodeling, including: bias correction, spatial cross-validation, model\n\tevaluation, raster interpolation, biotic \"velocity\" (speed and\n\tdirection of movement of a \"mass\" represented by a raster), interpolating\n\tacross a time series of rasters, and use of spatially imprecise records.\n\tThe heart of the package is a set of \"training\" functions which\n\tautomatically optimize model complexity based number of available\n\toccurrences. These algorithms include MaxEnt, MaxNet, boosted regression\n\ttrees/gradient boosting machines, generalized additive models,\n\tgeneralized linear models, natural splines, and random forests. To enhance\n\tinteroperability with other modeling packages, no new classes are created.\n\tThe package works with 'PROJ6' geodetic objects and coordinate reference\n\tsystems.  "
  },
  {
    "id": 11820,
    "package_name": "envir",
    "title": "Manage R Environments Better",
    "description": "Provides a small set of functions for managing R environments, with defaults designed to encourage usage patterns that scale well to larger code bases. It provides: import_from(), a flexible way to assign bindings that defaults to the current environment; include(), a vectorized alternative to base::source() that also default to the current environment; and attach_eval() and attach_source(), a way to evaluate expressions in attached environments. Together, these (and other) functions pair to provide a robust alternative to base::library() and base::source().",
    "version": "0.3.0",
    "maintainer": "Tomasz Kalinowski <kalinowskit@gmail.com>",
    "author": "Tomasz Kalinowski [aut, cre]",
    "url": "https://t-kalinowski.github.io/envir/",
    "bug_reports": "https://github.com/t-kalinowski/envir/issues",
    "repository": "https://cran.r-project.org/package=envir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "envir Manage R Environments Better Provides a small set of functions for managing R environments, with defaults designed to encourage usage patterns that scale well to larger code bases. It provides: import_from(), a flexible way to assign bindings that defaults to the current environment; include(), a vectorized alternative to base::source() that also default to the current environment; and attach_eval() and attach_source(), a way to evaluate expressions in attached environments. Together, these (and other) functions pair to provide a robust alternative to base::library() and base::source().  "
  },
  {
    "id": 11821,
    "package_name": "envirem",
    "title": "Generation of ENVIREM Variables",
    "description": "Generation of bioclimatic rasters that are complementary to the typical 19 bioclim variables.  ",
    "version": "3.1",
    "maintainer": "Pascal Title <pascal.title@stonybrook.edu>",
    "author": "Pascal O. Title [aut],\n  Jordan B. Bemmels [aut],\n  Pascal Title [cre]",
    "url": "https://github.com/ptitle/envirem",
    "bug_reports": "https://github.com/ptitle/envirem/issues",
    "repository": "https://cran.r-project.org/package=envirem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "envirem Generation of ENVIREM Variables Generation of bioclimatic rasters that are complementary to the typical 19 bioclim variables.    "
  },
  {
    "id": 11866,
    "package_name": "epm",
    "title": "EcoPhyloMapper",
    "description": "Facilitates the aggregation of species' geographic ranges from vector or raster spatial data, and that enables the calculation of various morphological and phylogenetic community metrics across geography. Citation: Title, PO, DL Swiderski and ML Zelditch (2022) <doi:10.1111/2041-210X.13914>. ",
    "version": "1.1.6",
    "maintainer": "Pascal Title <pascal.title@stonybrook.edu>",
    "author": "Pascal Title [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6316-0736>),\n  Donald Swiderski [aut],\n  Miriam Zelditch [aut]",
    "url": "https://github.com/ptitle/epm",
    "bug_reports": "https://github.com/ptitle/epm/issues",
    "repository": "https://cran.r-project.org/package=epm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epm EcoPhyloMapper Facilitates the aggregation of species' geographic ranges from vector or raster spatial data, and that enables the calculation of various morphological and phylogenetic community metrics across geography. Citation: Title, PO, DL Swiderski and ML Zelditch (2022) <doi:10.1111/2041-210X.13914>.   "
  },
  {
    "id": 11875,
    "package_name": "epsiwal",
    "title": "Exact Post Selection Inference with Applications to the Lasso",
    "description": "Implements the conditional estimation procedure of\n  Lee, Sun, Sun and Taylor (2016) <doi:10.1214/15-AOS1371>.\n  This procedure allows hypothesis testing on the mean of\n  a normal random vector subject to linear constraints.",
    "version": "0.1.0",
    "maintainer": "Steven E. Pav <shabbychef@gmail.com>",
    "author": "Steven E. Pav [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4197-6195>)",
    "url": "https://github.com/shabbychef/epsiwal",
    "bug_reports": "https://github.com/shabbychef/epsiwal/issues",
    "repository": "https://cran.r-project.org/package=epsiwal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epsiwal Exact Post Selection Inference with Applications to the Lasso Implements the conditional estimation procedure of\n  Lee, Sun, Sun and Taylor (2016) <doi:10.1214/15-AOS1371>.\n  This procedure allows hypothesis testing on the mean of\n  a normal random vector subject to linear constraints.  "
  },
  {
    "id": 11893,
    "package_name": "era",
    "title": "Year-Based Time Scales",
    "description": "Provides a consistent representation of year-based time scales as a\n    numeric vector with an associated 'era'. There are built-in era definitions\n    for many year numbering systems used in contemporary and historic calendars \n    (e.g. Common Era, Islamic 'Hijri' years); year-based time scales used in \n    archaeology, astronomy, geology, and other palaeosciences (e.g. \n    Before Present, SI-prefixed 'annus'); and support for arbitrary user-defined\n    eras. Years can converted from any one era to another using a generalised \n    transformation function. Methods are also provided for robust casting and \n    coercion between years and other numeric types, type-stable arithmetic with \n    years, and pretty-printing in tables.",
    "version": "0.5.0",
    "maintainer": "Joe Roe <joe@joeroe.io>",
    "author": "Joe Roe [aut, cre] (ORCID: <https://orcid.org/0000-0002-1011-1244>)",
    "url": "https://era.joeroe.io, https://github.com/joeroe/era",
    "bug_reports": "https://github.com/joeroe/era/issues",
    "repository": "https://cran.r-project.org/package=era",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "era Year-Based Time Scales Provides a consistent representation of year-based time scales as a\n    numeric vector with an associated 'era'. There are built-in era definitions\n    for many year numbering systems used in contemporary and historic calendars \n    (e.g. Common Era, Islamic 'Hijri' years); year-based time scales used in \n    archaeology, astronomy, geology, and other palaeosciences (e.g. \n    Before Present, SI-prefixed 'annus'); and support for arbitrary user-defined\n    eras. Years can converted from any one era to another using a generalised \n    transformation function. Methods are also provided for robust casting and \n    coercion between years and other numeric types, type-stable arithmetic with \n    years, and pretty-printing in tables.  "
  },
  {
    "id": 11913,
    "package_name": "errors",
    "title": "Uncertainty Propagation for R Vectors",
    "description": "Support for measurement errors in R vectors, matrices and arrays:\n    automatic uncertainty propagation and reporting.\n    Documentation about 'errors' is provided in the paper by Ucar, Pebesma &\n    Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in this package as a\n    vignette; see 'citation(\"errors\")' for details.",
    "version": "0.4.4",
    "maintainer": "I\u00f1aki Ucar <iucar@fedoraproject.org>",
    "author": "I\u00f1aki Ucar [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-6403-5550>),\n  Lionel Henry [ctb],\n  RStudio [cph] (Copyright for code written by RStudio employees.)",
    "url": "https://r-quantities.github.io/errors/,\nhttps://github.com/r-quantities/errors",
    "bug_reports": "https://github.com/r-quantities/errors/issues",
    "repository": "https://cran.r-project.org/package=errors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "errors Uncertainty Propagation for R Vectors Support for measurement errors in R vectors, matrices and arrays:\n    automatic uncertainty propagation and reporting.\n    Documentation about 'errors' is provided in the paper by Ucar, Pebesma &\n    Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in this package as a\n    vignette; see 'citation(\"errors\")' for details.  "
  },
  {
    "id": 11935,
    "package_name": "essentials",
    "title": "Essential Functions not Included in Base R",
    "description": "Functions for converting objects to scalars (vectors of length 1) \n    and a more inclusive definition of data that can be interpreted as numbers \n    (numeric and complex alike).",
    "version": "0.1.0",
    "maintainer": "Andrew Simmons <akwsimmo@gmail.com>",
    "author": "Andrew Simmons",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=essentials",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "essentials Essential Functions not Included in Base R Functions for converting objects to scalars (vectors of length 1) \n    and a more inclusive definition of data that can be interpreted as numbers \n    (numeric and complex alike).  "
  },
  {
    "id": 11938,
    "package_name": "estats",
    "title": "Fast and Light-Weight Energy Statistics",
    "description": "Fast and memory-less computation of the energy statistics related quantities for vectors and matrices. References include: Szekely G. J. and Rizzo M. L. (2014), <doi:10.1214/14-AOS1255>. Szekely G. J. and Rizzo M. L. (2023), <ISBN:9781482242744>. Tsagris M. and Papadakis M. (2025). <doi:10.48550/arXiv.2501.02849>.",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Manos Papadakis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=estats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "estats Fast and Light-Weight Energy Statistics Fast and memory-less computation of the energy statistics related quantities for vectors and matrices. References include: Szekely G. J. and Rizzo M. L. (2014), <doi:10.1214/14-AOS1255>. Szekely G. J. and Rizzo M. L. (2023), <ISBN:9781482242744>. Tsagris M. and Papadakis M. (2025). <doi:10.48550/arXiv.2501.02849>.  "
  },
  {
    "id": 12025,
    "package_name": "exactextractr",
    "title": "Fast Extraction from Raster Datasets using Polygons",
    "description": "Quickly and accurately summarizes raster values over polygonal areas (\"zonal statistics\").",
    "version": "0.10.1",
    "maintainer": "Daniel Baston <dbaston@isciences.com>",
    "author": "Daniel Baston [aut, cre],\n  ISciences, LLC [cph]",
    "url": "https://isciences.gitlab.io/exactextractr/,\nhttps://github.com/isciences/exactextractr",
    "bug_reports": "https://github.com/isciences/exactextractr/issues",
    "repository": "https://cran.r-project.org/package=exactextractr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exactextractr Fast Extraction from Raster Datasets using Polygons Quickly and accurately summarizes raster values over polygonal areas (\"zonal statistics\").  "
  },
  {
    "id": 12050,
    "package_name": "expandFunctions",
    "title": "Feature Matrix Builder",
    "description": "Generates feature matrix outputs from R object inputs\n    using a variety of expansion functions.  The generated\n    feature matrices have applications as inputs\n    for a variety of machine learning algorithms.\n    The expansion functions are based on coercing the input\n    to a matrix, treating the columns as features and\n    converting individual columns or combinations into blocks of\n    columns.\n    Currently these include expansion of columns by\n    efficient sparse embedding by vectors of lags,\n    quadratic expansion into squares and unique products,\n    powers by vectors of degree,\n    vectors of orthogonal polynomials functions,\n    and block random affine projection transformations (RAPTs).\n    The transformations are\n    magrittr- and cbind-friendly, and can be used in a\n    building block fashion.  For instance, taking the cos() of\n    the output of the RAPT transformation generates a\n    stationary kernel expansion via Bochner's theorem, and this\n    expansion can then be cbind-ed with other features.\n    Additionally, there are utilities for replacing features,\n    removing rows with NAs,\n    creating matrix samples of a given distribution,\n    a simple wrapper for LASSO with CV,\n    a Freeman-Tukey transform,\n    generalizations of the outer function,\n    matrix size-preserving discrete difference by row,\n    plotting, etc.",
    "version": "0.1.0",
    "maintainer": "Scott Miller <sam3CRAN@gmail.com>",
    "author": "Scott Miller [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=expandFunctions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "expandFunctions Feature Matrix Builder Generates feature matrix outputs from R object inputs\n    using a variety of expansion functions.  The generated\n    feature matrices have applications as inputs\n    for a variety of machine learning algorithms.\n    The expansion functions are based on coercing the input\n    to a matrix, treating the columns as features and\n    converting individual columns or combinations into blocks of\n    columns.\n    Currently these include expansion of columns by\n    efficient sparse embedding by vectors of lags,\n    quadratic expansion into squares and unique products,\n    powers by vectors of degree,\n    vectors of orthogonal polynomials functions,\n    and block random affine projection transformations (RAPTs).\n    The transformations are\n    magrittr- and cbind-friendly, and can be used in a\n    building block fashion.  For instance, taking the cos() of\n    the output of the RAPT transformation generates a\n    stationary kernel expansion via Bochner's theorem, and this\n    expansion can then be cbind-ed with other features.\n    Additionally, there are utilities for replacing features,\n    removing rows with NAs,\n    creating matrix samples of a given distribution,\n    a simple wrapper for LASSO with CV,\n    a Freeman-Tukey transform,\n    generalizations of the outer function,\n    matrix size-preserving discrete difference by row,\n    plotting, etc.  "
  },
  {
    "id": 12063,
    "package_name": "export",
    "title": "Streamlined Export of Graphs and Data Tables",
    "description": "Easily export 'R' graphs and statistical output to 'Microsoft\n    Office' / 'LibreOffice', 'Latex' and 'HTML' Documents, using sensible defaults\n    that result in publication-quality output with simple, straightforward commands.\n    Output to 'Microsoft Office' is in editable 'DrawingML' vector format for\n    graphs, and can use corporate template documents for styling. This enables\n    the production of standardized reports and also allows for manual tidy-up\n    of the layout of 'R' graphs in 'Powerpoint' before final publication. Export\n    of graphs is flexible, and functions enable the currently showing R graph\n    or the currently showing 'R' stats object to be exported, but also allow the\n    graphical or tabular output to be passed as objects. The package relies on package\n    'officer' for export to 'Office' documents,and output files are also fully compatible\n    with 'LibreOffice'. Base 'R', 'ggplot2' and 'lattice' plots are supported, as\n    well as a wide variety of 'R' stats objects, via wrappers to xtable(), broom::tidy() \n    and stargazer(), including aov(), lm(), glm(), lme(), glmnet() and coxph() as\n    well as matrices and data frames and many more...",
    "version": "0.3.2",
    "maintainer": "Tom Wenseleers <tom.wenseleers@kuleuven.be>",
    "author": "Tom Wenseleers [aut, cre],\n  Christophe Vanderaa [aut]",
    "url": "",
    "bug_reports": "https://github.com/tomwenseleers/export/issues",
    "repository": "https://cran.r-project.org/package=export",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "export Streamlined Export of Graphs and Data Tables Easily export 'R' graphs and statistical output to 'Microsoft\n    Office' / 'LibreOffice', 'Latex' and 'HTML' Documents, using sensible defaults\n    that result in publication-quality output with simple, straightforward commands.\n    Output to 'Microsoft Office' is in editable 'DrawingML' vector format for\n    graphs, and can use corporate template documents for styling. This enables\n    the production of standardized reports and also allows for manual tidy-up\n    of the layout of 'R' graphs in 'Powerpoint' before final publication. Export\n    of graphs is flexible, and functions enable the currently showing R graph\n    or the currently showing 'R' stats object to be exported, but also allow the\n    graphical or tabular output to be passed as objects. The package relies on package\n    'officer' for export to 'Office' documents,and output files are also fully compatible\n    with 'LibreOffice'. Base 'R', 'ggplot2' and 'lattice' plots are supported, as\n    well as a wide variety of 'R' stats objects, via wrappers to xtable(), broom::tidy() \n    and stargazer(), including aov(), lm(), glm(), lme(), glmnet() and coxph() as\n    well as matrices and data frames and many more...  "
  },
  {
    "id": 12101,
    "package_name": "eyeris",
    "title": "Flexible, Extensible, & Reproducible Pupillometry Preprocessing",
    "description": "Pupillometry offers a non-invasive window into the mind and has been used extensively as a psychophysiological readout of arousal signals linked with cognitive processes like attention, stress, and emotional states [Clewett et al. (2020) <doi:10.1038/s41467-020-17851-9>; Kret & Sjak-Shie (2018) <doi:10.3758/s13428-018-1075-y>; Strauch (2024) <doi:10.1016/j.tins.2024.06.002>]. Yet, despite decades of pupillometry research, many established packages and workflows to date lack design patterns based on Findability, Accessibility, Interoperability, and Reusability (FAIR) principles [see Wilkinson et al. (2016) <doi:10.1038/sdata.2016.18>]. 'eyeris' provides a modular, performant, and extensible preprocessing framework for pupillometry data with BIDS-like organization and interactive output reports [Esteban et al. (2019) <doi:10.1038/s41592-018-0235-4>; Gorgolewski et al. (2016) <doi:10.1038/sdata.2016.44>]. Development was supported, in part, by the Stanford Wu Tsai Human Performance Alliance, Stanford Ric Weiland Graduate Fellowship, Stanford Center for Mind, Brain, Computation and Technology, NIH National Institute on Aging Grants (R01-AG065255, R01-AG079345), NSF GRFP (DGE-2146755), McKnight Brain Research Foundation Clinical Translational Research Scholarship in Cognitive Aging and Age-Related Memory Loss, American Brain Foundation, and the American Academy of Neurology.",
    "version": "3.0.1",
    "maintainer": "Shawn Schwartz <shawn.t.schwartz@gmail.com>",
    "author": "Shawn Schwartz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6444-8451>),\n  Mingjian He [ctb],\n  Haopei Yang [ctb],\n  Alice Xue [ctb],\n  Gustavo Santiago-Reyes [ctb]",
    "url": "https://shawnschwartz.com/eyeris/,\nhttps://github.com/shawntz/eyeris/",
    "bug_reports": "https://github.com/shawntz/eyeris/issues",
    "repository": "https://cran.r-project.org/package=eyeris",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eyeris Flexible, Extensible, & Reproducible Pupillometry Preprocessing Pupillometry offers a non-invasive window into the mind and has been used extensively as a psychophysiological readout of arousal signals linked with cognitive processes like attention, stress, and emotional states [Clewett et al. (2020) <doi:10.1038/s41467-020-17851-9>; Kret & Sjak-Shie (2018) <doi:10.3758/s13428-018-1075-y>; Strauch (2024) <doi:10.1016/j.tins.2024.06.002>]. Yet, despite decades of pupillometry research, many established packages and workflows to date lack design patterns based on Findability, Accessibility, Interoperability, and Reusability (FAIR) principles [see Wilkinson et al. (2016) <doi:10.1038/sdata.2016.18>]. 'eyeris' provides a modular, performant, and extensible preprocessing framework for pupillometry data with BIDS-like organization and interactive output reports [Esteban et al. (2019) <doi:10.1038/s41592-018-0235-4>; Gorgolewski et al. (2016) <doi:10.1038/sdata.2016.44>]. Development was supported, in part, by the Stanford Wu Tsai Human Performance Alliance, Stanford Ric Weiland Graduate Fellowship, Stanford Center for Mind, Brain, Computation and Technology, NIH National Institute on Aging Grants (R01-AG065255, R01-AG079345), NSF GRFP (DGE-2146755), McKnight Brain Research Foundation Clinical Translational Research Scholarship in Cognitive Aging and Age-Related Memory Loss, American Brain Foundation, and the American Academy of Neurology.  "
  },
  {
    "id": 12158,
    "package_name": "factor256",
    "title": "Use Raw Vectors to Minimize Memory Consumption of Factors",
    "description": "Uses raw vectors to minimize memory consumption of categorical \n    variables with fewer than 256 unique values. Useful for analysis of large\n    datasets involving variables such as age, years, states, countries, or\n    education levels.",
    "version": "0.1.0",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=factor256",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "factor256 Use Raw Vectors to Minimize Memory Consumption of Factors Uses raw vectors to minimize memory consumption of categorical \n    variables with fewer than 256 unique values. Useful for analysis of large\n    datasets involving variables such as age, years, states, countries, or\n    education levels.  "
  },
  {
    "id": 12177,
    "package_name": "fairsubset",
    "title": "Choose Representative Subsets",
    "description": "Allows user to obtain subsets of columns of data or vectors within a list.  These subsets will match the original data in terms of average and variation, but have a consistent length of data per column.  It is intended for use on automated data generation which may not always output the same N per replicate or sample.",
    "version": "1.0",
    "maintainer": "Joe Delaney <delaneyj@musc.edu>",
    "author": "Joe Delaney",
    "url": "https://pubmed.ncbi.nlm.nih.gov/31583263/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fairsubset",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fairsubset Choose Representative Subsets Allows user to obtain subsets of columns of data or vectors within a list.  These subsets will match the original data in terms of average and variation, but have a consistent length of data per column.  It is intended for use on automated data generation which may not always output the same N per replicate or sample.  "
  },
  {
    "id": 12178,
    "package_name": "fake",
    "title": "Flexible Data Simulation Using the Multivariate Normal\nDistribution",
    "description": "This R package can be used to generate artificial data conditionally on pre-specified (simulated or user-defined) relationships between the variables and/or observations. Each observation is drawn from a multivariate Normal distribution where the mean vector and covariance matrix reflect the desired relationships. Outputs can be used to evaluate the performances of variable selection, graphical modelling, or clustering approaches by comparing the true and estimated structures (B Bodinier et al (2021) <arXiv:2106.02521>).",
    "version": "1.4.0",
    "maintainer": "Barbara Bodinier <barbara.bodinier@gmail.com>",
    "author": "Barbara Bodinier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fake",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fake Flexible Data Simulation Using the Multivariate Normal\nDistribution This R package can be used to generate artificial data conditionally on pre-specified (simulated or user-defined) relationships between the variables and/or observations. Each observation is drawn from a multivariate Normal distribution where the mean vector and covariance matrix reflect the desired relationships. Outputs can be used to evaluate the performances of variable selection, graphical modelling, or clustering approaches by comparing the true and estimated structures (B Bodinier et al (2021) <arXiv:2106.02521>).  "
  },
  {
    "id": 12184,
    "package_name": "fam.recrisk",
    "title": "Familial Recurrence Risk",
    "description": "Given vectors of family sizes and number of affecteds per family, calculates the risk of disease recurrence in an unaffected person, conditional on a family having at least k affected members. Methods also model heterogeneity of disease risk across families by fitting a mixture model, allowing for high and low risk families.",
    "version": "0.1",
    "maintainer": "Jason Sinnwell <sinnwell.jason@mayo.edu>",
    "author": "Schaid Daniel [aut],\n  Jason Sinnwell [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fam.recrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fam.recrisk Familial Recurrence Risk Given vectors of family sizes and number of affecteds per family, calculates the risk of disease recurrence in an unaffected person, conditional on a family having at least k affected members. Methods also model heterogeneity of disease risk across families by fitting a mixture model, allowing for high and low risk families.  "
  },
  {
    "id": 12223,
    "package_name": "fastText",
    "title": "Efficient Learning of Word Representations and Sentence\nClassification",
    "description": "An interface to the 'fastText' <https://github.com/facebookresearch/fastText> library for efficient learning of word representations and sentence classification. The 'fastText' algorithm is explained in detail in (i) \"Enriching Word Vectors with subword Information\", Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, 2017, <doi:10.1162/tacl_a_00051>; (ii) \"Bag of Tricks for Efficient Text Classification\", Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2017, <doi:10.18653/v1/e17-2068>; (iii) \"FastText.zip: Compressing text classification models\", Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, Tomas Mikolov, 2016, <arXiv:1612.03651>.",
    "version": "1.0.4",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>),\n  Facebook Inc [cph]",
    "url": "https://github.com/mlampros/fastText",
    "bug_reports": "https://github.com/mlampros/fastText/issues",
    "repository": "https://cran.r-project.org/package=fastText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastText Efficient Learning of Word Representations and Sentence\nClassification An interface to the 'fastText' <https://github.com/facebookresearch/fastText> library for efficient learning of word representations and sentence classification. The 'fastText' algorithm is explained in detail in (i) \"Enriching Word Vectors with subword Information\", Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, 2017, <doi:10.1162/tacl_a_00051>; (ii) \"Bag of Tricks for Efficient Text Classification\", Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2017, <doi:10.18653/v1/e17-2068>; (iii) \"FastText.zip: Compressing text classification models\", Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, Tomas Mikolov, 2016, <arXiv:1612.03651>.  "
  },
  {
    "id": 12224,
    "package_name": "fastTextR",
    "title": "An Interface to the 'fastText' Library",
    "description": "An interface to the 'fastText' library\n\t<https://github.com/facebookresearch/fastText>. The package\n\tcan be used for text classification and to learn word vectors.\n\tAn example how to use 'fastTextR' can be found in the 'README' file.",
    "version": "2.1.0",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "author": "Florian Schwendinger [aut],\n  Emil Hvitfeldt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0679-1945>)",
    "url": "https://github.com/EmilHvitfeldt/fastTextR",
    "bug_reports": "https://github.com/EmilHvitfeldt/fastTextR/issues",
    "repository": "https://cran.r-project.org/package=fastTextR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastTextR An Interface to the 'fastText' Library An interface to the 'fastText' library\n\t<https://github.com/facebookresearch/fastText>. The package\n\tcan be used for text classification and to learn word vectors.\n\tAn example how to use 'fastTextR' can be found in the 'README' file.  "
  },
  {
    "id": 12232,
    "package_name": "fastbioclim",
    "title": "Scalable and Efficient Derivation of Bioclimatic Variables",
    "description": "Provides a high-performance framework for deriving bioclimatic and custom summary variables from \n    large-scale climate raster data. The package features a dual-backend architecture that intelligently switches \n    between fast in-memory processing for smaller datasets (via the 'terra' package) and a memory-safe tiled approach \n    for massive datasets that do not fit in RAM (via 'exactextractr' and 'Rfast'). The main functions, \n    derive_bioclim() and derive_statistics(), offer a unified interface with advanced options for \n    custom time periods and static indices, making it suitable for a wide range of ecological and \n    environmental modeling applications. A software note is in preparation. In the meantime, you can visit \n    the package website <https://gepinillab.github.io/fastbioclim/> to find tutorials in English and Spanish.",
    "version": "0.3.0",
    "maintainer": "Gonzalo E. Pinilla-Buitrago <gepinillab@gmail.com>",
    "author": "Gonzalo E. Pinilla-Buitrago [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0065-945X>),\n  Luis Osorio-Olvera [aut] (ORCID:\n    <https://orcid.org/0000-0003-0701-5398>)",
    "url": "https://gepinillab.github.io/fastbioclim/",
    "bug_reports": "https://github.com/gepinillab/fastbioclim/issues",
    "repository": "https://cran.r-project.org/package=fastbioclim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastbioclim Scalable and Efficient Derivation of Bioclimatic Variables Provides a high-performance framework for deriving bioclimatic and custom summary variables from \n    large-scale climate raster data. The package features a dual-backend architecture that intelligently switches \n    between fast in-memory processing for smaller datasets (via the 'terra' package) and a memory-safe tiled approach \n    for massive datasets that do not fit in RAM (via 'exactextractr' and 'Rfast'). The main functions, \n    derive_bioclim() and derive_statistics(), offer a unified interface with advanced options for \n    custom time periods and static indices, making it suitable for a wide range of ecological and \n    environmental modeling applications. A software note is in preparation. In the meantime, you can visit \n    the package website <https://gepinillab.github.io/fastbioclim/> to find tutorials in English and Spanish.  "
  },
  {
    "id": 12233,
    "package_name": "fastcluster",
    "title": "Fast Hierarchical Clustering Routines for R and 'Python'",
    "description": "This is a two-in-one package which provides interfaces to\n        both R and 'Python'. It implements fast hierarchical, agglomerative\n        clustering routines. Part of the functionality is designed as drop-in\n        replacement for existing routines: linkage() in the 'SciPy' package\n        'scipy.cluster.hierarchy', hclust() in R's 'stats' package, and the\n        'flashClust' package. It provides the same functionality with the\n        benefit of a much faster implementation. Moreover, there are\n        memory-saving routines for clustering of vector data, which go beyond\n        what the existing packages provide. For information on how to install\n        the 'Python' files, see the file INSTALL in the source distribution.\n        Based on the present package, Christoph Dalitz also wrote a pure 'C++'\n        interface to 'fastcluster':\n        <https://lionel.kr.hs-niederrhein.de/~dalitz/data/hclust/>.",
    "version": "1.3.0",
    "maintainer": "Daniel M\u00fcllner <daniel@danifold.net>",
    "author": "Daniel M\u00fcllner [aut, cph, cre],\n  Google Inc. [cph]",
    "url": "https://danifold.net/fastcluster.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fastcluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastcluster Fast Hierarchical Clustering Routines for R and 'Python' This is a two-in-one package which provides interfaces to\n        both R and 'Python'. It implements fast hierarchical, agglomerative\n        clustering routines. Part of the functionality is designed as drop-in\n        replacement for existing routines: linkage() in the 'SciPy' package\n        'scipy.cluster.hierarchy', hclust() in R's 'stats' package, and the\n        'flashClust' package. It provides the same functionality with the\n        benefit of a much faster implementation. Moreover, there are\n        memory-saving routines for clustering of vector data, which go beyond\n        what the existing packages provide. For information on how to install\n        the 'Python' files, see the file INSTALL in the source distribution.\n        Based on the present package, Christoph Dalitz also wrote a pure 'C++'\n        interface to 'fastcluster':\n        <https://lionel.kr.hs-niederrhein.de/~dalitz/data/hclust/>.  "
  },
  {
    "id": 12241,
    "package_name": "fasterRaster",
    "title": "Faster Raster and Spatial Vector Processing Using 'GRASS'",
    "description": "Processing of large-in-memory/large-on disk rasters and spatial\n\tvectors using 'GRASS' <https://grass.osgeo.org/>. Most functions in the\n\t'terra' package are\trecreated. Processing of medium-sized and smaller\n\tspatial objects will nearly always be faster using 'terra' or 'sf', but\n\tfor large-in-memory/large-on-disk objects, 'fasterRaster' may be faster.\n\tTo use most of the functions, you must have the stand-alone version (not\n\tthe 'OSGeoW4' installer version) of 'GRASS' 8.0 or higher.",
    "version": "8.4.1.1",
    "maintainer": "Adam B. Smith <adam.smith@mobot.org>",
    "author": "Adam B. Smith [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-6420-1659>)",
    "url": "https://github.com/adamlilith/fasterRaster,\nhttps://adamlilith.github.io/fasterRaster/",
    "bug_reports": "https://github.com/adamlilith/fasterRaster/issues",
    "repository": "https://cran.r-project.org/package=fasterRaster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fasterRaster Faster Raster and Spatial Vector Processing Using 'GRASS' Processing of large-in-memory/large-on disk rasters and spatial\n\tvectors using 'GRASS' <https://grass.osgeo.org/>. Most functions in the\n\t'terra' package are\trecreated. Processing of medium-sized and smaller\n\tspatial objects will nearly always be faster using 'terra' or 'sf', but\n\tfor large-in-memory/large-on-disk objects, 'fasterRaster' may be faster.\n\tTo use most of the functions, you must have the stand-alone version (not\n\tthe 'OSGeoW4' installer version) of 'GRASS' 8.0 or higher.  "
  },
  {
    "id": 12242,
    "package_name": "fasterize",
    "title": "Fast Polygon to Raster Conversion",
    "description": "Provides a drop-in replacement for rasterize() from the 'raster'\n   package that takes polygon vector or data frame objects, and is much faster. \n   There is support for the main options provided by the rasterize() function, \n   including setting the field used and background value, and options for \n   aggregating multi-layer rasters. Uses the scan line algorithm attributed to\n   Wylie et al. (1967) <doi:10.1145/1465611.1465619>.",
    "version": "1.1.0",
    "maintainer": "Michael Sumner <mdsumner@gmail.com>",
    "author": "Noam Ross [aut] (Original author, ORCID:\n    <https://orcid.org/0000-0002-2136-0000>),\n  Michael Sumner [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-2471-7511>),\n  Jeroen Ooms [ctb],\n  Antoine Stevens [ctb],\n  EcoHealth Alliance [cph],\n  USAID PREDICT [fnd]",
    "url": "https://github.com/ecohealthalliance/fasterize",
    "bug_reports": "https://github.com/ecohealthalliance/fasterize/issues",
    "repository": "https://cran.r-project.org/package=fasterize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fasterize Fast Polygon to Raster Conversion Provides a drop-in replacement for rasterize() from the 'raster'\n   package that takes polygon vector or data frame objects, and is much faster. \n   There is support for the main options provided by the rasterize() function, \n   including setting the field used and background value, and options for \n   aggregating multi-layer rasters. Uses the scan line algorithm attributed to\n   Wylie et al. (1967) <doi:10.1145/1465611.1465619>.  "
  },
  {
    "id": 12243,
    "package_name": "fastfocal",
    "title": "Fast Multiscale Raster Extraction and Moving Window Analysis\nwith FFT",
    "description": "Provides fast moving-window (\"focal\") and buffer-based extraction \n    for raster data using the 'terra' package. Automatically selects between \n    a 'C++' backend (via 'terra') and a Fast Fourier Transform (FFT) backend \n    depending on problem size. The FFT backend supports sum and mean, while \n    other statistics (e.g., median, min, max, standard deviation) are handled \n    by the 'terra' backend. Supports multiple kernel types (e.g., circle, \n    rectangle, gaussian), with NA handling consistent with 'terra' via \n    'na.rm' and 'na.policy'. Operates on 'SpatRaster' objects and returns \n    results with the same geometry.",
    "version": "0.1.3",
    "maintainer": "Ho Yi Wan <hoyiwan@gmail.com>",
    "author": "Ho Yi Wan [aut, cre] (ORCID: <https://orcid.org/0000-0002-2146-8257>)",
    "url": "https://hoyiwan.github.io/fastfocal/,\nhttps://github.com/hoyiwan/fastfocal,\nhttps://doi.org/10.5281/zenodo.17074691",
    "bug_reports": "https://github.com/hoyiwan/fastfocal/issues",
    "repository": "https://cran.r-project.org/package=fastfocal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastfocal Fast Multiscale Raster Extraction and Moving Window Analysis\nwith FFT Provides fast moving-window (\"focal\") and buffer-based extraction \n    for raster data using the 'terra' package. Automatically selects between \n    a 'C++' backend (via 'terra') and a Fast Fourier Transform (FFT) backend \n    depending on problem size. The FFT backend supports sum and mean, while \n    other statistics (e.g., median, min, max, standard deviation) are handled \n    by the 'terra' backend. Supports multiple kernel types (e.g., circle, \n    rectangle, gaussian), with NA handling consistent with 'terra' via \n    'na.rm' and 'na.policy'. Operates on 'SpatRaster' objects and returns \n    results with the same geometry.  "
  },
  {
    "id": 12255,
    "package_name": "fastpng",
    "title": "Read and Write PNG Files with Configurable Decoder/Encoder\nOptions",
    "description": "Read and write PNG images with arrays, rasters, native\n    rasters, numeric arrays, integer arrays, raw vectors and indexed\n    values.  This PNG encoder exposes configurable internal options\n    enabling the user to select a speed-size tradeoff.  For example,\n    disabling compression can speed up writing PNG by a factor of 50.\n    Multiple image formats are supported including raster, native rasters,\n    and integer and numeric arrays at color depths of 1, 2, 3 or 4. 16-bit\n    images are also supported. This implementation uses the 'libspng' 'C'\n    library which is available from\n    <https://github.com/randy408/libspng/>.",
    "version": "0.1.7",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph],\n  Randy408 [aut, cph] (Author of bundled libspng),\n  The PNG Reference Library Authors [aut, cph],\n  Cosmin Truta [cph] (SSE2 optimised filter functions, NEON optimised\n    filter functions, NEON optimised palette expansion functions),\n  Glenn Randers-Pehrson [cph] (SSE2 optimised filter functions),\n  Andreas Dilger [cph],\n  Guy Eric Schalnat [cph],\n  Mike Klein [ctb] (SSE2 optimised filter functions),\n  Matt Sarett [ctb] (SSE2 optimised filter functions),\n  James Yu [ctb] (NEON optimised filter functions),\n  Mars Rullgard [ctb] (NEON optimised filter functions),\n  Arm Holdings [cph] (NEON optimised palette expansion functions),\n  Richard Townsend [ctb] (NEON optimised palette expansion functions)",
    "url": "https://github.com/coolbutuseless/fastpng",
    "bug_reports": "https://github.com/coolbutuseless/fastpng/issues",
    "repository": "https://cran.r-project.org/package=fastpng",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastpng Read and Write PNG Files with Configurable Decoder/Encoder\nOptions Read and write PNG images with arrays, rasters, native\n    rasters, numeric arrays, integer arrays, raw vectors and indexed\n    values.  This PNG encoder exposes configurable internal options\n    enabling the user to select a speed-size tradeoff.  For example,\n    disabling compression can speed up writing PNG by a factor of 50.\n    Multiple image formats are supported including raster, native rasters,\n    and integer and numeric arrays at color depths of 1, 2, 3 or 4. 16-bit\n    images are also supported. This implementation uses the 'libspng' 'C'\n    library which is available from\n    <https://github.com/randy408/libspng/>.  "
  },
  {
    "id": 12296,
    "package_name": "fctutils",
    "title": "Advanced Factor Manipulation Utilities",
    "description": "Provides a collection of utility functions for manipulating and analyzing factor vectors in R. It offers tools for filtering, splitting, combining, and reordering factor levels based on various criteria. The package is designed to simplify common tasks in categorical data analysis, making it easier to work with factors in a flexible and efficient manner.",
    "version": "0.0.7",
    "maintainer": "Kai Guo <guokai8@gmail.com>",
    "author": "Kai Guo [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fctutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fctutils Advanced Factor Manipulation Utilities Provides a collection of utility functions for manipulating and analyzing factor vectors in R. It offers tools for filtering, splitting, combining, and reordering factor levels based on various criteria. The package is designed to simplify common tasks in categorical data analysis, making it easier to work with factors in a flexible and efficient manner.  "
  },
  {
    "id": 12311,
    "package_name": "fdapace",
    "title": "Functional Data Analysis and Empirical Dynamics",
    "description": "A versatile package that provides implementation of various\n    methods of Functional Data Analysis (FDA) and Empirical Dynamics. The core of this\n    package is Functional Principal Component Analysis (FPCA), a key technique for\n    functional data analysis, for sparsely or densely sampled random trajectories\n    and time courses, via the Principal Analysis by Conditional Estimation\n    (PACE) algorithm. This core algorithm yields covariance and mean functions,\n    eigenfunctions and principal component (scores), for both functional data and\n    derivatives, for both dense (functional) and sparse (longitudinal) sampling designs.\n    For sparse designs, it provides fitted continuous trajectories with confidence bands,\n    even for subjects with very few longitudinal observations. PACE is a viable and\n    flexible alternative to random effects modeling of longitudinal data. There is also a\n    Matlab version (PACE) that contains some methods not available on fdapace and vice\n    versa. Updates to fdapace were supported by grants from NIH Echo and NSF DMS-1712864 and DMS-2014626. \n    Please cite our package if you use it (You may run the command citation(\"fdapace\") to get the citation format and bibtex entry).\n    References: Wang, J.L., Chiou, J., M\u00fcller, H.G. (2016) <doi:10.1146/annurev-statistics-041715-033624>; Chen, K., Zhang, X., Petersen, A., M\u00fcller, H.G. (2017) <doi:10.1007/s12561-015-9137-5>.",
    "version": "0.6.0",
    "maintainer": "Yidong Zhou <ydzhou@ucdavis.edu>",
    "author": "Yidong Zhou [cre, aut] (ORCID: <https://orcid.org/0000-0003-1423-1857>),\n  Han Chen [aut],\n  Su I Iao [aut],\n  Poorbita Kundu [aut],\n  Hang Zhou [aut],\n  Satarupa Bhattacharjee [aut],\n  Cody Carroll [aut] (ORCID: <https://orcid.org/0000-0003-3525-8653>),\n  Yaqing Chen [aut],\n  Xiongtao Dai [aut],\n  Jianing Fan [aut],\n  Alvaro Gajardo [aut],\n  Pantelis Z. Hadjipantelis [aut],\n  Kyunghee Han [aut],\n  Hao Ji [aut],\n  Changbo Zhu [aut],\n  Paromita Dubey [ctb],\n  Shu-Chin Lin [ctb],\n  Hans-Georg M\u00fcller [cph, ths, aut],\n  Jane-Ling Wang [cph, ths, aut]",
    "url": "https://github.com/functionaldata/tPACE",
    "bug_reports": "https://github.com/functionaldata/tPACE/issues",
    "repository": "https://cran.r-project.org/package=fdapace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdapace Functional Data Analysis and Empirical Dynamics A versatile package that provides implementation of various\n    methods of Functional Data Analysis (FDA) and Empirical Dynamics. The core of this\n    package is Functional Principal Component Analysis (FPCA), a key technique for\n    functional data analysis, for sparsely or densely sampled random trajectories\n    and time courses, via the Principal Analysis by Conditional Estimation\n    (PACE) algorithm. This core algorithm yields covariance and mean functions,\n    eigenfunctions and principal component (scores), for both functional data and\n    derivatives, for both dense (functional) and sparse (longitudinal) sampling designs.\n    For sparse designs, it provides fitted continuous trajectories with confidence bands,\n    even for subjects with very few longitudinal observations. PACE is a viable and\n    flexible alternative to random effects modeling of longitudinal data. There is also a\n    Matlab version (PACE) that contains some methods not available on fdapace and vice\n    versa. Updates to fdapace were supported by grants from NIH Echo and NSF DMS-1712864 and DMS-2014626. \n    Please cite our package if you use it (You may run the command citation(\"fdapace\") to get the citation format and bibtex entry).\n    References: Wang, J.L., Chiou, J., M\u00fcller, H.G. (2016) <doi:10.1146/annurev-statistics-041715-033624>; Chen, K., Zhang, X., Petersen, A., M\u00fcller, H.G. (2017) <doi:10.1007/s12561-015-9137-5>.  "
  },
  {
    "id": 12315,
    "package_name": "fdatest",
    "title": "Interval Testing Procedure for Functional Data",
    "description": "Implementation of the Interval Testing Procedure for functional data in different frameworks (i.e., one or two-population frameworks, functional linear models) by means of different basis expansions (i.e., B-spline, Fourier, and phase-amplitude Fourier). The current version of the package requires functional data evaluated on a uniform grid; it automatically projects each function on a chosen functional basis; it performs the entire family of multivariate tests; and, finally, it provides the matrix of the p-values of the previous tests and the vector of the corrected p-values. The functional basis, the coupled or uncoupled scenario, and the kind of test can be chosen by the user. The package provides also a plotting function creating a graphical output of the procedure: the p-value heat-map, the plot of the corrected p-values, and the plot of the functional data.",
    "version": "2.1.1",
    "maintainer": "Alessia Pini <alessia.pini@polimi.it>",
    "author": "Alessia Pini, Simone Vantini",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdatest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdatest Interval Testing Procedure for Functional Data Implementation of the Interval Testing Procedure for functional data in different frameworks (i.e., one or two-population frameworks, functional linear models) by means of different basis expansions (i.e., B-spline, Fourier, and phase-amplitude Fourier). The current version of the package requires functional data evaluated on a uniform grid; it automatically projects each function on a chosen functional basis; it performs the entire family of multivariate tests; and, finally, it provides the matrix of the p-values of the previous tests and the vector of the corrected p-values. The functional basis, the coupled or uncoupled scenario, and the kind of test can be chosen by the user. The package provides also a plotting function creating a graphical output of the procedure: the p-value heat-map, the plot of the corrected p-values, and the plot of the functional data.  "
  },
  {
    "id": 12325,
    "package_name": "fdth",
    "title": "Frequency Distribution Tables, Histograms and Polygons",
    "description": "Perform frequency distribution tables, associated histograms\n             and polygons from vector, data.frame and matrix objects for\n             numerical and categorical variables.",
    "version": "1.3-0",
    "maintainer": "J. C. Faria <joseclaudio.faria@gmail.com>",
    "author": "J. C. Faria [aut, cre],\n  I. B. Allaman [aut],\n  E. G. Jelihovschi [aut]",
    "url": "https://github.com/jcfaria/fdth",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdth Frequency Distribution Tables, Histograms and Polygons Perform frequency distribution tables, associated histograms\n             and polygons from vector, data.frame and matrix objects for\n             numerical and categorical variables.  "
  },
  {
    "id": 12354,
    "package_name": "ff",
    "title": "Memory-Efficient Storage of Large Data on Disk and Fast Access\nFunctions",
    "description": "The ff package provides data structures that are stored on\n\tdisk but behave (almost) as if they were in RAM by transparently \n\tmapping only a section (pagesize) in main memory - the effective \n\tvirtual memory consumption per ff object. ff supports R's standard \n\tatomic data types 'double', 'logical', 'raw' and 'integer' and \n\tnon-standard atomic types boolean (1 bit), quad (2 bit unsigned), \n\tnibble (4 bit unsigned), byte (1 byte signed with NAs), ubyte (1 byte \n\tunsigned), short (2 byte signed with NAs), ushort (2 byte unsigned), \n\tsingle (4 byte float with NAs). For example 'quad' allows efficient \n\tstorage of genomic data as an 'A','T','G','C' factor. The unsigned \n\ttypes support 'circular' arithmetic. There is also support for \n\tclose-to-atomic types 'factor', 'ordered', 'POSIXct', 'Date' and \n\tcustom close-to-atomic types. \n\tff not only has native C-support for vectors, matrices and arrays \n\twith flexible dimorder (major column-order, major row-order and \n\tgeneralizations for arrays). There is also a ffdf class not unlike \n\tdata.frames and import/export filters for csv files.\n\tff objects store raw data in binary flat files in native encoding,\n\tand complement this with metadata stored in R as physical and virtual\n\tattributes. ff objects have well-defined hybrid copying semantics, \n\twhich gives rise to certain performance improvements through \n\tvirtualization. ff objects can be stored and reopened across R \n\tsessions. ff files can be shared by multiple ff R objects \n\t(using different data en/de-coding schemes) in the same process \n\tor from multiple R processes to exploit parallelism. A wide choice of \n\tfinalizer options allows to work with 'permanent' files as well as \n\tcreating/removing 'temporary' ff files completely transparent to the \n\tuser. On certain OS/Filesystem combinations, creating the ff files\n\tworks without notable delay thanks to using sparse file allocation.\n\tSeveral access optimization techniques such as Hybrid Index \n\tPreprocessing and Virtualization are implemented to achieve good \n\tperformance even with large datasets, for example virtual matrix \n\ttranspose without touching a single byte on disk. Further, to reduce \n\tdisk I/O, 'logicals' and non-standard data types get stored native and \n\tcompact on binary flat files i.e. logicals take up exactly 2 bits to \n\trepresent TRUE, FALSE and NA. \n\tBeyond basic access functions, the ff package also provides \n\tcompatibility functions that facilitate writing code for ff and ram \n\tobjects and support for batch processing on ff objects (e.g. as.ram, \n\tas.ff, ffapply). ff interfaces closely with functionality from package \n\t'bit': chunked looping, fast bit operations and coercions between \n\tdifferent objects that can store subscript information ('bit', \n\t'bitwhich', ff 'boolean', ri range index, hi hybrid index). This allows\n\tto work interactively with selections of large datasets and quickly \n\tmodify selection criteria. \n\tFurther high-performance enhancements can be made available upon request. ",
    "version": "4.5.2",
    "maintainer": "Jens Oehlschl\u00e4gel <Jens.Oehlschlaegel@truecluster.com>",
    "author": "Daniel Adler [aut],\n  Christian Gl\u00e4ser [ctb],\n  Oleg Nenadic [ctb],\n  Jens Oehlschl\u00e4gel [aut, cre],\n  Martijn Schuemie [ctb],\n  Walter Zucchini [ctb]",
    "url": "https://github.com/truecluster/ff",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ff Memory-Efficient Storage of Large Data on Disk and Fast Access\nFunctions The ff package provides data structures that are stored on\n\tdisk but behave (almost) as if they were in RAM by transparently \n\tmapping only a section (pagesize) in main memory - the effective \n\tvirtual memory consumption per ff object. ff supports R's standard \n\tatomic data types 'double', 'logical', 'raw' and 'integer' and \n\tnon-standard atomic types boolean (1 bit), quad (2 bit unsigned), \n\tnibble (4 bit unsigned), byte (1 byte signed with NAs), ubyte (1 byte \n\tunsigned), short (2 byte signed with NAs), ushort (2 byte unsigned), \n\tsingle (4 byte float with NAs). For example 'quad' allows efficient \n\tstorage of genomic data as an 'A','T','G','C' factor. The unsigned \n\ttypes support 'circular' arithmetic. There is also support for \n\tclose-to-atomic types 'factor', 'ordered', 'POSIXct', 'Date' and \n\tcustom close-to-atomic types. \n\tff not only has native C-support for vectors, matrices and arrays \n\twith flexible dimorder (major column-order, major row-order and \n\tgeneralizations for arrays). There is also a ffdf class not unlike \n\tdata.frames and import/export filters for csv files.\n\tff objects store raw data in binary flat files in native encoding,\n\tand complement this with metadata stored in R as physical and virtual\n\tattributes. ff objects have well-defined hybrid copying semantics, \n\twhich gives rise to certain performance improvements through \n\tvirtualization. ff objects can be stored and reopened across R \n\tsessions. ff files can be shared by multiple ff R objects \n\t(using different data en/de-coding schemes) in the same process \n\tor from multiple R processes to exploit parallelism. A wide choice of \n\tfinalizer options allows to work with 'permanent' files as well as \n\tcreating/removing 'temporary' ff files completely transparent to the \n\tuser. On certain OS/Filesystem combinations, creating the ff files\n\tworks without notable delay thanks to using sparse file allocation.\n\tSeveral access optimization techniques such as Hybrid Index \n\tPreprocessing and Virtualization are implemented to achieve good \n\tperformance even with large datasets, for example virtual matrix \n\ttranspose without touching a single byte on disk. Further, to reduce \n\tdisk I/O, 'logicals' and non-standard data types get stored native and \n\tcompact on binary flat files i.e. logicals take up exactly 2 bits to \n\trepresent TRUE, FALSE and NA. \n\tBeyond basic access functions, the ff package also provides \n\tcompatibility functions that facilitate writing code for ff and ram \n\tobjects and support for batch processing on ff objects (e.g. as.ram, \n\tas.ff, ffapply). ff interfaces closely with functionality from package \n\t'bit': chunked looping, fast bit operations and coercions between \n\tdifferent objects that can store subscript information ('bit', \n\t'bitwhich', ff 'boolean', ri range index, hi hybrid index). This allows\n\tto work interactively with selections of large datasets and quickly \n\tmodify selection criteria. \n\tFurther high-performance enhancements can be made available upon request.   "
  },
  {
    "id": 12361,
    "package_name": "fftab",
    "title": "Tidy Manipulation of Fourier Transformed Data",
    "description": "The 'fftab' package stores Fourier coefficients in a tibble and \n  allows their manipulation in various ways. Functions are available for converting \n  between complex, rectangular ('re', 'im'), and polar ('mod', 'arg') representations, \n  as well as for extracting components as vectors or matrices. Inputs can include \n  vectors, time series, and arrays of arbitrary dimensions, which are restored \n  to their original form when inverting the transform. Since 'fftab' stores Fourier \n  frequencies as columns in the tibble, many standard operations on spectral data \n  can be easily performed using tidy packages like 'dplyr'.",
    "version": "0.1.0",
    "maintainer": "Timothy Keitt <tkeitt@gmail.com>",
    "author": "Timothy Keitt [aut, cre]",
    "url": "https://github.com/thk686/fftab, https://thk686.github.io/fftab/",
    "bug_reports": "https://github.com/thk686/fftab/issues",
    "repository": "https://cran.r-project.org/package=fftab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fftab Tidy Manipulation of Fourier Transformed Data The 'fftab' package stores Fourier coefficients in a tibble and \n  allows their manipulation in various ways. Functions are available for converting \n  between complex, rectangular ('re', 'im'), and polar ('mod', 'arg') representations, \n  as well as for extracting components as vectors or matrices. Inputs can include \n  vectors, time series, and arrays of arbitrary dimensions, which are restored \n  to their original form when inverting the transform. Since 'fftab' stores Fourier \n  frequencies as columns in the tibble, many standard operations on spectral data \n  can be easily performed using tidy packages like 'dplyr'.  "
  },
  {
    "id": 12378,
    "package_name": "fields",
    "title": "Tools for Spatial Data",
    "description": "For curve, surface and function fitting with an emphasis\n on splines, spatial data, geostatistics, and spatial statistics. The major\n methods\n include  Gaussian spatial process prediction (known as Kriging), cubic and thin plate splines, and compactly supported\n covariance functions for large data sets. The spline and spatial process\n methods are\n supported by functions that can determine the smoothing parameter\n (nugget and sill variance) and other covariance function parameters by cross\n validation and also by  maximum likelihood. For spatial process prediction\n there is an easy to use function that also estimates the correlation\n scale (range parameter).  A major feature is that any covariance function\n implemented in R and following a simple format can be used for\n spatial prediction. As included are fast approximations for prediction\n and conditional simulation for larger data sets.\n There are also many useful functions for plotting\n and working with spatial data as images. This package also contains\n an implementation of sparse matrix methods for large spatial data\n sets based the  R sparse matrix package spam. Use\n help(fields) to get started and for an overview. All package graphics functions\n focus on  extending base R graphics and are easy to interpret and modify.\n The fields source\n code is deliberately commented and provides useful explanations of\n numerical details as a companion to the manual pages. The commented\n source code can be viewed by expanding the source code version of this package\n and looking in the R subdirectory. The reference for fields can be generated\n by the citation function in R and has DOI <doi:10.5065/D6W957CT>. Development\n of this package was supported in part by the National Science Foundation  Grant\n 1417857,  the National Center for Atmospheric Research, and Colorado School of Mines.\n See the Fields URL\n for a vignette on using this package and some background on spatial statistics.",
    "version": "17.1",
    "maintainer": "Douglas Nychka <douglasnychka@gmail.com>",
    "author": "Douglas Nychka [aut, cre],\n  Reinhard Furrer [aut],\n  John Paige [aut],\n  Stephan Sain [aut],\n  Florian Gerber [aut],\n  Matthew Iverson [aut],\n  Rider Johnson [aut]",
    "url": "https://github.com/dnychka/fieldsRPackage",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fields",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fields Tools for Spatial Data For curve, surface and function fitting with an emphasis\n on splines, spatial data, geostatistics, and spatial statistics. The major\n methods\n include  Gaussian spatial process prediction (known as Kriging), cubic and thin plate splines, and compactly supported\n covariance functions for large data sets. The spline and spatial process\n methods are\n supported by functions that can determine the smoothing parameter\n (nugget and sill variance) and other covariance function parameters by cross\n validation and also by  maximum likelihood. For spatial process prediction\n there is an easy to use function that also estimates the correlation\n scale (range parameter).  A major feature is that any covariance function\n implemented in R and following a simple format can be used for\n spatial prediction. As included are fast approximations for prediction\n and conditional simulation for larger data sets.\n There are also many useful functions for plotting\n and working with spatial data as images. This package also contains\n an implementation of sparse matrix methods for large spatial data\n sets based the  R sparse matrix package spam. Use\n help(fields) to get started and for an overview. All package graphics functions\n focus on  extending base R graphics and are easy to interpret and modify.\n The fields source\n code is deliberately commented and provides useful explanations of\n numerical details as a companion to the manual pages. The commented\n source code can be viewed by expanding the source code version of this package\n and looking in the R subdirectory. The reference for fields can be generated\n by the citation function in R and has DOI <doi:10.5065/D6W957CT>. Development\n of this package was supported in part by the National Science Foundation  Grant\n 1417857,  the National Center for Atmospheric Research, and Colorado School of Mines.\n See the Fields URL\n for a vignette on using this package and some background on spatial statistics.  "
  },
  {
    "id": 12397,
    "package_name": "fillr",
    "title": "Fill Missing Values in Vectors",
    "description": "Edit vectors to fill missing values, based on the vector itself.",
    "version": "1.0.0",
    "maintainer": "Jelger van Zaane <me@jelgervanzaane.nl>",
    "author": "Jelger van Zaane [aut, cre]",
    "url": "https://jelger12.github.io/fillr/",
    "bug_reports": "https://github.com/jelger12/fillr/issues",
    "repository": "https://cran.r-project.org/package=fillr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fillr Fill Missing Values in Vectors Edit vectors to fill missing values, based on the vector itself.  "
  },
  {
    "id": 12412,
    "package_name": "fingerprint",
    "title": "Functions to Operate on Binary Fingerprint Data",
    "description": "Functions to manipulate binary fingerprints\n of arbitrary length. A fingerprint is represented by an object of S4 class 'fingerprint'\n which is internally represented a vector of integers, such\n that each element represents the position in the fingerprint that is set to 1.\n The bitwise logical functions in R are overridden so that they can be used directly\n with 'fingerprint' objects. A number of distance metrics are also\n available (many contributed by Michael Fadock). Fingerprints \n can be converted to Euclidean vectors (i.e., points on the unit hypersphere) and\n can also be folded using OR.  Arbitrary fingerprint formats can be handled via line\n handlers. Currently handlers are provided for CDK, MOE and BCI fingerprint data.",
    "version": "3.5.7",
    "maintainer": "Rajarshi Guha <rajarshi.guha@gmail.com>",
    "author": "Rajarshi Guha <rajarshi.guha@gmail.com>",
    "url": "",
    "bug_reports": "https://github.com/rajarshi/cdkr/issues",
    "repository": "https://cran.r-project.org/package=fingerprint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fingerprint Functions to Operate on Binary Fingerprint Data Functions to manipulate binary fingerprints\n of arbitrary length. A fingerprint is represented by an object of S4 class 'fingerprint'\n which is internally represented a vector of integers, such\n that each element represents the position in the fingerprint that is set to 1.\n The bitwise logical functions in R are overridden so that they can be used directly\n with 'fingerprint' objects. A number of distance metrics are also\n available (many contributed by Michael Fadock). Fingerprints \n can be converted to Euclidean vectors (i.e., points on the unit hypersphere) and\n can also be folded using OR.  Arbitrary fingerprint formats can be handled via line\n handlers. Currently handlers are provided for CDK, MOE and BCI fingerprint data.  "
  },
  {
    "id": 12436,
    "package_name": "fishgrowth",
    "title": "Fit Growth Curves to Fish Data",
    "description": "Fit growth models to otoliths and/or tagging data, using the 'RTMB'\n  package and maximum likelihood. The otoliths (or similar measurements of age)\n  provide direct observed coordinates of age and length. The tagging data\n  provide information about the observed length at release and length at\n  recapture at a later time, where the age at release is unknown and estimated\n  as a vector of parameters. The growth models provided by this package can be\n  fitted to otoliths only, tagging data only, or a combination of the two.\n  Growth variability can be modelled as constant or increasing with length.",
    "version": "1.0.4",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Arni Magnusson [aut, cre],\n  Mark Maunder [aut]",
    "url": "https://github.com/arni-magnusson/fishgrowth",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fishgrowth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fishgrowth Fit Growth Curves to Fish Data Fit growth models to otoliths and/or tagging data, using the 'RTMB'\n  package and maximum likelihood. The otoliths (or similar measurements of age)\n  provide direct observed coordinates of age and length. The tagging data\n  provide information about the observed length at release and length at\n  recapture at a later time, where the age at release is unknown and estimated\n  as a vector of parameters. The growth models provided by this package can be\n  fitted to otoliths only, tagging data only, or a combination of the two.\n  Growth variability can be modelled as constant or increasing with length.  "
  },
  {
    "id": 12444,
    "package_name": "fitHeavyTail",
    "title": "Mean and Covariance Matrix Estimation under Heavy Tails",
    "description": "Robust estimation methods for the mean vector, scatter matrix,\n    and covariance matrix (if it exists) from data (possibly containing NAs) \n    under multivariate heavy-tailed distributions such as angular Gaussian \n    (via Tyler's method), Cauchy, and Student's t distributions. Additionally, \n    a factor model structure can be specified for the covariance matrix. The\n    latest revision also includes the multivariate skewed t distribution.\n    The package is based on the papers: Sun, Babu, and Palomar (2014);\n    Sun, Babu, and Palomar (2015); Liu and Rubin (1995);\n    Zhou, Liu, Kumar, and Palomar (2019); Pascal, Ollila, and Palomar (2021).",
    "version": "0.2.0",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Daniel P. Palomar [cre, aut],\n  Rui Zhou [aut],\n  Xiwen Wang [aut],\n  Fr\u00e9d\u00e9ric Pascal [ctb],\n  Esa Ollila [ctb]",
    "url": "https://CRAN.R-project.org/package=fitHeavyTail,\nhttps://github.com/convexfi/fitHeavyTail,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2014.2348944,\nhttps://doi.org/10.1109/TSP.2015.2417513,\nhttps://doi.org/10.23919/EUSIPCO54536.2021.9616162",
    "bug_reports": "https://github.com/convexfi/fitHeavyTail/issues",
    "repository": "https://cran.r-project.org/package=fitHeavyTail",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fitHeavyTail Mean and Covariance Matrix Estimation under Heavy Tails Robust estimation methods for the mean vector, scatter matrix,\n    and covariance matrix (if it exists) from data (possibly containing NAs) \n    under multivariate heavy-tailed distributions such as angular Gaussian \n    (via Tyler's method), Cauchy, and Student's t distributions. Additionally, \n    a factor model structure can be specified for the covariance matrix. The\n    latest revision also includes the multivariate skewed t distribution.\n    The package is based on the papers: Sun, Babu, and Palomar (2014);\n    Sun, Babu, and Palomar (2015); Liu and Rubin (1995);\n    Zhou, Liu, Kumar, and Palomar (2019); Pascal, Ollila, and Palomar (2021).  "
  },
  {
    "id": 12454,
    "package_name": "fitlandr",
    "title": "Fit Vector Fields and Potential Landscapes from Intensive\nLongitudinal Data",
    "description": "A toolbox for estimating vector fields from intensive\n    longitudinal data, and construct potential landscapes thereafter. The\n    vector fields can be estimated with two nonparametric methods: the\n    Multivariate Vector Field Kernel Estimator (MVKE) by Bandi & Moloche\n    (2018) <doi:10.1017/S0266466617000305> and the Sparse Vector Field\n    Consensus (SparseVFC) algorithm by Ma et al.  (2013)\n    <doi:10.1016/j.patcog.2013.05.017>. The potential landscapes can be\n    constructed with a simulation-based approach with the 'simlandr'\n    package (Cui et al., 2021) <doi:10.31234/osf.io/pzva3>, or the\n    Bhattacharya et al. (2011) method for path integration\n    <doi:10.1186/1752-0509-5-85>.",
    "version": "0.1.0",
    "maintainer": "Jingmeng Cui <jingmeng.cui@outlook.com>",
    "author": "Jingmeng Cui [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3421-8457>)",
    "url": "https://sciurus365.github.io/fitlandr/,\nhttps://github.com/Sciurus365/fitlandr",
    "bug_reports": "https://github.com/Sciurus365/fitlandr/issues",
    "repository": "https://cran.r-project.org/package=fitlandr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fitlandr Fit Vector Fields and Potential Landscapes from Intensive\nLongitudinal Data A toolbox for estimating vector fields from intensive\n    longitudinal data, and construct potential landscapes thereafter. The\n    vector fields can be estimated with two nonparametric methods: the\n    Multivariate Vector Field Kernel Estimator (MVKE) by Bandi & Moloche\n    (2018) <doi:10.1017/S0266466617000305> and the Sparse Vector Field\n    Consensus (SparseVFC) algorithm by Ma et al.  (2013)\n    <doi:10.1016/j.patcog.2013.05.017>. The potential landscapes can be\n    constructed with a simulation-based approach with the 'simlandr'\n    package (Cui et al., 2021) <doi:10.31234/osf.io/pzva3>, or the\n    Bhattacharya et al. (2011) method for path integration\n    <doi:10.1186/1752-0509-5-85>.  "
  },
  {
    "id": 12483,
    "package_name": "flashr",
    "title": "Create Flashcards of Terms and Definitions",
    "description": "Provides functions for creating flashcard decks of terms and \n    definitions. This package creates HTML slides using 'revealjs' that can be \n    viewed in the 'RStudio' viewer or a web browser.  Users can create \n    flashcards from either existing built-in decks or create their own from CSV\n    files or vectors of function names.",
    "version": "0.3.0",
    "maintainer": "Jeffrey R. Stevens <jeffrey.r.stevens@protonmail.com>",
    "author": "Jeffrey R. Stevens [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2375-1360>)",
    "url": "https://github.com/JeffreyRStevens/flashr,\nhttps://jeffreyrstevens.github.io/flashr/",
    "bug_reports": "https://github.com/JeffreyRStevens/flashr/issues",
    "repository": "https://cran.r-project.org/package=flashr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flashr Create Flashcards of Terms and Definitions Provides functions for creating flashcard decks of terms and \n    definitions. This package creates HTML slides using 'revealjs' that can be \n    viewed in the 'RStudio' viewer or a web browser.  Users can create \n    flashcards from either existing built-in decks or create their own from CSV\n    files or vectors of function names.  "
  },
  {
    "id": 12513,
    "package_name": "flint",
    "title": "Fast Library for Number Theory",
    "description": "\n\tAn R interface to 'FLINT' <https://flintlib.org/>, a C library for\n\tnumber theory.  'FLINT' extends GNU 'MPFR' <https://www.mpfr.org/>\n\tand GNU 'MP' <https://gmplib.org/> with support for operations on\n\tstandard rings (the integers, the integers modulo n, finite\n\tfields, the rational, p-adic, real, and complex numbers) as well\n\tas matrices and polynomials over rings.  'FLINT' implements\n\tmidpoint-radius interval arithmetic, also known as ball\n\tarithmetic, in the real and complex numbers, enabling computation\n\tin arbitrary precision with rigorous propagation of rounding and\n\tother errors; see Johansson (2017) <doi:10.1109/TC.2017.2690633>.\n\tFinally, 'FLINT' provides ball arithmetic implementations of many\n\tspecial mathematical functions, with high coverage of reference\n\tworks such as the NIST Digital Library of Mathematical Functions\n\t<https://dlmf.nist.gov/>.  The R interface defines S4 classes,\n\tgeneric functions, and methods for representation and basic\n\toperations as well as plain R functions mirroring and vectorizing\n\tentry points in the C library.",
    "version": "0.1.2",
    "maintainer": "Mikael Jagan <jaganmn@mcmaster.ca>",
    "author": "Mikael Jagan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3542-2938>),\n  Martin Maechler [ctb] (ORCID: <https://orcid.org/0000-0002-8685-9910>)",
    "url": "https://github.com/jaganmn/flint",
    "bug_reports": "https://github.com/jaganmn/flint/issues",
    "repository": "https://cran.r-project.org/package=flint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flint Fast Library for Number Theory \n\tAn R interface to 'FLINT' <https://flintlib.org/>, a C library for\n\tnumber theory.  'FLINT' extends GNU 'MPFR' <https://www.mpfr.org/>\n\tand GNU 'MP' <https://gmplib.org/> with support for operations on\n\tstandard rings (the integers, the integers modulo n, finite\n\tfields, the rational, p-adic, real, and complex numbers) as well\n\tas matrices and polynomials over rings.  'FLINT' implements\n\tmidpoint-radius interval arithmetic, also known as ball\n\tarithmetic, in the real and complex numbers, enabling computation\n\tin arbitrary precision with rigorous propagation of rounding and\n\tother errors; see Johansson (2017) <doi:10.1109/TC.2017.2690633>.\n\tFinally, 'FLINT' provides ball arithmetic implementations of many\n\tspecial mathematical functions, with high coverage of reference\n\tworks such as the NIST Digital Library of Mathematical Functions\n\t<https://dlmf.nist.gov/>.  The R interface defines S4 classes,\n\tgeneric functions, and methods for representation and basic\n\toperations as well as plain R functions mirroring and vectorizing\n\tentry points in the C library.  "
  },
  {
    "id": 12522,
    "package_name": "float",
    "title": "32-Bit Floats",
    "description": "R comes with a suite of utilities for linear algebra with \"numeric\"\n    (double precision) vectors/matrices. However, sometimes single precision (or\n    less!) is more than enough for a particular task.  This package extends R's\n    linear algebra facilities to include 32-bit float (single precision) data.\n    Float vectors/matrices have half the precision of their \"numeric\"-type\n    counterparts but are generally faster to numerically operate on, for a\n    performance vs accuracy trade-off.  The internal representation is an S4\n    class, which allows us to keep the syntax identical to that of base R's.\n    Interaction between floats and base types for binary operators is generally\n    possible; in these cases, type promotion always defaults to the higher\n    precision.  The package ships with copies of the single precision 'BLAS' and\n    'LAPACK', which are automatically built in the event they are not available\n    on the system.",
    "version": "0.3-3",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre, cph],\n  Wei-Chen Chen [aut],\n  Dmitriy Selivanov [ctb] (improvements in external package linking),\n  ORNL [cph]",
    "url": "https://github.com/wrathematics/float",
    "bug_reports": "https://github.com/wrathematics/float/issues",
    "repository": "https://cran.r-project.org/package=float",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "float 32-Bit Floats R comes with a suite of utilities for linear algebra with \"numeric\"\n    (double precision) vectors/matrices. However, sometimes single precision (or\n    less!) is more than enough for a particular task.  This package extends R's\n    linear algebra facilities to include 32-bit float (single precision) data.\n    Float vectors/matrices have half the precision of their \"numeric\"-type\n    counterparts but are generally faster to numerically operate on, for a\n    performance vs accuracy trade-off.  The internal representation is an S4\n    class, which allows us to keep the syntax identical to that of base R's.\n    Interaction between floats and base types for binary operators is generally\n    possible; in these cases, type promotion always defaults to the higher\n    precision.  The package ships with copies of the single precision 'BLAS' and\n    'LAPACK', which are automatically built in the event they are not available\n    on the system.  "
  },
  {
    "id": 12560,
    "package_name": "fmtr",
    "title": "Easily Apply Formats to Data",
    "description": "Contains a set of functions that can be used to apply\n  formats to data frames or vectors.  The package aims to provide \n  functionality similar to that of SAS\u00ae formats. Formats are assigned to\n  the format attribute on data frame columns.  Then when the fdata() \n  function is called, a new data frame is created with the column data\n  formatted as specified.  The package also contains a value() function\n  to create a user-defined format, similar to a SAS\u00ae user-defined format.",
    "version": "1.7.0",
    "maintainer": "David Bosak <dbosak01@gmail.com>",
    "author": "David Bosak [aut, cre],\n  Chen Ling [aut]",
    "url": "https://fmtr.r-sassy.org, https://github.com/dbosak01/fmtr",
    "bug_reports": "https://github.com/dbosak01/fmtr/issues",
    "repository": "https://cran.r-project.org/package=fmtr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fmtr Easily Apply Formats to Data Contains a set of functions that can be used to apply\n  formats to data frames or vectors.  The package aims to provide \n  functionality similar to that of SAS\u00ae formats. Formats are assigned to\n  the format attribute on data frame columns.  Then when the fdata() \n  function is called, a new data frame is created with the column data\n  formatted as specified.  The package also contains a value() function\n  to create a user-defined format, similar to a SAS\u00ae user-defined format.  "
  },
  {
    "id": 12562,
    "package_name": "fnets",
    "title": "Factor-Adjusted Network Estimation and Forecasting for\nHigh-Dimensional Time Series",
    "description": "Implements methods for network estimation and forecasting of high-dimensional time series \n    exhibiting strong serial and cross-sectional correlations under a factor-adjusted vector autoregressive model.\n    See Barigozzi, Cho and Owens (2024+) <doi:10.1080/07350015.2023.2257270> for further descriptions of FNETS methodology and \n    Owens, Cho and Barigozzi (2024+) <arXiv:2301.11675> accompanying the R package.",
    "version": "0.1.6",
    "maintainer": "Haeran Cho <haeran.cho@bristol.ac.uk>",
    "author": "Matteo Barigozzi [aut],\n  Haeran Cho [cre, aut],\n  Dom Owens [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fnets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fnets Factor-Adjusted Network Estimation and Forecasting for\nHigh-Dimensional Time Series Implements methods for network estimation and forecasting of high-dimensional time series \n    exhibiting strong serial and cross-sectional correlations under a factor-adjusted vector autoregressive model.\n    See Barigozzi, Cho and Owens (2024+) <doi:10.1080/07350015.2023.2257270> for further descriptions of FNETS methodology and \n    Owens, Cho and Barigozzi (2024+) <arXiv:2301.11675> accompanying the R package.  "
  },
  {
    "id": 12618,
    "package_name": "formatdown",
    "title": "Formatting Numbers in 'rmarkdown' Documents",
    "description": "Provides a small set of tools for formatting numbers in R-markdown \n    documents. Convert a numerical vector to character strings in power-of-ten \n    form, decimal form, or measurement-units form; all are math-delimited for \n    rendering as inline equations. Can also convert text into math-delimited \n    text to match the font face and size of math-delimited numbers. Useful for \n    rendering single numbers in inline R code chunks and for rendering columns \n    in tables.   ",
    "version": "0.1.4",
    "maintainer": "Richard Layton <graphdoctor@gmail.com>",
    "author": "Richard Layton [aut, cre]",
    "url": "https://github.com/graphdr/formatdown/,\nhttps://graphdr.github.io/formatdown/,\nhttps://CRAN.R-project.org/package=formatdown",
    "bug_reports": "https://github.com/graphdr/formatdown/issues",
    "repository": "https://cran.r-project.org/package=formatdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "formatdown Formatting Numbers in 'rmarkdown' Documents Provides a small set of tools for formatting numbers in R-markdown \n    documents. Convert a numerical vector to character strings in power-of-ten \n    form, decimal form, or measurement-units form; all are math-delimited for \n    rendering as inline equations. Can also convert text into math-delimited \n    text to match the font face and size of math-delimited numbers. Useful for \n    rendering single numbers in inline R code chunks and for rendering columns \n    in tables.     "
  },
  {
    "id": 12619,
    "package_name": "formattable",
    "title": "Create 'Formattable' Data Structures",
    "description": "Provides functions to create formattable vectors and data frames.\n    'Formattable' vectors are printed with text formatting, and formattable\n    data frames are printed with multiple types of formatting in HTML\n    to improve the readability of data presented in tabular form rendered in\n    web pages.",
    "version": "0.2.1",
    "maintainer": "Kun Ren <ken@renkun.me>",
    "author": "Kun Ren [aut, cre],\n  Kenton Russell [aut]",
    "url": "https://renkun-ken.github.io/formattable/,\nhttps://github.com/renkun-ken/formattable",
    "bug_reports": "https://github.com/renkun-ken/formattable/issues",
    "repository": "https://cran.r-project.org/package=formattable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "formattable Create 'Formattable' Data Structures Provides functions to create formattable vectors and data frames.\n    'Formattable' vectors are printed with text formatting, and formattable\n    data frames are printed with multiple types of formatting in HTML\n    to improve the readability of data presented in tabular form rendered in\n    web pages.  "
  },
  {
    "id": 12649,
    "package_name": "fpopw",
    "title": "Weighted Segmentation using Functional Pruning and Optimal\nPartioning",
    "description": "Weighted-L2 FPOP Maidstone et al. (2017) <doi:10.1007/s11222-016-9636-3> and pDPA/FPSN Rigaill (2010) <arXiv:1004.0887> algorithm for detecting multiple changepoints in the mean of a vector. Also includes a few model selection functions using Lebarbier (2005) <doi:10.1016/j.sigpro.2004.11.012> and the 'capsushe' package.",
    "version": "1.1",
    "maintainer": "Guillem Rigaill <guillem.rigaill@inrae.fr>",
    "author": "Guillem Rigaill [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fpopw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fpopw Weighted Segmentation using Functional Pruning and Optimal\nPartioning Weighted-L2 FPOP Maidstone et al. (2017) <doi:10.1007/s11222-016-9636-3> and pDPA/FPSN Rigaill (2010) <arXiv:1004.0887> algorithm for detecting multiple changepoints in the mean of a vector. Also includes a few model selection functions using Lebarbier (2005) <doi:10.1016/j.sigpro.2004.11.012> and the 'capsushe' package.  "
  },
  {
    "id": 12659,
    "package_name": "frab",
    "title": "How to Add Two R Tables",
    "description": "Methods to \"add\" two R tables; also an alternative\n     interpretation of named vectors as generalized R tables, so that\n     c(a=1,b=2,c=3) + c(b=3,a=-1) will return c(b=5,c=3).  Uses\n     'disordR' discipline (Hankin, 2022,\n     <doi:10.48550/arXiv.2210.03856>).  Extraction and replacement\n     methods are provided.  The underlying mathematical structure is\n     the Free Abelian group, hence the name.  To cite in publications\n     please use Hankin (2023) <doi:10.48550/arXiv.2307.13184>.",
    "version": "0.0-6",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "author": "Robin K. S. Hankin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5982-0415>)",
    "url": "https://github.com/RobinHankin/frab",
    "bug_reports": "https://github.com/RobinHankin/frab",
    "repository": "https://cran.r-project.org/package=frab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "frab How to Add Two R Tables Methods to \"add\" two R tables; also an alternative\n     interpretation of named vectors as generalized R tables, so that\n     c(a=1,b=2,c=3) + c(b=3,a=-1) will return c(b=5,c=3).  Uses\n     'disordR' discipline (Hankin, 2022,\n     <doi:10.48550/arXiv.2210.03856>).  Extraction and replacement\n     methods are provided.  The underlying mathematical structure is\n     the Free Abelian group, hence the name.  To cite in publications\n     please use Hankin (2023) <doi:10.48550/arXiv.2307.13184>.  "
  },
  {
    "id": 12662,
    "package_name": "fracdist",
    "title": "Numerical CDFs for Fractional Unit Root and Cointegration Tests",
    "description": "Calculate numerical asymptotic distribution functions of likelihood ratio \n    statistics for fractional unit root tests and tests of cointegration rank. \n    For these distributions, the included functions calculate critical values \n    and P-values used in unit root tests, cointegration tests, and rank tests \n    in the Fractionally Cointegrated Vector Autoregression (FCVAR) model.\n    The functions implement procedures for tests described in the following articles:\n    Johansen, S. and M. \u00d8. Nielsen (2012) <doi:10.3982/ECTA9299>,\n    MacKinnon, J. G. and M. \u00d8. Nielsen (2014) <doi:10.1002/jae.2295>.",
    "version": "0.1.1",
    "maintainer": "Lealand Morin <lealand.morin@ucf.edu>",
    "author": "Lealand Morin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8539-1386>)",
    "url": "https://github.com/LeeMorinUCF/fracdist",
    "bug_reports": "https://github.com/LeeMorinUCF/fracdist/issues",
    "repository": "https://cran.r-project.org/package=fracdist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fracdist Numerical CDFs for Fractional Unit Root and Cointegration Tests Calculate numerical asymptotic distribution functions of likelihood ratio \n    statistics for fractional unit root tests and tests of cointegration rank. \n    For these distributions, the included functions calculate critical values \n    and P-values used in unit root tests, cointegration tests, and rank tests \n    in the Fractionally Cointegrated Vector Autoregression (FCVAR) model.\n    The functions implement procedures for tests described in the following articles:\n    Johansen, S. and M. \u00d8. Nielsen (2012) <doi:10.3982/ECTA9299>,\n    MacKinnon, J. G. and M. \u00d8. Nielsen (2014) <doi:10.1002/jae.2295>.  "
  },
  {
    "id": 12667,
    "package_name": "fractional",
    "title": "Vulgar Fractions in R",
    "description": "The main function of this package allows numerical vector objects to\n  be displayed with their values in vulgar fractional form.  This is convenient if\n  patterns can then be more easily detected.  In some cases replacing the components\n  of a numeric vector by a rational approximation can also be expected to remove\n  some component of round-off error.  The main functions form a re-implementation\n  of the functions 'fractions' and 'rational' of the MASS package, but using a\n  radically improved programming strategy.",
    "version": "0.1.3",
    "maintainer": "Bill Venables <bill.venables@gmail.com>",
    "author": "Bill Venables",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fractional",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fractional Vulgar Fractions in R The main function of this package allows numerical vector objects to\n  be displayed with their values in vulgar fractional form.  This is convenient if\n  patterns can then be more easily detected.  In some cases replacing the components\n  of a numeric vector by a rational approximation can also be expected to remove\n  some component of round-off error.  The main functions form a re-implementation\n  of the functions 'fractions' and 'rational' of the MASS package, but using a\n  radically improved programming strategy.  "
  },
  {
    "id": 12668,
    "package_name": "fracture",
    "title": "Convert Decimals to Fractions",
    "description": "Provides functions for converting decimals to a matrix of\n    numerators and denominators or a character vector of fractions.\n    Supports mixed or improper fractions, finding common denominators for\n    vectors of fractions, limiting denominators to powers of ten, and\n    limiting denominators to a maximum value.  Also includes helper\n    functions for finding the least common multiple and greatest common\n    divisor for a vector of integers.  Implemented using C++ for maximum\n    speed.",
    "version": "0.2.2",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>)",
    "url": "https://fracture.rossellhayes.com/,\nhttps://github.com/rossellhayes/fracture",
    "bug_reports": "https://github.com/rossellhayes/fracture/issues",
    "repository": "https://cran.r-project.org/package=fracture",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fracture Convert Decimals to Fractions Provides functions for converting decimals to a matrix of\n    numerators and denominators or a character vector of fractions.\n    Supports mixed or improper fractions, finding common denominators for\n    vectors of fractions, limiting denominators to powers of ten, and\n    limiting denominators to a maximum value.  Also includes helper\n    functions for finding the least common multiple and greatest common\n    divisor for a vector of integers.  Implemented using C++ for maximum\n    speed.  "
  },
  {
    "id": 12696,
    "package_name": "freqdistributionNogives",
    "title": "Automated Cumulative Frequency Plots for Grouped Distribution",
    "description": "Input has to be in the form of vectors of lower class limits and upper class limits and frequencies; the output will give a cumulative frequency distribution table with cumulative frequency plot.",
    "version": "0.1.1",
    "maintainer": "Harshit Budakoti <budakotihb@gmail.com>",
    "author": "Harshit Budakoti [aut, cre, cph]",
    "url": "https://github.com/Harshit-Budakoti/freqdistributionNogives",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=freqdistributionNogives",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "freqdistributionNogives Automated Cumulative Frequency Plots for Grouped Distribution Input has to be in the form of vectors of lower class limits and upper class limits and frequencies; the output will give a cumulative frequency distribution table with cumulative frequency plot.  "
  },
  {
    "id": 12707,
    "package_name": "friendlynumber",
    "title": "Translate Numbers into Number Words",
    "description": "Converts vectors of numbers into character vectors of numerals,\n    including cardinals (one, two, three) and ordinals (first, second, third). \n    Supports negative numbers, fractions, and arbitrary-precision integer and \n    high-precision floating-point vectors provided by the 'bignum' package.",
    "version": "1.0.0",
    "maintainer": "Ethan Sansom <ethan.sansom29@gmail.com>",
    "author": "Ethan Sansom [aut, cre, cph]",
    "url": "https://github.com/EthanSansom/friendlynumber,\nhttps://ethansansom.github.io/friendlynumber/",
    "bug_reports": "https://github.com/EthanSansom/friendlynumber/issues",
    "repository": "https://cran.r-project.org/package=friendlynumber",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "friendlynumber Translate Numbers into Number Words Converts vectors of numbers into character vectors of numerals,\n    including cardinals (one, two, three) and ordinals (first, second, third). \n    Supports negative numbers, fractions, and arbitrary-precision integer and \n    high-precision floating-point vectors provided by the 'bignum' package.  "
  },
  {
    "id": 12713,
    "package_name": "fromo",
    "title": "Fast Robust Moments",
    "description": "Fast, numerically robust computation of weighted moments via 'Rcpp'. \n   Supports computation on vectors and matrices, and Monoidal append of moments. \n   Moments and cumulants over running fixed length windows can be computed, \n   as well as over time-based windows.\n   Moment computations are via a generalization of Welford's method, as described\n   by Bennett et. (2009) <doi:10.1109/CLUSTR.2009.5289161>.",
    "version": "0.2.4",
    "maintainer": "Steven E. Pav <shabbychef@gmail.com>",
    "author": "Steven E. Pav [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4197-6195>)",
    "url": "https://github.com/shabbychef/fromo",
    "bug_reports": "https://github.com/shabbychef/fromo/issues",
    "repository": "https://cran.r-project.org/package=fromo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fromo Fast Robust Moments Fast, numerically robust computation of weighted moments via 'Rcpp'. \n   Supports computation on vectors and matrices, and Monoidal append of moments. \n   Moments and cumulants over running fixed length windows can be computed, \n   as well as over time-based windows.\n   Moment computations are via a generalization of Welford's method, as described\n   by Bennett et. (2009) <doi:10.1109/CLUSTR.2009.5289161>.  "
  },
  {
    "id": 12740,
    "package_name": "fucom",
    "title": "Full Consistency Method (FUCOM)",
    "description": "Full Consistency Method (FUCOM) for multi-criteria decision-making (MCDM), developed by Dragam Pamucar in 2018 (<doi:10.3390/sym10090393>). The goal of the method is to determine the weights of criteria such that the deviation from full consistency is minimized. Users provide a character vector specifying the ranking of each criterion according to its significance, starting from the criterion expected to have the highest weight to the least significant one. Additionally, users provide a numeric vector specifying the priority values for each criterion. The comparison is made with respect to the first-ranked (most significant) criterion. The function returns the optimized weights for each criterion (summing to 1), the comparative priority (Phi) values, the mathematical transitivity condition (w) value, and the minimum deviation from full consistency (DFC).",
    "version": "0.0.4",
    "maintainer": "Mateus Vanzetta <mateusvanzetta@id.uff.br>",
    "author": "Mateus Vanzetta [aut, cre],\n  Marcos Santos [ctb] (ORCID: <https://orcid.org/0000-0003-1533-5535>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fucom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fucom Full Consistency Method (FUCOM) Full Consistency Method (FUCOM) for multi-criteria decision-making (MCDM), developed by Dragam Pamucar in 2018 (<doi:10.3390/sym10090393>). The goal of the method is to determine the weights of criteria such that the deviation from full consistency is minimized. Users provide a character vector specifying the ranking of each criterion according to its significance, starting from the criterion expected to have the highest weight to the least significant one. Additionally, users provide a numeric vector specifying the priority values for each criterion. The comparison is made with respect to the first-ranked (most significant) criterion. The function returns the optimized weights for each criterion (summing to 1), the comparative priority (Phi) values, the mathematical transitivity condition (w) value, and the minimum deviation from full consistency (DFC).  "
  },
  {
    "id": 12761,
    "package_name": "functionals",
    "title": "Functional Programming with Parallelism and Progress Tracking",
    "description": "Provides functional tools such as fmap(), fwalk(), and fapply() \n    to iterate over vectors, data frames, or grouped data with optional parallelism \n    and real-time progress tracking. Designed for readable and reproducible workflows, \n    including support for Monte Carlo simulations and benchmarking.",
    "version": "0.5.0",
    "maintainer": "Imad EL BADISY <elbadisyimad@gmail.com>",
    "author": "Imad EL BADISY [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/ielbadisy/functionals/issues",
    "repository": "https://cran.r-project.org/package=functionals",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "functionals Functional Programming with Parallelism and Progress Tracking Provides functional tools such as fmap(), fwalk(), and fapply() \n    to iterate over vectors, data frames, or grouped data with optional parallelism \n    and real-time progress tracking. Designed for readable and reproducible workflows, \n    including support for Monte Carlo simulations and benchmarking.  "
  },
  {
    "id": 12808,
    "package_name": "fxl",
    "title": "'fxl' Single Case Design Charting Package",
    "description": "The 'fxl' Charting package is used to prepare and design single case design figures that are typically prepared in spreadsheet software. With 'fxl', there is no need to leave the R environment to prepare these works and many of the more unique conventions in single case experimental designs can be performed without the need for physically constructing features of plots (e.g., drawing annotations across plots). Support is provided for various different plotting arrangements (e.g., multiple baseline), annotations (e.g., brackets, arrows), and output formats (e.g., svg, rasters).",
    "version": "1.7.2",
    "maintainer": "Shawn Gilroy <sgilroy1@lsu.edu>",
    "author": "Shawn Gilroy [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1097-8366>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fxl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fxl 'fxl' Single Case Design Charting Package The 'fxl' Charting package is used to prepare and design single case design figures that are typically prepared in spreadsheet software. With 'fxl', there is no need to leave the R environment to prepare these works and many of the more unique conventions in single case experimental designs can be performed without the need for physically constructing features of plots (e.g., drawing annotations across plots). Support is provided for various different plotting arrangements (e.g., multiple baseline), annotations (e.g., brackets, arrows), and output formats (e.g., svg, rasters).  "
  },
  {
    "id": 12909,
    "package_name": "gauseR",
    "title": "Lotka-Volterra Models for Gause's 'Struggle for Existence'",
    "description": "A collection of tools and data for analyzing the Gause microcosm experiments, and for fitting Lotka-Volterra models to time series data. Includes methods for fitting single-species logistic growth, and multi-species interaction models, e.g. of competition, predator/prey relationships, or mutualism. See documentation for individual functions for examples. In general, see the lv_optim() function for examples of how to fit parameter values in multi-species systems. Note that the general methods applied here, as well as the form of the differential equations that we use, are described in detail in the Quantitative Ecology textbook by Lehman et al., available at <http://hdl.handle.net/11299/204551>, and in Lina K. M\u00fchlbauer, Maximilienne Schulze, W. Stanley Harpole, and Adam T. Clark. 'gauseR': Simple methods for fitting Lotka-Volterra models describing Gause's 'Struggle for Existence' in the journal Ecology and Evolution.",
    "version": "1.3",
    "maintainer": "Adam Clark <adam.tclark@gmail.com>",
    "author": "Adam Clark [aut, cre] (ORCID: <https://orcid.org/0000-0002-8843-3278>),\n  Lina M\u00fchlbauer [aut],\n  Maximilienne Schulze [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gauseR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gauseR Lotka-Volterra Models for Gause's 'Struggle for Existence' A collection of tools and data for analyzing the Gause microcosm experiments, and for fitting Lotka-Volterra models to time series data. Includes methods for fitting single-species logistic growth, and multi-species interaction models, e.g. of competition, predator/prey relationships, or mutualism. See documentation for individual functions for examples. In general, see the lv_optim() function for examples of how to fit parameter values in multi-species systems. Note that the general methods applied here, as well as the form of the differential equations that we use, are described in detail in the Quantitative Ecology textbook by Lehman et al., available at <http://hdl.handle.net/11299/204551>, and in Lina K. M\u00fchlbauer, Maximilienne Schulze, W. Stanley Harpole, and Adam T. Clark. 'gauseR': Simple methods for fitting Lotka-Volterra models describing Gause's 'Struggle for Existence' in the journal Ecology and Evolution.  "
  },
  {
    "id": 12911,
    "package_name": "gausscov",
    "title": "The Gaussian Covariate Method for Variable Selection",
    "description": "The standard linear regression theory whether frequentist or Bayesian is based on an 'assumed (revealed?) truth' (John Tukey) attitude to models. This is reflected in the language of statistical inference which involves a concept of truth, for example confidence intervals, hypothesis testing and consistency. The motivation behind this package was to remove the word true from the theory and practice of linear regression and to replace it by approximation. The approximations considered are the least squares approximations. An approximation is called valid if it contains no irrelevant covariates. This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate. The precise definition given in the paper  \"An Approximation Based Theory of Linear Regression\".  Only four simple equations are required. Moreover the Gaussian P-values can be simply derived from standard F P-values. Furthermore they are exact and valid whatever the data in contrast F P-values are only valid for specially designed simulations. A valid approximation is one where all the Gaussian P-values are less than a threshold p0 specified by the statistician, in this package with the default value 0.01. This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach. The will be demonstrated using high dimensional regression and vector autoregression real data sets. The goal is to find valid approximations. The search function is f1st which is a greedy forward selection procedure which results in either just one or no approximations which may however not be valid. If the size is less than than a threshold with default value 21 then an all subset procedure is called which returns the best valid subset. A good default start is f1st(y,x,kmn=15) The best function for returning multiple approximations is f3st which repeatedly calls f1st. For more information see the papers: L. Davies and L. Duembgen, \"Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities\", <doi:10.48550/arXiv.2202.01553>, L. Davies, \"An Approximation Based Theory of Linear Regression\", 2024, <doi:10.48550/arXiv.2402.09858>.",
    "version": "1.1.8",
    "maintainer": "Laurie Davies <pldavies44@cantab.net>",
    "author": "Laurie Davies [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gausscov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gausscov The Gaussian Covariate Method for Variable Selection The standard linear regression theory whether frequentist or Bayesian is based on an 'assumed (revealed?) truth' (John Tukey) attitude to models. This is reflected in the language of statistical inference which involves a concept of truth, for example confidence intervals, hypothesis testing and consistency. The motivation behind this package was to remove the word true from the theory and practice of linear regression and to replace it by approximation. The approximations considered are the least squares approximations. An approximation is called valid if it contains no irrelevant covariates. This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate. The precise definition given in the paper  \"An Approximation Based Theory of Linear Regression\".  Only four simple equations are required. Moreover the Gaussian P-values can be simply derived from standard F P-values. Furthermore they are exact and valid whatever the data in contrast F P-values are only valid for specially designed simulations. A valid approximation is one where all the Gaussian P-values are less than a threshold p0 specified by the statistician, in this package with the default value 0.01. This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach. The will be demonstrated using high dimensional regression and vector autoregression real data sets. The goal is to find valid approximations. The search function is f1st which is a greedy forward selection procedure which results in either just one or no approximations which may however not be valid. If the size is less than than a threshold with default value 21 then an all subset procedure is called which returns the best valid subset. A good default start is f1st(y,x,kmn=15) The best function for returning multiple approximations is f3st which repeatedly calls f1st. For more information see the papers: L. Davies and L. Duembgen, \"Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities\", <doi:10.48550/arXiv.2202.01553>, L. Davies, \"An Approximation Based Theory of Linear Regression\", 2024, <doi:10.48550/arXiv.2402.09858>.  "
  },
  {
    "id": 12929,
    "package_name": "gcKrig",
    "title": "Analysis of Geostatistical Count Data using Gaussian Copulas",
    "description": "Provides a variety of functions to analyze and model\n    geostatistical count data with Gaussian copulas, including\n 1) data simulation and visualization; \n 2) correlation structure assessment (here also known as the Normal To Anything); \n 3) calculate multivariate normal rectangle probabilities; \n 4) likelihood inference and parallel prediction at predictive locations.\n Description of the method is available from: Han and DeOliveira (2018) <doi:10.18637/jss.v087.i13>.",
    "version": "1.1.8",
    "maintainer": "Zifei Han <hanzifei1@gmail.com>",
    "author": "Zifei Han",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gcKrig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gcKrig Analysis of Geostatistical Count Data using Gaussian Copulas Provides a variety of functions to analyze and model\n    geostatistical count data with Gaussian copulas, including\n 1) data simulation and visualization; \n 2) correlation structure assessment (here also known as the Normal To Anything); \n 3) calculate multivariate normal rectangle probabilities; \n 4) likelihood inference and parallel prediction at predictive locations.\n Description of the method is available from: Han and DeOliveira (2018) <doi:10.18637/jss.v087.i13>.  "
  },
  {
    "id": 12932,
    "package_name": "gcdnet",
    "title": "The (Adaptive) LASSO and Elastic Net Penalized Least Squares,\nLogistic Regression, Hybrid Huberized Support Vector Machines,\nSquared Hinge Loss Support Vector Machines and Expectile\nRegression using a Fast Generalized Coordinate Descent\nAlgorithm",
    "description": "Implements a generalized coordinate descent (GCD) algorithm\n    for computing the solution paths of the hybrid Huberized support vector\n    machine (HHSVM) and its generalizations. Supported models include the\n    (adaptive) LASSO and elastic net penalized least squares, logistic\n    regression, HHSVM, squared hinge loss SVM and expectile regression.",
    "version": "1.0.6",
    "maintainer": "Yi Yang <yi.yang6@mcgill.ca>",
    "author": "Yi Yang <yi.yang6@mcgill.ca>, Yuwen Gu <yuwen.gu@uconn.edu>, Hui Zou\n    <hzou@stat.umn.edu>",
    "url": "https://github.com/emeryyi/gcdnet",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gcdnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gcdnet The (Adaptive) LASSO and Elastic Net Penalized Least Squares,\nLogistic Regression, Hybrid Huberized Support Vector Machines,\nSquared Hinge Loss Support Vector Machines and Expectile\nRegression using a Fast Generalized Coordinate Descent\nAlgorithm Implements a generalized coordinate descent (GCD) algorithm\n    for computing the solution paths of the hybrid Huberized support vector\n    machine (HHSVM) and its generalizations. Supported models include the\n    (adaptive) LASSO and elastic net penalized least squares, logistic\n    regression, HHSVM, squared hinge loss SVM and expectile regression.  "
  },
  {
    "id": 12944,
    "package_name": "gdalUtilities",
    "title": "Wrappers for 'GDAL' Utilities Executables",
    "description": "R's 'sf' package ships with self-contained 'GDAL'\n    executables, including a bare bones interface to several\n    'GDAL'-related utility programs collectively known as the 'GDAL\n    utilities'. For each of those utilities, this package provides an\n    R wrapper whose formal arguments closely mirror those of the\n    'GDAL' command line interface. The utilities operate on data\n    stored in files and typically write their output to other\n    files. Therefore, to process data stored in any of R's more common\n    spatial formats (i.e. those supported by the 'sf' and 'terra'\n    packages), first write them to disk, then process them with the\n    package's wrapper functions before reading the outputted results\n    back into R. GDAL function arguments introduced in GDAL version\n    3.5.2 or earlier are supported.",
    "version": "1.2.5",
    "maintainer": "Joshua O'Brien <joshmobrien@gmail.com>",
    "author": "Joshua O'Brien",
    "url": "https://github.com/JoshOBrien/gdalUtilities/",
    "bug_reports": "https://github.com/JoshOBrien/gdalUtilities/issues/",
    "repository": "https://cran.r-project.org/package=gdalUtilities",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdalUtilities Wrappers for 'GDAL' Utilities Executables R's 'sf' package ships with self-contained 'GDAL'\n    executables, including a bare bones interface to several\n    'GDAL'-related utility programs collectively known as the 'GDAL\n    utilities'. For each of those utilities, this package provides an\n    R wrapper whose formal arguments closely mirror those of the\n    'GDAL' command line interface. The utilities operate on data\n    stored in files and typically write their output to other\n    files. Therefore, to process data stored in any of R's more common\n    spatial formats (i.e. those supported by the 'sf' and 'terra'\n    packages), first write them to disk, then process them with the\n    package's wrapper functions before reading the outputted results\n    back into R. GDAL function arguments introduced in GDAL version\n    3.5.2 or earlier are supported.  "
  },
  {
    "id": 12945,
    "package_name": "gdalcubes",
    "title": "Earth Observation Data Cubes from Satellite Image Collections",
    "description": "Processing collections of Earth observation images as on-demand multispectral, multitemporal raster data cubes. Users\n    define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and \n    resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). Implemented functions on data cubes include reduction over space and time, \n    applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, \n    exporting data cubes as 'netCDF' or 'GeoTIFF' files, plotting, and extraction from spatial and or spatiotemporal features.  \n    All computational parts are implemented in C++, linking to the 'GDAL', 'netCDF', 'CURL', and 'SQLite' libraries. \n    See Appel and Pebesma (2019) <doi:10.3390/data4030092> for further details.",
    "version": "0.7.2",
    "maintainer": "Marius Appel <marius.appel@hs-bochum.de>",
    "author": "Marius Appel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5281-3896>),\n  Edzer Pebesma [ctb] (ORCID: <https://orcid.org/0000-0001-8049-7069>),\n  Roger Bivand [ctb],\n  Jeroen Ooms [ctb] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  Lewis Van Winkle [cph],\n  Ole Christian Eidheim [cph],\n  Howard Hinnant [cph],\n  Adrian Colomitchi [cph],\n  Florian Dang [cph],\n  Paul Thompson [cph],\n  Tomasz Kami\u0144ski [cph],\n  Dropbox, Inc. [cph]",
    "url": "https://github.com/appelmar/gdalcubes",
    "bug_reports": "https://github.com/appelmar/gdalcubes/issues/",
    "repository": "https://cran.r-project.org/package=gdalcubes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdalcubes Earth Observation Data Cubes from Satellite Image Collections Processing collections of Earth observation images as on-demand multispectral, multitemporal raster data cubes. Users\n    define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and \n    resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). Implemented functions on data cubes include reduction over space and time, \n    applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, \n    exporting data cubes as 'netCDF' or 'GeoTIFF' files, plotting, and extraction from spatial and or spatiotemporal features.  \n    All computational parts are implemented in C++, linking to the 'GDAL', 'netCDF', 'CURL', and 'SQLite' libraries. \n    See Appel and Pebesma (2019) <doi:10.3390/data4030092> for further details.  "
  },
  {
    "id": 12946,
    "package_name": "gdalraster",
    "title": "Bindings to 'GDAL'",
    "description": "API bindings to the Geospatial Data Abstraction Library ('GDAL',\n    <https://gdal.org>). Implements the 'GDAL' Raster and Vector Data Models.\n    Bindings are implemented with 'Rcpp' modules. Exposed C++ classes and\n    stand-alone functions wrap much of the 'GDAL' API and provide additional\n    functionality. Calling signatures resemble the native C, C++ and Python APIs\n    provided by the 'GDAL' project. Class 'GDALRaster' encapsulates a\n    'GDALDataset' and its raster band objects. Class 'GDALVector' encapsulates\n    an 'OGRLayer' and the 'GDALDataset' that contains it. Initial bindings are\n    provided to the unified 'gdal' command line interface added in 'GDAL' 3.11.\n    C++ stand-alone functions provide bindings to most 'GDAL' \"traditional\"\n    raster and vector utilities, including 'OGR' facilities for vector\n    geoprocessing, several algorithms, as well as the Geometry API ('GEOS' via\n    'GDAL' headers), the Spatial Reference Systems API, and methods for\n    coordinate transformation. Bindings to the Virtual Systems Interface ('VSI')\n    API implement standard file system operations abstracted for URLs, cloud\n    storage services, 'Zip'/'GZip'/'7z'/'RAR', in-memory files, as well as\n    regular local file systems. This provides a single interface for operating\n    on file system objects that works the same for any storage backend. A custom\n    raster calculator evaluates a user-defined R expression on a layer or stack\n    of layers, with pixel x/y available as variables in the expression. Raster\n    'combine()' identifies and counts unique pixel combinations across multiple\n    input layers, with optional raster output of the pixel-level combination\n    IDs. Basic plotting capability is provided for raster and vector display.\n    'gdalraster' leans toward minimalism and the use of simple, lightweight\n    objects for holding raw data. Currently, only minimal S3 class interfaces\n    have been implemented for selected R objects that contain spatial data.\n    'gdalraster' may be useful in applications that need scalable, low-level\n    I/O, or prefer a direct 'GDAL' API.",
    "version": "2.3.0",
    "maintainer": "Chris Toney <jctoney@gmail.com>",
    "author": "Chris Toney [aut, cre] (R interface/additional functionality),\n  Michael D. Sumner [ctb],\n  Frank Warmerdam [ctb, cph] (GDAL API documentation; src/progress_r.cpp\n    from gdal/port/cpl_progress.cpp),\n  Even Rouault [ctb, cph] (GDAL API documentation),\n  Marius Appel [ctb, cph] (configure.ac based on\n    https://github.com/appelmar/gdalcubes),\n  Daniel James [ctb, cph] (Boost combine hashes method in\n    src/cmb_table.h),\n  Peter Dimov [ctb, cph] (Boost combine hashes method in src/cmb_table.h)",
    "url": "https://usdaforestservice.github.io/gdalraster/,\nhttps://github.com/USDAForestService/gdalraster",
    "bug_reports": "https://github.com/USDAForestService/gdalraster/issues",
    "repository": "https://cran.r-project.org/package=gdalraster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdalraster Bindings to 'GDAL' API bindings to the Geospatial Data Abstraction Library ('GDAL',\n    <https://gdal.org>). Implements the 'GDAL' Raster and Vector Data Models.\n    Bindings are implemented with 'Rcpp' modules. Exposed C++ classes and\n    stand-alone functions wrap much of the 'GDAL' API and provide additional\n    functionality. Calling signatures resemble the native C, C++ and Python APIs\n    provided by the 'GDAL' project. Class 'GDALRaster' encapsulates a\n    'GDALDataset' and its raster band objects. Class 'GDALVector' encapsulates\n    an 'OGRLayer' and the 'GDALDataset' that contains it. Initial bindings are\n    provided to the unified 'gdal' command line interface added in 'GDAL' 3.11.\n    C++ stand-alone functions provide bindings to most 'GDAL' \"traditional\"\n    raster and vector utilities, including 'OGR' facilities for vector\n    geoprocessing, several algorithms, as well as the Geometry API ('GEOS' via\n    'GDAL' headers), the Spatial Reference Systems API, and methods for\n    coordinate transformation. Bindings to the Virtual Systems Interface ('VSI')\n    API implement standard file system operations abstracted for URLs, cloud\n    storage services, 'Zip'/'GZip'/'7z'/'RAR', in-memory files, as well as\n    regular local file systems. This provides a single interface for operating\n    on file system objects that works the same for any storage backend. A custom\n    raster calculator evaluates a user-defined R expression on a layer or stack\n    of layers, with pixel x/y available as variables in the expression. Raster\n    'combine()' identifies and counts unique pixel combinations across multiple\n    input layers, with optional raster output of the pixel-level combination\n    IDs. Basic plotting capability is provided for raster and vector display.\n    'gdalraster' leans toward minimalism and the use of simple, lightweight\n    objects for holding raw data. Currently, only minimal S3 class interfaces\n    have been implemented for selected R objects that contain spatial data.\n    'gdalraster' may be useful in applications that need scalable, low-level\n    I/O, or prefer a direct 'GDAL' API.  "
  },
  {
    "id": 12947,
    "package_name": "gdata",
    "title": "Various R Programming Tools for Data Manipulation",
    "description": "Various R programming tools for data manipulation, including\n  medical unit conversions, combining objects, character vector operations,\n  factor manipulation, obtaining information about R objects, generating\n  fixed-width format files, extracting components of date & time objects,\n  operations on columns of data frames, matrix operations, operations on\n  vectors, operations on data frames, value of last evaluated expression, and a\n  resample() wrapper for sample() that ensures consistent behavior for both\n  scalar and vector arguments.",
    "version": "3.0.1",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Gregory R. Warnes [aut],\n  Gregor Gorjanc [aut],\n  Arni Magnusson [aut, cre],\n  Liviu Andronic [aut],\n  Jim Rogers [aut],\n  Don MacQueen [aut],\n  Ales Korosec [aut],\n  Ben Bolker [ctb],\n  Michael Chirico [ctb],\n  Gabor Grothendieck [ctb],\n  Thomas Lumley [ctb],\n  Brian Ripley [ctb],\n  inoui llc [fnd]",
    "url": "https://github.com/r-gregmisc/gdata",
    "bug_reports": "https://github.com/r-gregmisc/gdata/issues",
    "repository": "https://cran.r-project.org/package=gdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdata Various R Programming Tools for Data Manipulation Various R programming tools for data manipulation, including\n  medical unit conversions, combining objects, character vector operations,\n  factor manipulation, obtaining information about R objects, generating\n  fixed-width format files, extracting components of date & time objects,\n  operations on columns of data frames, matrix operations, operations on\n  vectors, operations on data frames, value of last evaluated expression, and a\n  resample() wrapper for sample() that ensures consistent behavior for both\n  scalar and vector arguments.  "
  },
  {
    "id": 12950,
    "package_name": "gdiff",
    "title": "Graphical Difference Testing",
    "description": "Functions for performing graphical difference testing.     \n             Differences are generated between raster images.\n             Comparisons can be performed between different package\n             versions and between different R versions.",
    "version": "0.2-5",
    "maintainer": "Paul Murrell <paul@stat.auckland.ac.nz>",
    "author": "Paul Murrell",
    "url": "https://github.com/pmur002/,\nhttps://stattech.wordpress.fos.auckland.ac.nz/2020/01/06/2020-01-visual-testing-for-graphics-in-r/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gdiff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdiff Graphical Difference Testing Functions for performing graphical difference testing.     \n             Differences are generated between raster images.\n             Comparisons can be performed between different package\n             versions and between different R versions.  "
  },
  {
    "id": 12951,
    "package_name": "gdim",
    "title": "Estimate Graph Dimension using Cross-Validated Eigenvalues",
    "description": "Cross-validated eigenvalues are estimated by\n    splitting a graph into two parts, the training and the test graph.\n    The training graph is used to estimate eigenvectors, and\n    the test graph is used to evaluate the correlation between the training\n    eigenvectors and the eigenvectors of the test graph.\n    The correlations follow a simple central limit theorem that can\n    be used to estimate graph dimension via hypothesis testing, see\n    Chen et al. (2021) <doi:10.48550/arXiv.2108.03336> for details.",
    "version": "0.1.1",
    "maintainer": "Alex Hayes <alexpghayes@gmail.com>",
    "author": "Fan Chen [aut] (ORCID: <https://orcid.org/0000-0003-4508-6023>),\n  Alex Hayes [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-4985-5160>),\n  Karl Rohe [aut]",
    "url": "https://github.com/RoheLab/gdim, https://rohelab.github.io/gdim/",
    "bug_reports": "https://github.com/RoheLab/gdim/issues",
    "repository": "https://cran.r-project.org/package=gdim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdim Estimate Graph Dimension using Cross-Validated Eigenvalues Cross-validated eigenvalues are estimated by\n    splitting a graph into two parts, the training and the test graph.\n    The training graph is used to estimate eigenvectors, and\n    the test graph is used to evaluate the correlation between the training\n    eigenvectors and the eigenvectors of the test graph.\n    The correlations follow a simple central limit theorem that can\n    be used to estimate graph dimension via hypothesis testing, see\n    Chen et al. (2021) <doi:10.48550/arXiv.2108.03336> for details.  "
  },
  {
    "id": 12952,
    "package_name": "gdistance",
    "title": "Distances and Routes on Geographical Grids",
    "description": "Provides classes and functions to calculate various \n             distance measures and routes in heterogeneous geographic \n             spaces represented as grids. The package implements measures\n             to model dispersal histories first presented by van Etten and\n             Hijmans (2010) <doi:10.1371/journal.pone.0012060>. Least-cost\n             distances as well as more complex distances based on (constrained)\n             random walks can be calculated. The distances implemented in \n             the package are used in geographical genetics, accessibility \n             indicators, and may also have applications in other fields of\n             geospatial analysis.",
    "version": "1.6.5",
    "maintainer": "Andrew Marx <ajm.rpackages@gmail.com>",
    "author": "Jacob van Etten [aut] (ORCID: <https://orcid.org/0000-0001-7554-2558>),\n  Kau\u00ea de Sousa [ctb] (ORCID: <https://orcid.org/0000-0002-7571-7845>),\n  Andrew Marx [cre, ctb] (ORCID: <https://orcid.org/0000-0002-7456-1631>)",
    "url": "https://AgrDataSci.github.io/gdistance/",
    "bug_reports": "https://github.com/AgrDataSci/gdistance/issues",
    "repository": "https://cran.r-project.org/package=gdistance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdistance Distances and Routes on Geographical Grids Provides classes and functions to calculate various \n             distance measures and routes in heterogeneous geographic \n             spaces represented as grids. The package implements measures\n             to model dispersal histories first presented by van Etten and\n             Hijmans (2010) <doi:10.1371/journal.pone.0012060>. Least-cost\n             distances as well as more complex distances based on (constrained)\n             random walks can be calculated. The distances implemented in \n             the package are used in geographical genetics, accessibility \n             indicators, and may also have applications in other fields of\n             geospatial analysis.  "
  },
  {
    "id": 12960,
    "package_name": "gear",
    "title": "Geostatistical Analysis in R",
    "description": "Implements common geostatistical methods in a clean, straightforward, efficient manner. The methods are discussed in Schabenberger and Gotway (2004, <ISBN:9781584883227>) and Waller and Gotway (2004, <ISBN:9780471387718>).",
    "version": "0.3.4",
    "maintainer": "Joshua French <joshua.french@ucdenver.edu>",
    "author": "Joshua French",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gear",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gear Geostatistical Analysis in R Implements common geostatistical methods in a clean, straightforward, efficient manner. The methods are discussed in Schabenberger and Gotway (2004, <ISBN:9781584883227>) and Waller and Gotway (2004, <ISBN:9780471387718>).  "
  },
  {
    "id": 12962,
    "package_name": "gecko",
    "title": "Geographical Ecology and Conservation Knowledge Online",
    "description": "Includes a collection of geographical analysis functions aimed primarily at ecology and conservation science studies, allowing processing of both point and raster data. Now integrates SPECTRE (<https://biodiversityresearch.org/spectre/>), a dataset of global geospatial threat data, developed by the authors.",
    "version": "1.0.2",
    "maintainer": "Vasco V. Branco <vasco.branco@helsinki.fi>",
    "author": "Vasco V. Branco [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-7797-3183>),\n  Pedro Cardoso [aut] (ORCID: <https://orcid.org/0000-0001-8119-9960>),\n  Lu\u00eds Correia [ctb] (ORCID: <https://orcid.org/0000-0003-2439-1168>)",
    "url": "https://github.com/VascoBranco/gecko",
    "bug_reports": "https://github.com/VascoBranco/gecko/issues",
    "repository": "https://cran.r-project.org/package=gecko",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gecko Geographical Ecology and Conservation Knowledge Online Includes a collection of geographical analysis functions aimed primarily at ecology and conservation science studies, allowing processing of both point and raster data. Now integrates SPECTRE (<https://biodiversityresearch.org/spectre/>), a dataset of global geospatial threat data, developed by the authors.  "
  },
  {
    "id": 12964,
    "package_name": "geeCRT",
    "title": "Bias-Corrected GEE for Cluster Randomized Trials",
    "description": "Population-averaged models have been increasingly used in the design and analysis of \n             cluster randomized trials (CRTs). To facilitate the applications of population-averaged \n             models in CRTs, the package implements the generalized estimating equations (GEE) and \n             matrix-adjusted estimating equations (MAEE) approaches to jointly estimate the marginal \n             mean models correlation models both for general CRTs and stepped wedge CRTs. Despite the \n             general GEE/MAEE approach, the package also implements a fast cluster-period GEE method by \n             Li et al. (2022) <doi:10.1093/biostatistics/kxaa056>\n             specifically for stepped wedge CRTs with large and variable cluster-period sizes and gives \n             a simple and efficient estimating equations approach based on the cluster-period means to \n             estimate the intervention effects as well as correlation parameters. In addition, the package \n             also provides functions for generating correlated binary data with specific mean vector and \n             correlation matrix based on the multivariate probit method in Emrich and Piedmonte (1991) <doi:10.1080/00031305.1991.10475828> or \n             the conditional linear family method in Qaqish (2003) <doi:10.1093/biomet/90.2.455>. ",
    "version": "1.1.5",
    "maintainer": "Hengshi Yu <hengshi@umich.edu>",
    "author": "Hengshi Yu [aut, cre],\n  Fan Li [aut],\n  Paul Rathouz [aut],\n  Elizabeth L. Turner [aut],\n  John Preisser [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geeCRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geeCRT Bias-Corrected GEE for Cluster Randomized Trials Population-averaged models have been increasingly used in the design and analysis of \n             cluster randomized trials (CRTs). To facilitate the applications of population-averaged \n             models in CRTs, the package implements the generalized estimating equations (GEE) and \n             matrix-adjusted estimating equations (MAEE) approaches to jointly estimate the marginal \n             mean models correlation models both for general CRTs and stepped wedge CRTs. Despite the \n             general GEE/MAEE approach, the package also implements a fast cluster-period GEE method by \n             Li et al. (2022) <doi:10.1093/biostatistics/kxaa056>\n             specifically for stepped wedge CRTs with large and variable cluster-period sizes and gives \n             a simple and efficient estimating equations approach based on the cluster-period means to \n             estimate the intervention effects as well as correlation parameters. In addition, the package \n             also provides functions for generating correlated binary data with specific mean vector and \n             correlation matrix based on the multivariate probit method in Emrich and Piedmonte (1991) <doi:10.1080/00031305.1991.10475828> or \n             the conditional linear family method in Qaqish (2003) <doi:10.1093/biomet/90.2.455>.   "
  },
  {
    "id": 12974,
    "package_name": "geigen",
    "title": "Calculate Generalized Eigenvalues, the Generalized Schur\nDecomposition and the Generalized Singular Value Decomposition\nof a Matrix Pair with Lapack",
    "description": "Functions to compute generalized eigenvalues and eigenvectors,\n             the generalized Schur decomposition and\n             the generalized Singular Value Decomposition of a matrix pair,\n             using Lapack routines.",
    "version": "2.3",
    "maintainer": "Berend Hasselman <bhh@xs4all.nl>",
    "author": "Berend Hasselman [cre, aut],\n  Lapack authors [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geigen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geigen Calculate Generalized Eigenvalues, the Generalized Schur\nDecomposition and the Generalized Singular Value Decomposition\nof a Matrix Pair with Lapack Functions to compute generalized eigenvalues and eigenvectors,\n             the generalized Schur decomposition and\n             the generalized Singular Value Decomposition of a matrix pair,\n             using Lapack routines.  "
  },
  {
    "id": 12976,
    "package_name": "gek",
    "title": "Gradient-Enhanced Kriging",
    "description": "Gradient-Enhanced Kriging as an emulator for computer experiments based on Maximum-Likelihood estimation.",
    "version": "1.1.0",
    "maintainer": "Carmen van Meegen <vanmeegen@statistik.tu-dortmund.de>",
    "author": "Carmen van Meegen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4125-5088>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gek",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gek Gradient-Enhanced Kriging Gradient-Enhanced Kriging as an emulator for computer experiments based on Maximum-Likelihood estimation.  "
  },
  {
    "id": 13013,
    "package_name": "genero",
    "title": "Estimate Gender from Names in Spanish and Portuguese",
    "description": "Estimate gender from names in Spanish and Portuguese. \n    Works with vectors and dataframes. The estimation works not only\n    for first names but also full names. The package relies on a\n    compilation of common names with it's most frequent associated \n    gender in both languages which are used as look up tables for gender\n    inference.",
    "version": "0.1.0",
    "maintainer": "Juan Pablo Marin Diaz <jpmarindiaz@gmail.com>",
    "author": "Juan Pablo Marin Diaz [aut, cre]",
    "url": "https://github.com/datasketch/genero",
    "bug_reports": "https://github.com/datasketch/genero/issues",
    "repository": "https://cran.r-project.org/package=genero",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genero Estimate Gender from Names in Spanish and Portuguese Estimate gender from names in Spanish and Portuguese. \n    Works with vectors and dataframes. The estimation works not only\n    for first names but also full names. The package relies on a\n    compilation of common names with it's most frequent associated \n    gender in both languages which are used as look up tables for gender\n    inference.  "
  },
  {
    "id": 13040,
    "package_name": "gensvm",
    "title": "A Generalized Multiclass Support Vector Machine",
    "description": "The GenSVM classifier is a generalized multiclass support vector\n\tmachine (SVM). This classifier aims to find decision boundaries that\n\tseparate the classes with as wide a margin as possible. In GenSVM, the\n\tloss function is very flexible in the way that misclassifications are\n\tpenalized.  This allows the user to tune the classifier to the dataset\n\tat hand and potentially obtain higher classification accuracy than\n\talternative multiclass SVMs.  Moreover, this flexibility means that\n\tGenSVM has a number of other multiclass SVMs as special cases. One of\n\tthe other advantages of GenSVM is that it is trained in the primal\n\tspace, allowing the use of warm starts during optimization.  This\n\tmeans that for common tasks such as cross validation or repeated model\n\tfitting, GenSVM can be trained very quickly. Based on: G.J.J. van den\n\tBurg and P.J.F. Groenen (2018) <https://www.jmlr.org/papers/v17/14-526.html>.",
    "version": "0.1.7",
    "maintainer": "Gertjan van den Burg <gertjanvandenburg@gmail.com>",
    "author": "Gertjan van den Burg [aut, cre],\n  Patrick Groenen [ctb]",
    "url": "https://github.com/GjjvdBurg/RGenSVM\nhttps://jmlr.org/papers/v17/14-526.html",
    "bug_reports": "https://github.com/GjjvdBurg/RGenSVM",
    "repository": "https://cran.r-project.org/package=gensvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gensvm A Generalized Multiclass Support Vector Machine The GenSVM classifier is a generalized multiclass support vector\n\tmachine (SVM). This classifier aims to find decision boundaries that\n\tseparate the classes with as wide a margin as possible. In GenSVM, the\n\tloss function is very flexible in the way that misclassifications are\n\tpenalized.  This allows the user to tune the classifier to the dataset\n\tat hand and potentially obtain higher classification accuracy than\n\talternative multiclass SVMs.  Moreover, this flexibility means that\n\tGenSVM has a number of other multiclass SVMs as special cases. One of\n\tthe other advantages of GenSVM is that it is trained in the primal\n\tspace, allowing the use of warm starts during optimization.  This\n\tmeans that for common tasks such as cross validation or repeated model\n\tfitting, GenSVM can be trained very quickly. Based on: G.J.J. van den\n\tBurg and P.J.F. Groenen (2018) <https://www.jmlr.org/papers/v17/14-526.html>.  "
  },
  {
    "id": 13042,
    "package_name": "geoAr",
    "title": "Argentina's Spatial Data Toolbox",
    "description": "Collection of tools that facilitates data access and workflow for spatial analysis of Argentina. Includes historical information from censuses, administrative limits at different levels of aggregation, location of human settlements, among others. Since it is expected that the majority of users will be Spanish-speaking, the documentation of the package prioritizes this language, although an effort is made to also offer annotations in English. ",
    "version": "1.0.0",
    "maintainer": "Juan Pablo Ruiz Nicolini <juanpabloruiznicolini@gmail.com>",
    "author": "Juan Pablo Ruiz Nicolini [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3138-6343>),\n  Patricio Del Boca [aut],\n  Juan Gabriel Juara [aut]",
    "url": "https://github.com/PoliticaArgentina/geoAr",
    "bug_reports": "https://github.com/PoliticaArgentina/geoAr/issues",
    "repository": "https://cran.r-project.org/package=geoAr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoAr Argentina's Spatial Data Toolbox Collection of tools that facilitates data access and workflow for spatial analysis of Argentina. Includes historical information from censuses, administrative limits at different levels of aggregation, location of human settlements, among others. Since it is expected that the majority of users will be Spanish-speaking, the documentation of the package prioritizes this language, although an effort is made to also offer annotations in English.   "
  },
  {
    "id": 13043,
    "package_name": "geoBayes",
    "title": "Analysis of Geostatistical Data using Bayes and Empirical Bayes\nMethods",
    "description": "Functions to fit geostatistical data. The data can be\n        continuous, binary or count data and the models implemented are\n        flexible. Conjugate priors are assumed on some parameters while\n        inference on the other parameters can be done through a full\n        Bayesian analysis of by empirical Bayes methods.",
    "version": "0.7.4",
    "maintainer": "Evangelos Evangelou <e.evangelou@maths.bath.ac.uk>",
    "author": "Evangelos Evangelou [aut, cre],\n  Vivekananda Roy [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geoBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoBayes Analysis of Geostatistical Data using Bayes and Empirical Bayes\nMethods Functions to fit geostatistical data. The data can be\n        continuous, binary or count data and the models implemented are\n        flexible. Conjugate priors are assumed on some parameters while\n        inference on the other parameters can be done through a full\n        Bayesian analysis of by empirical Bayes methods.  "
  },
  {
    "id": 13044,
    "package_name": "geoFKF",
    "title": "Kriging Method for Spatial Functional Data",
    "description": "A Kriging method for functional datasets with spatial dependency.\n    This functional Kriging method avoids the need to estimate the\n    trace-variogram, and the curve is estimated by minimizing a quadratic\n    form. The curves in the functional dataset are smoothed using Fourier\n    series. The functional Kriging of this package is a modification of the\n    method proposed by Giraldo (2011) <doi:10.1007/s10651-010-0143-y>.",
    "version": "0.1.1",
    "maintainer": "Gilberto Sassi <sassi.pereira.gilberto@gmail.com>",
    "author": "Gilberto Sassi [aut, cre]",
    "url": "https://github.com/gilberto-sassi/geoFKF",
    "bug_reports": "https://github.com/gilberto-sassi/geoFKF/issues",
    "repository": "https://cran.r-project.org/package=geoFKF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoFKF Kriging Method for Spatial Functional Data A Kriging method for functional datasets with spatial dependency.\n    This functional Kriging method avoids the need to estimate the\n    trace-variogram, and the curve is estimated by minimizing a quadratic\n    form. The curves in the functional dataset are smoothed using Fourier\n    series. The functional Kriging of this package is a modification of the\n    method proposed by Giraldo (2011) <doi:10.1007/s10651-010-0143-y>.  "
  },
  {
    "id": 13045,
    "package_name": "geoFourierFDA",
    "title": "Ordinary Functional Kriging Using Fourier Smoothing and Gaussian\nQuadrature",
    "description": "Implementation of the ordinary functional kriging method \n    proposed by Giraldo (2011) <doi:10.1007/s10651-010-0143-y>. This\n    implements an alternative method to estimate the trace-variogram using\n    Fourier Smoothing and Gaussian Quadrature. ",
    "version": "0.1.0",
    "maintainer": "Gilberto Sassi <sassi.pereira.gilberto@gmail.com>",
    "author": "Gilberto Sassi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geoFourierFDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoFourierFDA Ordinary Functional Kriging Using Fourier Smoothing and Gaussian\nQuadrature Implementation of the ordinary functional kriging method \n    proposed by Giraldo (2011) <doi:10.1007/s10651-010-0143-y>. This\n    implements an alternative method to estimate the trace-variogram using\n    Fourier Smoothing and Gaussian Quadrature.   "
  },
  {
    "id": 13046,
    "package_name": "geoGAM",
    "title": "Select Sparse Geoadditive Models for Spatial Prediction",
    "description": "A model building procedure to build parsimonious geoadditive model from a large number of covariates. Continuous, binary and ordered categorical responses are supported. The model building is based on component wise gradient boosting with linear effects, smoothing splines and a smooth spatial surface to model spatial autocorrelation. The resulting covariate set after gradient boosting is further reduced through backward elimination and aggregation of factor levels. The package provides a model based bootstrap method to simulate prediction intervals for point predictions. A test data set of a soil mapping case study in Berne (Switzerland) is provided. Nussbaum, M., Walthert, L., Fraefel, M., Greiner, L., and Papritz, A. (2017) <doi:10.5194/soil-3-191-2017>. ",
    "version": "0.1-4",
    "maintainer": "Madlene Nussbaum <m.nussbaum@uu.nl>",
    "author": "Madlene Nussbaum [cre, aut],\n  Andreas Papritz [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geoGAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoGAM Select Sparse Geoadditive Models for Spatial Prediction A model building procedure to build parsimonious geoadditive model from a large number of covariates. Continuous, binary and ordered categorical responses are supported. The model building is based on component wise gradient boosting with linear effects, smoothing splines and a smooth spatial surface to model spatial autocorrelation. The resulting covariate set after gradient boosting is further reduced through backward elimination and aggregation of factor levels. The package provides a model based bootstrap method to simulate prediction intervals for point predictions. A test data set of a soil mapping case study in Berne (Switzerland) is provided. Nussbaum, M., Walthert, L., Fraefel, M., Greiner, L., and Papritz, A. (2017) <doi:10.5194/soil-3-191-2017>.   "
  },
  {
    "id": 13047,
    "package_name": "geoR",
    "title": "Analysis of Geostatistical Data",
    "description": "Geostatistical analysis including variogram-based, likelihood-based and Bayesian methods. Software companion for Diggle and Ribeiro (2007) <doi:10.1007/978-0-387-48536-2>. ",
    "version": "1.9-6",
    "maintainer": "Paulo Justiniano Ribeiro Jr <paulojus@ufpr.br>",
    "author": "Paulo Justiniano Ribeiro Jr [aut, cre],\n  Peter Diggle [aut],\n  Ole Christensen [ctb],\n  Martin Schlather [ctb],\n  Roger Bivand [ctb],\n  Brian Ripley [ctb]",
    "url": "http://www.leg.ufpr.br/geoR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoR Analysis of Geostatistical Data Geostatistical analysis including variogram-based, likelihood-based and Bayesian methods. Software companion for Diggle and Ribeiro (2007) <doi:10.1007/978-0-387-48536-2>.   "
  },
  {
    "id": 13049,
    "package_name": "geoTS",
    "title": "Methods for Handling and Analyzing Time Series of Satellite\nImages",
    "description": "Provides functions and methods for: splitting large raster objects\n             into smaller chunks, transferring images from a binary format into raster \n             layers, transferring raster layers into an 'RData' file, calculating the \n             maximum gap (amount of consecutive missing values) of a numeric vector, \n             and fitting harmonic regression models to periodic time series. The homoscedastic\n             harmonic regression model is based on G. Roerink, M. Menenti and W. Verhoef (2000) <doi:10.1080/014311600209814>.",
    "version": "0.1.10",
    "maintainer": "Inder Tecuapetla-G\u00f3mez\n<itecuapetla@conabio.gob.mx>",
    "author": "Inder Tecuapetla-G\u00f3mez [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geoTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoTS Methods for Handling and Analyzing Time Series of Satellite\nImages Provides functions and methods for: splitting large raster objects\n             into smaller chunks, transferring images from a binary format into raster \n             layers, transferring raster layers into an 'RData' file, calculating the \n             maximum gap (amount of consecutive missing values) of a numeric vector, \n             and fitting harmonic regression models to periodic time series. The homoscedastic\n             harmonic regression model is based on G. Roerink, M. Menenti and W. Verhoef (2000) <doi:10.1080/014311600209814>.  "
  },
  {
    "id": 13056,
    "package_name": "geocomplexity",
    "title": "Mitigating Spatial Bias Through Geographical Complexity",
    "description": "The geographical complexity of individual variables can be characterized by the differences in local attribute variables, while the common geographical complexity of multiple variables can be represented by fluctuations in the similarity of vectors composed of multiple variables. In spatial regression tasks, the goodness of fit can be improved by incorporating a geographical complexity representation vector during modeling, using a geographical complexity-weighted spatial weight matrix, or employing local geographical complexity kernel density. Similarly, in spatial sampling tasks, samples can be selected more effectively by using a method that weights based on geographical complexity. By optimizing performance in spatial regression and spatial sampling tasks, the spatial bias of the model can be effectively reduced.",
    "version": "0.2.1",
    "maintainer": "Wenbo Lv <lyu.geosocial@gmail.com>",
    "author": "Wenbo Lv [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0002-6003-3800>),\n  Yongze Song [aut] (ORCID: <https://orcid.org/0000-0003-3420-9622>),\n  Zehua Zhang [aut] (ORCID: <https://orcid.org/0000-0003-3462-4025>)",
    "url": "https://ausgis.github.io/geocomplexity/,\nhttps://github.com/ausgis/geocomplexity",
    "bug_reports": "https://github.com/ausgis/geocomplexity/issues",
    "repository": "https://cran.r-project.org/package=geocomplexity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geocomplexity Mitigating Spatial Bias Through Geographical Complexity The geographical complexity of individual variables can be characterized by the differences in local attribute variables, while the common geographical complexity of multiple variables can be represented by fluctuations in the similarity of vectors composed of multiple variables. In spatial regression tasks, the goodness of fit can be improved by incorporating a geographical complexity representation vector during modeling, using a geographical complexity-weighted spatial weight matrix, or employing local geographical complexity kernel density. Similarly, in spatial sampling tasks, samples can be selected more effectively by using a method that weights based on geographical complexity. By optimizing performance in spatial regression and spatial sampling tasks, the spatial bias of the model can be effectively reduced.  "
  },
  {
    "id": 13057,
    "package_name": "geodaData",
    "title": "Spatial Analysis Datasets for Teaching",
    "description": "Stores small spatial datasets used to teach basic spatial analysis\n    concepts. Datasets are based off of the 'GeoDa' software workbook and data\n    site <https://geodacenter.github.io/data-and-lab/> developed by Luc Anselin\n    and team at the University of Chicago. Datasets are stored as 'sf' objects.",
    "version": "0.1.0",
    "maintainer": "Angela Li <ali6@uchicago.edu>",
    "author": "Angela Li [aut, cre] (ORCID: <https://orcid.org/0000-0002-8956-419X>),\n  Luc Anselin [ctb] (Creator of original spatial datasets)",
    "url": "https://github.com/spatialanalysis/geodaData",
    "bug_reports": "https://github.com/spatialanalysis/geodaData/issues",
    "repository": "https://cran.r-project.org/package=geodaData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geodaData Spatial Analysis Datasets for Teaching Stores small spatial datasets used to teach basic spatial analysis\n    concepts. Datasets are based off of the 'GeoDa' software workbook and data\n    site <https://geodacenter.github.io/data-and-lab/> developed by Luc Anselin\n    and team at the University of Chicago. Datasets are stored as 'sf' objects.  "
  },
  {
    "id": 13060,
    "package_name": "geodist",
    "title": "Fast, Dependency-Free Geodesic Distance Calculations",
    "description": "Dependency-free, ultra fast calculation of geodesic\n    distances.  Includes the reference nanometre-accuracy geodesic\n    distances of Karney (2013) <doi:10.1007/s00190-012-0578-z>, as used by\n    the 'sf' package, as well as Haversine and Vincenty distances. Default\n    distance measure is the \"Mapbox cheap ruler\" which is generally more\n    accurate than Haversine or Vincenty for distances out to a few hundred\n    kilometres, and is considerably faster. The main function accepts one\n    or two inputs in almost any generic rectangular form, and returns\n    either matrices of pairwise distances, or vectors of sequential\n    distances.",
    "version": "0.1.1",
    "maintainer": "Mark Padgham <mark.padgham@email.com>",
    "author": "Mark Padgham [aut, cre],\n  Michael D. Sumner [aut],\n  Charles F.F Karney [cph] (Original author of included code for geodesic\n    distances)",
    "url": "https://github.com/hypertidy/geodist,\nhttps://hypertidy.github.io/geodist/",
    "bug_reports": "https://github.com/hypertidy/geodist/issues",
    "repository": "https://cran.r-project.org/package=geodist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geodist Fast, Dependency-Free Geodesic Distance Calculations Dependency-free, ultra fast calculation of geodesic\n    distances.  Includes the reference nanometre-accuracy geodesic\n    distances of Karney (2013) <doi:10.1007/s00190-012-0578-z>, as used by\n    the 'sf' package, as well as Haversine and Vincenty distances. Default\n    distance measure is the \"Mapbox cheap ruler\" which is generally more\n    accurate than Haversine or Vincenty for distances out to a few hundred\n    kilometres, and is considerably faster. The main function accepts one\n    or two inputs in almost any generic rectangular form, and returns\n    either matrices of pairwise distances, or vectors of sequential\n    distances.  "
  },
  {
    "id": 13062,
    "package_name": "geodl",
    "title": "Geospatial Semantic Segmentation with Torch and Terra",
    "description": "Provides tools for semantic segmentation of geospatial data using convolutional neural \n  network-based deep learning. Utility functions allow for creating masks, image chips, data frames listing image \n  chips in a directory, and DataSets for use within DataLoaders. Additional functions are provided to serve as checks \n  during the data preparation and training process. A UNet architecture can be defined with 4 blocks in the encoder, a \n  bottleneck block, and 4 blocks in the decoder. The UNet can accept a variable number of input channels, and the user \n  can define the number of feature maps produced in each encoder and decoder block and the bottleneck. Users can also \n  choose to (1) replace all rectified linear unit (ReLU) activation functions with leaky ReLU or swish, (2) implement attention gates along the \n  skip connections, (3) implement squeeze and excitation modules within the encoder blocks, (4) add residual connections \n  within all blocks, (5) replace the bottleneck with a modified atrous spatial pyramid pooling (ASPP) module, and/or \n  (6) implement deep supervision using predictions generated at each stage in the decoder. A unified focal loss framework is implemented after\n  Yeung et al. (2022) <doi:10.1016/j.compmedimag.2021.102026>. We have also implemented \n  assessment metrics using the 'luz' package including F1-score, recall, and precision. Trained models can be used to predict to spatial \n  data without the need to generate chips from larger spatial extents. Functions are available for performing accuracy assessment. The package \n  relies on 'torch' for implementing deep learning, which does not require the installation of a 'Python' environment. Raster geospatial \n  data are handled with 'terra'. Models can be trained using a Compute Unified Device Architecture (CUDA)-enabled graphics processing unit (GPU); \n  however, multi-GPU training is not supported by 'torch' in 'R'. ",
    "version": "0.3.1",
    "maintainer": "Aaron Maxwell <Aaron.Maxwell@mail.wvu.edu>",
    "author": "Aaron Maxwell [aut, cre, cph],\n  Sarah Farhadpour [aut],\n  Srinjoy Das [aut],\n  Yalin Yang [aut]",
    "url": "https://github.com/maxwell-geospatial/geodl,\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0315127,\nhttps://wvview.org/gslr/index.html",
    "bug_reports": "https://github.com/maxwell-geospatial/geodl/issues",
    "repository": "https://cran.r-project.org/package=geodl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geodl Geospatial Semantic Segmentation with Torch and Terra Provides tools for semantic segmentation of geospatial data using convolutional neural \n  network-based deep learning. Utility functions allow for creating masks, image chips, data frames listing image \n  chips in a directory, and DataSets for use within DataLoaders. Additional functions are provided to serve as checks \n  during the data preparation and training process. A UNet architecture can be defined with 4 blocks in the encoder, a \n  bottleneck block, and 4 blocks in the decoder. The UNet can accept a variable number of input channels, and the user \n  can define the number of feature maps produced in each encoder and decoder block and the bottleneck. Users can also \n  choose to (1) replace all rectified linear unit (ReLU) activation functions with leaky ReLU or swish, (2) implement attention gates along the \n  skip connections, (3) implement squeeze and excitation modules within the encoder blocks, (4) add residual connections \n  within all blocks, (5) replace the bottleneck with a modified atrous spatial pyramid pooling (ASPP) module, and/or \n  (6) implement deep supervision using predictions generated at each stage in the decoder. A unified focal loss framework is implemented after\n  Yeung et al. (2022) <doi:10.1016/j.compmedimag.2021.102026>. We have also implemented \n  assessment metrics using the 'luz' package including F1-score, recall, and precision. Trained models can be used to predict to spatial \n  data without the need to generate chips from larger spatial extents. Functions are available for performing accuracy assessment. The package \n  relies on 'torch' for implementing deep learning, which does not require the installation of a 'Python' environment. Raster geospatial \n  data are handled with 'terra'. Models can be trained using a Compute Unified Device Architecture (CUDA)-enabled graphics processing unit (GPU); \n  however, multi-GPU training is not supported by 'torch' in 'R'.   "
  },
  {
    "id": 13076,
    "package_name": "geomerge",
    "title": "Geospatial Data Integration",
    "description": "Geospatial data integration framework that merges raster, spatial polygon, and (dynamic) spatial points data into a spatial (panel) data frame at any geographical resolution.",
    "version": "0.3.4",
    "maintainer": "Karsten Donnay <kdonnay@gmx.net>",
    "author": "Karsten Donnay and Andrew M. Linke",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geomerge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geomerge Geospatial Data Integration Geospatial data integration framework that merges raster, spatial polygon, and (dynamic) spatial points data into a spatial (panel) data frame at any geographical resolution.  "
  },
  {
    "id": 13081,
    "package_name": "geomod",
    "title": "A Computer Program for Geotechnical Investigations",
    "description": "The 'geomod' does spatial prediction of the Geotechnical soil properties. \n    It predicts the spatial distribution of Geotechnical properties of soil e.g. shear strength, \n    permeability, plasticity index, Standard Penetration Test (SPT) counts, etc. The output of the prediction takes the form of a \n    map or a series of maps. It uses the interpolation technique where a single or statistically \u201cbest\u201d \n    estimate of spatial occurrence soil property is determined. The interpolation is based on both the \n    sampled data and a variogram model for the spatial correlation of the sampled data. \n    The single estimate is produced by a Kriging technique.",
    "version": "0.1.0",
    "maintainer": "Festus Ngeno <festus.k.ngeno@gmail.com>",
    "author": "Festus Ngeno [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geomod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geomod A Computer Program for Geotechnical Investigations The 'geomod' does spatial prediction of the Geotechnical soil properties. \n    It predicts the spatial distribution of Geotechnical properties of soil e.g. shear strength, \n    permeability, plasticity index, Standard Penetration Test (SPT) counts, etc. The output of the prediction takes the form of a \n    map or a series of maps. It uses the interpolation technique where a single or statistically \u201cbest\u201d \n    estimate of spatial occurrence soil property is determined. The interpolation is based on both the \n    sampled data and a variogram model for the spatial correlation of the sampled data. \n    The single estimate is produced by a Kriging technique.  "
  },
  {
    "id": 13084,
    "package_name": "geomultistar",
    "title": "Multidimensional Queries Enriched with Geographic Data",
    "description": "Multidimensional systems allow complex queries to be carried\n    out in an easy way. The geographical dimension, together with the\n    temporal dimension, plays a fundamental role in multidimensional\n    systems. Through this package, vector geographic data layers can be\n    associated to the attributes of geographic dimensions, so that the\n    results of multidimensional queries can be obtained directly as vector\n    layers.  The multidimensional structures on which we can define the\n    queries can be created from a flat table or imported directly using\n    functions from this package.",
    "version": "1.2.2",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/geomultistar/,\nhttps://github.com/josesamos/geomultistar",
    "bug_reports": "https://github.com/josesamos/geomultistar/issues",
    "repository": "https://cran.r-project.org/package=geomultistar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geomultistar Multidimensional Queries Enriched with Geographic Data Multidimensional systems allow complex queries to be carried\n    out in an easy way. The geographical dimension, together with the\n    temporal dimension, plays a fundamental role in multidimensional\n    systems. Through this package, vector geographic data layers can be\n    associated to the attributes of geographic dimensions, so that the\n    results of multidimensional queries can be obtained directly as vector\n    layers.  The multidimensional structures on which we can define the\n    queries can be created from a flat table or imported directly using\n    functions from this package.  "
  },
  {
    "id": 13088,
    "package_name": "geoperu",
    "title": "Download Spatial Datasets of Peru",
    "description": "Provides convenient access to the official spatial datasets of Peru as 'sf' objects in R. This package includes a wide range of geospatial data covering various aspects of Peruvian geography, such as: administrative divisions (Source: INEI <https://ide.inei.gob.pe/>), protected natural areas  (Source: GEO ANP - SERNANP <https://geo.sernanp.gob.pe/visorsernanp/>). All datasets are harmonized in terms of attributes, projection, and topology, ensuring consistency and ease of use for spatial analysis and visualization.",
    "version": "0.0.0.2",
    "maintainer": "Paul E. Santos Andrade <paulefrens@gmail.com>",
    "author": "Paul E. Santos Andrade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6635-0375>)",
    "url": "https://github.com/PaulESantos/geoperu,\nhttps://paulesantos.github.io/geoperu/",
    "bug_reports": "https://github.com/PaulESantos/geoperu/issues",
    "repository": "https://cran.r-project.org/package=geoperu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoperu Download Spatial Datasets of Peru Provides convenient access to the official spatial datasets of Peru as 'sf' objects in R. This package includes a wide range of geospatial data covering various aspects of Peruvian geography, such as: administrative divisions (Source: INEI <https://ide.inei.gob.pe/>), protected natural areas  (Source: GEO ANP - SERNANP <https://geo.sernanp.gob.pe/visorsernanp/>). All datasets are harmonized in terms of attributes, projection, and topology, ensuring consistency and ease of use for spatial analysis and visualization.  "
  },
  {
    "id": 13091,
    "package_name": "georob",
    "title": "Robust Geostatistical Analysis of Spatial Data",
    "description": "Provides functions for efficiently fitting linear models with spatially correlated errors by robust (Kuensch et al. (2011) <doi:10.3929/ethz-a-009900710>) and Gaussian (Harville (1977) <doi:10.1080/01621459.1977.10480998>) (Restricted) Maximum Likelihood and for computing robust and customary point and block external-drift Kriging predictions (Cressie (1993) <doi:10.1002/9781119115151>), along with utility functions for variogram modelling in ad hoc geostatistical analyses, model building, model evaluation by cross-validation, (conditional) simulation of Gaussian processes (Davies and Bryant (2013) <doi:10.18637/jss.v055.i09>), unbiased back-transformation of Kriging predictions of log-transformed data (Cressie (2006) <doi:10.1007/s11004-005-9022-8>).",
    "version": "0.3-23",
    "maintainer": "Andreas Papritz <papritz@retired.ethz.ch>",
    "author": "Andreas Papritz [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=georob",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "georob Robust Geostatistical Analysis of Spatial Data Provides functions for efficiently fitting linear models with spatially correlated errors by robust (Kuensch et al. (2011) <doi:10.3929/ethz-a-009900710>) and Gaussian (Harville (1977) <doi:10.1080/01621459.1977.10480998>) (Restricted) Maximum Likelihood and for computing robust and customary point and block external-drift Kriging predictions (Cressie (1993) <doi:10.1002/9781119115151>), along with utility functions for variogram modelling in ad hoc geostatistical analyses, model building, model evaluation by cross-validation, (conditional) simulation of Gaussian processes (Davies and Bryant (2013) <doi:10.18637/jss.v055.i09>), unbiased back-transformation of Kriging predictions of log-transformed data (Cressie (2006) <doi:10.1007/s11004-005-9022-8>).  "
  },
  {
    "id": 13092,
    "package_name": "geos",
    "title": "Open Source Geometry Engine ('GEOS') R API",
    "description": "Provides an R API to the Open Source Geometry Engine\n  ('GEOS') library (<https://libgeos.org/>) and a vector format \n  with which to efficiently store 'GEOS' geometries. High-performance functions \n  to extract information from, calculate relationships between, and\n  transform geometries are provided. Finally, facilities to import \n  and export geometry vectors to other spatial formats are provided.",
    "version": "0.2.4",
    "maintainer": "Dewey Dunnington <dewey@fishandwhistle.net>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Edzer Pebesma [aut] (ORCID: <https://orcid.org/0000-0001-8049-7069>)",
    "url": "https://paleolimbot.github.io/geos/,\nhttps://github.com/paleolimbot/geos/",
    "bug_reports": "https://github.com/paleolimbot/geos/issues",
    "repository": "https://cran.r-project.org/package=geos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geos Open Source Geometry Engine ('GEOS') R API Provides an R API to the Open Source Geometry Engine\n  ('GEOS') library (<https://libgeos.org/>) and a vector format \n  with which to efficiently store 'GEOS' geometries. High-performance functions \n  to extract information from, calculate relationships between, and\n  transform geometries are provided. Finally, facilities to import \n  and export geometry vectors to other spatial formats are provided.  "
  },
  {
    "id": 13093,
    "package_name": "geosapi",
    "title": "GeoServer REST API R Interface",
    "description": "Provides an R interface to the GeoServer REST API, allowing to upload \n and publish data in a GeoServer web-application and expose data to OGC Web-Services. \n The package currently supports all CRUD (Create,Read,Update,Delete) operations\n on GeoServer workspaces, namespaces, datastores (stores of vector data), featuretypes,\n layers, styles, as well as vector data upload operations. For more information about \n the GeoServer REST API, see <https://docs.geoserver.org/stable/en/user/rest/>.",
    "version": "0.7-2",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "author": "Emmanuel Blondel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5870-5762>)",
    "url": "https://github.com/eblondel/geosapi,\nhttps://eblondel.github.io/geosapi/, https://geoserver.org/",
    "bug_reports": "https://github.com/eblondel/geosapi/issues",
    "repository": "https://cran.r-project.org/package=geosapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geosapi GeoServer REST API R Interface Provides an R interface to the GeoServer REST API, allowing to upload \n and publish data in a GeoServer web-application and expose data to OGC Web-Services. \n The package currently supports all CRUD (Create,Read,Update,Delete) operations\n on GeoServer workspaces, namespaces, datastores (stores of vector data), featuretypes,\n layers, styles, as well as vector data upload operations. For more information about \n the GeoServer REST API, see <https://docs.geoserver.org/stable/en/user/rest/>.  "
  },
  {
    "id": 13096,
    "package_name": "geospark",
    "title": "Bring Local Sf to Spark",
    "description": "R binds 'GeoSpark' <http://geospark.datasyslab.org/> extending 'sparklyr' \n    <https://spark.rstudio.com/> R package to make distributed 'geocomputing' easier. Sf is a\n    package that provides [simple features] <https://en.wikipedia.org/wiki/Simple_Features> access\n    for R and which is a leading 'geospatial' data processing tool. 'Geospark' R package bring \n    the same simple features access like sf but running on Spark distributed system.",
    "version": "0.3.1",
    "maintainer": "Harry Zhu <7harryprince@gmail.com>",
    "author": "Harry Zhu [aut, cre],\n  Javier Luraschi [ctb]",
    "url": "",
    "bug_reports": "https://github.com/harryprince/geospark/issues",
    "repository": "https://cran.r-project.org/package=geospark",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geospark Bring Local Sf to Spark R binds 'GeoSpark' <http://geospark.datasyslab.org/> extending 'sparklyr' \n    <https://spark.rstudio.com/> R package to make distributed 'geocomputing' easier. Sf is a\n    package that provides [simple features] <https://en.wikipedia.org/wiki/Simple_Features> access\n    for R and which is a leading 'geospatial' data processing tool. 'Geospark' R package bring \n    the same simple features access like sf but running on Spark distributed system.  "
  },
  {
    "id": 13097,
    "package_name": "geospatialsuite",
    "title": "Comprehensive Geospatiotemporal Analysis and Multimodal\nIntegration Toolkit",
    "description": "A comprehensive toolkit for geospatiotemporal analysis\n    featuring 60+ vegetation indices, advanced raster visualization,\n    universal spatial mapping, water quality analysis, CDL crop analysis,\n    spatial interpolation, temporal analysis, and terrain analysis.\n    Designed for agricultural research, environmental monitoring, remote\n    sensing applications, and publication-quality mapping with support for\n    any geographic region and robust error handling. Methods include\n    vegetation indices calculations (Rouse et al. 1974), NDVI and enhanced\n    vegetation indices (Huete et al. 1997)\n    <doi:10.1016/S0034-4257(97)00104-1>, (Akanbi et al. 2024) \n    <doi:10.1007/s41651-023-00164-y>, spatial interpolation techniques\n    (Cressie 1993, ISBN:9780471002556), water quality indices (McFeeters\n    1996) <doi:10.1080/01431169608948714>, and crop data layer analysis\n    (USDA NASS 2024)\n    <https://www.nass.usda.gov/Research_and_Science/Cropland/>.  Funding:\n    This material is based upon financial support by the National Science\n    Foundation, EEC Division of Engineering Education and Centers, NSF\n    Engineering Research Center for Advancing Sustainable and Distributed\n    Fertilizer production (CASFER), NSF 20-553 Gen-4 Engineering Research\n    Centers award 2133576.",
    "version": "0.1.1",
    "maintainer": "Olatunde D. Akanbi <olatunde.akanbi@case.edu>",
    "author": "Olatunde D. Akanbi [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7719-2619>),\n  Vibha Mandayam [aut] (ORCID: <https://orcid.org/0009-0008-8628-9904>),\n  Yinghui Wu [aut] (ORCID: <https://orcid.org/0000-0003-3991-5155>),\n  Jeffrey Yarus [aut] (ORCID: <https://orcid.org/0000-0002-9331-9568>),\n  Erika I. Barcelos [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-9273-8488>),\n  Roger H. French [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6162-0532>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geospatialsuite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geospatialsuite Comprehensive Geospatiotemporal Analysis and Multimodal\nIntegration Toolkit A comprehensive toolkit for geospatiotemporal analysis\n    featuring 60+ vegetation indices, advanced raster visualization,\n    universal spatial mapping, water quality analysis, CDL crop analysis,\n    spatial interpolation, temporal analysis, and terrain analysis.\n    Designed for agricultural research, environmental monitoring, remote\n    sensing applications, and publication-quality mapping with support for\n    any geographic region and robust error handling. Methods include\n    vegetation indices calculations (Rouse et al. 1974), NDVI and enhanced\n    vegetation indices (Huete et al. 1997)\n    <doi:10.1016/S0034-4257(97)00104-1>, (Akanbi et al. 2024) \n    <doi:10.1007/s41651-023-00164-y>, spatial interpolation techniques\n    (Cressie 1993, ISBN:9780471002556), water quality indices (McFeeters\n    1996) <doi:10.1080/01431169608948714>, and crop data layer analysis\n    (USDA NASS 2024)\n    <https://www.nass.usda.gov/Research_and_Science/Cropland/>.  Funding:\n    This material is based upon financial support by the National Science\n    Foundation, EEC Division of Engineering Education and Centers, NSF\n    Engineering Research Center for Advancing Sustainable and Distributed\n    Fertilizer production (CASFER), NSF 20-553 Gen-4 Engineering Research\n    Centers award 2133576.  "
  },
  {
    "id": 13098,
    "package_name": "geospt",
    "title": "Geostatistical Analysis and Design of Optimal Spatial Sampling\nNetworks",
    "description": "Estimation of the variogram through trimmed mean, radial basis \n        functions (optimization, prediction and cross-validation), summary\n        statistics from cross-validation, pocket plot, and design of\n        optimal sampling networks through sequential and simultaneous\n        points methods.",
    "version": "1.0-6",
    "maintainer": "Ali Santacruz <amsantac@unal.edu.co>",
    "author": "Carlos Melo [aut] (ORCID: <https://orcid.org/0000-0002-5598-1913>),\n  Ali Santacruz [aut, cre],\n  Oscar Melo [aut] (ORCID: <https://orcid.org/0000-0002-0296-4511>)",
    "url": "https://github.com/amsantac/geospt",
    "bug_reports": "https://github.com/amsantac/geospt/issues",
    "repository": "https://cran.r-project.org/package=geospt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geospt Geostatistical Analysis and Design of Optimal Spatial Sampling\nNetworks Estimation of the variogram through trimmed mean, radial basis \n        functions (optimization, prediction and cross-validation), summary\n        statistics from cross-validation, pocket plot, and design of\n        optimal sampling networks through sequential and simultaneous\n        points methods.  "
  },
  {
    "id": 13100,
    "package_name": "geostan",
    "title": "Bayesian Spatial Analysis",
    "description": "For spatial data analysis; provides exploratory spatial analysis tools, spatial regression, spatial econometric, and disease mapping models, model diagnostics, and special methods for inference with small area survey data (e.g., the America Community Survey (ACS)) and censored population health monitoring data. Models are pre-specified using the Stan programming language, a platform for Bayesian inference using Markov chain Monte Carlo (MCMC). References: Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>; Donegan (2021) <doi:10.31219/osf.io/3ey65>; Donegan (2022) <doi:10.21105/joss.04716>; Donegan, Chun and Hughes (2020) <doi:10.1016/j.spasta.2020.100450>; Donegan, Chun and Griffith (2021) <doi:10.3390/ijerph18136856>; Morris et al. (2019) <doi:10.1016/j.sste.2019.100301>.",
    "version": "0.8.2",
    "maintainer": "Connor Donegan <connor.donegan@gmail.com>",
    "author": "Connor Donegan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9698-5443>),\n  Mitzi Morris [ctb],\n  Amy Tims [ctb]",
    "url": "https://connordonegan.github.io/geostan/",
    "bug_reports": "https://github.com/ConnorDonegan/geostan/issues",
    "repository": "https://cran.r-project.org/package=geostan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geostan Bayesian Spatial Analysis For spatial data analysis; provides exploratory spatial analysis tools, spatial regression, spatial econometric, and disease mapping models, model diagnostics, and special methods for inference with small area survey data (e.g., the America Community Survey (ACS)) and censored population health monitoring data. Models are pre-specified using the Stan programming language, a platform for Bayesian inference using Markov chain Monte Carlo (MCMC). References: Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>; Donegan (2021) <doi:10.31219/osf.io/3ey65>; Donegan (2022) <doi:10.21105/joss.04716>; Donegan, Chun and Hughes (2020) <doi:10.1016/j.spasta.2020.100450>; Donegan, Chun and Griffith (2021) <doi:10.3390/ijerph18136856>; Morris et al. (2019) <doi:10.1016/j.sste.2019.100301>.  "
  },
  {
    "id": 13101,
    "package_name": "geostats",
    "title": "An Introduction to Statistics for Geoscientists",
    "description": "A collection of datasets and simplified functions for an introductory (geo)statistics module at University College London. Provides functionality for compositional, directional and spatial data, including ternary diagrams, Wulff and Schmidt stereonets, and ordinary kriging interpolation. Implements logistic and (additive and centred) logratio transformations. Computes vector averages and concentration parameters for the von-Mises distribution. Includes a collection of natural and synthetic fractals, and a simulator for deterministic chaos using a magnetic pendulum example. The main purpose of these functions is pedagogical. Researchers can find more complete alternatives for these tools in other packages such as 'compositions', 'robCompositions', 'sp', 'gstat' and 'RFOC'. All the functions are written in plain R, with no compiled code and a minimal number of dependencies. Theoretical background and worked examples are available at <https://tinyurl.com/UCLgeostats/>.",
    "version": "1.6",
    "maintainer": "Pieter Vermeesch <p.vermeesch@ucl.ac.uk>",
    "author": "Pieter Vermeesch [aut, cre]",
    "url": "https://github.com/pvermees/geostats/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geostats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geostats An Introduction to Statistics for Geoscientists A collection of datasets and simplified functions for an introductory (geo)statistics module at University College London. Provides functionality for compositional, directional and spatial data, including ternary diagrams, Wulff and Schmidt stereonets, and ordinary kriging interpolation. Implements logistic and (additive and centred) logratio transformations. Computes vector averages and concentration parameters for the von-Mises distribution. Includes a collection of natural and synthetic fractals, and a simulator for deterministic chaos using a magnetic pendulum example. The main purpose of these functions is pedagogical. Researchers can find more complete alternatives for these tools in other packages such as 'compositions', 'robCompositions', 'sp', 'gstat' and 'RFOC'. All the functions are written in plain R, with no compiled code and a minimal number of dependencies. Theoretical background and worked examples are available at <https://tinyurl.com/UCLgeostats/>.  "
  },
  {
    "id": 13102,
    "package_name": "geostatsp",
    "title": "Geostatistical Modelling with Likelihood and Bayes",
    "description": "Geostatistical modelling facilities using 'SpatRaster' and 'SpatVector'\n    objects are provided. Non-Gaussian models are fit using 'INLA', and Gaussian\n    geostatistical models use Maximum Likelihood Estimation.  For details see Brown (2015) <doi:10.18637/jss.v063.i12>. The 'RandomFields' package is available at <https://www.wim.uni-mannheim.de/schlather/publications/software>.",
    "version": "2.0.8",
    "maintainer": "Patrick Brown <patrick.brown@utoronto.ca>",
    "author": "Patrick Brown [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geostatsp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geostatsp Geostatistical Modelling with Likelihood and Bayes Geostatistical modelling facilities using 'SpatRaster' and 'SpatVector'\n    objects are provided. Non-Gaussian models are fit using 'INLA', and Gaussian\n    geostatistical models use Maximum Likelihood Estimation.  For details see Brown (2015) <doi:10.18637/jss.v063.i12>. The 'RandomFields' package is available at <https://www.wim.uni-mannheim.de/schlather/publications/software>.  "
  },
  {
    "id": 13104,
    "package_name": "geotoolsR",
    "title": "Tools to Improve the Use of Geostatistic",
    "description": "The basic idea of this package is provides some tools to help the researcher to work with geostatistics. Initially, we present a collection of functions that allow the researchers to deal with spatial data using bootstrap procedure. There are five methods available and two ways to display them: bootstrap confidence interval - provides a two-sided bootstrap confidence interval; bootstrap plot - a graphic with the original variogram and each of the B bootstrap variograms.",
    "version": "1.2.1",
    "maintainer": "Diogo Francisco Rossoni <dfrossoni@uem.br>",
    "author": "Diogo Francisco Rossoni [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6337-6628>),\n  Vinicius Basseto Felix [aut],\n  Ricardo Puziol de Oliveira [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6134-5975>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geotoolsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geotoolsR Tools to Improve the Use of Geostatistic The basic idea of this package is provides some tools to help the researcher to work with geostatistics. Initially, we present a collection of functions that allow the researchers to deal with spatial data using bootstrap procedure. There are five methods available and two ways to display them: bootstrap confidence interval - provides a two-sided bootstrap confidence interval; bootstrap plot - a graphic with the original variogram and each of the B bootstrap variograms.  "
  },
  {
    "id": 13105,
    "package_name": "geotopbricks",
    "title": "An R Plug-in for the Distributed Hydrological Model GEOtop",
    "description": "It analyzes raster maps and other information as input/output\n    files from the Hydrological Distributed Model GEOtop. It contains functions\n    and methods to import maps and other keywords from geotop.inpts file. Some\n    examples with simulation cases of GEOtop 2.x/3.x are presented in the package.\n    Any information about the GEOtop Distributed Hydrological Model can be found in the provided documentation.",
    "version": "1.5.9.1",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-3508-5898>)",
    "url": "https://github.com/ecor/geotopbricks",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geotopbricks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geotopbricks An R Plug-in for the Distributed Hydrological Model GEOtop It analyzes raster maps and other information as input/output\n    files from the Hydrological Distributed Model GEOtop. It contains functions\n    and methods to import maps and other keywords from geotop.inpts file. Some\n    examples with simulation cases of GEOtop 2.x/3.x are presented in the package.\n    Any information about the GEOtop Distributed Hydrological Model can be found in the provided documentation.  "
  },
  {
    "id": 13151,
    "package_name": "ggOceanMaps",
    "title": "Plot Data on Oceanographic Maps using 'ggplot2'",
    "description": "Allows plotting data on bathymetric maps using 'ggplot2'. Plotting\n  oceanographic spatial data is made as simple as feasible, but also flexible\n  for custom modifications. Data that contain geographic information from \n  anywhere around the globe can be plotted on maps generated by the basemap()\n  or qmap() functions using 'ggplot2' layers separated by the '+' operator. The \n  package uses spatial shape- ('sf') and raster ('stars') files, geospatial \n  packages for R to manipulate, and the 'ggplot2' package to plot these \n  files. The package ships with low-resolution spatial data files and \n  higher resolution files for detailed maps are stored in the \n  'ggOceanMapsLargeData' repository on GitHub and downloaded automatically \n  when needed. ",
    "version": "2.2.0",
    "maintainer": "Mikko Vihtakari <mikko.vihtakari@hi.no>",
    "author": "Mikko Vihtakari [aut, cre] (affiliation: Institute of Marine Research,\n    ORCID: <https://orcid.org/0000-0003-0371-4319>),\n  Roger Bivand [ctb],\n  Hadley Wickham [ctb]",
    "url": "https://mikkovihtakari.github.io/ggOceanMaps/",
    "bug_reports": "https://github.com/MikkoVihtakari/ggOceanMaps/issues",
    "repository": "https://cran.r-project.org/package=ggOceanMaps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggOceanMaps Plot Data on Oceanographic Maps using 'ggplot2' Allows plotting data on bathymetric maps using 'ggplot2'. Plotting\n  oceanographic spatial data is made as simple as feasible, but also flexible\n  for custom modifications. Data that contain geographic information from \n  anywhere around the globe can be plotted on maps generated by the basemap()\n  or qmap() functions using 'ggplot2' layers separated by the '+' operator. The \n  package uses spatial shape- ('sf') and raster ('stars') files, geospatial \n  packages for R to manipulate, and the 'ggplot2' package to plot these \n  files. The package ships with low-resolution spatial data files and \n  higher resolution files for detailed maps are stored in the \n  'ggOceanMapsLargeData' repository on GitHub and downloaded automatically \n  when needed.   "
  },
  {
    "id": 13153,
    "package_name": "ggQQunif",
    "title": "Compare Big Datasets to the Uniform Distribution",
    "description": "A quantile-quantile plot can be used to compare a sample of p-values \n  to the uniform distribution.  But when the dataset is big (i.e. > 1e4 p-values),\n  plotting the quantile-quantile plot can be slow.  geom_QQ uses all the data to\n  calculate the quantiles, but thins it out in a way that focuses on points near zero \n  before plotting to speed up plotting and decrease file size, when vector graphics are stored.  ",
    "version": "0.1.5",
    "maintainer": "Robert Corty <rcorty@gmail.com>",
    "author": "Robert Corty [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggQQunif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggQQunif Compare Big Datasets to the Uniform Distribution A quantile-quantile plot can be used to compare a sample of p-values \n  to the uniform distribution.  But when the dataset is big (i.e. > 1e4 p-values),\n  plotting the quantile-quantile plot can be slow.  geom_QQ uses all the data to\n  calculate the quantiles, but thins it out in a way that focuses on points near zero \n  before plotting to speed up plotting and decrease file size, when vector graphics are stored.    "
  },
  {
    "id": 13171,
    "package_name": "ggbiplot",
    "title": "A Grammar of Graphics Implementation of Biplots",
    "description": "A 'ggplot2' based implementation of biplots, giving a representation of a dataset in\n    a two dimensional space accounting for the greatest variance, together with variable vectors\n    showing how the data variables relate to this space. It provides a \n    replacement for stats::biplot(), but with many enhancements to control the analysis and\n    graphical display. It implements \n    biplot and scree plot methods which can be used with the results of prcomp(), princomp(),\n    FactoMineR::PCA(), ade4::dudi.pca() or MASS::lda() and can be customized using 'ggplot2' techniques.",
    "version": "0.6.2",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Vincent Q. Vu [aut] (ORCID: <https://orcid.org/0000-0002-4689-0497>),\n  Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>),\n  Aghasi Tavadyan [ctb]",
    "url": "https://github.com/friendly/ggbiplot,\nhttps://friendly.github.io/ggbiplot/",
    "bug_reports": "https://github.com/friendly/ggbiplot/issues",
    "repository": "https://cran.r-project.org/package=ggbiplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggbiplot A Grammar of Graphics Implementation of Biplots A 'ggplot2' based implementation of biplots, giving a representation of a dataset in\n    a two dimensional space accounting for the greatest variance, together with variable vectors\n    showing how the data variables relate to this space. It provides a \n    replacement for stats::biplot(), but with many enhancements to control the analysis and\n    graphical display. It implements \n    biplot and scree plot methods which can be used with the results of prcomp(), princomp(),\n    FactoMineR::PCA(), ade4::dudi.pca() or MASS::lda() and can be customized using 'ggplot2' techniques.  "
  },
  {
    "id": 13193,
    "package_name": "ggdibbler",
    "title": "Add Uncertainty to Data Visualisations",
    "description": "A 'ggplot2' extension for visualising uncertainty with the goal\n    of signal suppression. Usually, uncertainty visualisation focuses on\n    expressing uncertainty as a distribution or probability, whereas\n    'ggdibbler' differentiates itself by viewing an uncertainty\n    visualisation as an adjustment to an existing graphic that\n    incorporates the inherent uncertainty in the estimates.  You provide\n    the code for an existing plot, but replace any of the variables with a\n    vector of distributions, and it will convert the visualisation into\n    it's signal suppression counterpart.",
    "version": "0.6.1",
    "maintainer": "Harriet Mason <harriet.m.mason@gmail.com>",
    "author": "Harriet Mason [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-4568-8215>),\n  Dianne Cook [aut, ths] (ORCID: <https://orcid.org/0000-0002-3813-7155>),\n  Sarah Goodwin [aut, ths] (ORCID:\n    <https://orcid.org/0000-0001-8894-8282>),\n  Susan VanderPlas [aut, ths] (ORCID:\n    <https://orcid.org/0000-0002-3803-0972>)",
    "url": "https://harriet-mason.github.io/ggdibbler/,\nhttps://github.com/harriet-mason/ggdibbler",
    "bug_reports": "https://github.com/harriet-mason/ggdibbler/issues",
    "repository": "https://cran.r-project.org/package=ggdibbler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggdibbler Add Uncertainty to Data Visualisations A 'ggplot2' extension for visualising uncertainty with the goal\n    of signal suppression. Usually, uncertainty visualisation focuses on\n    expressing uncertainty as a distribution or probability, whereas\n    'ggdibbler' differentiates itself by viewing an uncertainty\n    visualisation as an adjustment to an existing graphic that\n    incorporates the inherent uncertainty in the estimates.  You provide\n    the code for an existing plot, but replace any of the variables with a\n    vector of distributions, and it will convert the visualisation into\n    it's signal suppression counterpart.  "
  },
  {
    "id": 13206,
    "package_name": "ggfields",
    "title": "Add Vector Field Layers to Ggplots",
    "description": "Add vector field layers to ggplots. Ideal for visualising\n    wind speeds, water currents, electric/magnetic fields, etc.\n    Accepts data.frames, simple features (sf), and spatiotemporal arrays (stars)\n    objects as input. Vector fields are depicted as arrows starting at specified\n    locations, and with specified angles and radii.",
    "version": "0.0.7",
    "maintainer": "Pepijn de Vries <pepijn.devries@outlook.com>",
    "author": "Pepijn de Vries [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7961-6646>)",
    "url": "https://pepijn-devries.github.io/ggfields/,\nhttps://github.com/pepijn-devries/ggfields/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggfields",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggfields Add Vector Field Layers to Ggplots Add vector field layers to ggplots. Ideal for visualising\n    wind speeds, water currents, electric/magnetic fields, etc.\n    Accepts data.frames, simple features (sf), and spatiotemporal arrays (stars)\n    objects as input. Vector fields are depicted as arrows starting at specified\n    locations, and with specified angles and radii.  "
  },
  {
    "id": 13219,
    "package_name": "gggda",
    "title": "A 'ggplot2' Extension for Geometric Data Analysis",
    "description": "A variety of multivariable data summary statistics and \n    constructions have been proposed, either to generalize univariable analogs \n    or to exploit multivariable properties.\n    Notable among these are the bivariate peelings surveyed by Green \n    (1981, ISBN:978-0-471-28039-2),\n    the bag-and-bolster plots proposed by Rousseeuw &al (1999) \n    <doi:10.1080/00031305.1999.10474494>, and the minimum spanning trees used by\n    Jolliffe (2002) <doi:10.1007/b98835> to represent high-dimensional \n    relationships among data in a low-dimensional plot.\n    Additionally, biplots of singular value--decomposed tabular data, such as\n    from principal components analysis, make use of vectors, calibrated axes,\n    and other representations of variable elements to complement point markers\n    for case elements; see Gabriel (1971) <doi:10.1093/biomet/58.3.453> and\n    Gower & Harding (1988) <doi:10.1093/biomet/75.3.445> for original proposals.\n    Because they treat the abscissa and ordinate as commensurate or the data\n    elements themselves as point masses or unit vectors, these multivariable\n    tools can be thought of as belonging to geometric data analysis; see Podani\n    (2000, ISBN:90-5782-067-6) for techniques and applications and Le Roux &\n    Rouanet (2005) <doi:10.1007/1-4020-2236-0> for foundations. 'gggda' extends\n    Wickham's (2010) <doi:10.1198/jcgs.2009.07098> layered grammar of graphics\n    with statistical transformation (\"stat\") and geometric construction (\"geom\")\n    layers for many of these tools, as well as convenience coordinate systems\n    to emphasize intrinsic geometry of the data.",
    "version": "0.1.1",
    "maintainer": "Jason Cory Brunson <cornelioid@gmail.com>",
    "author": "Jason Cory Brunson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3126-9494>),\n  Emily Paul [ctb],\n  John Gracey [aut]",
    "url": "https://github.com/corybrunson/gggda,\nhttps://corybrunson.github.io/gggda/",
    "bug_reports": "https://github.com/corybrunson/gggda/issues",
    "repository": "https://cran.r-project.org/package=gggda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gggda A 'ggplot2' Extension for Geometric Data Analysis A variety of multivariable data summary statistics and \n    constructions have been proposed, either to generalize univariable analogs \n    or to exploit multivariable properties.\n    Notable among these are the bivariate peelings surveyed by Green \n    (1981, ISBN:978-0-471-28039-2),\n    the bag-and-bolster plots proposed by Rousseeuw &al (1999) \n    <doi:10.1080/00031305.1999.10474494>, and the minimum spanning trees used by\n    Jolliffe (2002) <doi:10.1007/b98835> to represent high-dimensional \n    relationships among data in a low-dimensional plot.\n    Additionally, biplots of singular value--decomposed tabular data, such as\n    from principal components analysis, make use of vectors, calibrated axes,\n    and other representations of variable elements to complement point markers\n    for case elements; see Gabriel (1971) <doi:10.1093/biomet/58.3.453> and\n    Gower & Harding (1988) <doi:10.1093/biomet/75.3.445> for original proposals.\n    Because they treat the abscissa and ordinate as commensurate or the data\n    elements themselves as point masses or unit vectors, these multivariable\n    tools can be thought of as belonging to geometric data analysis; see Podani\n    (2000, ISBN:90-5782-067-6) for techniques and applications and Le Roux &\n    Rouanet (2005) <doi:10.1007/1-4020-2236-0> for foundations. 'gggda' extends\n    Wickham's (2010) <doi:10.1198/jcgs.2009.07098> layered grammar of graphics\n    with statistical transformation (\"stat\") and geometric construction (\"geom\")\n    layers for many of these tools, as well as convenience coordinate systems\n    to emphasize intrinsic geometry of the data.  "
  },
  {
    "id": 13230,
    "package_name": "gghist",
    "title": "Plot the Histogram of a Numeric Vector",
    "description": "Wrapper around geom_histogram() of 'ggplot2' to plot the histogram of a numeric vector. This is especially useful, since qplot() was deprecated in 'ggplot2' 3.4.0.",
    "version": "0.1.0",
    "maintainer": "Frederik Ziebell <frederik.ziebell@gmail.com>",
    "author": "Frederik Ziebell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3673-1721>)",
    "url": "https://github.com/frederikziebell/gghist",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gghist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gghist Plot the Histogram of a Numeric Vector Wrapper around geom_histogram() of 'ggplot2' to plot the histogram of a numeric vector. This is especially useful, since qplot() was deprecated in 'ggplot2' 3.4.0.  "
  },
  {
    "id": 13233,
    "package_name": "ggimg",
    "title": "Graphics Layers for Plotting Image Data with 'ggplot2'",
    "description": "Provides two new layer types for displaying image data as layers\n  within the Grammar of Graphics framework. Displays images using either a\n  rectangle interface, with a fixed bounding box, or a point interface using a\n  central point and general size parameter. Images can be given as local\n  JPEG or PNG files, external resources, or as a list column containing\n  raster image data.",
    "version": "0.1.2",
    "maintainer": "Taylor B. Arnold <tarnold2@richmond.edu>",
    "author": "Taylor B. Arnold [aut, cre]",
    "url": "https://github.com/statsmaths/ggimg",
    "bug_reports": "https://github.com/statsmaths/ggimg/issues",
    "repository": "https://cran.r-project.org/package=ggimg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggimg Graphics Layers for Plotting Image Data with 'ggplot2' Provides two new layer types for displaying image data as layers\n  within the Grammar of Graphics framework. Displays images using either a\n  rectangle interface, with a fixed bounding box, or a point interface using a\n  central point and general size parameter. Images can be given as local\n  JPEG or PNG files, external resources, or as a list column containing\n  raster image data.  "
  },
  {
    "id": 13294,
    "package_name": "ggquiver",
    "title": "Quiver Plots for 'ggplot2'",
    "description": "An extension of 'ggplot2' to provide quiver plots to visualise vector fields. \n    This functionality is implemented using a geom to produce a new graphical layer, which\n    allows aesthetic options. This layer can be overlaid on a map to improve visualisation\n    of mapped data.",
    "version": "0.3.3",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "author": "Mitchell O'Hara-Wild [aut, cre]",
    "url": "https://github.com/mitchelloharawild/ggquiver,\nhttps://pkg.mitchelloharawild.com/ggquiver/",
    "bug_reports": "https://github.com/mitchelloharawild/ggquiver/issues",
    "repository": "https://cran.r-project.org/package=ggquiver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggquiver Quiver Plots for 'ggplot2' An extension of 'ggplot2' to provide quiver plots to visualise vector fields. \n    This functionality is implemented using a geom to produce a new graphical layer, which\n    allows aesthetic options. This layer can be overlaid on a map to improve visualisation\n    of mapped data.  "
  },
  {
    "id": 13300,
    "package_name": "ggrastr",
    "title": "Rasterize Layers for 'ggplot2'",
    "description": "Rasterize only specific layers of a 'ggplot2' plot while simultaneously keeping all labels and text in vector format. This allows users to keep plots within the reasonable size limit without loosing vector properties of the scale-sensitive information. ",
    "version": "1.0.2",
    "maintainer": "Evan Biederstedt <evan.biederstedt@gmail.com>",
    "author": "Viktor Petukhov [aut, cph],\n  Teun van den Brand [aut],\n  Evan Biederstedt [cre, aut]",
    "url": "https://github.com/VPetukhov/ggrastr",
    "bug_reports": "https://github.com/VPetukhov/ggrastr/issues",
    "repository": "https://cran.r-project.org/package=ggrastr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggrastr Rasterize Layers for 'ggplot2' Rasterize only specific layers of a 'ggplot2' plot while simultaneously keeping all labels and text in vector format. This allows users to keep plots within the reasonable size limit without loosing vector properties of the scale-sensitive information.   "
  },
  {
    "id": 13321,
    "package_name": "ggsolvencyii",
    "title": "A 'ggplot2'-Plot of Composition of Solvency II SCR: SF and IM",
    "description": "An implementation of 'ggplot2'-methods to present the composition of Solvency II Solvency Capital Requirement (SCR) as a series of concentric circle-parts. \n Solvency II (Solvency 2) is European insurance legislation, coming in force by the delegated acts of October 10, 2014.\n <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ%3AL%3A2015%3A012%3ATOC>. \n Additional files, defining the structure of the Standard Formula (SF) method of the SCR-calculation are provided.\n The structure files can be adopted for localization or for insurance companies who use Internal Models (IM).\n Options are available for combining smaller components, horizontal and vertical scaling, rotation, and plotting only some circle-parts.\n With outlines and connectors several SCR-compositions can be compared, for example in ORSA-scenarios (Own Risk and Solvency Assessment).",
    "version": "0.1.2",
    "maintainer": "Marco van Zanden <git@vanzanden.nl>",
    "author": "Marco van Zanden [aut, cre]",
    "url": "https://github.com/vanzanden/ggsolvencyii",
    "bug_reports": "https://github.com/vanzanden/ggsolvencyii/issues",
    "repository": "https://cran.r-project.org/package=ggsolvencyii",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggsolvencyii A 'ggplot2'-Plot of Composition of Solvency II SCR: SF and IM An implementation of 'ggplot2'-methods to present the composition of Solvency II Solvency Capital Requirement (SCR) as a series of concentric circle-parts. \n Solvency II (Solvency 2) is European insurance legislation, coming in force by the delegated acts of October 10, 2014.\n <https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ%3AL%3A2015%3A012%3ATOC>. \n Additional files, defining the structure of the Standard Formula (SF) method of the SCR-calculation are provided.\n The structure files can be adopted for localization or for insurance companies who use Internal Models (IM).\n Options are available for combining smaller components, horizontal and vertical scaling, rotation, and plotting only some circle-parts.\n With outlines and connectors several SCR-compositions can be compared, for example in ORSA-scenarios (Own Risk and Solvency Assessment).  "
  },
  {
    "id": 13354,
    "package_name": "ggvfields",
    "title": "Vector Field Visualizations with 'ggplot2'",
    "description": "A 'ggplot2' extension for visualizing vector fields in two-dimensional \n           space. Provides flexible tools for creating vector and stream field layers, \n           visualizing gradients and potential fields, and smoothing vector and \n           scalar data to estimate underlying patterns. ",
    "version": "1.0.0",
    "maintainer": "Dusty Turner <dusty.s.turner@gmail.com>",
    "author": "Dusty Turner [aut, cre],\n  David Kahle [aut],\n  Rodney X. Sturdivant [aut]",
    "url": "https://github.com/dusty-turner/ggvfields",
    "bug_reports": "https://github.com/dusty-turner/ggvfields/issues",
    "repository": "https://cran.r-project.org/package=ggvfields",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggvfields Vector Field Visualizations with 'ggplot2' A 'ggplot2' extension for visualizing vector fields in two-dimensional \n           space. Provides flexible tools for creating vector and stream field layers, \n           visualizing gradients and potential fields, and smoothing vector and \n           scalar data to estimate underlying patterns.   "
  },
  {
    "id": 13377,
    "package_name": "gimms",
    "title": "Download and Process GIMMS NDVI3g Data",
    "description": "This is a set of functions to retrieve information about GIMMS\n    NDVI3g files currently available online; download (and re-arrange, in the \n    case of NDVI3g.v0) the half-monthly data sets; import downloaded files from \n    ENVI binary (NDVI3g.v0) or NetCDF format (NDVI3g.v1) directly into R based \n    on the widespread 'raster' package; conduct quality control; and generate \n    monthly composites (e.g., maximum values) from the half-monthly input data. \n    As a special gimmick, a method is included to conveniently apply the \n    Mann-Kendall trend test upon 'Raster*' images, optionally featuring \n    trend-free pre-whitening to account for lag-1 autocorrelation.",
    "version": "1.2.4",
    "maintainer": "Florian Detsch <fdetsch@web.de>",
    "author": "Florian Detsch [cre, aut]",
    "url": "https://github.com/environmentalinformatics-marburg/gimms",
    "bug_reports": "https://github.com/environmentalinformatics-marburg/gimms/issues",
    "repository": "https://cran.r-project.org/package=gimms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gimms Download and Process GIMMS NDVI3g Data This is a set of functions to retrieve information about GIMMS\n    NDVI3g files currently available online; download (and re-arrange, in the \n    case of NDVI3g.v0) the half-monthly data sets; import downloaded files from \n    ENVI binary (NDVI3g.v0) or NetCDF format (NDVI3g.v1) directly into R based \n    on the widespread 'raster' package; conduct quality control; and generate \n    monthly composites (e.g., maximum values) from the half-monthly input data. \n    As a special gimmick, a method is included to conveniently apply the \n    Mann-Kendall trend test upon 'Raster*' images, optionally featuring \n    trend-free pre-whitening to account for lag-1 autocorrelation.  "
  },
  {
    "id": 13397,
    "package_name": "gkmSVM",
    "title": "Gapped-Kmer Support Vector Machine",
    "description": "Imports the 'gkmSVM' v2.0 functionalities into R <https://www.beerlab.org/gkmsvm/>\n    It also uses the 'kernlab' library (separate R package by different authors) for various SVM algorithms.\n    Users should note that the suggested packages 'rtracklayer', 'GenomicRanges', 'BSgenome', 'BiocGenerics', \n    'Biostrings', 'GenomeInfoDb', 'IRanges', and 'S4Vectors' are all BioConductor packages <https://bioconductor.org>.",
    "version": "0.83.0",
    "maintainer": "Mike Beer <mbeer@jhu.edu>",
    "author": "Mahmoud Ghandi",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gkmSVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gkmSVM Gapped-Kmer Support Vector Machine Imports the 'gkmSVM' v2.0 functionalities into R <https://www.beerlab.org/gkmsvm/>\n    It also uses the 'kernlab' library (separate R package by different authors) for various SVM algorithms.\n    Users should note that the suggested packages 'rtracklayer', 'GenomicRanges', 'BSgenome', 'BiocGenerics', \n    'Biostrings', 'GenomeInfoDb', 'IRanges', and 'S4Vectors' are all BioConductor packages <https://bioconductor.org>.  "
  },
  {
    "id": 13401,
    "package_name": "glarma",
    "title": "Generalized Linear Autoregressive Moving Average Models",
    "description": "Functions are provided for estimation, testing, diagnostic checking and forecasting of generalized linear autoregressive moving average (GLARMA) models for discrete valued time series with regression variables.  These are a class of observation driven non-linear non-Gaussian state space models. The state vector consists of a linear regression component plus an observation driven component consisting of an autoregressive-moving average (ARMA) filter of past predictive residuals. Currently three distributions (Poisson, negative binomial and binomial) can be used for the response series. Three options (Pearson, score-type and unscaled) for the residuals in the observation driven component are available. Estimation is via maximum likelihood (conditional on initializing values for the ARMA process) optimized using Fisher scoring or Newton Raphson iterative methods. Likelihood ratio and Wald tests for the observation driven component allow testing for serial dependence in generalized linear model settings. Graphical diagnostics including model fits, autocorrelation functions and probability integral transform residuals are included in the package. Several standard data sets are included in the package.",
    "version": "1.7-1",
    "maintainer": "William T.M. Dunsmuir <w.dunsmuir@unsw.edu.au>",
    "author": "William T.M. Dunsmuir [aut, cre],\n  Cenanning Li [aut],\n  David J. Scott [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glarma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glarma Generalized Linear Autoregressive Moving Average Models Functions are provided for estimation, testing, diagnostic checking and forecasting of generalized linear autoregressive moving average (GLARMA) models for discrete valued time series with regression variables.  These are a class of observation driven non-linear non-Gaussian state space models. The state vector consists of a linear regression component plus an observation driven component consisting of an autoregressive-moving average (ARMA) filter of past predictive residuals. Currently three distributions (Poisson, negative binomial and binomial) can be used for the response series. Three options (Pearson, score-type and unscaled) for the residuals in the observation driven component are available. Estimation is via maximum likelihood (conditional on initializing values for the ARMA process) optimized using Fisher scoring or Newton Raphson iterative methods. Likelihood ratio and Wald tests for the observation driven component allow testing for serial dependence in generalized linear model settings. Graphical diagnostics including model fits, autocorrelation functions and probability integral transform residuals are included in the package. Several standard data sets are included in the package.  "
  },
  {
    "id": 13470,
    "package_name": "gluedown",
    "title": "Wrap Vectors in Markdown Formatting",
    "description": "Ease the transition between R vectors and markdown text. With\n    'gluedown' and 'rmarkdown', users can create traditional vectors in R,\n    glue those strings together with the markdown syntax, and print those\n    formatted vectors directly to the document. This package primarily\n    uses GitHub Flavored Markdown (GFM), an offshoot of the unambiguous\n    CommonMark specification by John MacFarlane (2019)\n    <https://spec.commonmark.org/>.",
    "version": "1.0.9",
    "maintainer": "Kiernan Nicholls <k5cents@gmail.com>",
    "author": "Kiernan Nicholls [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9229-7897>)",
    "url": "https://k5cents.github.io/gluedown/,\nhttps://github.com/k5cents/gluedown/",
    "bug_reports": "https://github.com/k5cents/gluedown/issues",
    "repository": "https://cran.r-project.org/package=gluedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gluedown Wrap Vectors in Markdown Formatting Ease the transition between R vectors and markdown text. With\n    'gluedown' and 'rmarkdown', users can create traditional vectors in R,\n    glue those strings together with the markdown syntax, and print those\n    formatted vectors directly to the document. This package primarily\n    uses GitHub Flavored Markdown (GFM), an offshoot of the unambiguous\n    CommonMark specification by John MacFarlane (2019)\n    <https://spec.commonmark.org/>.  "
  },
  {
    "id": 13474,
    "package_name": "glyrepr",
    "title": "Representation for Glycan Compositions and Structures",
    "description": "Computational representations of glycan compositions and structures,\n    including details such as linkages, anomers, and substituents. Supports varying\n    levels of monosaccharide specificity (e.g., \"Hex\" or \"Gal\") and ambiguous linkages.\n    Provides robust parsing and generation of IUPAC-condensed structure strings.\n    Optimized for vectorized operations on glycan structures, with efficient handling\n    of duplications. As the cornerstone of the glycoverse ecosystem, this package\n    delivers the foundational data structures that power glycomics and glycoproteomics\n    analysis workflows.",
    "version": "0.9.0",
    "maintainer": "Bin Fu <23110220018@m.fudan.edu.cn>",
    "author": "Bin Fu [aut, cre, cph] (ORCID: <https://orcid.org/0000-0001-8567-2997>)",
    "url": "https://glycoverse.github.io/glyrepr/,\nhttps://github.com/glycoverse/glyrepr",
    "bug_reports": "https://github.com/glycoverse/glyrepr/issues",
    "repository": "https://cran.r-project.org/package=glyrepr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glyrepr Representation for Glycan Compositions and Structures Computational representations of glycan compositions and structures,\n    including details such as linkages, anomers, and substituents. Supports varying\n    levels of monosaccharide specificity (e.g., \"Hex\" or \"Gal\") and ambiguous linkages.\n    Provides robust parsing and generation of IUPAC-condensed structure strings.\n    Optimized for vectorized operations on glycan structures, with efficient handling\n    of duplications. As the cornerstone of the glycoverse ecosystem, this package\n    delivers the foundational data structures that power glycomics and glycoproteomics\n    analysis workflows.  "
  },
  {
    "id": 13477,
    "package_name": "gmGeostats",
    "title": "Geostatistics for Compositional Analysis",
    "description": "Support for geostatistical analysis of multivariate data, \n         in particular data with restrictions, e.g. positive amounts, \n         compositions, distributional data, microstructural data, etc. \n         It includes descriptive analysis and modelling for such data, both \n         from a two-point Gaussian perspective and multipoint perspective.\n         The methods mainly follow Tolosana-Delgado, Mueller and van den\n         Boogaart (2018) <doi:10.1007/s11004-018-9769-3>.",
    "version": "0.11.4",
    "maintainer": "K. Gerald van den Boogaart <support@boogaart.de>",
    "author": "Raimon Tolosana-Delgado [aut] (ORCID:\n    <https://orcid.org/0000-0001-9847-0462>),\n  Ute Mueller [aut],\n  K. Gerald van den Boogaart [ctb, cre],\n  Hassan Talebi [ctb, cph],\n  Helmholtz-Zentrum Dresden-Rossendorf [cph],\n  Edith Cowan University [cph]",
    "url": "https://codebase.helmholtz.cloud/geomet/gmGeostats",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gmGeostats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gmGeostats Geostatistics for Compositional Analysis Support for geostatistical analysis of multivariate data, \n         in particular data with restrictions, e.g. positive amounts, \n         compositions, distributional data, microstructural data, etc. \n         It includes descriptive analysis and modelling for such data, both \n         from a two-point Gaussian perspective and multipoint perspective.\n         The methods mainly follow Tolosana-Delgado, Mueller and van den\n         Boogaart (2018) <doi:10.1007/s11004-018-9769-3>.  "
  },
  {
    "id": 13493,
    "package_name": "gmvarkit",
    "title": "Estimate Gaussian and Student's t Mixture Vector Autoregressive\nModels",
    "description": "Unconstrained and constrained maximum likelihood estimation of structural and reduced form \n    Gaussian mixture vector autoregressive, Student's t mixture vector autoregressive, and Gaussian and Student's t\n    mixture vector autoregressive models, quantile residual tests, graphical diagnostics,\n    simulations, forecasting, and estimation of generalized impulse response function and generalized \n    forecast error variance decomposition.\n    Leena Kalliovirta, Mika Meitz, Pentti Saikkonen (2016) <doi:10.1016/j.jeconom.2016.02.012>,\n    Savi Virolainen (2025) <doi:10.1080/07350015.2024.2322090>,\n    Savi Virolainen (in press) <doi:10.1016/j.ecosta.2025.09.003>.",
    "version": "2.2.1",
    "maintainer": "Savi Virolainen <savi.virolainen@helsinki.fi>",
    "author": "Savi Virolainen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5075-6821>)",
    "url": "",
    "bug_reports": "https://github.com/saviviro/gmvarkit/issues",
    "repository": "https://cran.r-project.org/package=gmvarkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gmvarkit Estimate Gaussian and Student's t Mixture Vector Autoregressive\nModels Unconstrained and constrained maximum likelihood estimation of structural and reduced form \n    Gaussian mixture vector autoregressive, Student's t mixture vector autoregressive, and Gaussian and Student's t\n    mixture vector autoregressive models, quantile residual tests, graphical diagnostics,\n    simulations, forecasting, and estimation of generalized impulse response function and generalized \n    forecast error variance decomposition.\n    Leena Kalliovirta, Mika Meitz, Pentti Saikkonen (2016) <doi:10.1016/j.jeconom.2016.02.012>,\n    Savi Virolainen (2025) <doi:10.1080/07350015.2024.2322090>,\n    Savi Virolainen (in press) <doi:10.1016/j.ecosta.2025.09.003>.  "
  },
  {
    "id": 13564,
    "package_name": "gpkg",
    "title": "Utilities for the Open Geospatial Consortium 'GeoPackage' Format",
    "description": "Build Open Geospatial Consortium 'GeoPackage' files (<https://www.geopackage.org/>). 'GDAL' utilities for reading and writing spatial data are provided by the 'terra' package. Additional 'GeoPackage' and 'SQLite' features for attributes and tabular data are implemented with the 'RSQLite' package.",
    "version": "0.0.12",
    "maintainer": "Andrew Brown <brown.andrewg@gmail.com>",
    "author": "Andrew Brown [aut, cre]",
    "url": "https://humus.rocks/gpkg/, https://github.com/brownag/gpkg",
    "bug_reports": "https://github.com/brownag/gpkg/issues",
    "repository": "https://cran.r-project.org/package=gpkg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gpkg Utilities for the Open Geospatial Consortium 'GeoPackage' Format Build Open Geospatial Consortium 'GeoPackage' files (<https://www.geopackage.org/>). 'GDAL' utilities for reading and writing spatial data are provided by the 'terra' package. Additional 'GeoPackage' and 'SQLite' features for attributes and tabular data are implemented with the 'RSQLite' package.  "
  },
  {
    "id": 13593,
    "package_name": "granovaGG",
    "title": "Graphical Analysis of Variance Using ggplot2",
    "description": "Create what we call Elemental Graphics for display of\n    anova results. The term elemental derives from the fact\n    that each function is aimed at construction of\n    graphical displays that afford direct visualizations of\n    data with respect to the fundamental questions that\n    drive the particular anova methods. This package\n    represents a modification of the original granova\n    package; the key change is to use 'ggplot2', Hadley\n    Wickham's package based on Grammar of Graphics concepts\n    (due to Wilkinson). The main function is granovagg.1w()\n    (a graphic for one way ANOVA); two other functions\n    (granovagg.ds() and granovagg.contr()) are to construct\n    graphics for dependent sample analyses and\n    contrast-based analyses respectively. (The function\n    granova.2w(), which entails dynamic displays of data, is\n    not currently part of 'granovaGG'.) The 'granovaGG'\n    functions are to display data for any number of groups,\n    regardless of their sizes (however, very large data\n    sets or numbers of groups can be problematic). For\n    granovagg.1w() a specialized approach is used to\n    construct data-based contrast vectors for which anova\n    data are displayed. The result is that the graphics use\n    a straight line to facilitate clear interpretations\n    while being faithful to the standard effect test in\n    anova. The graphic results are complementary to\n    standard summary tables; indeed, numerical summary\n    statistics are provided as side effects of the graphic\n    constructions. granovagg.ds() and granovagg.contr() provide\n    graphic displays and numerical outputs for a dependent\n    sample and contrast-based analyses. The graphics based\n    on these functions can be especially helpful for\n    learning how the respective methods work to answer the\n    basic question(s) that drive the analyses. This means\n    they can be particularly helpful for students and\n    non-statistician analysts. But these methods can be of\n    assistance for work-a-day applications of many kinds,\n    as they can help to identify outliers, clusters or\n    patterns, as well as highlight the role of non-linear\n    transformations of data. In the case of granovagg.1w()\n    and granovagg.ds() several arguments are provided to\n    facilitate flexibility in the construction of graphics\n    that accommodate diverse features of data, according to\n    their corresponding display requirements. See the help\n    files for individual functions.",
    "version": "1.4.1",
    "maintainer": "Brian A. Danielak <briandanielak+granovagg@gmail.com>",
    "author": "Brian A. Danielak [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4279-6708>),\n  Robert M. Pruzek [aut],\n  William E. J. Doane [ctb] (ORCID:\n    <https://orcid.org/0000-0002-5047-8286>),\n  James E. Helmreich [ctb],\n  Jason Bryer [ctb]",
    "url": "https://github.com/briandk/granovaGG",
    "bug_reports": "https://github.com/briandk/granovaGG/issues",
    "repository": "https://cran.r-project.org/package=granovaGG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "granovaGG Graphical Analysis of Variance Using ggplot2 Create what we call Elemental Graphics for display of\n    anova results. The term elemental derives from the fact\n    that each function is aimed at construction of\n    graphical displays that afford direct visualizations of\n    data with respect to the fundamental questions that\n    drive the particular anova methods. This package\n    represents a modification of the original granova\n    package; the key change is to use 'ggplot2', Hadley\n    Wickham's package based on Grammar of Graphics concepts\n    (due to Wilkinson). The main function is granovagg.1w()\n    (a graphic for one way ANOVA); two other functions\n    (granovagg.ds() and granovagg.contr()) are to construct\n    graphics for dependent sample analyses and\n    contrast-based analyses respectively. (The function\n    granova.2w(), which entails dynamic displays of data, is\n    not currently part of 'granovaGG'.) The 'granovaGG'\n    functions are to display data for any number of groups,\n    regardless of their sizes (however, very large data\n    sets or numbers of groups can be problematic). For\n    granovagg.1w() a specialized approach is used to\n    construct data-based contrast vectors for which anova\n    data are displayed. The result is that the graphics use\n    a straight line to facilitate clear interpretations\n    while being faithful to the standard effect test in\n    anova. The graphic results are complementary to\n    standard summary tables; indeed, numerical summary\n    statistics are provided as side effects of the graphic\n    constructions. granovagg.ds() and granovagg.contr() provide\n    graphic displays and numerical outputs for a dependent\n    sample and contrast-based analyses. The graphics based\n    on these functions can be especially helpful for\n    learning how the respective methods work to answer the\n    basic question(s) that drive the analyses. This means\n    they can be particularly helpful for students and\n    non-statistician analysts. But these methods can be of\n    assistance for work-a-day applications of many kinds,\n    as they can help to identify outliers, clusters or\n    patterns, as well as highlight the role of non-linear\n    transformations of data. In the case of granovagg.1w()\n    and granovagg.ds() several arguments are provided to\n    facilitate flexibility in the construction of graphics\n    that accommodate diverse features of data, according to\n    their corresponding display requirements. See the help\n    files for individual functions.  "
  },
  {
    "id": 13605,
    "package_name": "graphicalVAR",
    "title": "Graphical VAR for Experience Sampling Data",
    "description": "Estimates within and between time point interactions in experience sampling data, using the Graphical vector autoregression model in combination with regularization. See also Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>.",
    "version": "0.3.4",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "author": "Sacha Epskamp [aut, cre],\n  Eren Asena [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=graphicalVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "graphicalVAR Graphical VAR for Experience Sampling Data Estimates within and between time point interactions in experience sampling data, using the Graphical vector autoregression model in combination with regularization. See also Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>.  "
  },
  {
    "id": 13610,
    "package_name": "graphpcor",
    "title": "Models for Correlation Matrices Based on Graphs",
    "description": "Implement some models for \n  correlation/covariance matrices including two approaches \n  to model correlation matrices from a graphical structure.\n  One use latent parent variables as proposed in\n  Sterrantino et. al. (2024) <doi:10.48550/arXiv.2312.06289>.\n  The other uses a graph to specify conditional \n  relations between the variables.\n  The graphical structure makes correlation matrices \n  interpretable and avoids the quadratic increase of \n  parameters as a function of the dimension. \n  In the first approach a natural sequence of simpler \n  models along with a complexity penalization is used.\n  The second penalizes deviations from a base model.\n  These can be used as prior for model parameters,\n  considering C code through the 'cgeneric' interface \n  for the 'INLA' package (<https://www.r-inla.org>). \n  This allows one to use these models as building \n  blocks combined and to other latent Gaussian models \n  in order to build complex data models.",
    "version": "0.1.12",
    "maintainer": "Elias Krainski <eliaskrainski@gmail.com>",
    "author": "Elias Krainski [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-7063-2615>),\n  Denis Rustand [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-9708-5220>),\n  Anna Freni-Sterrantino [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6602-6209>),\n  Janet van Niekerk [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-4334-2057>),\n  Haavard Rue\u2019 [aut] (ORCID: <https://orcid.org/0000-0002-0222-1881>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=graphpcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "graphpcor Models for Correlation Matrices Based on Graphs Implement some models for \n  correlation/covariance matrices including two approaches \n  to model correlation matrices from a graphical structure.\n  One use latent parent variables as proposed in\n  Sterrantino et. al. (2024) <doi:10.48550/arXiv.2312.06289>.\n  The other uses a graph to specify conditional \n  relations between the variables.\n  The graphical structure makes correlation matrices \n  interpretable and avoids the quadratic increase of \n  parameters as a function of the dimension. \n  In the first approach a natural sequence of simpler \n  models along with a complexity penalization is used.\n  The second penalizes deviations from a base model.\n  These can be used as prior for model parameters,\n  considering C code through the 'cgeneric' interface \n  for the 'INLA' package (<https://www.r-inla.org>). \n  This allows one to use these models as building \n  blocks combined and to other latent Gaussian models \n  in order to build complex data models.  "
  },
  {
    "id": 13668,
    "package_name": "groupedHyperframe.random",
    "title": "Simulated Grouped Hyper Data Frame",
    "description": "An intuitive interface to simulate (1) superimposed (marked) point patterns with vectorized parameterization of random point pattern and distribution of marks; and (2) grouped hyper data frame based on population parameters and subject-specific random effects.",
    "version": "0.2.0",
    "maintainer": "Tingting Zhan <tingtingzhan@gmail.com>",
    "author": "Tingting Zhan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9971-4844>),\n  Inna Chervoneva [aut] (ORCID: <https://orcid.org/0000-0002-9104-4505>)",
    "url": "https://github.com/tingtingzhan/groupedHyperframe.random",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=groupedHyperframe.random",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "groupedHyperframe.random Simulated Grouped Hyper Data Frame An intuitive interface to simulate (1) superimposed (marked) point patterns with vectorized parameterization of random point pattern and distribution of marks; and (2) grouped hyper data frame based on population parameters and subject-specific random effects.  "
  },
  {
    "id": 13687,
    "package_name": "grpnet",
    "title": "Group Elastic Net Regularized GLMs and GAMs",
    "description": "Efficient algorithms for fitting generalized linear and additive models with group elastic net penalties as described in Helwig (2025) <doi:10.1080/10618600.2024.2362232>. Implements group LASSO, group MCP, and group SCAD with an optional group ridge penalty. Computes the regularization path for linear regression (gaussian), multivariate regression (multigaussian), smoothed support vector machines (svm1), squared support vector machines (svm2), logistic regression (binomial), multinomial logistic regression (multinomial), log-linear count regression (poisson and negative.binomial), and log-linear continuous regression (gamma and inverse gaussian). Supports default and formula methods for model specification, k-fold cross-validation for tuning the regularization parameters, and nonparametric regression via tensor product reproducing kernel (smoothing spline) basis function expansion. ",
    "version": "1.0",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "author": "Nathaniel E. Helwig [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=grpnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grpnet Group Elastic Net Regularized GLMs and GAMs Efficient algorithms for fitting generalized linear and additive models with group elastic net penalties as described in Helwig (2025) <doi:10.1080/10618600.2024.2362232>. Implements group LASSO, group MCP, and group SCAD with an optional group ridge penalty. Computes the regularization path for linear regression (gaussian), multivariate regression (multigaussian), smoothed support vector machines (svm1), squared support vector machines (svm2), logistic regression (binomial), multinomial logistic regression (multinomial), log-linear count regression (poisson and negative.binomial), and log-linear continuous regression (gamma and inverse gaussian). Supports default and formula methods for model specification, k-fold cross-validation for tuning the regularization parameters, and nonparametric regression via tensor product reproducing kernel (smoothing spline) basis function expansion.   "
  },
  {
    "id": 13733,
    "package_name": "gtools",
    "title": "Various R Programming Tools",
    "description": "Functions to assist in R programming, including:\n  - assist in developing, updating, and maintaining R and R packages ('ask', 'checkRVersion',\n    'getDependencies', 'keywords', 'scat'),\n  - calculate the logit and inverse logit transformations ('logit', 'inv.logit'),\n  - test if a value is missing, empty or contains only NA and NULL values ('invalid'),\n  - manipulate R's .Last function ('addLast'),\n  - define macros ('defmacro'),\n  - detect odd and even integers ('odd', 'even'),\n  - convert strings containing non-ASCII characters (like single quotes) to plain ASCII ('ASCIIfy'),\n  - perform a binary search ('binsearch'),\n  - sort strings containing both numeric and character components ('mixedsort'),\n  - create a factor variable from the quantiles of a continuous variable ('quantcut'),\n  - enumerate permutations and combinations ('combinations', 'permutation'),\n  - calculate and convert between fold-change and log-ratio ('foldchange',\n    'logratio2foldchange', 'foldchange2logratio'),\n  - calculate probabilities and generate random numbers from Dirichlet distributions\n    ('rdirichlet', 'ddirichlet'),\n  - apply a function over adjacent subsets of a vector ('running'),\n  - modify the TCP_NODELAY ('de-Nagle') flag for socket objects,\n  - efficient 'rbind' of data frames, even if the column names don't match ('smartbind'),\n  - generate significance stars from p-values ('stars.pval'),\n  - convert characters to/from ASCII codes ('asc', 'chr'),\n  - convert character vector to ASCII representation ('ASCIIfy'),\n  - apply title capitalization rules to a character vector ('capwords').",
    "version": "3.9.5",
    "maintainer": "Ben Bolker <bolker@mcmaster.ca>",
    "author": "Gregory R. Warnes [aut],\n  Ben Bolker [aut, cre] (ORCID: <https://orcid.org/0000-0002-2127-0443>),\n  Thomas Lumley [aut],\n  Arni Magnusson [aut],\n  Bill Venables [aut],\n  Genei Ryodan [aut],\n  Steffen Moeller [aut],\n  Ian Wilson [ctb],\n  Mark Davis [ctb],\n  Nitin Jain [ctb],\n  Scott Chamberlain [ctb]",
    "url": "https://github.com/r-gregmisc/gtools",
    "bug_reports": "https://github.com/r-gregmisc/gtools/issues",
    "repository": "https://cran.r-project.org/package=gtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gtools Various R Programming Tools Functions to assist in R programming, including:\n  - assist in developing, updating, and maintaining R and R packages ('ask', 'checkRVersion',\n    'getDependencies', 'keywords', 'scat'),\n  - calculate the logit and inverse logit transformations ('logit', 'inv.logit'),\n  - test if a value is missing, empty or contains only NA and NULL values ('invalid'),\n  - manipulate R's .Last function ('addLast'),\n  - define macros ('defmacro'),\n  - detect odd and even integers ('odd', 'even'),\n  - convert strings containing non-ASCII characters (like single quotes) to plain ASCII ('ASCIIfy'),\n  - perform a binary search ('binsearch'),\n  - sort strings containing both numeric and character components ('mixedsort'),\n  - create a factor variable from the quantiles of a continuous variable ('quantcut'),\n  - enumerate permutations and combinations ('combinations', 'permutation'),\n  - calculate and convert between fold-change and log-ratio ('foldchange',\n    'logratio2foldchange', 'foldchange2logratio'),\n  - calculate probabilities and generate random numbers from Dirichlet distributions\n    ('rdirichlet', 'ddirichlet'),\n  - apply a function over adjacent subsets of a vector ('running'),\n  - modify the TCP_NODELAY ('de-Nagle') flag for socket objects,\n  - efficient 'rbind' of data frames, even if the column names don't match ('smartbind'),\n  - generate significance stars from p-values ('stars.pval'),\n  - convert characters to/from ASCII codes ('asc', 'chr'),\n  - convert character vector to ASCII representation ('ASCIIfy'),\n  - apply title capitalization rules to a character vector ('capwords').  "
  },
  {
    "id": 13796,
    "package_name": "handcodeR",
    "title": "Text Annotation App",
    "description": "Shiny-App that allows to annotate vectors of texts to predefined categories by hand. ",
    "version": "0.1.2",
    "maintainer": "Lukas Isermann <lukas.isermann@uni-mannheim.de>",
    "author": "Lukas Isermann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7195-9302>)",
    "url": "https://github.com/liserman/handcodeR/",
    "bug_reports": "https://github.com/liserman/handcodeR/issues",
    "repository": "https://cran.r-project.org/package=handcodeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "handcodeR Text Annotation App Shiny-App that allows to annotate vectors of texts to predefined categories by hand.   "
  },
  {
    "id": 13800,
    "package_name": "handyplots",
    "title": "Handy Plots",
    "description": "Several handy plots for quickly looking at the relationship between two numeric vectors of equal length. Quickly visualize scatter plots, residual plots, qq-plots, box plots, confidence intervals, and prediction intervals.",
    "version": "1.1.3",
    "maintainer": "Jonathan Schwartz <jzs1986@gmail.com>",
    "author": "Jonathan Schwartz",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=handyplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "handyplots Handy Plots Several handy plots for quickly looking at the relationship between two numeric vectors of equal length. Quickly visualize scatter plots, residual plots, qq-plots, box plots, confidence intervals, and prediction intervals.  "
  },
  {
    "id": 13808,
    "package_name": "happign",
    "title": "R Interface to 'IGN' Web Services",
    "description": "Automatic open data acquisition from resources of IGN\n    ('Institut National de Information Geographique et forestiere')\n    (<https://www.ign.fr/>). Available datasets include various types of\n    raster and vector data, such as digital elevation models, state\n    borders, spatial databases, cadastral parcels, and more. 'happign' also \n    provide access to API Carto (<https://apicarto.ign.fr/api/doc/>).",
    "version": "0.3.6",
    "maintainer": "Paul Carteron <carteronpaul@gmail.com>",
    "author": "Paul Carteron [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6942-6662>)",
    "url": "https://github.com/paul-carteron,\nhttps://paul-carteron.github.io/happign/",
    "bug_reports": "https://github.com/paul-carteron/happign/issues",
    "repository": "https://cran.r-project.org/package=happign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "happign R Interface to 'IGN' Web Services Automatic open data acquisition from resources of IGN\n    ('Institut National de Information Geographique et forestiere')\n    (<https://www.ign.fr/>). Available datasets include various types of\n    raster and vector data, such as digital elevation models, state\n    borders, spatial databases, cadastral parcels, and more. 'happign' also \n    provide access to API Carto (<https://apicarto.ign.fr/api/doc/>).  "
  },
  {
    "id": 13817,
    "package_name": "hash",
    "title": "Full Featured Implementation of Hash Tables/Associative\nArrays/Dictionaries",
    "description": "Implements a data structure similar to hashes in Perl and dictionaries in Python but with a purposefully R flavor. For objects of appreciable size, access using hashes outperforms native named lists and vectors.",
    "version": "2.2.6.3",
    "maintainer": "John Hughes <drjphughesjr@gmail.com>",
    "author": "Christopher Brown [aut],\n  John Hughes [cre, ctb]",
    "url": "http://www.johnhughes.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hash",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hash Full Featured Implementation of Hash Tables/Associative\nArrays/Dictionaries Implements a data structure similar to hashes in Perl and dictionaries in Python but with a purposefully R flavor. For objects of appreciable size, access using hashes outperforms native named lists and vectors.  "
  },
  {
    "id": 13818,
    "package_name": "hashids",
    "title": "Generate Short Unique YouTube-Like IDs (Hashes) from Integers",
    "description": "An R port of the hashids library.  hashids generates YouTube-like hashes from integers or vector of integers.  Hashes generated from integers are relatively short, unique and non-seqential.  hashids can be used to generate unique ids for URLs and hide database row numbers from the user.  By default hashids will avoid generating common English cursewords by preventing certain letters being next to each other.  hashids are not one-way: it is easy to encode an integer to a hashid and decode a hashid back into an integer.",
    "version": "0.9.0",
    "maintainer": "Alex Shum <Alex@ALShum.com>",
    "author": "Alex Shum [aut, cre],\n  Ivan Akimov [aut] (original author of hashids -- implemented in\n    javascript),\n  David Aurelio [ctb] (implemented hashids in python 2 and 3)",
    "url": "https://github.com/ALShum/hashids-r/, http://hashids.org",
    "bug_reports": "https://github.com/ALShum/hashids-r/issues",
    "repository": "https://cran.r-project.org/package=hashids",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hashids Generate Short Unique YouTube-Like IDs (Hashes) from Integers An R port of the hashids library.  hashids generates YouTube-like hashes from integers or vector of integers.  Hashes generated from integers are relatively short, unique and non-seqential.  hashids can be used to generate unique ids for URLs and hide database row numbers from the user.  By default hashids will avoid generating common English cursewords by preventing certain letters being next to each other.  hashids are not one-way: it is easy to encode an integer to a hashid and decode a hashid back into an integer.  "
  },
  {
    "id": 13819,
    "package_name": "hashmapR",
    "title": "Fast, Vectorized Hashmap",
    "description": "A fast, vectorized hashmap that is built on top of 'C++' std::unordered_map <https://en.cppreference.com/w/cpp/container/unordered_map.html>.\n    The map can hold any 'R' object as key / value as long as it is serializable and supports\n    vectorized insertion, lookup, and deletion.",
    "version": "1.0.1",
    "maintainer": "Sven Glinz <svenglinz@proton.me>",
    "author": "Sven Glinz [aut, cre]",
    "url": "https://github.com/svensglinz/hashmapR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hashmapR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hashmapR Fast, Vectorized Hashmap A fast, vectorized hashmap that is built on top of 'C++' std::unordered_map <https://en.cppreference.com/w/cpp/container/unordered_map.html>.\n    The map can hold any 'R' object as key / value as long as it is serializable and supports\n    vectorized insertion, lookup, and deletion.  "
  },
  {
    "id": 13820,
    "package_name": "hashr",
    "title": "Hash R Objects to Integers Fast",
    "description": "Apply an adaptation of the SuperFastHash algorithm to any R\n    object. Hash whole R objects or, for vectors or lists, hash R objects to obtain\n    a set of hash values that is stored in a structure equivalent to the input. See\n    <http://www.azillionmonkeys.com/qed/hash.html> for a description of the hash\n    algorithm.",
    "version": "0.1.4",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre],\n  Paul Hsieh [ctb]",
    "url": "https://github.com/markvanderloo/hashr",
    "bug_reports": "https://github.com/markvanderloo/hashr/issues",
    "repository": "https://cran.r-project.org/package=hashr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hashr Hash R Objects to Integers Fast Apply an adaptation of the SuperFastHash algorithm to any R\n    object. Hash whole R objects or, for vectors or lists, hash R objects to obtain\n    a set of hash values that is stored in a structure equivalent to the input. See\n    <http://www.azillionmonkeys.com/qed/hash.html> for a description of the hash\n    algorithm.  "
  },
  {
    "id": 13825,
    "package_name": "hazer",
    "title": "Identifying Foggy and Cloudy Images by Quantifying Haziness",
    "description": "Provides a set of functions to estimate haziness of an image based on RGB bands. It returns a haze factor, varying from 0 to 1, a metric for fogginess and cloudiness. The package also presents additional functions to estimate brightness, darkness and contrast rasters of the RGB image. This package can be used for several applications such as inference of weather quality data and performing environmental studies from interpreting digital images.",
    "version": "1.1.1",
    "maintainer": "Bijan Seyednasrollah <bijan.s.nasr@gmail.com>",
    "author": "Bijan Seyednasrollah",
    "url": "https://github.com/bnasr/hazer/",
    "bug_reports": "https://github.com/bnasr/hazer/issues",
    "repository": "https://cran.r-project.org/package=hazer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hazer Identifying Foggy and Cloudy Images by Quantifying Haziness Provides a set of functions to estimate haziness of an image based on RGB bands. It returns a haze factor, varying from 0 to 1, a metric for fogginess and cloudiness. The package also presents additional functions to estimate brightness, darkness and contrast rasters of the RGB image. This package can be used for several applications such as inference of weather quality data and performing environmental studies from interpreting digital images.  "
  },
  {
    "id": 13859,
    "package_name": "hdiVAR",
    "title": "Statistical Inference for Noisy Vector Autoregression",
    "description": "The model is high-dimensional vector autoregression with measurement error, also known as linear gaussian state-space model. Provable sparse expectation-maximization algorithm is provided for the estimation of transition matrix and noise variances. Global and simultaneous testings are implemented for transition matrix with false discovery rate control. For more information, see the accompanying paper: Lyu, X., Kang, J., & Li, L. (2023). \"Statistical inference for high-dimensional vector autoregression with measurement error\", Statistica Sinica.",
    "version": "1.0.2",
    "maintainer": "Xiang Lyu <xianglyu.public@gmail.com>",
    "author": "Xiang Lyu [aut, cre],\n  Jian Kang [aut],\n  Lexin Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdiVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdiVAR Statistical Inference for Noisy Vector Autoregression The model is high-dimensional vector autoregression with measurement error, also known as linear gaussian state-space model. Provable sparse expectation-maximization algorithm is provided for the estimation of transition matrix and noise variances. Global and simultaneous testings are implemented for transition matrix with false discovery rate control. For more information, see the accompanying paper: Lyu, X., Kang, J., & Li, L. (2023). \"Statistical inference for high-dimensional vector autoregression with measurement error\", Statistica Sinica.  "
  },
  {
    "id": 13864,
    "package_name": "hdpca",
    "title": "Principal Component Analysis in High-Dimensional Data",
    "description": "In high-dimensional settings:\n\tEstimate the number of distant spikes based on the Generalized Spiked Population (GSP) model.\n\tEstimate the population eigenvalues, angles between the sample and population eigenvectors, correlations between the sample and population PC scores, and the asymptotic shrinkage factors.\n\tAdjust the shrinkage bias in the predicted PC scores.\n\tDey, R. and Lee, S. (2019) <doi:10.1016/j.jmva.2019.02.007>.",
    "version": "1.1.5",
    "maintainer": "Rounak Dey <deyrnk@umich.edu>",
    "author": "Rounak Dey, Seunggeun Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdpca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdpca Principal Component Analysis in High-Dimensional Data In high-dimensional settings:\n\tEstimate the number of distant spikes based on the Generalized Spiked Population (GSP) model.\n\tEstimate the population eigenvalues, angles between the sample and population eigenvectors, correlations between the sample and population PC scores, and the asymptotic shrinkage factors.\n\tAdjust the shrinkage bias in the predicted PC scores.\n\tDey, R. and Lee, S. (2019) <doi:10.1016/j.jmva.2019.02.007>.  "
  },
  {
    "id": 13868,
    "package_name": "hdsvm",
    "title": "Fast Algorithm for Support Vector Machine",
    "description": "Implements an efficient algorithm for fitting the entire regularization path of support vector machine models with elastic-net penalties using a generalized coordinate descent scheme. The framework also supports SCAD and MCP penalties. It is designed for high-dimensional datasets and emphasizes numerical accuracy and computational efficiency. This package implements the algorithms proposed in Tang, Q., Zhang, Y., & Wang, B. (2022) <https://openreview.net/pdf?id=RvwMTDYTOb>.",
    "version": "1.0.2",
    "maintainer": "Yikai Zhang <yikai-zhang@uiowa.edu>",
    "author": "Yikai Zhang [aut, cre],\n  Qian Tang [aut],\n  Boxiang Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdsvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdsvm Fast Algorithm for Support Vector Machine Implements an efficient algorithm for fitting the entire regularization path of support vector machine models with elastic-net penalties using a generalized coordinate descent scheme. The framework also supports SCAD and MCP penalties. It is designed for high-dimensional datasets and emphasizes numerical accuracy and computational efficiency. This package implements the algorithms proposed in Tang, Q., Zhang, Y., & Wang, B. (2022) <https://openreview.net/pdf?id=RvwMTDYTOb>.  "
  },
  {
    "id": 13877,
    "package_name": "healthiar",
    "title": "Quantify and Monetize the Burden of Disease Attributable to\nExposure",
    "description": "This R package has been developed with a focus on air pollution and noise but can applied to other exposures. The initial development has been funded by the European Union project BEST-COST. Disclaimer: It is work in progress and the developers are not liable for any calculation errors or inaccuracies resulting from the use of this package.\n References (in chronological order): \n WHO (2003a) \"Assessing the environmental burden of disease at national and local levels\" <https://www.who.int/publications/i/item/9241546204> (accessed October 2025);\n WHO (2003b) \"Comparative quantification of health risks: Conceptual framework and methodological issues\" <doi:10.1186/1478-7954-1-1> (accessed October 2025);\n Miller & Hurley (2003) \"Life table methods for quantitative impact assessments in chronic mortality\" <doi:10.1136/jech.57.3.200> (accessed October 2025);\n Steenland & Armstrong (2006) \"An Overview of Methods for Calculating the Burden of Disease Due to Specific Risk Factors\" <doi:10.1097/01.ede.0000229155.05644.43> (accessed October 2025);\n Miller (2010) \"Report on estimation of mortality impacts of particulate air pollution in London\" <https://cleanair.london/app/uploads/CAL-098-Mayors-health-study-report-June-2010-1.pdf> (accessed October 2025);\n WHO (2011) \"Burden of disease from environmental noise\" <https://iris.who.int/items/723ab97c-5c33-4e3b-8df1-744aa5bc1c27> (accessed October 2025);\n Jerrett et al. (2013) \"Spatial Analysis of Air Pollution and Mortality in California\" <doi:10.1164/rccm.201303-0609OC> (accessed October 2025);\n GBD 2019 Risk Factors Collaborators (2020) \"Global burden of 87 risk factors in 204 countries and territories, 1990\u20132019\" <doi:10.1016/S0140-6736(20)30752-2> (accessed October 2025);\n VanderWeele (2019) \"Optimal Approximate Conversions of Odds Ratios and Hazard Ratios to Risk Ratios\" <doi: 10.1111/biom.13197> (accessed October 2025);\n WHO (2020) \"Health impact assessment of air pollution: AirQ+ life table manual\" <https://iris.who.int/bitstream/handle/10665/337683/WHO-EURO-2020-1559-41310-56212-eng.pdf?sequence=1> (accessed October 2025);\n ETC HE (2022) \"Health risk assessment of air pollution and the impact of the new WHO guidelines\" <https://www.eionet.europa.eu/etcs/all-etc-reports> (accessed October 2025);\n Kim et al. (2022) \"DALY Estimation Approaches: Understanding and Using the Incidence-based Approach and the Prevalence-based Approach\" <doi:10.3961/jpmph.21.597> (accessed October 2025);\n Pozzer et al. (2022) \"Mortality Attributable to Ambient Air Pollution: A Review of Global Estimates\" <doi:10.1029/2022GH000711> (accessed October 2025);\n Teaching group in EBM (2022) \"Evidence-based medicine research helper\" <https://ebm-helper.cn/en/Conv/HR_RR.html> (accessed October 2025).",
    "version": "0.2.1",
    "maintainer": "Alberto Castro <alberto.castrofernandez@swisstph.ch>",
    "author": "Alberto Castro [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4665-3299>),\n  Axel Luyten [aut] (ORCID: <https://orcid.org/0000-0002-7005-5889>),\n  Arno Pauwels [ctb] (ORCID: <https://orcid.org/0000-0001-7519-8080>),\n  Liliana Vazquez Fernandez [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3778-9415>),\n  Vanessa Gorasso [ctb] (ORCID: <https://orcid.org/0000-0001-6884-9316>),\n  Carl Michael Baravelli [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7772-5315>),\n  Susanne Breitner [ctb] (ORCID: <https://orcid.org/0000-0002-0956-6911>),\n  Maria Lepnurm [ctb] (ORCID: <https://orcid.org/0009-0009-4372-6227>),\n  Maria Jose Rueda Lopez [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2443-1038>),\n  Iracy Pimenta [ctb] (ORCID: <https://orcid.org/0000-0003-0032-1536>),\n  Andreia Novais [ctb] (ORCID: <https://orcid.org/0009-0007-7775-108X>),\n  Ana Barbosa [ctb] (ORCID: <https://orcid.org/0000-0002-9623-9002>),\n  Joao Vasco Santos [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4696-1002>),\n  Anette Kocbach Bolling [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4209-7448>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=healthiar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "healthiar Quantify and Monetize the Burden of Disease Attributable to\nExposure This R package has been developed with a focus on air pollution and noise but can applied to other exposures. The initial development has been funded by the European Union project BEST-COST. Disclaimer: It is work in progress and the developers are not liable for any calculation errors or inaccuracies resulting from the use of this package.\n References (in chronological order): \n WHO (2003a) \"Assessing the environmental burden of disease at national and local levels\" <https://www.who.int/publications/i/item/9241546204> (accessed October 2025);\n WHO (2003b) \"Comparative quantification of health risks: Conceptual framework and methodological issues\" <doi:10.1186/1478-7954-1-1> (accessed October 2025);\n Miller & Hurley (2003) \"Life table methods for quantitative impact assessments in chronic mortality\" <doi:10.1136/jech.57.3.200> (accessed October 2025);\n Steenland & Armstrong (2006) \"An Overview of Methods for Calculating the Burden of Disease Due to Specific Risk Factors\" <doi:10.1097/01.ede.0000229155.05644.43> (accessed October 2025);\n Miller (2010) \"Report on estimation of mortality impacts of particulate air pollution in London\" <https://cleanair.london/app/uploads/CAL-098-Mayors-health-study-report-June-2010-1.pdf> (accessed October 2025);\n WHO (2011) \"Burden of disease from environmental noise\" <https://iris.who.int/items/723ab97c-5c33-4e3b-8df1-744aa5bc1c27> (accessed October 2025);\n Jerrett et al. (2013) \"Spatial Analysis of Air Pollution and Mortality in California\" <doi:10.1164/rccm.201303-0609OC> (accessed October 2025);\n GBD 2019 Risk Factors Collaborators (2020) \"Global burden of 87 risk factors in 204 countries and territories, 1990\u20132019\" <doi:10.1016/S0140-6736(20)30752-2> (accessed October 2025);\n VanderWeele (2019) \"Optimal Approximate Conversions of Odds Ratios and Hazard Ratios to Risk Ratios\" <doi: 10.1111/biom.13197> (accessed October 2025);\n WHO (2020) \"Health impact assessment of air pollution: AirQ+ life table manual\" <https://iris.who.int/bitstream/handle/10665/337683/WHO-EURO-2020-1559-41310-56212-eng.pdf?sequence=1> (accessed October 2025);\n ETC HE (2022) \"Health risk assessment of air pollution and the impact of the new WHO guidelines\" <https://www.eionet.europa.eu/etcs/all-etc-reports> (accessed October 2025);\n Kim et al. (2022) \"DALY Estimation Approaches: Understanding and Using the Incidence-based Approach and the Prevalence-based Approach\" <doi:10.3961/jpmph.21.597> (accessed October 2025);\n Pozzer et al. (2022) \"Mortality Attributable to Ambient Air Pollution: A Review of Global Estimates\" <doi:10.1029/2022GH000711> (accessed October 2025);\n Teaching group in EBM (2022) \"Evidence-based medicine research helper\" <https://ebm-helper.cn/en/Conv/HR_RR.html> (accessed October 2025).  "
  },
  {
    "id": 13918,
    "package_name": "hetcorFS",
    "title": "Unsupervised Feature Selection using the Heterogeneous\nCorrelation Matrix",
    "description": "Unsupervised multivariate filter feature selection using the UFS-rHCM or UFS-cHCM algorithms based on the heterogeneous correlation matrix (HCM). The HCM consists of Pearson's correlations between numerical features, polyserial correlations between numerical and ordinal features,  and polychoric correlations between ordinal features. Tortora C., Madhvani S., Punzo A. (2025). \"Designing unsupervised mixed-type feature selection techniques using the heterogeneous correlation matrix.\" International Statistical Review <doi:10.1111/insr.70016>. This work was supported by the National Science foundation NSF Grant N 2209974 (Tortora) and by the Italian Ministry of University and Research (MUR) under the PRIN 2022 grant number 2022XRHT8R (CUP: E53D23005950006), as part of \u2018The SMILE Project: Statistical Modelling and Inference to Live the Environment\u2019, funded by the European Union \u2013 Next Generation EU (Punzo).",
    "version": "1.0.1",
    "maintainer": "Cristina Tortora <grikris1@gmail.com>",
    "author": "Cristina Tortora [aut, cre, fnd],\n  Antonio Punzo [aut],\n  Shaam Madhvani [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hetcorFS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hetcorFS Unsupervised Feature Selection using the Heterogeneous\nCorrelation Matrix Unsupervised multivariate filter feature selection using the UFS-rHCM or UFS-cHCM algorithms based on the heterogeneous correlation matrix (HCM). The HCM consists of Pearson's correlations between numerical features, polyserial correlations between numerical and ordinal features,  and polychoric correlations between ordinal features. Tortora C., Madhvani S., Punzo A. (2025). \"Designing unsupervised mixed-type feature selection techniques using the heterogeneous correlation matrix.\" International Statistical Review <doi:10.1111/insr.70016>. This work was supported by the National Science foundation NSF Grant N 2209974 (Tortora) and by the Italian Ministry of University and Research (MUR) under the PRIN 2022 grant number 2022XRHT8R (CUP: E53D23005950006), as part of \u2018The SMILE Project: Statistical Modelling and Inference to Live the Environment\u2019, funded by the European Union \u2013 Next Generation EU (Punzo).  "
  },
  {
    "id": 13919,
    "package_name": "heterocop",
    "title": "Semi-Parametric Estimation with Gaussian Copula",
    "description": "A method for estimating the correlation matrix of the Gaussian copula from the observed data. This package also contains a penalized estimation of the corresponding precision matrix, and enables to generate random vectors that are distributed according to a Gaussian copula.",
    "version": "1.0.0",
    "maintainer": "Ekaterina Tomilina <ekaterina.tomilina@inrae.fr>",
    "author": "Julie Cartier [aut],\n  Florence Jaffrezic [aut],\n  Gildas Mazo [aut],\n  Ekaterina Tomilina [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=heterocop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "heterocop Semi-Parametric Estimation with Gaussian Copula A method for estimating the correlation matrix of the Gaussian copula from the observed data. This package also contains a penalized estimation of the corresponding precision matrix, and enables to generate random vectors that are distributed according to a Gaussian copula.  "
  },
  {
    "id": 13942,
    "package_name": "hglm",
    "title": "Hierarchical Generalized Linear Models",
    "description": "Implemented here are procedures for fitting hierarchical generalized linear models (HGLM). It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the mean model. As statistical models, HGLMs were initially developed by Lee and Nelder (1996) <https://www.jstor.org/stable/2346105?seq=1>. We provide an implementation (Ronnegard, Alam and Shen 2010) <https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Roennegaard~et~al.pdf> following Lee, Nelder and Pawitan (2006) <ISBN: 9781420011340> with algorithms extended for spatial modeling (Alam, Ronnegard and Shen 2015) <https://journal.r-project.org/archive/2015/RJ-2015-017/RJ-2015-017.pdf>. ",
    "version": "2.2-1",
    "maintainer": "Xia Shen <xia.shen@ki.se>",
    "author": "Moudud Alam, Lars Ronnegard, Xia Shen",
    "url": "",
    "bug_reports": "https://r-forge.r-project.org/tracker/?group_id=558",
    "repository": "https://cran.r-project.org/package=hglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hglm Hierarchical Generalized Linear Models Implemented here are procedures for fitting hierarchical generalized linear models (HGLM). It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the mean model. As statistical models, HGLMs were initially developed by Lee and Nelder (1996) <https://www.jstor.org/stable/2346105?seq=1>. We provide an implementation (Ronnegard, Alam and Shen 2010) <https://journal.r-project.org/archive/2010-2/RJournal_2010-2_Roennegaard~et~al.pdf> following Lee, Nelder and Pawitan (2006) <ISBN: 9781420011340> with algorithms extended for spatial modeling (Alam, Ronnegard and Shen 2015) <https://journal.r-project.org/archive/2015/RJ-2015-017/RJ-2015-017.pdf>.   "
  },
  {
    "id": 13969,
    "package_name": "highd2means",
    "title": "High-Dimensional Tests for Two Population Mean Vectors",
    "description": "Tests for two high-dimensional population mean vectors. The user has the option to compute the asymptotic, the permutation or the bootstrap based p-value of the test. Some references are: Chen S.X. and Qin Y.L. (2010). <doi:10.1214/09-AOS716>, Cai T.T., Liu W., and Xia Y. (2014) <doi:10.1111/rssb.12034> and Yu X., Li D., Xue L. and Li, R. (2023) <doi:10.1080/01621459.2022.2061354>. ",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Manos Papadakis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=highd2means",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highd2means High-Dimensional Tests for Two Population Mean Vectors Tests for two high-dimensional population mean vectors. The user has the option to compute the asymptotic, the permutation or the bootstrap based p-value of the test. Some references are: Chen S.X. and Qin Y.L. (2010). <doi:10.1214/09-AOS716>, Cai T.T., Liu W., and Xia Y. (2014) <doi:10.1111/rssb.12034> and Yu X., Li D., Xue L. and Li, R. (2023) <doi:10.1080/01621459.2022.2061354>.   "
  },
  {
    "id": 13975,
    "package_name": "highmean",
    "title": "Two-Sample Tests for High-Dimensional Mean Vectors",
    "description": "Provides various tests for comparing high-dimensional mean vectors in two sample populations.",
    "version": "3.0",
    "maintainer": "Lifeng Lin <linl@umn.edu>",
    "author": "Lifeng Lin and Wei Pan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=highmean",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highmean Two-Sample Tests for High-Dimensional Mean Vectors Provides various tests for comparing high-dimensional mean vectors in two sample populations.  "
  },
  {
    "id": 13986,
    "package_name": "hillshader",
    "title": "Create Hillshade Relief Maps Using Ray-Tracing",
    "description": "A set of tools to create georeferenced hillshade relief \n    raster maps using ray-tracing and other advanced hill-shading \n    techniques. It includes a wrapper function to create a georeferenced,\n    ray-traced hillshade map from a digital elevation model, and other\n    functions that can be used in a rayshader pipeline. ",
    "version": "0.1.2",
    "maintainer": "Pierre Roudier <pierre.roudier@gmail.com>",
    "author": "Pierre Roudier [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7431-2603>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hillshader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hillshader Create Hillshade Relief Maps Using Ray-Tracing A set of tools to create georeferenced hillshade relief \n    raster maps using ray-tracing and other advanced hill-shading \n    techniques. It includes a wrapper function to create a georeferenced,\n    ray-traced hillshade map from a digital elevation model, and other\n    functions that can be used in a rayshader pipeline.   "
  },
  {
    "id": 14000,
    "package_name": "histoslider",
    "title": "A Histogram Slider Input for 'Shiny'",
    "description": "A histogram slider input binding for use in 'Shiny'. Currently supports creating histograms from numeric, date, and 'date-time' vectors.",
    "version": "0.1.1",
    "maintainer": "Carson Sievert <cpsievert1@gmail.com>",
    "author": "Carson Sievert [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4958-2844>),\n  Samuel Hogg [ctb, cph] (Original author of histoslider React component)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=histoslider",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "histoslider A Histogram Slider Input for 'Shiny' A histogram slider input binding for use in 'Shiny'. Currently supports creating histograms from numeric, date, and 'date-time' vectors.  "
  },
  {
    "id": 14046,
    "package_name": "houba",
    "title": "Manipulation of (Large) Memory-Mapped Objects (Vectors, Matrices\nand Arrays)",
    "description": "Manipulate data through memory-mapped files, as vectors, matrices or arrays.\n    Basic arithmetic functions are implemented, but currently no matrix arithmetic.\n    Can write and read descriptor files for compatibility with the 'bigmemory' package.",
    "version": "0.1.0",
    "maintainer": "Herv\u00e9 Perdry <herve.perdry@universite-paris-saclay.fr>",
    "author": "Herv\u00e9 Perdry [aut, cre],\n  Juliette Meyniel [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=houba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "houba Manipulation of (Large) Memory-Mapped Objects (Vectors, Matrices\nand Arrays) Manipulate data through memory-mapped files, as vectors, matrices or arrays.\n    Basic arithmetic functions are implemented, but currently no matrix arithmetic.\n    Can write and read descriptor files for compatibility with the 'bigmemory' package.  "
  },
  {
    "id": 14063,
    "package_name": "hrt",
    "title": "Heteroskedasticity Robust Testing",
    "description": "Functions for testing affine hypotheses on the regression coefficient vector in regression models with heteroskedastic errors: (i) a function for computing various test statistics (in particular using HC0-HC4 covariance estimators based on unrestricted or restricted residuals); (ii) a function for numerically approximating the size of a test based on such test statistics and a user-supplied critical value; and, most importantly, (iii) a function for determining size-controlling critical values for such test statistics and a user-supplied significance level (also incorporating a check of conditions under which such a size-controlling critical value exists). The three functions are based on results in Poetscher and Preinerstorfer (2021) \"Valid Heteroskedasticity Robust Testing\" <doi:10.48550/arXiv.2104.12597>, which will appear as <doi:10.1017/S0266466623000269>.",
    "version": "1.0.2",
    "maintainer": "David Preinerstorfer <david.preinerstorfer@wu.ac.at>",
    "author": "David Preinerstorfer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hrt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hrt Heteroskedasticity Robust Testing Functions for testing affine hypotheses on the regression coefficient vector in regression models with heteroskedastic errors: (i) a function for computing various test statistics (in particular using HC0-HC4 covariance estimators based on unrestricted or restricted residuals); (ii) a function for numerically approximating the size of a test based on such test statistics and a user-supplied critical value; and, most importantly, (iii) a function for determining size-controlling critical values for such test statistics and a user-supplied significance level (also incorporating a check of conditions under which such a size-controlling critical value exists). The three functions are based on results in Poetscher and Preinerstorfer (2021) \"Valid Heteroskedasticity Robust Testing\" <doi:10.48550/arXiv.2104.12597>, which will appear as <doi:10.1017/S0266466623000269>.  "
  },
  {
    "id": 14070,
    "package_name": "hspm",
    "title": "Heterogeneous Spatial Models",
    "description": "Spatial heterogeneity can be specified in various ways. 'hspm' is an ambitious project that aims at implementing various methodologies to control for heterogeneity in spatial models. The current version of 'hspm' deals with spatial and (non-spatial) regimes models. In particular, the package allows to estimate a general spatial regimes model with additional endogenous variables, specified in terms of a spatial lag of the dependent variable, the spatially lagged regressors, and, potentially, a spatially autocorrelated error term. Spatial regime models are estimated by instrumental variables and generalized methods of moments (see Arraiz et al., (2010) <doi:10.1111/j.1467-9787.2009.00618.x>, Bivand and Piras, (2015) <doi:10.18637/jss.v063.i18>, Drukker et al., (2013) <doi:10.1080/07474938.2013.741020>, Kelejian and Prucha, (2010) <doi:10.1016/j.jeconom.2009.10.025>).",
    "version": "1.1",
    "maintainer": "Gianfranco Piras <gpiras@mac.com>",
    "author": "Gianfranco Piras [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0225-6061>),\n  Mauricio Sarrias [aut] (ORCID: <https://orcid.org/0000-0001-5932-4817>)",
    "url": "https://github.com/gpiras/hspm",
    "bug_reports": "https://github.com/gpiras/hspm/issues",
    "repository": "https://cran.r-project.org/package=hspm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hspm Heterogeneous Spatial Models Spatial heterogeneity can be specified in various ways. 'hspm' is an ambitious project that aims at implementing various methodologies to control for heterogeneity in spatial models. The current version of 'hspm' deals with spatial and (non-spatial) regimes models. In particular, the package allows to estimate a general spatial regimes model with additional endogenous variables, specified in terms of a spatial lag of the dependent variable, the spatially lagged regressors, and, potentially, a spatially autocorrelated error term. Spatial regime models are estimated by instrumental variables and generalized methods of moments (see Arraiz et al., (2010) <doi:10.1111/j.1467-9787.2009.00618.x>, Bivand and Piras, (2015) <doi:10.18637/jss.v063.i18>, Drukker et al., (2013) <doi:10.1080/07474938.2013.741020>, Kelejian and Prucha, (2010) <doi:10.1016/j.jeconom.2009.10.025>).  "
  },
  {
    "id": 14082,
    "package_name": "htrSPRanalysis",
    "title": "Analysis of Surface Plasmon Resonance Data",
    "description": "Analysis of Surface Plasmon Resonance (SPR) and Biolayer Interferometry data, with automations for high-throughput SPR. This version of the package fits the 1: 1 binding model, with and without bulkshift. It offers optional local or global Rmax fitting. The user must provide a sample sheet and a Carterra output file in Carterra's current format. There is a utility function to convert from Carterra's old output format. The user may run a custom pipeline or use the provided 'Runscript', which will produce a pdf file containing fitted Rmax, ka, kd and standard errors, a plot of the sensorgram and fits, and a plot of residuals. The script will also produce a .csv file with all of the relevant parameters for each spot on the SPR chip.",
    "version": "0.1.0",
    "maintainer": "Janice McCarthy Developer <janice.mccarthy@duke.edu>",
    "author": "Janice McCarthy Developer [aut, cre, cph],\n  Kan Li Dev [aut],\n  S. Moses Dennison [aut],\n  Georgia D. Tomaras [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=htrSPRanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "htrSPRanalysis Analysis of Surface Plasmon Resonance Data Analysis of Surface Plasmon Resonance (SPR) and Biolayer Interferometry data, with automations for high-throughput SPR. This version of the package fits the 1: 1 binding model, with and without bulkshift. It offers optional local or global Rmax fitting. The user must provide a sample sheet and a Carterra output file in Carterra's current format. There is a utility function to convert from Carterra's old output format. The user may run a custom pipeline or use the provided 'Runscript', which will produce a pdf file containing fitted Rmax, ka, kd and standard errors, a plot of the sensorgram and fits, and a plot of residuals. The script will also produce a .csv file with all of the relevant parameters for each spot on the SPR chip.  "
  },
  {
    "id": 14125,
    "package_name": "hydflood",
    "title": "Flood Extents and Duration along the Rivers Elbe and Rhine",
    "description": "Raster based flood modelling internally using 'hyd1d', an R package\n    to interpolate 1d water level and gauging data. The package computes flood\n    extent and duration through strategies originally developed for 'INFORM',\n    an 'ArcGIS'-based hydro-ecological modelling framework. It does not provide\n    a full, physical hydraulic modelling algorithm, but a simplified, near real\n    time 'GIS' approach for flood extent and duration modelling. Computationally\n    demanding annual flood durations have been computed already and data\n    products were published by Weber (2022) <doi:10.1594/PANGAEA.948042>.",
    "version": "0.5.10",
    "maintainer": "Arnd Weber <arnd.weber@bafg.de>",
    "author": "Arnd Weber [aut, cre] (ORCID: <https://orcid.org/0000-0002-5973-2770>),\n  Stephan Rosenzweig [ctb],\n  Benjamin Eberhardt [ctb]",
    "url": "https://hydflood.bafg.de, https://github.com/bafg-bund/hydflood",
    "bug_reports": "https://github.com/bafg-bund/hydflood/issues/",
    "repository": "https://cran.r-project.org/package=hydflood",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hydflood Flood Extents and Duration along the Rivers Elbe and Rhine Raster based flood modelling internally using 'hyd1d', an R package\n    to interpolate 1d water level and gauging data. The package computes flood\n    extent and duration through strategies originally developed for 'INFORM',\n    an 'ArcGIS'-based hydro-ecological modelling framework. It does not provide\n    a full, physical hydraulic modelling algorithm, but a simplified, near real\n    time 'GIS' approach for flood extent and duration modelling. Computationally\n    demanding annual flood durations have been computed already and data\n    products were published by Weber (2022) <doi:10.1594/PANGAEA.948042>.  "
  },
  {
    "id": 14144,
    "package_name": "hyperSpec",
    "title": "Work with Hyperspectral Data, i.e. Spectra + Meta Information\n(Spatial, Time, Concentration, ...)",
    "description": "Comfortable ways to work with hyperspectral data sets.\n    I.e. spatially or time-resolved spectra, or spectra with any other kind\n    of information associated with each of the spectra. The spectra can be data\n    as obtained in XRF, UV/VIS, Fluorescence, AES, NIR, IR, Raman, NMR, MS,\n    etc. More generally, any data that is recorded over a discretized variable,\n    e.g. absorbance = f(wavelength), stored as a vector of absorbance values\n    for discrete wavelengths is suitable.",
    "version": "0.100.3",
    "maintainer": "Claudia Beleites <Claudia.Beleites@chemometrix.gmbh>",
    "author": "Claudia Beleites [aut, cre, dtc] (ORCID:\n    <https://orcid.org/0000-0003-1626-154X>),\n  Valter Sergo [aut],\n  Alois Bonifacio [ctb, dtc],\n  Marcel Dahms [ctb],\n  Bj\u00f6rn Egert [ctb],\n  Simon Fuller [ctb],\n  Vilmantas Gegzna [aut],\n  Rustam Guliev [ctb],\n  Bryan A. Hanson [ctb],\n  Michael Hermes [ctb],\n  Martin Kammer [dtc],\n  Roman Kiselev [ctb],\n  Sebastian Mellor [ctb]",
    "url": "https://r-hyperspec.github.io/hyperSpec/ (documentation),\nhttps://github.com/r-hyperspec/hyperSpec (code)",
    "bug_reports": "https://github.com/r-hyperspec/hyperSpec/issues",
    "repository": "https://cran.r-project.org/package=hyperSpec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hyperSpec Work with Hyperspectral Data, i.e. Spectra + Meta Information\n(Spatial, Time, Concentration, ...) Comfortable ways to work with hyperspectral data sets.\n    I.e. spatially or time-resolved spectra, or spectra with any other kind\n    of information associated with each of the spectra. The spectra can be data\n    as obtained in XRF, UV/VIS, Fluorescence, AES, NIR, IR, Raman, NMR, MS,\n    etc. More generally, any data that is recorded over a discretized variable,\n    e.g. absorbance = f(wavelength), stored as a vector of absorbance values\n    for discrete wavelengths is suitable.  "
  },
  {
    "id": 14149,
    "package_name": "hypergeo2",
    "title": "Generalized Hypergeometric Function with Tunable High Precision",
    "description": "Computation of generalized hypergeometric function with tunable high precision in a vectorized manner, with the floating-point datatypes from 'mpfr' or 'gmp' library. The computation is limited to real numbers.",
    "version": "0.2.0",
    "maintainer": "Xiurui Zhu <zxr6@163.com>",
    "author": "Xiurui Zhu [aut, cre]",
    "url": "https://github.com/zhuxr11/hypergeo2",
    "bug_reports": "https://github.com/zhuxr11/hypergeo2/issues",
    "repository": "https://cran.r-project.org/package=hypergeo2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hypergeo2 Generalized Hypergeometric Function with Tunable High Precision Computation of generalized hypergeometric function with tunable high precision in a vectorized manner, with the floating-point datatypes from 'mpfr' or 'gmp' library. The computation is limited to real numbers.  "
  },
  {
    "id": 14150,
    "package_name": "hyperoverlap",
    "title": "Overlap Detection in n-Dimensional Space",
    "description": "Uses support vector machines to identify a perfectly separating hyperplane (linear or curvilinear) between two entities in high-dimensional space. If this plane exists, the entities do not overlap. Applications include overlap detection in morphological, resource or environmental dimensions. More details can be found in: Brown et al. (2020) <doi:10.1111/2041-210X.13363> .",
    "version": "1.1.3",
    "maintainer": "Matilda Brown <m.brown2@kew.org>",
    "author": "Matilda Brown [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2536-8365>),\n  Greg Jordan [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hyperoverlap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hyperoverlap Overlap Detection in n-Dimensional Space Uses support vector machines to identify a perfectly separating hyperplane (linear or curvilinear) between two entities in high-dimensional space. If this plane exists, the entities do not overlap. Applications include overlap detection in morphological, resource or environmental dimensions. More details can be found in: Brown et al. (2020) <doi:10.1111/2041-210X.13363> .  "
  },
  {
    "id": 14151,
    "package_name": "hypervolume",
    "title": "High Dimensional Geometry, Set Operations, Projection, and\nInference Using Kernel Density Estimation, Support Vector\nMachines, and Convex Hulls",
    "description": "Estimates the shape and volume of high-dimensional datasets and performs set operations: intersection / overlap, union, unique components, inclusion test, and hole detection. Uses stochastic geometry approach to high-dimensional kernel density estimation, support vector machine delineation, and convex hull generation. Applications include modeling trait and niche hypervolumes and species distribution modeling.",
    "version": "3.1.6",
    "maintainer": "Benjamin Blonder <benjamin.blonder@berkeley.edu>",
    "author": "Benjamin Blonder [aut, cre],\n  Cecina Babich Morrow [aut],\n  Stuart Brown [aut],\n  Gregoire Butruille [aut],\n  Daniel Chen [aut],\n  Alex Laini [aut],\n  David J. Harris [aut],\n  Clement Violet [aut]",
    "url": "https://github.com/bblonder/hypervolume",
    "bug_reports": "https://github.com/bblonder/hypervolume/issues",
    "repository": "https://cran.r-project.org/package=hypervolume",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hypervolume High Dimensional Geometry, Set Operations, Projection, and\nInference Using Kernel Density Estimation, Support Vector\nMachines, and Convex Hulls Estimates the shape and volume of high-dimensional datasets and performs set operations: intersection / overlap, union, unique components, inclusion test, and hole detection. Uses stochastic geometry approach to high-dimensional kernel density estimation, support vector machine delineation, and convex hull generation. Applications include modeling trait and niche hypervolumes and species distribution modeling.  "
  },
  {
    "id": 14198,
    "package_name": "iRfcb",
    "title": "Tools for Managing Imaging FlowCytobot (IFCB) Data",
    "description": "A comprehensive suite of tools for managing, processing, and\n    analyzing data from the IFCB. I R FlowCytobot ('iRfcb') supports\n    quality control, geospatial analysis, and preparation of IFCB data for\n    publication in databases like <https://www.gbif.org>,\n    <https://www.obis.org>, <https://emodnet.ec.europa.eu/en>,\n    <https://shark.smhi.se/>, and <https://www.ecotaxa.org>. The package\n    integrates with the MATLAB 'ifcb-analysis' tool, which is described in\n    Sosik and Olson (2007) <doi:10.4319/lom.2007.5.204>, and provides\n    features for working with raw, manually classified, and machine\n    learning\u2013classified image datasets. Key functionalities include image\n    extraction, particle size distribution analysis, taxonomic data\n    handling, and biomass concentration calculations, essential for\n    plankton research.",
    "version": "0.6.0",
    "maintainer": "Anders Torstensson <anders.torstensson@smhi.se>",
    "author": "Anders Torstensson [aut, cre] (Swedish Meteorological and Hydrological\n    Institute, ORCID: <https://orcid.org/0000-0002-8283-656X>),\n  Kendra Hayashi [ctb] (ORCID: <https://orcid.org/0000-0003-1600-9504>),\n  Jamie Enslein [ctb],\n  Raphael Kudela [ctb] (ORCID: <https://orcid.org/0000-0002-8640-1205>),\n  Alle Lie [ctb] (ORCID: <https://orcid.org/0009-0001-8709-4841>),\n  Jayme Smith [ctb] (ORCID: <https://orcid.org/0000-0002-9669-4427>),\n  DTO-BioFlow [fnd] (Horizon Europe, HORIZON-MISS-2022-OCEAN-01-07),\n  SBDI [fnd] (Swedish Research Council, 2019-00242)",
    "url": "https://europeanifcbgroup.github.io/iRfcb/,\nhttps://github.com/EuropeanIFCBGroup/iRfcb",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iRfcb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iRfcb Tools for Managing Imaging FlowCytobot (IFCB) Data A comprehensive suite of tools for managing, processing, and\n    analyzing data from the IFCB. I R FlowCytobot ('iRfcb') supports\n    quality control, geospatial analysis, and preparation of IFCB data for\n    publication in databases like <https://www.gbif.org>,\n    <https://www.obis.org>, <https://emodnet.ec.europa.eu/en>,\n    <https://shark.smhi.se/>, and <https://www.ecotaxa.org>. The package\n    integrates with the MATLAB 'ifcb-analysis' tool, which is described in\n    Sosik and Olson (2007) <doi:10.4319/lom.2007.5.204>, and provides\n    features for working with raw, manually classified, and machine\n    learning\u2013classified image datasets. Key functionalities include image\n    extraction, particle size distribution analysis, taxonomic data\n    handling, and biomass concentration calculations, essential for\n    plankton research.  "
  },
  {
    "id": 14262,
    "package_name": "icosa",
    "title": "Global Triangular and Penta-Hexagonal Grids Based on Tessellated\nIcosahedra",
    "description": "Implementation of icosahedral grids in three dimensions. The spherical-triangular tessellation can be set to create grids with custom resolutions. Both the primary triangular and their inverted penta-hexagonal grids can be calculated. Additional functions are provided that allow plotting of the grids and associated data, the interaction of the grids with other raster and vector objects, and treating the grids as a graphs.",
    "version": "0.12.0",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "author": "Adam T. Kocsis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9028-665X>),\n  Deutsche Forschungsgemeinschaft [fnd],\n  FAU GeoZentrum Nordbayern [fnd]",
    "url": "https://icosa-grid.github.io/R-icosa/",
    "bug_reports": "https://github.com/icosa-grid/R-icosa/issues",
    "repository": "https://cran.r-project.org/package=icosa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icosa Global Triangular and Penta-Hexagonal Grids Based on Tessellated\nIcosahedra Implementation of icosahedral grids in three dimensions. The spherical-triangular tessellation can be set to create grids with custom resolutions. Both the primary triangular and their inverted penta-hexagonal grids can be calculated. Additional functions are provided that allow plotting of the grids and associated data, the interaction of the grids with other raster and vector objects, and treating the grids as a graphs.  "
  },
  {
    "id": 14274,
    "package_name": "ider",
    "title": "Various Methods for Estimating Intrinsic Dimension",
    "description": "An implementation of various methods for estimating intrinsic\n    dimension of vector-valued dataset or distance matrix. Most methods implemented\n    are based on different notion of fractal dimension such as the capacity\n    dimension, the box-counting dimension, and the information dimension.",
    "version": "0.1.1",
    "maintainer": "Hideitsu Hino <hideitsu.hino@gmail.com>",
    "author": "Hideitsu Hino",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ider",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ider Various Methods for Estimating Intrinsic Dimension An implementation of various methods for estimating intrinsic\n    dimension of vector-valued dataset or distance matrix. Most methods implemented\n    are based on different notion of fractal dimension such as the capacity\n    dimension, the box-counting dimension, and the information dimension.  "
  },
  {
    "id": 14284,
    "package_name": "idx2r",
    "title": "Convert Files to and from IDX Format to Vectors, Matrices and\nArrays",
    "description": "Convert files to and from IDX format to vectors, matrices and arrays.\n    IDX is a very simple file format designed for storing vectors and multidimensional matrices in\n    binary format. The format is described on the website from Yann LeCun\n    <http://yann.lecun.com/exdb/mnist/>. ",
    "version": "1.0.0",
    "maintainer": "Erik Doffagne <erik.doffagne@gmail.com>",
    "author": "Erik Doffagne",
    "url": "https://github.com/edoffagne/idx2r",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=idx2r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "idx2r Convert Files to and from IDX Format to Vectors, Matrices and\nArrays Convert files to and from IDX format to vectors, matrices and arrays.\n    IDX is a very simple file format designed for storing vectors and multidimensional matrices in\n    binary format. The format is described on the website from Yann LeCun\n    <http://yann.lecun.com/exdb/mnist/>.   "
  },
  {
    "id": 14285,
    "package_name": "ie2misc",
    "title": "Irucka Embry's Miscellaneous USGS Functions",
    "description": "A collection of Irucka Embry's miscellaneous USGS functions\n    (processing .exp and .psf files, statistical error functions,\n    \"+\" dyadic operator for use with NA, creating ADAPS and QW\n    spreadsheet files, calculating saturated enthalpy). Irucka created these\n    functions while a Cherokee Nation Technology Solutions (CNTS) United States\n    Geological Survey (USGS) Contractor and/or USGS employee.",
    "version": "0.9.2",
    "maintainer": "Irucka Embry <iembry@ecoccs.com>",
    "author": "Irucka Embry [aut, cre],\n  Anne Hoos [ctb],\n  Timothy H. Diehl [ctb]",
    "url": "https://gitlab.com/iembry/ie2misc",
    "bug_reports": "https://gitlab.com/iembry/ie2misc/-/issues",
    "repository": "https://cran.r-project.org/package=ie2misc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ie2misc Irucka Embry's Miscellaneous USGS Functions A collection of Irucka Embry's miscellaneous USGS functions\n    (processing .exp and .psf files, statistical error functions,\n    \"+\" dyadic operator for use with NA, creating ADAPS and QW\n    spreadsheet files, calculating saturated enthalpy). Irucka created these\n    functions while a Cherokee Nation Technology Solutions (CNTS) United States\n    Geological Survey (USGS) Contractor and/or USGS employee.  "
  },
  {
    "id": 14331,
    "package_name": "imageRy",
    "title": "Modify and Share Images",
    "description": "Tools for manipulating, visualizing, and exporting raster images in R.\n Designed as an educational resource for students learning the basics of remote sensing,\n the package provides user-friendly functions to apply color ramps, export RGB composites,\n and create multi-frame visualizations. Built on top of the 'terra' and 'ggplot2' packages.\n See <https://github.com/ducciorocchini/imageRy> for more details and examples.",
    "version": "0.3.0",
    "maintainer": "Ludovico Chieffallo <ludovico.chieffallo2@unibo.it>",
    "author": "Duccio Rocchini [aut],\n  Ludovico Chieffallo [aut, cre],\n  Giovanni Andrea Nocera [ctb],\n  Giacomo Panza [ctb],\n  Michele Torresani [aut],\n  Elisa Thouverai [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=imageRy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imageRy Modify and Share Images Tools for manipulating, visualizing, and exporting raster images in R.\n Designed as an educational resource for students learning the basics of remote sensing,\n the package provides user-friendly functions to apply color ramps, export RGB composites,\n and create multi-frame visualizations. Built on top of the 'terra' and 'ggplot2' packages.\n See <https://github.com/ducciorocchini/imageRy> for more details and examples.  "
  },
  {
    "id": 14390,
    "package_name": "incase",
    "title": "Pipe-Friendly Vector Replacement with Case Statements",
    "description": "Offers a pipe-friendly alternative to the 'dplyr' functions\n    case_when() and if_else(), as well as a number of user-friendly\n    simplifications for common use cases.  These functions accept a vector\n    as an optional first argument, allowing conditional statements to be\n    built using the 'magrittr' dot operator.  The functions also coerce\n    all outputs to the same type, meaning you no longer have to worry\n    about using specific typed variants of NA or explicitly declaring\n    integer outputs, and evaluate outputs somewhat lazily, so you don't\n    waste time on long operations that won't be used.",
    "version": "0.4.0",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>),\n  Patrice Kiener [ctb] (Contributed example for fn_case(), ORCID:\n    <https://orcid.org/0000-0002-0505-9920>)",
    "url": "https://pkg.rossellhayes.com/incase/,\nhttps://github.com/rossellhayes/incase",
    "bug_reports": "https://github.com/rossellhayes/incase/issues",
    "repository": "https://cran.r-project.org/package=incase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "incase Pipe-Friendly Vector Replacement with Case Statements Offers a pipe-friendly alternative to the 'dplyr' functions\n    case_when() and if_else(), as well as a number of user-friendly\n    simplifications for common use cases.  These functions accept a vector\n    as an optional first argument, allowing conditional statements to be\n    built using the 'magrittr' dot operator.  The functions also coerce\n    all outputs to the same type, meaning you no longer have to worry\n    about using specific typed variants of NA or explicitly declaring\n    integer outputs, and evaluate outputs somewhat lazily, so you don't\n    waste time on long operations that won't be used.  "
  },
  {
    "id": 14398,
    "package_name": "independenceWeights",
    "title": "Estimates Weights for Confounding Control for Continuous-Valued\nExposures",
    "description": "Estimates weights to make a continuous-valued exposure statistically independent of a vector of pre-treatment covariates using the method proposed in Huling, Greifer, and Chen (2021) <arxiv:2107.07086>. ",
    "version": "0.0.1",
    "maintainer": "Jared Huling <jaredhuling@gmail.com>",
    "author": "Jared Huling [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0670-4845>),\n  Noah Greifer [aut] (ORCID: <https://orcid.org/0000-0003-3067-7154>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=independenceWeights",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "independenceWeights Estimates Weights for Confounding Control for Continuous-Valued\nExposures Estimates weights to make a continuous-valued exposure statistically independent of a vector of pre-treatment covariates using the method proposed in Huling, Greifer, and Chen (2021) <arxiv:2107.07086>.   "
  },
  {
    "id": 14401,
    "package_name": "indexthis",
    "title": "Quick Indexation",
    "description": "Quick indexation of any type of vector or of any combination of those. Indexation turns a vector into an integer vector going from 1 to the number of unique elements. Indexes are important building blocks for many algorithms. The method is described at <https://github.com/lrberge/indexthis/>.",
    "version": "2.1.0",
    "maintainer": "Laurent Berge <laurent.berge@u-bordeaux.fr>",
    "author": "Laurent Berge [aut, cre],\n  Sebastian Krantz [ctb],\n  Morgan Jacob [ctb]",
    "url": "https://github.com/lrberge/indexthis",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=indexthis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "indexthis Quick Indexation Quick indexation of any type of vector or of any combination of those. Indexation turns a vector into an integer vector going from 1 to the number of unique elements. Indexes are important building blocks for many algorithms. The method is described at <https://github.com/lrberge/indexthis/>.  "
  },
  {
    "id": 14449,
    "package_name": "inops",
    "title": "Infix Operators for Detection, Subsetting and Replacement",
    "description": "Infix operators to detect, subset, and replace the elements matched by a given condition.\n  The functions have several variants of operator types, including subsets, ranges, regular expressions and others.\n  Implemented operators work on vectors, matrices, and lists.",
    "version": "0.0.1",
    "maintainer": "Antoine Fabri <antoine.fabri@gmail.com>",
    "author": "Antoine Fabri [aut, cre],\n  Karolis Koncevi\u010dius [aut]",
    "url": "https://github.com/moodymudskipper/inops",
    "bug_reports": "https://github.com/moodymudskipper/inops/issues",
    "repository": "https://cran.r-project.org/package=inops",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inops Infix Operators for Detection, Subsetting and Replacement Infix operators to detect, subset, and replace the elements matched by a given condition.\n  The functions have several variants of operator types, including subsets, ranges, regular expressions and others.\n  Implemented operators work on vectors, matrices, and lists.  "
  },
  {
    "id": 14451,
    "package_name": "inphr",
    "title": "Statistical Inference for Persistence Homology Data",
    "description": "A set of functions for performing null hypothesis testing on\n    samples of persistence diagrams using the theory of permutations. Currently,\n    only two-sample testing is implemented. Inputs can be either samples of\n    persistence diagrams themselves or vectorizations. In the former case, they\n    are embedded in a metric space using either the Bottleneck or Wasserstein\n    distance. In the former case, persistence data becomes functional data and\n    inference is performed using tools available in the 'fdatest' package. Main\n    reference for the interval-wise testing method: Pini A., Vantini S. (2017)\n    \"Interval-wise testing for functional data\" <doi:10.1080/10485252.2017.1306627>.\n    Main reference for inference on populations of networks: Lovato, I., Pini, A.,\n    Stamm, A., & Vantini, S. (2020) \"Model-free two-sample test for network-valued\n    data\" <doi:10.1016/j.csda.2019.106896>.",
    "version": "0.0.1",
    "maintainer": "Aymeric Stamm <aymeric.stamm@cnrs.fr>",
    "author": "Aymeric Stamm [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8725-3654>)",
    "url": "https://github.com/tdaverse/inphr,\nhttps://tdaverse.github.io/inphr/",
    "bug_reports": "https://github.com/tdaverse/inphr/issues",
    "repository": "https://cran.r-project.org/package=inphr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inphr Statistical Inference for Persistence Homology Data A set of functions for performing null hypothesis testing on\n    samples of persistence diagrams using the theory of permutations. Currently,\n    only two-sample testing is implemented. Inputs can be either samples of\n    persistence diagrams themselves or vectorizations. In the former case, they\n    are embedded in a metric space using either the Bottleneck or Wasserstein\n    distance. In the former case, persistence data becomes functional data and\n    inference is performed using tools available in the 'fdatest' package. Main\n    reference for the interval-wise testing method: Pini A., Vantini S. (2017)\n    \"Interval-wise testing for functional data\" <doi:10.1080/10485252.2017.1306627>.\n    Main reference for inference on populations of networks: Lovato, I., Pini, A.,\n    Stamm, A., & Vantini, S. (2020) \"Model-free two-sample test for network-valued\n    data\" <doi:10.1016/j.csda.2019.106896>.  "
  },
  {
    "id": 14452,
    "package_name": "inplace",
    "title": "In-place Operators for R",
    "description": "It provides in-place operators for R \n    that are equivalent to '+=', '-=', '*=', '/=' in C++. \n    Those can be applied on integer|double vectors|matrices.\n    You have also access to sweep operations (in-place).",
    "version": "0.1.2",
    "maintainer": "Florian Priv\u00e9 <florian.prive.21@gmail.com>",
    "author": "Florian Priv\u00e9 [aut, cre]",
    "url": "https://github.com/privefl/inplace",
    "bug_reports": "https://github.com/privefl/inplace/issues",
    "repository": "https://cran.r-project.org/package=inplace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inplace In-place Operators for R It provides in-place operators for R \n    that are equivalent to '+=', '-=', '*=', '/=' in C++. \n    Those can be applied on integer|double vectors|matrices.\n    You have also access to sweep operations (in-place).  "
  },
  {
    "id": 14470,
    "package_name": "intamap",
    "title": "Procedures for Automated Interpolation",
    "description": "Geostatistical interpolation has traditionally been done by manually fitting a variogram and then interpolating. Here, we introduce classes and methods that can do this interpolation automatically. Pebesma et al (2010) gives an overview of the methods behind and possible usage <doi:10.1016/j.cageo.2010.03.019>.",
    "version": "1.5-11",
    "maintainer": "Jon Olav Skoien <jon.skoien@gmail.com>",
    "author": "Edzer Pebesma [aut],\n  Jon Olav Skoien [aut, cre],\n  Olivier Baume [ctb],\n  A Chorti [ctb],\n  Dionisis Hristopulos [ctb],\n  Hannes Kazianka [ctb],\n  Stepahnie Melles [ctb],\n  Giannis Spiliopoulos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intamap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intamap Procedures for Automated Interpolation Geostatistical interpolation has traditionally been done by manually fitting a variogram and then interpolating. Here, we introduce classes and methods that can do this interpolation automatically. Pebesma et al (2010) gives an overview of the methods behind and possible usage <doi:10.1016/j.cageo.2010.03.019>.  "
  },
  {
    "id": 14480,
    "package_name": "inteq",
    "title": "Numerical Solution of Integral Equations",
    "description": "An R implementation of Matthew Thomas's 'Python' library 'inteq'. First, this solves Fredholm integral equations of the first kind ($f(s) = \\int_a^b K(s, y) g(y) dy$) using methods described by Twomey (1963) <doi:10.1145/321150.321157>. Second, this solves Volterra integral equations of the first kind ($f(s) = \\int_0^s K(s,y) g(t) dt$) using methods from Betto and Thomas (2021) <doi:10.48550/arXiv.2106.08496>. Third, this solves Voltera integral equations of the second kind ($g(s) = f(s) + \\int_a^s K(s,y) g(y) dy$) using methods from Linz (1969) <doi:10.1137/0706034>.",
    "version": "1.0",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "author": "Mark Clements [aut, cre],\n  Aaron Jehle [aut],\n  Matthew Thomas [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=inteq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inteq Numerical Solution of Integral Equations An R implementation of Matthew Thomas's 'Python' library 'inteq'. First, this solves Fredholm integral equations of the first kind ($f(s) = \\int_a^b K(s, y) g(y) dy$) using methods described by Twomey (1963) <doi:10.1145/321150.321157>. Second, this solves Volterra integral equations of the first kind ($f(s) = \\int_0^s K(s,y) g(t) dt$) using methods from Betto and Thomas (2021) <doi:10.48550/arXiv.2106.08496>. Third, this solves Voltera integral equations of the second kind ($g(s) = f(s) + \\int_a^s K(s,y) g(y) dy$) using methods from Linz (1969) <doi:10.1137/0706034>.  "
  },
  {
    "id": 14491,
    "package_name": "interleave",
    "title": "Converts Tabular Data to Interleaved Vectors",
    "description": "Converts matrices and lists of matrices into a single vector by interleaving \n    their values. That is, each element of the result vector is filled from the input \n    matrices one row at a time. This is the same as transposing a matrix, then removing the \n    dimension attribute, but is designed to operate on matrices in nested list structures.",
    "version": "0.1.2",
    "maintainer": "David Cooley <david.cooley.au@gmail.com>",
    "author": "David Cooley [aut, cre],\n  Mapbox [cph] (author of header library earcut.hpp)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=interleave",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interleave Converts Tabular Data to Interleaved Vectors Converts matrices and lists of matrices into a single vector by interleaving \n    their values. That is, each element of the result vector is filled from the input \n    matrices one row at a time. This is the same as transposing a matrix, then removing the \n    dimension attribute, but is designed to operate on matrices in nested list structures.  "
  },
  {
    "id": 14496,
    "package_name": "interpolation",
    "title": "Interpolation of Bivariate Functions",
    "description": "Provides two different methods, linear and nonlinear, to\n    interpolate a bivariate function, scalar-valued or vector-valued. \n    The interpolated data are not necessarily gridded. The algorithms \n    are performed by the 'C++' library 'CGAL' (<https://www.cgal.org/>).",
    "version": "0.1.1",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre]",
    "url": "https://github.com/stla/interpolation",
    "bug_reports": "https://github.com/stla/interpolation/issues",
    "repository": "https://cran.r-project.org/package=interpolation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interpolation Interpolation of Bivariate Functions Provides two different methods, linear and nonlinear, to\n    interpolate a bivariate function, scalar-valued or vector-valued. \n    The interpolated data are not necessarily gridded. The algorithms \n    are performed by the 'C++' library 'CGAL' (<https://www.cgal.org/>).  "
  },
  {
    "id": 14508,
    "package_name": "intkrige",
    "title": "A Numerical Implementation of Interval-Valued Kriging",
    "description": "An interval-valued extension of ordinary and simple kriging.\n    Optimization of the function is based on a generalized interval distance.\n    This creates a non-differentiable cost function that requires a\n    differentiable approximation to the absolute value function. This\n    differentiable approximation is optimized using a Newton-Raphson algorithm\n    with a penalty function to impose the constraints. Analyses in the package\n    are driven by the 'intsp' and 'intgrd' \n    classes, \n    which are interval-valued extensions of\n    'SpatialPointsDataFrame' and 'SpatialPixelsDataFrame' respectively. \n    The package includes several wrappers to functions in the \n    'gstat' and 'sp' packages.",
    "version": "1.0.2",
    "maintainer": "Brennan Bean <brennan.bean@usu.edu>",
    "author": "Brennan Bean [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intkrige",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intkrige A Numerical Implementation of Interval-Valued Kriging An interval-valued extension of ordinary and simple kriging.\n    Optimization of the function is based on a generalized interval distance.\n    This creates a non-differentiable cost function that requires a\n    differentiable approximation to the absolute value function. This\n    differentiable approximation is optimized using a Newton-Raphson algorithm\n    with a penalty function to impose the constraints. Analyses in the package\n    are driven by the 'intsp' and 'intgrd' \n    classes, \n    which are interval-valued extensions of\n    'SpatialPointsDataFrame' and 'SpatialPixelsDataFrame' respectively. \n    The package includes several wrappers to functions in the \n    'gstat' and 'sp' packages.  "
  },
  {
    "id": 14515,
    "package_name": "intrval",
    "title": "Relational Operators for Intervals",
    "description": "Evaluating if values\n  of vectors are within different open/closed intervals\n  (`x %[]% c(a, b)`), or if two closed\n  intervals overlap (`c(a1, b1) %[]o[]% c(a2, b2)`).\n  Operators for negation and directional relations also implemented.",
    "version": "1.0-0",
    "maintainer": "Peter Solymos <psolymos@gmail.com>",
    "author": "Peter Solymos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7337-1740>)",
    "url": "https://github.com/psolymos/intrval",
    "bug_reports": "https://github.com/psolymos/intrval/issues",
    "repository": "https://cran.r-project.org/package=intrval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intrval Relational Operators for Intervals Evaluating if values\n  of vectors are within different open/closed intervals\n  (`x %[]% c(a, b)`), or if two closed\n  intervals overlap (`c(a1, b1) %[]o[]% c(a2, b2)`).\n  Operators for negation and directional relations also implemented.  "
  },
  {
    "id": 14518,
    "package_name": "inum",
    "title": "Interval and Enum-Type Representation of Vectors",
    "description": "Enum-type representation of vectors and representation\n  of intervals, including a method of coercing variables in data frames.",
    "version": "1.0-5",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "author": "Torsten Hothorn [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=inum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inum Interval and Enum-Type Representation of Vectors Enum-type representation of vectors and representation\n  of intervals, including a method of coercing variables in data frames.  "
  },
  {
    "id": 14521,
    "package_name": "invctr",
    "title": "Infix Functions For Vector Operations",
    "description": "Vector operations between grapes: An infix-only package! The 'invctr' functions perform common and less common operations on vectors, data frames matrices and list objects:\n    - Extracting a value (range), or, finding the indices of a value (range).\n    - Trimming, or padding a vector with a value of your choice.\n    - Simple polynomial regression.\n    - Set and membership operations.\n    - General check & replace function for NAs, Inf and other values.",
    "version": "0.2.0",
    "maintainer": "Fred Hasselman <fred.hasselman@ru.nl>",
    "author": "Fred Hasselman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1384-8361>)",
    "url": "https://github.com/FredHasselman/invctr",
    "bug_reports": "https://github.com/FredHasselman/invctr/issues",
    "repository": "https://cran.r-project.org/package=invctr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "invctr Infix Functions For Vector Operations Vector operations between grapes: An infix-only package! The 'invctr' functions perform common and less common operations on vectors, data frames matrices and list objects:\n    - Extracting a value (range), or, finding the indices of a value (range).\n    - Trimming, or padding a vector with a value of your choice.\n    - Simple polynomial regression.\n    - Set and membership operations.\n    - General check & replace function for NAs, Inf and other values.  "
  },
  {
    "id": 14523,
    "package_name": "inverseRegex",
    "title": "Reverse Engineers Regular Expression Patterns for R Objects",
    "description": "Reverse engineer a regular expression pattern for the characters\n    contained in an R object. Individual characters can be categorised into\n    digits, letters, punctuation or spaces and encoded into run-lengths. This\n    can be used to summarise the structure of a dataset or identify non-standard\n    entries. Many non-character inputs such as numeric vectors and data frames\n    are supported.",
    "version": "0.2.0",
    "maintainer": "Jasper Watson <jasper.g.watson@gmail.com>",
    "author": "Jasper Watson [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/rntq472/inverseRegex/issues",
    "repository": "https://cran.r-project.org/package=inverseRegex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inverseRegex Reverse Engineers Regular Expression Patterns for R Objects Reverse engineer a regular expression pattern for the characters\n    contained in an R object. Individual characters can be categorised into\n    digits, letters, punctuation or spaces and encoded into run-lengths. This\n    can be used to summarise the structure of a dataset or identify non-standard\n    entries. Many non-character inputs such as numeric vectors and data frames\n    are supported.  "
  },
  {
    "id": 14541,
    "package_name": "ip2location.io",
    "title": "Batch IP Data Retrieval and Storage Using 'IP2Location.io'",
    "description": "A system for submitting multiple IP information queries to 'IP2Location.io'\u2019s IP Geolocation API and storing the resulting data in a dataframe. You provide a vector of IP addresses and your 'IP2Location.io' API key. The package returns a dataframe with one row per IP address and a column for each available data field (data fields not included in your API plan will contain NAs). This is the second submission of the package to CRAN.",
    "version": "0.0.0-2",
    "maintainer": "Oriane Georgeac <oriane.georgeac@gmail.com>",
    "author": "Oriane Georgeac [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6531-0075>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ip2location.io",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ip2location.io Batch IP Data Retrieval and Storage Using 'IP2Location.io' A system for submitting multiple IP information queries to 'IP2Location.io'\u2019s IP Geolocation API and storing the resulting data in a dataframe. You provide a vector of IP addresses and your 'IP2Location.io' API key. The package returns a dataframe with one row per IP address and a column for each available data field (data fields not included in your API plan will contain NAs). This is the second submission of the package to CRAN.  "
  },
  {
    "id": 14546,
    "package_name": "ipa",
    "title": "Convert Between Phonetic Alphabets",
    "description": "Converts character vectors between phonetic\n    representations.  Supports IPA (International Phonetic Alphabet),\n    X-SAMPA (Extended Speech Assessment Methods Phonetic Alphabet), and\n    ARPABET (used by the CMU Pronouncing Dictionary).",
    "version": "0.1.0",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>)",
    "url": "https://github.com/rossellhayes/ipa",
    "bug_reports": "https://github.com/rossellhayes/ipa/issues",
    "repository": "https://cran.r-project.org/package=ipa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ipa Convert Between Phonetic Alphabets Converts character vectors between phonetic\n    representations.  Supports IPA (International Phonetic Alphabet),\n    X-SAMPA (Extended Speech Assessment Methods Phonetic Alphabet), and\n    ARPABET (used by the CMU Pronouncing Dictionary).  "
  },
  {
    "id": 14584,
    "package_name": "irr",
    "title": "Various Coefficients of Interrater Reliability and Agreement",
    "description": "Coefficients of Interrater Reliability and Agreement for\n        quantitative, ordinal and nominal data: ICC, Finn-Coefficient,\n        Robinson's A, Kendall's W, Cohen's Kappa, ...",
    "version": "0.84.1",
    "maintainer": "Matthias Gamer <m.gamer@uke.uni-hamburg.de>",
    "author": "Matthias Gamer <m.gamer@uke.uni-hamburg.de>, Jim Lemon\n        <jim@bitwrit.com.au>, Ian Fellows <ifellows@uscd.edu> Puspendra\n        Singh <puspendra.pusp22@gmail.com>",
    "url": "https://www.r-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=irr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "irr Various Coefficients of Interrater Reliability and Agreement Coefficients of Interrater Reliability and Agreement for\n        quantitative, ordinal and nominal data: ICC, Finn-Coefficient,\n        Robinson's A, Kendall's W, Cohen's Kappa, ...  "
  },
  {
    "id": 14587,
    "package_name": "irrNA",
    "title": "Coefficients of Interrater Reliability \u2013 Generalized for\nRandomly Incomplete Datasets",
    "description": "Provides coefficients of interrater reliability that are generalized to cope with randomly incomplete (i.e. unbalanced) datasets without any imputation of missing values or any (row-wise or column-wise) omissions of actually available data. Applied to complete (balanced) datasets, these generalizations yield the same results as the common procedures, namely the Intraclass Correlation according to McGraw & Wong (1996) \\doi{10.1037/1082-989X.1.1.30} and the Coefficient of Concordance according to Kendall & Babington Smith (1939) \\doi{10.1214/aoms/1177732186}.",
    "version": "0.2.3",
    "maintainer": "Markus Brueckl <markus.brueckl@tu-berlin.de>",
    "author": "Markus Brueckl [aut, cre], Florian Heuer [aut, trl]",
    "url": "https://CRAN.R-project.org/package=irrNA",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=irrNA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "irrNA Coefficients of Interrater Reliability \u2013 Generalized for\nRandomly Incomplete Datasets Provides coefficients of interrater reliability that are generalized to cope with randomly incomplete (i.e. unbalanced) datasets without any imputation of missing values or any (row-wise or column-wise) omissions of actually available data. Applied to complete (balanced) datasets, these generalizations yield the same results as the common procedures, namely the Intraclass Correlation according to McGraw & Wong (1996) \\doi{10.1037/1082-989X.1.1.30} and the Coefficient of Concordance according to Kendall & Babington Smith (1939) \\doi{10.1214/aoms/1177732186}.  "
  },
  {
    "id": 14634,
    "package_name": "itcSegment",
    "title": "Individual Tree Crowns Segmentation",
    "description": "Three methods for Individual Tree Crowns (ITCs) delineation on remote sensing data: one is based on LiDAR data in x,y,z format and one on imagery data in raster format.",
    "version": "1.0",
    "maintainer": "Michele Dalponte <michele.dalponte@fmach.it>",
    "author": "Michele Dalponte",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=itcSegment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "itcSegment Individual Tree Crowns Segmentation Three methods for Individual Tree Crowns (ITCs) delineation on remote sensing data: one is based on LiDAR data in x,y,z format and one on imagery data in raster format.  "
  },
  {
    "id": 14639,
    "package_name": "iterators",
    "title": "Provides Iterator Construct",
    "description": "Support for iterators, which allow a programmer to traverse\n    through all the elements of a vector, list, or other collection of data.",
    "version": "1.0.14",
    "maintainer": "Folashade Daniel <fdaniel@microsoft.com>",
    "author": "Folashade Daniel [cre],\n  Revolution Analytics [aut, cph],\n  Steve Weston [aut]",
    "url": "https://github.com/RevolutionAnalytics/iterators",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iterators",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iterators Provides Iterator Construct Support for iterators, which allow a programmer to traverse\n    through all the elements of a vector, list, or other collection of data.  "
  },
  {
    "id": 14655,
    "package_name": "ivaBSS",
    "title": "Tools for Independent Vector Analysis",
    "description": "Independent vector analysis (IVA) is a blind source separation (BSS) model where several datasets are jointly unmixed. This package provides several methods for the unmixing together with some performance measures. For details, see Anderson et al. (2011) <doi:10.1109/TSP.2011.2181836> and Lee et al. (2007) <doi:10.1016/j.sigpro.2007.01.010>.",
    "version": "1.0.0",
    "maintainer": "Mika Sipil\u00e4 <mika.e.sipila@student.jyu.fi>",
    "author": "Mika Sipil\u00e4 [aut, cre],\n  Klaus Nordhausen [aut] (ORCID: <https://orcid.org/0000-0002-3758-8501>),\n  Sara Taskinen [aut] (ORCID: <https://orcid.org/0000-0001-9470-7258>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ivaBSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ivaBSS Tools for Independent Vector Analysis Independent vector analysis (IVA) is a blind source separation (BSS) model where several datasets are jointly unmixed. This package provides several methods for the unmixing together with some performance measures. For details, see Anderson et al. (2011) <doi:10.1109/TSP.2011.2181836> and Lee et al. (2007) <doi:10.1016/j.sigpro.2007.01.010>.  "
  },
  {
    "id": 14667,
    "package_name": "ivs",
    "title": "Interval Vectors",
    "description": "Provides a library for generic interval manipulations using a\n    new interval vector class. Capabilities include: locating various\n    kinds of relationships between two interval vectors, merging overlaps\n    within a single interval vector, splitting an interval vector on its\n    overlapping endpoints, and applying set theoretical operations on\n    interval vectors. Many of the operations in this package were inspired\n    by James Allen's interval algebra, Allen (1983)\n    <doi:10.1145/182.358434>.",
    "version": "0.2.0",
    "maintainer": "Davis Vaughan <davis@posit.co>",
    "author": "Davis Vaughan [aut, cre],\n  Posit Software, PBC [cph, fnd]",
    "url": "https://github.com/DavisVaughan/ivs,\nhttps://davisvaughan.github.io/ivs/",
    "bug_reports": "https://github.com/DavisVaughan/ivs/issues",
    "repository": "https://cran.r-project.org/package=ivs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ivs Interval Vectors Provides a library for generic interval manipulations using a\n    new interval vector class. Capabilities include: locating various\n    kinds of relationships between two interval vectors, merging overlaps\n    within a single interval vector, splitting an interval vector on its\n    overlapping endpoints, and applying set theoretical operations on\n    interval vectors. Many of the operations in this package were inspired\n    by James Allen's interval algebra, Allen (1983)\n    <doi:10.1145/182.358434>.  "
  },
  {
    "id": 14692,
    "package_name": "jamba",
    "title": "Just Analysis Methods Base",
    "description": "Just analysis methods ('jam') base functions\n    focused on bioinformatics.\n    Version- and gene-centric alphanumeric sort,\n    unique name and version assignment, colorized console and 'HTML' output,\n    color ramp and palette manipulation,\n    'Rmarkdown' cache import, styled 'Excel' worksheet import and export,\n    interpolated raster output from smooth scatter and image plots,\n    list to delimited vector, efficient list tools.",
    "version": "1.0.4",
    "maintainer": "James M. Ward <jmw86069@gmail.com>",
    "author": "James M. Ward [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9510-2848>)",
    "url": "https://jmw86069.github.io/jamba/",
    "bug_reports": "https://github.com/jmw86069/jamba/issues",
    "repository": "https://cran.r-project.org/package=jamba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jamba Just Analysis Methods Base Just analysis methods ('jam') base functions\n    focused on bioinformatics.\n    Version- and gene-centric alphanumeric sort,\n    unique name and version assignment, colorized console and 'HTML' output,\n    color ramp and palette manipulation,\n    'Rmarkdown' cache import, styled 'Excel' worksheet import and export,\n    interpolated raster output from smooth scatter and image plots,\n    list to delimited vector, efficient list tools.  "
  },
  {
    "id": 14720,
    "package_name": "jmatrix",
    "title": "Read from/Write to Disk Matrices with any Data Type in a Binary\nFormat",
    "description": "A mainly instrumental package meant to allow other packages whose core is written in 'C++' to read, write\n        and manipulate matrices in a binary format so that the memory used for them is no more than strictly needed. Its functionality\n        is already inside 'parallelpam' and 'scellpam', so if you have installed any of these, you do not need to install 'jmatrix'.\n        Using just the needed memory is not always true with 'R' matrices or vectors, since by default they are of double type. Trials\n        like the 'float' package have been done, but to use them you have to coerce a matrix already loaded in 'R' memory to a float matrix,\n        and then you can delete it. The problem comes when your computer has not memory enough to hold the matrix in the first place, so\n        you are forced to load it by chunks. This is the problem this package tries to address (with partial success, but this is a\n        difficult problem since 'R' is not a strictly typed language, which is anyway quite hard to get in an interpreted language).\n\tThis package allows the creation and manipulation of full, sparse and symmetric matrices of any standard data type.",
    "version": "1.5.2",
    "maintainer": "Juan Domingo <Juan.Domingo@uv.es>",
    "author": "Juan Domingo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4728-6256>),\n  Guillermo Ayala [ctb] (ORCID: <https://orcid.org/0000-0002-6231-2865>),\n  Spanish Ministry of Science and Innovation, MCIN/AEI\n    <doi:10.13039/501100011033> [fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jmatrix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jmatrix Read from/Write to Disk Matrices with any Data Type in a Binary\nFormat A mainly instrumental package meant to allow other packages whose core is written in 'C++' to read, write\n        and manipulate matrices in a binary format so that the memory used for them is no more than strictly needed. Its functionality\n        is already inside 'parallelpam' and 'scellpam', so if you have installed any of these, you do not need to install 'jmatrix'.\n        Using just the needed memory is not always true with 'R' matrices or vectors, since by default they are of double type. Trials\n        like the 'float' package have been done, but to use them you have to coerce a matrix already loaded in 'R' memory to a float matrix,\n        and then you can delete it. The problem comes when your computer has not memory enough to hold the matrix in the first place, so\n        you are forced to load it by chunks. This is the problem this package tries to address (with partial success, but this is a\n        difficult problem since 'R' is not a strictly typed language, which is anyway quite hard to get in an interpreted language).\n\tThis package allows the creation and manipulation of full, sparse and symmetric matrices of any standard data type.  "
  },
  {
    "id": 14725,
    "package_name": "jmuOutlier",
    "title": "Permutation Tests for Nonparametric Statistics",
    "description": "Performs a permutation test on the difference between two location parameters, a permutation correlation test, a permutation F-test, the Siegel-Tukey test, a ratio mean deviance test.  Also performs some graphing techniques, such as for confidence intervals, vector addition, and Fourier analysis; and includes functions related to the Laplace (double exponential) and triangular distributions.  Performs power calculations for the binomial test.",
    "version": "2.2",
    "maintainer": "Steven T. Garren <GARRENST@JMU.EDU>",
    "author": "Steven T. Garren",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jmuOutlier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jmuOutlier Permutation Tests for Nonparametric Statistics Performs a permutation test on the difference between two location parameters, a permutation correlation test, a permutation F-test, the Siegel-Tukey test, a ratio mean deviance test.  Also performs some graphing techniques, such as for confidence intervals, vector addition, and Fourier analysis; and includes functions related to the Laplace (double exponential) and triangular distributions.  Performs power calculations for the binomial test.  "
  },
  {
    "id": 14755,
    "package_name": "jpeg",
    "title": "Read and write JPEG images",
    "description": "This package provides an easy and simple way to read, write and display bitmap images stored in the JPEG format. It can read and write both files and in-memory raw vectors.",
    "version": "0.1-11",
    "maintainer": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "author": "Simon Urbanek [aut, cre, cph] (https://urbanek.org, ORCID:\n    <https://orcid.org/0000-0003-2297-1732>)",
    "url": "https://www.rforge.net/jpeg/",
    "bug_reports": "https://github.com/s-u/jpeg/issues",
    "repository": "https://cran.r-project.org/package=jpeg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jpeg Read and write JPEG images This package provides an easy and simple way to read, write and display bitmap images stored in the JPEG format. It can read and write both files and in-memory raw vectors.  "
  },
  {
    "id": 14809,
    "package_name": "kappaSize",
    "title": "Sample Size Estimation Functions for Studies of Interobserver\nAgreement",
    "description": "Contains basic tools for sample size estimation in studies of interobserver/interrater agreement (reliability).  Includes functions for both the power-based and confidence interval-based methods, with binary or multinomial outcomes and two through six raters.",
    "version": "1.2",
    "maintainer": "Michael A Rotondi <mrotondi@yorku.ca>",
    "author": "Michael A Rotondi <mrotondi@yorku.ca>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kappaSize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kappaSize Sample Size Estimation Functions for Studies of Interobserver\nAgreement Contains basic tools for sample size estimation in studies of interobserver/interrater agreement (reliability).  Includes functions for both the power-based and confidence interval-based methods, with binary or multinomial outcomes and two through six raters.  "
  },
  {
    "id": 14833,
    "package_name": "kendallknight",
    "title": "Efficient Implementation of Kendall's Correlation Coefficient\nComputation",
    "description": "The computational complexity of the implemented algorithm for\n    Kendall's correlation is O(n log(n)), which is faster than the base R\n    implementation with a computational complexity of O(n^2). For small vectors\n    (i.e., less than 100 observations), the time difference is negligible.\n    However, for larger vectors, the speed difference can be substantial and the\n    numerical difference is minimal. The references are\n    Knight (1966) <doi:10.2307/2282833>,\n    Abrevaya (1999) <doi:10.1016/S0165-1765(98)00255-9>,\n    Christensen (2005) <doi:10.1007/BF02736122> and\n    Emara (2024) <https://learningcpp.org/>.\n    This implementation is described in\n    Vargas Sepulveda (2025) <doi:10.1371/journal.pone.0326090>.",
    "version": "1.0.1",
    "maintainer": "Mauricio Vargas Sepulveda <m.vargas.sepulveda@gmail.com>",
    "author": "Mauricio Vargas Sepulveda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1017-7574>),\n  Loader Catherine [ctb] (original stirlerr implementations in C (2000)),\n  Ross Ihaka [ctb] (original chebyshev_eval, gammafn and lgammacor\n    implementations in C (1998))",
    "url": "https://pacha.dev/kendallknight/,\nhttps://github.com/pachadotdev/kendallknight",
    "bug_reports": "https://github.com/pachadotdev/kendallknight/issues",
    "repository": "https://cran.r-project.org/package=kendallknight",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kendallknight Efficient Implementation of Kendall's Correlation Coefficient\nComputation The computational complexity of the implemented algorithm for\n    Kendall's correlation is O(n log(n)), which is faster than the base R\n    implementation with a computational complexity of O(n^2). For small vectors\n    (i.e., less than 100 observations), the time difference is negligible.\n    However, for larger vectors, the speed difference can be substantial and the\n    numerical difference is minimal. The references are\n    Knight (1966) <doi:10.2307/2282833>,\n    Abrevaya (1999) <doi:10.1016/S0165-1765(98)00255-9>,\n    Christensen (2005) <doi:10.1007/BF02736122> and\n    Emara (2024) <https://learningcpp.org/>.\n    This implementation is described in\n    Vargas Sepulveda (2025) <doi:10.1371/journal.pone.0326090>.  "
  },
  {
    "id": 14848,
    "package_name": "kernlab",
    "title": "Kernel-Based Machine Learning Lab",
    "description": "Kernel-based machine learning methods for classification,\n        regression, clustering, novelty detection, quantile regression\n        and dimensionality reduction.  Among other methods 'kernlab'\n        includes Support Vector Machines, Spectral Clustering, Kernel\n        PCA, Gaussian Processes and a QP solver.",
    "version": "0.9-33",
    "maintainer": "Alexandros Karatzoglou <alexandros.karatzoglou@gmail.com>",
    "author": "Alexandros Karatzoglou [aut, cre],\n  Alex Smola [aut],\n  Kurt Hornik [aut] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  National ICT Australia (NICTA) [cph],\n  Michael A. Maniscalco [ctb, cph],\n  Choon Hui Teo [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kernlab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kernlab Kernel-Based Machine Learning Lab Kernel-based machine learning methods for classification,\n        regression, clustering, novelty detection, quantile regression\n        and dimensionality reduction.  Among other methods 'kernlab'\n        includes Support Vector Machines, Spectral Clustering, Kernel\n        PCA, Gaussian Processes and a QP solver.  "
  },
  {
    "id": 14853,
    "package_name": "kerntools",
    "title": "Kernel Functions and Tools for Machine Learning Applications",
    "description": "Kernel functions for diverse types of data (including, but not\n    restricted to: nonnegative and real vectors, real matrices, categorical\n    and ordinal variables, sets, strings), plus other utilities like kernel\n    similarity, kernel Principal Components Analysis (PCA) and features'\n    importance for Support Vector Machines (SVMs), which expand other 'R'\n    packages like 'kernlab'.",
    "version": "1.2.0",
    "maintainer": "Elies Ramon <eramon@everlyrusher.com>",
    "author": "Elies Ramon [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7953-8115>)",
    "url": "https://github.com/elies-ramon/kerntools,\nhttps://elies-ramon.github.io/kerntools/",
    "bug_reports": "https://github.com/elies-ramon/kerntools/issues",
    "repository": "https://cran.r-project.org/package=kerntools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kerntools Kernel Functions and Tools for Machine Learning Applications Kernel functions for diverse types of data (including, but not\n    restricted to: nonnegative and real vectors, real matrices, categorical\n    and ordinal variables, sets, strings), plus other utilities like kernel\n    similarity, kernel Principal Components Analysis (PCA) and features'\n    importance for Support Vector Machines (SVMs), which expand other 'R'\n    packages like 'kernlab'.  "
  },
  {
    "id": 14891,
    "package_name": "kissmig",
    "title": "a Keep It Simple Species Migration Model",
    "description": "Simulating species migration and range dynamics under stable or changing environmental conditions based on a simple, raster-based, deterministic or stochastic migration model. KISSMig runs on binary or quantitative suitability maps, which are pre-calculated with niche-based habitat suitability models (also called ecological niche models (ENMs) or species distribution models (SDMs)). Nobis & Normand (2014), <doi:10.1111/ecog.00930>.",
    "version": "2.0-1",
    "maintainer": "Michael P. Nobis <michael.nobis@wsl.ch>",
    "author": "Michael P. Nobis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-3285-1590>),\n  Signe Normand [ctb] (ORCID: <https://orcid.org/0000-0002-8782-4154>),\n  Dominik F. Landolt [ctb]",
    "url": "https://purl.oclc.org/wsl/kissmig",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kissmig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kissmig a Keep It Simple Species Migration Model Simulating species migration and range dynamics under stable or changing environmental conditions based on a simple, raster-based, deterministic or stochastic migration model. KISSMig runs on binary or quantitative suitability maps, which are pre-calculated with niche-based habitat suitability models (also called ecological niche models (ENMs) or species distribution models (SDMs)). Nobis & Normand (2014), <doi:10.1111/ecog.00930>.  "
  },
  {
    "id": 14892,
    "package_name": "kit",
    "title": "Data Manipulation Functions Implemented in C",
    "description": "Basic functions, implemented in C, for large data manipulation. Fast vectorised ifelse()/nested if()/switch() functions, psum()/pprod() functions equivalent to pmin()/pmax() plus others which are missing from base R. Most of these functions are callable at C level.",
    "version": "0.0.20",
    "maintainer": "Morgan Jacob <morgan.emailbox@gmail.com>",
    "author": "Morgan Jacob [aut, cre, cph],\n  Sebastian Krantz [ctb]",
    "url": "",
    "bug_reports": "https://github.com/2005m/kit/issues",
    "repository": "https://cran.r-project.org/package=kit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kit Data Manipulation Functions Implemented in C Basic functions, implemented in C, for large data manipulation. Fast vectorised ifelse()/nested if()/switch() functions, psum()/pprod() functions equivalent to pmin()/pmax() plus others which are missing from base R. Most of these functions are callable at C level.  "
  },
  {
    "id": 14903,
    "package_name": "klovan",
    "title": "Geostatistics Methods and Klovan Data",
    "description": "A comprehensive set of geostatistical, visual,\n    and analytical methods, in conjunction with the expanded version of the\n    acclaimed J.E. Klovan's mining dataset, are included in 'klovan'. This makes the\n    package an excellent learning resource for Principal Component Analysis (PCA),\n    Factor Analysis (FA), kriging, and other geostatistical techniques. Originally\n    published in the 1976 book 'Geological Factor Analysis', the included mining\n    dataset was assembled by Professor J. E. Klovan of the University of Calgary.\n    Being one of the first applications of FA in the geosciences, this dataset has\n    significant historical importance. As a well-regarded and published dataset, it\n    is an excellent resource for demonstrating the capabilities of PCA, FA, kriging,\n    and other geostatistical techniques in geosciences. For those interested in\n    these methods, the 'klovan' datasets provide a valuable and illustrative resource.\n    Note that some methods require the 'RGeostats' package. Please refer to the\n    README or Additional_repositories for installation instructions. This material is\n    based upon research in the Materials Data Science for Stockpile Stewardship Center\n    of Excellence (MDS3-COE), and supported by the Department of Energy's National\n    Nuclear Security Administration under Award Number DE-NA0004104.",
    "version": "0.1.0",
    "maintainer": "Roger H French <rxf131@case.edu>",
    "author": "Jonathan E Gordon [aut] (ORCID:\n    <https://orcid.org/0009-0007-5958-7386>),\n  Eric K Helfer [aut] (ORCID: <https://orcid.org/0000-0002-8958-7690>),\n  Hope E Omodolor [aut] (ORCID: <https://orcid.org/0009-0005-7842-406X>),\n  Jeffery M Yarus [aut] (ORCID: <https://orcid.org/0000-0002-9331-9568>),\n  Roger H French [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6162-0532>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=klovan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "klovan Geostatistics Methods and Klovan Data A comprehensive set of geostatistical, visual,\n    and analytical methods, in conjunction with the expanded version of the\n    acclaimed J.E. Klovan's mining dataset, are included in 'klovan'. This makes the\n    package an excellent learning resource for Principal Component Analysis (PCA),\n    Factor Analysis (FA), kriging, and other geostatistical techniques. Originally\n    published in the 1976 book 'Geological Factor Analysis', the included mining\n    dataset was assembled by Professor J. E. Klovan of the University of Calgary.\n    Being one of the first applications of FA in the geosciences, this dataset has\n    significant historical importance. As a well-regarded and published dataset, it\n    is an excellent resource for demonstrating the capabilities of PCA, FA, kriging,\n    and other geostatistical techniques in geosciences. For those interested in\n    these methods, the 'klovan' datasets provide a valuable and illustrative resource.\n    Note that some methods require the 'RGeostats' package. Please refer to the\n    README or Additional_repositories for installation instructions. This material is\n    based upon research in the Materials Data Science for Stockpile Stewardship Center\n    of Excellence (MDS3-COE), and supported by the Department of Energy's National\n    Nuclear Security Administration under Award Number DE-NA0004104.  "
  },
  {
    "id": 14947,
    "package_name": "krige",
    "title": "Geospatial Kriging with Metropolis Sampling",
    "description": "Estimates kriging models for geographical point-referenced data. Method is described in Gill (2020) <doi:10.1177/1532440020930197>.",
    "version": "0.6.2",
    "maintainer": "Jason S. Byers <jaybyers55@gmail.com>",
    "author": "Jason S. Byers [aut, cre], Le Bao [aut], Jamie Carson [aut], Jeff Gill [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=krige",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "krige Geospatial Kriging with Metropolis Sampling Estimates kriging models for geographical point-referenced data. Method is described in Gill (2020) <doi:10.1177/1532440020930197>.  "
  },
  {
    "id": 14948,
    "package_name": "kriging",
    "title": "Ordinary Kriging",
    "description": "An implementation of a simple and highly optimized ordinary kriging algorithm to plot geographical data.",
    "version": "1.2",
    "maintainer": "Omar E. Olmedo <omareolmedo@gmail.com>",
    "author": "Omar E. Olmedo",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kriging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kriging Ordinary Kriging An implementation of a simple and highly optimized ordinary kriging algorithm to plot geographical data.  "
  },
  {
    "id": 14976,
    "package_name": "l1rotation",
    "title": "Identify Loading Vectors under Sparsity in Factor Models",
    "description": "Simplify the loading matrix in factor models using the l1 criterion as proposed in Freyaldenhoven (2025) <doi:10.21799/frbp.wp.2020.25>. Given a data matrix, find the rotation of the loading matrix with the smallest l1-norm and/or test for the presence of local factors with main function local_factors().",
    "version": "1.0.1",
    "maintainer": "Ryan Kobler <kobleary@gmail.com>",
    "author": "Simon Freyaldenhoven [aut, cph],\n  Ryan Kobler [aut, cre]",
    "url": "https://kobleary.github.io/l1rotation/,\nhttps://github.com/SimonFreyaldenhoven/l1rotation",
    "bug_reports": "https://github.com/SimonFreyaldenhoven/l1rotation/issues",
    "repository": "https://cran.r-project.org/package=l1rotation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "l1rotation Identify Loading Vectors under Sparsity in Factor Models Simplify the loading matrix in factor models using the l1 criterion as proposed in Freyaldenhoven (2025) <doi:10.21799/frbp.wp.2020.25>. Given a data matrix, find the rotation of the loading matrix with the smallest l1-norm and/or test for the presence of local factors with main function local_factors().  "
  },
  {
    "id": 14984,
    "package_name": "labelVector",
    "title": "Label Attributes for Atomic Vectors",
    "description": "Labels are a common construct in statistical software providing a \n  human readable description of a variable. While variable names are succinct,\n  quick to type, and follow a language's naming conventions, labels may \n  be more illustrative and may use plain text and spaces. R does not provide\n  native support for labels. Some packages, however, have made this feature\n  available.  Most notably, the 'Hmisc' package provides labelling methods\n  for a number of different object. Due to design decisions, these methods\n  are not all exported, and so are unavailable for use in package development.\n  The 'labelVector' package supports labels for atomic vectors in a light-weight\n  design that is suitable for use in other packages.",
    "version": "0.1.2",
    "maintainer": "Benjamin Nutter <benjamin.nutter@gmail.com>",
    "author": "Benjamin Nutter [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=labelVector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "labelVector Label Attributes for Atomic Vectors Labels are a common construct in statistical software providing a \n  human readable description of a variable. While variable names are succinct,\n  quick to type, and follow a language's naming conventions, labels may \n  be more illustrative and may use plain text and spaces. R does not provide\n  native support for labels. Some packages, however, have made this feature\n  available.  Most notably, the 'Hmisc' package provides labelling methods\n  for a number of different object. Due to design decisions, these methods\n  are not all exported, and so are unavailable for use in package development.\n  The 'labelVector' package supports labels for atomic vectors in a light-weight\n  design that is suitable for use in other packages.  "
  },
  {
    "id": 14997,
    "package_name": "lacunaritycovariance",
    "title": "Gliding Box Lacunarity and Other Metrics for 2D Random Closed\nSets",
    "description": "Functions for estimating the gliding box lacunarity (GBL),\n    covariance, and pair-correlation of a random closed set (RACS) in 2D\n    from a binary coverage map (e.g. presence-absence land cover maps).\n    Contains a number of newly-developed covariance-based estimators of\n    GBL (Hingee et al., 2019) <doi:10.1007/s13253-019-00351-9> and\n    balanced estimators, proposed by Picka (2000)\n    <http://www.jstor.org/stable/1428408>, for covariance, centred\n    covariance, and pair-correlation.  Also contains methods for\n    estimating contagion-like properties of RACS and simulating 2D Boolean\n    models.  Binary coverage maps are usually represented as raster images\n    with pixel values of TRUE, FALSE or NA, with NA representing\n    unobserved pixels.  A demo for extracting such a binary map from a\n    geospatial data format is provided.  Binary maps may also be\n    represented using polygonal sets as the foreground, however for most\n    computations such maps are converted into raster images.  The package\n    is based on research conducted during the author's PhD studies.",
    "version": "1.1-9",
    "maintainer": "Kassel Liam Hingee <kassel.hingee@gmail.com>",
    "author": "Kassel Liam Hingee [aut, cre]",
    "url": "https://github.com/kasselhingee/lacunaritycovariance",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lacunaritycovariance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lacunaritycovariance Gliding Box Lacunarity and Other Metrics for 2D Random Closed\nSets Functions for estimating the gliding box lacunarity (GBL),\n    covariance, and pair-correlation of a random closed set (RACS) in 2D\n    from a binary coverage map (e.g. presence-absence land cover maps).\n    Contains a number of newly-developed covariance-based estimators of\n    GBL (Hingee et al., 2019) <doi:10.1007/s13253-019-00351-9> and\n    balanced estimators, proposed by Picka (2000)\n    <http://www.jstor.org/stable/1428408>, for covariance, centred\n    covariance, and pair-correlation.  Also contains methods for\n    estimating contagion-like properties of RACS and simulating 2D Boolean\n    models.  Binary coverage maps are usually represented as raster images\n    with pixel values of TRUE, FALSE or NA, with NA representing\n    unobserved pixels.  A demo for extracting such a binary map from a\n    geospatial data format is provided.  Binary maps may also be\n    represented using polygonal sets as the foreground, however for most\n    computations such maps are converted into raster images.  The package\n    is based on research conducted during the author's PhD studies.  "
  },
  {
    "id": 15018,
    "package_name": "landscapeR",
    "title": "Categorical Landscape Simulation Facility",
    "description": "Simulates categorical maps on actual geographical realms, starting from either empty landscapes or landscapes provided by the user (e.g. land use maps). Allows to tweak or create landscapes while retaining a high degree of control on its features, without the hassle of specifying each location attribute. In this it differs from other tools which generate null or neutral landscapes in a theoretical space. The basic algorithm currently implemented uses a simple agent style/cellular automata growth model, with no rules (apart from areas of exclusion) and von Neumann neighbourhood (four cells, aka Rook case). Outputs are raster dataset exportable to any common GIS format.",
    "version": "1.3.1",
    "maintainer": "Dario Masante <dario.masante@gmail.com>",
    "author": "Dario Masante [aut, cre],\n  Lora Murphy [ctb]",
    "url": "https://github.com/dariomasante/landscapeR",
    "bug_reports": "https://github.com/dariomasante/landscapeR/issues",
    "repository": "https://cran.r-project.org/package=landscapeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "landscapeR Categorical Landscape Simulation Facility Simulates categorical maps on actual geographical realms, starting from either empty landscapes or landscapes provided by the user (e.g. land use maps). Allows to tweak or create landscapes while retaining a high degree of control on its features, without the hassle of specifying each location attribute. In this it differs from other tools which generate null or neutral landscapes in a theoretical space. The basic algorithm currently implemented uses a simple agent style/cellular automata growth model, with no rules (apart from areas of exclusion) and von Neumann neighbourhood (four cells, aka Rook case). Outputs are raster dataset exportable to any common GIS format.  "
  },
  {
    "id": 15019,
    "package_name": "landscapemetrics",
    "title": "Landscape Metrics for Categorical Map Patterns",
    "description": "Calculates landscape metrics for categorical landscape patterns in \n    a tidy workflow. 'landscapemetrics' reimplements the most common metrics from\n    'FRAGSTATS' (<https://www.fragstats.org/>) and new ones from the current \n    literature on landscape metrics. This package supports 'terra' SpatRaster objects \n    as input arguments. It further provides utility functions to visualize patches, \n    select metrics and building blocks to develop new metrics.",
    "version": "2.2.1",
    "maintainer": "Maximilian H.K. Hesselbarth <mhk.hesselbarth@gmail.com>",
    "author": "Maximilian H.K. Hesselbarth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1125-9918>),\n  Marco Sciaini [aut] (ORCID: <https://orcid.org/0000-0002-3042-5435>),\n  Jakub Nowosad [aut] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\n  Sebastian Hanss [aut] (ORCID: <https://orcid.org/0000-0002-3990-4897>),\n  Laura J. Graham [ctb] (Input on package structure),\n  Jeffrey Hollister [ctb] (Input on package structure),\n  Kimberly A. With [ctb] (Input on package structure),\n  Florian Priv\u00e9 [ctb] (Original author of underlying C++ code for\n    get_nearestneighbour() function),\n  Project Nayuki [ctb] (Original author of underlying C++ code for\n    get_circumscribingcircle and lsm_p_circle),\n  Matt Strimas-Mackey [ctb] (Bugfix in sample_metrics())",
    "url": "https://r-spatialecology.github.io/landscapemetrics/",
    "bug_reports": "https://github.com/r-spatialecology/landscapemetrics/issues",
    "repository": "https://cran.r-project.org/package=landscapemetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "landscapemetrics Landscape Metrics for Categorical Map Patterns Calculates landscape metrics for categorical landscape patterns in \n    a tidy workflow. 'landscapemetrics' reimplements the most common metrics from\n    'FRAGSTATS' (<https://www.fragstats.org/>) and new ones from the current \n    literature on landscape metrics. This package supports 'terra' SpatRaster objects \n    as input arguments. It further provides utility functions to visualize patches, \n    select metrics and building blocks to develop new metrics.  "
  },
  {
    "id": 15056,
    "package_name": "lavaanExtra",
    "title": "Convenience Functions for Package 'lavaan'",
    "description": "Affords an alternative, vector-based syntax to 'lavaan', as well as other \n             convenience functions such as naming paths and defining indirect\n             links automatically, in addition to convenience formatting optimized\n             for a publication and script sharing workflow.",
    "version": "0.2.2",
    "maintainer": "R\u00e9mi Th\u00e9riault <remi.theriault@mail.mcgill.ca>",
    "author": "R\u00e9mi Th\u00e9riault [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4315-6788>)",
    "url": "https://lavaanExtra.remi-theriault.com",
    "bug_reports": "https://github.com/rempsyc/lavaanExtra/issues",
    "repository": "https://cran.r-project.org/package=lavaanExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaanExtra Convenience Functions for Package 'lavaan' Affords an alternative, vector-based syntax to 'lavaan', as well as other \n             convenience functions such as naming paths and defining indirect\n             links automatically, in addition to convenience formatting optimized\n             for a publication and script sharing workflow.  "
  },
  {
    "id": 15069,
    "package_name": "lazysf",
    "title": "Delayed Read for 'GDAL' Vector Data Sources",
    "description": "Lazy read for drawings. A 'dplyr' back end for data sources supported by \n    'GDAL' vector drivers, that allows working with local or remote sources as if they \n    are in-memory data frames. Basic features works with any drawing format ('GDAL vector \n    data source') supported by the 'sf' package. ",
    "version": "0.2.0",
    "maintainer": "Michael Sumner <mdsumner@gmail.com>",
    "author": "Michael Sumner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2471-7511>)",
    "url": "https://github.com/hypertidy/lazysf,\nhttps://hypertidy.github.io/lazysf/",
    "bug_reports": "https://github.com/hypertidy/lazysf/issues",
    "repository": "https://cran.r-project.org/package=lazysf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lazysf Delayed Read for 'GDAL' Vector Data Sources Lazy read for drawings. A 'dplyr' back end for data sources supported by \n    'GDAL' vector drivers, that allows working with local or remote sources as if they \n    are in-memory data frames. Basic features works with any drawing format ('GDAL vector \n    data source') supported by the 'sf' package.   "
  },
  {
    "id": 15076,
    "package_name": "lbm",
    "title": "Log Binomial Regression Model in Exact Method",
    "description": "Fit the log binomial regression model (LBM) by Exact method. Limited parameter space of \n    LBM causes trouble to find admissible estimates and fail to converge when MLE is close to or on \n    the boundary of space. Exact method utilizes the property of boundary vectors to re-parametrize \n    the model without losing any information, and fits the model on the standard fitting algorithm \n    with no convergence issues. ",
    "version": "0.9.0.2",
    "maintainer": "Chao Zhu <zhuchao9966@gmail.com>",
    "author": "Chao Zhu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lbm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lbm Log Binomial Regression Model in Exact Method Fit the log binomial regression model (LBM) by Exact method. Limited parameter space of \n    LBM causes trouble to find admissible estimates and fail to converge when MLE is close to or on \n    the boundary of space. Exact method utilizes the property of boundary vectors to re-parametrize \n    the model without losing any information, and fits the model on the standard fitting algorithm \n    with no convergence issues.   "
  },
  {
    "id": 15084,
    "package_name": "lconnect",
    "title": "Simple Tools to Compute Landscape Connectivity Metrics",
    "description": "Provides functions to upload vectorial data and derive landscape\n    connectivity metrics in habitat or matrix systems. Additionally, includes an \n    approach to assess individual patch contribution to the overall landscape \n    connectivity, enabling the prioritization of habitat patches. The computation\n    of landscape connectivity and patch importance are very useful in Landscape \n    Ecology research. The metrics available are: number of components, number of \n    links, size of the largest component, mean size of components, class coincidence\n    probability, landscape coincidence probability, characteristic path length, \n    expected cluster size, area-weighted flux and integral index of connectivity.\n    Pascual-Hortal, L., and Saura, S. (2006) <doi:10.1007/s10980-006-0013-z>\n    Urban, D., and Keitt, T. (2001) <doi:10.2307/2679983>\n    Laita, A., Kotiaho, J., Monkkonen, M. (2011) <doi:10.1007/s10980-011-9620-4>.",
    "version": "0.1.2",
    "maintainer": "Frederico Mestre <mestre.frederico@gmail.com>",
    "author": "Frederico Mestre [aut, cre],\n  Bruno Silva [aut],\n  Benjamin Branoff [ctb]",
    "url": "",
    "bug_reports": "https://github.com/FMestre1/lconnect/issues",
    "repository": "https://cran.r-project.org/package=lconnect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lconnect Simple Tools to Compute Landscape Connectivity Metrics Provides functions to upload vectorial data and derive landscape\n    connectivity metrics in habitat or matrix systems. Additionally, includes an \n    approach to assess individual patch contribution to the overall landscape \n    connectivity, enabling the prioritization of habitat patches. The computation\n    of landscape connectivity and patch importance are very useful in Landscape \n    Ecology research. The metrics available are: number of components, number of \n    links, size of the largest component, mean size of components, class coincidence\n    probability, landscape coincidence probability, characteristic path length, \n    expected cluster size, area-weighted flux and integral index of connectivity.\n    Pascual-Hortal, L., and Saura, S. (2006) <doi:10.1007/s10980-006-0013-z>\n    Urban, D., and Keitt, T. (2001) <doi:10.2307/2679983>\n    Laita, A., Kotiaho, J., Monkkonen, M. (2011) <doi:10.1007/s10980-011-9620-4>.  "
  },
  {
    "id": 15097,
    "package_name": "ldmppr",
    "title": "Estimate and Simulate from Location Dependent Marked Point\nProcesses",
    "description": "A suite of tools for estimating, assessing model fit, simulating from, and visualizing location dependent marked point processes characterized by regularity in the pattern.\n    You provide a reference marked point process, a set of raster images containing location specific covariates, and select the estimation algorithm and type of mark model.\n    'ldmppr' estimates the process and mark models and allows you to check the appropriateness of the model using a variety of diagnostic tools.\n    Once a satisfactory model fit is obtained, you can simulate from the model and visualize the results.\n    Documentation for the package 'ldmppr' is available in the form of a vignette.",
    "version": "1.0.4",
    "maintainer": "Lane Drew <lanetdrew@gmail.com>",
    "author": "Lane Drew [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0006-5427-4092>),\n  Andee Kaplan [aut] (ORCID: <https://orcid.org/0000-0002-2940-889X>)",
    "url": "https://github.com/lanedrew/ldmppr",
    "bug_reports": "https://github.com/lanedrew/ldmppr/issues",
    "repository": "https://cran.r-project.org/package=ldmppr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ldmppr Estimate and Simulate from Location Dependent Marked Point\nProcesses A suite of tools for estimating, assessing model fit, simulating from, and visualizing location dependent marked point processes characterized by regularity in the pattern.\n    You provide a reference marked point process, a set of raster images containing location specific covariates, and select the estimation algorithm and type of mark model.\n    'ldmppr' estimates the process and mark models and allows you to check the appropriateness of the model using a variety of diagnostic tools.\n    Once a satisfactory model fit is obtained, you can simulate from the model and visualize the results.\n    Documentation for the package 'ldmppr' is available in the form of a vignette.  "
  },
  {
    "id": 15115,
    "package_name": "leapp",
    "title": "Latent Effect Adjustment After Primary Projection",
    "description": "These functions take a gene expression value matrix, a\n        primary covariate vector, an additional known covariates\n        matrix.  A two stage analysis is applied to counter the effects\n        of latent variables on the rankings of hypotheses.  The\n        estimation and adjustment of latent effects are proposed by\n        Sun, Zhang and Owen (2011).  \"leapp\" is developed in the\n        context of microarray experiments, but may be used as a general\n        tool for high throughput data sets where dependence may be\n        involved.",
    "version": "1.3",
    "maintainer": "Yunting Sun <yunting.sun@gmail.com>",
    "author": "Yunting Sun <yunting.sun@gmail.com> , Nancy R.Zhang\n        <nzhang@stanford.edu>, Art B.Owen <owen@stanford.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=leapp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "leapp Latent Effect Adjustment After Primary Projection These functions take a gene expression value matrix, a\n        primary covariate vector, an additional known covariates\n        matrix.  A two stage analysis is applied to counter the effects\n        of latent variables on the rankings of hypotheses.  The\n        estimation and adjustment of latent effects are proposed by\n        Sun, Zhang and Owen (2011).  \"leapp\" is developed in the\n        context of microarray experiments, but may be used as a general\n        tool for high throughput data sets where dependence may be\n        involved.  "
  },
  {
    "id": 15127,
    "package_name": "legion",
    "title": "Forecasting Using Multivariate Models",
    "description": "Functions implementing multivariate state space models for purposes of time series analysis and forecasting.\n             The focus of the package is on multivariate models, such as Vector Exponential Smoothing,\n             Vector ETS (Error-Trend-Seasonal model) etc. It currently includes Vector Exponential\n             Smoothing (VES, de Silva et al., 2010, <doi:10.1177/1471082X0901000401>), Vector ETS (Svetunkov et al., 2023,\n             <doi:10.1016/j.ejor.2022.04.040>) and simulation function for VES.",
    "version": "0.2.1",
    "maintainer": "Ivan Svetunkov <ivan@svetunkov.com>",
    "author": "Ivan Svetunkov [aut, cre] (Senior Lecturer, Centre for Marketing\n    Analytics and Forecasting, Lancaster University, UK),\n  Kandrika Fadhlan Pritularga [aut] (Lecturer, Centre for Marketing\n    Analytics and Forecasting, Lancaster University, UK)",
    "url": "https://github.com/config-i1/legion",
    "bug_reports": "https://github.com/config-i1/legion/issues",
    "repository": "https://cran.r-project.org/package=legion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "legion Forecasting Using Multivariate Models Functions implementing multivariate state space models for purposes of time series analysis and forecasting.\n             The focus of the package is on multivariate models, such as Vector Exponential Smoothing,\n             Vector ETS (Error-Trend-Seasonal model) etc. It currently includes Vector Exponential\n             Smoothing (VES, de Silva et al., 2010, <doi:10.1177/1471082X0901000401>), Vector ETS (Svetunkov et al., 2023,\n             <doi:10.1016/j.ejor.2022.04.040>) and simulation function for VES.  "
  },
  {
    "id": 15146,
    "package_name": "lest",
    "title": "Vectorised Nested if-else Statements Similar to CASE WHEN in\n'SQL'",
    "description": "Functions for vectorised conditional recoding of\n    variables. case_when() enables you to vectorise multiple if and else\n    statements (like 'CASE WHEN' in 'SQL'). if_else() is a stricter and\n    more predictable version of ifelse() in 'base' that preserves\n    attributes. These functions are forked from 'dplyr' with all package\n    dependencies removed and behave identically to the originals.",
    "version": "1.1.0",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>),\n  Hadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\n  Romain Fran\u00e7ois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Lionel Henry [aut],\n  Kirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lest Vectorised Nested if-else Statements Similar to CASE WHEN in\n'SQL' Functions for vectorised conditional recoding of\n    variables. case_when() enables you to vectorise multiple if and else\n    statements (like 'CASE WHEN' in 'SQL'). if_else() is a stricter and\n    more predictable version of ifelse() in 'base' that preserves\n    attributes. These functions are forked from 'dplyr' with all package\n    dependencies removed and behave identically to the originals.  "
  },
  {
    "id": 15176,
    "package_name": "libdeflate",
    "title": "DEFLATE Compression and Static Library",
    "description": "Whole-buffer DEFLATE-based compression and \n    decompression of raw vectors using the 'libdeflate' library \n    (see <https://github.com/ebiggers/libdeflate>). Provides the user with additional \n    control over the speed and the quality of DEFLATE compression \n    compared to the fixed level of compression offered in R's \n    'memCompress()' function. Also provides the 'libdeflate' static library and \n    'C' headers along with a 'CMake' target and 'package\u2011config' file that \n    ease linking of 'libdeflate' in packages that compile and statically link \n    bundled libraries using 'CMake'.",
    "version": "1.24-7",
    "maintainer": "Tyler Morgan-Wall <tylermw@gmail.com>",
    "author": "Tyler Morgan-Wall [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3131-3814>),\n  Eric Biggers [aut, cph],\n  Google LLC [cph],\n  Kevin Ushey [cph]",
    "url": "",
    "bug_reports": "https://github.com/tylermorganwall/libdeflate/issues",
    "repository": "https://cran.r-project.org/package=libdeflate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "libdeflate DEFLATE Compression and Static Library Whole-buffer DEFLATE-based compression and \n    decompression of raw vectors using the 'libdeflate' library \n    (see <https://github.com/ebiggers/libdeflate>). Provides the user with additional \n    control over the speed and the quality of DEFLATE compression \n    compared to the fixed level of compression offered in R's \n    'memCompress()' function. Also provides the 'libdeflate' static library and \n    'C' headers along with a 'CMake' target and 'package\u2011config' file that \n    ease linking of 'libdeflate' in packages that compile and statically link \n    bundled libraries using 'CMake'.  "
  },
  {
    "id": 15198,
    "package_name": "lightsf",
    "title": "A Curated Collection of Georeferenced and Spatial Datasets",
    "description": "Provides a diverse collection of georeferenced and spatial datasets\n    from different domains including urban studies, housing markets, environmental\n    monitoring, transportation, and socio-economic indicators.\n    The package consolidates datasets from multiple open sources such as Kaggle,\n    chopin, spData, adespatial, and bivariateLeaflet.\n    It is designed for researchers, analysts, and educators interested in spatial\n    analysis, geostatistics, and geographic data visualization. \n    The datasets include point patterns, polygons, socio-economic data frames, and\n    network-like structures, allowing flexible exploration of geospatial phenomena.",
    "version": "0.1.0",
    "maintainer": "Ingrid Romero Pinilla <ingridpinilla11@gmail.com>",
    "author": "Ingrid Romero Pinilla [aut, cre]",
    "url": "https://github.com/roming20/lightsf,\nhttps://roming20.github.io/lightsf/",
    "bug_reports": "https://github.com/roming20/lightsf/issues",
    "repository": "https://cran.r-project.org/package=lightsf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lightsf A Curated Collection of Georeferenced and Spatial Datasets Provides a diverse collection of georeferenced and spatial datasets\n    from different domains including urban studies, housing markets, environmental\n    monitoring, transportation, and socio-economic indicators.\n    The package consolidates datasets from multiple open sources such as Kaggle,\n    chopin, spData, adespatial, and bivariateLeaflet.\n    It is designed for researchers, analysts, and educators interested in spatial\n    analysis, geostatistics, and geographic data visualization. \n    The datasets include point patterns, polygons, socio-economic data frames, and\n    network-like structures, allowing flexible exploration of geospatial phenomena.  "
  },
  {
    "id": 15212,
    "package_name": "lin.eval",
    "title": "Perform Polynomial Evaluation of Linearity",
    "description": "Evaluates whether the relationship between two vectors is linear or nonlinear. Performs a test to determine how well a linear model fits the data compared to higher order polynomial models. Jhang et al. (2004) <doi:10.1043/1543-2165(2004)128%3C44:EOLITC%3E2.0.CO;2>.",
    "version": "0.1.2",
    "maintainer": "Vishesh Shrivastav <vishesh2k6@gmail.com>",
    "author": "Vishesh Shrivastav",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lin.eval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lin.eval Perform Polynomial Evaluation of Linearity Evaluates whether the relationship between two vectors is linear or nonlinear. Performs a test to determine how well a linear model fits the data compared to higher order polynomial models. Jhang et al. (2004) <doi:10.1043/1543-2165(2004)128%3C44:EOLITC%3E2.0.CO;2>.  "
  },
  {
    "id": 15220,
    "package_name": "linemap",
    "title": "Line Maps",
    "description": "Create maps made of lines. The package contains one function:\n    linemap(). linemap() displays a map made of lines using a\n    raster or gridded data.",
    "version": "0.3.0",
    "maintainer": "Timoth\u00e9e Giraud <timothee.giraud@cnrs.fr>",
    "author": "Timoth\u00e9e Giraud [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-1932-3323>)",
    "url": "https://github.com/riatelab/linemap",
    "bug_reports": "https://github.com/riatelab/linemap/issues",
    "repository": "https://cran.r-project.org/package=linemap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "linemap Line Maps Create maps made of lines. The package contains one function:\n    linemap(). linemap() displays a map made of lines using a\n    raster or gridded data.  "
  },
  {
    "id": 15237,
    "package_name": "lintools",
    "title": "Manipulation of Linear Systems of (in)Equalities",
    "description": "Variable elimination (Gaussian elimination, Fourier-Motzkin elimination), \n    Moore-Penrose pseudoinverse, reduction to reduced row echelon form, value substitution,  \n    projecting a vector on the convex polytope described by a system of (in)equations, \n    simplify systems by removing spurious columns and rows and collapse implied equalities, \n    test if a matrix is totally unimodular, compute variable ranges implied by linear\n    (in)equalities.",
    "version": "0.1.7",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre],\n  Edwin de Jonge [aut]",
    "url": "https://github.com/data-cleaning/lintools",
    "bug_reports": "https://github.com/data-cleaning/lintools/issues",
    "repository": "https://cran.r-project.org/package=lintools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lintools Manipulation of Linear Systems of (in)Equalities Variable elimination (Gaussian elimination, Fourier-Motzkin elimination), \n    Moore-Penrose pseudoinverse, reduction to reduced row echelon form, value substitution,  \n    projecting a vector on the convex polytope described by a system of (in)equations, \n    simplify systems by removing spurious columns and rows and collapse implied equalities, \n    test if a matrix is totally unimodular, compute variable ranges implied by linear\n    (in)equalities.  "
  },
  {
    "id": 15250,
    "package_name": "listcompr",
    "title": "List Comprehension for R",
    "description": "Syntactic shortcuts for creating synthetic lists, vectors, \n    data frames, and matrices using list comprehension.",
    "version": "0.4.0",
    "maintainer": "Patrick Roocks <mail@p-roocks.de>",
    "author": "Patrick Roocks <mail@p-roocks.de>",
    "url": "https://github.com/patrickroocks/listcompr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=listcompr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "listcompr List Comprehension for R Syntactic shortcuts for creating synthetic lists, vectors, \n    data frames, and matrices using list comprehension.  "
  },
  {
    "id": 15312,
    "package_name": "locStra",
    "title": "Fast Implementation of (Local) Population Stratification Methods",
    "description": "Fast implementations to compute the genetic covariance matrix, the Jaccard similarity matrix, the s-matrix (the weighted Jaccard similarity matrix), and the (classic or robust) genomic relationship matrix of a (dense or sparse) input matrix (see Hahn, Lutz, Hecker, Prokopenko, Cho, Silverman, Weiss, and Lange (2020) <doi:10.1002/gepi.22356>). Full support for sparse matrices from the R-package 'Matrix'. Additionally, an implementation of the power method (von Mises iteration) to compute the largest eigenvector of a matrix is included, a function to perform an automated full run of global and local correlations in population stratification data, a function to compute sliding windows, and a function to invert minor alleles and to select those variants/loci exceeding a minimal cutoff value. New functionality in locStra allows one to extract the k leading eigenvectors of the genetic covariance matrix, Jaccard similarity matrix, s-matrix, and genomic relationship matrix via fast PCA without actually computing the similarity matrices. The fast PCA to compute the k leading eigenvectors can now also be run directly from 'bed'+'bim'+'fam' files.",
    "version": "1.9",
    "maintainer": "Georg Hahn <ghahn@hsph.harvard.edu>",
    "author": "Georg Hahn [aut,cre], Sharon M. Lutz [ctb], Christoph Lange [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=locStra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "locStra Fast Implementation of (Local) Population Stratification Methods Fast implementations to compute the genetic covariance matrix, the Jaccard similarity matrix, the s-matrix (the weighted Jaccard similarity matrix), and the (classic or robust) genomic relationship matrix of a (dense or sparse) input matrix (see Hahn, Lutz, Hecker, Prokopenko, Cho, Silverman, Weiss, and Lange (2020) <doi:10.1002/gepi.22356>). Full support for sparse matrices from the R-package 'Matrix'. Additionally, an implementation of the power method (von Mises iteration) to compute the largest eigenvector of a matrix is included, a function to perform an automated full run of global and local correlations in population stratification data, a function to compute sliding windows, and a function to invert minor alleles and to select those variants/loci exceeding a minimal cutoff value. New functionality in locStra allows one to extract the k leading eigenvectors of the genetic covariance matrix, Jaccard similarity matrix, s-matrix, and genomic relationship matrix via fast PCA without actually computing the similarity matrices. The fast PCA to compute the k leading eigenvectors can now also be run directly from 'bed'+'bim'+'fam' files.  "
  },
  {
    "id": 15336,
    "package_name": "lofifonts",
    "title": "Text Rendering with Bitmap and Vector Fonts",
    "description": "Alternate font rendering is useful when rendering text to novel\n   graphics outputs where modern font rendering is not available or \n   where bespoke text positioning is required. Bitmap and vector fonts allow for \n   custom layout and rendering using pixel coordinates and line drawing.  \n   Formatted text is created as a data.frame of pixel coordinates (for bitmap fonts)\n   or stroke coordinates (for vector fonts).  All text can be easily \n   previewed as a matrix or raster image.  \n   A selection of fonts is included with this package.",
    "version": "0.1.3",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph],\n  June Choe [ctb] (Contributed character positioning code),\n  Frederic Cambus [cph, tyd] (Creator of 'spleen' font),\n  GNU Unifont authors [cph, tyd] (Creators of 'unifont'),\n  Suraj Kurapati [cph, tyd] (Creator of 'Tamzen' font),\n  Scott Fial [cph, tyd] (Creator of 'Tamsyn' font upon which 'Tamzen' is\n    based),\n  Anders Hoff [cph, tyd] (Creator of 'gridfont' font)",
    "url": "https://github.com/coolbutuseless/lofifonts",
    "bug_reports": "https://github.com/coolbutuseless/lofifonts/issues",
    "repository": "https://cran.r-project.org/package=lofifonts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lofifonts Text Rendering with Bitmap and Vector Fonts Alternate font rendering is useful when rendering text to novel\n   graphics outputs where modern font rendering is not available or \n   where bespoke text positioning is required. Bitmap and vector fonts allow for \n   custom layout and rendering using pixel coordinates and line drawing.  \n   Formatted text is created as a data.frame of pixel coordinates (for bitmap fonts)\n   or stroke coordinates (for vector fonts).  All text can be easily \n   previewed as a matrix or raster image.  \n   A selection of fonts is included with this package.  "
  },
  {
    "id": 15390,
    "package_name": "longurl",
    "title": "Expand Short 'URLs'",
    "description": "Tools are provided to expand vectors of short URLs into long 'URLs'. \n    No 'API' services are used, which may mean that this operates more slowly than \n    'API' services do (since they usually cache results of expansions that every \n    user of the service requests). You can setup your own caching layer with the \n    'memoise' package if you wish to have a speedup during single sessions or add \n    larger dependencies, such as 'Redis', to gain a longer-term performance boost \n    at the expense of added complexity.",
    "version": "0.3.3",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>),\n  John Coene [ctb] (Issue #4)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=longurl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "longurl Expand Short 'URLs' Tools are provided to expand vectors of short URLs into long 'URLs'. \n    No 'API' services are used, which may mean that this operates more slowly than \n    'API' services do (since they usually cache results of expansions that every \n    user of the service requests). You can setup your own caching layer with the \n    'memoise' package if you wish to have a speedup during single sessions or add \n    larger dependencies, such as 'Redis', to gain a longer-term performance boost \n    at the expense of added complexity.  "
  },
  {
    "id": 15398,
    "package_name": "loopevd",
    "title": "Loop Functions for Extreme Value Distributions",
    "description": "Performs extreme value analysis at multiple locations using functions from the 'evd' package. Supports both point-based and gridded input data using the 'terra' package, enabling flexible looping across spatial datasets for batch processing of generalised extreme value, Gumbel fits.",
    "version": "1.0.2",
    "maintainer": "Julian O'Grady <julian.ogrady@csiro.au>",
    "author": "Julian O'Grady [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=loopevd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "loopevd Loop Functions for Extreme Value Distributions Performs extreme value analysis at multiple locations using functions from the 'evd' package. Supports both point-based and gridded input data using the 'terra' package, enabling flexible looping across spatial datasets for batch processing of generalised extreme value, Gumbel fits.  "
  },
  {
    "id": 15399,
    "package_name": "lorad",
    "title": "Lowest Radial Distance Method of Marginal Likelihood Estimation",
    "description": "Estimates marginal likelihood from a posterior sample using the method described in Wang et al. (2023) <doi:10.1093/sysbio/syad007>, which does not require evaluation of any additional points and requires only the log of the unnormalized posterior density for each sampled parameter vector.",
    "version": "0.0.1.0",
    "maintainer": "Analisa Milkey <analisa.milkey@uconn.edu>",
    "author": "Analisa Milkey [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1157-4363>),\n  Elena Korte [aut],\n  Paul O. Lewis [aut] (ORCID: <https://orcid.org/0000-0001-9852-8759>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lorad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lorad Lowest Radial Distance Method of Marginal Likelihood Estimation Estimates marginal likelihood from a posterior sample using the method described in Wang et al. (2023) <doi:10.1093/sysbio/syad007>, which does not require evaluation of any additional points and requires only the log of the unnormalized posterior density for each sampled parameter vector.  "
  },
  {
    "id": 15453,
    "package_name": "ltsk",
    "title": "Local Time Space Kriging",
    "description": "Implements local spatial and local spatiotemporal Kriging based on local spatial and local spatiotemporal variograms, respectively. The method is documented in Kumar et al (2013) <https://www.nature.com/articles/jes201352)>.",
    "version": "1.1.2",
    "maintainer": "Dong Liang <dliang@umces.edu>",
    "author": "Naresh Kumar, Dong Liang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ltsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ltsk Local Time Space Kriging Implements local spatial and local spatiotemporal Kriging based on local spatial and local spatiotemporal variograms, respectively. The method is documented in Kumar et al (2013) <https://www.nature.com/articles/jes201352)>.  "
  },
  {
    "id": 15455,
    "package_name": "ltxsparklines",
    "title": "Lightweight Sparklines for a LaTeX Document",
    "description": "Sparklines are small plots (about one line of text high),\n  made popular by Edward Tufte.  This package is the interface from R\n  to the LaTeX package sparklines by Andreas Loeffer and Dan Luecking\n  (<http://www.ctan.org/pkg/sparklines>).  It can work with Sweave or\n  knitr or other engines that produce TeX.  The package can be used to\n  plot vectors, matrices, data frames, time series (in ts or zoo format).",
    "version": "1.1.3",
    "maintainer": "Boris Veytsman <borisv@lk.net>",
    "author": "Boris Veytsman [aut, cre]",
    "url": "https://github.com/borisveytsman/ltxsparklines",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ltxsparklines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ltxsparklines Lightweight Sparklines for a LaTeX Document Sparklines are small plots (about one line of text high),\n  made popular by Edward Tufte.  This package is the interface from R\n  to the LaTeX package sparklines by Andreas Loeffer and Dan Luecking\n  (<http://www.ctan.org/pkg/sparklines>).  It can work with Sweave or\n  knitr or other engines that produce TeX.  The package can be used to\n  plot vectors, matrices, data frames, time series (in ts or zoo format).  "
  },
  {
    "id": 15459,
    "package_name": "lucid",
    "title": "Printing Floating Point Numbers in a Human-Friendly Format",
    "description": "Print vectors (and data frames) of floating point numbers\n    using a non-scientific format optimized for human readers.  Vectors of\n    numbers are rounded using significant digits, aligned at the decimal\n    point, and all zeros trailing the decimal point are dropped.  See:\n    Wright (2016). Lucid: An R Package for Pretty-Printing Floating Point\n    Numbers. In JSM Proceedings, Statistical Computing Section.\n    Alexandria, VA: American Statistical Association. 2270-2279.",
    "version": "1.9",
    "maintainer": "Kevin Wright <kw.stat@gmail.com>",
    "author": "Kevin Wright [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0617-8673>)",
    "url": "https://kwstat.github.io/lucid/, http://kwstat.github.io/lucid/",
    "bug_reports": "https://github.com/kwstat/lucid/issues",
    "repository": "https://cran.r-project.org/package=lucid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lucid Printing Floating Point Numbers in a Human-Friendly Format Print vectors (and data frames) of floating point numbers\n    using a non-scientific format optimized for human readers.  Vectors of\n    numbers are rounded using significant digits, aligned at the decimal\n    point, and all zeros trailing the decimal point are dropped.  See:\n    Wright (2016). Lucid: An R Package for Pretty-Printing Floating Point\n    Numbers. In JSM Proceedings, Statistical Computing Section.\n    Alexandria, VA: American Statistical Association. 2270-2279.  "
  },
  {
    "id": 15485,
    "package_name": "mFDP",
    "title": "Control of the Median of the FDP",
    "description": "Methods for controlling the median of the false discovery proportion (mFDP).\n\tDepending on the method, simultaneous or non-simultaneous inference is provided. \n\tThe methods take a vector of p-values or test statistics as input. ",
    "version": "0.2.1",
    "maintainer": "Jesse Hemerik <hemerik@ese.eur.nl>",
    "author": "Jesse Hemerik [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mFDP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mFDP Control of the Median of the FDP Methods for controlling the median of the false discovery proportion (mFDP).\n\tDepending on the method, simultaneous or non-simultaneous inference is provided. \n\tThe methods take a vector of p-values or test statistics as input.   "
  },
  {
    "id": 15526,
    "package_name": "magic",
    "title": "Create and Investigate Magic Squares",
    "description": "A collection of functions for the manipulation and\n analysis of arbitrarily dimensioned arrays.  The original motivation\n for the package was the development of efficient, vectorized\n algorithms for the creation and investigation of magic squares and\n high-dimensional magic hypercubes.",
    "version": "1.6-1",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "author": "Robin K. S. Hankin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5982-0415>)",
    "url": "https://github.com/RobinHankin/magic",
    "bug_reports": "https://github.com/RobinHankin/magic/issues",
    "repository": "https://cran.r-project.org/package=magic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "magic Create and Investigate Magic Squares A collection of functions for the manipulation and\n analysis of arbitrarily dimensioned arrays.  The original motivation\n for the package was the development of efficient, vectorized\n algorithms for the creation and investigation of magic squares and\n high-dimensional magic hypercubes.  "
  },
  {
    "id": 15531,
    "package_name": "magmaR",
    "title": "R-Client for Interacting with the 'UCSF Data Library'",
    "description": "A client for interacting with 'magma', the data warehouse of the\n    'UCSF Data Library'. 'magmaR' includes functions for querying and\n    downloading data from 'magma', in order to enable working with such data in\n    R, as well as for uploading local data to 'magma'.",
    "version": "1.0.4",
    "maintainer": "Daniel Bunis <daniel.bunis@ucsf.edu>",
    "author": "Daniel Bunis [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=magmaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "magmaR R-Client for Interacting with the 'UCSF Data Library' A client for interacting with 'magma', the data warehouse of the\n    'UCSF Data Library'. 'magmaR' includes functions for querying and\n    downloading data from 'magma', in order to enable working with such data in\n    R, as well as for uploading local data to 'magma'.  "
  },
  {
    "id": 15546,
    "package_name": "makeunique",
    "title": "Make Character Strings Unique",
    "description": "Make all elements of a character vector unique. \n    Differs from 'make.unique' by starting at 1 and allowing users to customise suffix format.",
    "version": "1.0.0",
    "maintainer": "Sam El-Kamand <sam.elkamand@gmail.com>",
    "author": "Sam El-Kamand [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2270-8088>)",
    "url": "https://github.com/selkamand/makeunique",
    "bug_reports": "https://github.com/selkamand/makeunique/issues",
    "repository": "https://cran.r-project.org/package=makeunique",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "makeunique Make Character Strings Unique Make all elements of a character vector unique. \n    Differs from 'make.unique' by starting at 1 and allowing users to customise suffix format.  "
  },
  {
    "id": 15550,
    "package_name": "malariaAtlas",
    "title": "An R Interface to Open-Access Malaria Data, Hosted by the\n'Malaria Atlas Project'",
    "description": "A suite of tools to allow you to download all \n  publicly available parasite rate survey points, mosquito occurrence points and raster surfaces from \n  the 'Malaria Atlas Project' <https://malariaatlas.org/> servers as well as utility functions for plotting\n  the downloaded data.",
    "version": "1.6.4",
    "maintainer": "Mauricio van den Berg <mauricio.vandenberg@thekids.org.au>",
    "author": "Mauricio van den Berg [aut, cre],\n  Daniel Pfeffer [aut] (ORCID: <https://orcid.org/0000-0002-2204-3488>),\n  Tim Lucas [aut] (ORCID: <https://orcid.org/0000-0003-4694-8107>),\n  Daniel May [aut] (ORCID: <https://orcid.org/0000-0003-0005-2452>),\n  Suzanne Keddie [aut] (ORCID: <https://orcid.org/0000-0003-1254-7794>),\n  Jen Rozier [aut] (ORCID: <https://orcid.org/0000-0002-2610-7557>),\n  Oliver Watson [aut] (ORCID: <https://orcid.org/0000-0003-2374-0741>),\n  Harry Gibson [aut] (ORCID: <https://orcid.org/0000-0001-6779-3250>),\n  Nick Golding [ctb],\n  David Smith [ctb]",
    "url": "https://github.com/malaria-atlas-project/malariaAtlas",
    "bug_reports": "https://github.com/malaria-atlas-project/malariaAtlas/issues",
    "repository": "https://cran.r-project.org/package=malariaAtlas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "malariaAtlas An R Interface to Open-Access Malaria Data, Hosted by the\n'Malaria Atlas Project' A suite of tools to allow you to download all \n  publicly available parasite rate survey points, mosquito occurrence points and raster surfaces from \n  the 'Malaria Atlas Project' <https://malariaatlas.org/> servers as well as utility functions for plotting\n  the downloaded data.  "
  },
  {
    "id": 15553,
    "package_name": "mall",
    "title": "Run Multiple Large Language Model Predictions Against a Table,\nor Vectors",
    "description": "Run multiple 'Large Language Model' predictions against a table. The\n    predictions run row-wise over a specified column. It works using a \n    one-shot prompt, along with the current row's content. The prompt that is used\n    will depend of the type of analysis needed.",
    "version": "0.2.0",
    "maintainer": "Edgar Ruiz <edgar@posit.co>",
    "author": "Edgar Ruiz [aut, cre],\n  Posit Software, PBC [cph, fnd]",
    "url": "https://mlverse.github.io/mall/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mall Run Multiple Large Language Model Predictions Against a Table,\nor Vectors Run multiple 'Large Language Model' predictions against a table. The\n    predictions run row-wise over a specified column. It works using a \n    one-shot prompt, along with the current row's content. The prompt that is used\n    will depend of the type of analysis needed.  "
  },
  {
    "id": 15579,
    "package_name": "mapboxapi",
    "title": "R Interface to 'Mapbox' Web Services",
    "description": "Includes support for 'Mapbox' Navigation APIs, including directions, \n  isochrones, and route optimization; the Search API for forward and reverse geocoding; \n  the Maps API for interacting with 'Mapbox' vector tilesets and visualizing \n  'Mapbox' maps in R; and 'Mapbox Tiling Service' and 'tippecanoe' for generating map tiles.\n  See <https://docs.mapbox.com/api/> for more information about the 'Mapbox' APIs.",
    "version": "0.6.2",
    "maintainer": "Kyle Walker <kyle@walker-data.com>",
    "author": "Kyle Walker [aut, cre],\n  Eli Pousson [ctb],\n  Anthony North [ctb, cph],\n  Miles McBain [ctb]",
    "url": "https://github.com/walkerke/mapboxapi,\nhttps://walker-data.com/mapboxapi/",
    "bug_reports": "https://github.com/walkerke/mapboxapi/issues",
    "repository": "https://cran.r-project.org/package=mapboxapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapboxapi R Interface to 'Mapbox' Web Services Includes support for 'Mapbox' Navigation APIs, including directions, \n  isochrones, and route optimization; the Search API for forward and reverse geocoding; \n  the Maps API for interacting with 'Mapbox' vector tilesets and visualizing \n  'Mapbox' maps in R; and 'Mapbox Tiling Service' and 'tippecanoe' for generating map tiles.\n  See <https://docs.mapbox.com/api/> for more information about the 'Mapbox' APIs.  "
  },
  {
    "id": 15590,
    "package_name": "mapiso",
    "title": "Create Contour Polygons from Regular Grids",
    "description": "Regularly spaced grids containing continuous data are transformed\n    to contour polygons. A grid can be defined by a data.frame (x, y, value),\n    an 'sf' object or a raster from 'terra'.",
    "version": "0.3.0",
    "maintainer": "Timoth\u00e9e Giraud <timothee.giraud@cnrs.fr>",
    "author": "Timoth\u00e9e Giraud [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-1932-3323>)",
    "url": "https://github.com/riatelab/mapiso",
    "bug_reports": "https://github.com/riatelab/mapiso/issues/",
    "repository": "https://cran.r-project.org/package=mapiso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapiso Create Contour Polygons from Regular Grids Regularly spaced grids containing continuous data are transformed\n    to contour polygons. A grid can be defined by a data.frame (x, y, value),\n    an 'sf' object or a raster from 'terra'.  "
  },
  {
    "id": 15596,
    "package_name": "mappestRisk",
    "title": "Create Maps Forecasting Risk of Pest Occurrence",
    "description": "There are three different modules: (1) model fitting and selection \n    using a set of the most commonly used equations describing developmental \n    responses to temperature helped by already existing R packages ('rTPC') \n    and nonlinear regression model functions from 'nls.multstart' \n    (Padfield et al. 2021, <doi:10.1111/2041-210X.13585>), with visualization \n    of model predictions to guide ecological criteria for model selection; \n    (2) calculation of suitability thermal limits, which consist on a \n    temperature interval delimiting the optimal performance zone or suitability; \n    and (3) climatic data extraction and visualization inspired on previous \n    research (Taylor et al. 2019, <doi:10.1111/1365-2664.13455>), with either \n    exportable rasters, static map images or html, interactive maps.",
    "version": "0.1.2",
    "maintainer": "Dar\u00edo San-Segundo Molina <dario.ssm2@gmail.com>",
    "author": "Dar\u00edo San-Segundo Molina [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7831-9623>),\n  A. M\u00e1rcia Barbosa [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-8972-7713>),\n  Antonio Jes\u00fas P\u00e9rez-Luque [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-1747-0469>),\n  Francisco Rodr\u00edguez-S\u00e1nchez [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-7981-1599>)",
    "url": "https://github.com/EcologyR/mappestRisk,\nhttps://ecologyr.github.io/mappestRisk/",
    "bug_reports": "https://github.com/EcologyR/mappestRisk/issues",
    "repository": "https://cran.r-project.org/package=mappestRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mappestRisk Create Maps Forecasting Risk of Pest Occurrence There are three different modules: (1) model fitting and selection \n    using a set of the most commonly used equations describing developmental \n    responses to temperature helped by already existing R packages ('rTPC') \n    and nonlinear regression model functions from 'nls.multstart' \n    (Padfield et al. 2021, <doi:10.1111/2041-210X.13585>), with visualization \n    of model predictions to guide ecological criteria for model selection; \n    (2) calculation of suitability thermal limits, which consist on a \n    temperature interval delimiting the optimal performance zone or suitability; \n    and (3) climatic data extraction and visualization inspired on previous \n    research (Taylor et al. 2019, <doi:10.1111/1365-2664.13455>), with either \n    exportable rasters, static map images or html, interactive maps.  "
  },
  {
    "id": 15604,
    "package_name": "mapsRinteractive",
    "title": "Local Adaptation and Evaluation of Raster Maps",
    "description": "Local adaptation and evaluation of maps of continuous attributes in raster format by use of point location data.",
    "version": "2.0.1",
    "maintainer": "Kristin Persson <kristin.persson@slu.se>",
    "author": "Kristin Persson [aut, cre, cph],\n  Mats Soderstrom [ctb, cph],\n  John Mutua [ctb]",
    "url": "https://CRAN.R-project.org/package=mapsRinteractive",
    "bug_reports": "https://github.com/kriper0217/mapsRinteractive/issues",
    "repository": "https://cran.r-project.org/package=mapsRinteractive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapsRinteractive Local Adaptation and Evaluation of Raster Maps Local adaptation and evaluation of maps of continuous attributes in raster format by use of point location data.  "
  },
  {
    "id": 15644,
    "package_name": "maskr",
    "title": "Visual Class for Vectors with Non-Publishing Requirements",
    "description": "Create vectors with sticky flags for elements that should not be\n    displayed. Numeric vectors have basic subset and arithmetic methods\n    implemented.",
    "version": "0.1.0",
    "maintainer": "Ian Powell <powell.ian.n@gmail.com>",
    "author": "Ian Powell [cre, aut, cph]",
    "url": "https://github.com/inpowell/maskr",
    "bug_reports": "https://github.com/inpowell/maskr/issues",
    "repository": "https://cran.r-project.org/package=maskr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maskr Visual Class for Vectors with Non-Publishing Requirements Create vectors with sticky flags for elements that should not be\n    displayed. Numeric vectors have basic subset and arithmetic methods\n    implemented.  "
  },
  {
    "id": 15651,
    "package_name": "matchFeat",
    "title": "One-to-One Feature Matching",
    "description": "Statistical methods to match feature vectors between multiple datasets in a one-to-one fashion. Given a fixed number of classes/distributions, for each unit, exactly one vector of each class is observed without label. The goal is to label the feature vectors using each label exactly once so to produce the best match across datasets, e.g. by minimizing the variability within classes. Statistical solutions based on empirical loss functions and probabilistic modeling are provided. The 'Gurobi' software and its 'R' interface package are required for one of the package functions (match.2x()) and can be obtained at <https://www.gurobi.com/> (free academic license). For more details, refer to Degras (2022) <doi:10.1080/10618600.2022.2074429> \"Scalable feature matching for large data collections\" and Bandelt, Maas, and Spieksma (2004) <doi:10.1057/palgrave.jors.2601723> \"Local search heuristics for multi-index assignment problems with decomposable costs\".",
    "version": "1.0",
    "maintainer": "David Degras <david.degras@umb.edu>",
    "author": "David Degras",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matchFeat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matchFeat One-to-One Feature Matching Statistical methods to match feature vectors between multiple datasets in a one-to-one fashion. Given a fixed number of classes/distributions, for each unit, exactly one vector of each class is observed without label. The goal is to label the feature vectors using each label exactly once so to produce the best match across datasets, e.g. by minimizing the variability within classes. Statistical solutions based on empirical loss functions and probabilistic modeling are provided. The 'Gurobi' software and its 'R' interface package are required for one of the package functions (match.2x()) and can be obtained at <https://www.gurobi.com/> (free academic license). For more details, refer to Degras (2022) <doi:10.1080/10618600.2022.2074429> \"Scalable feature matching for large data collections\" and Bandelt, Maas, and Spieksma (2004) <doi:10.1057/palgrave.jors.2601723> \"Local search heuristics for multi-index assignment problems with decomposable costs\".  "
  },
  {
    "id": 15668,
    "package_name": "matlib",
    "title": "Matrix Functions for Teaching and Learning Linear Algebra and\nMultivariate Statistics",
    "description": "A collection of matrix functions for teaching and learning matrix\n    linear algebra as used in multivariate statistical methods. Many of these functions are\n    designed for tutorial purposes in learning matrix algebra ideas using R. In some\n    cases, functions are provided for concepts available elsewhere in R, but where\n    the function call or name is not obvious. In other cases, functions are provided\n    to show or demonstrate an algorithm. In addition, a collection of functions are\n    provided for drawing vector diagrams in 2D and 3D and for rendering matrix\n    expressions and equations in LaTeX.",
    "version": "1.0.1",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>),\n  John Fox [aut] (ORCID: <https://orcid.org/0000-0002-1196-8012>),\n  Phil Chalmers [aut] (ORCID: <https://orcid.org/0000-0001-5332-2810>),\n  Georges Monette [ctb] (ORCID: <https://orcid.org/0000-0003-0076-5532>),\n  Gaston Sanchez [ctb]",
    "url": "https://github.com/friendly/matlib,\nhttp://friendly.github.io/matlib/",
    "bug_reports": "https://github.com/friendly/matlib/issues",
    "repository": "https://cran.r-project.org/package=matlib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matlib Matrix Functions for Teaching and Learning Linear Algebra and\nMultivariate Statistics A collection of matrix functions for teaching and learning matrix\n    linear algebra as used in multivariate statistical methods. Many of these functions are\n    designed for tutorial purposes in learning matrix algebra ideas using R. In some\n    cases, functions are provided for concepts available elsewhere in R, but where\n    the function call or name is not obvious. In other cases, functions are provided\n    to show or demonstrate an algorithm. In addition, a collection of functions are\n    provided for drawing vector diagrams in 2D and 3D and for rendering matrix\n    expressions and equations in LaTeX.  "
  },
  {
    "id": 15675,
    "package_name": "matrixLaplacian",
    "title": "Normalized Laplacian Matrix and Laplacian Map",
    "description": "Constructs the normalized Laplacian matrix of a square matrix, returns the eigenvectors (singular vectors) and visualization of normalized Laplacian map.",
    "version": "1.0",
    "maintainer": "Tianhao Wu <tianhao.wu@yale.edu>",
    "author": "Tianhao Wu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matrixLaplacian",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrixLaplacian Normalized Laplacian Matrix and Laplacian Map Constructs the normalized Laplacian matrix of a square matrix, returns the eigenvectors (singular vectors) and visualization of normalized Laplacian map.  "
  },
  {
    "id": 15678,
    "package_name": "matrixStats",
    "title": "Functions that Apply to Rows and Columns of Matrices (and to\nVectors)",
    "description": "High-performing functions operating on rows and columns of matrices, e.g. col / rowMedians(), col / rowRanks(), and col / rowSds().  Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized.  There are also optimized vector-based methods, e.g. binMeans(), madDiff() and weightedMedian().",
    "version": "1.5.0",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph],\n  Constantin Ahlmann-Eltze [ctb],\n  Hector Corrada Bravo [ctb],\n  Robert Gentleman [ctb],\n  Jan Gleixner [ctb],\n  Peter Hickey [ctb],\n  Ola Hossjer [ctb],\n  Harris Jaffee [ctb],\n  Dongcan Jiang [ctb],\n  Peter Langfelder [ctb],\n  Brian Montgomery [ctb],\n  Angelina Panagopoulou [ctb],\n  Hugh Parsonage [ctb],\n  Jakob Peder Pettersen [ctb]",
    "url": "https://github.com/HenrikBengtsson/matrixStats",
    "bug_reports": "https://github.com/HenrikBengtsson/matrixStats/issues",
    "repository": "https://cran.r-project.org/package=matrixStats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrixStats Functions that Apply to Rows and Columns of Matrices (and to\nVectors) High-performing functions operating on rows and columns of matrices, e.g. col / rowMedians(), col / rowRanks(), and col / rowSds().  Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized.  There are also optimized vector-based methods, e.g. binMeans(), madDiff() and weightedMedian().  "
  },
  {
    "id": 15680,
    "package_name": "matrixTests",
    "title": "Fast Statistical Hypothesis Tests on Rows and Columns of\nMatrices",
    "description": "Functions to perform fast statistical hypothesis tests on rows/columns of matrices.\n  The main goals are: 1) speed via vectorization, 2) output that is detailed and easy to use,\n  3) compatibility with tests implemented in R (like those available in the 'stats' package).",
    "version": "0.2.3.1",
    "maintainer": "Karolis Koncevi\u010dius <karolis.koncevicius@gmail.com>",
    "author": "Karolis Koncevi\u010dius [aut, cre]",
    "url": "https://github.com/karoliskoncevicius/matrixTests",
    "bug_reports": "https://github.com/karoliskoncevicius/matrixTests/issues",
    "repository": "https://cran.r-project.org/package=matrixTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrixTests Fast Statistical Hypothesis Tests on Rows and Columns of\nMatrices Functions to perform fast statistical hypothesis tests on rows/columns of matrices.\n  The main goals are: 1) speed via vectorization, 2) output that is detailed and easy to use,\n  3) compatibility with tests implemented in R (like those available in the 'stats' package).  "
  },
  {
    "id": 15708,
    "package_name": "mazeinda",
    "title": "Monotonic Association on Zero-Inflated Data",
    "description": "Methods for calculating and testing the significance of\n  pairwise monotonic association from and based on the work of\n  Pimentel (2009) <doi:10.4135/9781412985291.n2>. Computation of association of vectors from one\n  or multiple sets can be performed in parallel thanks to the\n  packages 'foreach' and 'doMC'.",
    "version": "0.0.2",
    "maintainer": "Alice Albasi <albasialice@gmail.com>",
    "author": "Alice Albasi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mazeinda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mazeinda Monotonic Association on Zero-Inflated Data Methods for calculating and testing the significance of\n  pairwise monotonic association from and based on the work of\n  Pimentel (2009) <doi:10.4135/9781412985291.n2>. Computation of association of vectors from one\n  or multiple sets can be performed in parallel thanks to the\n  packages 'foreach' and 'doMC'.  "
  },
  {
    "id": 15716,
    "package_name": "mbg",
    "title": "Model-Based Geostatistics",
    "description": "\n    Modern model-based geostatistics for point-referenced data. This package provides a\n    simple interface to run spatial machine learning models and geostatistical models\n    that estimate a continuous (raster) surface from point-referenced outcomes and,\n    optionally, a set of raster covariates. The package also includes functions to\n    summarize raster outcomes by (polygon) region while preserving uncertainty.",
    "version": "1.1.0",
    "maintainer": "Nathaniel Henry <nat@henryspatialanalysis.com>",
    "author": "Nathaniel Henry [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8150-4988>),\n  Benjamin Mayala [aut]",
    "url": "https://henryspatialanalysis.github.io/mbg/",
    "bug_reports": "https://github.com/henryspatialanalysis/mbg/issues",
    "repository": "https://cran.r-project.org/package=mbg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mbg Model-Based Geostatistics \n    Modern model-based geostatistics for point-referenced data. This package provides a\n    simple interface to run spatial machine learning models and geostatistical models\n    that estimate a continuous (raster) surface from point-referenced outcomes and,\n    optionally, a set of raster covariates. The package also includes functions to\n    summarize raster outcomes by (polygon) region while preserving uncertainty.  "
  },
  {
    "id": 15745,
    "package_name": "mcgf",
    "title": "Markov Chain Gaussian Fields Simulation and Parameter Estimation",
    "description": "Simulating and estimating (regime-switching) Markov chain Gaussian \n    fields with covariance functions of the Gneiting class (Gneiting 2002) \n    <doi:10.1198/016214502760047113>. It supports parameter estimation by \n    weighted least squares and maximum likelihood methods, and produces Kriging \n    forecasts and intervals for existing and new locations.",
    "version": "1.1.1",
    "maintainer": "Tianxia Jia <tianxia.jia@ucalgary.ca>",
    "author": "Tianxia Jia [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5430-5019>)",
    "url": "https://github.com/tianxia-jia/mcgf,\nhttps://tianxia-jia.github.io/mcgf/",
    "bug_reports": "https://github.com/tianxia-jia/mcgf/issues",
    "repository": "https://cran.r-project.org/package=mcgf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcgf Markov Chain Gaussian Fields Simulation and Parameter Estimation Simulating and estimating (regime-switching) Markov chain Gaussian \n    fields with covariance functions of the Gneiting class (Gneiting 2002) \n    <doi:10.1198/016214502760047113>. It supports parameter estimation by \n    weighted least squares and maximum likelihood methods, and produces Kriging \n    forecasts and intervals for existing and new locations.  "
  },
  {
    "id": 15753,
    "package_name": "mcmc",
    "title": "Markov Chain Monte Carlo",
    "description": "Simulates continuous distributions of random vectors using\n    Markov chain Monte Carlo (MCMC).  Users specify the distribution by an\n    R function that evaluates the log unnormalized density.  Algorithms\n    are random walk Metropolis algorithm (function metrop), simulated\n    tempering (function temper), and morphometric random walk Metropolis\n    (Johnson and Geyer, 2012, <doi:10.1214/12-AOS1048>,\n    function morph.metrop),\n    which achieves geometric ergodicity by change of variable.",
    "version": "0.9-8",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "author": "Charles J. Geyer <geyer@umn.edu> and Leif T. Johnson\n     <ltjohnson@google.com>",
    "url": "http://www.stat.umn.edu/geyer/mcmc/,\nhttps://github.com/cjgeyer/mcmc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcmc Markov Chain Monte Carlo Simulates continuous distributions of random vectors using\n    Markov chain Monte Carlo (MCMC).  Users specify the distribution by an\n    R function that evaluates the log unnormalized density.  Algorithms\n    are random walk Metropolis algorithm (function metrop), simulated\n    tempering (function temper), and morphometric random walk Metropolis\n    (Johnson and Geyer, 2012, <doi:10.1214/12-AOS1048>,\n    function morph.metrop),\n    which achieves geometric ergodicity by change of variable.  "
  },
  {
    "id": 15781,
    "package_name": "mda",
    "title": "Mixture and Flexible Discriminant Analysis",
    "description": "Mixture and flexible discriminant analysis, multivariate\n        adaptive regression splines (MARS), BRUTO, and vector-response smoothing splines.\n\tHastie, Tibshirani and Friedman (2009) \"Elements of Statistical Learning (second edition, chap 12)\" Springer, New York. ",
    "version": "0.5-5",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "author": "Trevor Hastie [aut, cre] (Original co-author of the S package `mda`),\n  Robert Tibshirani [aut] (Original co-author of the S package `mda`),\n  Balasubramanian Narasimhan [ctb] (Contributed to the upgrading of code),\n  Friedrich Leisch [ctb] (Original R port from the S package),\n  Kurt Hornik [ctb] (Original R port from the S package),\n  Brian Ripley [ctb] (Original R port from the S package)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mda Mixture and Flexible Discriminant Analysis Mixture and flexible discriminant analysis, multivariate\n        adaptive regression splines (MARS), BRUTO, and vector-response smoothing splines.\n\tHastie, Tibshirani and Friedman (2009) \"Elements of Statistical Learning (second edition, chap 12)\" Springer, New York.   "
  },
  {
    "id": 15794,
    "package_name": "mdpeer",
    "title": "Graph-Constrained Regression with Enhanced Regularization\nParameters Selection",
    "description": "Provides graph-constrained regression methods in which\n    regularization parameters are selected automatically via estimation of\n    equivalent Linear Mixed Model formulation. 'riPEER' (ridgified Partially\n    Empirical Eigenvectors for Regression) method employs a penalty term being\n    a linear combination of graph-originated and ridge-originated penalty terms,\n    whose two regularization parameters are ML estimators from corresponding\n    Linear Mixed Model solution; a graph-originated penalty term allows imposing\n    similarity between coefficients based on graph information given whereas\n    additional ridge-originated penalty term facilitates parameters estimation:\n    it reduces computational issues arising from singularity in a graph-originated\n    penalty matrix and yields plausible results in situations when graph information\n    is not informative. 'riPEERc' (ridgified Partially Empirical Eigenvectors\n    for Regression with constant) method utilizes addition of a diagonal matrix\n    multiplied by a predefined (small) scalar to handle the non-invertibility of\n    a graph Laplacian matrix. 'vrPEER' (variable reducted PEER) method performs\n    variable-reduction procedure to handle the non-invertibility of a graph\n    Laplacian matrix.",
    "version": "1.0.1",
    "maintainer": "Marta Karas <marta.karass@gmail.com>",
    "author": "Marta Karas [aut, cre],\n  Damian Brzyski [ctb],\n  Jaroslaw Harezlak [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mdpeer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mdpeer Graph-Constrained Regression with Enhanced Regularization\nParameters Selection Provides graph-constrained regression methods in which\n    regularization parameters are selected automatically via estimation of\n    equivalent Linear Mixed Model formulation. 'riPEER' (ridgified Partially\n    Empirical Eigenvectors for Regression) method employs a penalty term being\n    a linear combination of graph-originated and ridge-originated penalty terms,\n    whose two regularization parameters are ML estimators from corresponding\n    Linear Mixed Model solution; a graph-originated penalty term allows imposing\n    similarity between coefficients based on graph information given whereas\n    additional ridge-originated penalty term facilitates parameters estimation:\n    it reduces computational issues arising from singularity in a graph-originated\n    penalty matrix and yields plausible results in situations when graph information\n    is not informative. 'riPEERc' (ridgified Partially Empirical Eigenvectors\n    for Regression with constant) method utilizes addition of a diagonal matrix\n    multiplied by a predefined (small) scalar to handle the non-invertibility of\n    a graph Laplacian matrix. 'vrPEER' (variable reducted PEER) method performs\n    variable-reduction procedure to handle the non-invertibility of a graph\n    Laplacian matrix.  "
  },
  {
    "id": 15801,
    "package_name": "meanr",
    "title": "Sentiment Analysis Scorer",
    "description": "Sentiment analysis is a popular technique in text mining that\n    attempts to determine the emotional state of some text. We provide a new\n    implementation of a common method for computing sentiment, whereby words are\n    scored as positive or negative according to a dictionary lookup. Then the\n    sum of those scores is returned for the document. We use the 'Hu' and 'Liu'\n    sentiment dictionary ('Hu' and 'Liu', 2004) <doi:10.1145/1014052.1014073>\n    for determining sentiment. The scoring function is 'vectorized' by document,\n    and scores for multiple documents are computed in parallel via 'OpenMP'.",
    "version": "0.1-6",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre]",
    "url": "https://github.com/wrathematics/meanr",
    "bug_reports": "https://github.com/wrathematics/meanr/issues",
    "repository": "https://cran.r-project.org/package=meanr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meanr Sentiment Analysis Scorer Sentiment analysis is a popular technique in text mining that\n    attempts to determine the emotional state of some text. We provide a new\n    implementation of a common method for computing sentiment, whereby words are\n    scored as positive or negative according to a dictionary lookup. Then the\n    sum of those scores is returned for the document. We use the 'Hu' and 'Liu'\n    sentiment dictionary ('Hu' and 'Liu', 2004) <doi:10.1145/1014052.1014073>\n    for determining sentiment. The scoring function is 'vectorized' by document,\n    and scores for multiple documents are computed in parallel via 'OpenMP'.  "
  },
  {
    "id": 15816,
    "package_name": "medfate",
    "title": "Mediterranean Forest Simulation",
    "description": "Simulate Mediterranean forest functioning and dynamics using cohort-based description of vegetation [De Caceres et al. (2015) <doi:10.1016/j.agrformet.2015.06.012>; De Caceres et al. (2021) <doi:10.1016/j.agrformet.2020.108233>].",
    "version": "4.8.4",
    "maintainer": "Miquel De C\u00e1ceres <miquelcaceres@gmail.com>",
    "author": "Miquel De C\u00e1ceres [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7132-2080>),\n  Nicolas Martin-StPaul [aut] (ORCID:\n    <https://orcid.org/0000-0001-7574-0108>),\n  V\u00edctor Granda [aut] (ORCID: <https://orcid.org/0000-0002-0469-1991>),\n  Antoine Cabon [aut] (ORCID: <https://orcid.org/0000-0001-6426-1726>),\n  Ars\u00e8ne Druel [aut] (ORCID: <https://orcid.org/0000-0002-3938-0085>),\n  Julien Ruffault [aut] (ORCID: <https://orcid.org/0000-0003-3647-8172>),\n  Jordi Mart\u00ednez-Vilalta [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2332-7298>),\n  Maurizio Mencuccini [ctb] (ORCID:\n    <https://orcid.org/0000-0003-0840-1477>),\n  Fran\u00e7ois Pimont [ctb] (ORCID: <https://orcid.org/0000-0002-9842-6207>),\n  Herv\u00e9 Cochard [ctb] (ORCID: <https://orcid.org/0000-0002-2727-7072>),\n  Aitor Am\u00e9ztegui [ctb] (ORCID: <https://orcid.org/0000-0003-2006-1559>),\n  L\u00e9a Veuillen [ctb] (ORCID: <https://orcid.org/0000-0002-2790-3627>),\n  Shengli Huang [ctb] (ORCID: <https://orcid.org/0000-0003-3927-7042>),\n  John Burkardt [cph] (Copyright holder of C++ code in 'incgamma.cpp')",
    "url": "https://emf-creaf.github.io/medfate/,\nhttps://github.com/emf-creaf/medfate",
    "bug_reports": "https://github.com/emf-creaf/medfate/issues",
    "repository": "https://cran.r-project.org/package=medfate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "medfate Mediterranean Forest Simulation Simulate Mediterranean forest functioning and dynamics using cohort-based description of vegetation [De Caceres et al. (2015) <doi:10.1016/j.agrformet.2015.06.012>; De Caceres et al. (2021) <doi:10.1016/j.agrformet.2020.108233>].  "
  },
  {
    "id": 15817,
    "package_name": "medfateland",
    "title": "Mediterranean Landscape Simulation",
    "description": "Simulate forest hydrology, forest function and dynamics over landscapes [De Caceres et al. (2015) <doi:10.1016/j.agrformet.2015.06.012>]. Parallelization is allowed in several simulation functions and simulations may be conducted including spatial processes such as lateral water transfer and seed dispersal.",
    "version": "2.8.1",
    "maintainer": "Miquel De C\u00e1ceres <miquelcaceres@gmail.com>",
    "author": "Miquel De C\u00e1ceres [aut, cre],\n  Aitor Am\u00e9ztegui [aut] (ORCID: <https://orcid.org/0000-0003-2006-1559>),\n  Mar\u00eda Gonz\u00e1lez [aut] (ORCID: <https://orcid.org/0000-0002-2227-8404>),\n  N\u00faria Aquilu\u00e9 [aut],\n  Daniel Caviedes-Voulli\u00e8me [aut],\n  Mario Morales-Hern\u00e1ndez [aut],\n  Mario Beltr\u00e1n [ctb],\n  Rodrigo Balaguer-Romano [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2808-6777>),\n  Roberto Molowny-Horas [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2626-6379>)",
    "url": "https://emf-creaf.github.io/medfateland/,\nhttps://github.com/emf-creaf/medfateland",
    "bug_reports": "https://github.com/emf-creaf/medfateland/issues",
    "repository": "https://cran.r-project.org/package=medfateland",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "medfateland Mediterranean Landscape Simulation Simulate forest hydrology, forest function and dynamics over landscapes [De Caceres et al. (2015) <doi:10.1016/j.agrformet.2015.06.012>]. Parallelization is allowed in several simulation functions and simulations may be conducted including spatial processes such as lateral water transfer and seed dispersal.  "
  },
  {
    "id": 15838,
    "package_name": "memgene",
    "title": "Spatial Pattern Detection in Genetic Distance Data Using Moran's\nEigenvector Maps",
    "description": "Can detect relatively weak spatial genetic patterns by using Moran's Eigenvector Maps (MEM) to extract only the spatial component of genetic variation.  Has applications in landscape genetics where the movement and dispersal of organisms are studied using neutral genetic variation.",
    "version": "1.0.3",
    "maintainer": "Paul Galpern <pgalpern@ucalgary.ca>",
    "author": "Pedro Peres-Neto [aut],\n  Paul Galpern [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=memgene",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "memgene Spatial Pattern Detection in Genetic Distance Data Using Moran's\nEigenvector Maps Can detect relatively weak spatial genetic patterns by using Moran's Eigenvector Maps (MEM) to extract only the spatial component of genetic variation.  Has applications in landscape genetics where the movement and dispersal of organisms are studied using neutral genetic variation.  "
  },
  {
    "id": 15862,
    "package_name": "metR",
    "title": "Tools for Easier Analysis of Meteorological Fields",
    "description": "Many useful functions and extensions for dealing with\n    meteorological data in the tidy data framework. Extends 'ggplot2' for\n    better plotting of scalar and vector fields and provides commonly used\n    analysis methods in the atmospheric sciences.",
    "version": "0.18.3",
    "maintainer": "Elio Campitelli <eliocampitelli@gmail.com>",
    "author": "Elio Campitelli [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-7742-9230>)",
    "url": "https://eliocamp.github.io/metR/, https://github.com/eliocamp/metR",
    "bug_reports": "https://github.com/eliocamp/metR/issues",
    "repository": "https://cran.r-project.org/package=metR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metR Tools for Easier Analysis of Meteorological Fields Many useful functions and extensions for dealing with\n    meteorological data in the tidy data framework. Extends 'ggplot2' for\n    better plotting of scalar and vector fields and provides commonly used\n    analysis methods in the atmospheric sciences.  "
  },
  {
    "id": 15881,
    "package_name": "metaSVR",
    "title": "Support Vector Regression with Metaheuristic Algorithms\nOptimization",
    "description": "Provides a hybrid modeling framework combining Support Vector Regression (SVR) with metaheuristic optimization algorithms, including the Archimedes Optimization Algorithm (AO) (Hashim et al. (2021) <doi:10.1007/s10489-020-01893-z>), Coot Bird Optimization (CBO) (Naruei & Keynia (2021) <doi:10.1016/j.eswa.2021.115352>), and their hybrid (AOCBO), as well as several others such as Harris Hawks Optimization (HHO) (Heidari et al. (2019) <doi:10.1016/j.future.2019.02.028>), Gray Wolf Optimizer (GWO) (Mirjalili et al. (2014) <doi:10.1016/j.advengsoft.2013.12.007>), Ant Lion Optimization (ALO) (Mirjalili (2015) <doi:10.1016/j.advengsoft.2015.01.010>), and Enhanced Harris Hawk Optimization with Coot Bird Optimization (EHHOCBO) (Cui et al. (2023) <doi:10.32604/cmes.2023.026019>). The package enables automatic tuning of SVR hyperparameters (cost, gamma, and epsilon) to enhance prediction performance. Suitable for regression tasks in domains such as renewable energy forecasting and hourly data prediction. For more details about implementation and parameter bounds see: Setiawan et al. (2021) <doi:10.1016/j.procs.2020.12.003> and Liu et al. (2018) <doi:10.1155/2018/6076475>.  ",
    "version": "0.1.0",
    "maintainer": "Rechtiana Putri Arini <rparini17@gmail.com>",
    "author": "Rechtiana Putri Arini [aut, cre],\n  Robert Kurniawan [aut],\n  I Nyoman Setiawan [aut],\n  Zulhan Andika Asyraf [aut]",
    "url": "https://github.com/rechtianaputri/metaSVR",
    "bug_reports": "https://github.com/rechtianaputri/metaSVR/issues",
    "repository": "https://cran.r-project.org/package=metaSVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaSVR Support Vector Regression with Metaheuristic Algorithms\nOptimization Provides a hybrid modeling framework combining Support Vector Regression (SVR) with metaheuristic optimization algorithms, including the Archimedes Optimization Algorithm (AO) (Hashim et al. (2021) <doi:10.1007/s10489-020-01893-z>), Coot Bird Optimization (CBO) (Naruei & Keynia (2021) <doi:10.1016/j.eswa.2021.115352>), and their hybrid (AOCBO), as well as several others such as Harris Hawks Optimization (HHO) (Heidari et al. (2019) <doi:10.1016/j.future.2019.02.028>), Gray Wolf Optimizer (GWO) (Mirjalili et al. (2014) <doi:10.1016/j.advengsoft.2013.12.007>), Ant Lion Optimization (ALO) (Mirjalili (2015) <doi:10.1016/j.advengsoft.2015.01.010>), and Enhanced Harris Hawk Optimization with Coot Bird Optimization (EHHOCBO) (Cui et al. (2023) <doi:10.32604/cmes.2023.026019>). The package enables automatic tuning of SVR hyperparameters (cost, gamma, and epsilon) to enhance prediction performance. Suitable for regression tasks in domains such as renewable energy forecasting and hourly data prediction. For more details about implementation and parameter bounds see: Setiawan et al. (2021) <doi:10.1016/j.procs.2020.12.003> and Liu et al. (2018) <doi:10.1155/2018/6076475>.    "
  },
  {
    "id": 15940,
    "package_name": "meteo",
    "title": "RFSI & STRK Interpolation for Meteo and Environmental Variables",
    "description": "Random Forest Spatial Interpolation (RFSI, Sekuli\u0107 et al. (2020) <doi:10.3390/rs12101687>) and spatio-temporal geostatistical (spatio-temporal regression Kriging (STRK)) interpolation for meteorological (Kilibarda et al. (2014) <doi:10.1002/2013JD020803>, Sekuli\u0107 et al. (2020) <doi:10.1007/s00704-019-03077-3>) and other environmental variables. Contains global spatio-temporal models calculated using publicly available data.",
    "version": "2.0-3",
    "maintainer": "Aleksandar Sekuli\u0107 <asekulic@grf.bg.ac.rs>",
    "author": "Milan Kilibarda [aut] (ORCID: <https://orcid.org/0000-0002-2930-3596>),\n  Aleksandar Sekuli\u0107 [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5515-2779>),\n  Tomislav Hengl [ctb],\n  Edzer Pebesma [ctb],\n  Benedikt Graeler [ctb]",
    "url": "https://www.r-pkg.org/pkg/meteo,\nhttps://r-forge.r-project.org/projects/meteo/,\nhttps://github.com/AleksandarSekulic/Rmeteo",
    "bug_reports": "https://github.com/AleksandarSekulic/Rmeteo/issues",
    "repository": "https://cran.r-project.org/package=meteo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meteo RFSI & STRK Interpolation for Meteo and Environmental Variables Random Forest Spatial Interpolation (RFSI, Sekuli\u0107 et al. (2020) <doi:10.3390/rs12101687>) and spatio-temporal geostatistical (spatio-temporal regression Kriging (STRK)) interpolation for meteorological (Kilibarda et al. (2014) <doi:10.1002/2013JD020803>, Sekuli\u0107 et al. (2020) <doi:10.1007/s00704-019-03077-3>) and other environmental variables. Contains global spatio-temporal models calculated using publicly available data.  "
  },
  {
    "id": 15942,
    "package_name": "meteoForecast",
    "title": "Numerical Weather Predictions",
    "description": "Access to several Numerical Weather Prediction services both in raster format and as a time series for a location. Currently it works with GFS <https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast>, MeteoGalicia <https://www.meteogalicia.gal/web/modelos/threddsIndex.action>, NAM <https://www.ncei.noaa.gov/products/weather-climate-models/north-american-mesoscale>, and RAP <https://www.ncei.noaa.gov/products/weather-climate-models/rapid-refresh-update>.",
    "version": "0.57",
    "maintainer": "Oscar Perpinan Lamigueiro <oscar.perpinan@upm.es>",
    "author": "Oscar Perpinan Lamigueiro [cre, aut],\n  Marcelo Pinho Almeida [ctb]",
    "url": "https://codeberg.org/oscarperpinan/meteoForecast",
    "bug_reports": "https://codeberg.org/oscarperpinan/meteoForecast/issues",
    "repository": "https://cran.r-project.org/package=meteoForecast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meteoForecast Numerical Weather Predictions Access to several Numerical Weather Prediction services both in raster format and as a time series for a location. Currently it works with GFS <https://www.ncei.noaa.gov/products/weather-climate-models/global-forecast>, MeteoGalicia <https://www.meteogalicia.gal/web/modelos/threddsIndex.action>, NAM <https://www.ncei.noaa.gov/products/weather-climate-models/north-american-mesoscale>, and RAP <https://www.ncei.noaa.gov/products/weather-climate-models/rapid-refresh-update>.  "
  },
  {
    "id": 15956,
    "package_name": "mewAvg",
    "title": "A Fixed Memeory Moving Expanding Window Average",
    "description": "Compute the average of a sequence of random vectors\n  in a moving expanding window using a fixed amount of memory.",
    "version": "0.3.1",
    "maintainer": "Adam L. Pintar <adam.pintar@nist.gov>",
    "author": "Adam L. Pintar and Zachary H. Levine",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mewAvg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mewAvg A Fixed Memeory Moving Expanding Window Average Compute the average of a sequence of random vectors\n  in a moving expanding window using a fixed amount of memory.  "
  },
  {
    "id": 15980,
    "package_name": "mgwrsar",
    "title": "GWR, Mixed GWR and Multiscale GWR with Spatial Autocorrelation",
    "description": "Functions for computing (Mixed and Multiscale) Geographically Weighted Regression with spatial autocorrelation, Geniaux and Martinetti (2017) <doi:10.1016/j.regsciurbeco.2017.04.001>.",
    "version": "1.1",
    "maintainer": "Ghislain Geniaux <ghislain.geniaux@inrae.fr>",
    "author": "Ghislain Geniaux [aut, cre],\n  Davide Martinetti [aut],\n  C\u00e9sar Martinez [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mgwrsar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mgwrsar GWR, Mixed GWR and Multiscale GWR with Spatial Autocorrelation Functions for computing (Mixed and Multiscale) Geographically Weighted Regression with spatial autocorrelation, Geniaux and Martinetti (2017) <doi:10.1016/j.regsciurbeco.2017.04.001>.  "
  },
  {
    "id": 15981,
    "package_name": "mhazard",
    "title": "Nonparametric and Semiparametric Methods for Multivariate\nFailure Time Data",
    "description": "Nonparametric survival function estimates and semiparametric\n    regression for the multivariate failure time data with right-censoring.\n    For nonparametric survival function estimates, the Volterra, Dabrowska,\n    and Prentice-Cai estimates for bivariate failure time data may be\n    computed as well as the Dabrowska estimate for the trivariate failure\n    time data. Bivariate marginal hazard rate regression can be fitted for\n    the bivariate failure time data. Functions are also provided to compute\n    (bootstrap) confidence intervals and plot the estimates of the bivariate\n    survival function. For details, see \"The Statistical Analysis of\n    Multivariate Failure Time Data: A Marginal Modeling Approach\", Prentice,\n    R., Zhao, S. (2019, ISBN: 978-1-4822-5657-4), CRC Press.",
    "version": "0.2.3",
    "maintainer": "Eric Bair <eric.bair@sciome.com>",
    "author": "Eric Bair [aut, cre],\n  Taylor Petty [aut],\n  Shanshan Zhao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mhazard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mhazard Nonparametric and Semiparametric Methods for Multivariate\nFailure Time Data Nonparametric survival function estimates and semiparametric\n    regression for the multivariate failure time data with right-censoring.\n    For nonparametric survival function estimates, the Volterra, Dabrowska,\n    and Prentice-Cai estimates for bivariate failure time data may be\n    computed as well as the Dabrowska estimate for the trivariate failure\n    time data. Bivariate marginal hazard rate regression can be fitted for\n    the bivariate failure time data. Functions are also provided to compute\n    (bootstrap) confidence intervals and plot the estimates of the bivariate\n    survival function. For details, see \"The Statistical Analysis of\n    Multivariate Failure Time Data: A Marginal Modeling Approach\", Prentice,\n    R., Zhao, S. (2019, ISBN: 978-1-4822-5657-4), CRC Press.  "
  },
  {
    "id": 16033,
    "package_name": "micvar",
    "title": "Order Selection in Vector Autoregression by Mean Square\nInformation Criteria",
    "description": "Implements order selection for Vector Autoregressive (VAR) models using the Mean Square Information Criterion (MIC). Unlike standard methods such as AIC and BIC, MIC is likelihood-free. This method consistently estimates VAR order and has robust performance under model misspecification. For more details, see Hellstern and Shojaie (2025) <doi:10.48550/arXiv.2511.19761>.",
    "version": "0.1.0",
    "maintainer": "Michael Hellstern <mikeh1@uw.edu>",
    "author": "Michael Hellstern [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=micvar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "micvar Order Selection in Vector Autoregression by Mean Square\nInformation Criteria Implements order selection for Vector Autoregressive (VAR) models using the Mean Square Information Criterion (MIC). Unlike standard methods such as AIC and BIC, MIC is likelihood-free. This method consistently estimates VAR order and has robust performance under model misspecification. For more details, see Hellstern and Shojaie (2025) <doi:10.48550/arXiv.2511.19761>.  "
  },
  {
    "id": 16047,
    "package_name": "mig",
    "title": "Multivariate Inverse Gaussian Distribution",
    "description": "Provides utilities for estimation for the multivariate inverse Gaussian distribution of Minami (2003) <doi:10.1081/STA-120025379>, including random vector generation and explicit estimators of the location vector and scale matrix. The package implements kernel density estimators discussed in Belzile, Desgagnes, Genest and Ouimet (2024) <doi:10.48550/arXiv.2209.04757> for smoothing multivariate data on half-spaces.",
    "version": "2.0",
    "maintainer": "Leo Belzile <belzilel@gmail.com>",
    "author": "Leo Belzile [aut, cre] (ORCID: <https://orcid.org/0000-0002-9135-014X>),\n  Frederic Ouimet [aut] (ORCID: <https://orcid.org/0000-0001-7933-5265>)",
    "url": "",
    "bug_reports": "https://github.com/lbelzile/mig/issues",
    "repository": "https://cran.r-project.org/package=mig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mig Multivariate Inverse Gaussian Distribution Provides utilities for estimation for the multivariate inverse Gaussian distribution of Minami (2003) <doi:10.1081/STA-120025379>, including random vector generation and explicit estimators of the location vector and scale matrix. The package implements kernel density estimators discussed in Belzile, Desgagnes, Genest and Ouimet (2024) <doi:10.48550/arXiv.2209.04757> for smoothing multivariate data on half-spaces.  "
  },
  {
    "id": 16055,
    "package_name": "mildsvm",
    "title": "Multiple-Instance Learning with Support Vector Machines",
    "description": "Weakly supervised (WS), multiple instance (MI) data lives in\n    numerous interesting applications such as drug discovery, object\n    detection, and tumor prediction on whole slide images. The 'mildsvm'\n    package provides an easy way to learn from this data by training\n    Support Vector Machine (SVM)-based classifiers. It also contains\n    helpful functions for building and printing multiple instance data\n    frames. The core methods from 'mildsvm' come from the following\n    references: Kent and Yu (2024) <doi:10.1214/24-AOAS1876>; Xiao, Liu, and Hao\n    (2018) <doi:10.1109/TNNLS.2017.2766164>; Muandet et al. (2012)\n    <https://proceedings.neurips.cc/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf>;\n    Chu and Keerthi (2007) <doi:10.1162/neco.2007.19.3.792>; and Andrews\n    et al. (2003)\n    <https://papers.nips.cc/paper/2232-support-vector-machines-for-multiple-instance-learning.pdf>.\n    Many functions use the 'Gurobi' optimization back-end to improve the\n    optimization problem speed; the 'gurobi' R package and associated\n    software can be downloaded from <https://www.gurobi.com> after\n    obtaining a license.",
    "version": "0.4.1",
    "maintainer": "Sean Kent <skent259@gmail.com>",
    "author": "Sean Kent [aut, cre] (ORCID: <https://orcid.org/0000-0001-8697-9069>),\n  Yifei Liou [aut]",
    "url": "https://github.com/skent259/mildsvm",
    "bug_reports": "https://github.com/skent259/mildsvm/issues",
    "repository": "https://cran.r-project.org/package=mildsvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mildsvm Multiple-Instance Learning with Support Vector Machines Weakly supervised (WS), multiple instance (MI) data lives in\n    numerous interesting applications such as drug discovery, object\n    detection, and tumor prediction on whole slide images. The 'mildsvm'\n    package provides an easy way to learn from this data by training\n    Support Vector Machine (SVM)-based classifiers. It also contains\n    helpful functions for building and printing multiple instance data\n    frames. The core methods from 'mildsvm' come from the following\n    references: Kent and Yu (2024) <doi:10.1214/24-AOAS1876>; Xiao, Liu, and Hao\n    (2018) <doi:10.1109/TNNLS.2017.2766164>; Muandet et al. (2012)\n    <https://proceedings.neurips.cc/paper/2012/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf>;\n    Chu and Keerthi (2007) <doi:10.1162/neco.2007.19.3.792>; and Andrews\n    et al. (2003)\n    <https://papers.nips.cc/paper/2232-support-vector-machines-for-multiple-instance-learning.pdf>.\n    Many functions use the 'Gurobi' optimization back-end to improve the\n    optimization problem speed; the 'gurobi' R package and associated\n    software can be downloaded from <https://www.gurobi.com> after\n    obtaining a license.  "
  },
  {
    "id": 16161,
    "package_name": "mixedfact",
    "title": "Generate and Analyze Mixed-Level Blocked Factorial Designs",
    "description": "Generates blocked designs for mixed-level factorial experiments for a \n    given block size. Internally, it uses finite-field based, collapsed, and heuristic \n    methods to construct block structures that minimize confounding between block \n    effects and factorial effects. The package creates the full treatment combination \n    table, partitions runs into blocks, and computes detailed confounding diagnostics \n    for main effects and two-factor interactions. It also checks orthogonal factorial \n    structure (OFS) and computes efficiencies of factorial effects using the methods \n    of Nair and Rao (1948) <doi:10.1111/j.2517-6161.1948.tb00005.x>. When OFS is not satisfied but \n    the design has equal treatment replications and equal block sizes, a general method \n    based on the C-matrix and custom contrast vectors is used to compute efficiencies. \n    The output includes the generated design, finite-field metadata, confounding \n    summaries, OFS diagnostics, and efficiency results.",
    "version": "0.1.1",
    "maintainer": "Sukanta Dash <sukanta.iasri@gmail.com>",
    "author": "Archana A [aut],\n  Sukanta Dash [aut, cre],\n  Anil Kumar [aut],\n  Medram Verma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mixedfact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixedfact Generate and Analyze Mixed-Level Blocked Factorial Designs Generates blocked designs for mixed-level factorial experiments for a \n    given block size. Internally, it uses finite-field based, collapsed, and heuristic \n    methods to construct block structures that minimize confounding between block \n    effects and factorial effects. The package creates the full treatment combination \n    table, partitions runs into blocks, and computes detailed confounding diagnostics \n    for main effects and two-factor interactions. It also checks orthogonal factorial \n    structure (OFS) and computes efficiencies of factorial effects using the methods \n    of Nair and Rao (1948) <doi:10.1111/j.2517-6161.1948.tb00005.x>. When OFS is not satisfied but \n    the design has equal treatment replications and equal block sizes, a general method \n    based on the C-matrix and custom contrast vectors is used to compute efficiencies. \n    The output includes the generated design, finite-field metadata, confounding \n    summaries, OFS diagnostics, and efficiency results.  "
  },
  {
    "id": 16184,
    "package_name": "mkssd",
    "title": "Efficient Multi-Level k-Circulant Supersaturated Designs",
    "description": "Generates efficient balanced non-aliased multi-level k-circulant supersaturated designs by interchanging the elements of the generator vector. Attempts to generate a supersaturated design that has chisquare efficiency more than user specified efficiency level (mef). Displays the progress of generation of an efficient multi-level k-circulant design through a progress bar. The progress of 100% means that one full round of interchange is completed. More than one full round (typically 4-5 rounds) of interchange may be required for larger designs. ",
    "version": "1.2",
    "maintainer": "B N Mandal <mandal.stat@gmail.com>",
    "author": "B N Mandal <mandal.stat@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mkssd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mkssd Efficient Multi-Level k-Circulant Supersaturated Designs Generates efficient balanced non-aliased multi-level k-circulant supersaturated designs by interchanging the elements of the generator vector. Attempts to generate a supersaturated design that has chisquare efficiency more than user specified efficiency level (mef). Displays the progress of generation of an efficient multi-level k-circulant design through a progress bar. The progress of 100% means that one full round of interchange is completed. More than one full round (typically 4-5 rounds) of interchange may be required for larger designs.   "
  },
  {
    "id": 16185,
    "package_name": "mlVAR",
    "title": "Multi-Level Vector Autoregression",
    "description": "Estimates the multi-level vector autoregression model on time-series data.\n             Three network structures are obtained: temporal networks, contemporaneous\n             networks and between-subjects networks.",
    "version": "0.5.2",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "author": "Sacha Epskamp [aut, cre],\n  Marie K. Deserno [aut],\n  Laura F. Bringmann [aut],\n  Myrthe Veenman [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlVAR Multi-Level Vector Autoregression Estimates the multi-level vector autoregression model on time-series data.\n             Three network structures are obtained: temporal networks, contemporaneous\n             networks and between-subjects networks.  "
  },
  {
    "id": 16194,
    "package_name": "mlearning",
    "title": "Machine Learning Algorithms with Unified Interface and Confusion\nMatrices",
    "description": "A unified interface is provided to various machine learning\n  algorithms like linear or quadratic discriminant analysis, k-nearest\n  neighbors, random forest, support vector machine, ... It allows to train,\n  test, and apply cross-validation using similar functions and function\n  arguments with a minimalist and clean, formula-based interface. Missing data\n  are processed the same way as base and stats R functions for all algorithms,\n  both in training and testing. Confusion matrices are also provided with a rich\n  set of metrics calculated and a few specific plots.",
    "version": "1.2.1",
    "maintainer": "Philippe Grosjean <phgrosjean@sciviews.org>",
    "author": "Philippe Grosjean [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2694-9471>),\n  Kevin Denis [aut]",
    "url": "https://www.sciviews.org/mlearning/",
    "bug_reports": "https://github.com/SciViews/mlearning/issues",
    "repository": "https://cran.r-project.org/package=mlearning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlearning Machine Learning Algorithms with Unified Interface and Confusion\nMatrices A unified interface is provided to various machine learning\n  algorithms like linear or quadratic discriminant analysis, k-nearest\n  neighbors, random forest, support vector machine, ... It allows to train,\n  test, and apply cross-validation using similar functions and function\n  arguments with a minimalist and clean, formula-based interface. Missing data\n  are processed the same way as base and stats R functions for all algorithms,\n  both in training and testing. Confusion matrices are also provided with a rich\n  set of metrics calculated and a few specific plots.  "
  },
  {
    "id": 16238,
    "package_name": "mlr3spatial",
    "title": "Support for Spatial Objects Within the 'mlr3' Ecosystem",
    "description": "Extends the 'mlr3' ML framework with methods for spatial\n    objects. Data storage and prediction are supported for packages\n    'terra', 'raster' and 'stars'.",
    "version": "0.6.1",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Marc Becker [aut, cre] (ORCID: <https://orcid.org/0000-0002-8115-0400>),\n  Patrick Schratz [aut] (ORCID: <https://orcid.org/0000-0003-0748-6624>)",
    "url": "https://mlr3spatial.mlr-org.com,\nhttps://github.com/mlr-org/mlr3spatial",
    "bug_reports": "https://github.com/mlr-org/mlr3spatial/issues",
    "repository": "https://cran.r-project.org/package=mlr3spatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3spatial Support for Spatial Objects Within the 'mlr3' Ecosystem Extends the 'mlr3' ML framework with methods for spatial\n    objects. Data storage and prediction are supported for packages\n    'terra', 'raster' and 'stars'.  "
  },
  {
    "id": 16252,
    "package_name": "mlsjunkgen",
    "title": "Use the MLS Junk Generator Algorithm to Generate a Stream of\nPseudo-Random Numbers",
    "description": "Generate a stream of pseudo-random numbers generated using the MLS \n    Junk Generator algorithm. Functions exist to generate single pseudo-random \n    numbers as well as a vector, data frame, or matrix of pseudo-random numbers.",
    "version": "0.1.2",
    "maintainer": "Steve Myles <steve@mylesandmyles.info>",
    "author": "Steve Myles [aut, cre]",
    "url": "https://stevemyles.site/mlsjunkgen/,\nhttps://github.com/scumdogsteev/mlsjunkgen",
    "bug_reports": "https://github.com/scumdogsteev/mlsjunkgen/issues",
    "repository": "https://cran.r-project.org/package=mlsjunkgen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlsjunkgen Use the MLS Junk Generator Algorithm to Generate a Stream of\nPseudo-Random Numbers Generate a stream of pseudo-random numbers generated using the MLS \n    Junk Generator algorithm. Functions exist to generate single pseudo-random \n    numbers as well as a vector, data frame, or matrix of pseudo-random numbers.  "
  },
  {
    "id": 16253,
    "package_name": "mlspatial",
    "title": "Machine Learning and Mapping for Spatial Epidemiology",
    "description": "Provides tools for the integration, visualisation, and modelling of spatial epidemiological data using the method described in Azeez, A., & Noel, C. (2025). 'Predictive Modelling and Spatial Distribution of Pancreatic Cancer in Africa Using Machine Learning-Based Spatial Model' <doi:10.5281/zenodo.16529986> and <doi:10.5281/zenodo.16529016>. It facilitates the analysis of geographic health data by combining modern spatial mapping tools with advanced machine learning (ML) algorithms. 'mlspatial' enables users to import and pre-process shapefile and associated demographic or disease incidence data, generate richly annotated thematic maps, and apply predictive models, including Random Forest, 'XGBoost', and Support Vector Regression, to identify spatial patterns and risk factors. It is suited for spatial epidemiologists, public health researchers, and GIS analysts aiming to uncover hidden geographic patterns in health-related outcomes and inform evidence-based interventions.",
    "version": "0.1.0",
    "maintainer": "Adeboye Azeez <azizadeboye@gmail.com>",
    "author": "Adeboye Azeez [aut, cre],\n  Colin Noel [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlspatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlspatial Machine Learning and Mapping for Spatial Epidemiology Provides tools for the integration, visualisation, and modelling of spatial epidemiological data using the method described in Azeez, A., & Noel, C. (2025). 'Predictive Modelling and Spatial Distribution of Pancreatic Cancer in Africa Using Machine Learning-Based Spatial Model' <doi:10.5281/zenodo.16529986> and <doi:10.5281/zenodo.16529016>. It facilitates the analysis of geographic health data by combining modern spatial mapping tools with advanced machine learning (ML) algorithms. 'mlspatial' enables users to import and pre-process shapefile and associated demographic or disease incidence data, generate richly annotated thematic maps, and apply predictive models, including Random Forest, 'XGBoost', and Support Vector Regression, to identify spatial patterns and risk factors. It is suited for spatial epidemiologists, public health researchers, and GIS analysts aiming to uncover hidden geographic patterns in health-related outcomes and inform evidence-based interventions.  "
  },
  {
    "id": 16296,
    "package_name": "mnormt",
    "title": "The Multivariate Normal and t Distributions, and Their Truncated\nVersions",
    "description": "Functions are provided for computing the density and the\n  distribution function of d-dimensional normal and \"t\" random variables,\n  possibly truncated (on one side or two sides),  and for generating random \n  vectors sampled from these distributions, except sampling from the truncated\n  \"t\". Moments of arbitrary order of a multivariate truncated normal are\n  computed, and converted to cumulants up to order 4. \n  Probabilities are computed via non-Monte Carlo methods; different routines \n  are used in the case d=1, d=2, d=3, d>3, if d denotes the dimensionality.",
    "version": "2.1.1",
    "maintainer": "Adelchi Azzalini <adelchi.azzalini@unipd.it>",
    "author": "Adelchi Azzalini [aut, cre],\n   Alan Genz [aut] (most Fortran code),\n   Alan Miller [ctb] (Fortran routine PHI),\n   Michael J. Wichura  [ctb] (Fortran routine PHINV),\n   G. W. Hill [ctb] (Fortran routine STDINV),\n   Yihong Ge [ctb] (Fortran routines BNVU and MVBVU).",
    "url": "http://azzalini.stat.unipd.it/SW/Pkg-mnormt/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mnormt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mnormt The Multivariate Normal and t Distributions, and Their Truncated\nVersions Functions are provided for computing the density and the\n  distribution function of d-dimensional normal and \"t\" random variables,\n  possibly truncated (on one side or two sides),  and for generating random \n  vectors sampled from these distributions, except sampling from the truncated\n  \"t\". Moments of arbitrary order of a multivariate truncated normal are\n  computed, and converted to cumulants up to order 4. \n  Probabilities are computed via non-Monte Carlo methods; different routines \n  are used in the case d=1, d=2, d=3, d>3, if d denotes the dimensionality.  "
  },
  {
    "id": 16306,
    "package_name": "mod09nrt",
    "title": "Extraction of Bands from MODIS Surface Reflectance Product MOD09\nNRT",
    "description": "Package for processing downloaded MODIS Surface reflectance\n    Product HDF files. Specifically, MOD09 surface reflectance product files, and\n    the associated MOD03 geolocation files (for MODIS-TERRA). The package will be\n    most effective if the user installs MRTSwath (MODIS Reprojection Tool for swath\n    products; <https://lpdaac.usgs.gov/tools/modis_reprojection_tool_swath>, and\n    adds the directory with the MRTSwath executable to the default R PATH by editing\n    ~/.Rprofile.",
    "version": "0.14",
    "maintainer": "Rishabh Gupta <rishabh.uk@gmail.com>",
    "author": "Rishabh Gupta <rishabh.uk@gmail.com>, Nicholas J. Matzke, Dept. of Integrative Biology, U.C. Berkeley",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mod09nrt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mod09nrt Extraction of Bands from MODIS Surface Reflectance Product MOD09\nNRT Package for processing downloaded MODIS Surface reflectance\n    Product HDF files. Specifically, MOD09 surface reflectance product files, and\n    the associated MOD03 geolocation files (for MODIS-TERRA). The package will be\n    most effective if the user installs MRTSwath (MODIS Reprojection Tool for swath\n    products; <https://lpdaac.usgs.gov/tools/modis_reprojection_tool_swath>, and\n    adds the directory with the MRTSwath executable to the default R PATH by editing\n    ~/.Rprofile.  "
  },
  {
    "id": 16321,
    "package_name": "modelc",
    "title": "A Linear Model to 'SQL' Compiler",
    "description": "This is a cross-platform linear model to 'SQL' compiler. It generates 'SQL' from linear and generalized linear models. Its interface consists of a single function, modelc(), which takes the output of lm() or glm() functions (or any object which has the same signature) and outputs a 'SQL' character vector representing the predictions on the scale of the response variable as described in Dunn & Smith (2018) <doi:10.1007/978-1-4419-0118-7> and originating in Nelder & Wedderburn (1972) <doi:10.2307/2344614>. The resultant 'SQL' can be included in a 'SELECT' statement and returns output similar to that of the glm.predict() or lm.predict() predictions, assuming numeric types are represented in the database using sufficient precision. Currently log and identity link functions are supported.",
    "version": "1.0.0.0",
    "maintainer": "Hugo Saavedra <analytics+hugo@sparkfish.com>",
    "author": "Sparkfish Analytics [cph],\n  Hugo Saavedra [aut, cre]",
    "url": "https://github.com/sparkfish/modelc",
    "bug_reports": "https://github.com/sparkfish/modelc/issues",
    "repository": "https://cran.r-project.org/package=modelc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modelc A Linear Model to 'SQL' Compiler This is a cross-platform linear model to 'SQL' compiler. It generates 'SQL' from linear and generalized linear models. Its interface consists of a single function, modelc(), which takes the output of lm() or glm() functions (or any object which has the same signature) and outputs a 'SQL' character vector representing the predictions on the scale of the response variable as described in Dunn & Smith (2018) <doi:10.1007/978-1-4419-0118-7> and originating in Nelder & Wedderburn (1972) <doi:10.2307/2344614>. The resultant 'SQL' can be included in a 'SELECT' statement and returns output similar to that of the glm.predict() or lm.predict() predictions, assuming numeric types are represented in the database using sufficient precision. Currently log and identity link functions are supported.  "
  },
  {
    "id": 16334,
    "package_name": "modgetxl",
    "title": "A 'shiny' Module for Reading Excel Sheets",
    "description": "This is a shiny module that presents a file picker user interface to get an Excel file name, and reads the Excel sheets using 'readxl' package and returns the resulting sheet(s) as a vector and data in dataframe(s).",
    "version": "0.4",
    "maintainer": "Yadu Balehosur <yadu@vizualytics.com>",
    "author": "Yadu Balehosur <yadu@vizualytics.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=modgetxl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modgetxl A 'shiny' Module for Reading Excel Sheets This is a shiny module that presents a file picker user interface to get an Excel file name, and reads the Excel sheets using 'readxl' package and returns the resulting sheet(s) as a vector and data in dataframe(s).  "
  },
  {
    "id": 16375,
    "package_name": "moocore",
    "title": "Core Mathematical Functions for Multi-Objective Optimization",
    "description": "Fast implementations of mathematical operations and performance metrics for multi-objective optimization, including filtering and ranking of dominated vectors according to Pareto optimality, hypervolume metric, C.M. Fonseca, L. Paquete, M. L\u00f3pez-Ib\u00e1\u00f1ez (2006) <doi:10.1109/CEC.2006.1688440>, epsilon indicator, inverted generational distance, computation of the empirical attainment function, V.G. da Fonseca, C.M. Fonseca, A.O. Hall (2001) <doi:10.1007/3-540-44719-9_15>, and  Vorob'ev threshold, expectation and deviation, M. Binois, D. Ginsbourger, O. Roustant (2015) <doi:10.1016/j.ejor.2014.07.032>, among others.",
    "version": "0.1.10",
    "maintainer": "Manuel L\u00f3pez-Ib\u00e1\u00f1ez <manuel.lopez-ibanez@manchester.ac.uk>",
    "author": "Manuel L\u00f3pez-Ib\u00e1\u00f1ez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9974-1295>),\n  Carlos Fonseca [ctb],\n  Lu\u00eds Paquete [ctb],\n  Andreia P. Guerreiro [ctb],\n  Micka\u00ebl Binois [ctb],\n  Michael H. Buselli [cph] (AVL-tree library),\n  Wessel Dankers [cph] (AVL-tree library),\n  NumPy Developers [cph] (RNG and ziggurat constants),\n  Jean-Sebastien Roy [cph] (mt19937 library),\n  Makoto Matsumoto [cph] (mt19937 library),\n  Takuji Nishimura [cph] (mt19937 library)",
    "url": "https://multi-objective.github.io/moocore/r/,\nhttps://github.com/multi-objective/moocore",
    "bug_reports": "https://github.com/multi-objective/moocore/issues",
    "repository": "https://cran.r-project.org/package=moocore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "moocore Core Mathematical Functions for Multi-Objective Optimization Fast implementations of mathematical operations and performance metrics for multi-objective optimization, including filtering and ranking of dominated vectors according to Pareto optimality, hypervolume metric, C.M. Fonseca, L. Paquete, M. L\u00f3pez-Ib\u00e1\u00f1ez (2006) <doi:10.1109/CEC.2006.1688440>, epsilon indicator, inverted generational distance, computation of the empirical attainment function, V.G. da Fonseca, C.M. Fonseca, A.O. Hall (2001) <doi:10.1007/3-540-44719-9_15>, and  Vorob'ev threshold, expectation and deviation, M. Binois, D. Ginsbourger, O. Roustant (2015) <doi:10.1016/j.ejor.2014.07.032>, among others.  "
  },
  {
    "id": 16383,
    "package_name": "moose",
    "title": "Mean Squared Out-of-Sample Error Projection",
    "description": "Projects mean squared out-of-sample error for a linear regression based upon the methodology developed in Rohlfs (2022) <doi:10.48550/arXiv.2209.01493>.  It consumes as inputs the lm object from an estimated OLS regression (based on the \"training sample\") and a data.frame of out-of-sample cases (the \"test sample\") that have non-missing values for the same predictors. The test sample may or may not include data on the outcome variable; if it does, that variable is not used. The aim of the exercise is to project what what mean squared out-of-sample error can be expected given the predictor values supplied in the test sample. Output consists of a list of three elements: the projected mean squared out-of-sample error, the projected out-of-sample R-squared, and a vector of out-of-sample \"hat\" or \"leverage\" values, as defined in the paper.",
    "version": "0.0.1",
    "maintainer": "Chris Rohlfs <car2228@columbia.edu>",
    "author": "Chris Rohlfs [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7714-9231>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=moose",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "moose Mean Squared Out-of-Sample Error Projection Projects mean squared out-of-sample error for a linear regression based upon the methodology developed in Rohlfs (2022) <doi:10.48550/arXiv.2209.01493>.  It consumes as inputs the lm object from an estimated OLS regression (based on the \"training sample\") and a data.frame of out-of-sample cases (the \"test sample\") that have non-missing values for the same predictors. The test sample may or may not include data on the outcome variable; if it does, that variable is not used. The aim of the exercise is to project what what mean squared out-of-sample error can be expected given the predictor values supplied in the test sample. Output consists of a list of three elements: the projected mean squared out-of-sample error, the projected out-of-sample R-squared, and a vector of out-of-sample \"hat\" or \"leverage\" values, as defined in the paper.  "
  },
  {
    "id": 16388,
    "package_name": "morph",
    "title": "3D Segmentation of Voxels into Morphologic Classes",
    "description": "Automatically segments a 3D array of voxels into mutually exclusive morphological elements. This package extends existing work for segmenting 2D binary raster data. A paper documenting this approach has been accepted for publication in the journal Landscape Ecology. Detailed references will be updated here once those are known.",
    "version": "1.1.0",
    "maintainer": "Tarmo K. Remmel <remmelt@yorku.ca>",
    "author": "Tarmo K. Remmel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6251-876X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=morph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morph 3D Segmentation of Voxels into Morphologic Classes Automatically segments a 3D array of voxels into mutually exclusive morphological elements. This package extends existing work for segmenting 2D binary raster data. A paper documenting this approach has been accepted for publication in the journal Landscape Ecology. Detailed references will be updated here once those are known.  "
  },
  {
    "id": 16400,
    "package_name": "mosaicCalc",
    "title": "R-Language Based Calculus Operations for Teaching",
    "description": "Software to support the introductory *MOSAIC Calculus* \n    textbook <https://www.mosaic-web.org/MOSAIC-Calculus/>),\n    one of many data- and modeling-oriented educational resources developed by \n    Project MOSAIC (<https://www.mosaic-web.org/>). Provides symbolic and\n    numerical differentiation and integration, as well as support for \n    applied linear algebra (for data science), and differential equations/dynamics.\n    Includes grammar-of-graphics-based functions for drawing vector fields, trajectories, etc.\n    The software is suitable for general use, but intended mainly for teaching calculus.",
    "version": "0.6.4",
    "maintainer": "Daniel Kaplan <kaplan@macalester.edu>",
    "author": "Daniel T. Kaplan <kaplan@macalester.edu>, Randall Pruim <rpruim@calvin.edu>, Nicholas J. Horton <nhorton@amherst.edu>",
    "url": "https://github.com/ProjectMOSAIC/mosaicCalc",
    "bug_reports": "https://github.com/ProjectMOSAIC/mosaicCalc/issues",
    "repository": "https://cran.r-project.org/package=mosaicCalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mosaicCalc R-Language Based Calculus Operations for Teaching Software to support the introductory *MOSAIC Calculus* \n    textbook <https://www.mosaic-web.org/MOSAIC-Calculus/>),\n    one of many data- and modeling-oriented educational resources developed by \n    Project MOSAIC (<https://www.mosaic-web.org/>). Provides symbolic and\n    numerical differentiation and integration, as well as support for \n    applied linear algebra (for data science), and differential equations/dynamics.\n    Includes grammar-of-graphics-based functions for drawing vector fields, trajectories, etc.\n    The software is suitable for general use, but intended mainly for teaching calculus.  "
  },
  {
    "id": 16407,
    "package_name": "motif",
    "title": "Local Pattern Analysis",
    "description": "Describes spatial patterns of categorical raster data for \n    any defined regular and irregular areas. \n    Patterns are described quantitatively using built-in signatures \n    based on co-occurrence matrices but also allows for \n    any user-defined functions. \n    It enables spatial analysis such as search, change detection,\n    and clustering to be performed on spatial patterns (Nowosad (2021) <doi:10.1007/s10980-020-01135-0>).",
    "version": "0.6.5",
    "maintainer": "Jakub Nowosad <nowosad.jakub@gmail.com>",
    "author": "Jakub Nowosad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1057-3721>)",
    "url": "https://jakubnowosad.com/motif/",
    "bug_reports": "https://github.com/Nowosad/motif/issues",
    "repository": "https://cran.r-project.org/package=motif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "motif Local Pattern Analysis Describes spatial patterns of categorical raster data for \n    any defined regular and irregular areas. \n    Patterns are described quantitatively using built-in signatures \n    based on co-occurrence matrices but also allows for \n    any user-defined functions. \n    It enables spatial analysis such as search, change detection,\n    and clustering to be performed on spatial patterns (Nowosad (2021) <doi:10.1007/s10980-020-01135-0>).  "
  },
  {
    "id": 16422,
    "package_name": "movecost",
    "title": "Calculation of Slope-Dependant Accumulated Cost Surface,\nLeast-Cost Paths, Least-Cost Corridors, Least-Cost Networks\nRelated to Human Movement Across the Landscape",
    "description": "Provides the facility to calculate non-isotropic accumulated cost surface, least-cost paths, least-cost corridors, least-cost networks using a number of human-movement-related cost functions that can be selected by the user. It just requires a Digital Terrain Model, a start location and (optionally) destination locations. See Alberti (2019) <doi:10.1016/j.softx.2019.100331>.",
    "version": "2.1",
    "maintainer": "Gianmarco Alberti <gianmarcoalberti@gmail.com>",
    "author": "Gianmarco Alberti [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=movecost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "movecost Calculation of Slope-Dependant Accumulated Cost Surface,\nLeast-Cost Paths, Least-Cost Corridors, Least-Cost Networks\nRelated to Human Movement Across the Landscape Provides the facility to calculate non-isotropic accumulated cost surface, least-cost paths, least-cost corridors, least-cost networks using a number of human-movement-related cost functions that can be selected by the user. It just requires a Digital Terrain Model, a start location and (optionally) destination locations. See Alberti (2019) <doi:10.1016/j.softx.2019.100331>.  "
  },
  {
    "id": 16430,
    "package_name": "mpath",
    "title": "Regularized Linear Models",
    "description": "Algorithms compute robust estimators for loss functions in the concave convex (CC) family by the iteratively reweighted convex optimization (IRCO), an extension of the iteratively reweighted least squares (IRLS). The IRCO reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include robust (penalized) generalized linear models and robust support vector machines. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,\n      Wang et al. (2015) <doi:10.1002/bimj.201400143>,\n      Wang et al. (2016) <doi:10.1177/0962280214530608>,\n      Wang (2021) <doi:10.1007/s11749-021-00770-2>,\n      Wang (2024) <doi:10.1111/anzs.12409>.",
    "version": "0.4-2.26",
    "maintainer": "Zhu Wang <zwang145@uthsc.edu>",
    "author": "Zhu Wang, with contributions from Achim Zeileis, Simon Jackman, Brian Ripley, and Patrick Breheny",
    "url": "https://github.com/zhuwang46/mpath",
    "bug_reports": "https://github.com/zhuwang46/mpath",
    "repository": "https://cran.r-project.org/package=mpath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mpath Regularized Linear Models Algorithms compute robust estimators for loss functions in the concave convex (CC) family by the iteratively reweighted convex optimization (IRCO), an extension of the iteratively reweighted least squares (IRLS). The IRCO reduces the weight of the observation that leads to a large loss; it also provides weights to help identify outliers. Applications include robust (penalized) generalized linear models and robust support vector machines. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,\n      Wang et al. (2015) <doi:10.1002/bimj.201400143>,\n      Wang et al. (2016) <doi:10.1177/0962280214530608>,\n      Wang (2021) <doi:10.1007/s11749-021-00770-2>,\n      Wang (2024) <doi:10.1111/anzs.12409>.  "
  },
  {
    "id": 16482,
    "package_name": "msdrought",
    "title": "Seasonal Mid-Summer Drought Characteristics",
    "description": "Characterization of a mid-summer drought (MSD) with precipitation\n    based statistics. The MSD is a phenomenon of decreased rainfall during a \n    typical rainy season. It is a feature of rainfall in much of Central America\n    and is also found in other locations, typically those with a Mediterranean \n    climate. Details on the metrics are in Maurer et al. (2022) \n    <doi:10.5194/hess-26-1425-2022>.",
    "version": "0.1.1",
    "maintainer": "Ed Maurer <emaurer@scu.edu>",
    "author": "Turner Uyeda [aut],\n  Ed Maurer [aut, cre, cph],\n  Iris Stewart-Frey [aut],\n  Kenneth Joseph [aut],\n  Alex Avila [aut]",
    "url": "https://github.com/EdM44/msdrought/,\nhttps://edm44.github.io/msdrought/",
    "bug_reports": "https://github.com/EdM44/msdrought/issues",
    "repository": "https://cran.r-project.org/package=msdrought",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "msdrought Seasonal Mid-Summer Drought Characteristics Characterization of a mid-summer drought (MSD) with precipitation\n    based statistics. The MSD is a phenomenon of decreased rainfall during a \n    typical rainy season. It is a feature of rainfall in much of Central America\n    and is also found in other locations, typically those with a Mediterranean \n    climate. Details on the metrics are in Maurer et al. (2022) \n    <doi:10.5194/hess-26-1425-2022>.  "
  },
  {
    "id": 16527,
    "package_name": "mulset",
    "title": "Multiset Intersection Generator",
    "description": "Computes efficient data distributions from highly inconsistent datasets with many missing values using multi-set intersections. Based upon hash functions, 'mulset' can quickly identify intersections from very large matrices of input vectors across columns and rows and thus provides scalable solution for dealing with missing values. Tomic et al. (2019) <doi:10.1101/545186>.",
    "version": "1.0.0",
    "maintainer": "Ivan Tomic <info@ivantomic.com>",
    "author": "Ivan Tomic [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3596-681X>),\n  Adriana Tomic [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-9885-3535>)",
    "url": "https://github.com/LogIN-/mulset",
    "bug_reports": "https://github.com/LogIN-/mulset/issues",
    "repository": "https://cran.r-project.org/package=mulset",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mulset Multiset Intersection Generator Computes efficient data distributions from highly inconsistent datasets with many missing values using multi-set intersections. Based upon hash functions, 'mulset' can quickly identify intersections from very large matrices of input vectors across columns and rows and thus provides scalable solution for dealing with missing values. Tomic et al. (2019) <doi:10.1101/545186>.  "
  },
  {
    "id": 16532,
    "package_name": "multcompView",
    "title": "Visualizations of Paired Comparisons",
    "description": "Convert a logical vector or a vector of\n    p-values or a correlation, difference, or distance\n    matrix into a display identifying the pairs for\n    which the differences were not significantly\n    different.  Designed for use in conjunction with\n    the output of functions like TukeyHSD, dist{stats},\n    simint, simtest, csimint, csimtest{multcomp},\n    friedmanmc, kruskalmc{pgirmess}.",
    "version": "0.1-10",
    "maintainer": "Luciano Selzer <luciano.selzer@gmail.com>",
    "author": "Spencer Graves, Hans-Peter Piepho and Luciano Selzer with\n    help from Sundar Dorai-Raj",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multcompView",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multcompView Visualizations of Paired Comparisons Convert a logical vector or a vector of\n    p-values or a correlation, difference, or distance\n    matrix into a display identifying the pairs for\n    which the differences were not significantly\n    different.  Designed for use in conjunction with\n    the output of functions like TukeyHSD, dist{stats},\n    simint, simtest, csimint, csimtest{multcomp},\n    friedmanmc, kruskalmc{pgirmess}.  "
  },
  {
    "id": 16537,
    "package_name": "multiApply",
    "title": "Apply Functions to Multiple Multidimensional Arrays or Vectors",
    "description": "The base apply function and its variants, as well as the related\n    functions in the 'plyr' package, typically apply user-defined functions to a\n    single argument (or a list of vectorized arguments in the case of mapply). The\n    'multiApply' package extends this paradigm with its only function, Apply, which\n    efficiently applies functions taking one or a list of multiple unidimensional\n    or multidimensional arrays (or combinations thereof) as input. The input\n    arrays can have different numbers of dimensions as well as different dimension\n    lengths, and the applied function can return one or a list of unidimensional or\n    multidimensional arrays as output. This saves development time by preventing the\n    R user from writing often error-prone and memory-inefficient loops dealing with\n    multiple complex arrays. Also, a remarkable feature of Apply is the transparent\n    use of multi-core through its parameter 'ncores'. In contrast to the base apply\n    function, this package suggests the use of 'target dimensions' as opposite\n    to the 'margins' for specifying the dimensions relevant to the function to be\n    applied.",
    "version": "2.1.5",
    "maintainer": "Victoria Agudetse <victoria.agudetse@bsc.es>",
    "author": "BSC-CNS [aut, cph],\n  Nicolau Manubens [aut],\n  Alasdair Hunter [aut],\n  An-Chi Ho [ctb],\n  Nuria Perez [ctb],\n  Victoria Agudetse [ctb, cre]",
    "url": "https://earth.bsc.es/gitlab/ces/multiApply",
    "bug_reports": "https://earth.bsc.es/gitlab/ces/multiApply/-/issues",
    "repository": "https://cran.r-project.org/package=multiApply",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multiApply Apply Functions to Multiple Multidimensional Arrays or Vectors The base apply function and its variants, as well as the related\n    functions in the 'plyr' package, typically apply user-defined functions to a\n    single argument (or a list of vectorized arguments in the case of mapply). The\n    'multiApply' package extends this paradigm with its only function, Apply, which\n    efficiently applies functions taking one or a list of multiple unidimensional\n    or multidimensional arrays (or combinations thereof) as input. The input\n    arrays can have different numbers of dimensions as well as different dimension\n    lengths, and the applied function can return one or a list of unidimensional or\n    multidimensional arrays as output. This saves development time by preventing the\n    R user from writing often error-prone and memory-inefficient loops dealing with\n    multiple complex arrays. Also, a remarkable feature of Apply is the transparent\n    use of multi-core through its parameter 'ncores'. In contrast to the base apply\n    function, this package suggests the use of 'target dimensions' as opposite\n    to the 'margins' for specifying the dimensions relevant to the function to be\n    applied.  "
  },
  {
    "id": 16548,
    "package_name": "multiScaleR",
    "title": "Methods for Optimizing Scales of Effect",
    "description": "A tool for optimizing scales of effect when modeling ecological processes in space. Specifically, the scale parameter of a distance-weighted kernel distribution is identified for all environmental layers included in the model. Includes functions to assist in model selection, model evaluation, efficient transformation of raster surfaces using fast Fourier transformation, and projecting models. For more details see Peterman (2025) <doi:10.21203/rs.3.rs-7246115/v1>.",
    "version": "0.4.5",
    "maintainer": "Bill Peterman <Peterman.73@osu.edu>",
    "author": "Bill Peterman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5229-9268>)",
    "url": "https://github.com/wpeterman/multiScaleR",
    "bug_reports": "https://github.com/wpeterman/multiScaleR/issues",
    "repository": "https://cran.r-project.org/package=multiScaleR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multiScaleR Methods for Optimizing Scales of Effect A tool for optimizing scales of effect when modeling ecological processes in space. Specifically, the scale parameter of a distance-weighted kernel distribution is identified for all environmental layers included in the model. Includes functions to assist in model selection, model evaluation, efficient transformation of raster surfaces using fast Fourier transformation, and projecting models. For more details see Peterman (2025) <doi:10.21203/rs.3.rs-7246115/v1>.  "
  },
  {
    "id": 16554,
    "package_name": "multibreakeR",
    "title": "Tests for a Structural Change in Multivariate Time Series",
    "description": "Flexible implementation of a structural change point detection algorithm for multivariate time series.\n    It authorizes inclusion of trends, exogenous variables, and break test on the intercept or on the full vector autoregression system.\n    Bai, Lumsdaine, and Stock (1998) <doi:10.1111/1467-937X.00051>.",
    "version": "0.1.0",
    "maintainer": "Loic Marechal <loic.marechal@unil.ch>",
    "author": "Loic Marechal [cre, aut]",
    "url": "https://github.com/loicym/multibreakeR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multibreakeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multibreakeR Tests for a Structural Change in Multivariate Time Series Flexible implementation of a structural change point detection algorithm for multivariate time series.\n    It authorizes inclusion of trends, exogenous variables, and break test on the intercept or on the full vector autoregression system.\n    Bai, Lumsdaine, and Stock (1998) <doi:10.1111/1467-937X.00051>.  "
  },
  {
    "id": 16591,
    "package_name": "multinomineq",
    "title": "Bayesian Inference for Multinomial Models with Inequality\nConstraints",
    "description": "\n    Implements Gibbs sampling and Bayes factors for multinomial models with\n    linear inequality constraints on the vector of probability parameters. As\n    special cases, the model class includes models that predict a linear order \n    of binomial probabilities (e.g., p[1] < p[2] < p[3] < .50) and mixture models \n    assuming that the parameter vector p must be inside the convex hull of a \n    finite number of predicted patterns (i.e., vertices). A formal definition of \n    inequality-constrained multinomial models and the implemented computational\n    methods is provided in: Heck, D.W., & Davis-Stober, C.P. (2019). \n    Multinomial models with linear inequality constraints: Overview and improvements \n    of computational methods for Bayesian inference. Journal of Mathematical \n    Psychology, 91, 70-87. <doi:10.1016/j.jmp.2019.03.004>.\n    Inequality-constrained multinomial models have applications in the area of \n    judgment and decision making to fit and test random utility models  \n    (Regenwetter, M., Dana, J., & Davis-Stober, C.P. (2011). Transitivity of \n    preferences. Psychological Review, 118, 42\u201356, <doi:10.1037/a0021150>) or to \n    perform outcome-based strategy classification to select the decision strategy \n    that provides the best account for a vector of observed choice frequencies \n    (Heck, D.W., Hilbig, B.E., & Moshagen, M. (2017). From information \n    processing to decisions: Formalizing and comparing probabilistic choice models. \n    Cognitive Psychology, 96, 26\u201340. <doi:10.1016/j.cogpsych.2017.05.003>).",
    "version": "0.2.6",
    "maintainer": "Daniel W. Heck <daniel.heck@uni-marburg.de>",
    "author": "Daniel W. Heck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6302-9252>)",
    "url": "https://github.com/danheck/multinomineq",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multinomineq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multinomineq Bayesian Inference for Multinomial Models with Inequality\nConstraints \n    Implements Gibbs sampling and Bayes factors for multinomial models with\n    linear inequality constraints on the vector of probability parameters. As\n    special cases, the model class includes models that predict a linear order \n    of binomial probabilities (e.g., p[1] < p[2] < p[3] < .50) and mixture models \n    assuming that the parameter vector p must be inside the convex hull of a \n    finite number of predicted patterns (i.e., vertices). A formal definition of \n    inequality-constrained multinomial models and the implemented computational\n    methods is provided in: Heck, D.W., & Davis-Stober, C.P. (2019). \n    Multinomial models with linear inequality constraints: Overview and improvements \n    of computational methods for Bayesian inference. Journal of Mathematical \n    Psychology, 91, 70-87. <doi:10.1016/j.jmp.2019.03.004>.\n    Inequality-constrained multinomial models have applications in the area of \n    judgment and decision making to fit and test random utility models  \n    (Regenwetter, M., Dana, J., & Davis-Stober, C.P. (2011). Transitivity of \n    preferences. Psychological Review, 118, 42\u201356, <doi:10.1037/a0021150>) or to \n    perform outcome-based strategy classification to select the decision strategy \n    that provides the best account for a vector of observed choice frequencies \n    (Heck, D.W., Hilbig, B.E., & Moshagen, M. (2017). From information \n    processing to decisions: Formalizing and comparing probabilistic choice models. \n    Cognitive Psychology, 96, 26\u201340. <doi:10.1016/j.cogpsych.2017.05.003>).  "
  },
  {
    "id": 16609,
    "package_name": "multivar",
    "title": "Penalized Estimation of Multiple-Subject Vector Autoregressive\n(multi-VAR) Models",
    "description": "Functions for simulating, estimating and forecasting stationary Vector Autoregressive (VAR) models for multiple subject data using the penalized multi-VAR framework in Fisher, Kim and Pipiras (2020) <arXiv:2007.05052>. ",
    "version": "1.1.0",
    "maintainer": "Zachary Fisher <fish.zachary@gmail.com>",
    "author": "Zachary Fisher [aut, cre],\n  Younghoon Kim [ctb],\n  Vladas Pipiras [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multivar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multivar Penalized Estimation of Multiple-Subject Vector Autoregressive\n(multi-VAR) Models Functions for simulating, estimating and forecasting stationary Vector Autoregressive (VAR) models for multiple subject data using the penalized multi-VAR framework in Fisher, Kim and Pipiras (2020) <arXiv:2007.05052>.   "
  },
  {
    "id": 16610,
    "package_name": "multivariance",
    "title": "Measuring Multivariate Dependence Using Distance Multivariance",
    "description": "Distance multivariance is a measure of dependence which can be used to detect \n    and quantify dependence of arbitrarily many random vectors. The necessary functions are\n    implemented in this packages and examples are given. It includes: distance multivariance, \n    distance multicorrelation, dependence structure detection, tests of independence and\n    copula versions of distance multivariance based on the Monte Carlo empirical transform.\n    Detailed references are given in the package description, as starting point for the \n    theoretic background we refer to:\n    B. B\u00f6ttcher, Dependence and Dependence Structures: Estimation and Visualization Using \n    the Unifying Concept of Distance Multivariance. Open Statistics, Vol. 1, No. 1 (2020), \n    <doi:10.1515/stat-2020-0001>.",
    "version": "2.4.1",
    "maintainer": "Bj\u00f6rn B\u00f6ttcher <bjoern.boettcher@tu-dresden.de>",
    "author": "Bj\u00f6rn B\u00f6ttcher [aut, cre],\n  Martin Keller-Ressel [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multivariance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multivariance Measuring Multivariate Dependence Using Distance Multivariance Distance multivariance is a measure of dependence which can be used to detect \n    and quantify dependence of arbitrarily many random vectors. The necessary functions are\n    implemented in this packages and examples are given. It includes: distance multivariance, \n    distance multicorrelation, dependence structure detection, tests of independence and\n    copula versions of distance multivariance based on the Monte Carlo empirical transform.\n    Detailed references are given in the package description, as starting point for the \n    theoretic background we refer to:\n    B. B\u00f6ttcher, Dependence and Dependence Structures: Estimation and Visualization Using \n    the Unifying Concept of Distance Multivariance. Open Statistics, Vol. 1, No. 1 (2020), \n    <doi:10.1515/stat-2020-0001>.  "
  },
  {
    "id": 16628,
    "package_name": "musicMCT",
    "title": "Analyze the Structure of Musical Scales",
    "description": "Analysis of musical scales (& modes, grooves, etc.) in the vein of\n    Sherrill 2025 <doi:10.1215/00222909-11595194>.\n    The initials MCT in the package title refer to the article's title: \"Modal\n    Color Theory.\" Offers support for conventional musical pitch class set\n    theory as developed by Forte (1973, ISBN: 9780300016109) and David Lewin\n    (1987, ISBN: 9780300034936), as well as for the continuous geometries of\n    Callender, Quinn, & Tymoczko (2008) <doi:10.1126/science.1153021>.\n    Identifies structural properties of scales and calculates derived values\n    (sign vector, color number, brightness ratio, etc.). Creates plots such as\n    \"brightness graphs\" which visualize these properties.",
    "version": "0.3.0",
    "maintainer": "Paul Sherrill <paul.sherrill@utah.edu>",
    "author": "Paul Sherrill [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0002-3617-016X>)",
    "url": "https://satbq.github.io/musicMCT/,\nhttps://github.com/satbq/musicMCT",
    "bug_reports": "https://github.com/satbq/musicMCT/issues",
    "repository": "https://cran.r-project.org/package=musicMCT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "musicMCT Analyze the Structure of Musical Scales Analysis of musical scales (& modes, grooves, etc.) in the vein of\n    Sherrill 2025 <doi:10.1215/00222909-11595194>.\n    The initials MCT in the package title refer to the article's title: \"Modal\n    Color Theory.\" Offers support for conventional musical pitch class set\n    theory as developed by Forte (1973, ISBN: 9780300016109) and David Lewin\n    (1987, ISBN: 9780300034936), as well as for the continuous geometries of\n    Callender, Quinn, & Tymoczko (2008) <doi:10.1126/science.1153021>.\n    Identifies structural properties of scales and calculates derived values\n    (sign vector, color number, brightness ratio, etc.). Creates plots such as\n    \"brightness graphs\" which visualize these properties.  "
  },
  {
    "id": 16632,
    "package_name": "mutationtypes",
    "title": "Validate and Convert Mutational Impacts Using Standard Genomic\nDictionaries",
    "description": "Check concordance of a vector of mutation impacts with standard dictionaries such as Sequence Ontology (SO) <http://www.sequenceontology.org/>, Mutation Annotation Format (MAF) <https://docs.gdc.cancer.gov/Encyclopedia/pages/Mutation_Annotation_Format_TCGAv2/> or Prediction and Annotation of Variant Effects (PAVE) <https://github.com/hartwigmedical/hmftools/tree/master/pave>. \n  It enables conversion between SO/PAVE and MAF terms and selection of the most severe consequence where multiple ampersand (&) delimited impacts are given.",
    "version": "0.0.1",
    "maintainer": "Sam El-Kamand <sam.elkamand@gmail.com>",
    "author": "Sam El-Kamand [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2270-8088>),\n  Children's Cancer Institute Australia [cph]",
    "url": "https://github.com/selkamand/mutationtypes",
    "bug_reports": "https://github.com/selkamand/mutationtypes/issues",
    "repository": "https://cran.r-project.org/package=mutationtypes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mutationtypes Validate and Convert Mutational Impacts Using Standard Genomic\nDictionaries Check concordance of a vector of mutation impacts with standard dictionaries such as Sequence Ontology (SO) <http://www.sequenceontology.org/>, Mutation Annotation Format (MAF) <https://docs.gdc.cancer.gov/Encyclopedia/pages/Mutation_Annotation_Format_TCGAv2/> or Prediction and Annotation of Variant Effects (PAVE) <https://github.com/hartwigmedical/hmftools/tree/master/pave>. \n  It enables conversion between SO/PAVE and MAF terms and selection of the most severe consequence where multiple ampersand (&) delimited impacts are given.  "
  },
  {
    "id": 16661,
    "package_name": "mvhtests",
    "title": "Multivariate Hypothesis Tests",
    "description": "Hypothesis tests for multivariate data. Tests for one and two mean vectors, multivariate analysis of variance, tests for one, two or more covariance matrices. References include: Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. ISBN: 978-0124712522. London: Academic Press.",
    "version": "1.1",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvhtests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvhtests Multivariate Hypothesis Tests Hypothesis tests for multivariate data. Tests for one and two mean vectors, multivariate analysis of variance, tests for one, two or more covariance matrices. References include: Mardia K.V., Kent J.T. and Bibby J.M. (1979). Multivariate Analysis. ISBN: 978-0124712522. London: Academic Press.  "
  },
  {
    "id": 16667,
    "package_name": "mvnfast",
    "title": "Fast Multivariate Normal and Student's t Methods",
    "description": "Provides computationally efficient tools related to the\n    multivariate normal and Student's t distributions. The main functionalities\n    are: simulating multivariate random vectors, evaluating multivariate normal or\n    Student's t densities and Mahalanobis distances. These tools are very efficient\n    thanks to the use of C++ code and of the OpenMP API.",
    "version": "0.2.8",
    "maintainer": "Matteo Fasiolo <matteo.fasiolo@gmail.com>",
    "author": "Matteo Fasiolo [aut, cre],\n  Thijs van den Berg [ctb]",
    "url": "https://github.com/mfasiolo/mvnfast/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvnfast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvnfast Fast Multivariate Normal and Student's t Methods Provides computationally efficient tools related to the\n    multivariate normal and Student's t distributions. The main functionalities\n    are: simulating multivariate random vectors, evaluating multivariate normal or\n    Student's t densities and Mahalanobis distances. These tools are very efficient\n    thanks to the use of C++ code and of the OpenMP API.  "
  },
  {
    "id": 16670,
    "package_name": "mvnmle",
    "title": "ML Estimation for Multivariate Normal Data with Missing Values",
    "description": "Finds the Maximum Likelihood (ML) Estimate of the mean vector\n        and variance-covariance matrix for multivariate normal data\n        with missing values.",
    "version": "0.1-11.2",
    "maintainer": "Mao Kobayashi <kobamao.jp@gmail.com>",
    "author": "Kevin Gross [aut] (ORCID: <https://orcid.org/0000-0001-5612-7519>),\n  Douglas Bates [aut],\n  Mao Kobayashi [cre]",
    "url": "https://github.com/indenkun/mvnmle",
    "bug_reports": "https://github.com/indenkun/mvnmle/issues",
    "repository": "https://cran.r-project.org/package=mvnmle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvnmle ML Estimation for Multivariate Normal Data with Missing Values Finds the Maximum Likelihood (ML) Estimate of the mean vector\n        and variance-covariance matrix for multivariate normal data\n        with missing values.  "
  },
  {
    "id": 16673,
    "package_name": "mvnpermute",
    "title": "Generate New Multivariate Normal Samples from Permutations",
    "description": "Given a vector of multivariate normal data, a matrix of\n    covariates and the data covariance matrix, generate new multivariate normal\n    samples that have the same covariance matrix based on permutations of\n    the transformed data residuals.",
    "version": "1.0.1",
    "maintainer": "Mark Abney <abney@uchicago.edu>",
    "author": "Mark Abney [cre, aut]",
    "url": "https://github.com/markabney/MVNpermute",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvnpermute",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvnpermute Generate New Multivariate Normal Samples from Permutations Given a vector of multivariate normal data, a matrix of\n    covariates and the data covariance matrix, generate new multivariate normal\n    samples that have the same covariance matrix based on permutations of\n    the transformed data residuals.  "
  },
  {
    "id": 16675,
    "package_name": "mvout",
    "title": "Robust Multivariate Outlier Detection",
    "description": "Detection of multivariate outliers using robust estimates of location and scale. The Minimum Covariance Determinant (MCD) estimator is used to calculate robust estimates of the mean vector and covariance matrix. Outliers are determined based on robust Mahalanobis distances using either an unstructured covariance matrix, a principal components structured covariance matrix, or a factor analysis structured covariance matrix. Includes options for specifying the direction of interest for outlier detection for each variable.",
    "version": "1.2",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "author": "Jesus E. Delgado [aut],\n  Jed T. Elison [ctb],\n  Nathaniel E. Helwig [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvout",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvout Robust Multivariate Outlier Detection Detection of multivariate outliers using robust estimates of location and scale. The Minimum Covariance Determinant (MCD) estimator is used to calculate robust estimates of the mean vector and covariance matrix. Outliers are determined based on robust Mahalanobis distances using either an unstructured covariance matrix, a principal components structured covariance matrix, or a factor analysis structured covariance matrix. Includes options for specifying the direction of interest for outlier detection for each variable.  "
  },
  {
    "id": 16679,
    "package_name": "mvrsquared",
    "title": "Compute the Coefficient of Determination for Vector or Matrix\nOutcomes",
    "description": "Compute the coefficient of determination for outcomes in n-dimensions. \n  May be useful for multidimensional predictions (such as a multinomial model) or\n  calculating goodness of fit from latent variable models such as probabilistic\n  topic models like latent Dirichlet allocation or deterministic topic models \n  like latent semantic analysis. Based on Jones (2019) <arXiv:1911.11061>.",
    "version": "0.1.5",
    "maintainer": "Tommy Jones <jones.thos.w@gmail.com>",
    "author": "Tommy Jones [aut, cre] (ORCID: <https://orcid.org/0000-0001-6457-2452>),\n  Thomas Nagler [ctb] (ORCID: <https://orcid.org/0000-0003-1855-0046>)",
    "url": "https://github.com/TommyJones/mvrsquared",
    "bug_reports": "https://github.com/TommyJones/mvrsquared/issues",
    "repository": "https://cran.r-project.org/package=mvrsquared",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvrsquared Compute the Coefficient of Determination for Vector or Matrix\nOutcomes Compute the coefficient of determination for outcomes in n-dimensions. \n  May be useful for multidimensional predictions (such as a multinomial model) or\n  calculating goodness of fit from latent variable models such as probabilistic\n  topic models like latent Dirichlet allocation or deterministic topic models \n  like latent semantic analysis. Based on Jones (2019) <arXiv:1911.11061>.  "
  },
  {
    "id": 16692,
    "package_name": "mxkssd",
    "title": "Efficient Mixed-Level k-Circulant Supersaturated Designs",
    "description": "Generates efficient balanced mixed-level k-circulant supersaturated designs by interchanging the elements of the generator vector. Attempts to generate a supersaturated design that has EfNOD efficiency more than user specified efficiency level (mef). Displays the progress of generation of an efficient mixed-level k-circulant design through a progress bar. The progress of 100 per cent means that one full round of interchange is completed. More than one full round (typically 4-5 rounds) of interchange may be required for larger designs. For more details, please see Mandal, B.N., Gupta V. K. and Parsad, R. (2011). Construction of Efficient Mixed-Level k-Circulant Supersaturated Designs, Journal of Statistical Theory and Practice, 5:4, 627-648, <doi:10.1080/15598608.2011.10483735>.",
    "version": "1.2",
    "maintainer": "B N Mandal <mandal.stat@gmail.com>",
    "author": "B N Mandal <mandal.stat@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mxkssd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mxkssd Efficient Mixed-Level k-Circulant Supersaturated Designs Generates efficient balanced mixed-level k-circulant supersaturated designs by interchanging the elements of the generator vector. Attempts to generate a supersaturated design that has EfNOD efficiency more than user specified efficiency level (mef). Displays the progress of generation of an efficient mixed-level k-circulant design through a progress bar. The progress of 100 per cent means that one full round of interchange is completed. More than one full round (typically 4-5 rounds) of interchange may be required for larger designs. For more details, please see Mandal, B.N., Gupta V. K. and Parsad, R. (2011). Construction of Efficient Mixed-Level k-Circulant Supersaturated Designs, Journal of Statistical Theory and Practice, 5:4, 627-648, <doi:10.1080/15598608.2011.10483735>.  "
  },
  {
    "id": 16710,
    "package_name": "na.tools",
    "title": "Comprehensive Library for Working with Missing (NA) Values in\nVectors",
    "description": "\n    This comprehensive toolkit provide a consistent and \n    extensible framework for working with missing values in vectors. The \n    companion package 'tidyimpute' provides similar functionality for list-like \n    and table-like structures).\n    Functions exist for detection, removal, replacement, imputation, \n    recollection, etc. of 'NAs'.",
    "version": "0.3.1",
    "maintainer": "Christopher Brown <chris.brown@decisionpatterns.com>",
    "author": "Christopher Brown [aut, cre],\n  Decision Patterns [cph]",
    "url": "https://github.com/decisionpatterns/na.tools",
    "bug_reports": "https://github.com/decisionpatterns/na.tools/issues",
    "repository": "https://cran.r-project.org/package=na.tools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "na.tools Comprehensive Library for Working with Missing (NA) Values in\nVectors \n    This comprehensive toolkit provide a consistent and \n    extensible framework for working with missing values in vectors. The \n    companion package 'tidyimpute' provides similar functionality for list-like \n    and table-like structures).\n    Functions exist for detection, removal, replacement, imputation, \n    recollection, etc. of 'NAs'.  "
  },
  {
    "id": 16747,
    "package_name": "naturalsort",
    "title": "Natural Ordering",
    "description": "Provides functions related to human natural ordering.\n    It handles adjacent digits in a character sequence as a number so that\n    natural sort function arranges a character vector by their numbers, not digit\n    characters. It is typically seen when operating systems lists file names. For\n    example, a sequence a-1.png, a-2.png, a-10.png looks naturally ordered because 1\n    < 2 < 10 and natural sort algorithm arranges so whereas general sort algorithms\n    arrange it into a-1.png, a-10.png, a-2.png owing to their third and fourth\n    characters.",
    "version": "0.1.3",
    "maintainer": "Kosei Abe <mail@recyclebin.jp>",
    "author": "Kosei Abe",
    "url": "",
    "bug_reports": "https://github.com/kos59125/naturalsort/issues",
    "repository": "https://cran.r-project.org/package=naturalsort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "naturalsort Natural Ordering Provides functions related to human natural ordering.\n    It handles adjacent digits in a character sequence as a number so that\n    natural sort function arranges a character vector by their numbers, not digit\n    characters. It is typically seen when operating systems lists file names. For\n    example, a sequence a-1.png, a-2.png, a-10.png looks naturally ordered because 1\n    < 2 < 10 and natural sort algorithm arranges so whereas general sort algorithms\n    arrange it into a-1.png, a-10.png, a-2.png owing to their third and fourth\n    characters.  "
  },
  {
    "id": 16770,
    "package_name": "ncf",
    "title": "Spatial Covariance Functions",
    "description": "Spatial (cross-)covariance and related geostatistical tools: the\n        nonparametric (cross-)covariance function , the spline correlogram, the\n        nonparametric phase coherence function, local indicators of spatial \n        association (LISA), (Mantel) correlogram, (Partial) Mantel test.",
    "version": "1.3-2",
    "maintainer": "Ottar N. Bjornstad <onb1@psu.edu>",
    "author": "Ottar N. Bjornstad [aut, cre],\n  Jun Cai [ctb]",
    "url": "https://ento.psu.edu/directory/onb1",
    "bug_reports": "https://github.com/objornstad/ncf/issues",
    "repository": "https://cran.r-project.org/package=ncf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ncf Spatial Covariance Functions Spatial (cross-)covariance and related geostatistical tools: the\n        nonparametric (cross-)covariance function , the spline correlogram, the\n        nonparametric phase coherence function, local indicators of spatial \n        association (LISA), (Mantel) correlogram, (Partial) Mantel test.  "
  },
  {
    "id": 16816,
    "package_name": "netSEM",
    "title": "Network Structural Equation Modeling",
    "description": "The network structural equation modeling conducts a network \n    statistical analysis on a data frame of coincident observations of \n    multiple continuous variables [1]. \n    It builds a pathway model by exploring a pool of domain knowledge guided \n    candidate statistical relationships between each of the variable pairs, \n    selecting the 'best fit' on the basis of a specific criteria such as \n    adjusted r-squared value.\n    This material is based upon work supported by the U.S. National Science \n    Foundation Award EEC-2052776 and EEC-2052662 for the MDS-Rely IUCRC Center, \n    under the NSF Solicitation: \n    NSF 20-570 Industry-University Cooperative Research Centers Program \n    [1] Bruckman, Laura S., Nicholas R. Wheeler, Junheng Ma, Ethan Wang, \n    Carl K. Wang, Ivan Chou, Jiayang Sun, and Roger H. French. (2013) \n    <doi:10.1109/ACCESS.2013.2267611>.",
    "version": "0.6.2",
    "maintainer": "Laura S. Bruckman <lsh41@case.edu>",
    "author": "Wei-Heng Huang [aut] (ORCID: <https://orcid.org/0000-0002-6609-4981>),\n  Nicholas R. Wheeler [aut] (ORCID:\n    <https://orcid.org/0000-0003-2248-8919>),\n  Addison G. Klinke [aut] (ORCID:\n    <https://orcid.org/0000-0002-6985-7657>),\n  Yifan Xu [aut] (ORCID: <https://orcid.org/0000-0003-1696-0228>),\n  Wenyu Du [aut] (ORCID: <https://orcid.org/0000-0002-8672-9104>),\n  Amit K. Verma [aut] (ORCID: <https://orcid.org/0000-0003-1580-3001>),\n  Abdulkerim Gok [aut] (ORCID: <https://orcid.org/0000-0003-3433-7106>),\n  Devin A. Gordon [ctb] (ORCID: <https://orcid.org/0000-0002-5919-0422>),\n  Yu Wang [ctb] (ORCID: <https://orcid.org/0000-0003-1353-2578>),\n  Sameera Nalin Venkat [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9114-4602>),\n  HeinHtet Aung [ctb] (ORCID: <https://orcid.org/0000-0002-8191-8213>),\n  Yeajin Jo [ctb] (ORCID: <https://orcid.org/0000-0001-7026-3779>),\n  Xuanji Yu [ctb] (ORCID: <https://orcid.org/0000-0003-1967-3769>),\n  Kemal Ozdemirli [ctb],\n  Jonathan Gordon [ctb] (ORCID: <https://orcid.org/0009-0007-5958-7386>),\n  Jayvic Cristian Jimenez [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2342-5648>),\n  Jiqi Liu [ctb] (ORCID: <https://orcid.org/0000-0003-2016-4160>),\n  Alan J. Curran [ctb] (ORCID: <https://orcid.org/0000-0002-4505-8359>),\n  Justin S. Fada [ctb] (ORCID: <https://orcid.org/0000-0002-0029-5051>),\n  Xuan Ma [ctb] (ORCID: <https://orcid.org/0000-0003-2361-2846>),\n  Jennifer L. Braid [ctb] (ORCID:\n    <https://orcid.org/0000-0002-0677-7756>),\n  Jennifer L. W. Carter [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6702-729X>),\n  Laura S. Bruckman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1271-1072>),\n  Roger H. French [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6162-0532>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=netSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "netSEM Network Structural Equation Modeling The network structural equation modeling conducts a network \n    statistical analysis on a data frame of coincident observations of \n    multiple continuous variables [1]. \n    It builds a pathway model by exploring a pool of domain knowledge guided \n    candidate statistical relationships between each of the variable pairs, \n    selecting the 'best fit' on the basis of a specific criteria such as \n    adjusted r-squared value.\n    This material is based upon work supported by the U.S. National Science \n    Foundation Award EEC-2052776 and EEC-2052662 for the MDS-Rely IUCRC Center, \n    under the NSF Solicitation: \n    NSF 20-570 Industry-University Cooperative Research Centers Program \n    [1] Bruckman, Laura S., Nicholas R. Wheeler, Junheng Ma, Ethan Wang, \n    Carl K. Wang, Ivan Chou, Jiayang Sun, and Roger H. French. (2013) \n    <doi:10.1109/ACCESS.2013.2267611>.  "
  },
  {
    "id": 16933,
    "package_name": "nlcv",
    "title": "Nested Loop Cross Validation",
    "description": "Nested loop cross validation for classification purposes for misclassification error rate estimation.\n  The package supports several methodologies for feature selection: random forest, Student t-test, limma, \n  and provides an interface to the following classification methods in the 'MLInterfaces' package: linear, \n  quadratic discriminant analyses, random forest, bagging, prediction analysis for microarray, generalized \n  linear model, support vector machine (svm and ksvm). Visualizations to assess the quality of\n  the classifier are included: plot of the ranks of the features, scores plot for a specific \n  classification algorithm and number of features, misclassification rate \n  for the different number of features and classification algorithms tested and ROC plot.\n  For further details about the methodology, please check:\n  Markus Ruschhaupt, Wolfgang Huber, Annemarie Poustka, and Ulrich Mansmann (2004) \n  <doi:10.2202/1544-6115.1078>.",
    "version": "0.3.6",
    "maintainer": "Laure Cougnaud <laure.cougnaud@openanalytics.eu>",
    "author": "Willem Talloen [aut],\n  Tobias Verbeke [aut],\n  Laure Cougnaud [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nlcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlcv Nested Loop Cross Validation Nested loop cross validation for classification purposes for misclassification error rate estimation.\n  The package supports several methodologies for feature selection: random forest, Student t-test, limma, \n  and provides an interface to the following classification methods in the 'MLInterfaces' package: linear, \n  quadratic discriminant analyses, random forest, bagging, prediction analysis for microarray, generalized \n  linear model, support vector machine (svm and ksvm). Visualizations to assess the quality of\n  the classifier are included: plot of the ranks of the features, scores plot for a specific \n  classification algorithm and number of features, misclassification rate \n  for the different number of features and classification algorithms tested and ROC plot.\n  For further details about the methodology, please check:\n  Markus Ruschhaupt, Wolfgang Huber, Annemarie Poustka, and Ulrich Mansmann (2004) \n  <doi:10.2202/1544-6115.1078>.  "
  },
  {
    "id": 16936,
    "package_name": "nlist",
    "title": "Lists of Numeric Atomic Objects",
    "description": "Create and manipulate numeric list ('nlist') objects.  An\n    'nlist' is an S3 list of uniquely named numeric objects.  An numeric\n    object is an integer or double vector, matrix or array.  An 'nlists'\n    object is a S3 class list of 'nlist' objects with the same names,\n    dimensionalities and typeofs.  Numeric list objects are of interest\n    because they are the raw data inputs for analytic engines such as\n    'JAGS', 'STAN' and 'TMB'.  Numeric lists objects, which are useful for\n    storing multiple realizations of of simulated data sets, can be\n    converted to coda::mcmc and coda::mcmc.list objects.",
    "version": "0.4.0",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "author": "Joe Thorley [aut, cre] (ORCID: <https://orcid.org/0000-0002-7683-4592>),\n  Kirill M\u00fcller [ctb] (ORCID: <https://orcid.org/0000-0002-1416-3412>),\n  Nadine Hussein [ctb] (ORCID: <https://orcid.org/0000-0003-4470-8361>),\n  Ayla Pearson [ctb] (ORCID: <https://orcid.org/0000-0001-7388-1222>),\n  Poisson Consulting [cph, fnd]",
    "url": "https://github.com/poissonconsulting/nlist",
    "bug_reports": "https://github.com/poissonconsulting/nlist/issues",
    "repository": "https://cran.r-project.org/package=nlist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlist Lists of Numeric Atomic Objects Create and manipulate numeric list ('nlist') objects.  An\n    'nlist' is an S3 list of uniquely named numeric objects.  An numeric\n    object is an integer or double vector, matrix or array.  An 'nlists'\n    object is a S3 class list of 'nlist' objects with the same names,\n    dimensionalities and typeofs.  Numeric list objects are of interest\n    because they are the raw data inputs for analytic engines such as\n    'JAGS', 'STAN' and 'TMB'.  Numeric lists objects, which are useful for\n    storing multiple realizations of of simulated data sets, can be\n    converted to coda::mcmc and coda::mcmc.list objects.  "
  },
  {
    "id": 16965,
    "package_name": "nlsic",
    "title": "Non Linear Least Squares with Inequality Constraints",
    "description": "We solve non linear least squares problems with optional\n    equality and/or inequality constraints. Non linear iterations are\n    globalized with back-tracking method. Linear problems are solved by\n    dense QR decomposition from 'LAPACK' which can limit the size of\n    treated problems. On the other side, we avoid condition number\n    degradation which happens in classical quadratic programming approach.\n    Inequality constraints treatment on each non\n    linear iteration is based on 'NNLS' method (by Lawson and Hanson).\n    We provide an original function 'lsi_ln' for solving linear least squares\n    problem with inequality constraints in least norm sens. Thus if Jacobian of\n    the problem is rank deficient a solution still can be provided.\n    However, truncation errors are probable in this case.\n    Equality constraints are treated by using a basis of Null-space.\n    User defined function calculating residuals must return a list having\n    residual vector (not their squared sum) and Jacobian. If Jacobian is\n    not in the returned list, package 'numDeriv' is used to calculated\n    finite difference version of Jacobian. The 'NLSIC' method was fist\n    published in Sokol et al. (2012) <doi:10.1093/bioinformatics/btr716>.",
    "version": "1.1.1",
    "maintainer": "Serguei Sokol <sokol@insa-toulouse.fr>",
    "author": "Serguei Sokol [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5674-3327>)",
    "url": "https://github.com/MathsCell/nlsic",
    "bug_reports": "https://github.com/MathsCell/nlsic/issues",
    "repository": "https://cran.r-project.org/package=nlsic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlsic Non Linear Least Squares with Inequality Constraints We solve non linear least squares problems with optional\n    equality and/or inequality constraints. Non linear iterations are\n    globalized with back-tracking method. Linear problems are solved by\n    dense QR decomposition from 'LAPACK' which can limit the size of\n    treated problems. On the other side, we avoid condition number\n    degradation which happens in classical quadratic programming approach.\n    Inequality constraints treatment on each non\n    linear iteration is based on 'NNLS' method (by Lawson and Hanson).\n    We provide an original function 'lsi_ln' for solving linear least squares\n    problem with inequality constraints in least norm sens. Thus if Jacobian of\n    the problem is rank deficient a solution still can be provided.\n    However, truncation errors are probable in this case.\n    Equality constraints are treated by using a basis of Null-space.\n    User defined function calculating residuals must return a list having\n    residual vector (not their squared sum) and Jacobian. If Jacobian is\n    not in the returned list, package 'numDeriv' is used to calculated\n    finite difference version of Jacobian. The 'NLSIC' method was fist\n    published in Sokol et al. (2012) <doi:10.1093/bioinformatics/btr716>.  "
  },
  {
    "id": 16983,
    "package_name": "nnR",
    "title": "Neural Networks Made Algebraic",
    "description": "Do algebraic operations on neural networks. We seek here to implement\n  in R, operations on neural networks and their resulting approximations. Our operations derive\n  their descriptions mainly from\n  Rafi S., Padgett, J.L., and Nakarmi, U. (2024), \"Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials\", <doi:10.48550/arXiv.2402.01058>, \n  Grohs P., Hornung, F., Jentzen, A. et al. (2023), \"Space-time error estimates for deep neural network approximations for differential equations\", <doi:10.1007/s10444-022-09970-2>,\n  Jentzen A., Kuckuck B., von Wurstemberger, P. (2023), \"Mathematical Introduction to Deep Learning Methods, Implementations, and Theory\" <doi:10.48550/arXiv.2310.20360>.\n  Our implementation is meant mainly as a pedagogical tool, and proof of concept. Faster implementations with \n  deeper vectorizations may be made in future versions. ",
    "version": "0.1.0",
    "maintainer": "Shakil Rafi <sarafi@uark.edu>",
    "author": "Shakil Rafi [aut, cre] (ORCID: <https://orcid.org/0000-0003-3791-9697>),\n  Joshua Lee Padgett [aut] (ORCID:\n    <https://orcid.org/0000-0001-9369-351X>),\n  Ukash Nakarmi [ctb] (ORCID: <https://orcid.org/0000-0002-5351-3956>)",
    "url": "https://github.com/2shakilrafi/nnR/",
    "bug_reports": "https://github.com/2shakilrafi/nnR/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc",
    "repository": "https://cran.r-project.org/package=nnR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nnR Neural Networks Made Algebraic Do algebraic operations on neural networks. We seek here to implement\n  in R, operations on neural networks and their resulting approximations. Our operations derive\n  their descriptions mainly from\n  Rafi S., Padgett, J.L., and Nakarmi, U. (2024), \"Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials\", <doi:10.48550/arXiv.2402.01058>, \n  Grohs P., Hornung, F., Jentzen, A. et al. (2023), \"Space-time error estimates for deep neural network approximations for differential equations\", <doi:10.1007/s10444-022-09970-2>,\n  Jentzen A., Kuckuck B., von Wurstemberger, P. (2023), \"Mathematical Introduction to Deep Learning Methods, Implementations, and Theory\" <doi:10.48550/arXiv.2310.20360>.\n  Our implementation is meant mainly as a pedagogical tool, and proof of concept. Faster implementations with \n  deeper vectorizations may be made in future versions.   "
  },
  {
    "id": 16990,
    "package_name": "nngeo",
    "title": "k-Nearest Neighbor Join for Spatial Data",
    "description": "K-nearest neighbor search for projected and non-projected 'sf' spatial layers. Nearest neighbor search uses (1) C code from 'GeographicLib' for lon-lat point layers, (2) function knn() from package 'nabor' for projected point layers, or (3) function st_distance() from package 'sf' for line or polygon layers. The package also includes several other utility functions for spatial analysis.",
    "version": "0.4.8",
    "maintainer": "Michael Dorman <dorman@post.bgu.ac.il>",
    "author": "Michael Dorman [aut, cre],\n  Johnathan Rush [ctb],\n  Ian Hough [ctb],\n  Dominic Russel [ctb],\n  Luigi Ranghetti [ctb],\n  Attilio Benini [ctb],\n  Arnaud Tarroux [ctb],\n  Felipe Matas [ctb],\n  Charles F.F Karney [ctb, cph] (Author of included C code from\n    'GeographicLib' for geodesic distance)",
    "url": "https://michaeldorman.github.io/nngeo/,\nhttps://github.com/michaeldorman/nngeo/",
    "bug_reports": "https://github.com/michaeldorman/nngeo/issues/",
    "repository": "https://cran.r-project.org/package=nngeo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nngeo k-Nearest Neighbor Join for Spatial Data K-nearest neighbor search for projected and non-projected 'sf' spatial layers. Nearest neighbor search uses (1) C code from 'GeographicLib' for lon-lat point layers, (2) function knn() from package 'nabor' for projected point layers, or (3) function st_distance() from package 'sf' for line or polygon layers. The package also includes several other utility functions for spatial analysis.  "
  },
  {
    "id": 16998,
    "package_name": "noah",
    "title": "Create Unique Pseudonymous Animal Names",
    "description": "Generate pseudonymous animal names that are delightful and easy to \n    remember like the Likable Leech and the Proud Chickadee. A unique pseudonym \n    can be created for every unique element in a vector or row in a data frame. \n    Pseudonyms can be customized and tracked over time, so that the same input \n    is always assigned the same pseudonym.",
    "version": "0.1.0",
    "maintainer": "Tobias Busch <teebusch@gmail.com>",
    "author": "Tobias Busch [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8390-7892>)",
    "url": "https://github.com/Teebusch/noah",
    "bug_reports": "https://github.com/Teebusch/noah/issues",
    "repository": "https://cran.r-project.org/package=noah",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "noah Create Unique Pseudonymous Animal Names Generate pseudonymous animal names that are delightful and easy to \n    remember like the Likable Leech and the Proud Chickadee. A unique pseudonym \n    can be created for every unique element in a vector or row in a data frame. \n    Pseudonyms can be customized and tracked over time, so that the same input \n    is always assigned the same pseudonym.  "
  },
  {
    "id": 17011,
    "package_name": "nolock",
    "title": "Append 'WITH (NOLOCK)' to 'SQL' Queries, Get Packages in Active\nScript",
    "description": "Provides a suite of tools that can assist in enhancing the \n    processing efficiency of 'SQL' and 'R' scripts. \n    - The 'libr_unused()' retrieves a vector of package names that are called \n    within an 'R' script but are never actually used in the script.\n    - The 'libr_used()' retrieves a vector of package names actively utilized \n    within an 'R' script; packages loaded using 'library()' but not actually \n    used in the script will not be included.\n    - The 'libr_called()' retrieves a vector of all package names which are \n    called within an 'R' script.\n    - 'nolock()' appends 'WITH (nolock)' to all tables in 'SQL' queries. This \n    facilitates reading from databases in scenarios where non-blocking reads are \n    preferable, such as in high-transaction environments.",
    "version": "1.1.0",
    "maintainer": "Arkadiusz W. Pajda <arkadiusz.pajda.97@onet.pl>",
    "author": "Arkadiusz W. Pajda [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nolock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nolock Append 'WITH (NOLOCK)' to 'SQL' Queries, Get Packages in Active\nScript Provides a suite of tools that can assist in enhancing the \n    processing efficiency of 'SQL' and 'R' scripts. \n    - The 'libr_unused()' retrieves a vector of package names that are called \n    within an 'R' script but are never actually used in the script.\n    - The 'libr_used()' retrieves a vector of package names actively utilized \n    within an 'R' script; packages loaded using 'library()' but not actually \n    used in the script will not be included.\n    - The 'libr_called()' retrieves a vector of all package names which are \n    called within an 'R' script.\n    - 'nolock()' appends 'WITH (nolock)' to all tables in 'SQL' queries. This \n    facilitates reading from databases in scenarios where non-blocking reads are \n    preferable, such as in high-transaction environments.  "
  },
  {
    "id": 17012,
    "package_name": "nombre",
    "title": "Number Names",
    "description": "Converts numeric vectors to character vectors of English\n    number names. Provides conversion to cardinals, ordinals, numerators,\n    and denominators. Supports negative and non-integer numbers.",
    "version": "0.4.1",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>),\n  Eli Pousson [ctb]",
    "url": "https://nombre.rossellhayes.com,\nhttps://github.com/rossellhayes/nombre",
    "bug_reports": "https://github.com/rossellhayes/nombre/issues",
    "repository": "https://cran.r-project.org/package=nombre",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nombre Number Names Converts numeric vectors to character vectors of English\n    number names. Provides conversion to cardinals, ordinals, numerators,\n    and denominators. Supports negative and non-integer numbers.  "
  },
  {
    "id": 17044,
    "package_name": "normaliseR",
    "title": "Re-Scale Vectors and Time-Series Features",
    "description": "Provides standardized access to a range of re-scaling methods for numerical vectors\n    and time-series features calculated within the 'theft' ecosystem.",
    "version": "0.1.2",
    "maintainer": "Trent Henderson <then6675@uni.sydney.edu.au>",
    "author": "Trent Henderson [cre, aut]",
    "url": "https://hendersontrent.github.io/normaliseR/",
    "bug_reports": "https://github.com/hendersontrent/normaliseR/issues",
    "repository": "https://cran.r-project.org/package=normaliseR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "normaliseR Re-Scale Vectors and Time-Series Features Provides standardized access to a range of re-scaling methods for numerical vectors\n    and time-series features calculated within the 'theft' ecosystem.  "
  },
  {
    "id": 17098,
    "package_name": "npphen",
    "title": "Vegetation Phenological Cycle and Anomaly Detection using Remote\nSensing Data",
    "description": "Calculates phenological cycle and anomalies using a non-parametric\n    approach applied to time series of vegetation indices derived from remote sensing data \n    or field measurements. The package implements basic and high-level functions for \n    manipulating vector data (numerical series) and raster data (satellite derived products).\n    Processing of very large raster files is supported. For more information, please check \n    the following paper:\n    Ch\u00e1vez et al. (2023) <doi:10.3390/rs15010073>.",
    "version": "2.0.1",
    "maintainer": "Jos\u00e9 A. Lastra <jose.lastra@pucv.cl>",
    "author": "Roberto O. Ch\u00e1vez [aut] (ORCID:\n    <https://orcid.org/0000-0001-6782-3579>),\n  Sergio A. Estay [aut] (ORCID: <https://orcid.org/0000-0002-3797-8964>),\n  Jos\u00e9 A. Lastra [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-6159-2201>),\n  Carlos G. Riquelme [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4861-8355>)",
    "url": "https://github.com/labGRS/npphen, https://labgrs.github.io/npphen/",
    "bug_reports": "https://github.com/labGRS/npphen/issues",
    "repository": "https://cran.r-project.org/package=npphen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "npphen Vegetation Phenological Cycle and Anomaly Detection using Remote\nSensing Data Calculates phenological cycle and anomalies using a non-parametric\n    approach applied to time series of vegetation indices derived from remote sensing data \n    or field measurements. The package implements basic and high-level functions for \n    manipulating vector data (numerical series) and raster data (satellite derived products).\n    Processing of very large raster files is supported. For more information, please check \n    the following paper:\n    Ch\u00e1vez et al. (2023) <doi:10.3390/rs15010073>.  "
  },
  {
    "id": 17103,
    "package_name": "nproc",
    "title": "Neyman-Pearson (NP) Classification Algorithms and NP Receiver\nOperating Characteristic (NP-ROC) Curves",
    "description": "In many binary classification applications, such as disease\n    diagnosis and spam detection, practitioners commonly face the need to limit\n    type I error (i.e., the conditional probability of misclassifying a class 0\n    observation as class 1) so that it remains below a desired threshold. To address\n    this need, the Neyman-Pearson (NP) classification paradigm is a natural choice;\n    it minimizes type II error (i.e., the conditional probability of misclassifying\n    a class 1 observation as class 0) while enforcing an upper bound, alpha, on the\n    type I error. Although the NP paradigm has a century-long history in hypothesis\n    testing, it has not been well recognized and implemented in classification\n    schemes. Common practices that directly limit the empirical type I error to\n    no more than alpha do not satisfy the type I error control objective because\n    the resulting classifiers are still likely to have type I errors much larger\n    than alpha. As a result, the NP paradigm has not been properly implemented\n    for many classification scenarios in practice. In this work, we develop the\n    first umbrella algorithm that implements the NP paradigm for all scoring-type\n    classification methods, including popular methods such as logistic regression,\n    support vector machines and random forests. Powered by this umbrella algorithm,\n    we propose a novel graphical tool for NP classification methods: NP receiver\n    operating characteristic (NP-ROC) bands, motivated by the popular receiver\n    operating characteristic (ROC) curves. NP-ROC bands will help choose in a data\n    adaptive way and compare different NP classifiers. ",
    "version": "2.1.5",
    "maintainer": "Yang Feng <yangfengstat@gmail.com>",
    "author": "Yang Feng [aut, cre],\n  Jessica Li [aut],\n  Xin Tong [aut],\n  Ye Tian [ctb]",
    "url": "http://advances.sciencemag.org/content/4/2/eaao1659",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nproc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nproc Neyman-Pearson (NP) Classification Algorithms and NP Receiver\nOperating Characteristic (NP-ROC) Curves In many binary classification applications, such as disease\n    diagnosis and spam detection, practitioners commonly face the need to limit\n    type I error (i.e., the conditional probability of misclassifying a class 0\n    observation as class 1) so that it remains below a desired threshold. To address\n    this need, the Neyman-Pearson (NP) classification paradigm is a natural choice;\n    it minimizes type II error (i.e., the conditional probability of misclassifying\n    a class 1 observation as class 0) while enforcing an upper bound, alpha, on the\n    type I error. Although the NP paradigm has a century-long history in hypothesis\n    testing, it has not been well recognized and implemented in classification\n    schemes. Common practices that directly limit the empirical type I error to\n    no more than alpha do not satisfy the type I error control objective because\n    the resulting classifiers are still likely to have type I errors much larger\n    than alpha. As a result, the NP paradigm has not been properly implemented\n    for many classification scenarios in practice. In this work, we develop the\n    first umbrella algorithm that implements the NP paradigm for all scoring-type\n    classification methods, including popular methods such as logistic regression,\n    support vector machines and random forests. Powered by this umbrella algorithm,\n    we propose a novel graphical tool for NP classification methods: NP receiver\n    operating characteristic (NP-ROC) bands, motivated by the popular receiver\n    operating characteristic (ROC) curves. NP-ROC bands will help choose in a data\n    adaptive way and compare different NP classifiers.   "
  },
  {
    "id": 17107,
    "package_name": "npsp",
    "title": "Nonparametric Spatial Statistics",
    "description": "Multidimensional nonparametric spatial (spatio-temporal) geostatistics.\n    S3 classes and methods for multidimensional: linear binning,\n    local polynomial kernel regression (spatial trend estimation), density and variogram estimation.\n    Nonparametric methods for simultaneous inference on both spatial trend\n    and variogram functions (for spatial processes).\n    Nonparametric residual kriging (spatial prediction).\n    For details on these methods see, for example, Fernandez-Casal and Francisco-Fernandez (2014) \n    <doi:10.1007/s00477-013-0817-8> or Castillo-Paez et al. (2019) <doi:10.1016/j.csda.2019.01.017>.",
    "version": "0.7-13",
    "maintainer": "Ruben Fernandez-Casal <rubenfcasal@gmail.com>",
    "author": "Ruben Fernandez-Casal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5785-3739>)",
    "url": "https://rubenfcasal.github.io/npsp/,\nhttps://github.com/rubenfcasal/npsp/",
    "bug_reports": "https://github.com/rubenfcasal/npsp/issues/",
    "repository": "https://cran.r-project.org/package=npsp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "npsp Nonparametric Spatial Statistics Multidimensional nonparametric spatial (spatio-temporal) geostatistics.\n    S3 classes and methods for multidimensional: linear binning,\n    local polynomial kernel regression (spatial trend estimation), density and variogram estimation.\n    Nonparametric methods for simultaneous inference on both spatial trend\n    and variogram functions (for spatial processes).\n    Nonparametric residual kriging (spatial prediction).\n    For details on these methods see, for example, Fernandez-Casal and Francisco-Fernandez (2014) \n    <doi:10.1007/s00477-013-0817-8> or Castillo-Paez et al. (2019) <doi:10.1016/j.csda.2019.01.017>.  "
  },
  {
    "id": 17118,
    "package_name": "nscancor",
    "title": "Non-Negative and Sparse CCA",
    "description": "Two implementations of canonical correlation analysis\n        (CCA) that are based on iterated regression. By choosing the\n        appropriate regression algorithm for each data domain, it is\n        possible to enforce sparsity, non-negativity or other kinds of\n        constraints on the projection vectors. Multiple canonical\n        variables are computed sequentially using a generalized\n        deflation scheme, where the additional correlation not\n        explained by previous variables is maximized. nscancor() is\n        used to analyze paired data from two domains, and has the same\n        interface as cancor() from the 'stats' package (plus some extra\n        parameters). mcancor() is appropriate for analyzing data from\n        three or more domains. See\n        <https://sigg-iten.ch/learningbits/2014/01/20/canonical-correlation-analysis-under-constraints/>\n        and Sigg et al. (2007) <doi:10.1109/MLSP.2007.4414315> for more\n        details.",
    "version": "0.7.0-6",
    "maintainer": "Christian Sigg <christian@sigg-iten.ch>",
    "author": "Christian Sigg [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0003-1067-9224>),\n  R Core team [cph, ctb] (cancor() interface and documentation)",
    "url": "https://sigg-iten.ch/research/",
    "bug_reports": "https://github.com/chrsigg/nscancor/issues",
    "repository": "https://cran.r-project.org/package=nscancor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nscancor Non-Negative and Sparse CCA Two implementations of canonical correlation analysis\n        (CCA) that are based on iterated regression. By choosing the\n        appropriate regression algorithm for each data domain, it is\n        possible to enforce sparsity, non-negativity or other kinds of\n        constraints on the projection vectors. Multiple canonical\n        variables are computed sequentially using a generalized\n        deflation scheme, where the additional correlation not\n        explained by previous variables is maximized. nscancor() is\n        used to analyze paired data from two domains, and has the same\n        interface as cancor() from the 'stats' package (plus some extra\n        parameters). mcancor() is appropriate for analyzing data from\n        three or more domains. See\n        <https://sigg-iten.ch/learningbits/2014/01/20/canonical-correlation-analysis-under-constraints/>\n        and Sigg et al. (2007) <doi:10.1109/MLSP.2007.4414315> for more\n        details.  "
  },
  {
    "id": 17119,
    "package_name": "nseq",
    "title": "Count of Sequential Events",
    "description": "Count the occurrence of sequences of values in a vector that meets certain conditions of length and magnitude. The method is based on the Run Length Encoding algorithm, available with base R, inspired by A. H. Robinson and C. Cherry (1967) <doi:10.1109/PROC.1967.5493>.",
    "version": "0.1.1",
    "maintainer": "Raphael Saldanha <raphael.de-freitas-saldanha@inria.fr>",
    "author": "Raphael Saldanha [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0652-8466>)",
    "url": "https://rfsaldanha.github.io/nseq/",
    "bug_reports": "https://github.com/rfsaldanha/nseq/issues",
    "repository": "https://cran.r-project.org/package=nseq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nseq Count of Sequential Events Count the occurrence of sequences of values in a vector that meets certain conditions of length and magnitude. The method is based on the Run Length Encoding algorithm, available with base R, inspired by A. H. Robinson and C. Cherry (1967) <doi:10.1109/PROC.1967.5493>.  "
  },
  {
    "id": 17127,
    "package_name": "nsyllable",
    "title": "Count Syllables in Character Vectors",
    "description": "Counts syllables in character vectors for English words.  Imputes syllables as the number of vowel sequences for words not found.  ",
    "version": "1.0.1",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Carnegie Mellon University [cph] (CMU Pronunciation Dictionary (c)\n    1993-2015)",
    "url": "https://github.com/quanteda/nsyllable",
    "bug_reports": "https://github.com/quanteda/nsyllable/issues",
    "repository": "https://cran.r-project.org/package=nsyllable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nsyllable Count Syllables in Character Vectors Counts syllables in character vectors for English words.  Imputes syllables as the number of vowel sequences for words not found.    "
  },
  {
    "id": 17141,
    "package_name": "numbersBR",
    "title": "Validate, Compare and Format Identification Numbers from Brazil",
    "description": "Validate, format and compare identification numbers used in Brazil.\n  These numbers are used to identify individuals (CPF), vehicles (RENAVAN),\n  companies (CNPJ) and etc.\n  Functions to format, validate and compare these numbers have been \n  implemented in a vectorized way in order to speed up\n  validations and comparisons in big datasets.",
    "version": "0.0.2",
    "maintainer": "Wilson Freitas <wilson.freitas@gmail.com>",
    "author": "Wilson Freitas [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=numbersBR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "numbersBR Validate, Compare and Format Identification Numbers from Brazil Validate, format and compare identification numbers used in Brazil.\n  These numbers are used to identify individuals (CPF), vehicles (RENAVAN),\n  companies (CNPJ) and etc.\n  Functions to format, validate and compare these numbers have been \n  implemented in a vectorized way in order to speed up\n  validations and comparisons in big datasets.  "
  },
  {
    "id": 17182,
    "package_name": "oceanmap",
    "title": "A Plotting Toolbox for 2D Oceanographic Data",
    "description": "Plotting toolbox for 2D oceanographic data (satellite data, sea surface temperature, chlorophyll, ocean fronts & bathymetry). Recognized classes and formats include netcdf, Raster, '.nc' and '.gz' files.",
    "version": "0.1.7",
    "maintainer": "Robert K. Bauer <marine.biologging@gmail.com>",
    "author": "Robert K. Bauer [aut, cre] (ORCID: 0000-0003-4224-8023)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=oceanmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oceanmap A Plotting Toolbox for 2D Oceanographic Data Plotting toolbox for 2D oceanographic data (satellite data, sea surface temperature, chlorophyll, ocean fronts & bathymetry). Recognized classes and formats include netcdf, Raster, '.nc' and '.gz' files.  "
  },
  {
    "id": 17215,
    "package_name": "officedown",
    "title": "Enhanced 'R Markdown' Format for 'Word' and 'PowerPoint'",
    "description": "Allows production of 'Microsoft' corporate documents from 'R\n    Markdown' by reusing formatting defined in 'Microsoft Word' documents.\n    You can reuse table styles, list styles but also add column sections,\n    landscape oriented pages. Table and image captions as well as\n    cross-references are transformed into 'Microsoft Word' fields,\n    allowing documents edition and merging without issue with references;\n    the syntax conforms to the 'bookdown' cross-reference definition.\n    Objects generated by the 'officer' package are also supported in the\n    'knitr' chunks.  'Microsoft PowerPoint' presentations also benefit\n    from this as well as the ability to produce editable vector graphics\n    in 'PowerPoint' and also to define placeholder where content is to be\n    added.",
    "version": "0.4.1",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre, cph],\n  ArData [cph],\n  Institut f\u00fcr Qualit\u00e4tssicherung und Transparenz im Gesundheitswesen\n    [fnd],\n  Noam Ross [aut] (rmarkdown implementation for rvg),\n  ArData [cph],\n  Martin Camitz [ctb]",
    "url": "https://ardata-fr.github.io/officeverse/,\nhttps://davidgohel.github.io/officedown/",
    "bug_reports": "https://github.com/davidgohel/officedown/issues",
    "repository": "https://cran.r-project.org/package=officedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "officedown Enhanced 'R Markdown' Format for 'Word' and 'PowerPoint' Allows production of 'Microsoft' corporate documents from 'R\n    Markdown' by reusing formatting defined in 'Microsoft Word' documents.\n    You can reuse table styles, list styles but also add column sections,\n    landscape oriented pages. Table and image captions as well as\n    cross-references are transformed into 'Microsoft Word' fields,\n    allowing documents edition and merging without issue with references;\n    the syntax conforms to the 'bookdown' cross-reference definition.\n    Objects generated by the 'officer' package are also supported in the\n    'knitr' chunks.  'Microsoft PowerPoint' presentations also benefit\n    from this as well as the ability to produce editable vector graphics\n    in 'PowerPoint' and also to define placeholder where content is to be\n    added.  "
  },
  {
    "id": 17238,
    "package_name": "omnibus",
    "title": "Helper Tools for Managing Data, Dates, Missing Values, and Text",
    "description": "An assortment of helper functions for managing data (e.g.,\n\trotating values in matrices by a user-defined angle, switching from\n\trow- to column-indexing), dates (e.g., intuiting year from messy date\n\tstrings), handling missing values (e.g., removing elements/rows across\n\tmultiple vectors or matrices if any have an NA), text (e.g.,\n\tflushing reports to the console in real-time); and combining data frames\n\twith different schema (copying, filling, or concatenating columns or\n\tapplying functions before combining).",
    "version": "1.2.15",
    "maintainer": "Adam B. Smith <adam.smith@mobot.org>",
    "author": "Adam B. Smith [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-6420-1659>)",
    "url": "https://github.com/adamlilith/omnibus,\nhttps://adamlilith.github.io/omnibus/",
    "bug_reports": "https://github.com/adamlilith/omnibus/issues",
    "repository": "https://cran.r-project.org/package=omnibus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "omnibus Helper Tools for Managing Data, Dates, Missing Values, and Text An assortment of helper functions for managing data (e.g.,\n\trotating values in matrices by a user-defined angle, switching from\n\trow- to column-indexing), dates (e.g., intuiting year from messy date\n\tstrings), handling missing values (e.g., removing elements/rows across\n\tmultiple vectors or matrices if any have an NA), text (e.g.,\n\tflushing reports to the console in real-time); and combining data frames\n\twith different schema (copying, filling, or concatenating columns or\n\tapplying functions before combining).  "
  },
  {
    "id": 17253,
    "package_name": "onehot",
    "title": "Fast Onehot Encoding for Data.frames",
    "description": "Quickly create numeric matrices for machine learning algorithms\n    that require them. It converts factor columns into onehot vectors.",
    "version": "0.1.1",
    "maintainer": "Eric E. Graves <gravcon5@gmail.com>",
    "author": "Eric E. Graves [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=onehot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "onehot Fast Onehot Encoding for Data.frames Quickly create numeric matrices for machine learning algorithms\n    that require them. It converts factor columns into onehot vectors.  "
  },
  {
    "id": 17278,
    "package_name": "ontophylo",
    "title": "Ontology-Informed Phylogenetic Comparative Analyses",
    "description": "Provides new tools for analyzing discrete trait data integrating bio-ontologies and phylogenetics. It expands on the previous work of Tarasov et al. (2019) <doi:10.1093/isd/ixz009>. The PARAMO pipeline allows to reconstruct ancestral phenomes treating groups of morphological traits as a single complex character. The pipeline incorporates knowledge from ontologies during the amalgamation of individual character stochastic maps.\n  Here we expand the current PARAMO functionality by adding new statistical methods for inferring evolutionary phenome dynamics using non-homogeneous Poisson process (NHPP). The new functionalities include: (1) reconstruction of evolutionary rate shifts of phenomes across lineages and time; (2) reconstruction of morphospace dynamics through time; and (3) estimation of rates of phenome evolution at different levels of anatomical hierarchy (e.g., entire body or specific regions only). The package also includes user-friendly tools for visualizing evolutionary rates of different anatomical regions using vector images of the organisms of interest.",
    "version": "1.1.3",
    "maintainer": "Diego S. Porto <diegosporto@gmail.com>",
    "author": "Diego S. Porto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1657-9606>),\n  Sergei Tarasov [aut] (ORCID: <https://orcid.org/0000-0001-5237-2330>)",
    "url": "https://github.com/diegosasso/ontophylo",
    "bug_reports": "https://github.com/diegosasso/ontophylo/issues",
    "repository": "https://cran.r-project.org/package=ontophylo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ontophylo Ontology-Informed Phylogenetic Comparative Analyses Provides new tools for analyzing discrete trait data integrating bio-ontologies and phylogenetics. It expands on the previous work of Tarasov et al. (2019) <doi:10.1093/isd/ixz009>. The PARAMO pipeline allows to reconstruct ancestral phenomes treating groups of morphological traits as a single complex character. The pipeline incorporates knowledge from ontologies during the amalgamation of individual character stochastic maps.\n  Here we expand the current PARAMO functionality by adding new statistical methods for inferring evolutionary phenome dynamics using non-homogeneous Poisson process (NHPP). The new functionalities include: (1) reconstruction of evolutionary rate shifts of phenomes across lineages and time; (2) reconstruction of morphospace dynamics through time; and (3) estimation of rates of phenome evolution at different levels of anatomical hierarchy (e.g., entire body or specific regions only). The package also includes user-friendly tools for visualizing evolutionary rates of different anatomical regions using vector images of the organisms of interest.  "
  },
  {
    "id": 17294,
    "package_name": "openSkies",
    "title": "Retrieval, Analysis and Visualization of Air Traffic Data",
    "description": "Provides functionalities and data structures to retrieve, analyze and visualize aviation \n    data. It includes a client interface to the 'OpenSky' API <https://opensky-network.org>. It allows \n    retrieval of flight information, as well as aircraft state vectors.",
    "version": "1.2.2",
    "maintainer": "Rafael Ayala <rafaelayalahernandez@gmail.com>",
    "author": "Rafael Ayala [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9332-4623>),\n  Daniel Ayala [aut] (ORCID: <https://orcid.org/0000-0003-2095-1009>),\n  David Ruiz [aut] (ORCID: <https://orcid.org/0000-0003-4460-5493>),\n  Aleix Sell\u00e9s [aut],\n  Lara Selles Vidal [aut] (ORCID:\n    <https://orcid.org/0000-0003-2537-6824>)",
    "url": "",
    "bug_reports": "https://github.com/Rafael-Ayala/openSkies/issues",
    "repository": "https://cran.r-project.org/package=openSkies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openSkies Retrieval, Analysis and Visualization of Air Traffic Data Provides functionalities and data structures to retrieve, analyze and visualize aviation \n    data. It includes a client interface to the 'OpenSky' API <https://opensky-network.org>. It allows \n    retrieval of flight information, as well as aircraft state vectors.  "
  },
  {
    "id": 17360,
    "package_name": "optiscale",
    "title": "Optimal Scaling",
    "description": "Optimal scaling of a data vector, relative to a set of targets, is\n    obtained through a least-squares transformation subject to appropriate measurement\n    constraints. The targets are usually predicted values from a statistical\n    model. If the data are nominal level, then the transformation must be\n    identity-preserving. If the data are ordinal level, then the\n    transformation must be monotonic. If the data are discrete, then tied data\n    values must remain tied in the optimal transformation. If the data are\n    continuous, then tied data values can be untied in the optimal\n    transformation.",
    "version": "1.2.3",
    "maintainer": "Dave Armstrong <davearmstrong.ps@gmail.com>",
    "author": "Dave Armstrong [aut, cre],\n  William Jacoby [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=optiscale",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "optiscale Optimal Scaling Optimal scaling of a data vector, relative to a set of targets, is\n    obtained through a least-squares transformation subject to appropriate measurement\n    constraints. The targets are usually predicted values from a statistical\n    model. If the data are nominal level, then the transformation must be\n    identity-preserving. If the data are ordinal level, then the\n    transformation must be monotonic. If the data are discrete, then tied data\n    values must remain tied in the optimal transformation. If the data are\n    continuous, then tied data values can be untied in the optimal\n    transformation.  "
  },
  {
    "id": 17377,
    "package_name": "ordering",
    "title": "Test, Check, Verify, Investigate the Monotonic Properties of\nVectors",
    "description": "Functions to test/check/verify/investigate the ordering of vectors. \n    The 'is_[strictly_]*' family of functions test vectors for \n    'sorted', 'monotonic', 'increasing', 'decreasing' order; 'is_constant' \n    and 'is_incremental' test for the degree of ordering. `ordering` \n    provides a numeric indication of ordering -2 (strictly decreasing) to \n    2 (strictly increasing).",
    "version": "0.7.0",
    "maintainer": "Christopher Brown <chris.brown@decisionpatterns.com>",
    "author": "Christopher Brown [aut, cre],\n  Decision Patterns [cph]",
    "url": "https://github.com/decisionpatterns/ordering",
    "bug_reports": "https://github.com/decisionpatterns/ordering/issues",
    "repository": "https://cran.r-project.org/package=ordering",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ordering Test, Check, Verify, Investigate the Monotonic Properties of\nVectors Functions to test/check/verify/investigate the ordering of vectors. \n    The 'is_[strictly_]*' family of functions test vectors for \n    'sorted', 'monotonic', 'increasing', 'decreasing' order; 'is_constant' \n    and 'is_incremental' test for the degree of ordering. `ordering` \n    provides a numeric indication of ordering -2 (strictly decreasing) to \n    2 (strictly increasing).  "
  },
  {
    "id": 17379,
    "package_name": "orderstats",
    "title": "Efficiently Generates Random Order Statistic Variables",
    "description": "All the methods in this package generate a vector of uniform order statistics using a beta distribution and use an inverse cumulative distribution function for some distribution to give a vector of random order statistic variables for some distribution. This is much more efficient than using a loop since it is directly sampling from the order statistic distribution.",
    "version": "0.1.0",
    "maintainer": "Christian Sims <christian_j_sims@yahoo.com>",
    "author": "Christian Sims <christian_j_sims@yahoo.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=orderstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "orderstats Efficiently Generates Random Order Statistic Variables All the methods in this package generate a vector of uniform order statistics using a beta distribution and use an inverse cumulative distribution function for some distribution to give a vector of random order statistic variables for some distribution. This is much more efficient than using a loop since it is directly sampling from the order statistic distribution.  "
  },
  {
    "id": 17386,
    "package_name": "ordinalNet",
    "title": "Penalized Ordinal Regression",
    "description": "Fits ordinal regression models with elastic net penalty.\n    Supported model families include cumulative probability, stopping ratio, \n    continuation ratio, and adjacent category. These families are a subset of \n    vector glm's which belong to a model class we call the elementwise link \n    multinomial-ordinal (ELMO) class. Each family in this class links a vector \n    of covariates to a vector of class probabilities. Each of these families \n    has a parallel form, which is appropriate for ordinal response data, as \n    well as a nonparallel form that is appropriate for an unordered categorical\n    response, or as a more flexible model for ordinal data. The parallel model\n    has a single set of coefficients, whereas the nonparallel model has a set of\n    coefficients for each response category except the baseline category. It is \n    also possible to fit a model with both parallel and nonparallel terms, which \n    we call the semi-parallel model. The semi-parallel model has the flexibility \n    of the nonparallel model, but the elastic net penalty shrinks it toward the \n    parallel model. For details, refer to Wurm, Hanlon, and Rathouz (2021) \n    <doi:10.18637/jss.v099.i06>.",
    "version": "2.13",
    "maintainer": "Michael Wurm <wurm@uwalumni.com>",
    "author": "Michael Wurm [aut, cre],\n  Paul Rathouz [aut],\n  Bret Hanlon [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ordinalNet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ordinalNet Penalized Ordinal Regression Fits ordinal regression models with elastic net penalty.\n    Supported model families include cumulative probability, stopping ratio, \n    continuation ratio, and adjacent category. These families are a subset of \n    vector glm's which belong to a model class we call the elementwise link \n    multinomial-ordinal (ELMO) class. Each family in this class links a vector \n    of covariates to a vector of class probabilities. Each of these families \n    has a parallel form, which is appropriate for ordinal response data, as \n    well as a nonparallel form that is appropriate for an unordered categorical\n    response, or as a more flexible model for ordinal data. The parallel model\n    has a single set of coefficients, whereas the nonparallel model has a set of\n    coefficients for each response category except the baseline category. It is \n    also possible to fit a model with both parallel and nonparallel terms, which \n    we call the semi-parallel model. The semi-parallel model has the flexibility \n    of the nonparallel model, but the elastic net penalty shrinks it toward the \n    parallel model. For details, refer to Wurm, Hanlon, and Rathouz (2021) \n    <doi:10.18637/jss.v099.i06>.  "
  },
  {
    "id": 17395,
    "package_name": "oreo",
    "title": "Large Amplitude Oscillatory Shear (LAOS)",
    "description": "The Sequence of Physical Processes (SPP) framework is a way of interpreting the transient data derived from oscillatory rheological tests. It is designed to allow both the linear and non-linear deformation regimes to be understood within a single unified framework. This code provides a convenient way to determine the SPP framework metrics for a given sample of oscillatory data. It will produce a text file containing the SPP metrics, which the user can then plot using their software of choice. It can also produce a second text file with additional derived data (components of tangent, normal, and binormal vectors), as well as pre-plotted figures if so desired. It is the R version of the Package SPP by Simon Rogers Group for Soft Matter (Simon A. Rogers, Brian M. Erwin, Dimitris Vlassopoulos, Michel Cloitre (2011) <doi:10.1122/1.3544591>).",
    "version": "1.0",
    "maintainer": "Serena Berretta <serena.berretta@ge.imati.cnr.it>",
    "author": "Serena Berretta [aut, cre],\n  Giorgio Luciano [aut],\n  Kristian Hovde Liland [ctb],\n  Simon Rogers [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=oreo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oreo Large Amplitude Oscillatory Shear (LAOS) The Sequence of Physical Processes (SPP) framework is a way of interpreting the transient data derived from oscillatory rheological tests. It is designed to allow both the linear and non-linear deformation regimes to be understood within a single unified framework. This code provides a convenient way to determine the SPP framework metrics for a given sample of oscillatory data. It will produce a text file containing the SPP metrics, which the user can then plot using their software of choice. It can also produce a second text file with additional derived data (components of tangent, normal, and binormal vectors), as well as pre-plotted figures if so desired. It is the R version of the Package SPP by Simon Rogers Group for Soft Matter (Simon A. Rogers, Brian M. Erwin, Dimitris Vlassopoulos, Michel Cloitre (2011) <doi:10.1122/1.3544591>).  "
  },
  {
    "id": 17418,
    "package_name": "osc",
    "title": "Orthodromic Spatial Clustering",
    "description": "Allows distance based spatial clustering of georeferenced data by implementing the City Clustering Algorithm - CCA. Multiple versions allow clustering for a matrix, raster and single coordinates on a plain (Euclidean distance) or on a sphere (great-circle or orthodromic distance).",
    "version": "1.0.5",
    "maintainer": "Steffen Kriewald <kriewald@pik-potsdam.de>",
    "author": "Steffen Kriewald, Till Fluschnik, Dominik Reusser, Diego Rybski",
    "url": "https://www.pik-potsdam.de/~kriewald/osc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=osc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "osc Orthodromic Spatial Clustering Allows distance based spatial clustering of georeferenced data by implementing the City Clustering Algorithm - CCA. Multiple versions allow clustering for a matrix, raster and single coordinates on a plain (Euclidean distance) or on a sphere (great-circle or orthodromic distance).  "
  },
  {
    "id": 17462,
    "package_name": "ows4R",
    "title": "Interface to OGC Web-Services (OWS)",
    "description": "Provides an Interface to Web-Services defined as standards by the Open Geospatial Consortium (OGC), including Web Feature Service\n (WFS) for vector data, Web Coverage Service (WCS), Catalogue Service (CSW) for ISO/OGC metadata, Web Processing Service (WPS) for data processes,\n and associated standards such as the common web-service specification (OWS) and OGC Filter Encoding. Partial support is provided for the Web Map \n Service (WMS). The purpose is to add support for additional OGC service standards such as Web Coverage Processing Service (WCPS), the Sensor \n Observation Service (SOS), or even new standard services emerging such OGC API or SensorThings.",
    "version": "0.5",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "author": "Emmanuel Blondel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5870-5762>),\n  Alexandre Bennici [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2160-3487>),\n  Norbert Billet [ctb]",
    "url": "https://github.com/eblondel/ows4R,\nhttps://eblondel.github.io/ows4R/,\nhttps://www.ogc.org/standards/",
    "bug_reports": "https://github.com/eblondel/ows4R/issues",
    "repository": "https://cran.r-project.org/package=ows4R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ows4R Interface to OGC Web-Services (OWS) Provides an Interface to Web-Services defined as standards by the Open Geospatial Consortium (OGC), including Web Feature Service\n (WFS) for vector data, Web Coverage Service (WCS), Catalogue Service (CSW) for ISO/OGC metadata, Web Processing Service (WPS) for data processes,\n and associated standards such as the common web-service specification (OWS) and OGC Filter Encoding. Partial support is provided for the Web Map \n Service (WMS). The purpose is to add support for additional OGC service standards such as Web Coverage Processing Service (WCPS), the Sensor \n Observation Service (SOS), or even new standard services emerging such OGC API or SensorThings.  "
  },
  {
    "id": 17478,
    "package_name": "pMEM",
    "title": "Predictive Moran's Eigenvector Maps",
    "description": "Calculation of Predictive Moran's eigenvector maps (pMEM), as\n  defined by Gu\u00e9nard and Legendre (In Press) \"Spatially-explicit predictions\n  using spatial eigenvector maps\" <doi:10.5281/zenodo.13356457>. Methods in\n  Ecology and Evolution. This method enables scientists to predict the values of\n  spatially-structured environmental variables. Multiple types of pMEM are\n  defined, each one implemented on the basis of spatial weighting function\n  taking a range parameter, and sometimes also a shape parameter. The code's\n  modular nature enables programers to implement new pMEM by defining new\n  spatial weighting functions.",
    "version": "0.1-1",
    "maintainer": "Guillaume Gu\u00e9nard <guillaume.guenard@umontreal.ca>",
    "author": "Guillaume Gu\u00e9nard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0761-3072>),\n  Pierre Legendre [ctb] (ORCID: <https://orcid.org/0000-0002-3838-3305>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pMEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pMEM Predictive Moran's Eigenvector Maps Calculation of Predictive Moran's eigenvector maps (pMEM), as\n  defined by Gu\u00e9nard and Legendre (In Press) \"Spatially-explicit predictions\n  using spatial eigenvector maps\" <doi:10.5281/zenodo.13356457>. Methods in\n  Ecology and Evolution. This method enables scientists to predict the values of\n  spatially-structured environmental variables. Multiple types of pMEM are\n  defined, each one implemented on the basis of spatial weighting function\n  taking a range parameter, and sometimes also a shape parameter. The code's\n  modular nature enables programers to implement new pMEM by defining new\n  spatial weighting functions.  "
  },
  {
    "id": 17489,
    "package_name": "pack",
    "title": "Convert Values to/from Raw Vectors",
    "description": "Functions to easily convert data to binary formats other programs/machines can understand.",
    "version": "0.1-2",
    "maintainer": "Joshua M. Ulrich <josh.m.ulrich@gmail.com>",
    "author": "Joshua M. Ulrich [cre, aut],\n  Ken Williams [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pack Convert Values to/from Raw Vectors Functions to easily convert data to binary formats other programs/machines can understand.  "
  },
  {
    "id": 17501,
    "package_name": "pacotest",
    "title": "Testing for Partial Copulas and the Simplifying Assumption in\nVine Copulas",
    "description": "Routines for two different test types, the Constant Conditional Correlation (CCC) test and the Vectorial Independence (VI) test are provided (Kurz and Spanhel (2022) <doi:10.1214/22-EJS2051>). The tests can be applied to check whether a conditional copula coincides with its partial copula. Functions to test whether a regular vine copula satisfies the so-called simplifying assumption or to test a single copula within a regular vine copula to be a (j-1)-th order partial copula are available. The CCC test comes with a decision tree approach to allow testing in high-dimensional settings.",
    "version": "0.4.3",
    "maintainer": "Malte S. Kurz <mkurz-software@gmx.de>",
    "author": "Malte S. Kurz [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/MalteKurz/pacotest/issues",
    "repository": "https://cran.r-project.org/package=pacotest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pacotest Testing for Partial Copulas and the Simplifying Assumption in\nVine Copulas Routines for two different test types, the Constant Conditional Correlation (CCC) test and the Vectorial Independence (VI) test are provided (Kurz and Spanhel (2022) <doi:10.1214/22-EJS2051>). The tests can be applied to check whether a conditional copula coincides with its partial copula. Functions to test whether a regular vine copula satisfies the so-called simplifying assumption or to test a single copula within a regular vine copula to be a (j-1)-th order partial copula are available. The CCC test comes with a decision tree approach to allow testing in high-dimensional settings.  "
  },
  {
    "id": 17521,
    "package_name": "palaeoSig",
    "title": "Significance Tests for Palaeoenvironmental Reconstructions",
    "description": "Several tests of quantitative palaeoenvironmental reconstructions \n  from microfossil assemblages, including the null model tests of the \n  statistically significant of reconstructions developed by Telford and Birks\n  (2011) <doi:10.1016/j.quascirev.2011.03.002>, and tests of the effect of \n  spatial autocorrelation on transfer function model performance using methods \n  from Telford and Birks (2009) <doi:10.1016/j.quascirev.2008.12.020> and \n  Trachsel and Telford (2016) <doi:10.5194/cp-12-1215-2016>. Age-depth models with \n  generalized mixed-effect regression from Heegaard et al (2005)\n  <doi:10.1191/0959683605hl836rr> are also included.",
    "version": "2.1-4",
    "maintainer": "Richard Telford <Richard.Telford@uib.no>",
    "author": "Richard Telford [aut, cre, cph],\n  Mathias Trachsel [ctb]",
    "url": "https://richardjtelford.github.io/palaeoSig/",
    "bug_reports": "https://github.com/richardjtelford/palaeoSig/issues",
    "repository": "https://cran.r-project.org/package=palaeoSig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "palaeoSig Significance Tests for Palaeoenvironmental Reconstructions Several tests of quantitative palaeoenvironmental reconstructions \n  from microfossil assemblages, including the null model tests of the \n  statistically significant of reconstructions developed by Telford and Birks\n  (2011) <doi:10.1016/j.quascirev.2011.03.002>, and tests of the effect of \n  spatial autocorrelation on transfer function model performance using methods \n  from Telford and Birks (2009) <doi:10.1016/j.quascirev.2008.12.020> and \n  Trachsel and Telford (2016) <doi:10.5194/cp-12-1215-2016>. Age-depth models with \n  generalized mixed-effect regression from Heegaard et al (2005)\n  <doi:10.1191/0959683605hl836rr> are also included.  "
  },
  {
    "id": 17529,
    "package_name": "paleomorph",
    "title": "Geometric Morphometric Tools for Paleobiology",
    "description": "Fill missing symmetrical data with mirroring, calculate Procrustes alignments with or without scaling, and compute standard or vector correlation and covariance matrices (congruence coefficients) of 3D landmarks. Tolerates missing data for all analyses.",
    "version": "0.1.4",
    "maintainer": "Tim Lucas <timcdlucas@gmail.com>",
    "author": "Tim Lucas, Anjali Goswami",
    "url": "https://github.com/timcdlucas/paleomorph/",
    "bug_reports": "https://github.com/timcdlucas/paleomorph/issues",
    "repository": "https://cran.r-project.org/package=paleomorph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paleomorph Geometric Morphometric Tools for Paleobiology Fill missing symmetrical data with mirroring, calculate Procrustes alignments with or without scaling, and compute standard or vector correlation and covariance matrices (congruence coefficients) of 3D landmarks. Tolerates missing data for all analyses.  "
  },
  {
    "id": 17534,
    "package_name": "paletteknife",
    "title": "Create Colour Scales and Legend from Continuous or Categorical\nVectors",
    "description": "Streamlines the steps for adding colour scales and associated legends \n         when working with base R graphics, especially for interactive use. Popular\n         palettes are included and pretty legends produced when mapping a large \n         variety of vector classes to a colour scale. An additional helper for \n         adding axes and grid lines complements the base::plot() work flow.",
    "version": "0.4.2",
    "maintainer": "John Hobbs <johnxhobbs@gmail.com>",
    "author": "John Hobbs [aut, cre]",
    "url": "https://github.com/johnxhobbs/paletteknife",
    "bug_reports": "https://github.com/johnxhobbs/paletteknife/issues",
    "repository": "https://cran.r-project.org/package=paletteknife",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paletteknife Create Colour Scales and Legend from Continuous or Categorical\nVectors Streamlines the steps for adding colour scales and associated legends \n         when working with base R graphics, especially for interactive use. Popular\n         palettes are included and pretty legends produced when mapping a large \n         variety of vector classes to a colour scale. An additional helper for \n         adding axes and grid lines complements the base::plot() work flow.  "
  },
  {
    "id": 17535,
    "package_name": "palettes",
    "title": "Methods for Colour Vectors and Colour Palettes",
    "description": "Provides a comprehensive library for colour vectors and colour\n    palettes using a new family of colour classes (palettes_colour and\n    palettes_palette) that always print as hex codes with colour previews.\n    Capabilities include: formatting, casting and coercion, extraction and\n    updating of components, plotting, colour mixing arithmetic, and colour\n    interpolation.",
    "version": "0.2.2",
    "maintainer": "Michael McCarthy <m.mccarthy1624@gmail.com>",
    "author": "Michael McCarthy [aut, cre, cph]",
    "url": "https://mccarthy-m-g.github.io/palettes/,\nhttps://github.com/mccarthy-m-g/palettes",
    "bug_reports": "https://github.com/mccarthy-m-g/palettes/issues",
    "repository": "https://cran.r-project.org/package=palettes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "palettes Methods for Colour Vectors and Colour Palettes Provides a comprehensive library for colour vectors and colour\n    palettes using a new family of colour classes (palettes_colour and\n    palettes_palette) that always print as hex codes with colour previews.\n    Capabilities include: formatting, casting and coercion, extraction and\n    updating of components, plotting, colour mixing arithmetic, and colour\n    interpolation.  "
  },
  {
    "id": 17542,
    "package_name": "palr",
    "title": "Colour Palettes for Data",
    "description": "Colour palettes for data, based on some well known public data\n    sets. Includes helper functions to map absolute values to known palettes, and \n    capture the work of image colour mapping as raster data sets. ",
    "version": "0.4.0",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre],\n  Abigael Proctor [ctb] (Named the package),\n  Tomas Remenyi [ctb] (Provided colours for element_pal),\n  R Core Team and contributors worldwide [ctb] (source code of\n    image.default)",
    "url": "https://github.com/AustralianAntarcticDivision/palr",
    "bug_reports": "https://github.com/AustralianAntarcticDivision/palr/issues",
    "repository": "https://cran.r-project.org/package=palr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "palr Colour Palettes for Data Colour palettes for data, based on some well known public data\n    sets. Includes helper functions to map absolute values to known palettes, and \n    capture the work of image colour mapping as raster data sets.   "
  },
  {
    "id": 17561,
    "package_name": "panelvar",
    "title": "Panel Vector Autoregression",
    "description": "We extend two general methods of moment estimators to panel vector \n    autoregression models (PVAR) with p lags of endogenous variables, predetermined \n    and strictly exogenous variables. This general PVAR model contains the first \n    difference GMM estimator by Holtz-Eakin et al. (1988) <doi:10.2307/1913103>, \n    Arellano and Bond (1991) <doi:10.2307/2297968> and the system GMM estimator \n    by Blundell and Bond (1998) <doi:10.1016/S0304-4076(98)00009-8>. We also \n    provide specification tests (Hansen overidentification test, lag selection \n    criterion and stability test of the PVAR polynomial) and classical structural \n    analysis for PVAR models such as orthogonal and generalized impulse response \n    functions, bootstrapped confidence intervals for impulse response analysis and \n    forecast error variance decompositions.",
    "version": "0.5.6",
    "maintainer": "Robert Ferstl <robert.ferstl@ur.de>",
    "author": "Michael Sigmund [aut],\n  Robert Ferstl [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=panelvar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "panelvar Panel Vector Autoregression We extend two general methods of moment estimators to panel vector \n    autoregression models (PVAR) with p lags of endogenous variables, predetermined \n    and strictly exogenous variables. This general PVAR model contains the first \n    difference GMM estimator by Holtz-Eakin et al. (1988) <doi:10.2307/1913103>, \n    Arellano and Bond (1991) <doi:10.2307/2297968> and the system GMM estimator \n    by Blundell and Bond (1998) <doi:10.1016/S0304-4076(98)00009-8>. We also \n    provide specification tests (Hansen overidentification test, lag selection \n    criterion and stability test of the PVAR polynomial) and classical structural \n    analysis for PVAR models such as orthogonal and generalized impulse response \n    functions, bootstrapped confidence intervals for impulse response analysis and \n    forecast error variance decompositions.  "
  },
  {
    "id": 17562,
    "package_name": "pannotator",
    "title": "Visualisation and Annotation of 360 Degree Imagery",
    "description": "Provides a customisable R 'shiny' app for immersively\n    visualising, mapping and annotating panospheric (360 degree) imagery.\n    The flexible interface allows annotation of any geocoded images using\n    up to 4 user specified dropdown menus. The app uses 'leaflet' to\n    render maps that display the geo-locations of images and panellum\n    <https://pannellum.org/>, a lightweight panorama viewer for the web,\n    to render images in virtual 360 degree viewing mode. Key functions\n    include the ability to draw on & export parts of 360 images for\n    downstream applications. Users can also draw polygons and points on\n    map imagery related to the panoramic images and export them for\n    further analysis. Downstream applications include using annotations to\n    train Artificial Intelligence/Machine Learning (AI/ML) models and\n    geospatial modelling and analysis of camera based survey data.",
    "version": "1.0.0.4",
    "maintainer": "Nunzio Knerr <Nunzio.Knerr@csiro.au>",
    "author": "Nunzio Knerr [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0562-9479>),\n  Robert Godfree [aut] (ORCID: <https://orcid.org/0000-0002-4263-2917>),\n  Matthew Petroff [ctb],\n  CSIRO [cph]",
    "url": "https://github.com/NunzioKnerr/pannotator_package_source",
    "bug_reports": "https://github.com/NunzioKnerr/pannotator_package_source/issues",
    "repository": "https://cran.r-project.org/package=pannotator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pannotator Visualisation and Annotation of 360 Degree Imagery Provides a customisable R 'shiny' app for immersively\n    visualising, mapping and annotating panospheric (360 degree) imagery.\n    The flexible interface allows annotation of any geocoded images using\n    up to 4 user specified dropdown menus. The app uses 'leaflet' to\n    render maps that display the geo-locations of images and panellum\n    <https://pannellum.org/>, a lightweight panorama viewer for the web,\n    to render images in virtual 360 degree viewing mode. Key functions\n    include the ability to draw on & export parts of 360 images for\n    downstream applications. Users can also draw polygons and points on\n    map imagery related to the panoramic images and export them for\n    further analysis. Downstream applications include using annotations to\n    train Artificial Intelligence/Machine Learning (AI/ML) models and\n    geospatial modelling and analysis of camera based survey data.  "
  },
  {
    "id": 17591,
    "package_name": "pargasite",
    "title": "Pollution-Associated Risk Geospatial Analysis Site",
    "description": "Offers tools to estimate and visualize levels of major pollutants\n             (CO, NO2, SO2, Ozone, PM2.5 and PM10) across the conterminous\n             United States for user-defined time ranges. Provides functions to\n             retrieve pollutant data from the U.S. Environmental Protection\n             Agency\u2019s 'Air Quality System' (AQS) API service\n             <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             interactive visualization through a 'shiny' application, allowing\n             users to explore pollutant levels for a given location over time\n             relative to the National Ambient Air Quality Standards (NAAQS).",
    "version": "2.1.1",
    "maintainer": "Jaehyun Joo <jaehyunjoo@outlook.com>",
    "author": "Jaehyun Joo [aut, cre],\n  Rebecca Greenblatt [aut],\n  Avantika Diwadkar [aut],\n  Nisha Narayanan [aut],\n  Blanca Himes [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pargasite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pargasite Pollution-Associated Risk Geospatial Analysis Site Offers tools to estimate and visualize levels of major pollutants\n             (CO, NO2, SO2, Ozone, PM2.5 and PM10) across the conterminous\n             United States for user-defined time ranges. Provides functions to\n             retrieve pollutant data from the U.S. Environmental Protection\n             Agency\u2019s 'Air Quality System' (AQS) API service\n             <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             interactive visualization through a 'shiny' application, allowing\n             users to explore pollutant levels for a given location over time\n             relative to the National Ambient Air Quality Standards (NAAQS).  "
  },
  {
    "id": 17608,
    "package_name": "particles",
    "title": "A Graph Based Particle Simulator Based on D3-Force",
    "description": "Simulating particle movement in 2D space has many\n    application. The 'particles' package implements a particle simulator\n    based on the ideas behind the 'd3-force' 'JavaScript' library.\n    'particles' implements all forces defined in 'd3-force' as well as\n    others such as vector fields, traps, and attractors.",
    "version": "0.2.4",
    "maintainer": "Thomas Lin Pedersen <thomasp85@gmail.com>",
    "author": "Thomas Lin Pedersen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5147-4711>),\n  Andrei Kashcha [ctb]",
    "url": "https://github.com/thomasp85/particles",
    "bug_reports": "https://github.com/thomasp85/particles/issues",
    "repository": "https://cran.r-project.org/package=particles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "particles A Graph Based Particle Simulator Based on D3-Force Simulating particle movement in 2D space has many\n    application. The 'particles' package implements a particle simulator\n    based on the ideas behind the 'd3-force' 'JavaScript' library.\n    'particles' implements all forces defined in 'd3-force' as well as\n    others such as vector fields, traps, and attractors.  "
  },
  {
    "id": 17611,
    "package_name": "partitionComparison",
    "title": "Implements Measures for the Comparison of Two Partitions",
    "description": "Provides several measures ((dis)similarity, distance/metric,\n    correlation, entropy) for comparing two partitions of the same set of\n    objects. The different measures can be assigned to three different\n    classes: Pair comparison (containing the famous Jaccard and Rand\n    indices), set based, and information theory based.\n    Many of the implemented measures can be found in\n    Albatineh AN, Niewiadomska-Bugaj M and Mihalko D (2006)\n    <doi:10.1007/s00357-006-0017-z> and\n    Meila M (2007) <doi:10.1016/j.jmva.2006.11.013>.\n    Partitions are represented by vectors of class labels which allow a\n    straightforward integration with existing clustering algorithms\n    (e.g. kmeans()). The package is mostly based on the S4 object system.",
    "version": "0.2.6",
    "maintainer": "Fabian Ball <mail@fabian-ball.de>",
    "author": "Fabian Ball [aut, cre, cph, ctb],\n  Andreas Geyer-Schulz [cph]",
    "url": "https://github.com/KIT-IISM-EM/partitionComparison",
    "bug_reports": "https://github.com/KIT-IISM-EM/partitionComparison/issues",
    "repository": "https://cran.r-project.org/package=partitionComparison",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "partitionComparison Implements Measures for the Comparison of Two Partitions Provides several measures ((dis)similarity, distance/metric,\n    correlation, entropy) for comparing two partitions of the same set of\n    objects. The different measures can be assigned to three different\n    classes: Pair comparison (containing the famous Jaccard and Rand\n    indices), set based, and information theory based.\n    Many of the implemented measures can be found in\n    Albatineh AN, Niewiadomska-Bugaj M and Mihalko D (2006)\n    <doi:10.1007/s00357-006-0017-z> and\n    Meila M (2007) <doi:10.1016/j.jmva.2006.11.013>.\n    Partitions are represented by vectors of class labels which allow a\n    straightforward integration with existing clustering algorithms\n    (e.g. kmeans()). The package is mostly based on the S4 object system.  "
  },
  {
    "id": 17647,
    "package_name": "pavo",
    "title": "Perceptual Analysis, Visualization and Organization of Spectral\nColour Data",
    "description": "A cohesive framework for the spectral and spatial analysis of \n    colour described in Maia, Eliason, Bitton, Doucet & Shawkey (2013) \n    <doi:10.1111/2041-210X.12069> and Maia, Gruson, Endler & White (2019)\n    <doi:10.1111/2041-210X.13174>.",
    "version": "2.9.0",
    "maintainer": "Thomas White <thomas.white026@gmail.com>",
    "author": "Thomas White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3976-1734>),\n  Rafael Maia [aut] (ORCID: <https://orcid.org/0000-0002-7563-9795>),\n  Hugo Gruson [aut] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\n  John Endler [aut],\n  Chad Eliason [aut],\n  Pierre-Paul Bitton [aut] (ORCID:\n    <https://orcid.org/0000-0001-5984-2331>)",
    "url": "http://pavo.colrverse.com, https://github.com/rmaia/pavo/",
    "bug_reports": "https://github.com/rmaia/pavo/issues",
    "repository": "https://cran.r-project.org/package=pavo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pavo Perceptual Analysis, Visualization and Organization of Spectral\nColour Data A cohesive framework for the spectral and spatial analysis of \n    colour described in Maia, Eliason, Bitton, Doucet & Shawkey (2013) \n    <doi:10.1111/2041-210X.12069> and Maia, Gruson, Endler & White (2019)\n    <doi:10.1111/2041-210X.13174>.  "
  },
  {
    "id": 17666,
    "package_name": "pbapply",
    "title": "Adding Progress Bar to '*apply' Functions",
    "description": "A lightweight package that adds\n  progress bar to vectorized R functions\n  ('*apply'). The implementation can easily be added\n  to functions where showing the progress is\n  useful (e.g. bootstrap). The type and style of the\n  progress bar (with percentages or remaining time)\n  can be set through options.\n  Supports several parallel processing backends including mirai and future.",
    "version": "1.7-4",
    "maintainer": "Peter Solymos <psolymos@gmail.com>",
    "author": "Peter Solymos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7337-1740>),\n  Zygmunt Zawadzki [aut],\n  Henrik Bengtsson [ctb],\n  R Core Team [cph, ctb]",
    "url": "https://github.com/psolymos/pbapply,\nhttps://peter.solymos.org/pbapply/",
    "bug_reports": "https://github.com/psolymos/pbapply/issues",
    "repository": "https://cran.r-project.org/package=pbapply",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pbapply Adding Progress Bar to '*apply' Functions A lightweight package that adds\n  progress bar to vectorized R functions\n  ('*apply'). The implementation can easily be added\n  to functions where showing the progress is\n  useful (e.g. bootstrap). The type and style of the\n  progress bar (with percentages or remaining time)\n  can be set through options.\n  Supports several parallel processing backends including mirai and future.  "
  },
  {
    "id": 17673,
    "package_name": "pbivnorm",
    "title": "Vectorized Bivariate Normal CDF",
    "description": "Provides a vectorized R function for calculating\n    probabilities from a standard bivariate normal CDF.",
    "version": "0.6.0",
    "maintainer": "Brenton Kenkel <brenton.kenkel@gmail.com>",
    "author": "Fortran code by Alan Genz.  R code by Brenton Kenkel,\n    based on Adelchi Azzalini's 'mnormt' package.",
    "url": "https://github.com/brentonk/pbivnorm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pbivnorm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pbivnorm Vectorized Bivariate Normal CDF Provides a vectorized R function for calculating\n    probabilities from a standard bivariate normal CDF.  "
  },
  {
    "id": 17677,
    "package_name": "pbmcapply",
    "title": "Tracking the Progress of Mc*pply with Progress Bar",
    "description": "A light-weight package helps you track and visualize\n  the progress of parallel version of vectorized R functions (mc*apply).\n  Parallelization (mc.core > 1) works only on *nix (Linux, Unix such as macOS) system due to\n  the lack of fork() functionality, which is essential for mc*apply, on Windows.",
    "version": "1.5.1",
    "maintainer": "Kevin kuang <kvn.kuang@mail.utoronto.ca>",
    "author": "Kevin Kuang (aut), Quyu Kong (ctb), Francesco Napolitano (ctb)",
    "url": "https://github.com/kvnkuang/pbmcapply",
    "bug_reports": "https://github.com/kvnkuang/pbmcapply/issues",
    "repository": "https://cran.r-project.org/package=pbmcapply",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pbmcapply Tracking the Progress of Mc*pply with Progress Bar A light-weight package helps you track and visualize\n  the progress of parallel version of vectorized R functions (mc*apply).\n  Parallelization (mc.core > 1) works only on *nix (Linux, Unix such as macOS) system due to\n  the lack of fork() functionality, which is essential for mc*apply, on Windows.  "
  },
  {
    "id": 17681,
    "package_name": "pbv",
    "title": "Probabilities for Bivariate Normal Distribution",
    "description": "\n    Computes probabilities of the bivariate normal distribution\n    in a vectorized R function (Drezner & Wesolowsky, 1990, \n    <doi:10.1080/00949659008811236>).",
    "version": "0.5-47",
    "maintainer": "Alexander Robitzsch <robitzsch@ipn.uni-kiel.de>",
    "author": "Alexander Robitzsch [aut,cre] (<https://orcid.org/0000-0002-8226-3132>)",
    "url": "https://github.com/alexanderrobitzsch/pbv,\nhttps://sites.google.com/view/alexander-robitzsch/software",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pbv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pbv Probabilities for Bivariate Normal Distribution \n    Computes probabilities of the bivariate normal distribution\n    in a vectorized R function (Drezner & Wesolowsky, 1990, \n    <doi:10.1080/00949659008811236>).  "
  },
  {
    "id": 17697,
    "package_name": "pcev",
    "title": "Principal Component of Explained Variance",
    "description": "Principal component of explained variance (PCEV) is a statistical\n    tool for the analysis of a multivariate response vector. It is a dimension-\n    reduction technique, similar to Principal component analysis (PCA), that seeks\n    to maximize the proportion of variance (in the response vector) being explained\n    by a set of covariates.",
    "version": "2.2.2",
    "maintainer": "Maxime Turgeon <maxime.turgeon@mail.mcgill.ca>",
    "author": "Maxime Turgeon [aut, cre],\n  Aurelie Labbe [aut],\n  Karim Oualkacha [aut],\n  Stepan Grinek [aut]",
    "url": "http://github.com/GreenwoodLab/pcev",
    "bug_reports": "http://github.com/GreenwoodLab/pcev/issues",
    "repository": "https://cran.r-project.org/package=pcev",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcev Principal Component of Explained Variance Principal component of explained variance (PCEV) is a statistical\n    tool for the analysis of a multivariate response vector. It is a dimension-\n    reduction technique, similar to Principal component analysis (PCA), that seeks\n    to maximize the proportion of variance (in the response vector) being explained\n    by a set of covariates.  "
  },
  {
    "id": 17718,
    "package_name": "pdcor",
    "title": "Fast and Light-Weight Partial Distance Correlation",
    "description": "Fast and memory-less computation of the partial distance correlation for vectors and matrices. Permutation-based and asymptotic hypothesis testing for zero partial distance correlation are also performed. References include: Szekely G. J. and Rizzo M. L. (2014). \"Partial distance correlation with methods for dissimilarities\". The Annals Statistics, 42(6): 2382--2412. <doi:10.1214/14-AOS1255>. Shen C., Panda S. and Vogelstein J. T. (2022). \"The Chi-Square Test of Distance Correlation\". Journal of Computational and Graphical Statistics, 31(1): 254--262. <doi:10.1080/10618600.2021.1938585>. Szekely G. J. and Rizzo M. L. (2023). \"The Energy of Data and Distance Correlation\". Chapman and Hall/CRC. <ISBN:9781482242744>. Kontemeniotis N., Vargiakakis R. and Tsagris M. (2025). On independence testing using the (partial) distance correlation. <doi:10.48550/arXiv.2506.15659>.  ",
    "version": "1.2",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Nikolaos Kontemeniotis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pdcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdcor Fast and Light-Weight Partial Distance Correlation Fast and memory-less computation of the partial distance correlation for vectors and matrices. Permutation-based and asymptotic hypothesis testing for zero partial distance correlation are also performed. References include: Szekely G. J. and Rizzo M. L. (2014). \"Partial distance correlation with methods for dissimilarities\". The Annals Statistics, 42(6): 2382--2412. <doi:10.1214/14-AOS1255>. Shen C., Panda S. and Vogelstein J. T. (2022). \"The Chi-Square Test of Distance Correlation\". Journal of Computational and Graphical Statistics, 31(1): 254--262. <doi:10.1080/10618600.2021.1938585>. Szekely G. J. and Rizzo M. L. (2023). \"The Energy of Data and Distance Correlation\". Chapman and Hall/CRC. <ISBN:9781482242744>. Kontemeniotis N., Vargiakakis R. and Tsagris M. (2025). On independence testing using the (partial) distance correlation. <doi:10.48550/arXiv.2506.15659>.    "
  },
  {
    "id": 17751,
    "package_name": "pedometrics",
    "title": "Miscellaneous Pedometric Tools",
    "description": "An R implementation of methods employed in the field of pedometrics, soil science\n    discipline dedicated to studying the spatial, temporal, and spatio-temporal variation of soil\n    using statistical and computational methods. The methods found here include the calibration of\n    linear regression models using covariate selection strategies, computation of summary validation\n    statistics for predictions, generation of summary plots, evaluation of the local quality of a\n    geostatistical model of uncertainty, and so on. Other functions simply extend the\n    functionalities of or facilitate the usage of functions from other packages that are commonly\n    used for the analysis of soil data. Formerly available versions of suggested packages no longer\n    available from CRAN can be obtained from the CRAN archive\n    <https://cran.r-project.org/src/contrib/Archive/>.",
    "version": "0.12.1",
    "maintainer": "Alessandro Samuel-Rosa <alessandrosamuelrosa@gmail.com>",
    "author": "Alessandro Samuel-Rosa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0877-1320>),\n  Lucia Helena Cunha dos Anjos [ths] (ORCID:\n    <https://orcid.org/0000-0003-0063-3521>),\n  Gustavo Vasques [ths] (ORCID: <https://orcid.org/0000-0001-9463-1898>),\n  Gerard B M Heuvelink [ths] (ORCID:\n    <https://orcid.org/0000-0003-0959-9358>),\n  Juan Carlos Ruiz Cuetos [ctb],\n  Maria Eugenia Polo Garcia [ctb],\n  Pablo Garcia Rodriguez [ctb],\n  Joshua French [ctb],\n  Ken Kleinman [ctb],\n  Dick Brus [ctb] (ORCID: <https://orcid.org/0000-0003-2194-4783>),\n  Frank Harrell Jr [ctb],\n  Ruo Xu [ctb]",
    "url": "https://github.com/Laboratorio-de-Pedometria/pedometrics-package",
    "bug_reports": "https://github.com/Laboratorio-de-Pedometria/pedometrics-package/issues",
    "repository": "https://cran.r-project.org/package=pedometrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pedometrics Miscellaneous Pedometric Tools An R implementation of methods employed in the field of pedometrics, soil science\n    discipline dedicated to studying the spatial, temporal, and spatio-temporal variation of soil\n    using statistical and computational methods. The methods found here include the calibration of\n    linear regression models using covariate selection strategies, computation of summary validation\n    statistics for predictions, generation of summary plots, evaluation of the local quality of a\n    geostatistical model of uncertainty, and so on. Other functions simply extend the\n    functionalities of or facilitate the usage of functions from other packages that are commonly\n    used for the analysis of soil data. Formerly available versions of suggested packages no longer\n    available from CRAN can be obtained from the CRAN archive\n    <https://cran.r-project.org/src/contrib/Archive/>.  "
  },
  {
    "id": 17767,
    "package_name": "penalizedSVM",
    "title": "Feature Selection SVM using Penalty Functions",
    "description": "Support Vector Machine (SVM) classification with simultaneous feature selection using penalty\n        functions is implemented. The smoothly clipped absolute deviation (SCAD),\n        'L1-norm', 'Elastic Net' ('L1-norm' and 'L2-norm') and 'Elastic\n        SCAD' (SCAD and 'L2-norm') penalties are available. The tuning\n        parameters can be found using either a fixed grid or a interval\n        search.",
    "version": "1.2.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Natalia Becker [aut],\n  Wiebke Werft [aut],\n  Axel Benner [aut],\n  Frederic Bertrand [cre]",
    "url": "https://github.com/fbertran/penalizedSVM,\nhttps://fbertran.github.io/penalizedSVM/",
    "bug_reports": "https://github.com/fbertran/penalizedSVM/issues",
    "repository": "https://cran.r-project.org/package=penalizedSVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "penalizedSVM Feature Selection SVM using Penalty Functions Support Vector Machine (SVM) classification with simultaneous feature selection using penalty\n        functions is implemented. The smoothly clipped absolute deviation (SCAD),\n        'L1-norm', 'Elastic Net' ('L1-norm' and 'L2-norm') and 'Elastic\n        SCAD' (SCAD and 'L2-norm') penalties are available. The tuning\n        parameters can be found using either a fixed grid or a interval\n        search.  "
  },
  {
    "id": 17793,
    "package_name": "permChacko",
    "title": "Chacko Test for Order-Restriction with Permutation",
    "description": "Implements an extension of the Chacko chi-square test for\n    ordered vectors (Chacko, 1966, <https://www.jstor.org/stable/25051572>).\n    Our extension brings the Chacko test to the computer age by implementing\n    a permutation test to offer a numeric estimate of the p-value, which is\n    particularly useful when the analytic solution is not available.",
    "version": "1.0.1",
    "maintainer": "Waldir Leoncio <w.l.netto@medisin.uio.no>",
    "author": "Waldir Leoncio [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6719-6162>),\n  Graeme Ruxton [aut],\n  Morten Wang Fagerland [aut]",
    "url": "https://ocbe-uio.github.io/permChacko/",
    "bug_reports": "https://github.com/ocbe-uio/permChacko/issues",
    "repository": "https://cran.r-project.org/package=permChacko",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "permChacko Chacko Test for Order-Restriction with Permutation Implements an extension of the Chacko chi-square test for\n    ordered vectors (Chacko, 1966, <https://www.jstor.org/stable/25051572>).\n    Our extension brings the Chacko test to the computer age by implementing\n    a permutation test to offer a numeric estimate of the p-value, which is\n    particularly useful when the analytic solution is not available.  "
  },
  {
    "id": 17836,
    "package_name": "pgirmess",
    "title": "Spatial Analysis and Data Mining for Field Ecologists",
    "description": "Set of tools for reading, writing and transforming spatial and seasonal data, model selection and specific statistical tests for ecologists. It includes functions to interpolate regular positions of points between landmarks, to discretize polylines into regular point positions, link distant observations to points and convert a bounding box in a spatial object. It also provides miscellaneous functions for field ecologists such as spatial statistics and inference on diversity indexes, writing data.frame with Chinese characters.",
    "version": "2.0.3",
    "maintainer": "Patrick Giraudoux <patrick.giraudoux@univ-fcomte.fr>",
    "author": "Patrick Giraudoux [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2376-0136>),\n  Jean-Philippe Antonietti [ctb],\n  Colin Beale [ctb],\n  Ulrike Groemping [ctb],\n  Renaud Lancelot [ctb],\n  David Pleydell [ctb],\n  Mike Treglia [ctb]",
    "url": "https://github.com/pgiraudoux/pgirmess",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pgirmess",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pgirmess Spatial Analysis and Data Mining for Field Ecologists Set of tools for reading, writing and transforming spatial and seasonal data, model selection and specific statistical tests for ecologists. It includes functions to interpolate regular positions of points between landmarks, to discretize polylines into regular point positions, link distant observations to points and convert a bounding box in a spatial object. It also provides miscellaneous functions for field ecologists such as spatial statistics and inference on diversity indexes, writing data.frame with Chinese characters.  "
  },
  {
    "id": 17874,
    "package_name": "phenomap",
    "title": "Projecting Satellite-Derived Phenology in Space",
    "description": "This takes in a series of multi-layer raster files and returns a phenology projection raster, following methodologies described in John (2016) <https://etda.libraries.psu.edu/catalog/13521clj5135>.",
    "version": "2.0.1",
    "maintainer": "Christian John <cjohn@ucsb.edu>",
    "author": "Christian John [aut, cre]",
    "url": "https://github.com/JepsonNomad/phenomap",
    "bug_reports": "https://github.com/JepsonNomad/phenomap/issues",
    "repository": "https://cran.r-project.org/package=phenomap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phenomap Projecting Satellite-Derived Phenology in Space This takes in a series of multi-layer raster files and returns a phenology projection raster, following methodologies described in John (2016) <https://etda.libraries.psu.edu/catalog/13521clj5135>.  "
  },
  {
    "id": 17885,
    "package_name": "phonenumber",
    "title": "Convert Letters to Numbers and Back as on a Telephone Keypad",
    "description": "Convert English letters to numbers or numbers to English\n    letters as on a telephone keypad. When converting letters to numbers,\n    a character vector is returned with \"A,\" \"B,\" or \"C\" becoming 2, \"D,\"\n    \"E\", or \"F\" becoming 3, etc. When converting numbers to letters, a\n    character vector is returned with multiple elements (i.e., \"2\" becomes\n    a vector of \"A,\" \"B,\" and \"C\").",
    "version": "0.2.3",
    "maintainer": "Steve Myles <steve@mylesandmyles.info>",
    "author": "Steve Myles [aut, cre]",
    "url": "https://stevemyles.site/phonenumber/,\nhttps://github.com/scumdogsteev/phonenumber",
    "bug_reports": "https://github.com/scumdogsteev/phonenumber/issues",
    "repository": "https://cran.r-project.org/package=phonenumber",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phonenumber Convert Letters to Numbers and Back as on a Telephone Keypad Convert English letters to numbers or numbers to English\n    letters as on a telephone keypad. When converting letters to numbers,\n    a character vector is returned with \"A,\" \"B,\" or \"C\" becoming 2, \"D,\"\n    \"E\", or \"F\" becoming 3, etc. When converting numbers to letters, a\n    character vector is returned with multiple elements (i.e., \"2\" becomes\n    a vector of \"A,\" \"B,\" and \"C\").  "
  },
  {
    "id": 17909,
    "package_name": "phylin",
    "title": "Spatial Interpolation of Genetic Data",
    "description": "The spatial interpolation of genetic distances between\n\t     samples is based on a modified kriging method that\n\t     accepts a genetic distance matrix and generates a map of\n\t     probability of lineage presence. This package also offers\n\t     tools to generate a map of  potential contact zones\n\t     between groups with user-defined thresholds in the tree\n\t     to account for old and recent divergence. Additionally,\n\t     it has functions for IDW interpolation using genetic data\n\t     and midpoints.",
    "version": "2.0.2",
    "maintainer": "Pedro Tarroso <ptarroso@cibio.up.pt>",
    "author": "Pedro Tarroso, Guillermo Velo-Anton, Silvia Carvalho",
    "url": "https://www.r-project.org, https://github.com/ptarroso/phylin",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phylin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylin Spatial Interpolation of Genetic Data The spatial interpolation of genetic distances between\n\t     samples is based on a modified kriging method that\n\t     accepts a genetic distance matrix and generates a map of\n\t     probability of lineage presence. This package also offers\n\t     tools to generate a map of  potential contact zones\n\t     between groups with user-defined thresholds in the tree\n\t     to account for old and recent divergence. Additionally,\n\t     it has functions for IDW interpolation using genetic data\n\t     and midpoints.  "
  },
  {
    "id": 17918,
    "package_name": "phyloraster",
    "title": "Evolutionary Diversity Metrics for Raster Data",
    "description": "Phylogenetic Diversity (PD, Faith 1992), Evolutionary\n    Distinctiveness (ED, Isaac et al. 2007), Phylogenetic Endemism (PE,\n    Rosauer et al. 2009; Laffan et al. 2016), and Weighted Endemism (WE,\n    Laffan et al. 2016) for presence-absence raster.  Faith, D. P. (1992)\n    <doi:10.1016/0006-3207(92)91201-3> Isaac, N. J. et al. (2007)\n    <doi:10.1371/journal.pone.0000296> Laffan, S. W. et al. (2016)\n    <doi:10.1111/2041-210X.12513> Rosauer, D. et al. (2009)\n    <doi:10.1111/j.1365-294X.2009.04311.x>.",
    "version": "2.2.0",
    "maintainer": "Gabriela Alves-Ferreira <gabriela-alves77@hotmail.com>",
    "author": "Gabriela Alves-Ferreira [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5661-3381>),\n  Fl\u00e1vio M. M. Mota [aut] (ORCID:\n    <https://orcid.org/0000-0002-0308-7151>),\n  Neander Marcel Heming [aut] (ORCID:\n    <https://orcid.org/0000-0003-2461-5045>)",
    "url": "https://CRAN.R-project.org/package=phyloraster,\nhttps://github.com/gabferreira/phyloraster,\nhttps:https://gabferreira.github.io/phyloraster/,\nhttps://doi.org/10.1111/ecog.06902",
    "bug_reports": "https://github.com/gabferreira/phyloraster/issues",
    "repository": "https://cran.r-project.org/package=phyloraster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phyloraster Evolutionary Diversity Metrics for Raster Data Phylogenetic Diversity (PD, Faith 1992), Evolutionary\n    Distinctiveness (ED, Isaac et al. 2007), Phylogenetic Endemism (PE,\n    Rosauer et al. 2009; Laffan et al. 2016), and Weighted Endemism (WE,\n    Laffan et al. 2016) for presence-absence raster.  Faith, D. P. (1992)\n    <doi:10.1016/0006-3207(92)91201-3> Isaac, N. J. et al. (2007)\n    <doi:10.1371/journal.pone.0000296> Laffan, S. W. et al. (2016)\n    <doi:10.1111/2041-210X.12513> Rosauer, D. et al. (2009)\n    <doi:10.1111/j.1365-294X.2009.04311.x>.  "
  },
  {
    "id": 17965,
    "package_name": "piqp",
    "title": "R Interface to Proximal Interior Point Quadratic Programming\nSolver",
    "description": "An embedded proximal interior point quadratic programming solver, which can solve dense and sparse quadratic programs, described in Schwan, Jiang, Kuhn, and Jones (2023) <doi:10.48550/arXiv.2304.00290>. Combining an infeasible interior point method with the proximal method of multipliers, the algorithm can handle ill-conditioned convex quadratic programming problems without the need for linear independence of the constraints. The solver is written in header only 'C++ 14' leveraging the 'Eigen' library for vectorized linear algebra. For small dense problems, vectorized instructions and cache locality can be exploited more efficiently. Allocation free problem updates and re-solves are also provided.",
    "version": "0.2.2",
    "maintainer": "Balasubramanian Narasimhan <naras@stanford.edu>",
    "author": "Balasubramanian Narasimhan [aut, cre],\n  Roland Schwan [aut, cph],\n  Yuning Jiang [aut],\n  Daniel Kuhn [aut],\n  Colin N. Jones [aut]",
    "url": "https://predict-epfl.github.io/piqp-r/",
    "bug_reports": "https://github.com/PREDICT-EPFL/piqp-r/issues",
    "repository": "https://cran.r-project.org/package=piqp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "piqp R Interface to Proximal Interior Point Quadratic Programming\nSolver An embedded proximal interior point quadratic programming solver, which can solve dense and sparse quadratic programs, described in Schwan, Jiang, Kuhn, and Jones (2023) <doi:10.48550/arXiv.2304.00290>. Combining an infeasible interior point method with the proximal method of multipliers, the algorithm can handle ill-conditioned convex quadratic programming problems without the need for linear independence of the constraints. The solver is written in header only 'C++ 14' leveraging the 'Eigen' library for vectorized linear algebra. For small dense problems, vectorized instructions and cache locality can be exploited more efficiently. Allocation free problem updates and re-solves are also provided.  "
  },
  {
    "id": 17993,
    "package_name": "pkgverse",
    "title": "Build a Meta-Package Universe",
    "description": "Build your own universe of packages similar to the 'tidyverse'\n    package <https://tidyverse.org/> with this meta-package creator. Create a\n    package-verse, or meta package, by supplying a custom name for the\n    collection of packages and the vector of desired package names to include\u2013\n    and optionally supply a destination directory, an indicator of whether to\n    keep the created package directory, and/or a vector of verbs implement via\n    the 'usethis' <http://usethis.r-lib.org/> package.",
    "version": "0.0.1",
    "maintainer": "Michael Wayne Kearney <kearneymw@missouri.edu>",
    "author": "Michael Wayne Kearney [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0730-4694>)",
    "url": "https://pkgverse.mikewk.com",
    "bug_reports": "https://github.com/mkearney/pkgverse/issues",
    "repository": "https://cran.r-project.org/package=pkgverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pkgverse Build a Meta-Package Universe Build your own universe of packages similar to the 'tidyverse'\n    package <https://tidyverse.org/> with this meta-package creator. Create a\n    package-verse, or meta package, by supplying a custom name for the\n    collection of packages and the vector of desired package names to include\u2013\n    and optionally supply a destination directory, an indicator of whether to\n    keep the created package directory, and/or a vector of verbs implement via\n    the 'usethis' <http://usethis.r-lib.org/> package.  "
  },
  {
    "id": 18023,
    "package_name": "plink",
    "title": "IRT Separate Calibration Linking Methods",
    "description": "Item response theory based methods are used to compute\n        linking constants and conduct chain linking of unidimensional\n        or multidimensional tests for multiple groups under a common\n        item design.  The unidimensional methods include the Mean/Mean,\n        Mean/Sigma, Haebara, and Stocking-Lord methods for dichotomous\n        (1PL, 2PL and 3PL) and/or polytomous (graded response, partial\n        credit/generalized partial credit, nominal, and multiple-choice\n        model) items.  The multidimensional methods include the least\n        squares method and extensions of the Haebara and Stocking-Lord\n        method using single or multiple dilation parameters for\n        multidimensional extensions of all the unidimensional\n        dichotomous and polytomous item response models.  The package\n        also includes functions for importing item and/or ability\n        parameters from common IRT software, conducting IRT true score\n        and observed score equating, and plotting item response\n        curves/surfaces, vector plots, information plots, and comparison \n        plots for examining parameter drift.",
    "version": "1.5-1",
    "maintainer": "Jonathan P. Weeks <weeksjp@gmail.com>",
    "author": "Jonathan P. Weeks <weeksjp@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=plink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plink IRT Separate Calibration Linking Methods Item response theory based methods are used to compute\n        linking constants and conduct chain linking of unidimensional\n        or multidimensional tests for multiple groups under a common\n        item design.  The unidimensional methods include the Mean/Mean,\n        Mean/Sigma, Haebara, and Stocking-Lord methods for dichotomous\n        (1PL, 2PL and 3PL) and/or polytomous (graded response, partial\n        credit/generalized partial credit, nominal, and multiple-choice\n        model) items.  The multidimensional methods include the least\n        squares method and extensions of the Haebara and Stocking-Lord\n        method using single or multiple dilation parameters for\n        multidimensional extensions of all the unidimensional\n        dichotomous and polytomous item response models.  The package\n        also includes functions for importing item and/or ability\n        parameters from common IRT software, conducting IRT true score\n        and observed score equating, and plotting item response\n        curves/surfaces, vector plots, information plots, and comparison \n        plots for examining parameter drift.  "
  },
  {
    "id": 18072,
    "package_name": "plu",
    "title": "Dynamically Pluralize Phrases",
    "description": "Converts English phrases to singular or plural form based on\n    the length of an associated vector.  Contains helper functions to\n    create natural language lists from vectors and to include the length\n    of a vector in natural language.",
    "version": "0.3.0",
    "maintainer": "Alexander Rossell Hayes <alexander@rossellhayes.com>",
    "author": "Alexander Rossell Hayes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9412-0457>)",
    "url": "https://pkg.rossellhayes.com/plu/,\nhttps://github.com/rossellhayes/plu",
    "bug_reports": "https://github.com/rossellhayes/plu/issues",
    "repository": "https://cran.r-project.org/package=plu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plu Dynamically Pluralize Phrases Converts English phrases to singular or plural form based on\n    the length of an associated vector.  Contains helper functions to\n    create natural language lists from vectors and to include the length\n    of a vector in natural language.  "
  },
  {
    "id": 18076,
    "package_name": "plumbr",
    "title": "Mutable and Dynamic Data Models",
    "description": "The base R data.frame, like any vector, is\n    copied upon modification. This behavior is at odds with\n    that of GUIs and interactive graphics. To rectify this,\n    plumbr provides a mutable, dynamic tabular data model.\n    Models may be chained together to form the complex\n    plumbing necessary for sophisticated graphical\n    interfaces. Also included is a\n    general framework for linking datasets; an typical\n    use case would be a linked brush.",
    "version": "0.6.10",
    "maintainer": "Michael Lawrence <michafla@gene.com>",
    "author": "Michael Lawrence, Hadley Wickham",
    "url": "https://github.com/ggobi/plumbr/wiki",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=plumbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plumbr Mutable and Dynamic Data Models The base R data.frame, like any vector, is\n    copied upon modification. This behavior is at odds with\n    that of GUIs and interactive graphics. To rectify this,\n    plumbr provides a mutable, dynamic tabular data model.\n    Models may be chained together to form the complex\n    plumbing necessary for sophisticated graphical\n    interfaces. Also included is a\n    general framework for linking datasets; an typical\n    use case would be a linked brush.  "
  },
  {
    "id": 18108,
    "package_name": "png",
    "title": "Read and write PNG images",
    "description": "This package provides an easy and simple way to read, write and display bitmap images stored in the PNG format. It can read and write both files and in-memory raw vectors.",
    "version": "0.1-8",
    "maintainer": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "author": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "url": "http://www.rforge.net/png/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=png",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "png Read and write PNG images This package provides an easy and simple way to read, write and display bitmap images stored in the PNG format. It can read and write both files and in-memory raw vectors.  "
  },
  {
    "id": 18192,
    "package_name": "popsom7",
    "title": "A Fast, User-Friendly Implementation of Self-Organizing Maps\n(SOMs)",
    "description": "Methods for building self-organizing maps (SOMs) with a number of distinguishing features such automatic centroid detection and cluster visualization using starbursts.  For more details see the paper \"Improved Interpretability of the Unified Distance Matrix with Connected Components\" by Hamel and Brown (2011) in <ISBN:1-60132-168-6>.  The package provides user-friendly access to two models we construct: (a) a SOM model and (b) a centroid based clustering model. The package also exposes a number of quality metrics for the quantitative evaluation of the map, Hamel (2016) <doi:10.1007/978-3-319-28518-4_4>.  Finally, we reintroduced our fast, vectorized training algorithm for SOM with substantial improvements. It is about an order of magnitude faster than the canonical, stochastic C implementation <doi:10.1007/978-3-030-01057-7_60>.",
    "version": "7.1.0",
    "maintainer": "Lutz Hamel <lutzhamel@uri.edu>",
    "author": "Lutz Hamel [aut, cre],\n  Benjamin Ott [aut],\n  Gregory Breard [aut],\n  Robert Tatoian [aut],\n  Michael Eiger [aut],\n  Vishakh Gopu [aut]",
    "url": "https://github.com/lutzhamel/popsom7",
    "bug_reports": "https://github.com/lutzhamel/popsom7/issues",
    "repository": "https://cran.r-project.org/package=popsom7",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "popsom7 A Fast, User-Friendly Implementation of Self-Organizing Maps\n(SOMs) Methods for building self-organizing maps (SOMs) with a number of distinguishing features such automatic centroid detection and cluster visualization using starbursts.  For more details see the paper \"Improved Interpretability of the Unified Distance Matrix with Connected Components\" by Hamel and Brown (2011) in <ISBN:1-60132-168-6>.  The package provides user-friendly access to two models we construct: (a) a SOM model and (b) a centroid based clustering model. The package also exposes a number of quality metrics for the quantitative evaluation of the map, Hamel (2016) <doi:10.1007/978-3-319-28518-4_4>.  Finally, we reintroduced our fast, vectorized training algorithm for SOM with substantial improvements. It is about an order of magnitude faster than the canonical, stochastic C implementation <doi:10.1007/978-3-030-01057-7_60>.  "
  },
  {
    "id": 18255,
    "package_name": "ppcSpatial",
    "title": "Spatial Analysis of Pakistan Population Census",
    "description": "Spatial Analysis for exploration of Pakistan Population Census 2017 (<https://www.pbs.gov.pk/content/population-census>). It uses data from R package 'PakPC2017'.",
    "version": "0.3.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre],\n  Muhammad Arfan Dilber [aut, ctb]",
    "url": "https://github.com/MYaseen208/ppcSpatial",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ppcSpatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppcSpatial Spatial Analysis of Pakistan Population Census Spatial Analysis for exploration of Pakistan Population Census 2017 (<https://www.pbs.gov.pk/content/population-census>). It uses data from R package 'PakPC2017'.  "
  },
  {
    "id": 18266,
    "package_name": "pplot",
    "title": "Chronological and Ordered p-Plots for Empirical Data",
    "description": "Generates chronological and ordered p-plots for data vectors or vectors of p-values. The p-plot visualizes the evolution of the p-value of a significance test across the sampled data. It allows for assessing the consistency of the observed effects, for detecting the presence of potential moderator variables, and for estimating the influence of outlier values on the observed results. For non-significant findings, it can diagnose patterns indicative of underpowered study designs. The p-plot can thus either back the binary accept-vs-reject decision of common null-hypothesis significance tests, or it can qualify this decision and stimulate additional empirical work to arrive at more robust and replicable statistical inferences.",
    "version": "0.9",
    "maintainer": "Roland Pfister <mail@roland-pfister.net>",
    "author": "Roland Pfister [aut, cre],\n  Christian Frings [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pplot Chronological and Ordered p-Plots for Empirical Data Generates chronological and ordered p-plots for data vectors or vectors of p-values. The p-plot visualizes the evolution of the p-value of a significance test across the sampled data. It allows for assessing the consistency of the observed effects, for detecting the presence of potential moderator variables, and for estimating the influence of outlier values on the observed results. For non-significant findings, it can diagnose patterns indicative of underpowered study designs. The p-plot can thus either back the binary accept-vs-reject decision of common null-hypothesis significance tests, or it can qualify this decision and stimulate additional empirical work to arrive at more robust and replicable statistical inferences.  "
  },
  {
    "id": 18310,
    "package_name": "predictoR",
    "title": "Predictive Data Analysis System",
    "description": "Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning, Support Vector Machines and Bayesian Methods.",
    "version": "4.1.5",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "author": "Oldemar Rodriguez [aut, cre],\n  Diego Jim\u00e9nez [ctb, prg],\n  Andr\u00e9s Navarro [ctb, prg]",
    "url": "https://promidat.website/",
    "bug_reports": "https://github.com/PROMiDAT/predictoR/issues",
    "repository": "https://cran.r-project.org/package=predictoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "predictoR Predictive Data Analysis System Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning, Support Vector Machines and Bayesian Methods.  "
  },
  {
    "id": 18362,
    "package_name": "primes",
    "title": "Fast Functions for Prime Numbers",
    "description": "Fast functions for dealing with prime numbers, such as testing\n    whether a number is prime and generating a sequence prime numbers.\n    Additional functions include finding prime factors and Ruth-Aaron pairs,\n    finding next and previous prime numbers in the series, finding or estimating\n    the nth prime, estimating the number of primes less than or equal to an\n    arbitrary number, computing primorials, prime k-tuples (e.g., twin primes),\n    finding the greatest common divisor and smallest (least) common multiple,\n    testing whether two numbers are coprime, and computing Euler's totient\n    function. Most functions are vectorized for speed and convenience.",
    "version": "1.6.1",
    "maintainer": "Paul Egeler <paulegeler@gmail.com>",
    "author": "Os Keyes [aut],\n  Paul Egeler [aut, cre] (ORCID: <https://orcid.org/0000-0001-6948-9498>)",
    "url": "https://github.com/ironholds/primes",
    "bug_reports": "https://github.com/ironholds/primes/issues",
    "repository": "https://cran.r-project.org/package=primes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "primes Fast Functions for Prime Numbers Fast functions for dealing with prime numbers, such as testing\n    whether a number is prime and generating a sequence prime numbers.\n    Additional functions include finding prime factors and Ruth-Aaron pairs,\n    finding next and previous prime numbers in the series, finding or estimating\n    the nth prime, estimating the number of primes less than or equal to an\n    arbitrary number, computing primorials, prime k-tuples (e.g., twin primes),\n    finding the greatest common divisor and smallest (least) common multiple,\n    testing whether two numbers are coprime, and computing Euler's totient\n    function. Most functions are vectorized for speed and convenience.  "
  },
  {
    "id": 18389,
    "package_name": "proccalibrad",
    "title": "Extraction of Bands from MODIS Calibrated Radiances MOD02 NRT",
    "description": "Package for processing downloaded MODIS Calibrated radiances\n    Product HDF files. Specifically, MOD02 calibrated radiance product files, and\n    the associated MOD03 geolocation files (for MODIS-TERRA). The package will be\n    most effective if the user installs MRTSwath (MODIS Reprojection Tool for swath\n    products; <https://lpdaac.usgs.gov/tools/modis_reprojection_tool_swath>, and\n    adds the directory with the MRTSwath executable to the default R PATH by editing\n    ~/.Rprofile.",
    "version": "0.14",
    "maintainer": "Rishabh Gupta <rishabh.uk@gmail.com>",
    "author": "Rishabh Gupta <rishabh.uk@gmail.com>, Nicholas J. Matzke",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=proccalibrad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "proccalibrad Extraction of Bands from MODIS Calibrated Radiances MOD02 NRT Package for processing downloaded MODIS Calibrated radiances\n    Product HDF files. Specifically, MOD02 calibrated radiance product files, and\n    the associated MOD03 geolocation files (for MODIS-TERRA). The package will be\n    most effective if the user installs MRTSwath (MODIS Reprojection Tool for swath\n    products; <https://lpdaac.usgs.gov/tools/modis_reprojection_tool_swath>, and\n    adds the directory with the MRTSwath executable to the default R PATH by editing\n    ~/.Rprofile.  "
  },
  {
    "id": 18403,
    "package_name": "profExtrema",
    "title": "Compute and Visualize Profile Extrema Functions",
    "description": "Computes profile extrema functions for arbitrary functions. If the function is expensive-to-evaluate it computes profile extrema by emulating the function with a Gaussian process (using package 'DiceKriging'). In this case uncertainty quantification on the profile extrema can also be computed. The different plotting functions for profile extrema give the user a tool to better locate excursion sets. ",
    "version": "0.2.1",
    "maintainer": "Dario Azzimonti <dario.azzimonti@gmail.com>",
    "author": "Dario Azzimonti [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5080-3061>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=profExtrema",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "profExtrema Compute and Visualize Profile Extrema Functions Computes profile extrema functions for arbitrary functions. If the function is expensive-to-evaluate it computes profile extrema by emulating the function with a Gaussian process (using package 'DiceKriging'). In this case uncertainty quantification on the profile extrema can also be computed. The different plotting functions for profile extrema give the user a tool to better locate excursion sets.   "
  },
  {
    "id": 18448,
    "package_name": "protolite",
    "title": "Highly Optimized Protocol Buffer Serializers",
    "description": "Pure C++ implementations for reading and writing several common data \n    formats based on Google protocol-buffers. Currently supports 'rexp.proto' for \n    serialized R objects, 'geobuf.proto' for binary geojson, and 'mvt.proto' for \n    vector tiles. This package uses the auto-generated C++ code by protobuf-compiler, \n    hence the entire serialization is optimized at compile time. The 'RProtoBuf' \n    package on the other hand uses the protobuf runtime library to provide a general-\n    purpose toolkit for reading and writing arbitrary protocol-buffer data in R.",
    "version": "2.3.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://github.com/jeroen/protolite\nhttps://jeroen.r-universe.dev/protolite",
    "bug_reports": "https://github.com/jeroen/protolite/issues",
    "repository": "https://cran.r-project.org/package=protolite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "protolite Highly Optimized Protocol Buffer Serializers Pure C++ implementations for reading and writing several common data \n    formats based on Google protocol-buffers. Currently supports 'rexp.proto' for \n    serialized R objects, 'geobuf.proto' for binary geojson, and 'mvt.proto' for \n    vector tiles. This package uses the auto-generated C++ code by protobuf-compiler, \n    hence the entire serialization is optimized at compile time. The 'RProtoBuf' \n    package on the other hand uses the protobuf runtime library to provide a general-\n    purpose toolkit for reading and writing arbitrary protocol-buffer data in R.  "
  },
  {
    "id": 18490,
    "package_name": "psgp",
    "title": "Projected Spatial Gaussian Process Methods",
    "description": "Implements projected sparse Gaussian process Kriging ('Ingram et. al.', 2008, <doi:10.1007/s00477-007-0163-9>) as an additional method for the 'intamap' package. More details on implementation ('Barillec et. al.', 2010, <doi:10.1016/j.cageo.2010.05.008>).",
    "version": "0.3-25",
    "maintainer": "Ben Ingram <ingrambr.work@gmail.com>",
    "author": "Ben Ingram [aut, cre] (ORCID: <https://orcid.org/0000-0003-4557-4342>),\n  Remi Barillec [aut],\n  Jon Olav Skoien [aut] (ORCID: <https://orcid.org/0000-0002-8706-1986>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psgp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psgp Projected Spatial Gaussian Process Methods Implements projected sparse Gaussian process Kriging ('Ingram et. al.', 2008, <doi:10.1007/s00477-007-0163-9>) as an additional method for the 'intamap' package. More details on implementation ('Barillec et. al.', 2010, <doi:10.1016/j.cageo.2010.05.008>).  "
  },
  {
    "id": 18508,
    "package_name": "psvmSDR",
    "title": "Unified Principal Sufficient Dimension Reduction Package",
    "description": "A unified and user-friendly framework for applying the principal sufficient dimension reduction methods for both linear and nonlinear cases. The package has an extendable power by varying loss functions for the support vector machine, even for an user-defined arbitrary function, unless those are convex and differentiable everywhere over the support (Li et al. (2011) <doi:10.1214/11-AOS932>). Also, it provides a real-time sufficient dimension reduction update procedure using the principal least squares support vector machine (Artemiou et al. (2021) <doi:10.1016/j.patcog.2020.107768>).",
    "version": "2.0.1",
    "maintainer": "Jungmin Shin <c16267@gmail.com>",
    "author": "Jungmin Shin [aut, cre],\n  Seung Jun Shin [aut],\n  Andreas Artemiou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psvmSDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psvmSDR Unified Principal Sufficient Dimension Reduction Package A unified and user-friendly framework for applying the principal sufficient dimension reduction methods for both linear and nonlinear cases. The package has an extendable power by varying loss functions for the support vector machine, even for an user-defined arbitrary function, unless those are convex and differentiable everywhere over the support (Li et al. (2011) <doi:10.1214/11-AOS932>). Also, it provides a real-time sufficient dimension reduction update procedure using the principal least squares support vector machine (Artemiou et al. (2021) <doi:10.1016/j.patcog.2020.107768>).  "
  },
  {
    "id": 18534,
    "package_name": "ptvalue",
    "title": "Working with Precision Teaching Values",
    "description": "An implementation of an S3 class based on a double vector for storing and displaying precision teaching measures, representing a growing or a decaying (multiplicative) change between two frequencies. The main format method allows researchers to display measures (including data.frame) that respect the established conventions in the precision teaching community (i.e., prefixed multiplication or division symbol, displayed number <= 1). Basic multiplication and division methods are allowed and other useful functions are provided for creating, converting or inverting precision teaching measures. For more details, see Pennypacker, Gutierrez and Lindsley (2003, ISBN: 1-881317-13-7).",
    "version": "0.2.0",
    "maintainer": "Alexandre Gellen-Kamel <kamel.ag@outlook.com>",
    "author": "Alexandre Gellen-Kamel [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0382-2478>)",
    "url": "https://github.com/agkamel/ptvalue",
    "bug_reports": "https://github.com/agkamel/ptvalue/issues",
    "repository": "https://cran.r-project.org/package=ptvalue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ptvalue Working with Precision Teaching Values An implementation of an S3 class based on a double vector for storing and displaying precision teaching measures, representing a growing or a decaying (multiplicative) change between two frequencies. The main format method allows researchers to display measures (including data.frame) that respect the established conventions in the precision teaching community (i.e., prefixed multiplication or division symbol, displayed number <= 1). Basic multiplication and division methods are allowed and other useful functions are provided for creating, converting or inverting precision teaching measures. For more details, see Pennypacker, Gutierrez and Lindsley (2003, ISBN: 1-881317-13-7).  "
  },
  {
    "id": 18567,
    "package_name": "pvars",
    "title": "VAR Modeling for Heterogeneous Panels",
    "description": "Implements (1) panel cointegration rank tests, (2) estimators for panel\n    vector autoregressive (VAR) models, and (3) identification methods for panel\n    structural vector autoregressive (SVAR) models as described in the accompanying vignette.\n    The implemented functions allow to account for cross-sectional dependence\n    and for structural breaks in the deterministic terms of the VAR processes.\n    Among the large set of functions, particularly noteworthy are those that implement\n    (1) the correlation-augmented inverse normal test on the cointegration rank\n    by Arsova and Oersal (2021, <doi:10.1016/j.ecosta.2020.05.002>),\n    (2) the two-step estimator for pooled cointegrating vectors\n    by Breitung (2005, <doi:10.1081/ETC-200067895>), and\n    (3) the pooled identification based on independent component analysis\n    by Herwartz and Wang (2024, <doi:10.1002/jae.3044>).",
    "version": "1.1.1",
    "maintainer": "Lennart Empting <lennart.empting@vwl.uni-due.de>",
    "author": "Lennart Empting [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-5068-4639>)",
    "url": "https://github.com/Lenni89/pvars",
    "bug_reports": "https://github.com/Lenni89/pvars/issues",
    "repository": "https://cran.r-project.org/package=pvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pvars VAR Modeling for Heterogeneous Panels Implements (1) panel cointegration rank tests, (2) estimators for panel\n    vector autoregressive (VAR) models, and (3) identification methods for panel\n    structural vector autoregressive (SVAR) models as described in the accompanying vignette.\n    The implemented functions allow to account for cross-sectional dependence\n    and for structural breaks in the deterministic terms of the VAR processes.\n    Among the large set of functions, particularly noteworthy are those that implement\n    (1) the correlation-augmented inverse normal test on the cointegration rank\n    by Arsova and Oersal (2021, <doi:10.1016/j.ecosta.2020.05.002>),\n    (2) the two-step estimator for pooled cointegrating vectors\n    by Breitung (2005, <doi:10.1081/ETC-200067895>), and\n    (3) the pooled identification based on independent component analysis\n    by Herwartz and Wang (2024, <doi:10.1002/jae.3044>).  "
  },
  {
    "id": 18595,
    "package_name": "pyramid",
    "title": "Draw Population Pyramid",
    "description": "Drawing population pyramid using (1) data.frame or (2) vectors.\n\t The former is named as pyramid() and the latter pyramids(), as wrapper\n\t function of pyramid().  pyramidf() is the function to draw population\n\t pyramid within the specified frame.",
    "version": "1.5",
    "maintainer": "Minato Nakazawa <minato-nakazawa@umin.net>",
    "author": "Minato Nakazawa <minato-nakazawa@umin.net>",
    "url": "http://minato.sip21c.org/swtips/Rgraphics.html#PYRAMID",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pyramid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pyramid Draw Population Pyramid Drawing population pyramid using (1) data.frame or (2) vectors.\n\t The former is named as pyramid() and the latter pyramids(), as wrapper\n\t function of pyramid().  pyramidf() is the function to draw population\n\t pyramid within the specified frame.  "
  },
  {
    "id": 18633,
    "package_name": "qfasar",
    "title": "Quantitative Fatty Acid Signature Analysis in R",
    "description": "An implementation of Quantitative Fatty Acid Signature\n    Analysis (QFASA) in R.  QFASA is a method of estimating the diet\n    composition of predators.  The fundamental unit of information in\n    QFASA is a fatty acid signature (signature), which is a vector of\n    proportions describing the composition of fatty acids within lipids.\n    Signature data from at least one predator and from samples of all\n    potential prey types are required.  Calibration coefficients, which\n    adjust for the differential metabolism of individual fatty acids by\n    predators, are also required. Given those data inputs, a predator\n    signature is modeled as a mixture of prey signatures and its diet\n    estimate is obtained as the mixture that minimizes a measure of\n    distance between the observed and modeled signatures.  A variety of\n    estimation options and simulation capabilities are implemented.\n    Please refer to the vignette for additional details and references.",
    "version": "1.2.1",
    "maintainer": "Jeffrey F. Bromaghin <jbromaghin@usgs.gov>",
    "author": "Jeffrey F. Bromaghin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qfasar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qfasar Quantitative Fatty Acid Signature Analysis in R An implementation of Quantitative Fatty Acid Signature\n    Analysis (QFASA) in R.  QFASA is a method of estimating the diet\n    composition of predators.  The fundamental unit of information in\n    QFASA is a fatty acid signature (signature), which is a vector of\n    proportions describing the composition of fatty acids within lipids.\n    Signature data from at least one predator and from samples of all\n    potential prey types are required.  Calibration coefficients, which\n    adjust for the differential metabolism of individual fatty acids by\n    predators, are also required. Given those data inputs, a predator\n    signature is modeled as a mixture of prey signatures and its diet\n    estimate is obtained as the mixture that minimizes a measure of\n    distance between the observed and modeled signatures.  A variety of\n    estimation options and simulation capabilities are implemented.\n    Please refer to the vignette for additional details and references.  "
  },
  {
    "id": 18656,
    "package_name": "qoi",
    "title": "Read and Write QOI Images",
    "description": "The new QOI file format offers a very simple but efficient image compression algorithm. This package provides an easy and simple way to read, write and display bitmap images stored in the QOI (Quite Ok Image) format. It can read and write both files and in-memory raw vectors.",
    "version": "0.1.0",
    "maintainer": "Johannes Friedrich <Johannes.Friedrich@posteo.de>",
    "author": "Johannes Friedrich [aut, trl, cre],\n  Dominic Szablewski [cph] (C library 'qoi')",
    "url": "https://github.com/JohannesFriedrich/qoi4R",
    "bug_reports": "https://github.com/JohannesFriedrich/qoi4R/issues",
    "repository": "https://cran.r-project.org/package=qoi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qoi Read and Write QOI Images The new QOI file format offers a very simple but efficient image compression algorithm. This package provides an easy and simple way to read, write and display bitmap images stored in the QOI (Quite Ok Image) format. It can read and write both files and in-memory raw vectors.  "
  },
  {
    "id": 18708,
    "package_name": "quadVAR",
    "title": "Quadratic Vector Autoregression",
    "description": "Estimate quadratic vector autoregression models with the\n    strong hierarchy using the Regularization Algorithm under Marginality\n    Principle (RAMP) by Hao et al. (2018)\n    <doi:10.1080/01621459.2016.1264956>, compare the performance with\n    linear models, and construct networks with partial derivatives.",
    "version": "0.1.2",
    "maintainer": "Jingmeng Cui <jingmeng.cui@outlook.com>",
    "author": "Jingmeng Cui [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3421-8457>)",
    "url": "https://github.com/Sciurus365/quadVAR,\nhttps://sciurus365.github.io/quadVAR/",
    "bug_reports": "https://github.com/Sciurus365/quadVAR/issues",
    "repository": "https://cran.r-project.org/package=quadVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quadVAR Quadratic Vector Autoregression Estimate quadratic vector autoregression models with the\n    strong hierarchy using the Regularization Algorithm under Marginality\n    Principle (RAMP) by Hao et al. (2018)\n    <doi:10.1080/01621459.2016.1264956>, compare the performance with\n    linear models, and construct networks with partial derivatives.  "
  },
  {
    "id": 18712,
    "package_name": "quadmesh",
    "title": "Quadrangle Mesh",
    "description": "Create surface forms from matrix or 'raster' data for flexible plotting and\n conversion to other mesh types. The functions 'quadmesh' or 'triangmesh'\n produce a continuous surface as a 'mesh3d' object as used by the 'rgl'\n package. This is used for plotting raster data in 3D (optionally with\n texture), and allows the application of a map projection without data loss and \n many processing applications that are restricted by inflexible regular grid rasters.\n There are discrete forms of these continuous surfaces available with\n 'dquadmesh' and 'dtriangmesh' functions.",
    "version": "0.5.5",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre]",
    "url": "https://github.com/hypertidy/quadmesh",
    "bug_reports": "https://github.com/hypertidy/quadmesh/issues",
    "repository": "https://cran.r-project.org/package=quadmesh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quadmesh Quadrangle Mesh Create surface forms from matrix or 'raster' data for flexible plotting and\n conversion to other mesh types. The functions 'quadmesh' or 'triangmesh'\n produce a continuous surface as a 'mesh3d' object as used by the 'rgl'\n package. This is used for plotting raster data in 3D (optionally with\n texture), and allows the application of a map projection without data loss and \n many processing applications that are restricted by inflexible regular grid rasters.\n There are discrete forms of these continuous surfaces available with\n 'dquadmesh' and 'dtriangmesh' functions.  "
  },
  {
    "id": 18717,
    "package_name": "quadtree",
    "title": "Region Quadtrees for Spatial Data",
    "description": "Provides functionality for working with raster-like quadtrees\n    (also called \u201cregion quadtrees\u201d), which allow for variable-sized\n    cells. The package allows for flexibility in the quadtree creation\n    process.  Several functions defining how to split and aggregate cells\n    are provided, and custom functions can be written for both of these\n    processes. In addition, quadtrees can be created using other quadtrees\n    as \u201ctemplates\u201d, so that the new quadtree's structure is identical to\n    the template quadtree. The package also includes functionality for\n    modifying quadtrees, querying values, saving quadtrees to a file, and\n    calculating least-cost paths using the quadtree as a resistance\n    surface.",
    "version": "0.1.14",
    "maintainer": "Derek Friend <dafriend.R@gmail.com>",
    "author": "Derek Friend [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6909-8769>),\n  Andrew Brown [ctb],\n  Randolph Voorhies [cph] (Author of included 'cereal' library),\n  Shane Grant [cph] (Author of included 'cereal' library),\n  Juan Pedro Bolivar Puente [cph] (Author of included 'cereal' library)",
    "url": "https://github.com/dfriend21/quadtree/,\nhttps://dfriend21.github.io/quadtree/",
    "bug_reports": "https://github.com/dfriend21/quadtree/issues/",
    "repository": "https://cran.r-project.org/package=quadtree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quadtree Region Quadtrees for Spatial Data Provides functionality for working with raster-like quadtrees\n    (also called \u201cregion quadtrees\u201d), which allow for variable-sized\n    cells. The package allows for flexibility in the quadtree creation\n    process.  Several functions defining how to split and aggregate cells\n    are provided, and custom functions can be written for both of these\n    processes. In addition, quadtrees can be created using other quadtrees\n    as \u201ctemplates\u201d, so that the new quadtree's structure is identical to\n    the template quadtree. The package also includes functionality for\n    modifying quadtrees, querying values, saving quadtrees to a file, and\n    calculating least-cost paths using the quadtree as a resistance\n    surface.  "
  },
  {
    "id": 18719,
    "package_name": "qualmap",
    "title": "Opinionated Approach for Digitizing Semi-Structured Qualitative\nGIS Data",
    "description": "Provides a set of functions for taking qualitative GIS data, hand drawn on a map, and \n   converting it to a simple features object. These tools are focused on data that are drawn on a map\n   that contains some type of polygon features. For each area identified on the map, the id numbers\n   of these polygons can be entered as vectors and transformed using qualmap.",
    "version": "0.2.2",
    "maintainer": "Christopher Prener <chris.prener@gmail.com>",
    "author": "Christopher Prener [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4310-9888>)",
    "url": "https://chris-prener.github.io/qualmap/",
    "bug_reports": "https://github.com/chris-prener/qualmap/issues",
    "repository": "https://cran.r-project.org/package=qualmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qualmap Opinionated Approach for Digitizing Semi-Structured Qualitative\nGIS Data Provides a set of functions for taking qualitative GIS data, hand drawn on a map, and \n   converting it to a simple features object. These tools are focused on data that are drawn on a map\n   that contains some type of polygon features. For each area identified on the map, the id numbers\n   of these polygons can be entered as vectors and transformed using qualmap.  "
  },
  {
    "id": 18732,
    "package_name": "quantities",
    "title": "Quantity Calculus for R Vectors",
    "description": "Integration of the 'units' and 'errors' packages for a complete\n    quantity calculus system for R vectors, matrices and arrays, with automatic\n    propagation, conversion, derivation and simplification of magnitudes and\n    uncertainties. Documentation about 'units' and 'errors' is provided in the\n    papers by Pebesma, Mailund & Hiebert (2016, <doi:10.32614/RJ-2016-061>) and\n    by Ucar, Pebesma & Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in\n    those packages as vignettes; see 'citation(\"quantities\")' for details.",
    "version": "0.2.3",
    "maintainer": "I\u00f1aki Ucar <iucar@fedoraproject.org>",
    "author": "I\u00f1aki Ucar [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-6403-5550>)",
    "url": "https://r-quantities.github.io/quantities/,\nhttps://github.com/r-quantities/quantities",
    "bug_reports": "https://github.com/r-quantities/quantities/issues",
    "repository": "https://cran.r-project.org/package=quantities",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantities Quantity Calculus for R Vectors Integration of the 'units' and 'errors' packages for a complete\n    quantity calculus system for R vectors, matrices and arrays, with automatic\n    propagation, conversion, derivation and simplification of magnitudes and\n    uncertainties. Documentation about 'units' and 'errors' is provided in the\n    papers by Pebesma, Mailund & Hiebert (2016, <doi:10.32614/RJ-2016-061>) and\n    by Ucar, Pebesma & Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in\n    those packages as vignettes; see 'citation(\"quantities\")' for details.  "
  },
  {
    "id": 18733,
    "package_name": "quantkriging",
    "title": "Quantile Kriging for Stochastic Simulations with Replication",
    "description": "A re-implementation of quantile kriging. Quantile kriging was described by Plumlee and Tuo (2014) <doi:10.1080/00401706.2013.860919>.  With computational savings when dealing with replication from the recent paper by Binois, Gramacy, and Ludovski (2018) <doi:10.1080/10618600.2018.1458625> it is now possible to apply quantile kriging to a wider class of problems.  In addition to fitting the model, other useful tools are provided such as the ability to automatically perform leave-one-out cross validation.",
    "version": "0.1.0",
    "maintainer": "Kevin R. Quinlan <quinlan5@llnl.gov>",
    "author": "Kevin R. Quinlan [aut, cre],\n  Jim R. Leek [aut],\n  Lawrence Livermore National Security [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quantkriging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantkriging Quantile Kriging for Stochastic Simulations with Replication A re-implementation of quantile kriging. Quantile kriging was described by Plumlee and Tuo (2014) <doi:10.1080/00401706.2013.860919>.  With computational savings when dealing with replication from the recent paper by Binois, Gramacy, and Ludovski (2018) <doi:10.1080/10618600.2018.1458625> it is now possible to apply quantile kriging to a wider class of problems.  In addition to fitting the model, other useful tools are provided such as the ability to automatically perform leave-one-out cross validation.  "
  },
  {
    "id": 18754,
    "package_name": "quest",
    "title": "Prepare Questionnaire Data for Analysis",
    "description": "Offers a suite of functions to prepare questionnaire data for analysis (perhaps other types of data as well). By data preparation, I mean data analytic tasks to get your raw data ready for statistical modeling (e.g., regression). There are functions to investigate missing data, reshape data, validate responses, recode variables, score questionnaires, center variables, aggregate by groups, shift scores (i.e., leads or lags), etc. It provides functions for both single level and multilevel (i.e., grouped) data. With a few exceptions (e.g., ncases()), functions without an \"s\" at the end of their primary word (e.g., center_by()) act on atomic vectors, while functions with an \"s\" at the end of their primary word (e.g., centers_by()) act on multiple columns of a data.frame.",
    "version": "0.2.1",
    "maintainer": "David Disabato <ddisab01@gmail.com>",
    "author": "David Disabato [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7094-4996>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quest Prepare Questionnaire Data for Analysis Offers a suite of functions to prepare questionnaire data for analysis (perhaps other types of data as well). By data preparation, I mean data analytic tasks to get your raw data ready for statistical modeling (e.g., regression). There are functions to investigate missing data, reshape data, validate responses, recode variables, score questionnaires, center variables, aggregate by groups, shift scores (i.e., leads or lags), etc. It provides functions for both single level and multilevel (i.e., grouped) data. With a few exceptions (e.g., ncases()), functions without an \"s\" at the end of their primary word (e.g., center_by()) act on atomic vectors, while functions with an \"s\" at the end of their primary word (e.g., centers_by()) act on multiple columns of a data.frame.  "
  },
  {
    "id": 18760,
    "package_name": "quickPlot",
    "title": "A System of Plotting Optimized for Speed and Modularity",
    "description": "A high-level plotting system, compatible with `ggplot2` objects, \n    maps from `sf`, `terra`, `raster`, `sp`. It is built primarily on the \n    'grid' package. The objective of the package is to provide a plotting system \n    that is built for speed and modularity. This is useful for quick visualizations \n    when testing code and for plotting multiple figures to the same device from \n    independent sources that may be independent of one another (i.e., different \n    function or modules the create the visualizations).",
    "version": "1.0.4",
    "maintainer": "Eliot J B McIntire <eliot.mcintire@canada.ca>",
    "author": "Eliot J B McIntire [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6914-8316>),\n  Alex M Chubaty [aut] (ORCID: <https://orcid.org/0000-0001-7146-8135>),\n  His Majesty the King in Right of Canada, as represented by the Minister\n    of Natural Resources Canada [cph]",
    "url": "https://quickplot.predictiveecology.org,\nhttps://github.com/PredictiveEcology/quickPlot",
    "bug_reports": "https://github.com/PredictiveEcology/quickPlot/issues",
    "repository": "https://cran.r-project.org/package=quickPlot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quickPlot A System of Plotting Optimized for Speed and Modularity A high-level plotting system, compatible with `ggplot2` objects, \n    maps from `sf`, `terra`, `raster`, `sp`. It is built primarily on the \n    'grid' package. The objective of the package is to provide a plotting system \n    that is built for speed and modularity. This is useful for quick visualizations \n    when testing code and for plotting multiple figures to the same device from \n    independent sources that may be independent of one another (i.e., different \n    function or modules the create the visualizations).  "
  },
  {
    "id": 18819,
    "package_name": "rARPACK",
    "title": "Solvers for Large Scale Eigenvalue and SVD Problems",
    "description": "Previously an R wrapper of the 'ARPACK' library\n    <http://www.caam.rice.edu/software/ARPACK/>, and now a shell of the\n    R package 'RSpectra', an R interface to the 'Spectra' library\n    <http://yixuan.cos.name/spectra/> for solving large scale\n    eigenvalue/vector problems. The current version of 'rARPACK'\n    simply imports and exports the functions provided by 'RSpectra'.\n    New users of 'rARPACK' are advised to switch to the 'RSpectra' package.",
    "version": "0.11-0",
    "maintainer": "Yixuan Qiu <yixuan.qiu@cos.name>",
    "author": "Yixuan Qiu, Jiali Mei and authors of the ARPACK library. See file\n    AUTHORS for details.",
    "url": "https://github.com/yixuan/rARPACK",
    "bug_reports": "https://github.com/yixuan/rARPACK/issues",
    "repository": "https://cran.r-project.org/package=rARPACK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rARPACK Solvers for Large Scale Eigenvalue and SVD Problems Previously an R wrapper of the 'ARPACK' library\n    <http://www.caam.rice.edu/software/ARPACK/>, and now a shell of the\n    R package 'RSpectra', an R interface to the 'Spectra' library\n    <http://yixuan.cos.name/spectra/> for solving large scale\n    eigenvalue/vector problems. The current version of 'rARPACK'\n    simply imports and exports the functions provided by 'RSpectra'.\n    New users of 'rARPACK' are advised to switch to the 'RSpectra' package.  "
  },
  {
    "id": 18829,
    "package_name": "rBeta2009",
    "title": "The Beta Random Number and Dirichlet Random Vector Generating\nFunctions",
    "description": "Contains functions to generate random numbers\n        from the beta distribution and random vectors from the\n        Dirichlet distribution.",
    "version": "1.0.1",
    "maintainer": "Ching-Wei Cheng <aks43725@gmail.com>",
    "author": "Ching-Wei Cheng [aut, cre],\n  Ying-Chao Hung [aut],\n  Narayanaswamy Balakrishnan [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rBeta2009",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rBeta2009 The Beta Random Number and Dirichlet Random Vector Generating\nFunctions Contains functions to generate random numbers\n        from the beta distribution and random vectors from the\n        Dirichlet distribution.  "
  },
  {
    "id": 18842,
    "package_name": "rEMM",
    "title": "Extensible Markov Model for Modelling Temporal Relationships\nBetween Clusters",
    "description": "Implements TRACDS (Temporal Relationships \n    between Clusters for Data Streams), a generalization of \n    Extensible Markov Model (EMM). TRACDS adds a temporal or order model\n    to data stream clustering by superimposing a dynamically adapting\n    Markov Chain. Also provides an implementation of EMM (TRACDS on top of tNN \n    data stream clustering). Development of this \n    package was supported in part by NSF IIS-0948893 and R21HG005912 from \n    the National Human Genome Research Institute. Hahsler and Dunham (2010) <doi:10.18637/jss.v035.i05>.",
    "version": "1.2.1",
    "maintainer": "Michael Hahsler <mhahsler@lyle.smu.edu>",
    "author": "Michael Hahsler [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2716-1405>),\n  Margaret H. Dunham [ctb]",
    "url": "https://github.com/mhahsler/rEMM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rEMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rEMM Extensible Markov Model for Modelling Temporal Relationships\nBetween Clusters Implements TRACDS (Temporal Relationships \n    between Clusters for Data Streams), a generalization of \n    Extensible Markov Model (EMM). TRACDS adds a temporal or order model\n    to data stream clustering by superimposing a dynamically adapting\n    Markov Chain. Also provides an implementation of EMM (TRACDS on top of tNN \n    data stream clustering). Development of this \n    package was supported in part by NSF IIS-0948893 and R21HG005912 from \n    the National Human Genome Research Institute. Hahsler and Dunham (2010) <doi:10.18637/jss.v035.i05>.  "
  },
  {
    "id": 18866,
    "package_name": "rLakeHabitat",
    "title": "Interpolate Bathymetry and Quantify Physical Aquatic Habitat",
    "description": "Offers bathymetric interpolation using Inverse Distance Weighted and Ordinary Kriging via the 'gstat' and 'terra' packages. Other functions focus on quantifying physical aquatic habitats (e.g., littoral, epliminion, metalimnion, hypolimnion) from interpolated digital elevation models (DEMs). Functions were designed to calculate these metrics across water levels for use in reservoirs but can be applied to any DEM and will provide values for fixed conditions. Parameters like Secchi disk depth or estimated photic zone, thermocline depth, and water level fluctuation depth are included in most functions.",
    "version": "1.0.1",
    "maintainer": "Tristan Blechinger <tblechin@uwyo.edu>",
    "author": "Tristan Blechinger [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-6730-8856>),\n  Sean Bertalot [aut] (ORCID: <https://orcid.org/0000-0001-8099-0144>)",
    "url": "https://gitlab.com/tristanblechinger/rlakehabitat",
    "bug_reports": "https://gitlab.com/tristanblechinger/rlakehabitat/-/issues",
    "repository": "https://cran.r-project.org/package=rLakeHabitat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rLakeHabitat Interpolate Bathymetry and Quantify Physical Aquatic Habitat Offers bathymetric interpolation using Inverse Distance Weighted and Ordinary Kriging via the 'gstat' and 'terra' packages. Other functions focus on quantifying physical aquatic habitats (e.g., littoral, epliminion, metalimnion, hypolimnion) from interpolated digital elevation models (DEMs). Functions were designed to calculate these metrics across water levels for use in reservoirs but can be applied to any DEM and will provide values for fixed conditions. Parameters like Secchi disk depth or estimated photic zone, thermocline depth, and water level fluctuation depth are included in most functions.  "
  },
  {
    "id": 18897,
    "package_name": "rSpectral",
    "title": "Spectral Modularity Clustering",
    "description": "Implements the network clustering algorithm described in \n            Newman (2006) <doi:10.1103/PhysRevE.74.036104>. The complete \n            iterative algorithm comprises of two steps. In the first step, the \n            network is expressed in terms of its leading eigenvalue and \n            eigenvector and recursively partition into two communities. \n            Partitioning occurs if the maximum positive \n            eigenvalue is greater than the tolerance (10e-5) for the current \n            partition, and if it results in a positive contribution to the \n            Modularity.\n            Given an initial separation using the leading eigen step, 'rSpectral' \n            then continues to maximise for the change in Modularity using a \n            fine-tuning step - or variate thereof. The first stage here is to \n            find the node which, when moved from one community to another, \n            gives the maximum change in Modularity. This node\u2019s community is \n            then fixed and we repeat the process until all nodes have been moved. \n            The whole process is repeated from this new state until the change \n            in the Modularity, between the new and old state, is less than the \n            predefined tolerance.\n            A slight variant of the fine-tuning step, which can improve speed \n            of the calculation, is also provided. Instead of moving each node \n            into each community in turn, we only consider moves of neighbouring \n            nodes, found in different communities, to the community of the \n            current node of interest. The two steps process is repeatedly \n            applied to each new community found, subdivided each community \n            into two new communities, until we are unable to find any division \n            that results in a positive change in Modularity. ",
    "version": "1.0.0.14",
    "maintainer": "Anatoly Sorokin <lptolik@gmail.com>",
    "author": "Colin Mclean [aut] (algorithm implementation in Rcpp functions),\n  Anatoly Sorokin [aut, cre] (R functions, cranification, documentation,\n    testing, maintenance)",
    "url": "https://github.com/cmclean5/rSpectral",
    "bug_reports": "https://github.com/cmclean5/rSpectral/issues/",
    "repository": "https://cran.r-project.org/package=rSpectral",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rSpectral Spectral Modularity Clustering Implements the network clustering algorithm described in \n            Newman (2006) <doi:10.1103/PhysRevE.74.036104>. The complete \n            iterative algorithm comprises of two steps. In the first step, the \n            network is expressed in terms of its leading eigenvalue and \n            eigenvector and recursively partition into two communities. \n            Partitioning occurs if the maximum positive \n            eigenvalue is greater than the tolerance (10e-5) for the current \n            partition, and if it results in a positive contribution to the \n            Modularity.\n            Given an initial separation using the leading eigen step, 'rSpectral' \n            then continues to maximise for the change in Modularity using a \n            fine-tuning step - or variate thereof. The first stage here is to \n            find the node which, when moved from one community to another, \n            gives the maximum change in Modularity. This node\u2019s community is \n            then fixed and we repeat the process until all nodes have been moved. \n            The whole process is repeated from this new state until the change \n            in the Modularity, between the new and old state, is less than the \n            predefined tolerance.\n            A slight variant of the fine-tuning step, which can improve speed \n            of the calculation, is also provided. Instead of moving each node \n            into each community in turn, we only consider moves of neighbouring \n            nodes, found in different communities, to the community of the \n            current node of interest. The two steps process is repeatedly \n            applied to each new community found, subdivided each community \n            into two new communities, until we are unable to find any division \n            that results in a positive change in Modularity.   "
  },
  {
    "id": 18930,
    "package_name": "raem",
    "title": "Analytic Element Modeling of Steady Single-Layer Groundwater\nFlow",
    "description": "A model of single-layer groundwater flow in steady-state\n    under the Dupuit-Forchheimer assumption can be created by placing\n    elements such as wells, area-sinks and line-sinks at arbitrary\n    locations in the flow field. Output variables include hydraulic head\n    and the discharge vector. Particle traces can be computed numerically\n    in three dimensions. The underlying theory is described in Haitjema\n    (1995) <doi:10.1016/B978-0-12-316550-3.X5000-4> and references\n    therein.",
    "version": "0.1.0",
    "maintainer": "Cas Neyens <cas.neyens@gmail.com>",
    "author": "Cas Neyens [aut, cre, cph]",
    "url": "https://github.com/cneyens/raem, https://cneyens.github.io/raem/",
    "bug_reports": "https://github.com/cneyens/raem/issues",
    "repository": "https://cran.r-project.org/package=raem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raem Analytic Element Modeling of Steady Single-Layer Groundwater\nFlow A model of single-layer groundwater flow in steady-state\n    under the Dupuit-Forchheimer assumption can be created by placing\n    elements such as wells, area-sinks and line-sinks at arbitrary\n    locations in the flow field. Output variables include hydraulic head\n    and the discharge vector. Particle traces can be computed numerically\n    in three dimensions. The underlying theory is described in Haitjema\n    (1995) <doi:10.1016/B978-0-12-316550-3.X5000-4> and references\n    therein.  "
  },
  {
    "id": 18932,
    "package_name": "rafsi",
    "title": "Ranking of Alternatives with the RAFSI Method",
    "description": "Ranking of Alternatives through Functional mapping of criterion sub-intervals into a Single Interval Method is designed to perform multi-criteria decision-making (MCDM), developed by Mali\u0161a \u017di\u017eovic in 2020 (<doi:10.3390/math8061015>). \n              It calculates the final sorted rankings based on a decision matrix where rows represent alternatives \n              and columns represent criteria. The method uses:\n              - A numeric vector of weights for each criterion (the sum of weights must be 1).\n              - A numeric vector of ideal values for each criterion.\n              - A numeric vector of anti-ideal values for each criterion.\n              - Numeric values representing the extent to which the ideal value is preferred over the anti-ideal value, \n                and the extent to which the anti-ideal value is considered worse.\n              The function standardizes the decision matrix, normalizes the data, applies weights, and returns the \n              final sorted rankings. ",
    "version": "0.0.2",
    "maintainer": "Mateus Vanzetta <mateusvanzetta@id.uff.br>",
    "author": "Mateus Vanzetta [aut, cre],\n  Marcos Santos [ctb] (ORCID: <https://orcid.org/0000-0003-1533-5535>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rafsi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rafsi Ranking of Alternatives with the RAFSI Method Ranking of Alternatives through Functional mapping of criterion sub-intervals into a Single Interval Method is designed to perform multi-criteria decision-making (MCDM), developed by Mali\u0161a \u017di\u017eovic in 2020 (<doi:10.3390/math8061015>). \n              It calculates the final sorted rankings based on a decision matrix where rows represent alternatives \n              and columns represent criteria. The method uses:\n              - A numeric vector of weights for each criterion (the sum of weights must be 1).\n              - A numeric vector of ideal values for each criterion.\n              - A numeric vector of anti-ideal values for each criterion.\n              - Numeric values representing the extent to which the ideal value is preferred over the anti-ideal value, \n                and the extent to which the anti-ideal value is considered worse.\n              The function standardizes the decision matrix, normalizes the data, applies weights, and returns the \n              final sorted rankings.   "
  },
  {
    "id": 18947,
    "package_name": "rampage",
    "title": "Calibrated Color Ramps",
    "description": "Value-calibrated color ramps can be useful to emphasize patterns in data from complex distributions. Colors can be tied to specific values, and the association can be expanded into full color ramps that also include the relationship between colors and values. Such ramps can be used in a variety of cases when heatmap-type plots are necessary, including the visualization of vector and raster spatial data, such as topographies.",
    "version": "0.2.0",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "author": "Adam T. Kocsis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9028-665X>),\n  Deutsche Forschungsgemeinschaft [fnd],\n  FAU GeoZentrum Nordbayern [fnd]",
    "url": "https://adamtkocsis.com/rampage/",
    "bug_reports": "https://github.com/adamkocsis/rampage/issues",
    "repository": "https://cran.r-project.org/package=rampage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rampage Calibrated Color Ramps Value-calibrated color ramps can be useful to emphasize patterns in data from complex distributions. Colors can be tied to specific values, and the association can be expanded into full color ramps that also include the relationship between colors and values. Such ramps can be used in a variety of cases when heatmap-type plots are necessary, including the visualization of vector and raster spatial data, such as topographies.  "
  },
  {
    "id": 18948,
    "package_name": "ramps",
    "title": "Bayesian Geostatistical Modeling with RAMPS",
    "description": "Bayesian geostatistical modeling of Gaussian processes using a\n    reparameterized and marginalized posterior sampling (RAMPS) algorithm\n    designed to lower autocorrelation in MCMC samples.  Package performance is\n    tuned for large spatial datasets.",
    "version": "0.6.18",
    "maintainer": "Brian J Smith <brian-j-smith@uiowa.edu>",
    "author": "Brian J Smith [aut, cre],\n  Jun Yan [aut],\n  Mary Kathryn Cowles [aut]",
    "url": "https://www.jstatsoft.org/v25/i10",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ramps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ramps Bayesian Geostatistical Modeling with RAMPS Bayesian geostatistical modeling of Gaussian processes using a\n    reparameterized and marginalized posterior sampling (RAMPS) algorithm\n    designed to lower autocorrelation in MCMC samples.  Package performance is\n    tuned for large spatial datasets.  "
  },
  {
    "id": 18949,
    "package_name": "ramsvm",
    "title": "Reinforced Angle-Based Multicategory Support Vector Machines",
    "description": "Provides a solution path for Reinforced Angle-based Multicategory \n    Support Vector Machines, with linear learning, polynomial learning, and \n    Gaussian kernel learning. C. Zhang, Y. Liu, J. Wang and H. Zhu. (2016) \n    <doi:10.1080/10618600.2015.1043010>.",
    "version": "2.4",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "author": "Chong Zhang [aut],\n  Yufeng Liu [aut],\n  Shannon T. Holloway [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ramsvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ramsvm Reinforced Angle-Based Multicategory Support Vector Machines Provides a solution path for Reinforced Angle-based Multicategory \n    Support Vector Machines, with linear learning, polynomial learning, and \n    Gaussian kernel learning. C. Zhang, Y. Liu, J. Wang and H. Zhu. (2016) \n    <doi:10.1080/10618600.2015.1043010>.  "
  },
  {
    "id": 18954,
    "package_name": "randnet",
    "title": "Random Network Model Estimation, Selection and Parameter Tuning",
    "description": "Model fitting, model selection and parameter tuning procedures for a class of random network models. Many useful network modeling, estimation, and processing methods are included. The work to build and improve this package is partially supported by the NSF grants DMS-2015298 and DMS-2015134.",
    "version": "1.0",
    "maintainer": "Tianxi Li <tianxili@umn.edu>",
    "author": "Tianxi Li [aut, cre],\n  Elizeveta Levina [aut],\n  Ji Zhu [aut],\n  Can M. Le [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=randnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randnet Random Network Model Estimation, Selection and Parameter Tuning Model fitting, model selection and parameter tuning procedures for a class of random network models. Many useful network modeling, estimation, and processing methods are included. The work to build and improve this package is partially supported by the NSF grants DMS-2015298 and DMS-2015134.  "
  },
  {
    "id": 18966,
    "package_name": "randomMachines",
    "title": "An Ensemble Modeling using Random Machines",
    "description": "A novel ensemble method employing Support Vector Machines (SVMs) as base learners. This powerful ensemble model is designed for both classification (Ara A., et. al, 2021) <doi:10.6339/21-JDS1014>, and regression (Ara A., et. al, 2021) <doi:10.1016/j.eswa.2022.117107> problems, offering versatility and robust performance across different datasets and compared with other consolidated methods as Random Forests (Maia M, et. al, 2021) <doi:10.6339/21-JDS1025>.",
    "version": "0.1.1",
    "maintainer": "Mateus Maia <mateus.maiamarques@glasgow.ac.uk>",
    "author": "Mateus Maia [aut, cre] (ORCID: <https://orcid.org/0000-0001-7056-386X>),\n  Anderson Ara [cte] (ORCID: <https://orcid.org/0000-0002-1041-2768>),\n  Gabriel Ribeiro [cte]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=randomMachines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randomMachines An Ensemble Modeling using Random Machines A novel ensemble method employing Support Vector Machines (SVMs) as base learners. This powerful ensemble model is designed for both classification (Ara A., et. al, 2021) <doi:10.6339/21-JDS1014>, and regression (Ara A., et. al, 2021) <doi:10.1016/j.eswa.2022.117107> problems, offering versatility and robust performance across different datasets and compared with other consolidated methods as Random Forests (Maia M, et. al, 2021) <doi:10.6339/21-JDS1025>.  "
  },
  {
    "id": 19016,
    "package_name": "rassta",
    "title": "Raster-Based Spatial Stratification Algorithms",
    "description": "Algorithms for the spatial stratification of landscapes, sampling and modeling of \n    spatially-varying phenomena. These algorithms offer a simple framework for the stratification \n    of geographic space based on raster layers representing landscape factors and/or factor scales. \n    The stratification process follows a hierarchical approach, which is based on first level units \n    (i.e., classification units) and second-level units (i.e., stratification units). Nonparametric \n    techniques allow to measure the correspondence between the geographic space and the landscape \n    configuration represented by the units. These correspondence metrics are useful to define \n    sampling schemes and to model the spatial variability of environmental phenomena. The \n    theoretical background of the algorithms and code examples are presented in Fuentes et al. (2022). \n    <doi:10.32614/RJ-2022-036>.",
    "version": "1.0.6",
    "maintainer": "Bryan A. Fuentes <bryandrep@gmail.com>",
    "author": "Bryan A. Fuentes [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3506-7101>),\n  Minerva J. Dorantes [aut] (ORCID:\n    <https://orcid.org/0000-0002-2877-832X>),\n  John R. Tipton [aut],\n  Robert J. Hijmans [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5872-2872>),\n  Andrew G. Brown [ctb]",
    "url": "https://bafuentes.github.io/rassta/",
    "bug_reports": "https://github.com/bafuentes/rassta/issues/",
    "repository": "https://cran.r-project.org/package=rassta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rassta Raster-Based Spatial Stratification Algorithms Algorithms for the spatial stratification of landscapes, sampling and modeling of \n    spatially-varying phenomena. These algorithms offer a simple framework for the stratification \n    of geographic space based on raster layers representing landscape factors and/or factor scales. \n    The stratification process follows a hierarchical approach, which is based on first level units \n    (i.e., classification units) and second-level units (i.e., stratification units). Nonparametric \n    techniques allow to measure the correspondence between the geographic space and the landscape \n    configuration represented by the units. These correspondence metrics are useful to define \n    sampling schemes and to model the spatial variability of environmental phenomena. The \n    theoretical background of the algorithms and code examples are presented in Fuentes et al. (2022). \n    <doi:10.32614/RJ-2022-036>.  "
  },
  {
    "id": 19017,
    "package_name": "rasterDT",
    "title": "Fast Raster Summary and Manipulation",
    "description": "\n  Fast alternatives to several relatively slow 'raster' package\n  functions. For large rasters, the functions run from 5 to\n  approximately 100 times faster than the 'raster' package functions\n  they replace. The 'fasterize' package, on which one function in this\n  package depends, includes an implementation of the scan line\n  algorithm attributed to Wylie et al. (1967)\n  <doi:10.1145/1465611.1465619>.",
    "version": "0.3.2",
    "maintainer": "Joshua O'Brien <joshmobrien@gmail.com>",
    "author": "Joshua O'Brien",
    "url": "https://github.com/JoshOBrien/rasterDT/",
    "bug_reports": "https://github.com/JoshOBrien/rasterDT/issues/",
    "repository": "https://cran.r-project.org/package=rasterDT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterDT Fast Raster Summary and Manipulation \n  Fast alternatives to several relatively slow 'raster' package\n  functions. For large rasters, the functions run from 5 to\n  approximately 100 times faster than the 'raster' package functions\n  they replace. The 'fasterize' package, on which one function in this\n  package depends, includes an implementation of the scan line\n  algorithm attributed to Wylie et al. (1967)\n  <doi:10.1145/1465611.1465619>.  "
  },
  {
    "id": 19018,
    "package_name": "rasterImage",
    "title": "An Improved Wrapper of image()",
    "description": "This is a wrapper function for image(), which makes reasonable\n    raster plots with nice axis and other useful features.",
    "version": "0.4.0",
    "maintainer": "Martin Seilmayer <m.seilmayer@hzdr.de>",
    "author": "Martin Seilmayer",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rasterImage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterImage An Improved Wrapper of image() This is a wrapper function for image(), which makes reasonable\n    raster plots with nice axis and other useful features.  "
  },
  {
    "id": 19019,
    "package_name": "rasterKernelEstimates",
    "title": "Kernel Based Estimates on in-Memory Raster Images",
    "description": "Performs kernel based estimates on in-memory raster images \n  from the raster package.  These kernel estimates include local means\n  variances, modes, and quantiles.  All results are in the form of \n  raster images, preserving original resolution and projection attributes.",
    "version": "1.0.2",
    "maintainer": "Jonathan Lisic <jlisic@gmail.com>",
    "author": "Jonathan Lisic [aut, cre]",
    "url": "http://meanmean.me/blog/rasterKernel/rasterKernel.html",
    "bug_reports": "https://github.com/jlisic/rasterKernelEstimates/issues",
    "repository": "https://cran.r-project.org/package=rasterKernelEstimates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterKernelEstimates Kernel Based Estimates on in-Memory Raster Images Performs kernel based estimates on in-memory raster images \n  from the raster package.  These kernel estimates include local means\n  variances, modes, and quantiles.  All results are in the form of \n  raster images, preserving original resolution and projection attributes.  "
  },
  {
    "id": 19020,
    "package_name": "rasterList",
    "title": "A Raster Where Cells are Generic Objects",
    "description": "A S4 class has been created such that complex operations can be\n    executed on each cell of a raster map. The raster of objects contains a raster map with the addition of a list of generic objects: one\n    object for each raster cells. It allows to write few lines of R code for complex\n    map algebra. Two environmental applications about frequency analysis of raster\n    map of precipitation and creation of a raster map of soil water retention curves\n    have been presented.",
    "version": "0.5.21",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano [aut, cre]",
    "url": "https://github.com/ecor/rasterList",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rasterList",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterList A Raster Where Cells are Generic Objects A S4 class has been created such that complex operations can be\n    executed on each cell of a raster map. The raster of objects contains a raster map with the addition of a list of generic objects: one\n    object for each raster cells. It allows to write few lines of R code for complex\n    map algebra. Two environmental applications about frequency analysis of raster\n    map of precipitation and creation of a raster map of soil water retention curves\n    have been presented.  "
  },
  {
    "id": 19021,
    "package_name": "rasterVis",
    "title": "Visualization Methods for Raster Data",
    "description": "Methods for enhanced visualization and interaction with raster data. It implements visualization methods for quantitative data and categorical data, both for univariate and multivariate rasters. It also provides methods to display spatiotemporal rasters, and vector fields. See the website for examples.",
    "version": "0.51.7",
    "maintainer": "Oscar Perpinan Lamigueiro <oscar.perpinan@upm.es>",
    "author": "Oscar Perpinan Lamigueiro [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4134-7196>),\n  Robert Hijmans [aut],\n  Alexandre Courtiol [ctb]",
    "url": "https://oscarperpinan.codeberg.page/rastervis/",
    "bug_reports": "https://codeberg.org/oscarperpinan/rastervis/issues",
    "repository": "https://cran.r-project.org/package=rasterVis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterVis Visualization Methods for Raster Data Methods for enhanced visualization and interaction with raster data. It implements visualization methods for quantitative data and categorical data, both for univariate and multivariate rasters. It also provides methods to display spatiotemporal rasters, and vector fields. See the website for examples.  "
  },
  {
    "id": 19022,
    "package_name": "rasterbc",
    "title": "Access Forest Ecology Layers for British Columbia in 2001-2018",
    "description": "R-based access to a large set of data variables relevant to forest ecology in British Columbia (BC), Canada. Layers\n    are in raster format at 100m resolution in the BC Albers projection, hosted at the Federated Research Data Repository (FRDR)\n    with <doi:10.20383/101.0283>. The collection includes: elevation; biogeoclimatic zone; wildfire; cutblocks; forest attributes from\n    Hansen et al. (2013) <doi:10.1139/cjfr-2013-0401> and Beaudoin et al. (2017) <doi:10.1139/cjfr-2017-0184>; and rasterized\n    Forest Insect and Disease Survey (FIDS) maps for a number of insect pest species, all covering the period 2001-2018.\n    Users supply a polygon or point location in the province of BC, and 'rasterbc' will download the overlapping raster tiles\n    hosted at FRDR, merging them as needed and returning the result in R as a 'SpatRaster' object. Metadata associated with these\n    layers, and code for downloading them from their original sources can be found in the 'github' repository\n    <https://github.com/deankoch/rasterbc_src>.",
    "version": "1.0.2",
    "maintainer": "Dean Koch <dkoch@ualberta.ca>",
    "author": "Dean Koch [aut, cre] (ORCID: <https://orcid.org/0000-0002-8849-859X>)",
    "url": "https://github.com/deankoch/rasterbc,\nhttps://github.com/deankoch/rasterbc_src",
    "bug_reports": "https://github.com/deankoch/rasterbc/issues",
    "repository": "https://cran.r-project.org/package=rasterbc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterbc Access Forest Ecology Layers for British Columbia in 2001-2018 R-based access to a large set of data variables relevant to forest ecology in British Columbia (BC), Canada. Layers\n    are in raster format at 100m resolution in the BC Albers projection, hosted at the Federated Research Data Repository (FRDR)\n    with <doi:10.20383/101.0283>. The collection includes: elevation; biogeoclimatic zone; wildfire; cutblocks; forest attributes from\n    Hansen et al. (2013) <doi:10.1139/cjfr-2013-0401> and Beaudoin et al. (2017) <doi:10.1139/cjfr-2017-0184>; and rasterized\n    Forest Insect and Disease Survey (FIDS) maps for a number of insect pest species, all covering the period 2001-2018.\n    Users supply a polygon or point location in the province of BC, and 'rasterbc' will download the overlapping raster tiles\n    hosted at FRDR, merging them as needed and returning the result in R as a 'SpatRaster' object. Metadata associated with these\n    layers, and code for downloading them from their original sources can be found in the 'github' repository\n    <https://github.com/deankoch/rasterbc_src>.  "
  },
  {
    "id": 19024,
    "package_name": "rasterize",
    "title": "Rasterize Graphical Output",
    "description": "Provides R functions to selectively rasterize components\n             of 'grid' output.  ",
    "version": "0.1",
    "maintainer": "Paul Murrell <paul@stat.auckland.ac.nz>",
    "author": "Paul Murrell",
    "url": "https://github.com/pmur002/rasterize,\nhttps://stattech.wordpress.fos.auckland.ac.nz/2018/05/25/2018-05-selective-raster-graphics/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rasterize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterize Rasterize Graphical Output Provides R functions to selectively rasterize components\n             of 'grid' output.    "
  },
  {
    "id": 19025,
    "package_name": "rasterpdf",
    "title": "Plot Raster Graphics in PDF Files",
    "description": "The ability to plot raster graphics in PDF files can be useful\n    when one needs multi-page documents, but the plots contain so many\n    individual elements that (the usual) use of vector graphics results in\n    inconveniently large file sizes. Internally, the package plots each\n    individual page as a PNG, and then combines them in one PDF file.",
    "version": "0.1.1",
    "maintainer": "Ilari Scheinin <ilari.scheinin+rasterpdf@gmail.com>",
    "author": "Ilari Scheinin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4696-9066>)",
    "url": "https://ilarischeinin.github.io/rasterpdf,\nhttps://github.com/ilarischeinin/rasterpdf",
    "bug_reports": "https://github.com/ilarischeinin/rasterpdf/issues",
    "repository": "https://cran.r-project.org/package=rasterpdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterpdf Plot Raster Graphics in PDF Files The ability to plot raster graphics in PDF files can be useful\n    when one needs multi-page documents, but the plots contain so many\n    individual elements that (the usual) use of vector graphics results in\n    inconveniently large file sizes. Internally, the package plots each\n    individual page as a PNG, and then combines them in one PDF file.  "
  },
  {
    "id": 19026,
    "package_name": "rasterpic",
    "title": "Convert Digital Images into 'SpatRaster' Objects",
    "description": "Generate 'SpatRaster' objects, as defined by the 'terra'\n    package, from digital images, using a specified spatial object as a\n    geographical reference.",
    "version": "0.3.0",
    "maintainer": "Diego Hernang\u00f3mez <diego.hernangomezherrero@gmail.com>",
    "author": "Diego Hernang\u00f3mez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>)",
    "url": "https://dieghernan.github.io/rasterpic/,\nhttps://github.com/dieghernan/rasterpic",
    "bug_reports": "https://github.com/dieghernan/rasterpic/issues",
    "repository": "https://cran.r-project.org/package=rasterpic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterpic Convert Digital Images into 'SpatRaster' Objects Generate 'SpatRaster' objects, as defined by the 'terra'\n    package, from digital images, using a specified spatial object as a\n    geographical reference.  "
  },
  {
    "id": 19052,
    "package_name": "rayvertex",
    "title": "3D Software Rasterizer",
    "description": "Rasterize images using a 3D software renderer. 3D scenes are created either by importing external files, building scenes out of the included objects, or by constructing meshes manually. Supports point and directional lights, anti-aliased lines, shadow mapping, transparent objects, translucent objects, multiple materials types, reflection, refraction, environment maps, multicore rendering, bloom, tone-mapping, and screen-space ambient occlusion.",
    "version": "0.12.0",
    "maintainer": "Tyler Morgan-Wall <tylermw@gmail.com>",
    "author": "Tyler Morgan-Wall [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3131-3814>),\n  Syoyo Fujita [ctb, cph],\n  Vilya Harvey [ctb, cph],\n  G-Truc Creation [ctb, cph],\n  Sean Barrett [ctb, cph]",
    "url": "https://www.rayvertex.com,\nhttps://github.com/tylermorganwall/rayvertex",
    "bug_reports": "https://github.com/tylermorganwall/rayvertex/issues",
    "repository": "https://cran.r-project.org/package=rayvertex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rayvertex 3D Software Rasterizer Rasterize images using a 3D software renderer. 3D scenes are created either by importing external files, building scenes out of the included objects, or by constructing meshes manually. Supports point and directional lights, anti-aliased lines, shadow mapping, transparent objects, translucent objects, multiple materials types, reflection, refraction, environment maps, multicore rendering, bloom, tone-mapping, and screen-space ambient occlusion.  "
  },
  {
    "id": 19068,
    "package_name": "rbit",
    "title": "Binary Indexed Tree",
    "description": "A simple implementation of Binary Indexed Tree by R. The BinaryIndexedTree class supports construction of Binary Indexed Tree from a vector, update of a value in the vector and query for the sum of a interval of the vector.",
    "version": "1.0.0",
    "maintainer": "Jialun Zhang <reatank@foxmail.com>",
    "author": "Jialun Zhang, Zhilan Fan, Hang Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rbit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbit Binary Indexed Tree A simple implementation of Binary Indexed Tree by R. The BinaryIndexedTree class supports construction of Binary Indexed Tree from a vector, update of a value in the vector and query for the sum of a interval of the vector.  "
  },
  {
    "id": 19100,
    "package_name": "rchroma",
    "title": "A Client for 'ChromaDB'",
    "description": "Client for 'ChromaDB', a vector database for storing and querying embeddings. This package provides a convenient interface to interact with the REST API of 'ChromaDB' <https://docs.trychroma.com>.",
    "version": "0.2.0",
    "maintainer": "David Schoch <david@schochastics.net>",
    "author": "David Schoch [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2952-4812>),\n  Christoph Sax [aut],\n  cynkra LLC [cph]",
    "url": "https://github.com/cynkra/rchroma",
    "bug_reports": "https://github.com/cynkra/rchroma/issues",
    "repository": "https://cran.r-project.org/package=rchroma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rchroma A Client for 'ChromaDB' Client for 'ChromaDB', a vector database for storing and querying embeddings. This package provides a convenient interface to interact with the REST API of 'ChromaDB' <https://docs.trychroma.com>.  "
  },
  {
    "id": 19148,
    "package_name": "rdocdump",
    "title": "Dump 'R' Package Source, Documentation, and Vignettes into One\nFile",
    "description": "Dump source code, documentation and vignettes of an 'R'\n    package into a single file. Supports installed packages, tar.gz\n    archives, and package source directories. If the package is not\n    installed, only its source is automatically downloaded from CRAN for\n    processing. The output is a single plain text file or a character\n    vector, which is useful to ingest complete package documentation and\n    source into a large language model (LLM) or pass it further to other\n    tools, such as 'ragnar' <https://github.com/tidyverse/ragnar> to\n    create a Retrieval-Augmented Generation (RAG) workflow.",
    "version": "0.1.1",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6690-5345>)",
    "url": "https://github.com/e-kotov/rdocdump,\nhttps://www.ekotov.pro/rdocdump/",
    "bug_reports": "https://github.com/e-kotov/rdocdump/issues",
    "repository": "https://cran.r-project.org/package=rdocdump",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdocdump Dump 'R' Package Source, Documentation, and Vignettes into One\nFile Dump source code, documentation and vignettes of an 'R'\n    package into a single file. Supports installed packages, tar.gz\n    archives, and package source directories. If the package is not\n    installed, only its source is automatically downloaded from CRAN for\n    processing. The output is a single plain text file or a character\n    vector, which is useful to ingest complete package documentation and\n    source into a large language model (LLM) or pass it further to other\n    tools, such as 'ragnar' <https://github.com/tidyverse/ragnar> to\n    create a Retrieval-Augmented Generation (RAG) workflow.  "
  },
  {
    "id": 19156,
    "package_name": "rdwd",
    "title": "Select and Download Climate Data from 'DWD' (German Weather\nService)",
    "description": "Handle climate data from the 'DWD' ('Deutscher Wetterdienst', see \n             <https://www.dwd.de/EN/climate_environment/cdc/cdc_node_en.html> for more information).\n             Choose observational time series from meteorological stations with 'selectDWD()'.\n             Find raster data from radar and interpolation according to <https://bookdown.org/brry/rdwd/raster-data.html>.\n             Download (multiple) data sets with progress bars and no re-downloads through 'dataDWD()'.\n             Read both tabular observational data and binary gridded datasets with 'readDWD()'.",
    "version": "1.9.3",
    "maintainer": "Berry Boessenkool <berry-b@gmx.de>",
    "author": "Berry Boessenkool [aut, cre]",
    "url": "https://bookdown.org/brry/rdwd/",
    "bug_reports": "https://github.com/brry/rdwd/issues",
    "repository": "https://cran.r-project.org/package=rdwd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdwd Select and Download Climate Data from 'DWD' (German Weather\nService) Handle climate data from the 'DWD' ('Deutscher Wetterdienst', see \n             <https://www.dwd.de/EN/climate_environment/cdc/cdc_node_en.html> for more information).\n             Choose observational time series from meteorological stations with 'selectDWD()'.\n             Find raster data from radar and interpolation according to <https://bookdown.org/brry/rdwd/raster-data.html>.\n             Download (multiple) data sets with progress bars and no re-downloads through 'dataDWD()'.\n             Read both tabular observational data and binary gridded datasets with 'readDWD()'.  "
  },
  {
    "id": 19246,
    "package_name": "redcas",
    "title": "An Interface to the Computer Algebra System 'REDUCE'",
    "description": "'REDUCE' is a portable general-purpose computer algebra system supporting scalar, vector, matrix and tensor algebra, symbolic differential and integral calculus, arbitrary precision numerical calculations and output in 'LaTeX' format. 'REDUCE' is based on 'Lisp' and is available on the two dialects 'Portable Standard Lisp' ('PSL') and 'Codemist Standard Lisp' ('CSL'). The 'redcas' package provides an interface for executing arbitrary 'REDUCE' code interactively from 'R', returning output as character vectors. 'R' code and 'REDUCE' code can be interspersed. It also provides a specialized function for calling the 'REDUCE' feature for solving systems of equations, returning the output as an 'R' object designed for the purpose. A further specialized function uses 'REDUCE' features to generate 'LaTeX' output and post-processes this for direct use in 'LaTeX' documents, e.g. using 'Sweave'.",
    "version": "0.1.1",
    "maintainer": "Martin Gregory <mairtin.macghreagoir@gmail.com>",
    "author": "Martin Gregory [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=redcas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redcas An Interface to the Computer Algebra System 'REDUCE' 'REDUCE' is a portable general-purpose computer algebra system supporting scalar, vector, matrix and tensor algebra, symbolic differential and integral calculus, arbitrary precision numerical calculations and output in 'LaTeX' format. 'REDUCE' is based on 'Lisp' and is available on the two dialects 'Portable Standard Lisp' ('PSL') and 'Codemist Standard Lisp' ('CSL'). The 'redcas' package provides an interface for executing arbitrary 'REDUCE' code interactively from 'R', returning output as character vectors. 'R' code and 'REDUCE' code can be interspersed. It also provides a specialized function for calling the 'REDUCE' feature for solving systems of equations, returning the output as an 'R' object designed for the purpose. A further specialized function uses 'REDUCE' features to generate 'LaTeX' output and post-processes this for direct use in 'LaTeX' documents, e.g. using 'Sweave'.  "
  },
  {
    "id": 19255,
    "package_name": "ref.ICAR",
    "title": "Objective Bayes Intrinsic Conditional Autoregressive Model for\nAreal Data",
    "description": "Implements an objective Bayes intrinsic conditional autoregressive \n    prior. This model provides an objective Bayesian approach for modeling spatially \n    correlated areal data using an intrinsic conditional autoregressive prior on a vector of \n    spatial random effects.",
    "version": "2.0.2",
    "maintainer": "Erica M. Porter <emporte@clemson.edu>",
    "author": "Erica M. Porter [aut, cre],\n  Matthew J. Keefe [aut],\n  Christopher T. Franck [aut],\n  Marco A.R. Ferreira [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ref.ICAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ref.ICAR Objective Bayes Intrinsic Conditional Autoregressive Model for\nAreal Data Implements an objective Bayes intrinsic conditional autoregressive \n    prior. This model provides an objective Bayesian approach for modeling spatially \n    correlated areal data using an intrinsic conditional autoregressive prior on a vector of \n    spatial random effects.  "
  },
  {
    "id": 19259,
    "package_name": "refinr",
    "title": "Cluster and Merge Similar Values Within a Character Vector",
    "description": "These functions take a character vector as input, identify and \n  cluster similar values, and then merge clusters together so their values \n  become identical. The functions are an implementation of the key collision \n  and ngram fingerprint algorithms from the open source tool Open Refine \n  <https://openrefine.org/>. More info on key collision and ngram fingerprint \n  can be found here <https://openrefine.org/docs/technical-reference/clustering-in-depth>.",
    "version": "0.3.3",
    "maintainer": "Chris Muir <chrismuirRVA@gmail.com>",
    "author": "Chris Muir [aut, cre]",
    "url": "https://github.com/ChrisMuir/refinr",
    "bug_reports": "https://github.com/ChrisMuir/refinr/issues",
    "repository": "https://cran.r-project.org/package=refinr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refinr Cluster and Merge Similar Values Within a Character Vector These functions take a character vector as input, identify and \n  cluster similar values, and then merge clusters together so their values \n  become identical. The functions are an implementation of the key collision \n  and ngram fingerprint algorithms from the open source tool Open Refine \n  <https://openrefine.org/>. More info on key collision and ngram fingerprint \n  can be found here <https://openrefine.org/docs/technical-reference/clustering-in-depth>.  "
  },
  {
    "id": 19264,
    "package_name": "refreg",
    "title": "Conditional Multivariate Reference Regions",
    "description": "An R package for estimating conditional multivariate reference regions. \n    The reference region is non parametrically estimated using a kernel density estimator.\n    Covariates effects on the multivariate response means vector and variance-covariance\n    matrix, thus on the region shape, are estimated by flexible additive predictors. \n    Continuous covariates non linear effects might be estimated using penalized splines smoothers.\n    Confidence intervals for the covariates estimated effects might be derived from\n    bootstrap resampling. Kernel density bandwidth can be estimated with different methods, including\n    a method that optimize the region coverage. Numerical, and graphical, summaries\n    can be obtained by the user in order to evaluate reference region performance with real data.\n    Full mathematical details can be found in <doi:10.1002/sim.9163> and <doi:10.1007/s00477-020-01901-1>.",
    "version": "0.1.1",
    "maintainer": "Oscar Lado-Baleato <oscarlado.baleato@usc.es>",
    "author": "Oscar Lado-Baleato [cre, aut],\n  Javier Roca-Pardinas [aut, ctb],\n  Carmen Cadarso-Suarez [aut, ths],\n  Gude Francisco [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=refreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refreg Conditional Multivariate Reference Regions An R package for estimating conditional multivariate reference regions. \n    The reference region is non parametrically estimated using a kernel density estimator.\n    Covariates effects on the multivariate response means vector and variance-covariance\n    matrix, thus on the region shape, are estimated by flexible additive predictors. \n    Continuous covariates non linear effects might be estimated using penalized splines smoothers.\n    Confidence intervals for the covariates estimated effects might be derived from\n    bootstrap resampling. Kernel density bandwidth can be estimated with different methods, including\n    a method that optimize the region coverage. Numerical, and graphical, summaries\n    can be obtained by the user in order to evaluate reference region performance with real data.\n    Full mathematical details can be found in <doi:10.1002/sim.9163> and <doi:10.1007/s00477-020-01901-1>.  "
  },
  {
    "id": 19278,
    "package_name": "regexSelect",
    "title": "Regular Expressions in 'shiny' Select Lists",
    "description": "'shiny' extension that adds regular expression filtering capabilities to \n  the choice vector of the select list.",
    "version": "1.0.0",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre]",
    "url": "https://github.com/yonicd/regexSelect",
    "bug_reports": "https://github.com/yonicd/regexSelect/issues",
    "repository": "https://cran.r-project.org/package=regexSelect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regexSelect Regular Expressions in 'shiny' Select Lists 'shiny' extension that adds regular expression filtering capabilities to \n  the choice vector of the select list.  "
  },
  {
    "id": 19280,
    "package_name": "regional",
    "title": "Intra- and Inter-Regional Similarity",
    "description": "Calculates intra-regional and inter-regional similarities based on user-provided\n    spatial vector objects (regions) and spatial raster objects (cells with values).\n    Implemented metrics include inhomogeneity, isolation \n    (Haralick and Shapiro (1985) <doi:10.1016/S0734-189X(85)90153-7>, \n    Jasiewicz et al. (2018) <doi:10.1016/j.cageo.2018.06.003>), \n    and distinction (Nowosad (2021) <doi:10.1080/13658816.2021.1893324>).",
    "version": "0.4.4",
    "maintainer": "Jakub Nowosad <nowosad.jakub@gmail.com>",
    "author": "Jakub Nowosad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1057-3721>)",
    "url": "https://jakubnowosad.com/regional/",
    "bug_reports": "https://github.com/Nowosad/regional/issues",
    "repository": "https://cran.r-project.org/package=regional",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regional Intra- and Inter-Regional Similarity Calculates intra-regional and inter-regional similarities based on user-provided\n    spatial vector objects (regions) and spatial raster objects (cells with values).\n    Implemented metrics include inhomogeneity, isolation \n    (Haralick and Shapiro (1985) <doi:10.1016/S0734-189X(85)90153-7>, \n    Jasiewicz et al. (2018) <doi:10.1016/j.cageo.2018.06.003>), \n    and distinction (Nowosad (2021) <doi:10.1080/13658816.2021.1893324>).  "
  },
  {
    "id": 19297,
    "package_name": "regressoR",
    "title": "Regression Data Analysis System",
    "description": "Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as linear regression, penalized regression, k-nearest neighbors, decision trees, ada boosting, extreme gradient boosting, random forest, neural networks, deep learning and support vector machines.",
    "version": "4.0.4",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "author": "Oldemar Rodriguez [aut, cre],\n  Andres Navarro D. [ctb, prg],\n  Diego Jimenez A. [ctb, prg],\n  Ariel Arroyo S. [ctb, prg],\n  Joseline Quiros M. [ctb, prg]",
    "url": "https://promidat.website/",
    "bug_reports": "https://github.com/PROMiDAT/predictoR/issues",
    "repository": "https://cran.r-project.org/package=regressoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regressoR Regression Data Analysis System Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as linear regression, penalized regression, k-nearest neighbors, decision trees, ada boosting, extreme gradient boosting, random forest, neural networks, deep learning and support vector machines.  "
  },
  {
    "id": 19311,
    "package_name": "relatable",
    "title": "Functions for Mapping Key-Value Pairs, Many-to-Many,\nOne-to-Many, and Many-to-One Relations",
    "description": "Functions to safely map from a vector of keys to a vector of values, determine properties of a given relation, or ensure a relation conforms to a given type, such as many-to-many, one-to-many, injective, surjective, or bijective. Permits default return values for use similar to a vectorised switch statement, as well as safely handling large vectors, NAs, and duplicate mappings.",
    "version": "1.0.0",
    "maintainer": "Dominic Jarkey <dominic.jarkey@gmail.com>",
    "author": "Dominic Jarkey [aut, cre]",
    "url": "https://github.com/domjarkey/relatable",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=relatable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "relatable Functions for Mapping Key-Value Pairs, Many-to-Many,\nOne-to-Many, and Many-to-One Relations Functions to safely map from a vector of keys to a vector of values, determine properties of a given relation, or ensure a relation conforms to a given type, such as many-to-many, one-to-many, injective, surjective, or bijective. Permits default return values for use similar to a vectorised switch statement, as well as safely handling large vectors, NAs, and duplicate mappings.  "
  },
  {
    "id": 19326,
    "package_name": "remap",
    "title": "Regional Spatial Modeling with Continuous Borders",
    "description": "Automatically creates separate regression models for different spatial \n    regions. The prediction surface is smoothed using a regional border smoothing \n    method. If regional models are continuous, the resulting prediction surface is \n    continuous across the spatial dimensions, even at region borders. Methodology \n    is described in Wagstaff and Bean (2023) <doi:10.32614/RJ-2023-004>.",
    "version": "0.3.2",
    "maintainer": "Jadon Wagstaff <jadonw@gmail.com>",
    "author": "Jadon Wagstaff [aut, cre],\n  Brennan Bean [aut]",
    "url": "https://github.com/jadonwagstaff/remap",
    "bug_reports": "https://github.com/jadonwagstaff/remap/issues",
    "repository": "https://cran.r-project.org/package=remap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "remap Regional Spatial Modeling with Continuous Borders Automatically creates separate regression models for different spatial \n    regions. The prediction surface is smoothed using a regional border smoothing \n    method. If regional models are continuous, the resulting prediction surface is \n    continuous across the spatial dimensions, even at region borders. Methodology \n    is described in Wagstaff and Bean (2023) <doi:10.32614/RJ-2023-004>.  "
  },
  {
    "id": 19327,
    "package_name": "rematch",
    "title": "Match Regular Expressions with a Nicer 'API'",
    "description": "A small wrapper on 'regexpr' to extract the matches and\n    captured groups from the match of a regular expression to a character\n    vector.",
    "version": "2.0.0",
    "maintainer": "Gabor Csardi <csardi.gabor@gmail.com>",
    "author": "Gabor Csardi",
    "url": "https://github.com/gaborcsardi/rematch",
    "bug_reports": "https://github.com/gaborcsardi/rematch/issues",
    "repository": "https://cran.r-project.org/package=rematch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rematch Match Regular Expressions with a Nicer 'API' A small wrapper on 'regexpr' to extract the matches and\n    captured groups from the match of a regular expression to a character\n    vector.  "
  },
  {
    "id": 19368,
    "package_name": "reproducible",
    "title": "Enhance Reproducibility of R Code",
    "description": "A collection of high-level, machine- and OS-independent tools\n    for making reproducible and reusable content in R.\n    The two workhorse functions are Cache() and prepInputs(). Cache()\n    allows for nested caching, is robust to environments and objects with\n    environments (like functions), and deals with some classes of \n    file-backed R objects e.g., from terra and raster packages. \n    Both functions have been developed to \n    be foundational components of data retrieval\n    and processing in continuous workflow situations. In both functions,\n    efforts are made to make the first and subsequent calls of functions have \n    the same result, but faster at subsequent times by way of checksums\n    and digesting. Several features are still under development, including\n    cloud storage of cached objects allowing for sharing between users. Several\n    advanced options are available, see ?reproducibleOptions().",
    "version": "2.1.2",
    "maintainer": "Eliot J B McIntire <eliot.mcintire@canada.ca>",
    "author": "Eliot J B McIntire [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6914-8316>),\n  Alex M Chubaty [aut] (ORCID: <https://orcid.org/0000-0001-7146-8135>),\n  Tati Micheletti [ctb] (ORCID: <https://orcid.org/0000-0003-4838-8342>),\n  Ceres Barros [ctb] (ORCID: <https://orcid.org/0000-0003-4036-977X>),\n  Ian Eddy [ctb] (ORCID: <https://orcid.org/0000-0001-7397-2116>),\n  His Majesty the King in Right of Canada, as represented by the Minister\n    of Natural Resources Canada [cph]",
    "url": "https://reproducible.predictiveecology.org,\nhttps://github.com/PredictiveEcology/reproducible",
    "bug_reports": "https://github.com/PredictiveEcology/reproducible/issues",
    "repository": "https://cran.r-project.org/package=reproducible",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reproducible Enhance Reproducibility of R Code A collection of high-level, machine- and OS-independent tools\n    for making reproducible and reusable content in R.\n    The two workhorse functions are Cache() and prepInputs(). Cache()\n    allows for nested caching, is robust to environments and objects with\n    environments (like functions), and deals with some classes of \n    file-backed R objects e.g., from terra and raster packages. \n    Both functions have been developed to \n    be foundational components of data retrieval\n    and processing in continuous workflow situations. In both functions,\n    efforts are made to make the first and subsequent calls of functions have \n    the same result, but faster at subsequent times by way of checksums\n    and digesting. Several features are still under development, including\n    cloud storage of cached objects allowing for sharing between users. Several\n    advanced options are available, see ?reproducibleOptions().  "
  },
  {
    "id": 19417,
    "package_name": "retistruct",
    "title": "Retinal Reconstruction Program",
    "description": "Reconstructs retinae by morphing a flat surface with cuts\n    (a dissected flat-mount retina) onto a curvilinear surface (the\n    standard retinal shape). It can estimate the position of a point\n    on the intact adult retina to within 8 degrees of arc (3.6% of\n    nasotemporal axis). The coordinates in reconstructed retinae can\n    be transformed to visuotopic coordinates. For more details see\n    Sterratt, D. C., Lyngholm, D., Willshaw, D. J. and Thompson, I. D.\n    (2013) <doi:10.1371/journal.pcbi.1002921>.",
    "version": "0.8.1",
    "maintainer": "David C. Sterratt <david.c.sterratt@ed.ac.uk>",
    "author": "David C. Sterratt [aut, cre, cph],\n  Daniel Lyngholm [aut, cph],\n  Jan Okul [aut, cph]",
    "url": "http://davidcsterratt.github.io/retistruct/",
    "bug_reports": "https://github.com/davidcsterratt/retistruct/issues",
    "repository": "https://cran.r-project.org/package=retistruct",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "retistruct Retinal Reconstruction Program Reconstructs retinae by morphing a flat surface with cuts\n    (a dissected flat-mount retina) onto a curvilinear surface (the\n    standard retinal shape). It can estimate the position of a point\n    on the intact adult retina to within 8 degrees of arc (3.6% of\n    nasotemporal axis). The coordinates in reconstructed retinae can\n    be transformed to visuotopic coordinates. For more details see\n    Sterratt, D. C., Lyngholm, D., Willshaw, D. J. and Thompson, I. D.\n    (2013) <doi:10.1371/journal.pcbi.1002921>.  "
  },
  {
    "id": 19449,
    "package_name": "rflsgen",
    "title": "Neutral Landscape Generator with Targets on Landscape Indices",
    "description": "Interface to the 'flsgen' neutral landscape generator <https://github.com/dimitri-justeau/flsgen>. It allows to\n  - Generate fractal terrain;\n  - Generate landscape structures satisfying user targets over landscape indices;\n  - Generate landscape raster from landscape structures.",
    "version": "1.2.2",
    "maintainer": "Dimitri Justeau-Allaire <dimitri.justeau@gmail.com>",
    "author": "Dimitri Justeau-Allaire, Gr\u00e9goire Blanchard, Thomas Ibanez, Xavier Lorca, Ghislain Vieilledent, Philippe Birnbaum",
    "url": "https://dimitri-justeau.github.io/rflsgen/,\nhttps://dimitri-justeau.github.io/rflsgen/",
    "bug_reports": "https://github.com/dimitri-justeau/rflsgen/issues",
    "repository": "https://cran.r-project.org/package=rflsgen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rflsgen Neutral Landscape Generator with Targets on Landscape Indices Interface to the 'flsgen' neutral landscape generator <https://github.com/dimitri-justeau/flsgen>. It allows to\n  - Generate fractal terrain;\n  - Generate landscape structures satisfying user targets over landscape indices;\n  - Generate landscape raster from landscape structures.  "
  },
  {
    "id": 19458,
    "package_name": "rgabriel",
    "title": "Gabriel Multiple Comparison Test and Plot the Confidence\nInterval on Barplot",
    "description": "Analyze multi-level one-way\n        experimental designs where there are unequal sample\n        sizes and population variance homogeneity can not be assumed.\n        To conduct the Gabriel test <doi:10.2307/2286265>, create two vectors: one for your \n        observations and one for the factor level of each observation. \n        The function, rgabriel, conduct the test and save the output as\n        a vector to input into the gabriel.plot function, which produces \n        a confidence interval plot for Multiple Comparison.",
    "version": "0.9",
    "maintainer": "Miao YU <yufreecas@gmail.com>",
    "author": "Miao YU [aut, cre] (ORCID: <https://orcid.org/0000-0002-2804-6014>),\n  Yihui XIE [aut] (ORCID: <https://orcid.org/0000-0003-0645-5666>)",
    "url": "https://github.com/yufree/rgabriel,\nhttp://yufree.github.io/rgabriel/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rgabriel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgabriel Gabriel Multiple Comparison Test and Plot the Confidence\nInterval on Barplot Analyze multi-level one-way\n        experimental designs where there are unequal sample\n        sizes and population variance homogeneity can not be assumed.\n        To conduct the Gabriel test <doi:10.2307/2286265>, create two vectors: one for your \n        observations and one for the factor level of each observation. \n        The function, rgabriel, conduct the test and save the output as\n        a vector to input into the gabriel.plot function, which produces \n        a confidence interval plot for Multiple Comparison.  "
  },
  {
    "id": 19465,
    "package_name": "rgeomorphon",
    "title": "A Lightweight Implementation of the 'Geomorphon' Algorithm",
    "description": "A lightweight implementation of the geomorphon terrain\n  form classification algorithm of Jasiewicz and Stepinski (2013)\n  <doi:10.1016/j.geomorph.2012.11.005> based largely on the \n  'GRASS GIS' 'r.geomorphon' module. This implementation employs \n  a novel algorithm written in C++ and 'RcppParallel'.",
    "version": "0.3.0",
    "maintainer": "Andrew Brown <brown.andrewg@gmail.com>",
    "author": "Andrew Brown [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4565-533X>)",
    "url": "https://github.com/brownag/rgeomorphon/,\nhttps://humus.rocks/rgeomorphon/",
    "bug_reports": "https://github.com/brownag/rgeomorphon/issues",
    "repository": "https://cran.r-project.org/package=rgeomorphon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgeomorphon A Lightweight Implementation of the 'Geomorphon' Algorithm A lightweight implementation of the geomorphon terrain\n  form classification algorithm of Jasiewicz and Stepinski (2013)\n  <doi:10.1016/j.geomorph.2012.11.005> based largely on the \n  'GRASS GIS' 'r.geomorphon' module. This implementation employs \n  a novel algorithm written in C++ and 'RcppParallel'.  "
  },
  {
    "id": 19482,
    "package_name": "rgugik",
    "title": "Search and Retrieve Spatial Data from 'GUGiK'",
    "description": "Automatic open data acquisition from resources of Polish Head Office\n    of Geodesy and Cartography ('G\u0142\u00f3wny Urz\u0105d Geodezji i Kartografii')\n    (<https://www.gov.pl/web/gugik>).\n    Available datasets include various types of numeric, raster and vector data,\n    such as orthophotomaps, digital elevation models (digital terrain models,\n    digital surface model, point clouds), state register of borders, spatial\n    databases, geometries of cadastral parcels, 3D models of buildings, and more.\n    It is also possible to geocode addresses or objects using the geocodePL_get()\n    function.",
    "version": "0.4.2",
    "maintainer": "Krzysztof Dyba <adres7@gmail.com>",
    "author": "Krzysztof Dyba [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8614-3816>),\n  Jakub Nowosad [aut] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\n  Maciej Ber\u0119sewicz [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8281-4301>),\n  Grzegorz Sapijaszko [ctb],\n  GUGiK [ctb] (source of the data)",
    "url": "https://kadyb.github.io/rgugik/, https://github.com/kadyb/rgugik",
    "bug_reports": "https://github.com/kadyb/rgugik/issues",
    "repository": "https://cran.r-project.org/package=rgugik",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgugik Search and Retrieve Spatial Data from 'GUGiK' Automatic open data acquisition from resources of Polish Head Office\n    of Geodesy and Cartography ('G\u0142\u00f3wny Urz\u0105d Geodezji i Kartografii')\n    (<https://www.gov.pl/web/gugik>).\n    Available datasets include various types of numeric, raster and vector data,\n    such as orthophotomaps, digital elevation models (digital terrain models,\n    digital surface model, point clouds), state register of borders, spatial\n    databases, geometries of cadastral parcels, 3D models of buildings, and more.\n    It is also possible to geocode addresses or objects using the geocodePL_get()\n    function.  "
  },
  {
    "id": 19510,
    "package_name": "riemtan",
    "title": "Riemannian Metrics for Symmetric Positive Definite Matrices",
    "description": "Implements various Riemannian metrics for symmetric positive definite matrices, including AIRM (Affine Invariant Riemannian Metric, <doi:10.1007/s11263-005-3222-z>), Log-Euclidean (<doi:10.1002/mrm.20965>), Euclidean, Log-Cholesky (<doi:10.1137/18M1221084>), and Bures-Wasserstein metrics (<doi:10.1016/j.exmath.2018.01.002>). Provides functions for computing logarithmic and exponential maps, vectorization, and statistical operations on the manifold of positive definite matrices.",
    "version": "0.2.5",
    "maintainer": "Nicolas Escobar <nescoba@iu.edu>",
    "author": "Nicolas Escobar [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-0800-5692>),\n  Jaroslaw Harezlak [ths]",
    "url": "https://nicoesve.github.io/riemtan/",
    "bug_reports": "https://github.com/nicoesve/riemtan/issues",
    "repository": "https://cran.r-project.org/package=riemtan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riemtan Riemannian Metrics for Symmetric Positive Definite Matrices Implements various Riemannian metrics for symmetric positive definite matrices, including AIRM (Affine Invariant Riemannian Metric, <doi:10.1007/s11263-005-3222-z>), Log-Euclidean (<doi:10.1002/mrm.20965>), Euclidean, Log-Cholesky (<doi:10.1137/18M1221084>), and Bures-Wasserstein metrics (<doi:10.1016/j.exmath.2018.01.002>). Provides functions for computing logarithmic and exponential maps, vectorization, and statistical operations on the manifold of positive definite matrices.  "
  },
  {
    "id": 19547,
    "package_name": "rivnet",
    "title": "Extract and Analyze Rivers from Elevation Data",
    "description": "Seamless extraction of river networks from digital \n    elevation models data. The package allows analysis of digital \n\televation models that can be either externally provided or\n\tdownloaded from open source repositories (thus interfacing\n\twith the 'elevatr' package). Extraction is performed via the \n\t'D8' flow direction algorithm of TauDEM (Terrain Analysis Using \n\tDigital Elevation Models), thus interfacing with the 'traudem' \n\tpackage. Resulting river networks are compatible with functions \n\tfrom the 'OCNet' package. See Carraro (2023) \n\t<doi:10.5194/hess-27-3733-2023> for a presentation of the package.",
    "version": "0.6.0",
    "maintainer": "Luca Carraro <luca.carraro@hotmail.it>",
    "author": "Luca Carraro [cre, aut],\n  University of Zurich [cph, fnd]",
    "url": "https://lucarraro.github.io/rivnet/",
    "bug_reports": "https://github.com/lucarraro/rivnet/issues",
    "repository": "https://cran.r-project.org/package=rivnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rivnet Extract and Analyze Rivers from Elevation Data Seamless extraction of river networks from digital \n    elevation models data. The package allows analysis of digital \n\televation models that can be either externally provided or\n\tdownloaded from open source repositories (thus interfacing\n\twith the 'elevatr' package). Extraction is performed via the \n\t'D8' flow direction algorithm of TauDEM (Terrain Analysis Using \n\tDigital Elevation Models), thus interfacing with the 'traudem' \n\tpackage. Resulting river networks are compatible with functions \n\tfrom the 'OCNet' package. See Carraro (2023) \n\t<doi:10.5194/hess-27-3733-2023> for a presentation of the package.  "
  },
  {
    "id": 19564,
    "package_name": "rkriging",
    "title": "Kriging Modeling",
    "description": "An 'Eigen'-based computationally efficient 'C++' implementation for fitting various kriging models to data. This research is supported by U.S. National Science Foundation grant DMS-2310637.",
    "version": "1.0.2",
    "maintainer": "Chaofan Huang <10billhuang01@gmail.com>",
    "author": "Chaofan Huang [aut, cre],\n  V. Roshan Joseph [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rkriging",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rkriging Kriging Modeling An 'Eigen'-based computationally efficient 'C++' implementation for fitting various kriging models to data. This research is supported by U.S. National Science Foundation grant DMS-2310637.  "
  },
  {
    "id": 19570,
    "package_name": "rle",
    "title": "Common Functions for Run-Length Encoded Vectors",
    "description": "Common 'base' and 'stats' methods for 'rle' objects, aiming to make it possible to treat them transparently as vectors.",
    "version": "0.10.0",
    "maintainer": "Pavel N. Krivitsky <pavel@statnet.org>",
    "author": "Pavel N. Krivitsky [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9101-3362>)",
    "url": "",
    "bug_reports": "https://github.com/statnet/rle/issues",
    "repository": "https://cran.r-project.org/package=rle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rle Common Functions for Run-Length Encoded Vectors Common 'base' and 'stats' methods for 'rle' objects, aiming to make it possible to treat them transparently as vectors.  "
  },
  {
    "id": 19571,
    "package_name": "rleafmap",
    "title": "Interactive Maps with R and Leaflet",
    "description": "Display spatial data with interactive maps powered by the open-\n    source JavaScript library 'Leaflet' (see <https://leafletjs.com/>). Maps can be\n    rendered in a web browser or displayed in the HTML viewer pane of 'RStudio'.\n    This package is designed to be easy to use and can create complex maps with\n    vector and raster data, web served map tiles and interface elements.",
    "version": "0.2.1",
    "maintainer": "Francois Keck <francois.keck@gmail.com>",
    "author": "Francois Keck <francois.keck@gmail.com>",
    "url": "http://www.francoiskeck.fr/rleafmap/,\nhttps://github.com/fkeck/rleafmap",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rleafmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rleafmap Interactive Maps with R and Leaflet Display spatial data with interactive maps powered by the open-\n    source JavaScript library 'Leaflet' (see <https://leafletjs.com/>). Maps can be\n    rendered in a web browser or displayed in the HTML viewer pane of 'RStudio'.\n    This package is designed to be easy to use and can create complex maps with\n    vector and raster data, web served map tiles and interface elements.  "
  },
  {
    "id": 19591,
    "package_name": "rmapzen",
    "title": "Client for 'Mapzen' and Related Map APIs",
    "description": "Provides an interface to 'Mapzen'-based APIs (including \n    geocode.earth, Nextzen, and NYC GeoSearch) for geographic search \n    and geocoding, isochrone calculation, and vector data to draw map tiles. \n    See <https://www.mapzen.com/documentation/> for more information. The original \n    Mapzen has gone out of business, but 'rmapzen' can be set up to work with \n    any provider who implements the Mapzen API. ",
    "version": "0.5.1",
    "maintainer": "Tarak Shah <tarak.shah@gmail.com>",
    "author": "Tarak Shah [aut, cre],\n  Daniel Possenriede [ctb]",
    "url": "https://tarakc02.github.io/rmapzen/",
    "bug_reports": "https://github.com/tarakc02/rmapzen/issues",
    "repository": "https://cran.r-project.org/package=rmapzen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmapzen Client for 'Mapzen' and Related Map APIs Provides an interface to 'Mapzen'-based APIs (including \n    geocode.earth, Nextzen, and NYC GeoSearch) for geographic search \n    and geocoding, isochrone calculation, and vector data to draw map tiles. \n    See <https://www.mapzen.com/documentation/> for more information. The original \n    Mapzen has gone out of business, but 'rmapzen' can be set up to work with \n    any provider who implements the Mapzen API.   "
  },
  {
    "id": 19597,
    "package_name": "rmcmc",
    "title": "Robust Markov Chain Monte Carlo Methods",
    "description": "Functions for simulating Markov chains using the Barker proposal\n    to compute Markov chain Monte Carlo (MCMC) estimates of expectations with\n    respect to a target distribution on a real-valued vector space. The Barker\n    proposal, described in Livingstone and Zanella (2022)\n    <doi:10.1111/rssb.12482>, is a gradient-based MCMC algorithm inspired by the\n    Barker accept-reject rule. It combines the robustness of simpler MCMC\n    schemes, such as random-walk Metropolis, with the efficiency of\n    gradient-based methods, such as the Metropolis adjusted Langevin algorithm. \n    The key function provided by the package is sample_chain(), which allows\n    sampling a Markov chain with a specified target distribution as its\n    stationary distribution. The chain is sampled by generating proposals and\n    accepting or rejecting them using a Metropolis-Hasting acceptance rule.\n    During an initial warm-up stage, the parameters of the proposal distribution\n    can be adapted, with adapters available to both: tune the scale of the\n    proposals by coercing the average acceptance rate to a target value; tune\n    the shape of the proposals to match covariance estimates under the target \n    distribution. As well as the default Barker proposal, the package also\n    provides implementations of alternative proposal distributions, such as\n    (Gaussian) random walk and Langevin proposals. Optionally, if 'BridgeStan's\n    R interface <https://roualdes.us/bridgestan/latest/languages/r.html>,\n    available on GitHub <https://github.com/roualdes/bridgestan>, is installed,\n    then 'BridgeStan' can be used to specify the target distribution to sample\n    from.",
    "version": "0.1.2",
    "maintainer": "Matthew M. Graham <m.graham@ucl.ac.uk>",
    "author": "Matthew M. Graham [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9104-7960>),\n  Samuel Livingstone [aut] (ORCID:\n    <https://orcid.org/0000-0002-7277-086X>),\n  University College London [cph],\n  Engineering and Physical Sciences Research Council [fnd]",
    "url": "https://github.com/UCL/rmcmc, http://github-pages.ucl.ac.uk/rmcmc/",
    "bug_reports": "https://github.com/UCL/rmcmc/issues",
    "repository": "https://cran.r-project.org/package=rmcmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmcmc Robust Markov Chain Monte Carlo Methods Functions for simulating Markov chains using the Barker proposal\n    to compute Markov chain Monte Carlo (MCMC) estimates of expectations with\n    respect to a target distribution on a real-valued vector space. The Barker\n    proposal, described in Livingstone and Zanella (2022)\n    <doi:10.1111/rssb.12482>, is a gradient-based MCMC algorithm inspired by the\n    Barker accept-reject rule. It combines the robustness of simpler MCMC\n    schemes, such as random-walk Metropolis, with the efficiency of\n    gradient-based methods, such as the Metropolis adjusted Langevin algorithm. \n    The key function provided by the package is sample_chain(), which allows\n    sampling a Markov chain with a specified target distribution as its\n    stationary distribution. The chain is sampled by generating proposals and\n    accepting or rejecting them using a Metropolis-Hasting acceptance rule.\n    During an initial warm-up stage, the parameters of the proposal distribution\n    can be adapted, with adapters available to both: tune the scale of the\n    proposals by coercing the average acceptance rate to a target value; tune\n    the shape of the proposals to match covariance estimates under the target \n    distribution. As well as the default Barker proposal, the package also\n    provides implementations of alternative proposal distributions, such as\n    (Gaussian) random walk and Langevin proposals. Optionally, if 'BridgeStan's\n    R interface <https://roualdes.us/bridgestan/latest/languages/r.html>,\n    available on GitHub <https://github.com/roualdes/bridgestan>, is installed,\n    then 'BridgeStan' can be used to specify the target distribution to sample\n    from.  "
  },
  {
    "id": 19607,
    "package_name": "rmerec",
    "title": "MEREC - Method Based on the Removal Effects of Criteria",
    "description": "Implementation of the MEthod based on the Removal Effects of Criteria - MEREC- a new objective weighting method for determining criteria weights for Multiple Criteria Decision Making problems, created by Mehdi Keshavarz-Ghorabaee (2021) <doi:10.3390/sym13040525>. Given a decision matrix, the function return the Merec\u00b4s weight vector and all intermediate matrix/vectors used to calculate it.",
    "version": "0.1.1",
    "maintainer": "Lucas Sebasti\u00e3o de Paula <lucas@ime.eb.br>",
    "author": "Lucas Sebasti\u00e3o de Paula [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2522-6538>),\n  Bernardo Silva [ctb] (ORCID: <https://orcid.org/0000-0003-1466-3393>),\n  Marcos Santos [ctb] (ORCID: <https://orcid.org/0000-0003-1533-5535>)",
    "url": "https://github.com/lucassp/rmerec",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rmerec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmerec MEREC - Method Based on the Removal Effects of Criteria Implementation of the MEthod based on the Removal Effects of Criteria - MEREC- a new objective weighting method for determining criteria weights for Multiple Criteria Decision Making problems, created by Mehdi Keshavarz-Ghorabaee (2021) <doi:10.3390/sym13040525>. Given a decision matrix, the function return the Merec\u00b4s weight vector and all intermediate matrix/vectors used to calculate it.  "
  },
  {
    "id": 19612,
    "package_name": "rminer",
    "title": "Machine Learning Classification and Regression Methods",
    "description": "Facilitates the use of machine learning algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.5.0 improved mparheuristic function (new hyperparameter heuristics); 1.4.9 / 1.4.8 improved help, several warning and error code fixes (more stable version, all examples run correctly); 1.4.7 - improved Importance function and examples, minor error fixes; 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new \"lssvm\" model and improved mparheuristic() function; 1.4.2 new \"NMAE\" metric, \"xgboost\" and \"cv.glmnet\" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version.",
    "version": "1.5.0",
    "maintainer": "Paulo Cortez <pcortez@dsi.uminho.pt>",
    "author": "Paulo Cortez [aut, cre]",
    "url": "https://cran.r-project.org/package=rminer",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rminer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rminer Machine Learning Classification and Regression Methods Facilitates the use of machine learning algorithms in classification and regression (including time series forecasting) tasks by presenting a short and coherent set of functions. Versions: 1.5.0 improved mparheuristic function (new hyperparameter heuristics); 1.4.9 / 1.4.8 improved help, several warning and error code fixes (more stable version, all examples run correctly); 1.4.7 - improved Importance function and examples, minor error fixes; 1.4.6 / 1.4.5 / 1.4.4 new automated machine learning (AutoML) and ensembles, via improved fit(), mining() and mparheuristic() functions, and new categorical preprocessing, via improved delevels() function; 1.4.3 new metrics (e.g., macro precision, explained variance), new \"lssvm\" model and improved mparheuristic() function; 1.4.2 new \"NMAE\" metric, \"xgboost\" and \"cv.glmnet\" models (16 classification and 18 regression models); 1.4.1 new tutorial and more robust version; 1.4 - new classification and regression models, with a total of 14 classification and 15 regression methods, including: Decision Trees, Neural Networks, Support Vector Machines, Random Forests, Bagging and Boosting; 1.3 and 1.3.1 - new classification and regression metrics; 1.2 - new input importance methods via improved Importance() function; 1.0 - first version.  "
  },
  {
    "id": 19617,
    "package_name": "rmonocypher",
    "title": "Easy Encryption of R Objects using Strong Modern Cryptography",
    "description": "Encrypt R objects to a raw vector or file using modern cryptographic\n    techniques.  Password-based key derivation is with 'Argon2' (<https://en.wikipedia.org/wiki/Argon2>).\n    Objects are serialized and then encrypted using 'XChaCha20-Poly1305' (<https://en.wikipedia.org/wiki/ChaCha20-Poly1305>)\n    which follows RFC 8439 for authenticated encryption (<https://en.wikipedia.org/wiki/Authenticated_encryption>).\n    Cryptographic functions are provided by the included 'monocypher' 'C' library (<https://monocypher.org>).",
    "version": "0.1.8",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph],\n  Loup Vaillant [aut, cph] (Author and copyright holder of the included\n    'monocyper' library),\n  Michael Savage [aut, cph] (Author and copyright holder of the included\n    'monocyper' library),\n  Fabio Scotomi [aut, cph] (Author and copyright holder of the included\n    'monocyper' library)",
    "url": "https://github.com/coolbutuseless/rmonocypher",
    "bug_reports": "https://github.com/coolbutuseless/rmonocypher/issues",
    "repository": "https://cran.r-project.org/package=rmonocypher",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmonocypher Easy Encryption of R Objects using Strong Modern Cryptography Encrypt R objects to a raw vector or file using modern cryptographic\n    techniques.  Password-based key derivation is with 'Argon2' (<https://en.wikipedia.org/wiki/Argon2>).\n    Objects are serialized and then encrypted using 'XChaCha20-Poly1305' (<https://en.wikipedia.org/wiki/ChaCha20-Poly1305>)\n    which follows RFC 8439 for authenticated encryption (<https://en.wikipedia.org/wiki/Authenticated_encryption>).\n    Cryptographic functions are provided by the included 'monocypher' 'C' library (<https://monocypher.org>).  "
  },
  {
    "id": 19625,
    "package_name": "rmsfuns",
    "title": "Quickly View Data Frames in 'Excel', Build Folder Paths and\nCreate Date Vectors",
    "description": "Contains several useful navigation helper functions, including easily building\n    folder paths, quick viewing dataframes in 'Excel', creating date vectors and changing the\n    console prompt to reflect time.",
    "version": "1.0.0.1",
    "maintainer": "Nico Katzke <nfkatzke@gmail.com>",
    "author": "Nico Katzke [aut, cre]",
    "url": "https://rmsfuns.nfkatzke.com",
    "bug_reports": "https://github.com/nicktz/rmsfuns/issues",
    "repository": "https://cran.r-project.org/package=rmsfuns",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmsfuns Quickly View Data Frames in 'Excel', Build Folder Paths and\nCreate Date Vectors Contains several useful navigation helper functions, including easily building\n    folder paths, quick viewing dataframes in 'Excel', creating date vectors and changing the\n    console prompt to reflect time.  "
  },
  {
    "id": 19648,
    "package_name": "roads",
    "title": "Road Network Projection",
    "description": "Iterative least cost path and minimum spanning tree methods for projecting \n    forest road networks. The methods connect a set of target points to an existing \n    road network using 'igraph' <https://igraph.org> to identify least cost routes.\n    The cost of constructing a road segment between adjacent pixels is determined\n    by a user supplied weight raster and a weight function; options include the\n    average of adjacent weight raster values, and a function of the elevation \n    differences between adjacent cells that penalizes steep grades. These road\n    network projection methods are intended for integration into R workflows and \n    modelling frameworks used for forecasting forest change, and can be applied \n    over multiple time-steps without rebuilding a graph at each time-step.",
    "version": "1.2.0",
    "maintainer": "Sarah Endicott <sarah.endicott@ec.gc.ca>",
    "author": "Sarah Endicott [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9644-5343>),\n  Kyle Lochhead [aut],\n  Josie Hughes [aut],\n  Patrick Kirby [aut],\n  Her Majesty the Queen in Right of Canada as represented by the Minister\n    of the Environment [cph] (Copyright holder for included functions\n    buildSimList, getLandingsFromTarget, pathsToLines, plotRoads,\n    projectRoads, rasterizeLine, rasterToLineSegments),\n  Province of British Columbia [cph] (Copyright holder for included\n    functions getGraph, lcpList, mstList, shortestPaths,\n    getClosestRoad, buildSnapRoads)",
    "url": "https://github.com/LandSciTech/roads,\nhttps://landscitech.github.io/roads/",
    "bug_reports": "https://github.com/LandSciTech/roads/issues",
    "repository": "https://cran.r-project.org/package=roads",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roads Road Network Projection Iterative least cost path and minimum spanning tree methods for projecting \n    forest road networks. The methods connect a set of target points to an existing \n    road network using 'igraph' <https://igraph.org> to identify least cost routes.\n    The cost of constructing a road segment between adjacent pixels is determined\n    by a user supplied weight raster and a weight function; options include the\n    average of adjacent weight raster values, and a function of the elevation \n    differences between adjacent cells that penalizes steep grades. These road\n    network projection methods are intended for integration into R workflows and \n    modelling frameworks used for forecasting forest change, and can be applied \n    over multiple time-steps without rebuilding a graph at each time-step.  "
  },
  {
    "id": 19711,
    "package_name": "roclab",
    "title": "ROC-Optimizing Binary Classifiers",
    "description": "Implements ROC (Receiver Operating Characteristic)\u2013Optimizing \n    Binary Classifiers, supporting both linear and kernel models. Both model \n    types provide a variety of surrogate loss functions. In addition, linear \n    models offer multiple regularization penalties, whereas kernel models \n    support a range of kernel functions. Scalability for large datasets is \n    achieved through approximation-based options, which accelerate training \n    and make fitting feasible on large data. Utilities are provided for model \n    training, prediction, and cross-validation. The implementation builds on \n    the ROC-Optimizing Support Vector Machines. For more information, see \n    Hern\u00e0ndez-Orallo, Jos\u00e9, et al. (2004) <doi:10.1145/1046456.1046489>, \n    presented in the ROC Analysis in AI Workshop (ROCAI-2004).",
    "version": "0.1.4",
    "maintainer": "Gimun Bae <gimunbae0201@gmail.com>",
    "author": "Gimun Bae [aut, cre],\n  Seung Jun Shin [aut]",
    "url": "https://github.com/gimunBae/roclab",
    "bug_reports": "https://github.com/gimunBae/roclab/issues",
    "repository": "https://cran.r-project.org/package=roclab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roclab ROC-Optimizing Binary Classifiers Implements ROC (Receiver Operating Characteristic)\u2013Optimizing \n    Binary Classifiers, supporting both linear and kernel models. Both model \n    types provide a variety of surrogate loss functions. In addition, linear \n    models offer multiple regularization penalties, whereas kernel models \n    support a range of kernel functions. Scalability for large datasets is \n    achieved through approximation-based options, which accelerate training \n    and make fitting feasible on large data. Utilities are provided for model \n    training, prediction, and cross-validation. The implementation builds on \n    the ROC-Optimizing Support Vector Machines. For more information, see \n    Hern\u00e0ndez-Orallo, Jos\u00e9, et al. (2004) <doi:10.1145/1046456.1046489>, \n    presented in the ROC Analysis in AI Workshop (ROCAI-2004).  "
  },
  {
    "id": 19741,
    "package_name": "roperators",
    "title": "Additional Operators to Help you Write Cleaner R Code",
    "description": "Provides string arithmetic, reassignment operators, logical operators\n  that handle missing values, and extra logical operators such as floating point\n  equality and all or nothing. The intent is to allow R users to write code that\n  is easier to read, write, and maintain while providing a friendlier experience\n  to new R users from other language backgrounds (such as 'Python') who are used\n  to concepts such as x += 1 and 'foo' + 'bar'.\n  Includes operators for not in, easy floating point comparisons, === equivalent, and SQL-like \n  like operations (), etc. \n  We also added in some extra helper functions, such as OS checks, pasting\n  in Oxford comma format, and functions to get the first, last, nth, or most common \n  element of a vector or word in a string. ",
    "version": "1.3.14",
    "maintainer": "Ben Wiseman <benjamin.wiseman@kornferry.com>",
    "author": "Ben Wiseman [cre, aut, ccp],\n  Steven Nydick [aut, ccp] (ORCID:\n    <https://orcid.org/0000-0002-2908-1188>),\n  Jeff Jones [aut, led]",
    "url": "https://benwiseman.github.io/roperators/,\nhttps://github.com/BenWiseman/roperators",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=roperators",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roperators Additional Operators to Help you Write Cleaner R Code Provides string arithmetic, reassignment operators, logical operators\n  that handle missing values, and extra logical operators such as floating point\n  equality and all or nothing. The intent is to allow R users to write code that\n  is easier to read, write, and maintain while providing a friendlier experience\n  to new R users from other language backgrounds (such as 'Python') who are used\n  to concepts such as x += 1 and 'foo' + 'bar'.\n  Includes operators for not in, easy floating point comparisons, === equivalent, and SQL-like \n  like operations (), etc. \n  We also added in some extra helper functions, such as OS checks, pasting\n  in Oxford comma format, and functions to get the first, last, nth, or most common \n  element of a vector or word in a string.   "
  },
  {
    "id": 19749,
    "package_name": "rosm",
    "title": "Plot Raster Map Tiles from Open Street Map and Other Sources",
    "description": "Download and plot Open Street Map <https://www.openstreetmap.org/>,\n    Bing Maps <https://www.bing.com/maps> and other tiled map sources. Use to create \n    basemaps quickly and add hillshade to vector-based maps.",
    "version": "0.3.0",
    "maintainer": "Dewey Dunnington <dewey@fishandwhistle.net>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Timoth\u00e9e Giraud [ctb]",
    "url": "https://github.com/paleolimbot/rosm",
    "bug_reports": "https://github.com/paleolimbot/rosm/issues",
    "repository": "https://cran.r-project.org/package=rosm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rosm Plot Raster Map Tiles from Open Street Map and Other Sources Download and plot Open Street Map <https://www.openstreetmap.org/>,\n    Bing Maps <https://www.bing.com/maps> and other tiled map sources. Use to create \n    basemaps quickly and add hillshade to vector-based maps.  "
  },
  {
    "id": 19794,
    "package_name": "rpostgis",
    "title": "R Interface to a 'PostGIS' Database",
    "description": "Provides an interface between R and 'PostGIS'-enabled\n    'PostgreSQL' databases to transparently transfer spatial\n    data. Both vector (points, lines, polygons) and raster data are\n    supported in read and write modes. Also provides convenience\n    functions to execute common procedures in 'PostgreSQL/PostGIS'.",
    "version": "1.6.0",
    "maintainer": "Adrian Cidre Gonzalez <adrian.cidre@gmail.com>",
    "author": "Adrian Cidre Gonzalez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3310-3052>),\n  Mathieu Basille [aut] (ORCID: <https://orcid.org/0000-0001-9366-7127>),\n  David Bucklin [aut]",
    "url": "https://cidree.github.io/rpostgis/,\nhttps://github.com/Cidree/rpostgis",
    "bug_reports": "https://github.com/Cidree/rpostgis/issues",
    "repository": "https://cran.r-project.org/package=rpostgis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpostgis R Interface to a 'PostGIS' Database Provides an interface between R and 'PostGIS'-enabled\n    'PostgreSQL' databases to transparently transfer spatial\n    data. Both vector (points, lines, polygons) and raster data are\n    supported in read and write modes. Also provides convenience\n    functions to execute common procedures in 'PostgreSQL/PostGIS'.  "
  },
  {
    "id": 19813,
    "package_name": "rrandvec",
    "title": "Generate Random Vectors Whose Components Sum Up to One",
    "description": "A single method implementing multiple approaches to generate pseudo-random vectors whose components sum up to one (see, e.g., Maziero (2015) <doi:10.1007/s13538-015-0337-8>). The components of such vectors can for example be used for weighting objectives when reducing multi-objective optimisation problems to a single-objective problem in the socalled weighted sum scalarisation approach.",
    "version": "1.0.0",
    "maintainer": "Jakob Bossek <j.bossek@gmail.com>",
    "author": "Jakob Bossek [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4121-4668>)",
    "url": "https://jakobbossek.github.io/rrandvec/,\nhttps://github.com/jakobbossek/rrandvec",
    "bug_reports": "https://github.com/jakobbossek/rrandvec/issues",
    "repository": "https://cran.r-project.org/package=rrandvec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rrandvec Generate Random Vectors Whose Components Sum Up to One A single method implementing multiple approaches to generate pseudo-random vectors whose components sum up to one (see, e.g., Maziero (2015) <doi:10.1007/s13538-015-0337-8>). The components of such vectors can for example be used for weighting objectives when reducing multi-objective optimisation problems to a single-objective problem in the socalled weighted sum scalarisation approach.  "
  },
  {
    "id": 19865,
    "package_name": "rsparse",
    "title": "Statistical Learning on Sparse Matrices",
    "description": "Implements many algorithms for statistical learning on \n  sparse matrices - matrix factorizations, matrix completion, \n  elastic net regressions, factorization machines. \n  Also 'rsparse' enhances 'Matrix' package by providing methods for \n  multithreaded <sparse, dense> matrix products and native slicing of \n  the sparse matrices in Compressed Sparse Row (CSR) format.\n  List of the algorithms for regression problems:\n  1) Elastic Net regression via Follow The Proximally-Regularized Leader (FTRL) \n  Stochastic Gradient Descent (SGD), as per McMahan et al(, <doi:10.1145/2487575.2488200>)\n  2) Factorization Machines via SGD, as per Rendle (2010, <doi:10.1109/ICDM.2010.127>)\n  List of algorithms for matrix factorization and matrix completion:\n  1) Weighted Regularized Matrix Factorization (WRMF) via Alternating Least \n  Squares (ALS) - paper by Hu, Koren, Volinsky (2008, <doi:10.1109/ICDM.2008.22>)\n  2) Maximum-Margin Matrix Factorization via ALS, paper by Rennie, Srebro \n  (2005, <doi:10.1145/1102351.1102441>)\n  3) Fast Truncated Singular Value Decomposition (SVD), Soft-Thresholded SVD, \n  Soft-Impute matrix completion via ALS - paper by Hastie, Mazumder \n  et al. (2014, <doi:10.48550/arXiv.1410.2596>)\n  4) Linear-Flow matrix factorization, from 'Practical linear models for \n  large-scale one-class collaborative filtering' by Sedhain, Bui, Kawale et al \n  (2016, ISBN:978-1-57735-770-4)\n  5) GlobalVectors (GloVe) matrix factorization via SGD, paper by Pennington, \n  Socher, Manning (2014, <https://aclanthology.org/D14-1162/>)\n  Package is reasonably fast and memory efficient - it allows to work with large\n  datasets - millions of rows and millions of columns. This is particularly useful \n  for practitioners working on recommender systems.",
    "version": "0.5.3",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "author": "Dmitriy Selivanov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5413-1506>),\n  David Cortes [ctb],\n  Drew Schmidt [ctb] (configure script for BLAS, LAPACK detection),\n  Wei-Chen Chen [ctb] (configure script and work on linking to float\n    package)",
    "url": "https://github.com/dselivanov/rsparse",
    "bug_reports": "https://github.com/dselivanov/rsparse/issues",
    "repository": "https://cran.r-project.org/package=rsparse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsparse Statistical Learning on Sparse Matrices Implements many algorithms for statistical learning on \n  sparse matrices - matrix factorizations, matrix completion, \n  elastic net regressions, factorization machines. \n  Also 'rsparse' enhances 'Matrix' package by providing methods for \n  multithreaded <sparse, dense> matrix products and native slicing of \n  the sparse matrices in Compressed Sparse Row (CSR) format.\n  List of the algorithms for regression problems:\n  1) Elastic Net regression via Follow The Proximally-Regularized Leader (FTRL) \n  Stochastic Gradient Descent (SGD), as per McMahan et al(, <doi:10.1145/2487575.2488200>)\n  2) Factorization Machines via SGD, as per Rendle (2010, <doi:10.1109/ICDM.2010.127>)\n  List of algorithms for matrix factorization and matrix completion:\n  1) Weighted Regularized Matrix Factorization (WRMF) via Alternating Least \n  Squares (ALS) - paper by Hu, Koren, Volinsky (2008, <doi:10.1109/ICDM.2008.22>)\n  2) Maximum-Margin Matrix Factorization via ALS, paper by Rennie, Srebro \n  (2005, <doi:10.1145/1102351.1102441>)\n  3) Fast Truncated Singular Value Decomposition (SVD), Soft-Thresholded SVD, \n  Soft-Impute matrix completion via ALS - paper by Hastie, Mazumder \n  et al. (2014, <doi:10.48550/arXiv.1410.2596>)\n  4) Linear-Flow matrix factorization, from 'Practical linear models for \n  large-scale one-class collaborative filtering' by Sedhain, Bui, Kawale et al \n  (2016, ISBN:978-1-57735-770-4)\n  5) GlobalVectors (GloVe) matrix factorization via SGD, paper by Pennington, \n  Socher, Manning (2014, <https://aclanthology.org/D14-1162/>)\n  Package is reasonably fast and memory efficient - it allows to work with large\n  datasets - millions of rows and millions of columns. This is particularly useful \n  for practitioners working on recommender systems.  "
  },
  {
    "id": 19921,
    "package_name": "rtop",
    "title": "Interpolation of Data with Variable Spatial Support",
    "description": "Data with irregular spatial support, such as runoff related data or data from administrative units, can with 'rtop' be interpolated to locations without observations with the top-kriging method. A description of the package is given by Sk\u00f8ien et al (2014) <doi:10.1016/j.cageo.2014.02.009>.",
    "version": "0.6-17",
    "maintainer": "Jon Olav Sk\u00f8ien <jon.skoien@gmail.com>",
    "author": "Jon Olav Sk\u00f8ien [aut, cre],\n  Qingyun Duan [ctb] (For the original FORTRAN code of SCE-UA, translated\n    and modified to R in this package.)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rtop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtop Interpolation of Data with Variable Spatial Support Data with irregular spatial support, such as runoff related data or data from administrative units, can with 'rtop' be interpolated to locations without observations with the top-kriging method. A description of the package is given by Sk\u00f8ien et al (2014) <doi:10.1016/j.cageo.2014.02.009>.  "
  },
  {
    "id": 19927,
    "package_name": "rts",
    "title": "Raster Time Series Analysis",
    "description": "This framework aims to provide classes and methods for manipulating and processing of raster time series data (e.g. a time series of satellite images).",
    "version": "1.1-14",
    "maintainer": "Babak Naimi <naimi.b@gmail.com>",
    "author": "Babak Naimi",
    "url": "https://r-gis.net/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rts Raster Time Series Analysis This framework aims to provide classes and methods for manipulating and processing of raster time series data (e.g. a time series of satellite images).  "
  },
  {
    "id": 19946,
    "package_name": "runner",
    "title": "Running Operations for Vectors",
    "description": "Lightweight library for rolling windows operations. Package enables\n  full control over the window length, window lag and a time indices. With a runner \n  one can apply any R function on a rolling windows. The package eases work with \n  equally and unequally spaced time series.",
    "version": "0.4.5",
    "maintainer": "Dawid Ka\u0142\u0119dkowski <dawid.kaledkowski@gmail.com>",
    "author": "Dawid Ka\u0142\u0119dkowski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9533-457X>)",
    "url": "",
    "bug_reports": "https://github.com/gogonzo/runner/issues",
    "repository": "https://cran.r-project.org/package=runner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "runner Running Operations for Vectors Lightweight library for rolling windows operations. Package enables\n  full control over the window length, window lag and a time indices. With a runner \n  one can apply any R function on a rolling windows. The package eases work with \n  equally and unequally spaced time series.  "
  },
  {
    "id": 19957,
    "package_name": "rvMF",
    "title": "Fast Generation of von Mises-Fisher Distributed Pseudo-Random\nVectors",
    "description": "Generates pseudo-random vectors that follow an arbitrary von Mises-Fisher distribution on a sphere. This method is fast and efficient when generating a large number of pseudo-random vectors. Functions to generate random variates and compute density for the distribution of an inner product between von Mises-Fisher random vector and its mean direction are also provided. Details are in Kang and Oh (2024) <doi:10.1007/s11222-024-10419-3>.",
    "version": "0.1.0",
    "maintainer": "Seungwoo Kang <kangsw0401@snu.ac.kr>",
    "author": "Seungwoo Kang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8082-0794>),\n  Hee-Seok Oh [aut]",
    "url": "https://github.com/seungwoo-stat/rvMF",
    "bug_reports": "https://github.com/seungwoo-stat/rvMF/issues",
    "repository": "https://cran.r-project.org/package=rvMF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvMF Fast Generation of von Mises-Fisher Distributed Pseudo-Random\nVectors Generates pseudo-random vectors that follow an arbitrary von Mises-Fisher distribution on a sphere. This method is fast and efficient when generating a large number of pseudo-random vectors. Functions to generate random variates and compute density for the distribution of an inner product between von Mises-Fisher random vector and its mean direction are also provided. Details are in Kang and Oh (2024) <doi:10.1007/s11222-024-10419-3>.  "
  },
  {
    "id": 19960,
    "package_name": "rvec",
    "title": "Vectors Representing Random Variables",
    "description": "Random vectors, called rvecs. An rvec holds\n    multiple draws, but tries to behave like a standard\n    R vector, including working well in data frames.\n    Rvecs are useful for analysing\n    output from a simulation or a Bayesian analysis.",
    "version": "1.0.0",
    "maintainer": "John Bryant <john@bayesiandemography.com>",
    "author": "John Bryant [aut, cre],\n  Bayesian Demography Limited [cph]",
    "url": "https://bayesiandemography.github.io/rvec/,\nhttps://github.com/bayesiandemography/rvec",
    "bug_reports": "https://github.com/bayesiandemography/rvec/issues",
    "repository": "https://cran.r-project.org/package=rvec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvec Vectors Representing Random Variables Random vectors, called rvecs. An rvec holds\n    multiple draws, but tries to behave like a standard\n    R vector, including working well in data frames.\n    Rvecs are useful for analysing\n    output from a simulation or a Bayesian analysis.  "
  },
  {
    "id": 19961,
    "package_name": "rvg",
    "title": "R Graphics Devices for 'Office' Vector Graphics Output",
    "description": "Vector Graphics devices for 'Microsoft PowerPoint' and\n    'Microsoft Excel'. Functions extending package 'officer' are provided\n    to embed 'DrawingML' graphics into 'Microsoft PowerPoint'\n    presentations and 'Microsoft Excel' workbooks.",
    "version": "0.4.0",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  ArData [cph],\n  Bob Rudis [ctb] (the javascript code used by function set_attr),\n  Francois Brunetti [ctb] (clipping algorithms)",
    "url": "https://ardata-fr.github.io/officeverse/,\nhttps://davidgohel.github.io/rvg/",
    "bug_reports": "https://github.com/davidgohel/rvg/issues",
    "repository": "https://cran.r-project.org/package=rvg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvg R Graphics Devices for 'Office' Vector Graphics Output Vector Graphics devices for 'Microsoft PowerPoint' and\n    'Microsoft Excel'. Functions extending package 'officer' are provided\n    to embed 'DrawingML' graphics into 'Microsoft PowerPoint'\n    presentations and 'Microsoft Excel' workbooks.  "
  },
  {
    "id": 19981,
    "package_name": "rworldxtra",
    "title": "Country boundaries at high resolution",
    "description": "High resolution vector country boundaries derived from\n        Natural Earth data, can be plotted in rworldmap.",
    "version": "1.01",
    "maintainer": "Andy South <southandy@gmail.com>",
    "author": "Andy South",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rworldxtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rworldxtra Country boundaries at high resolution High resolution vector country boundaries derived from\n        Natural Earth data, can be plotted in rworldmap.  "
  },
  {
    "id": 20034,
    "package_name": "saeHB.spatial",
    "title": "Small Area Estimation Hierarchical Bayes For Spatial Model",
    "description": "Provides several functions and datasets for area level of Small Area Estimation under Spatial Model using Hierarchical Bayesian (HB) Method. Model-based estimators include the HB estimators based on a Spatial Fay-Herriot model with univariate normal distribution for variable of interest.The 'rjags' package is employed to obtain parameter estimates. For the reference, see Rao and Molina (2015) <doi:10.1002/9781118735855>.",
    "version": "0.1.1",
    "maintainer": "Arina Mana Sikana <sikanaradrianan@gmail.com>",
    "author": "Arina Mana Sikana [aut, cre],\n  Azka Ubaidillah [aut]",
    "url": "https://github.com/arinams/saeHB.spatial",
    "bug_reports": "https://github.com/arinams/saeHB.spatial/issues",
    "repository": "https://cran.r-project.org/package=saeHB.spatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saeHB.spatial Small Area Estimation Hierarchical Bayes For Spatial Model Provides several functions and datasets for area level of Small Area Estimation under Spatial Model using Hierarchical Bayesian (HB) Method. Model-based estimators include the HB estimators based on a Spatial Fay-Herriot model with univariate normal distribution for variable of interest.The 'rjags' package is employed to obtain parameter estimates. For the reference, see Rao and Molina (2015) <doi:10.1002/9781118735855>.  "
  },
  {
    "id": 20061,
    "package_name": "salad",
    "title": "Simple Automatic Differentiation",
    "description": "Handles both vector and matrices, using a flexible S4 class for automatic differentiation.\n  The method used is forward automatic differentiation. Many functions and methods have been defined, \n  so that in most cases, functions written without automatic differentiation in mind can be \n  used without change. ",
    "version": "1.2",
    "maintainer": "Herv\u00e9 Perdry <herve.perdry@universite-paris-saclay.fr>",
    "author": "Herv\u00e9 Perdry [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=salad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "salad Simple Automatic Differentiation Handles both vector and matrices, using a flexible S4 class for automatic differentiation.\n  The method used is forward automatic differentiation. Many functions and methods have been defined, \n  so that in most cases, functions written without automatic differentiation in mind can be \n  used without change.   "
  },
  {
    "id": 20104,
    "package_name": "sanon",
    "title": "Stratified Analysis with Nonparametric Covariable Adjustment",
    "description": "There are several functions to implement the method for analysis in a randomized clinical trial with strata with following key features. A stratified Mann-Whitney estimator addresses the comparison between two randomized groups for a strictly ordinal response variable. The multivariate vector of such stratified Mann-Whitney estimators for multivariate response variables can be considered for one or more response variables such as in repeated measurements and these can have missing completely at random (MCAR) data. Non-parametric covariance adjustment is also considered with the minimal assumption of randomization. The p-value for hypothesis test and confidence interval are provided.",
    "version": "1.6",
    "maintainer": "Atsushi Kawaguchi <kawa_a24@yahoo.co.jp>",
    "author": "Atsushi Kawaguchi",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sanon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sanon Stratified Analysis with Nonparametric Covariable Adjustment There are several functions to implement the method for analysis in a randomized clinical trial with strata with following key features. A stratified Mann-Whitney estimator addresses the comparison between two randomized groups for a strictly ordinal response variable. The multivariate vector of such stratified Mann-Whitney estimators for multivariate response variables can be considered for one or more response variables such as in repeated measurements and these can have missing completely at random (MCAR) data. Non-parametric covariance adjustment is also considered with the minimal assumption of randomization. The p-value for hypothesis test and confidence interval are provided.  "
  },
  {
    "id": 20107,
    "package_name": "santoku",
    "title": "A Versatile Cutting Tool",
    "description": "A tool for cutting data into intervals. Allows singleton intervals.\n  Always includes the whole range of data by default. Flexible labelling. \n  Convenience functions for cutting by quantiles etc. Handles dates, times, units\n  and other vectors.",
    "version": "1.1.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "author": "David Hugh-Jones [aut, cre],\n  Daniel Possenriede [ctb]",
    "url": "https://github.com/hughjonesd/santoku,\nhttps://hughjonesd.github.io/santoku/",
    "bug_reports": "https://github.com/hughjonesd/santoku/issues",
    "repository": "https://cran.r-project.org/package=santoku",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "santoku A Versatile Cutting Tool A tool for cutting data into intervals. Allows singleton intervals.\n  Always includes the whole range of data by default. Flexible labelling. \n  Convenience functions for cutting by quantiles etc. Handles dates, times, units\n  and other vectors.  "
  },
  {
    "id": 20109,
    "package_name": "sapevom",
    "title": "Group Ordinal Method for Multiple Criteria Decision-Making",
    "description": "Implementation of SAPEVO-M, a Group Ordinal Method for Multiple Criteria Decision-Making (MCDM). SAPEVO-M is an acronym for Simple Aggregation of Preferences Expressed by Ordinal Vectors Group Decision Making. This method provides alternatives ranking given decision makers' preferences: criteria preferences and alternatives preferences for each criterion.This method is described in Gomes et al. (2020) <doi: 10.1590/0101-7438.2020.040.00226524 >.",
    "version": "0.2.0",
    "maintainer": "Raquel Coutinho <rdouradocoutinho@gmail.com>",
    "author": "Raquel Coutinho [aut, cre],\n  Marcos dos Santos [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sapevom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sapevom Group Ordinal Method for Multiple Criteria Decision-Making Implementation of SAPEVO-M, a Group Ordinal Method for Multiple Criteria Decision-Making (MCDM). SAPEVO-M is an acronym for Simple Aggregation of Preferences Expressed by Ordinal Vectors Group Decision Making. This method provides alternatives ranking given decision makers' preferences: criteria preferences and alternatives preferences for each criterion.This method is described in Gomes et al. (2020) <doi: 10.1590/0101-7438.2020.040.00226524 >.  "
  },
  {
    "id": 20113,
    "package_name": "sara4r",
    "title": "An R-GUI for Spatial Analysis of Surface Runoff using the\nNRCS-CN Method",
    "description": "A Graphical user interface to calculate the rainfall-runoff relation using the Natural Resources Conservation Service - Curve Number method (NRCS-CN method) but include modifications by Hawkins et al., (2002) about the Initial Abstraction. This GUI follows the programming logic of a previously published software (Hernandez-Guzman et al., 2011)<doi:10.1016/j.envsoft.2011.07.006>. It is a raster-based GIS tool that outputs runoff estimates from Land use/land cover and hydrologic soil group maps.\n    This package has already been published in Journal of Hydroinformatics (Hernandez-Guzman et al., 2021)<doi:10.2166/hydro.2020.087> but it is under constant development at the Institute about Natural Resources Research (INIRENA) from the Universidad Michoacana de San Nicolas de Hidalgo and represents a collaborative effort between the Hydro-Geomatic Lab (INIRENA) with the Environmental Management Lab (CIAD, A.C.).",
    "version": "0.1.0",
    "maintainer": "Rafael Hernandez-Guzman <rhernandez.g@gmail.com>",
    "author": "Rafael Hernandez-Guzman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2711-9015>),\n  Arturo Ruiz-Luna [aut] (ORCID: <https://orcid.org/0000-0001-6878-0929>)",
    "url": "https://hydro-geomatic-lab.com/,\nhttps://hydro-geomatic-lab.com/sara4r.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sara4r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sara4r An R-GUI for Spatial Analysis of Surface Runoff using the\nNRCS-CN Method A Graphical user interface to calculate the rainfall-runoff relation using the Natural Resources Conservation Service - Curve Number method (NRCS-CN method) but include modifications by Hawkins et al., (2002) about the Initial Abstraction. This GUI follows the programming logic of a previously published software (Hernandez-Guzman et al., 2011)<doi:10.1016/j.envsoft.2011.07.006>. It is a raster-based GIS tool that outputs runoff estimates from Land use/land cover and hydrologic soil group maps.\n    This package has already been published in Journal of Hydroinformatics (Hernandez-Guzman et al., 2021)<doi:10.2166/hydro.2020.087> but it is under constant development at the Institute about Natural Resources Research (INIRENA) from the Universidad Michoacana de San Nicolas de Hidalgo and represents a collaborative effort between the Hydro-Geomatic Lab (INIRENA) with the Environmental Management Lab (CIAD, A.C.).  "
  },
  {
    "id": 20126,
    "package_name": "satellite",
    "title": "Handling and Manipulating Remote Sensing Data",
    "description": "Herein, we provide a broad variety of functions which are useful\n    for handling, manipulating, and visualizing satellite-based remote sensing \n    data. These operations range from mere data import and layer handling (eg \n    subsetting), over Raster* typical data wrangling (eg crop, extend), to more \n    sophisticated (pre-)processing tasks typically applied to satellite imagery \n    (eg atmospheric and topographic correction). This functionality is \n    complemented by a full access to the satellite layers' metadata at any \n    stage and the documentation of performed actions in a separate log file. \n    Currently available sensors include Landsat 4-5 (TM), 7 (ETM+), and 8 \n    (OLI/TIRS Combined), and additional compatibility is ensured for the Landsat \n    Global Land Survey data set. ",
    "version": "1.0.6",
    "maintainer": "Florian Detsch <fdetsch@web.de>",
    "author": "Thomas Nauss [aut],\n  Hanna Meyer [aut],\n  Tim Appelhans [aut],\n  Florian Detsch [aut, cre]",
    "url": "https://github.com/environmentalinformatics-marburg/satellite",
    "bug_reports": "https://github.com/environmentalinformatics-marburg/satellite/issues",
    "repository": "https://cran.r-project.org/package=satellite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "satellite Handling and Manipulating Remote Sensing Data Herein, we provide a broad variety of functions which are useful\n    for handling, manipulating, and visualizing satellite-based remote sensing \n    data. These operations range from mere data import and layer handling (eg \n    subsetting), over Raster* typical data wrangling (eg crop, extend), to more \n    sophisticated (pre-)processing tasks typically applied to satellite imagery \n    (eg atmospheric and topographic correction). This functionality is \n    complemented by a full access to the satellite layers' metadata at any \n    stage and the documentation of performed actions in a separate log file. \n    Currently available sensors include Landsat 4-5 (TM), 7 (ETM+), and 8 \n    (OLI/TIRS Combined), and additional compatibility is ensured for the Landsat \n    Global Land Survey data set.   "
  },
  {
    "id": 20127,
    "package_name": "satin",
    "title": "Visualisation and Analysis of Ocean Data Derived from Satellites",
    "description": "With 'satin' functions, visualisation, data extraction and further analysis like producing climatologies from several images, and anomalies of satellite derived ocean data can be easily done.  Reading functions can import a user defined geographical extent of data stored in netCDF files.  Currently supported ocean data sources include NASA's Oceancolor web page <https://oceancolor.gsfc.nasa.gov/>, sensors VIIRS-SNPP; MODIS-Terra; MODIS-Aqua; and SeaWiFS.  Available variables from this source includes chlorophyll concentration, sea surface temperature (SST), and several others.  Data sources specific for SST that can be imported too includes Pathfinder AVHRR <https://www.ncei.noaa.gov/products/avhrr-pathfinder-sst> and GHRSST <https://www.ghrsst.org/>.  In addition, ocean productivity data produced by Oregon State University can also be handled previous conversion from HDF4 to HDF5 format.  Many other ocean variables can be processed by importing netCDF data files from two European Union's Copernicus Marine Service databases <https://marine.copernicus.eu/>, namely Global Ocean Physical Reanalysis and Global Ocean Biogeochemistry Hindcast.",
    "version": "1.2.0",
    "maintainer": "H\u00e9ctor Villalobos <hvillalo@ipn.mx>",
    "author": "H\u00e9ctor Villalobos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6424-4050>),\n  Eduardo Gonz\u00e1lez-Rodr\u00edguez [aut]",
    "url": "https://github.com/hvillalo/satin",
    "bug_reports": "https://github.com/hvillalo/satin/issues",
    "repository": "https://cran.r-project.org/package=satin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "satin Visualisation and Analysis of Ocean Data Derived from Satellites With 'satin' functions, visualisation, data extraction and further analysis like producing climatologies from several images, and anomalies of satellite derived ocean data can be easily done.  Reading functions can import a user defined geographical extent of data stored in netCDF files.  Currently supported ocean data sources include NASA's Oceancolor web page <https://oceancolor.gsfc.nasa.gov/>, sensors VIIRS-SNPP; MODIS-Terra; MODIS-Aqua; and SeaWiFS.  Available variables from this source includes chlorophyll concentration, sea surface temperature (SST), and several others.  Data sources specific for SST that can be imported too includes Pathfinder AVHRR <https://www.ncei.noaa.gov/products/avhrr-pathfinder-sst> and GHRSST <https://www.ghrsst.org/>.  In addition, ocean productivity data produced by Oregon State University can also be handled previous conversion from HDF4 to HDF5 format.  Many other ocean variables can be processed by importing netCDF data files from two European Union's Copernicus Marine Service databases <https://marine.copernicus.eu/>, namely Global Ocean Physical Reanalysis and Global Ocean Biogeochemistry Hindcast.  "
  },
  {
    "id": 20128,
    "package_name": "satres",
    "title": "Grouping Satellite Bands by Spectral and Spatial Resolution",
    "description": "Given raster files directly downloaded from various websites,\n    it generates a raster structure where it merges them if they are tiles\n    of the same scene and classifies them according to their spectral and\n    spatial resolution for easy access by name.",
    "version": "1.1.1",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/satres/,\nhttps://github.com/josesamos/satres",
    "bug_reports": "https://github.com/josesamos/satres/issues",
    "repository": "https://cran.r-project.org/package=satres",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "satres Grouping Satellite Bands by Spectral and Spatial Resolution Given raster files directly downloaded from various websites,\n    it generates a raster structure where it merges them if they are tiles\n    of the same scene and classifies them according to their spectral and\n    spatial resolution for easy access by name.  "
  },
  {
    "id": 20142,
    "package_name": "sboost",
    "title": "Machine Learning with AdaBoost on Decision Stumps",
    "description": "Creates classifier for binary outcomes using Adaptive Boosting \n    (AdaBoost) algorithm on decision stumps with a fast C++ implementation. \n    For a description of AdaBoost, see Freund and Schapire (1997) \n    <doi:10.1006/jcss.1997.1504>. This type of classifier is nonlinear, but\n    easy to interpret and visualize. Feature vectors may be a combination of\n    continuous (numeric) and categorical (string, factor) elements. Methods \n    for classifier assessment, predictions, and cross-validation also included.",
    "version": "0.1.2",
    "maintainer": "Jadon Wagstaff <jadonw@gmail.com>",
    "author": "Jadon Wagstaff [aut, cre]",
    "url": "https://github.com/jadonwagstaff/sboost",
    "bug_reports": "https://github.com/jadonwagstaff/sboost/issues",
    "repository": "https://cran.r-project.org/package=sboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sboost Machine Learning with AdaBoost on Decision Stumps Creates classifier for binary outcomes using Adaptive Boosting \n    (AdaBoost) algorithm on decision stumps with a fast C++ implementation. \n    For a description of AdaBoost, see Freund and Schapire (1997) \n    <doi:10.1006/jcss.1997.1504>. This type of classifier is nonlinear, but\n    easy to interpret and visualize. Feature vectors may be a combination of\n    continuous (numeric) and categorical (string, factor) elements. Methods \n    for classifier assessment, predictions, and cross-validation also included.  "
  },
  {
    "id": 20159,
    "package_name": "scITD",
    "title": "Single-Cell Interpretable Tensor Decomposition",
    "description": "Single-cell Interpretable Tensor Decomposition (scITD) employs the \n    Tucker tensor decomposition to extract multicell-type gene expression patterns \n    that vary across donors/individuals. This tool is geared for use with single-cell\n    RNA-sequencing datasets consisting of many source donors. The method has a wide\n    range of potential applications, including the study of inter-individual variation\n    at the population-level, patient sub-grouping/stratification, and the analysis\n    of sample-level batch effects. Each \"multicellular process\" that is extracted \n    consists of (A) a multi cell type gene loadings matrix and (B) a\n    corresponding donor scores vector indicating the level at which the corresponding\n    loadings matrix is expressed in each donor. Additional methods are implemented\n    to aid in selecting an appropriate number of factors and to evaluate stability\n    of the decomposition. Additional tools are provided for downstream analysis,\n    including integration of gene set enrichment analysis and ligand-receptor analysis. \n    Tucker, L.R. (1966) <doi:10.1007/BF02289464>. Unkel, S., Hannachi, A., Trendafilov, N. T., & Jolliffe, I. T. (2011) <doi:10.1007/s13253-011-0055-9>. Zhou, G., & Cichocki, A. (2012) <doi:10.2478/v10175-012-0051-4>.",
    "version": "1.0.4",
    "maintainer": "Jonathan Mitchel <jonathan.mitchel3@gmail.com>",
    "author": "Jonathan Mitchel [cre, aut],\n  Evan Biederstedt [aut],\n  Peter Kharchenko [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scITD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scITD Single-Cell Interpretable Tensor Decomposition Single-cell Interpretable Tensor Decomposition (scITD) employs the \n    Tucker tensor decomposition to extract multicell-type gene expression patterns \n    that vary across donors/individuals. This tool is geared for use with single-cell\n    RNA-sequencing datasets consisting of many source donors. The method has a wide\n    range of potential applications, including the study of inter-individual variation\n    at the population-level, patient sub-grouping/stratification, and the analysis\n    of sample-level batch effects. Each \"multicellular process\" that is extracted \n    consists of (A) a multi cell type gene loadings matrix and (B) a\n    corresponding donor scores vector indicating the level at which the corresponding\n    loadings matrix is expressed in each donor. Additional methods are implemented\n    to aid in selecting an appropriate number of factors and to evaluate stability\n    of the decomposition. Additional tools are provided for downstream analysis,\n    including integration of gene set enrichment analysis and ligand-receptor analysis. \n    Tucker, L.R. (1966) <doi:10.1007/BF02289464>. Unkel, S., Hannachi, A., Trendafilov, N. T., & Jolliffe, I. T. (2011) <doi:10.1007/s13253-011-0055-9>. Zhou, G., & Cichocki, A. (2012) <doi:10.2478/v10175-012-0051-4>.  "
  },
  {
    "id": 20164,
    "package_name": "scPOEM",
    "title": "Single-Cell Meta-Path Based Omic Embedding",
    "description": "Provide a workflow to jointly embed chromatin accessibility peaks and expressed genes into a shared low-dimensional space using paired single-cell ATAC-seq (scATAC-seq) and single-cell RNA-seq (scRNA-seq) data. It integrates regulatory relationships among peak-peak interactions (via 'Cicero'), peak-gene interactions (via Lasso, random forest, and XGBoost), and gene-gene interactions (via principal component regression). With the input of paired scATAC-seq and scRNA-seq data matrices, it assigns a low-dimensional feature vector to each gene and peak. Additionally, it supports the reconstruction of gene-gene network with low-dimensional projections (via epsilon-NN) and then the comparison of the networks of two conditions through manifold alignment implemented in 'scTenifoldNet'. See <doi:10.1093/bioinformatics/btaf483> for more details.",
    "version": "0.1.3",
    "maintainer": "Yuntong Hou <houyt223@gmail.com>",
    "author": "Yuntong Hou [aut, cre] (ORCID: <https://orcid.org/0009-0005-0587-4692>),\n  Yan Zhong [aut, ctb] (ORCID: <https://orcid.org/0000-0003-2412-043X>),\n  Yeran Chen [ctb],\n  Youshi Chang [ctb],\n  Yongjian Yang [ctb] (ORCID: <https://orcid.org/0000-0002-4135-5014>),\n  Xinyue Zheng [ctb],\n  James Cai [ctb] (ORCID: <https://orcid.org/0000-0002-8081-6725>)",
    "url": "https://github.com/Houyt23/scPOEM",
    "bug_reports": "https://github.com/Houyt23/scPOEM/issues",
    "repository": "https://cran.r-project.org/package=scPOEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scPOEM Single-Cell Meta-Path Based Omic Embedding Provide a workflow to jointly embed chromatin accessibility peaks and expressed genes into a shared low-dimensional space using paired single-cell ATAC-seq (scATAC-seq) and single-cell RNA-seq (scRNA-seq) data. It integrates regulatory relationships among peak-peak interactions (via 'Cicero'), peak-gene interactions (via Lasso, random forest, and XGBoost), and gene-gene interactions (via principal component regression). With the input of paired scATAC-seq and scRNA-seq data matrices, it assigns a low-dimensional feature vector to each gene and peak. Additionally, it supports the reconstruction of gene-gene network with low-dimensional projections (via epsilon-NN) and then the comparison of the networks of two conditions through manifold alignment implemented in 'scTenifoldNet'. See <doi:10.1093/bioinformatics/btaf483> for more details.  "
  },
  {
    "id": 20188,
    "package_name": "scapesClassification",
    "title": "User-Defined Classification of Raster Surfaces",
    "description": "Series of algorithms to translate  users' mental models of seascapes, \n  landscapes and, more generally, of geographic features into computer representations \n  (classifications). Spaces and geographic objects are classified with user-defined \n  rules taking into account spatial data as well as spatial relationships among \n  different classes and objects.",
    "version": "1.0.0",
    "maintainer": "Gerald H. Taranto <gh.taranto@gmail.com>",
    "author": "Gerald H. Taranto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7968-1982>)",
    "url": "https://github.com/ghTaranto/scapesClassification,\nhttps://ghtaranto.github.io/scapesClassification/",
    "bug_reports": "https://github.com/ghTaranto/scapesClassification/issues",
    "repository": "https://cran.r-project.org/package=scapesClassification",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scapesClassification User-Defined Classification of Raster Surfaces Series of algorithms to translate  users' mental models of seascapes, \n  landscapes and, more generally, of geographic features into computer representations \n  (classifications). Spaces and geographic objects are classified with user-defined \n  rules taking into account spatial data as well as spatial relationships among \n  different classes and objects.  "
  },
  {
    "id": 20194,
    "package_name": "scattermore",
    "title": "Scatterplots with More Points",
    "description": "C-based conversion of large scatterplot data to rasters plus other \n             operations such as data blurring or data alpha blending. Speeds up \n             plotting of data with millions of points.",
    "version": "1.2",
    "maintainer": "Mirek Kratochvil <exa.exa@gmail.com>",
    "author": "Tereza Kulichova [aut],\n  Mirek Kratochvil [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7356-4075>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scattermore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scattermore Scatterplots with More Points C-based conversion of large scatterplot data to rasters plus other \n             operations such as data blurring or data alpha blending. Speeds up \n             plotting of data with millions of points.  "
  },
  {
    "id": 20209,
    "package_name": "sched",
    "title": "Request Scheduler",
    "description": "Offers classes and functions to contact web servers while enforcing scheduling rules required by the sites. The URL class makes it easy to construct a URL by providing parameters as a vector. The Request class allows to describes SOAP (Simple Object Access Protocol) or standard requests: URL, method (POST or GET), header, body. The Scheduler class controls the request frequency for each server address by mean of rules (Rule class). The RequestResult class permits to get the request status to handle error cases and the content.",
    "version": "1.0.3",
    "maintainer": "Pierrick Roger <pierrick.roger@cea.fr>",
    "author": "Pierrick Roger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8177-4873>)",
    "url": "https://gitlab.com/cnrgh/databases/r-sched",
    "bug_reports": "https://gitlab.com/cnrgh/databases/r-sched/-/issues",
    "repository": "https://cran.r-project.org/package=sched",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sched Request Scheduler Offers classes and functions to contact web servers while enforcing scheduling rules required by the sites. The URL class makes it easy to construct a URL by providing parameters as a vector. The Request class allows to describes SOAP (Simple Object Access Protocol) or standard requests: URL, method (POST or GET), header, body. The Scheduler class controls the request frequency for each server address by mean of rules (Rule class). The RequestResult class permits to get the request status to handle error cases and the content.  "
  },
  {
    "id": 20277,
    "package_name": "sdam",
    "title": "Social Dynamics and Complexity in the Ancient Mediterranean",
    "description": "Provides digital tools for performing analyses within Social Dynamics and complexity in the Ancient Mediterranean (SDAM), which is a research group based at the Department of History and Classical Studies at Aarhus University. ",
    "version": "1.1.4",
    "maintainer": "Antonio Rivero Ostoic <jaro@cas.au.dk>",
    "author": "Antonio Rivero Ostoic [aut, cre],\n  Adela Sobotkova [ctb],\n  Vojtech Kase [ctb],\n  Petra Hermankova [ctb]",
    "url": "https://github.com/sdam-au/sdam/",
    "bug_reports": "https://github.com/sdam-au/sdam/issues/",
    "repository": "https://cran.r-project.org/package=sdam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdam Social Dynamics and Complexity in the Ancient Mediterranean Provides digital tools for performing analyses within Social Dynamics and complexity in the Ancient Mediterranean (SDAM), which is a research group based at the Department of History and Classical Studies at Aarhus University.   "
  },
  {
    "id": 20283,
    "package_name": "sdcSpatial",
    "title": "Statistical Disclosure Control for Spatial Data",
    "description": "Privacy protected raster maps \n  can be created from spatial point data. Protection\n  methods include smoothing of dichotomous variables by de Jonge and de Wolf (2016) \n  <doi:10.1007/978-3-319-45381-1_9>, continuous variables by de Wolf and \n  de Jonge (2018) <doi:10.1007/978-3-319-99771-1_23>, suppressing \n  revealing values and a generalization of the quad tree method by \n  Su\u00f1\u00e9, Rovira, Ib\u00e1\u00f1ez and Farr\u00e9 (2017) <doi:10.2901/EUROSTAT.C2017.001>.",
    "version": "0.6.1",
    "maintainer": "Edwin de Jonge <edwindjonge@gmail.com>",
    "author": "Edwin de Jonge [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6580-4718>),\n  Peter-Paul de Wolf [aut],\n  Douwe Hut [ctb],\n  Sapphire Han [ctb]",
    "url": "https://github.com/edwindj/sdcSpatial",
    "bug_reports": "https://github.com/edwindj/sdcSpatial/issues",
    "repository": "https://cran.r-project.org/package=sdcSpatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdcSpatial Statistical Disclosure Control for Spatial Data Privacy protected raster maps \n  can be created from spatial point data. Protection\n  methods include smoothing of dichotomous variables by de Jonge and de Wolf (2016) \n  <doi:10.1007/978-3-319-45381-1_9>, continuous variables by de Wolf and \n  de Jonge (2018) <doi:10.1007/978-3-319-99771-1_23>, suppressing \n  revealing values and a generalization of the quad tree method by \n  Su\u00f1\u00e9, Rovira, Ib\u00e1\u00f1ez and Farr\u00e9 (2017) <doi:10.2901/EUROSTAT.C2017.001>.  "
  },
  {
    "id": 20315,
    "package_name": "secretbase",
    "title": "Cryptographic Hash, Extendable-Output and Base64 Functions",
    "description": "Fast and memory-efficient streaming hash functions and base64\n    encoding / decoding. Hashes strings and raw vectors directly. Stream hashes\n    files which can be larger than memory, as well as in-memory objects through\n    R's serialization mechanism. Implementations include the SHA-256, SHA-3 and\n    'Keccak' cryptographic hash functions, SHAKE256 extendable-output function\n    (XOF), and 'SipHash' pseudo-random function.",
    "version": "1.0.5",
    "maintainer": "Charlie Gao <charlie.gao@shikokuchuo.net>",
    "author": "Charlie Gao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0750-061X>),\n  Hibiki AI Limited [cph]",
    "url": "https://shikokuchuo.net/secretbase/,\nhttps://github.com/shikokuchuo/secretbase/",
    "bug_reports": "https://github.com/shikokuchuo/secretbase/issues",
    "repository": "https://cran.r-project.org/package=secretbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "secretbase Cryptographic Hash, Extendable-Output and Base64 Functions Fast and memory-efficient streaming hash functions and base64\n    encoding / decoding. Hashes strings and raw vectors directly. Stream hashes\n    files which can be larger than memory, as well as in-memory objects through\n    R's serialization mechanism. Implementations include the SHA-256, SHA-3 and\n    'Keccak' cryptographic hash functions, SHAKE256 extendable-output function\n    (XOF), and 'SipHash' pseudo-random function.  "
  },
  {
    "id": 20324,
    "package_name": "seecolor",
    "title": "View Colors Used in R Objects in the Console",
    "description": "Output colors used in literal vectors, palettes and plot objects (ggplot).",
    "version": "0.2.0",
    "maintainer": "Shangchen Song <s.song@ufl.edu>",
    "author": "Shangchen Song [aut, cre]",
    "url": "https://github.com/lovestat/seecolor",
    "bug_reports": "https://github.com/lovestat/seecolor/issues",
    "repository": "https://cran.r-project.org/package=seecolor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seecolor View Colors Used in R Objects in the Console Output colors used in literal vectors, palettes and plot objects (ggplot).  "
  },
  {
    "id": 20347,
    "package_name": "seismicRoll",
    "title": "Fast Rolling Functions for Seismology using 'Rcpp'",
    "description": "Fast versions of seismic analysis functions that 'roll' over a\n    vector of values. See the 'RcppRoll' package for alternative\n    versions of basic statistical functions such as rolling mean,\n    median, etc.",
    "version": "1.1.5",
    "maintainer": "Gillian Sharer <gillian.sharer@earthscope.org>",
    "author": "Jonathan Callahan [aut],\n  Rob Casey [aut],\n  Mary Templeton [aut],\n  Gillian Sharer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=seismicRoll",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seismicRoll Fast Rolling Functions for Seismology using 'Rcpp' Fast versions of seismic analysis functions that 'roll' over a\n    vector of values. See the 'RcppRoll' package for alternative\n    versions of basic statistical functions such as rolling mean,\n    median, etc.  "
  },
  {
    "id": 20415,
    "package_name": "sentiment.ai",
    "title": "Simple Sentiment Analysis Using Deep Learning",
    "description": "Sentiment Analysis via deep learning and gradient boosting models with a lot of the underlying hassle taken care of to make the process as simple as possible. \n  In addition to out-performing traditional, lexicon-based sentiment analysis (see <https://benwiseman.github.io/sentiment.ai/#Benchmarks>),\n  it also allows the user to create embedding vectors for text which can be used in other analyses.\n  GPU acceleration is supported on Windows and Linux.",
    "version": "0.1.1",
    "maintainer": "Ben Wiseman <benjamin.h.wiseman@gmail.com>",
    "author": "Ben Wiseman [cre, aut, ccp],\n  Steven Nydick [aut] (ORCID: <https://orcid.org/0000-0002-2908-1188>),\n  Tristan Wisner [aut],\n  Fiona Lodge [ctb],\n  Yu-Ann Wang [ctb],\n  Veronica Ge [art],\n  Korn Ferry Institute [fnd]",
    "url": "https://benwiseman.github.io/sentiment.ai/,\nhttps://github.com/BenWiseman/sentiment.ai",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sentiment.ai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sentiment.ai Simple Sentiment Analysis Using Deep Learning Sentiment Analysis via deep learning and gradient boosting models with a lot of the underlying hassle taken care of to make the process as simple as possible. \n  In addition to out-performing traditional, lexicon-based sentiment analysis (see <https://benwiseman.github.io/sentiment.ai/#Benchmarks>),\n  it also allows the user to create embedding vectors for text which can be used in other analyses.\n  GPU acceleration is supported on Windows and Linux.  "
  },
  {
    "id": 20474,
    "package_name": "sfhotspot",
    "title": "Hot-Spot Analysis with Simple Features",
    "description": "Identify and understand clusters of points (typically representing \n    the locations of places or events) stored in simple-features (SF) objects.\n    This is useful for analysing, for example, hot-spots of crime events. The \n    package emphasises producing results from point SF data in a single step \n    using reasonable default values for all other arguments, to aid rapid data \n    analysis by users who are starting out. Functions available include kernel\n    density estimation (for details, see Yip (2020) \n    <doi:10.22224/gistbok/2020.1.12>), analysis of spatial association (Getis\n    and Ord (1992) <doi:10.1111/j.1538-4632.1992.tb00261.x>) and hot-spot\n    classification (Chainey (2020) ISBN:158948584X).",
    "version": "1.0.0",
    "maintainer": "Matt Ashby <matthew.ashby@ucl.ac.uk>",
    "author": "Matt Ashby [aut, cre] (ORCID: <https://orcid.org/0000-0003-4201-9239>)",
    "url": "http://pkgs.lesscrime.info/sfhotspot/",
    "bug_reports": "https://github.com/mpjashby/sfhotspot/issues",
    "repository": "https://cran.r-project.org/package=sfhotspot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sfhotspot Hot-Spot Analysis with Simple Features Identify and understand clusters of points (typically representing \n    the locations of places or events) stored in simple-features (SF) objects.\n    This is useful for analysing, for example, hot-spots of crime events. The \n    package emphasises producing results from point SF data in a single step \n    using reasonable default values for all other arguments, to aid rapid data \n    analysis by users who are starting out. Functions available include kernel\n    density estimation (for details, see Yip (2020) \n    <doi:10.22224/gistbok/2020.1.12>), analysis of spatial association (Getis\n    and Ord (1992) <doi:10.1111/j.1538-4632.1992.tb00261.x>) and hot-spot\n    classification (Chainey (2020) ISBN:158948584X).  "
  },
  {
    "id": 20476,
    "package_name": "sfislands",
    "title": "Streamlines the Process of Fitting Areal Spatial Models",
    "description": "Helpers for addressing the issue of disconnected spatial units. \n    It allows for convenient adding and removal of neighbourhood connectivity between areal units prior to modelling, with the visual aid of maps.\n    Post-modelling, it reduces the human workload for extracting, tidying and mapping predictions from areal models.",
    "version": "1.1.2",
    "maintainer": "Kevin Horan <kevin.horan.2021@mumail.ie>",
    "author": "Kevin Horan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0003-9378-0084>),\n  Katarina Domijan [aut, ths] (ORCID:\n    <https://orcid.org/0000-0002-4268-2236>),\n  Chris Brunsdon [aut, ths] (ORCID:\n    <https://orcid.org/0000-0003-4254-1780>)",
    "url": "https://github.com/horankev/sfislands,\nhttps://horankev.github.io/sfislands/",
    "bug_reports": "https://github.com/horankev/sfislands/issues",
    "repository": "https://cran.r-project.org/package=sfislands",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sfislands Streamlines the Process of Fitting Areal Spatial Models Helpers for addressing the issue of disconnected spatial units. \n    It allows for convenient adding and removal of neighbourhood connectivity between areal units prior to modelling, with the visual aid of maps.\n    Post-modelling, it reduces the human workload for extracting, tidying and mapping predictions from areal models.  "
  },
  {
    "id": 20477,
    "package_name": "sfnetworks",
    "title": "Tidy Geospatial Networks",
    "description": "Provides a tidy approach to spatial network\n    analysis, in the form of classes and functions that enable a seamless\n    interaction between the network analysis package 'tidygraph' and the\n    spatial analysis package 'sf'.",
    "version": "0.6.5",
    "maintainer": "Lucas van der Meer <luukvandermeer@live.nl>",
    "author": "Lucas van der Meer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6336-8628>),\n  Lorena Abad [aut] (ORCID: <https://orcid.org/0000-0003-0554-734X>),\n  Andrea Gilardi [aut] (ORCID: <https://orcid.org/0000-0002-9424-7439>),\n  Robin Lovelace [aut] (ORCID: <https://orcid.org/0000-0001-5679-6536>)",
    "url": "https://luukvdmeer.github.io/sfnetworks/,\nhttps://github.com/luukvdmeer/sfnetworks",
    "bug_reports": "https://github.com/luukvdmeer/sfnetworks/issues/",
    "repository": "https://cran.r-project.org/package=sfnetworks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sfnetworks Tidy Geospatial Networks Provides a tidy approach to spatial network\n    analysis, in the form of classes and functions that enable a seamless\n    interaction between the network analysis package 'tidygraph' and the\n    spatial analysis package 'sf'.  "
  },
  {
    "id": 20489,
    "package_name": "sgeostat",
    "title": "An Object-Oriented Framework for Geostatistical Modeling in S+",
    "description": "An Object-oriented Framework for Geostatistical Modeling in S+ \n  containing functions for variogram estimation, variogram fitting and kriging\n  as well as some plot functions. Written entirely in S, therefore works only\n  for small data sets in acceptable computing time.",
    "version": "1.0-27",
    "maintainer": "Albrecht Gebhardt <albrecht.gebhardt@aau.at>",
    "author": "S original by James J. Majure <majure@iastate.edu> Iowa State University, R port + extensions by Albrecht Gebhardt <albrecht.gebhardt@aau.at>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sgeostat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sgeostat An Object-Oriented Framework for Geostatistical Modeling in S+ An Object-oriented Framework for Geostatistical Modeling in S+ \n  containing functions for variogram estimation, variogram fitting and kriging\n  as well as some plot functions. Written entirely in S, therefore works only\n  for small data sets in acceptable computing time.  "
  },
  {
    "id": 20496,
    "package_name": "sgolay",
    "title": "Efficient Savitzky-Golay Filtering",
    "description": "Smoothing signals and computing their derivatives is a common\n requirement in signal processing workflows. Savitzky-Golay filters are a\n established method able to do both (Savitzky and Golay, 1964 <doi:10.1021/ac60214a047>).\n This package implements one dimensional Savitzky-Golay filters that can be applied to\n vectors and matrices (either row-wise or column-wise).\n Vectorization and memory allocations have been profiled to reduce computational\n fingerprint. Short filter lengths are implemented in the direct space, while\n longer filters are implemented in frequency space, using a Fast Fourier\n Transform (FFT).",
    "version": "1.0.3",
    "maintainer": "Sergio Oller Moreno <sergioller@gmail.com>",
    "author": "Sergio Oller Moreno [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8994-1549>),\n  Robert Gentleman [ctb, cph] (Contributor to src/stats_filter.c and\n    src/fft.c (derived from the stats R package)),\n  Ross Ihaka [ctb, cph] (Contributor to src/stats_filter.c and src/fft.c\n    (derived from the stats R package)),\n  Brian Ripley [ctb, cph] (Contributor to src/stats_filter.c and\n    src/fft.c (derived from the stats R package)),\n  Martin Maechler [ctb, cph] (Contributor to src/stats_filter.c and\n    src/fft.c (derived from the stats R package)),\n  Duncan Murdoch [ctb, cph] (Contributor to src/stats_filter.c and\n    src/fft.c (derived from the stats R package)),\n  Institute for Bioengineering of Catalonia [cph]",
    "url": "https://github.com/zeehio/sgolay",
    "bug_reports": "https://github.com/zeehio/sgolay/issues",
    "repository": "https://cran.r-project.org/package=sgolay",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sgolay Efficient Savitzky-Golay Filtering Smoothing signals and computing their derivatives is a common\n requirement in signal processing workflows. Savitzky-Golay filters are a\n established method able to do both (Savitzky and Golay, 1964 <doi:10.1021/ac60214a047>).\n This package implements one dimensional Savitzky-Golay filters that can be applied to\n vectors and matrices (either row-wise or column-wise).\n Vectorization and memory allocations have been profiled to reduce computational\n fingerprint. Short filter lengths are implemented in the direct space, while\n longer filters are implemented in frequency space, using a Fast Fourier\n Transform (FFT).  "
  },
  {
    "id": 20502,
    "package_name": "sgsR",
    "title": "Structurally Guided Sampling",
    "description": "Structurally guided sampling (SGS) approaches for airborne laser scanning (ALS; LIDAR). Primary functions provide means \n    to generate data-driven stratifications & methods for allocating samples. Intermediate functions for calculating and extracting important information \n    about input covariates and samples are also included. Processing outcomes are intended to help forest and environmental management\n    practitioners better optimize field sample placement as well as assess and augment existing sample networks in the context of data\n    distributions and conditions. ALS data is the primary intended use case, however any rasterized remote sensing data can be used, \n    enabling data-driven stratifications and sampling approaches.",
    "version": "1.5.0",
    "maintainer": "Tristan RH Goodbody <goodbody.t@gmail.com>",
    "author": "Tristan RH Goodbody [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6894-7925>),\n  Nicholas C Coops [aut] (ORCID: <https://orcid.org/0000-0002-0151-9037>),\n  Martin Queinnec [aut] (ORCID: <https://orcid.org/0000-0002-2741-1032>)",
    "url": "https://github.com/tgoodbody/sgsR,\nhttps://tgoodbody.github.io/sgsR/",
    "bug_reports": "https://github.com/tgoodbody/sgsR/issues",
    "repository": "https://cran.r-project.org/package=sgsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sgsR Structurally Guided Sampling Structurally guided sampling (SGS) approaches for airborne laser scanning (ALS; LIDAR). Primary functions provide means \n    to generate data-driven stratifications & methods for allocating samples. Intermediate functions for calculating and extracting important information \n    about input covariates and samples are also included. Processing outcomes are intended to help forest and environmental management\n    practitioners better optimize field sample placement as well as assess and augment existing sample networks in the context of data\n    distributions and conditions. ALS data is the primary intended use case, however any rasterized remote sensing data can be used, \n    enabling data-driven stratifications and sampling approaches.  "
  },
  {
    "id": 20518,
    "package_name": "shar",
    "title": "Species-Habitat Associations",
    "description": "\n  Analyse species-habitat associations in R. Therefore, information about the location \n  of the species (as a point pattern) is needed together with environmental conditions \n  (as a categorical raster). To test for significance habitat associations, one of \n  the two components is randomized. Methods are mainly based on Plotkin et al. (2000) \n  <doi:10.1006/jtbi.2000.2158> and Harms et al. (2001) <doi:10.1111/j.1365-2745.2001.00615.x>.",
    "version": "2.3.1",
    "maintainer": "Maximilian H.K. Hesselbarth <mhk.hesselbarth@gmail.com>",
    "author": "Maximilian H.K. Hesselbarth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1125-9918>),\n  Marco Sciaini [aut] (ORCID: <https://orcid.org/0000-0002-3042-5435>),\n  Chris Wudel [aut] (ORCID: <https://orcid.org/0000-0003-0446-4665>),\n  Zeke Marshall [ctb] (ORCID: <https://orcid.org/0000-0001-9260-7827>),\n  Thomas Etherington [ctb] (ORCID:\n    <https://orcid.org/0000-0002-3187-075X>),\n  Janosch Heinermann [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5080-8591>)",
    "url": "https://r-spatialecology.github.io/shar/",
    "bug_reports": "https://github.com/r-spatialecology/shar/issues/",
    "repository": "https://cran.r-project.org/package=shar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shar Species-Habitat Associations \n  Analyse species-habitat associations in R. Therefore, information about the location \n  of the species (as a point pattern) is needed together with environmental conditions \n  (as a categorical raster). To test for significance habitat associations, one of \n  the two components is randomized. Methods are mainly based on Plotkin et al. (2000) \n  <doi:10.1006/jtbi.2000.2158> and Harms et al. (2001) <doi:10.1111/j.1365-2745.2001.00615.x>.  "
  },
  {
    "id": 20653,
    "package_name": "showtext",
    "title": "Using Fonts More Easily in R Graphs",
    "description": "Making it easy to use various types of fonts ('TrueType',\n    'OpenType', Type 1, web fonts, etc.) in R graphs, and supporting most output\n    formats of R graphics including PNG, PDF and SVG. Text glyphs will be converted\n    into polygons or raster images, hence after the plot has been created, it no\n    longer relies on the font files. No external software such as 'Ghostscript' is\n    needed to use this package.",
    "version": "0.9-7",
    "maintainer": "Yixuan Qiu <yixuan.qiu@cos.name>",
    "author": "Yixuan Qiu and authors/contributors of the\n    included software. See file AUTHORS for details.",
    "url": "https://github.com/yixuan/showtext",
    "bug_reports": "https://github.com/yixuan/showtext/issues",
    "repository": "https://cran.r-project.org/package=showtext",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "showtext Using Fonts More Easily in R Graphs Making it easy to use various types of fonts ('TrueType',\n    'OpenType', Type 1, web fonts, etc.) in R graphs, and supporting most output\n    formats of R graphics including PNG, PDF and SVG. Text glyphs will be converted\n    into polygons or raster images, hence after the plot has been created, it no\n    longer relies on the font files. No external software such as 'Ghostscript' is\n    needed to use this package.  "
  },
  {
    "id": 20660,
    "package_name": "shrinkTVPVAR",
    "title": "Efficient Bayesian Inference for TVP-VAR-SV Models with\nShrinkage",
    "description": "Efficient Markov chain Monte Carlo (MCMC) algorithms for fully Bayesian estimation of time-varying parameter \n  vector autoregressive models with stochastic volatility (TVP-VAR-SV) under shrinkage priors and dynamic shrinkage processes. \n  Details on the TVP-VAR-SV model and the shrinkage priors can be found in Cadonna et al. (2020) <doi:10.3390/econometrics8020020>, \n  details on the software can be found in Knaus et al. (2021) <doi:10.18637/jss.v100.i13>, while details on the dynamic shrinkage process\n  can be found in Knaus and Fr\u00fchwirth-Schnatter (2023) <doi:10.48550/arXiv.2312.10487>.",
    "version": "1.0.1",
    "maintainer": "Peter Knaus <peter.knaus@wu.ac.at>",
    "author": "Peter Knaus [aut, cre] (ORCID: <https://orcid.org/0000-0001-6498-7084>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shrinkTVPVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shrinkTVPVAR Efficient Bayesian Inference for TVP-VAR-SV Models with\nShrinkage Efficient Markov chain Monte Carlo (MCMC) algorithms for fully Bayesian estimation of time-varying parameter \n  vector autoregressive models with stochastic volatility (TVP-VAR-SV) under shrinkage priors and dynamic shrinkage processes. \n  Details on the TVP-VAR-SV model and the shrinkage priors can be found in Cadonna et al. (2020) <doi:10.3390/econometrics8020020>, \n  details on the software can be found in Knaus et al. (2021) <doi:10.18637/jss.v100.i13>, while details on the dynamic shrinkage process\n  can be found in Knaus and Fr\u00fchwirth-Schnatter (2023) <doi:10.48550/arXiv.2312.10487>.  "
  },
  {
    "id": 20661,
    "package_name": "shrinkem",
    "title": "Approximate Bayesian Regularization for Parsimonious Estimates",
    "description": "Approximate Bayesian regularization using Gaussian approximations. The input is a vector of estimates\n             and a Gaussian error covariance matrix of the key parameters. Bayesian shrinkage is then applied\n             to obtain parsimonious solutions. The method is described on \n             Karimova, van Erp, Leenders, and Mulder (2024) <DOI:10.31234/osf.io/2g8qm>. Gibbs samplers are used\n             for model fitting. The shrinkage priors that are supported are Gaussian (ridge) priors, Laplace\n             (lasso) priors (Park and Casella, 2008 <DOI:10.1198/016214508000000337>), and horseshoe priors\n             (Carvalho, et al., 2010; <DOI:10.1093/biomet/asq017>). These priors include an option\n             for grouped regularization of different subsets of parameters (Meier et al., 2008; \n             <DOI:10.1111/j.1467-9868.2007.00627.x>). F priors are used for the penalty\n             parameters lambda^2 (Mulder and Pericchi, 2018 <DOI:10.1214/17-BA1092>). This correspond to\n             half-Cauchy priors on lambda (Carvalho, Polson, Scott, 2010 <DOI:10.1093/biomet/asq017>).",
    "version": "0.2.0",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "author": "Joris Mulder [aut, cre],\n  Diana Karimova [aut, ctb],\n  Sara van Erp [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shrinkem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shrinkem Approximate Bayesian Regularization for Parsimonious Estimates Approximate Bayesian regularization using Gaussian approximations. The input is a vector of estimates\n             and a Gaussian error covariance matrix of the key parameters. Bayesian shrinkage is then applied\n             to obtain parsimonious solutions. The method is described on \n             Karimova, van Erp, Leenders, and Mulder (2024) <DOI:10.31234/osf.io/2g8qm>. Gibbs samplers are used\n             for model fitting. The shrinkage priors that are supported are Gaussian (ridge) priors, Laplace\n             (lasso) priors (Park and Casella, 2008 <DOI:10.1198/016214508000000337>), and horseshoe priors\n             (Carvalho, et al., 2010; <DOI:10.1093/biomet/asq017>). These priors include an option\n             for grouped regularization of different subsets of parameters (Meier et al., 2008; \n             <DOI:10.1111/j.1467-9868.2007.00627.x>). F priors are used for the penalty\n             parameters lambda^2 (Mulder and Pericchi, 2018 <DOI:10.1214/17-BA1092>). This correspond to\n             half-Cauchy priors on lambda (Carvalho, Polson, Scott, 2010 <DOI:10.1093/biomet/asq017>).  "
  },
  {
    "id": 20668,
    "package_name": "sicure",
    "title": "Single-Index Mixture Cure Models",
    "description": "Single-index mixture cure models allow estimating the probability of cure and the latency depending on a vector (or functional) covariate, avoiding the curse of dimensionality. The vector of parameters that defines the model can be estimated by maximum likelihood. A nonparametric estimator for the conditional density of the susceptible population is provided. For more details, see Pi\u00f1eiro-Lamas (2024) (<https://ruc.udc.es/dspace/handle/2183/37035>). Funding: This work, integrated into the framework of PERTE for Vanguard Health, has been co-financed by the Spanish Ministry of Science, Innovation and Universities with funds from the European Union NextGenerationEU, from the Recovery, Transformation and Resilience Plan (PRTR-C17.I1) and from the Autonomous Community of Galicia within the framework of the Biotechnology Plan Applied to Health.",
    "version": "0.1.1",
    "maintainer": "Beatriz Pi\u00f1eiro-Lamas <b.pineiro.lamas@udc.es>",
    "author": "Beatriz Pi\u00f1eiro-Lamas [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0673-5377>),\n  Ana L\u00f3pez-Cheda [aut],\n  Ricardo Cao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sicure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sicure Single-Index Mixture Cure Models Single-index mixture cure models allow estimating the probability of cure and the latency depending on a vector (or functional) covariate, avoiding the curse of dimensionality. The vector of parameters that defines the model can be estimated by maximum likelihood. A nonparametric estimator for the conditional density of the susceptible population is provided. For more details, see Pi\u00f1eiro-Lamas (2024) (<https://ruc.udc.es/dspace/handle/2183/37035>). Funding: This work, integrated into the framework of PERTE for Vanguard Health, has been co-financed by the Spanish Ministry of Science, Innovation and Universities with funds from the European Union NextGenerationEU, from the Recovery, Transformation and Resilience Plan (PRTR-C17.I1) and from the Autonomous Community of Galicia within the framework of the Biotechnology Plan Applied to Health.  "
  },
  {
    "id": 20697,
    "package_name": "sim2Dpredictr",
    "title": "Simulate Outcomes Using Spatially Dependent Design Matrices",
    "description": "Provides tools for simulating spatially dependent predictors (continuous or binary),\n    which are used to generate scalar outcomes in a (generalized) linear model framework. Continuous\n    predictors are generated using traditional multivariate normal distributions or Gauss Markov random\n    fields with several correlation function approaches (e.g., see Rue (2001) <doi:10.1111/1467-9868.00288>\n    and Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>), while binary predictors are generated using\n    a Boolean model (see Cressie and Wikle (2011, ISBN: 978-0-471-69274-4)). Parameter vectors \n\texhibiting spatial clustering can also be easily specified by the user.  ",
    "version": "0.1.1",
    "maintainer": "Justin Leach <jleach@uab.edu>",
    "author": "Justin Leach [aut, cre, cph]",
    "url": "https://github.com/jmleach-bst/sim2Dpredictr",
    "bug_reports": "https://github.com/jmleach-bst/sim2Dpredictr",
    "repository": "https://cran.r-project.org/package=sim2Dpredictr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sim2Dpredictr Simulate Outcomes Using Spatially Dependent Design Matrices Provides tools for simulating spatially dependent predictors (continuous or binary),\n    which are used to generate scalar outcomes in a (generalized) linear model framework. Continuous\n    predictors are generated using traditional multivariate normal distributions or Gauss Markov random\n    fields with several correlation function approaches (e.g., see Rue (2001) <doi:10.1111/1467-9868.00288>\n    and Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>), while binary predictors are generated using\n    a Boolean model (see Cressie and Wikle (2011, ISBN: 978-0-471-69274-4)). Parameter vectors \n\texhibiting spatial clustering can also be easily specified by the user.    "
  },
  {
    "id": 20818,
    "package_name": "sjSDM",
    "title": "Scalable Joint Species Distribution Modeling",
    "description": "A scalable and fast method for estimating joint Species Distribution Models (jSDMs) for big community data, including eDNA data. The package estimates a full (i.e. non-latent) jSDM with different response distributions (including the traditional multivariate probit model). The package allows to perform variation partitioning (VP) / ANOVA on the fitted models to separate the contribution of environmental, spatial, and biotic associations. In addition, the total R-squared can be further partitioned per species and site to reveal the internal metacommunity structure, see Leibold et al., <doi:10.1111/oik.08618>. The internal structure can then be regressed against environmental and spatial distinctiveness, richness, and traits to analyze metacommunity assembly processes.  The package includes support for accounting for spatial autocorrelation and the option to fit responses using deep neural networks instead of a standard linear predictor. As described in Pichler & Hartig (2021) <doi:10.1111/2041-210X.13687>, scalability is achieved by using a Monte Carlo approximation of the joint likelihood implemented via 'PyTorch' and 'reticulate', which can be run on CPUs or GPUs.",
    "version": "1.0.7",
    "maintainer": "Maximilian Pichler <maximilian.pichler@biologie.uni-regensburg.de>",
    "author": "Maximilian Pichler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2252-8327>),\n  Florian Hartig [aut] (ORCID: <https://orcid.org/0000-0002-6255-9059>),\n  Wang Cai [ctb]",
    "url": "https://github.com/TheoreticalEcology/s-jSDM/",
    "bug_reports": "https://github.com/TheoreticalEcology/s-jSDM/issues",
    "repository": "https://cran.r-project.org/package=sjSDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sjSDM Scalable Joint Species Distribution Modeling A scalable and fast method for estimating joint Species Distribution Models (jSDMs) for big community data, including eDNA data. The package estimates a full (i.e. non-latent) jSDM with different response distributions (including the traditional multivariate probit model). The package allows to perform variation partitioning (VP) / ANOVA on the fitted models to separate the contribution of environmental, spatial, and biotic associations. In addition, the total R-squared can be further partitioned per species and site to reveal the internal metacommunity structure, see Leibold et al., <doi:10.1111/oik.08618>. The internal structure can then be regressed against environmental and spatial distinctiveness, richness, and traits to analyze metacommunity assembly processes.  The package includes support for accounting for spatial autocorrelation and the option to fit responses using deep neural networks instead of a standard linear predictor. As described in Pichler & Hartig (2021) <doi:10.1111/2041-210X.13687>, scalability is achieved by using a Monte Carlo approximation of the joint likelihood implemented via 'PyTorch' and 'reticulate', which can be run on CPUs or GPUs.  "
  },
  {
    "id": 20820,
    "package_name": "sjlabelled",
    "title": "Labelled Data Utility Functions",
    "description": "Collection of functions dealing with labelled data, like reading and \n    writing data between R and other statistical software packages like 'SPSS',\n    'SAS' or 'Stata', and working with labelled data. This includes easy ways \n    to get, set or change value and variable label attributes, to convert \n    labelled vectors into factors or numeric (and vice versa), or to deal with \n    multiple declared missing values.",
    "version": "1.2.0",
    "maintainer": "Daniel L\u00fcdecke <d.luedecke@uke.de>",
    "author": "Daniel L\u00fcdecke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8895-3206>),\n  avid Ranzolin [ctb],\n  Jonathan De Troye [ctb]",
    "url": "https://strengejacke.github.io/sjlabelled/",
    "bug_reports": "https://github.com/strengejacke/sjlabelled/issues",
    "repository": "https://cran.r-project.org/package=sjlabelled",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sjlabelled Labelled Data Utility Functions Collection of functions dealing with labelled data, like reading and \n    writing data between R and other statistical software packages like 'SPSS',\n    'SAS' or 'Stata', and working with labelled data. This includes easy ways \n    to get, set or change value and variable label attributes, to convert \n    labelled vectors into factors or numeric (and vice versa), or to deal with \n    multiple declared missing values.  "
  },
  {
    "id": 20857,
    "package_name": "slendr",
    "title": "A Simulation Framework for Spatiotemporal Population Genetics",
    "description": "A framework for simulating spatially explicit genomic data which\n    leverages real cartographic information for programmatic and visual encoding\n    of spatiotemporal population dynamics on real geographic landscapes. Population\n    genetic models are then automatically executed by the 'SLiM' software by Haller\n    et al. (2019) <doi:10.1093/molbev/msy228> behind the scenes, using a custom\n    built-in simulation 'SLiM' script. Additionally, fully abstract spatial models\n    not tied to a specific geographic location are supported, and users can also\n    simulate data from standard, non-spatial, random-mating models. These can be\n    simulated either with the 'SLiM' built-in back-end script, or using an efficient\n    coalescent population genetics simulator 'msprime' by Baumdicker et al. (2022)\n    <doi:10.1093/genetics/iyab229> with a custom-built 'Python' script bundled with the\n    R package. Simulated genomic data is saved in a tree-sequence format and can be\n    loaded, manipulated, and summarised using tree-sequence functionality via an R\n    interface to the 'Python' module 'tskit' by Kelleher et al. (2019)\n    <doi:10.1038/s41588-019-0483-y>. Complete model configuration, simulation and\n    analysis pipelines can be therefore constructed without a need to leave the R\n    environment, eliminating friction between disparate tools for population genetic\n    simulations and data analysis.",
    "version": "1.3.0",
    "maintainer": "Martin Petr <contact@bodkan.net>",
    "author": "Martin Petr [aut, cre] (ORCID: <https://orcid.org/0000-0003-4879-8421>)",
    "url": "https://github.com/bodkan/slendr",
    "bug_reports": "https://github.com/bodkan/slendr/issues",
    "repository": "https://cran.r-project.org/package=slendr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "slendr A Simulation Framework for Spatiotemporal Population Genetics A framework for simulating spatially explicit genomic data which\n    leverages real cartographic information for programmatic and visual encoding\n    of spatiotemporal population dynamics on real geographic landscapes. Population\n    genetic models are then automatically executed by the 'SLiM' software by Haller\n    et al. (2019) <doi:10.1093/molbev/msy228> behind the scenes, using a custom\n    built-in simulation 'SLiM' script. Additionally, fully abstract spatial models\n    not tied to a specific geographic location are supported, and users can also\n    simulate data from standard, non-spatial, random-mating models. These can be\n    simulated either with the 'SLiM' built-in back-end script, or using an efficient\n    coalescent population genetics simulator 'msprime' by Baumdicker et al. (2022)\n    <doi:10.1093/genetics/iyab229> with a custom-built 'Python' script bundled with the\n    R package. Simulated genomic data is saved in a tree-sequence format and can be\n    loaded, manipulated, and summarised using tree-sequence functionality via an R\n    interface to the 'Python' module 'tskit' by Kelleher et al. (2019)\n    <doi:10.1038/s41588-019-0483-y>. Complete model configuration, simulation and\n    analysis pipelines can be therefore constructed without a need to leave the R\n    environment, eliminating friction between disparate tools for population genetic\n    simulations and data analysis.  "
  },
  {
    "id": 20862,
    "package_name": "slippymath",
    "title": "Slippy Map Tile Tools",
    "description": "Provides functions for performing common tasks when working with\n  slippy map tile service APIs e.g. Google maps, Open Street Map, Mapbox, Stamen,\n  among others. Functionality includes converting from latitude and longitude to\n  tile numbers, determining tile bounding boxes, and compositing tiles to a\n  georeferenced raster image.",
    "version": "0.3.1",
    "maintainer": "Miles McBain <miles.mcbain@gmail.com>",
    "author": "Miles McBain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2865-2548>),\n  Michael Sumner [aut]",
    "url": "https://www.github.com/milesmcbain/slippymath",
    "bug_reports": "https://www.github.com/milesmcbain/slippymath/issues",
    "repository": "https://cran.r-project.org/package=slippymath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "slippymath Slippy Map Tile Tools Provides functions for performing common tasks when working with\n  slippy map tile service APIs e.g. Google maps, Open Street Map, Mapbox, Stamen,\n  among others. Functionality includes converting from latitude and longitude to\n  tile numbers, determining tile bounding boxes, and compositing tiles to a\n  georeferenced raster image.  "
  },
  {
    "id": 20872,
    "package_name": "smacof",
    "title": "Multidimensional Scaling",
    "description": "Implements the following approaches for multidimensional scaling (MDS) based on stress minimization using majorization (smacof): ratio/interval/ordinal/spline MDS on symmetric dissimilarity matrices, MDS with external constraints on the configuration, individual differences scaling (idioscal, indscal), MDS with spherical restrictions, and ratio/interval/ordinal/spline unfolding (circular restrictions, row-conditional). Various tools and extensions like jackknife MDS, bootstrap MDS, permutation tests, MDS biplots, gravity models, unidimensional scaling, drift vectors (asymmetric MDS), classical scaling, and Procrustes are implemented as well.  ",
    "version": "2.1-7",
    "maintainer": "Patrick Mair <mair@fas.harvard.edu>",
    "author": "Patrick Mair [aut, cre],\n  Jan De Leeuw [aut],\n  Patrick J. F. Groenen [aut],\n  Ingwer Borg [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=smacof",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smacof Multidimensional Scaling Implements the following approaches for multidimensional scaling (MDS) based on stress minimization using majorization (smacof): ratio/interval/ordinal/spline MDS on symmetric dissimilarity matrices, MDS with external constraints on the configuration, individual differences scaling (idioscal, indscal), MDS with spherical restrictions, and ratio/interval/ordinal/spline unfolding (circular restrictions, row-conditional). Various tools and extensions like jackknife MDS, bootstrap MDS, permutation tests, MDS biplots, gravity models, unidimensional scaling, drift vectors (asymmetric MDS), classical scaling, and Procrustes are implemented as well.    "
  },
  {
    "id": 20880,
    "package_name": "smartmap",
    "title": "Smartly Create Maps from R Objects",
    "description": "Preview spatial data as 'leaflet' maps with minimal\n  effort. smartmap is optimized for interactive use and distinguishes itself \n  from similar packages because it does not need real spatial ('sp' or 'sf')\n  objects an input; instead, it tries to automatically coerce everything that \n  looks like spatial data to sf objects or leaflet maps. It - for example -  \n  supports direct mapping of: a vector containing a single coordinate pair,\n  a two column matrix, a data.frame with longitude and latitude columns, or\n  the path or URL to a (possibly compressed) 'shapefile'.",
    "version": "0.1.1",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=smartmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smartmap Smartly Create Maps from R Objects Preview spatial data as 'leaflet' maps with minimal\n  effort. smartmap is optimized for interactive use and distinguishes itself \n  from similar packages because it does not need real spatial ('sp' or 'sf')\n  objects an input; instead, it tries to automatically coerce everything that \n  looks like spatial data to sf objects or leaflet maps. It - for example -  \n  supports direct mapping of: a vector containing a single coordinate pair,\n  a two column matrix, a data.frame with longitude and latitude columns, or\n  the path or URL to a (possibly compressed) 'shapefile'.  "
  },
  {
    "id": 20939,
    "package_name": "snapKrig",
    "title": "Fast Kriging and Geostatistics on Grids with Kronecker\nCovariance",
    "description": "Geostatistical modeling and kriging with\n    gridded data using spatially separable covariance functions (Kronecker\n    covariances). Kronecker products in these models provide shortcuts for\n    solving large matrix problems in likelihood and conditional mean,\n    making 'snapKrig' computationally efficient with large grids. The package\n    supplies its own S3 grid object class, and a host of methods including\n    plot, print, Ops, square bracket replace/assign, and more. Our computational\n    methods are described in Koch, Lele, Lewis (2020) <doi:10.7939/r3-g6qb-bq70>.",
    "version": "0.0.2",
    "maintainer": "Dean Koch <dkoch@ualberta.ca>",
    "author": "Dean Koch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8849-859X>)",
    "url": "https://github.com/deankoch/snapKrig",
    "bug_reports": "https://github.com/deankoch/snapKrig/issues",
    "repository": "https://cran.r-project.org/package=snapKrig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snapKrig Fast Kriging and Geostatistics on Grids with Kronecker\nCovariance Geostatistical modeling and kriging with\n    gridded data using spatially separable covariance functions (Kronecker\n    covariances). Kronecker products in these models provide shortcuts for\n    solving large matrix problems in likelihood and conditional mean,\n    making 'snapKrig' computationally efficient with large grids. The package\n    supplies its own S3 grid object class, and a host of methods including\n    plot, print, Ops, square bracket replace/assign, and more. Our computational\n    methods are described in Koch, Lele, Lewis (2020) <doi:10.7939/r3-g6qb-bq70>.  "
  },
  {
    "id": 20961,
    "package_name": "snvecR",
    "title": "Calculate Earth\u2019s Obliquity and Precession in the Past",
    "description": "Easily calculate precession and obliquity from an orbital solution (defaults to ZB18a from Zeebe and Lourens (2019) <doi:10.1126/science.aax0612>) and assumed or reconstructed values for tidal dissipation (Td) and dynamical ellipticity (Ed). This is a translation and adaptation of the 'C'-code in the supplementary material to Zeebe and Lourens (2022) <doi:10.1029/2021PA004349>, with further details on the methodology described in Zeebe (2022) <doi:10.3847/1538-3881/ac80f8>. The name of the 'C'-routine is 'snvec', which refers to the key units of computation: spin vector s and orbit normal vector n.",
    "version": "3.10.1",
    "maintainer": "Ilja Kocken <ikocken@hawaii.edu>",
    "author": "Ilja Kocken [aut, cre, trl, cph] (ORCID:\n    <https://orcid.org/0000-0003-2196-8718>),\n  Richard Zeebe [aut] (ORCID: <https://orcid.org/0000-0003-0806-8387>)",
    "url": "https://japhir.github.io/snvecR/, https://github.com/japhir/snvecR",
    "bug_reports": "https://github.com/japhir/snvecR/issues",
    "repository": "https://cran.r-project.org/package=snvecR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snvecR Calculate Earth\u2019s Obliquity and Precession in the Past Easily calculate precession and obliquity from an orbital solution (defaults to ZB18a from Zeebe and Lourens (2019) <doi:10.1126/science.aax0612>) and assumed or reconstructed values for tidal dissipation (Td) and dynamical ellipticity (Ed). This is a translation and adaptation of the 'C'-code in the supplementary material to Zeebe and Lourens (2022) <doi:10.1029/2021PA004349>, with further details on the methodology described in Zeebe (2022) <doi:10.3847/1538-3881/ac80f8>. The name of the 'C'-routine is 'snvec', which refers to the key units of computation: spin vector s and orbit normal vector n.  "
  },
  {
    "id": 20998,
    "package_name": "sommer",
    "title": "Solving Mixed Model Equations in R",
    "description": "Structural multivariate-univariate linear mixed model solver for estimation of multiple random effects with unknown variance-covariance structures (e.g., heterogeneous and unstructured) and known covariance among levels of random effects (e.g., pedigree and genomic relationship matrices) (Covarrubias-Pazaran, 2016 <doi:10.1371/journal.pone.0156744>; Maier et al., 2015 <doi:10.1016/j.ajhg.2014.12.006>; Jensen et al., 1997). REML estimates can be obtained using the Direct-Inversion Newton-Raphson and Direct-Inversion Average Information algorithms for the problems r x r (r being the number of records) or using the Henderson-based average information algorithm for the problem c x c (c being the number of coefficients to estimate). Spatial models can also be fitted using the two-dimensional spline functionality available.",
    "version": "4.4.4",
    "maintainer": "Giovanny Covarrubias-Pazaran <cova_ruber@live.com.mx>",
    "author": "Giovanny Covarrubias-Pazaran [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7194-3837>)",
    "url": "https://github.com/covaruber/sommer",
    "bug_reports": "https://github.com/covaruber/sommer/issues",
    "repository": "https://cran.r-project.org/package=sommer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sommer Solving Mixed Model Equations in R Structural multivariate-univariate linear mixed model solver for estimation of multiple random effects with unknown variance-covariance structures (e.g., heterogeneous and unstructured) and known covariance among levels of random effects (e.g., pedigree and genomic relationship matrices) (Covarrubias-Pazaran, 2016 <doi:10.1371/journal.pone.0156744>; Maier et al., 2015 <doi:10.1016/j.ajhg.2014.12.006>; Jensen et al., 1997). REML estimates can be obtained using the Direct-Inversion Newton-Raphson and Direct-Inversion Average Information algorithms for the problems r x r (r being the number of records) or using the Henderson-based average information algorithm for the problem c x c (c being the number of coefficients to estimate). Spatial models can also be fitted using the two-dimensional spline functionality available.  "
  },
  {
    "id": 21021,
    "package_name": "spANOVA",
    "title": "Analysis of Field Trials with Geostatistics & Spatial AR Models",
    "description": "Perform analysis of variance when the experimental units are spatially correlated. There are two methods to deal with spatial dependence: Spatial autoregressive models (see Rossoni, D. F., & Lima, R. R. (2019) <doi:10.28951/rbb.v37i2.388>) and geostatistics (see Pontes, J. M., & Oliveira, M. S. D. (2004) <doi:10.1590/S1413-70542004000100018>). For both methods, there are three multicomparison procedure available: Tukey, multivariate T, and Scott-Knott.",
    "version": "0.99.4",
    "maintainer": "Castro L. R. <lucasroberto.castro@gmail.com>",
    "author": "Castro L. R. [aut, cre, cph],\n  Renato R. R. [aut, ths],\n  Rossoni D. F. [aut],\n  Nogueira C.H. [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spANOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spANOVA Analysis of Field Trials with Geostatistics & Spatial AR Models Perform analysis of variance when the experimental units are spatially correlated. There are two methods to deal with spatial dependence: Spatial autoregressive models (see Rossoni, D. F., & Lima, R. R. (2019) <doi:10.28951/rbb.v37i2.388>) and geostatistics (see Pontes, J. M., & Oliveira, M. S. D. (2004) <doi:10.1590/S1413-70542004000100018>). For both methods, there are three multicomparison procedure available: Tukey, multivariate T, and Scott-Knott.  "
  },
  {
    "id": 21022,
    "package_name": "spAbundance",
    "title": "Univariate and Multivariate Spatial Modeling of Species\nAbundance",
    "description": "Fits single-species (univariate) and multi-species (multivariate) non-spatial and spatial abundance models in a Bayesian framework using Markov Chain Monte Carlo (MCMC). Spatial models are fit using Nearest Neighbor Gaussian Processes (NNGPs). Details on NNGP models are given in Datta, Banerjee, Finley, and Gelfand (2016) <doi:10.1080/01621459.2015.1044091> and Finley, Datta, and Banerjee (2022) <doi:10.18637/jss.v103.i05>. Fits single-species and multi-species spatial and non-spatial versions of generalized linear mixed models (Gaussian, Poisson, Negative Binomial), N-mixture models (Royle 2004 <doi:10.1111/j.0006-341X.2004.00142.x>) and hierarchical distance sampling models (Royle, Dawson, Bates (2004) <doi:10.1890/03-3127>). Multi-species spatial models are fit using a spatial factor modeling approach with NNGPs for computational efficiency. ",
    "version": "0.2.1",
    "maintainer": "Jeffrey Doser <jwdoser@ncsu.edu>",
    "author": "Jeffrey Doser [aut, cre],\n  Andrew Finley [aut]",
    "url": "https://www.doserlab.com/files/spabundance-web\nhttps://groups.google.com/g/spocc-spabund-users",
    "bug_reports": "https://github.com/biodiverse/spAbundance/issues",
    "repository": "https://cran.r-project.org/package=spAbundance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spAbundance Univariate and Multivariate Spatial Modeling of Species\nAbundance Fits single-species (univariate) and multi-species (multivariate) non-spatial and spatial abundance models in a Bayesian framework using Markov Chain Monte Carlo (MCMC). Spatial models are fit using Nearest Neighbor Gaussian Processes (NNGPs). Details on NNGP models are given in Datta, Banerjee, Finley, and Gelfand (2016) <doi:10.1080/01621459.2015.1044091> and Finley, Datta, and Banerjee (2022) <doi:10.18637/jss.v103.i05>. Fits single-species and multi-species spatial and non-spatial versions of generalized linear mixed models (Gaussian, Poisson, Negative Binomial), N-mixture models (Royle 2004 <doi:10.1111/j.0006-341X.2004.00142.x>) and hierarchical distance sampling models (Royle, Dawson, Bates (2004) <doi:10.1890/03-3127>). Multi-species spatial models are fit using a spatial factor modeling approach with NNGPs for computational efficiency.   "
  },
  {
    "id": 21025,
    "package_name": "spBPS",
    "title": "Bayesian Predictive Stacking for Scalable Geospatial Transfer\nLearning",
    "description": "Provides functions for Bayesian Predictive Stacking within the Bayesian transfer learning framework for geospatial artificial systems, as introduced in \"Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach\" (Presicce and Banerjee, 2024) <doi:10.48550/arXiv.2410.09504>. This methodology enables efficient Bayesian geostatistical modeling, utilizing predictive stacking to improve inference across spatial datasets. The core functions leverage 'C++' for high-performance computation, making the framework well-suited for large-scale spatial data analysis in parallel and distributed computing environments. Designed for scalability, it allows seamless application in computationally demanding scenarios.",
    "version": "0.0-4",
    "maintainer": "Luca Presicce <l.presicce@campus.unimib.it>",
    "author": "Luca Presicce [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-7062-3523>),\n  Sudipto Banerjee [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spBPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spBPS Bayesian Predictive Stacking for Scalable Geospatial Transfer\nLearning Provides functions for Bayesian Predictive Stacking within the Bayesian transfer learning framework for geospatial artificial systems, as introduced in \"Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach\" (Presicce and Banerjee, 2024) <doi:10.48550/arXiv.2410.09504>. This methodology enables efficient Bayesian geostatistical modeling, utilizing predictive stacking to improve inference across spatial datasets. The core functions leverage 'C++' for high-performance computation, making the framework well-suited for large-scale spatial data analysis in parallel and distributed computing environments. Designed for scalability, it allows seamless application in computationally demanding scenarios.  "
  },
  {
    "id": 21029,
    "package_name": "spData",
    "title": "Datasets for Spatial Analysis",
    "description": "Diverse spatial datasets for demonstrating, benchmarking and teaching spatial data analysis. \n    It includes R data of class sf (defined by the package 'sf'), Spatial ('sp'), and nb ('spdep').\n    Unlike other spatial data packages such as 'rnaturalearth' and 'maps', \n    it also contains data stored in a range of file formats including GeoJSON and GeoPackage, but from version 2.3.4, no longer ESRI Shapefile - use GeoPackage instead. \n    Some of the datasets are designed to illustrate specific analysis techniques.\n    cycle_hire() and cycle_hire_osm(), for example, is designed to illustrate point pattern analysis techniques.",
    "version": "2.3.4",
    "maintainer": "Jakub Nowosad <nowosad.jakub@gmail.com>",
    "author": "Roger Bivand [aut] (ORCID: <https://orcid.org/0000-0003-2392-6140>),\n  Jakub Nowosad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1057-3721>),\n  Robin Lovelace [aut] (ORCID: <https://orcid.org/0000-0001-5679-6536>),\n  Angelos Mimis [ctb],\n  Mark Monmonier [ctb] (author of the state.vbm dataset),\n  Greg Snow [ctb] (author of the state.vbm dataset)",
    "url": "https://jakubnowosad.com/spData/",
    "bug_reports": "https://github.com/Nowosad/spData/issues",
    "repository": "https://cran.r-project.org/package=spData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spData Datasets for Spatial Analysis Diverse spatial datasets for demonstrating, benchmarking and teaching spatial data analysis. \n    It includes R data of class sf (defined by the package 'sf'), Spatial ('sp'), and nb ('spdep').\n    Unlike other spatial data packages such as 'rnaturalearth' and 'maps', \n    it also contains data stored in a range of file formats including GeoJSON and GeoPackage, but from version 2.3.4, no longer ESRI Shapefile - use GeoPackage instead. \n    Some of the datasets are designed to illustrate specific analysis techniques.\n    cycle_hire() and cycle_hire_osm(), for example, is designed to illustrate point pattern analysis techniques.  "
  },
  {
    "id": 21039,
    "package_name": "spNetwork",
    "title": "Spatial Analysis on Network",
    "description": "Perform spatial analysis on network.\n    Implement several methods for spatial analysis on network: Network Kernel Density estimation, \n    building of spatial matrices based on network distance ('listw' objects from 'spdep' package), K functions estimation \n    for point pattern analysis on network, k nearest neighbours on network, reachable area calculation, and graph generation\n    References: Okabe et al (2019) <doi:10.1080/13658810802475491>;\n    Okabe et al (2012, ISBN:978-0470770818);Baddeley et al (2015, ISBN:9781482210200).",
    "version": "0.4.4.7",
    "maintainer": "Jeremy Gelb <jeremy.gelb@ucs.inrs.ca>",
    "author": "Jeremy Gelb [aut, cre] (ORCID: <https://orcid.org/0000-0002-7114-2714>),\n  Philippe Apparicio [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6466-9342>)",
    "url": "https://jeremygelb.github.io/spNetwork/",
    "bug_reports": "https://github.com/JeremyGelb/spNetwork/issues",
    "repository": "https://cran.r-project.org/package=spNetwork",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spNetwork Spatial Analysis on Network Perform spatial analysis on network.\n    Implement several methods for spatial analysis on network: Network Kernel Density estimation, \n    building of spatial matrices based on network distance ('listw' objects from 'spdep' package), K functions estimation \n    for point pattern analysis on network, k nearest neighbours on network, reachable area calculation, and graph generation\n    References: Okabe et al (2019) <doi:10.1080/13658810802475491>;\n    Okabe et al (2012, ISBN:978-0470770818);Baddeley et al (2015, ISBN:9781482210200).  "
  },
  {
    "id": 21040,
    "package_name": "spOccupancy",
    "title": "Single-Species, Multi-Species, and Integrated Spatial Occupancy\nModels",
    "description": "Fits single-species, multi-species, and integrated non-spatial and spatial occupancy models using Markov Chain Monte Carlo (MCMC). Models are fit using Polya-Gamma data augmentation detailed in Polson, Scott, and Windle (2013) <doi:10.1080/01621459.2013.829001>. Spatial models are fit using either Gaussian processes or Nearest Neighbor Gaussian Processes (NNGP) for large spatial datasets. Details on NNGP models are given in Datta, Banerjee, Finley, and Gelfand (2016) <doi:10.1080/01621459.2015.1044091> and Finley, Datta, and Banerjee (2022) <doi:10.18637/jss.v103.i05>. Provides functionality for data integration of multiple single-species occupancy data sets using a joint likelihood framework. Details on data integration are given in Miller, Pacifici, Sanderlin, and Reich (2019) <doi:10.1111/2041-210X.13110>. Details on single-species and multi-species models are found in MacKenzie, Nichols, Lachman, Droege, Royle, and Langtimm (2002) <doi:10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2> and Dorazio and Royle <doi:10.1198/016214505000000015>, respectively. ",
    "version": "0.8.0",
    "maintainer": "Jeffrey Doser <jwdoser@ncsu.edu>",
    "author": "Jeffrey Doser [aut, cre],\n  Andrew Finley [aut],\n  Marc Kery [ctb]",
    "url": "https://www.doserlab.com/files/spoccupancy-web,\nhttps://groups.google.com/g/spocc-spabund-users,\nhttps://github.com/biodiverse/spOccupancy",
    "bug_reports": "https://github.com/biodiverse/spOccupancy/issues",
    "repository": "https://cran.r-project.org/package=spOccupancy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spOccupancy Single-Species, Multi-Species, and Integrated Spatial Occupancy\nModels Fits single-species, multi-species, and integrated non-spatial and spatial occupancy models using Markov Chain Monte Carlo (MCMC). Models are fit using Polya-Gamma data augmentation detailed in Polson, Scott, and Windle (2013) <doi:10.1080/01621459.2013.829001>. Spatial models are fit using either Gaussian processes or Nearest Neighbor Gaussian Processes (NNGP) for large spatial datasets. Details on NNGP models are given in Datta, Banerjee, Finley, and Gelfand (2016) <doi:10.1080/01621459.2015.1044091> and Finley, Datta, and Banerjee (2022) <doi:10.18637/jss.v103.i05>. Provides functionality for data integration of multiple single-species occupancy data sets using a joint likelihood framework. Details on data integration are given in Miller, Pacifici, Sanderlin, and Reich (2019) <doi:10.1111/2041-210X.13110>. Details on single-species and multi-species models are found in MacKenzie, Nichols, Lachman, Droege, Royle, and Langtimm (2002) <doi:10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2> and Dorazio and Royle <doi:10.1198/016214505000000015>, respectively.   "
  },
  {
    "id": 21042,
    "package_name": "spStack",
    "title": "Bayesian Geostatistics Using Predictive Stacking",
    "description": "Fits Bayesian hierarchical spatial and spatial-temporal process\n    models for point-referenced Gaussian, Poisson, binomial, and binary data\n    using stacking of predictive densities. It involves sampling from\n    analytically available posterior distributions conditional upon candidate\n    values of the spatial process parameters and, subsequently assimilate\n    inference from these individual posterior distributions using Bayesian\n    predictive stacking. Our algorithm is highly parallelizable and hence, much\n    faster than traditional Markov chain Monte Carlo algorithms while delivering\n    competitive predictive performance. See Zhang, Tang, and Banerjee (2025)\n    <doi:10.1080/01621459.2025.2566449>, and, Pan, Zhang, Bradley, and Banerjee\n    (2025) <doi:10.48550/arXiv.2406.04655> for details.",
    "version": "1.1.2",
    "maintainer": "Soumyakanti Pan <span18@ucla.edu>",
    "author": "Soumyakanti Pan [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-9889-7112>),\n  Sudipto Banerjee [aut]",
    "url": "https://span-18.github.io/spStack-dev/",
    "bug_reports": "https://github.com/SPan-18/spStack-dev/issues",
    "repository": "https://cran.r-project.org/package=spStack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spStack Bayesian Geostatistics Using Predictive Stacking Fits Bayesian hierarchical spatial and spatial-temporal process\n    models for point-referenced Gaussian, Poisson, binomial, and binary data\n    using stacking of predictive densities. It involves sampling from\n    analytically available posterior distributions conditional upon candidate\n    values of the spatial process parameters and, subsequently assimilate\n    inference from these individual posterior distributions using Bayesian\n    predictive stacking. Our algorithm is highly parallelizable and hence, much\n    faster than traditional Markov chain Monte Carlo algorithms while delivering\n    competitive predictive performance. See Zhang, Tang, and Banerjee (2025)\n    <doi:10.1080/01621459.2025.2566449>, and, Pan, Zhang, Bradley, and Banerjee\n    (2025) <doi:10.48550/arXiv.2406.04655> for details.  "
  },
  {
    "id": 21046,
    "package_name": "spaMM",
    "title": "Mixed-Effect Models, with or without Spatial Random Effects",
    "description": "Inference based on models with or without spatially-correlated random effects, multivariate responses, or non-Gaussian random effects (e.g., Beta). Variation in residual variance (heteroscedasticity) can itself be represented by a mixed-effect model. Both classical geostatistical models (Rousset and Ferdy 2014 <doi:10.1111/ecog.00566>), and Markov random field models on irregular grids (as considered in the 'INLA' package, <https://www.r-inla.org>), can be fitted, with distinct computational procedures exploiting the sparse matrix representations for the latter case and other autoregressive models. Laplace approximations are used for likelihood or restricted  likelihood. Penalized quasi-likelihood and other variants discussed in the h-likelihood literature (Lee and Nelder 2001 <doi:10.1093/biomet/88.4.987>) are also implemented. ",
    "version": "4.6.1",
    "maintainer": "Fran\u00e7ois Rousset <francois.rousset@umontpellier.fr>",
    "author": "Fran\u00e7ois Rousset [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4670-0371>),\n  Jean-Baptiste Ferdy [aut, cph],\n  Alexandre Courtiol [aut] (ORCID:\n    <https://orcid.org/0000-0003-0637-2959>)",
    "url": "https://gitlab.mbb.univ-montp2.fr/francois/spamm-ref",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spaMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spaMM Mixed-Effect Models, with or without Spatial Random Effects Inference based on models with or without spatially-correlated random effects, multivariate responses, or non-Gaussian random effects (e.g., Beta). Variation in residual variance (heteroscedasticity) can itself be represented by a mixed-effect model. Both classical geostatistical models (Rousset and Ferdy 2014 <doi:10.1111/ecog.00566>), and Markov random field models on irregular grids (as considered in the 'INLA' package, <https://www.r-inla.org>), can be fitted, with distinct computational procedures exploiting the sparse matrix representations for the latter case and other autoregressive models. Laplace approximations are used for likelihood or restricted  likelihood. Penalized quasi-likelihood and other variants discussed in the h-likelihood literature (Lee and Nelder 2001 <doi:10.1093/biomet/88.4.987>) are also implemented.   "
  },
  {
    "id": 21058,
    "package_name": "spanish",
    "title": "Translate Quantities from Strings to Integer and Back. Misc\nFunctions on Spanish Data",
    "description": "Character vector to numerical translation in Euros from Spanish\n    spelled monetary quantities. Reverse translation from integer to Spanish.\n    Upper limit is up to the millions range. Geocoding via Cadastral web site.",
    "version": "0.4.2",
    "maintainer": "Jose Manuel Vera Oteo <vera.josemanuel@gmail.com>",
    "author": "Jose Manuel Vera Oteo [aut, cre]",
    "url": "https://ropenspain.github.io/spanish/",
    "bug_reports": "https://github.com/verajosemanuel/spanish/issues",
    "repository": "https://cran.r-project.org/package=spanish",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spanish Translate Quantities from Strings to Integer and Back. Misc\nFunctions on Spanish Data Character vector to numerical translation in Euros from Spanish\n    spelled monetary quantities. Reverse translation from integer to Spanish.\n    Upper limit is up to the millions range. Geocoding via Cadastral web site.  "
  },
  {
    "id": 21079,
    "package_name": "sparseEigen",
    "title": "Computation of Sparse Eigenvectors of a Matrix",
    "description": "Computation of sparse eigenvectors of a matrix (aka sparse PCA)\n    with running time 2-3 orders of magnitude lower than existing methods and\n    better final performance in terms of recovery of sparsity pattern and \n    estimation of numerical values. Can handle covariance matrices as well as \n    data matrices with real or complex-valued entries. Different levels of \n    sparsity can be specified for each individual ordered eigenvector and the \n    method is robust in parameter selection. See vignette for a detailed \n    documentation and comparison, with several illustrative examples. \n    The package is based on the paper:\n    K. Benidis, Y. Sun, P. Babu, and D. P. Palomar (2016). \"Orthogonal Sparse PCA \n    and Covariance Estimation via Procrustes Reformulation,\" IEEE Transactions on \n    Signal Processing <doi:10.1109/TSP.2016.2605073>.",
    "version": "0.1.0",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Konstantinos Benidis [aut],\n  Daniel P. Palomar [cre, aut]",
    "url": "https://github.com/dppalomar/sparseEigen,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2016.2605073",
    "bug_reports": "https://github.com/dppalomar/sparseEigen/issues",
    "repository": "https://cran.r-project.org/package=sparseEigen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseEigen Computation of Sparse Eigenvectors of a Matrix Computation of sparse eigenvectors of a matrix (aka sparse PCA)\n    with running time 2-3 orders of magnitude lower than existing methods and\n    better final performance in terms of recovery of sparsity pattern and \n    estimation of numerical values. Can handle covariance matrices as well as \n    data matrices with real or complex-valued entries. Different levels of \n    sparsity can be specified for each individual ordered eigenvector and the \n    method is robust in parameter selection. See vignette for a detailed \n    documentation and comparison, with several illustrative examples. \n    The package is based on the paper:\n    K. Benidis, Y. Sun, P. Babu, and D. P. Palomar (2016). \"Orthogonal Sparse PCA \n    and Covariance Estimation via Procrustes Reformulation,\" IEEE Transactions on \n    Signal Processing <doi:10.1109/TSP.2016.2605073>.  "
  },
  {
    "id": 21091,
    "package_name": "sparseSVM",
    "title": "Solution Paths of Sparse High-Dimensional Support Vector Machine\nwith Lasso or Elastic-Net Regularization",
    "description": "Offers a fast algorithm for fitting solution paths of sparse SVM models with lasso or elastic-net regularization. Reference: Congrui Yi and Jian Huang (2017) <doi:10.1080/10618600.2016.1256816>.",
    "version": "1.1-7",
    "maintainer": "Congrui Yi <eric.ycr@gmail.com>",
    "author": "Congrui Yi [aut, cre],\n  Yaohui Zeng [aut]",
    "url": "https://github.com/CY-dev/sparseSVM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sparseSVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseSVM Solution Paths of Sparse High-Dimensional Support Vector Machine\nwith Lasso or Elastic-Net Regularization Offers a fast algorithm for fitting solution paths of sparse SVM models with lasso or elastic-net regularization. Reference: Congrui Yi and Jian Huang (2017) <doi:10.1080/10618600.2016.1256816>.  "
  },
  {
    "id": 21097,
    "package_name": "sparsepca",
    "title": "Sparse Principal Component Analysis (SPCA)",
    "description": "Sparse principal component analysis (SPCA) attempts to find sparse weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach provides better interpretability for the principal components in high-dimensional data settings. This is, because the principal components are formed as a linear combination of only a few of the original variables. This package provides efficient routines to compute SPCA. Specifically, a variable projection solver is used to compute the sparse solution. In addition, a fast randomized accelerated SPCA routine and a robust SPCA routine is provided. Robust SPCA allows to capture grossly corrupted entries in the data. The methods are discussed in detail by N. Benjamin Erichson et al. (2018) <arXiv:1804.00341>. ",
    "version": "0.1.2",
    "maintainer": "N. Benjamin Erichson <erichson@uw.edu>",
    "author": "N. Benjamin Erichson, Peng Zheng, and Sasha Aravkin",
    "url": "https://github.com/erichson/spca",
    "bug_reports": "https://github.com/erichson/spca/issues",
    "repository": "https://cran.r-project.org/package=sparsepca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparsepca Sparse Principal Component Analysis (SPCA) Sparse principal component analysis (SPCA) attempts to find sparse weight vectors (loadings), i.e., a weight vector with only a few 'active' (nonzero) values. This approach provides better interpretability for the principal components in high-dimensional data settings. This is, because the principal components are formed as a linear combination of only a few of the original variables. This package provides efficient routines to compute SPCA. Specifically, a variable projection solver is used to compute the sparse solution. In addition, a fast randomized accelerated SPCA routine and a robust SPCA routine is provided. Robust SPCA allows to capture grossly corrupted entries in the data. The methods are discussed in detail by N. Benjamin Erichson et al. (2018) <arXiv:1804.00341>.   "
  },
  {
    "id": 21109,
    "package_name": "spatemR",
    "title": "Generalized Spatial Autoregresive Models for Mean and Variance",
    "description": "Modeling spatial dependencies in dependent variables, extending traditional spatial regression approaches. It allows for the joint modeling of both the mean and the variance of the dependent variable, incorporating semiparametric effects in both models. Based on generalized additive models (GAM), the package enables the inclusion of non-parametric terms while maintaining the classical theoretical framework of spatial regression. Additionally, it implements the Generalized Spatial Autoregression (GSAR) model, which extends classical methods like logistic Spatial Autoregresive Models (SAR), probit Spatial Autoregresive Models (SAR), and Poisson Spatial Autoregresive Models (SAR), offering greater flexibility in modeling spatial dependencies and significantly improving computational efficiency and the statistical properties of the estimators. Related work includes: a) J.D. Toloza-Delgado, Melo O.O., Cruz N.A. (2024). \"Joint spatial modeling of mean and non-homogeneous variance combining semiparametric SAR and GAMLSS models for hedonic prices\". <doi:10.1016/j.spasta.2024.100864>. b) Cruz, N. A., Toloza-Delgado, J. D., Melo, O. O. (2024). \"Generalized spatial autoregressive model\". <doi:10.48550/arXiv.2412.00945>. ",
    "version": "1.2.0",
    "maintainer": "Nelson Alirio Cruz Gutierrez <nelson-alirio.cruz@uib.es>",
    "author": "Nelson Alirio Cruz Gutierrez [aut, cre, cph],\n  Oscar Orlando Melo [aut],\n  Jurgen Toloza-Delgado [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spatemR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatemR Generalized Spatial Autoregresive Models for Mean and Variance Modeling spatial dependencies in dependent variables, extending traditional spatial regression approaches. It allows for the joint modeling of both the mean and the variance of the dependent variable, incorporating semiparametric effects in both models. Based on generalized additive models (GAM), the package enables the inclusion of non-parametric terms while maintaining the classical theoretical framework of spatial regression. Additionally, it implements the Generalized Spatial Autoregression (GSAR) model, which extends classical methods like logistic Spatial Autoregresive Models (SAR), probit Spatial Autoregresive Models (SAR), and Poisson Spatial Autoregresive Models (SAR), offering greater flexibility in modeling spatial dependencies and significantly improving computational efficiency and the statistical properties of the estimators. Related work includes: a) J.D. Toloza-Delgado, Melo O.O., Cruz N.A. (2024). \"Joint spatial modeling of mean and non-homogeneous variance combining semiparametric SAR and GAMLSS models for hedonic prices\". <doi:10.1016/j.spasta.2024.100864>. b) Cruz, N. A., Toloza-Delgado, J. D., Melo, O. O. (2024). \"Generalized spatial autoregressive model\". <doi:10.48550/arXiv.2412.00945>.   "
  },
  {
    "id": 21113,
    "package_name": "spatial",
    "title": "Functions for Kriging and Point Pattern Analysis",
    "description": "Functions for kriging and point pattern analysis.",
    "version": "7.3-18",
    "maintainer": "Brian Ripley <Brian.Ripley@R-project.org>",
    "author": "Brian Ripley [aut, cre, cph],\n  Roger Bivand [ctb],\n  William Venables [cph]",
    "url": "http://www.stats.ox.ac.uk/pub/MASS4/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatial Functions for Kriging and Point Pattern Analysis Functions for kriging and point pattern analysis.  "
  },
  {
    "id": 21115,
    "package_name": "spatialCovariance",
    "title": "Computation of Spatial Covariance Matrices for Data on\nRectangles",
    "description": "Functions that compute the spatial covariance matrix for the matern and power classes of spatial models, for data that arise on rectangular units.  This code can also be used for the change of support problem and for spatial data that arise on irregularly shaped regions like counties or zipcodes by laying a fine grid of rectangles and aggregating the integrals in a form of Riemann integration.",
    "version": "0.6-9",
    "maintainer": "David Clifford <david.clifford+CRAN@gmail.com>",
    "author": "David Clifford <david.clifford+CRAN@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spatialCovariance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialCovariance Computation of Spatial Covariance Matrices for Data on\nRectangles Functions that compute the spatial covariance matrix for the matern and power classes of spatial models, for data that arise on rectangular units.  This code can also be used for the change of support problem and for spatial data that arise on irregularly shaped regions like counties or zipcodes by laying a fine grid of rectangles and aggregating the integrals in a form of Riemann integration.  "
  },
  {
    "id": 21116,
    "package_name": "spatialEco",
    "title": "Spatial Analysis and Modelling Utilities",
    "description": "Utilities to support spatial data manipulation, query, sampling\n    and modelling in ecological applications. Functions include models for species \n\tpopulation density, spatial smoothing, multivariate separability, point process \n\tmodel for creating pseudo- absences and sub-sampling, Quadrant-based sampling and \n\tanalysis, auto-logistic modeling, sampling models, cluster optimization, statistical \n\texploratory tools and raster-based metrics.",
    "version": "2.0-3",
    "maintainer": "Jeffrey S. Evans <jeffrey_evans@tnc.org>",
    "author": "Jeffrey S. Evans [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5533-7044>),\n  Melanie A. Murphy [ctb],\n  Karthik Ram [ctb]",
    "url": "https://github.com/jeffreyevans/spatialEco,\nhttps://jeffreyevans.github.io/spatialEco/",
    "bug_reports": "https://github.com/jeffreyevans/spatialEco/issues",
    "repository": "https://cran.r-project.org/package=spatialEco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialEco Spatial Analysis and Modelling Utilities Utilities to support spatial data manipulation, query, sampling\n    and modelling in ecological applications. Functions include models for species \n\tpopulation density, spatial smoothing, multivariate separability, point process \n\tmodel for creating pseudo- absences and sub-sampling, Quadrant-based sampling and \n\tanalysis, auto-logistic modeling, sampling models, cluster optimization, statistical \n\texploratory tools and raster-based metrics.  "
  },
  {
    "id": 21118,
    "package_name": "spatialRF",
    "title": "Easy Spatial Modeling with Random Forest",
    "description": "Automatic generation and selection of spatial predictors for spatial regression with Random Forest. Spatial predictors are surrogates of variables driving the spatial structure of a response variable. The package offers two methods to generate spatial predictors from a distance matrix among training cases: 1) Moran's Eigenvector Maps (MEMs; Dray, Legendre, and Peres-Neto 2006 <DOI:10.1016/j.ecolmodel.2006.02.015>): computed as the eigenvectors of a weighted matrix of distances; 2) RFsp (Hengl et al. <DOI:10.7717/peerj.5518>): columns of the distance matrix used as spatial predictors. Spatial predictors help minimize the spatial autocorrelation of the model residuals and facilitate an honest assessment of the importance scores of the non-spatial predictors. Additionally, functions to reduce multicollinearity, identify relevant variable interactions, tune random forest hyperparameters, assess model transferability via spatial cross-validation, and explore model results via partial dependence curves and interaction surfaces are included in the package. The modelling functions are built around the highly efficient 'ranger' package (Wright and Ziegler 2017 <DOI:10.18637/jss.v077.i01>).  ",
    "version": "1.1.4",
    "maintainer": "Blas M. Benito <blasbenito@gmail.com>",
    "author": "Blas M. Benito [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5105-7232>)",
    "url": "https://blasbenito.github.io/spatialRF/",
    "bug_reports": "https://github.com/BlasBenito/spatialRF/issues/",
    "repository": "https://cran.r-project.org/package=spatialRF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialRF Easy Spatial Modeling with Random Forest Automatic generation and selection of spatial predictors for spatial regression with Random Forest. Spatial predictors are surrogates of variables driving the spatial structure of a response variable. The package offers two methods to generate spatial predictors from a distance matrix among training cases: 1) Moran's Eigenvector Maps (MEMs; Dray, Legendre, and Peres-Neto 2006 <DOI:10.1016/j.ecolmodel.2006.02.015>): computed as the eigenvectors of a weighted matrix of distances; 2) RFsp (Hengl et al. <DOI:10.7717/peerj.5518>): columns of the distance matrix used as spatial predictors. Spatial predictors help minimize the spatial autocorrelation of the model residuals and facilitate an honest assessment of the importance scores of the non-spatial predictors. Additionally, functions to reduce multicollinearity, identify relevant variable interactions, tune random forest hyperparameters, assess model transferability via spatial cross-validation, and explore model results via partial dependence curves and interaction surfaces are included in the package. The modelling functions are built around the highly efficient 'ranger' package (Wright and Ziegler 2017 <DOI:10.18637/jss.v077.i01>).    "
  },
  {
    "id": 21119,
    "package_name": "spatialTIME",
    "title": "Spatial Analysis of Vectra Immunoflourescent Data",
    "description": "Visualization and analysis  of Vectra Immunoflourescent\n    data. Options for calculating both the univariate and bivariate Ripley's K\n    are included. Calculations are performed using a permutation-based \n    approach presented by Wilson et al.  <doi:10.1101/2021.04.27.21256104>. ",
    "version": "1.3.4-5",
    "maintainer": "Fridley Lab <fridley.lab@moffitt.org>",
    "author": "Jordan Creed [aut],\n  Ram Thapa [aut],\n  Christopher Wilson [aut],\n  Alex Soupir [aut],\n  Oscar Ospina [aut],\n  Julia Wrobel [aut],\n  Brooke Fridley [cph],\n  Fridley Lab [cre]",
    "url": "https://github.com/FridleyLab/spatialTIME",
    "bug_reports": "https://github.com/FridleyLab/spatialTIME/issues",
    "repository": "https://cran.r-project.org/package=spatialTIME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialTIME Spatial Analysis of Vectra Immunoflourescent Data Visualization and analysis  of Vectra Immunoflourescent\n    data. Options for calculating both the univariate and bivariate Ripley's K\n    are included. Calculations are performed using a permutation-based \n    approach presented by Wilson et al.  <doi:10.1101/2021.04.27.21256104>.   "
  },
  {
    "id": 21120,
    "package_name": "spatialising",
    "title": "Ising Model for Spatial Data",
    "description": "Performs simulations of binary spatial raster data using\n    the Ising model (Ising (1925) <doi:10.1007/BF02980577>; \n    Onsager (1944) <doi:10.1103/PhysRev.65.117>). It allows to set a few\n    parameters that represent internal and external pressures, and the number \n    of simulations (Stepinski and Nowosad (2023) <doi:10.1098/rsos.231005>).",
    "version": "0.6.2",
    "maintainer": "Jakub Nowosad <nowosad.jakub@gmail.com>",
    "author": "Jakub Nowosad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1057-3721>)",
    "url": "https://jakubnowosad.com/spatialising/",
    "bug_reports": "https://github.com/Nowosad/spatialising/issues",
    "repository": "https://cran.r-project.org/package=spatialising",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialising Ising Model for Spatial Data Performs simulations of binary spatial raster data using\n    the Ising model (Ising (1925) <doi:10.1007/BF02980577>; \n    Onsager (1944) <doi:10.1103/PhysRev.65.117>). It allows to set a few\n    parameters that represent internal and external pressures, and the number \n    of simulations (Stepinski and Nowosad (2023) <doi:10.1098/rsos.231005>).  "
  },
  {
    "id": 21124,
    "package_name": "spatialwarnings",
    "title": "Spatial Early Warning Signals of Ecosystem Degradation",
    "description": "Tools to compute and assess significance of early-warnings signals (EWS) of ecosystem degradation. EWS are spatial metrics derived from raster data -- e.g. spatial autocorrelation -- that increase before an ecosystem undergoes a non-linear transition (Genin et al. (2018) <doi:10.1111/2041-210X.13058>).",
    "version": "3.1.1",
    "maintainer": "Alexandre Genin <alexandre.genin@sete.cnrs.fr>",
    "author": "Alain Danet [aut],\n  Alexandre Genin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3333-1338>),\n  Vishwesha Guttal [aut],\n  Sonia Kefi [aut],\n  Sabiha Majumder [aut],\n  Sumithra Sankaran [aut],\n  Florian Schneider [aut]",
    "url": "https://github.com/spatial-ews/spatialwarnings",
    "bug_reports": "https://github.com/spatial-ews/spatialwarnings/issues",
    "repository": "https://cran.r-project.org/package=spatialwarnings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialwarnings Spatial Early Warning Signals of Ecosystem Degradation Tools to compute and assess significance of early-warnings signals (EWS) of ecosystem degradation. EWS are spatial metrics derived from raster data -- e.g. spatial autocorrelation -- that increase before an ecosystem undergoes a non-linear transition (Genin et al. (2018) <doi:10.1111/2041-210X.13058>).  "
  },
  {
    "id": 21155,
    "package_name": "spdynmod",
    "title": "Spatio-Dynamic Wetland Plant Communities Model",
    "description": "A spatio-dynamic modelling package that focuses on three\n    characteristic wetland plant communities in a semiarid Mediterranean\n    wetland in response to hydrological pressures from the catchment. The\n    package includes the data on watershed hydrological pressure and the\n    initial raster maps of plant communities but also allows for random initial\n    distribution of plant communities. For more detailed info see: Martinez-Lopez et al. (2015) <doi:10.1016/j.ecolmodel.2014.11.024>.",
    "version": "1.1.6",
    "maintainer": "Javier Martinez-Lopez <javi.martinez.lopez@gmail.com>",
    "author": "Javier Martinez-Lopez <javi.martinez.lopez@gmail.com>, Babak Naimi\n    <naimi.b@gmail.com>, Julia Martinez-Fernandez <juliamf@um.es>",
    "url": "https://github.com/javimarlop/spdynmod",
    "bug_reports": "https://github.com/javimarlop/spdynmod/issues",
    "repository": "https://cran.r-project.org/package=spdynmod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spdynmod Spatio-Dynamic Wetland Plant Communities Model A spatio-dynamic modelling package that focuses on three\n    characteristic wetland plant communities in a semiarid Mediterranean\n    wetland in response to hydrological pressures from the catchment. The\n    package includes the data on watershed hydrological pressure and the\n    initial raster maps of plant communities but also allows for random initial\n    distribution of plant communities. For more detailed info see: Martinez-Lopez et al. (2015) <doi:10.1016/j.ecolmodel.2014.11.024>.  "
  },
  {
    "id": 21176,
    "package_name": "spectrolab",
    "title": "Class and Methods for Spectral Data",
    "description": "Input/Output, processing and visualization of spectra taken with different spectrometers, including SVC (Spectra Vista), ASD and PSR (Spectral Evolution). Implements an S3 class spectra that other packages can build on. Provides methods to access, plot, manipulate, splice sensor overlap, vector normalize and smooth spectra.",
    "version": "0.0.19",
    "maintainer": "Jose Eduardo Meireles <jemeireles@gmail.com>",
    "author": "Jose Eduardo Meireles [aut, cre],\n  Anna K. Schweiger [aut],\n  Jeannine Cavender-Bares [aut]",
    "url": "https://CRAN.R-project.org/package=spectrolab",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spectrolab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spectrolab Class and Methods for Spectral Data Input/Output, processing and visualization of spectra taken with different spectrometers, including SVC (Spectra Vista), ASD and PSR (Spectral Evolution). Implements an S3 class spectra that other packages can build on. Provides methods to access, plot, manipulate, splice sensor overlap, vector normalize and smooth spectra.  "
  },
  {
    "id": 21187,
    "package_name": "spell.replacer",
    "title": "Probabilistic Spelling Correction in a Character Vector",
    "description": "Automatically replaces \"misspelled\" words in a character vector\n    based on their string distance from a list of words sorted by their frequency\n    in a corpus. The default word list provided in the package comes from \n    the Corpus of Contemporary American English. Uses the Jaro-Winkler distance\n    metric for string similarity as implemented in van der Loo (2014) \n    <doi:10.32614/RJ-2014-011>. The word frequency data is derived from \n    Davies (2008-) \"The Corpus of Contemporary American English (COCA)\" \n    <https://www.english-corpora.org/coca/>.",
    "version": "1.0.1",
    "maintainer": "David Brown <dwb2@andrew.cmu.edu>",
    "author": "David Brown [aut, cre] (ORCID: <https://orcid.org/0000-0001-7745-6354>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spell.replacer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spell.replacer Probabilistic Spelling Correction in a Character Vector Automatically replaces \"misspelled\" words in a character vector\n    based on their string distance from a list of words sorted by their frequency\n    in a corpus. The default word list provided in the package comes from \n    the Corpus of Contemporary American English. Uses the Jaro-Winkler distance\n    metric for string similarity as implemented in van der Loo (2014) \n    <doi:10.32614/RJ-2014-011>. The word frequency data is derived from \n    Davies (2008-) \"The Corpus of Contemporary American English (COCA)\" \n    <https://www.english-corpora.org/coca/>.  "
  },
  {
    "id": 21192,
    "package_name": "spex",
    "title": "Spatial Extent Tools",
    "description": "Functions to produce a fully fledged 'geo-spatial' object extent as a\n    'SpatialPolygonsDataFrame'. Also included are functions to generate polygons\n    from raster data using 'quadmesh' techniques, a round number buffered extent, and\n    general spatial-extent and 'raster-like' extent helpers missing from the originating\n    packages. Some latitude-based tools for polar maps are included. ",
    "version": "0.7.1",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre]",
    "url": "https://mdsumner.github.io/spex/",
    "bug_reports": "https://github.com/mdsumner/spex/issues",
    "repository": "https://cran.r-project.org/package=spex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spex Spatial Extent Tools Functions to produce a fully fledged 'geo-spatial' object extent as a\n    'SpatialPolygonsDataFrame'. Also included are functions to generate polygons\n    from raster data using 'quadmesh' techniques, a round number buffered extent, and\n    general spatial-extent and 'raster-like' extent helpers missing from the originating\n    packages. Some latitude-based tools for polar maps are included.   "
  },
  {
    "id": 21195,
    "package_name": "spfilteR",
    "title": "Semiparametric Spatial Filtering with Eigenvectors in\n(Generalized) Linear Models",
    "description": "Tools to decompose (transformed) spatial connectivity matrices and perform supervised or unsupervised semiparametric spatial filtering in a regression framework. The package supports unsupervised spatial filtering in standard linear as well as some generalized linear regression models.",
    "version": "2.1.0",
    "maintainer": "Sebastian Juhl <sebastian.juhl@t-online.de>",
    "author": "Sebastian Juhl [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7123-5398>)",
    "url": "https://github.com/sjuhl/spfilteR",
    "bug_reports": "https://github.com/sjuhl/spfilteR/issues",
    "repository": "https://cran.r-project.org/package=spfilteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spfilteR Semiparametric Spatial Filtering with Eigenvectors in\n(Generalized) Linear Models Tools to decompose (transformed) spatial connectivity matrices and perform supervised or unsupervised semiparametric spatial filtering in a regression framework. The package supports unsupervised spatial filtering in standard linear as well as some generalized linear regression models.  "
  },
  {
    "id": 21217,
    "package_name": "spinyReg",
    "title": "Sparse Generative Model and Its EM Algorithm",
    "description": "Implements a generative model that uses a\n    spike-and-slab like prior distribution obtained by multiplying a\n    deterministic binary vector. Such a model allows an EM algorithm,\n    optimizing a type-II log-likelihood.",
    "version": "0.1-0",
    "maintainer": "Julien Chiquet <julien.chiquet@gmail.com>",
    "author": "Charles Bouveyron, Julien Chiquet, Pierre Latouche, Pierre-Alexandre\n    Mattei",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spinyReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spinyReg Sparse Generative Model and Its EM Algorithm Implements a generative model that uses a\n    spike-and-slab like prior distribution obtained by multiplying a\n    deterministic binary vector. Such a model allows an EM algorithm,\n    optimizing a type-II log-likelihood.  "
  },
  {
    "id": 21222,
    "package_name": "spldv",
    "title": "Spatial Models for Limited Dependent Variables",
    "description": "The current version of this package estimates spatial autoregressive models for binary dependent variables using GMM estimators <doi:10.18637/jss.v107.i08>. It supports one-step (Pinkse and Slade, 1998) <doi:10.1016/S0304-4076(97)00097-3> and two-step GMM estimator along with the linearized GMM estimator proposed by Klier and McMillen (2008) <doi:10.1198/073500107000000188>. It also allows for either Probit or Logit model and compute the average marginal effects. All these models are presented in Sarrias and Piras (2023) <doi:10.1016/j.jocm.2023.100432>. ",
    "version": "0.1.3",
    "maintainer": "Mauricio Sarrias <msarrias86@gmail.com>",
    "author": "Mauricio Sarrias [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5932-4817>),\n  Gianfranco Piras [aut] (ORCID: <https://orcid.org/0000-0003-0225-6061>),\n  Daniel McMillen [ctb]",
    "url": "https://github.com/gpiras/spldv",
    "bug_reports": "https://github.com/gpiras/spldv/issues",
    "repository": "https://cran.r-project.org/package=spldv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spldv Spatial Models for Limited Dependent Variables The current version of this package estimates spatial autoregressive models for binary dependent variables using GMM estimators <doi:10.18637/jss.v107.i08>. It supports one-step (Pinkse and Slade, 1998) <doi:10.1016/S0304-4076(97)00097-3> and two-step GMM estimator along with the linearized GMM estimator proposed by Klier and McMillen (2008) <doi:10.1198/073500107000000188>. It also allows for either Probit or Logit model and compute the average marginal effects. All these models are presented in Sarrias and Piras (2023) <doi:10.1016/j.jocm.2023.100432>.   "
  },
  {
    "id": 21241,
    "package_name": "spm",
    "title": "Spatial Predictive Modeling",
    "description": "Introduction to some novel accurate hybrid methods of geostatistical and machine learning methods for spatial predictive modelling. It contains two commonly used geostatistical methods, two machine learning methods, four hybrid methods and two averaging methods. For each method, two functions are provided. One function is for assessing the predictive errors and accuracy of the method based on cross-validation. The other one is for generating spatial predictions using the method. For details please see: Li, J., Potter, A., Huang, Z., Daniell, J. J. and Heap, A. (2010) <https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/71407>\n  Li, J., Heap, A. D., Potter, A., Huang, Z. and Daniell, J. (2011) <doi:10.1016/j.csr.2011.05.015>\n  Li, J., Heap, A. D., Potter, A. and Daniell, J. (2011) <doi:10.1016/j.envsoft.2011.07.004>\n  Li, J., Potter, A., Huang, Z. and Heap, A. (2012) <https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/74030>.",
    "version": "1.2.3",
    "maintainer": "Jin Li <jinli68@gmail.com>",
    "author": "Jin Li [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spm Spatial Predictive Modeling Introduction to some novel accurate hybrid methods of geostatistical and machine learning methods for spatial predictive modelling. It contains two commonly used geostatistical methods, two machine learning methods, four hybrid methods and two averaging methods. For each method, two functions are provided. One function is for assessing the predictive errors and accuracy of the method based on cross-validation. The other one is for generating spatial predictions using the method. For details please see: Li, J., Potter, A., Huang, Z., Daniell, J. J. and Heap, A. (2010) <https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/71407>\n  Li, J., Heap, A. D., Potter, A., Huang, Z. and Daniell, J. (2011) <doi:10.1016/j.csr.2011.05.015>\n  Li, J., Heap, A. D., Potter, A. and Daniell, J. (2011) <doi:10.1016/j.envsoft.2011.07.004>\n  Li, J., Potter, A., Huang, Z. and Heap, A. (2012) <https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/74030>.  "
  },
  {
    "id": 21242,
    "package_name": "spm2",
    "title": "Spatial Predictive Modeling",
    "description": "An updated and extended version of 'spm' package, by introducing some further novel functions for modern statistical methods (i.e., generalised linear models, glmnet, generalised least squares), thin plate splines, support vector machine, kriging methods (i.e., simple kriging, universal kriging, block kriging, kriging with an external drift), and novel hybrid methods (228 hybrids plus numerous variants) of modern statistical methods or machine learning methods with mathematical and/or univariate geostatistical methods for spatial predictive modelling. For each method, two functions are provided, with one function for assessing the predictive errors and accuracy of the method based on cross-validation, and the other for generating spatial predictions. It also contains a couple of functions for data preparation and predictive accuracy assessment. ",
    "version": "1.1.3",
    "maintainer": "Jin Li <jinli68@gmail.com>",
    "author": "Jin Li [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spm2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spm2 Spatial Predictive Modeling An updated and extended version of 'spm' package, by introducing some further novel functions for modern statistical methods (i.e., generalised linear models, glmnet, generalised least squares), thin plate splines, support vector machine, kriging methods (i.e., simple kriging, universal kriging, block kriging, kriging with an external drift), and novel hybrid methods (228 hybrids plus numerous variants) of modern statistical methods or machine learning methods with mathematical and/or univariate geostatistical methods for spatial predictive modelling. For each method, two functions are provided, with one function for assessing the predictive errors and accuracy of the method based on cross-validation, and the other for generating spatial predictions. It also contains a couple of functions for data preparation and predictive accuracy assessment.   "
  },
  {
    "id": 21244,
    "package_name": "spmoran",
    "title": "Fast Spatial and Spatio-Temporal Regression using Moran\nEigenvectors",
    "description": "A collection of functions for estimating spatial and spatio-temporal regression models. Moran eigenvectors are used as spatial basis functions to efficiently approximate spatially dependent Gaussian processes (i.e., random effects eigenvector spatial filtering; see Murakami and Griffith 2015 <doi: 10.1007/s10109-015-0213-7>). The implemented models include linear regression with residual spatial dependence, spatially/spatio-temporally varying coefficient models (Murakami et al., 2017, 2024; <doi:10.1016/j.spasta.2016.12.001>,<doi:10.48550/arXiv.2410.07229>), spatially filtered unconditional quantile regression (Murakami and Seya, 2019 <doi:10.1002/env.2556>), Gaussian and non-Gaussian spatial mixed models through compositionally-warping (Murakami et al. 2021, <doi:10.1016/j.spasta.2021.100520>).",
    "version": "0.3.3",
    "maintainer": "Daisuke Murakami <dmuraka@ism.ac.jp>",
    "author": "Daisuke Murakami [aut, cre]",
    "url": "https://github.com/dmuraka/spmoran",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spmoran",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spmoran Fast Spatial and Spatio-Temporal Regression using Moran\nEigenvectors A collection of functions for estimating spatial and spatio-temporal regression models. Moran eigenvectors are used as spatial basis functions to efficiently approximate spatially dependent Gaussian processes (i.e., random effects eigenvector spatial filtering; see Murakami and Griffith 2015 <doi: 10.1007/s10109-015-0213-7>). The implemented models include linear regression with residual spatial dependence, spatially/spatio-temporally varying coefficient models (Murakami et al., 2017, 2024; <doi:10.1016/j.spasta.2016.12.001>,<doi:10.48550/arXiv.2410.07229>), spatially filtered unconditional quantile regression (Murakami and Seya, 2019 <doi:10.1002/env.2556>), Gaussian and non-Gaussian spatial mixed models through compositionally-warping (Murakami et al. 2021, <doi:10.1016/j.spasta.2021.100520>).  "
  },
  {
    "id": 21246,
    "package_name": "spnn",
    "title": "Scale Invariant Probabilistic Neural Networks",
    "description": "Scale invariant version of the original PNN proposed by Specht (1990) <doi:10.1016/0893-6080(90)90049-q> with the added functionality of allowing for smoothing along multiple dimensions while accounting for covariances within the data set. It is written in the R statistical programming language. Given a data set with categorical variables, we use this algorithm to estimate the probabilities of a new observation vector belonging to a specific category. This type of neural network provides the benefits of fast training time relative to backpropagation and statistical generalization with only a small set of known observations.",
    "version": "1.3.0",
    "maintainer": "Romin Ebrahimi <romin.ebrahimi@utexas.edu>",
    "author": "Romin Ebrahimi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spnn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spnn Scale Invariant Probabilistic Neural Networks Scale invariant version of the original PNN proposed by Specht (1990) <doi:10.1016/0893-6080(90)90049-q> with the added functionality of allowing for smoothing along multiple dimensions while accounting for covariances within the data set. It is written in the R statistical programming language. Given a data set with categorical variables, we use this algorithm to estimate the probabilities of a new observation vector belonging to a specific category. This type of neural network provides the benefits of fast training time relative to backpropagation and statistical generalization with only a small set of known observations.  "
  },
  {
    "id": 21274,
    "package_name": "sptotal",
    "title": "Predicting Totals and Weighted Sums from Spatial Data",
    "description": "Performs predictions of totals and weighted sums, or finite population block kriging, on spatial data using the methods in Ver Hoef (2008) <doi:10.1007/s10651-007-0035-y>. The primary outputs are an estimate of the total, mean, or weighted sum in the region, an estimated prediction variance, and a plot of the predicted and observed values. This is useful primarily to users with ecological data that are counts or densities measured on some sites in a finite area of interest. Spatial prediction for the total count or average density in the entire region can then be done using the functions in this package. ",
    "version": "1.0.1",
    "maintainer": "Matt Higham <mhigham@stlawu.edu>",
    "author": "Matt Higham [cre, aut],\n  Jay Ver Hoef [aut],\n  Bryce Frank [aut],\n  Michael Dumelle [aut] (ORCID: <https://orcid.org/0000-0002-3393-5529>)",
    "url": "https://highamm.github.io/sptotal/index.html",
    "bug_reports": "https://github.com/highamm/sptotal/issues",
    "repository": "https://cran.r-project.org/package=sptotal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sptotal Predicting Totals and Weighted Sums from Spatial Data Performs predictions of totals and weighted sums, or finite population block kriging, on spatial data using the methods in Ver Hoef (2008) <doi:10.1007/s10651-007-0035-y>. The primary outputs are an estimate of the total, mean, or weighted sum in the region, an estimated prediction variance, and a plot of the predicted and observed values. This is useful primarily to users with ecological data that are counts or densities measured on some sites in a finite area of interest. Spatial prediction for the total count or average density in the entire region can then be done using the functions in this package.   "
  },
  {
    "id": 21317,
    "package_name": "ssebiEF",
    "title": "Calculation of SSEBI and Evaporative Fraction from Raster Data",
    "description": "Calculates a modified Simplified Surface Energy Balance Index (SSEBI) and the Evaporative Fraction (EF) using geospatial raster data such as albedo and surface-air temperature difference (TS\u2013TA). The SSEBI is computed from albedo and TS\u2013TA to estimate surface moisture and evaporative dynamics, providing a robust assessment of surface dryness while accounting for atmospheric variations. Based on Roerink, Su, and Menenti (2000) <doi:10.1016/S1464-1909(99)00128-8>.",
    "version": "1.0.1",
    "maintainer": "Gaelle Hamelin <gaelle.hamelin@institut-agro.fr>",
    "author": "Gaelle Hamelin [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-2148-7937>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ssebiEF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ssebiEF Calculation of SSEBI and Evaporative Fraction from Raster Data Calculates a modified Simplified Surface Energy Balance Index (SSEBI) and the Evaporative Fraction (EF) using geospatial raster data such as albedo and surface-air temperature difference (TS\u2013TA). The SSEBI is computed from albedo and TS\u2013TA to estimate surface moisture and evaporative dynamics, providing a robust assessment of surface dryness while accounting for atmospheric variations. Based on Roerink, Su, and Menenti (2000) <doi:10.1016/S1464-1909(99)00128-8>.  "
  },
  {
    "id": 21338,
    "package_name": "sstvars",
    "title": "Toolkit for Reduced Form and Structural Smooth Transition Vector\nAutoregressive Models",
    "description": "Penalized and non-penalized maximum likelihood estimation of smooth\n  transition vector autoregressive models with various types of transition weight\n  functions, conditional distributions, and identification methods. Constrained\n  estimation with various types of constraints is available. Residual based\n  model diagnostics, forecasting, simulations, counterfactual analysis, and\n  computation of impulse response functions, generalized impulse response functions,\n  generalized forecast error variance decompositions, as well as historical\n  decompositions. See\n  Heather Anderson, Farshid Vahid (1998) <doi:10.1016/S0304-4076(97)00076-6>,\n  Helmut L\u00fctkepohl, Aleksei Net\u0161unajev (2017) <doi:10.1016/j.jedc.2017.09.001>,\n  Markku Lanne, Savi Virolainen (2025) <doi:10.1016/j.jedc.2025.105162>,\n  Savi Virolainen (2025) <doi:10.48550/arXiv.2404.19707>.",
    "version": "1.2.2",
    "maintainer": "Savi Virolainen <savi.virolainen@helsinki.fi>",
    "author": "Savi Virolainen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5075-6821>)",
    "url": "https://github.com/saviviro/sstvars",
    "bug_reports": "https://github.com/saviviro/sstvars/issues",
    "repository": "https://cran.r-project.org/package=sstvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sstvars Toolkit for Reduced Form and Structural Smooth Transition Vector\nAutoregressive Models Penalized and non-penalized maximum likelihood estimation of smooth\n  transition vector autoregressive models with various types of transition weight\n  functions, conditional distributions, and identification methods. Constrained\n  estimation with various types of constraints is available. Residual based\n  model diagnostics, forecasting, simulations, counterfactual analysis, and\n  computation of impulse response functions, generalized impulse response functions,\n  generalized forecast error variance decompositions, as well as historical\n  decompositions. See\n  Heather Anderson, Farshid Vahid (1998) <doi:10.1016/S0304-4076(97)00076-6>,\n  Helmut L\u00fctkepohl, Aleksei Net\u0161unajev (2017) <doi:10.1016/j.jedc.2017.09.001>,\n  Markku Lanne, Savi Virolainen (2025) <doi:10.1016/j.jedc.2025.105162>,\n  Savi Virolainen (2025) <doi:10.48550/arXiv.2404.19707>.  "
  },
  {
    "id": 21346,
    "package_name": "sta",
    "title": "Seasonal Trend Analysis for Time Series Imagery in R",
    "description": "Efficiently estimate shape parameters of periodic time series \n    imagery with which  a statistical seasonal trend analysis (STA) is subsequently performed. \n    STA output can be exported in conventional raster formats. \n    Methods to visualize STA output are also implemented as well as the calculation \n    of additional basic statistics. STA is based on (R. Eastman, F. Sangermano, \n    B. Ghimire, H. Zhu, H. Chen, N. Neeti, Y. Cai, E. Machado and S. Crema, 2009) <doi:10.1080/01431160902755338>.",
    "version": "0.1.7",
    "maintainer": "Inder Tecuapetla-Gomez <itecuapetla@conabio.gob.mx>",
    "author": "Inder Tecuapetla-Gomez [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sta Seasonal Trend Analysis for Time Series Imagery in R Efficiently estimate shape parameters of periodic time series \n    imagery with which  a statistical seasonal trend analysis (STA) is subsequently performed. \n    STA output can be exported in conventional raster formats. \n    Methods to visualize STA output are also implemented as well as the calculation \n    of additional basic statistics. STA is based on (R. Eastman, F. Sangermano, \n    B. Ghimire, H. Zhu, H. Chen, N. Neeti, Y. Cai, E. Machado and S. Crema, 2009) <doi:10.1080/01431160902755338>.  "
  },
  {
    "id": 21379,
    "package_name": "starsExtra",
    "title": "Miscellaneous Functions for Working with 'stars' Rasters",
    "description": "Miscellaneous functions for working with 'stars' objects, mainly single-band rasters. Currently includes functions for: (1) focal filtering, (2) detrending of Digital Elevation Models, (3) calculating flow length, (4) calculating the Convergence Index, (5) calculating topographic aspect and topographic slope.",
    "version": "0.2.8",
    "maintainer": "Michael Dorman <dorman@post.bgu.ac.il>",
    "author": "Michael Dorman [aut, cre]",
    "url": "https://michaeldorman.github.io/starsExtra/,\nhttps://github.com/michaeldorman/starsExtra/",
    "bug_reports": "https://github.com/michaeldorman/starsExtra/issues/",
    "repository": "https://cran.r-project.org/package=starsExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starsExtra Miscellaneous Functions for Working with 'stars' Rasters Miscellaneous functions for working with 'stars' objects, mainly single-band rasters. Currently includes functions for: (1) focal filtering, (2) detrending of Digital Elevation Models, (3) calculating flow length, (4) calculating the Convergence Index, (5) calculating topographic aspect and topographic slope.  "
  },
  {
    "id": 21386,
    "package_name": "starvars",
    "title": "Vector Logistic Smooth Transition Models Estimation and\nPrediction",
    "description": "Allows the user to estimate a vector logistic smooth transition autoregressive model via maximum log-likelihood or nonlinear least squares. It further permits to test for linearity in the multivariate framework against a vector logistic smooth transition autoregressive model with a single transition variable. The estimation method is discussed in Terasvirta and Yang (2014, <doi:10.1108/S0731-9053(2013)0000031008>). Also, realized covariances can be constructed from stock market prices or returns, as explained in Andersen et al. (2001, <doi:10.1016/S0304-405X(01)00055-1>).",
    "version": "1.1.10",
    "maintainer": "Andrea Bucci <andrea.bucci@unich.it>",
    "author": "Andrea Bucci [aut, cre, cph],\n  Giulio Palomba [aut],\n  Eduardo Rossi [aut],\n  Andrea Faragalli [ctb]",
    "url": "https://github.com/andbucci/starvars",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=starvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starvars Vector Logistic Smooth Transition Models Estimation and\nPrediction Allows the user to estimate a vector logistic smooth transition autoregressive model via maximum log-likelihood or nonlinear least squares. It further permits to test for linearity in the multivariate framework against a vector logistic smooth transition autoregressive model with a single transition variable. The estimation method is discussed in Terasvirta and Yang (2014, <doi:10.1108/S0731-9053(2013)0000031008>). Also, realized covariances can be constructed from stock market prices or returns, as explained in Andersen et al. (2001, <doi:10.1016/S0304-405X(01)00055-1>).  "
  },
  {
    "id": 21403,
    "package_name": "statgenHTP",
    "title": "High Throughput Phenotyping (HTP) Data Analysis",
    "description": "Phenotypic analysis of data coming from high throughput \n    phenotyping (HTP) platforms, including different types of outlier detection,\n    spatial analysis, and parameter estimation. The package is being developed\n    within the EPPN2020 project (<https://cordis.europa.eu/project/id/731013>).\n    Some functions have been created to be used in conjunction with the R \n    package 'asreml' for the 'ASReml' software, which can be obtained upon \n    purchase from 'VSN' international (<https://vsni.co.uk/software/asreml-r/>).",
    "version": "1.0.9.1",
    "maintainer": "Bart-Jan van Rossum <bart-jan.vanrossum@wur.nl>",
    "author": "Emilie J Millet [aut] (ORCID: <https://orcid.org/0000-0002-2913-4892>),\n  Maria Xose Rodriguez Alvarez [aut] (ORCID:\n    <https://orcid.org/0000-0002-1329-9238>),\n  Diana Marcela Perez Valencia [aut] (ORCID:\n    <https://orcid.org/0000-0002-9053-2929>),\n  Isabelle Sanchez [aut],\n  Nadine Hilgert [aut],\n  Bart-Jan van Rossum [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8673-2514>),\n  Fred van Eeuwijk [aut] (ORCID: <https://orcid.org/0000-0003-3672-2921>),\n  Martin Boer [aut]",
    "url": "https://biometris.github.io/statgenHTP/index.html,\nhttps://github.com/Biometris/statgenHTP/",
    "bug_reports": "https://github.com/Biometris/statgenHTP/issues",
    "repository": "https://cran.r-project.org/package=statgenHTP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statgenHTP High Throughput Phenotyping (HTP) Data Analysis Phenotypic analysis of data coming from high throughput \n    phenotyping (HTP) platforms, including different types of outlier detection,\n    spatial analysis, and parameter estimation. The package is being developed\n    within the EPPN2020 project (<https://cordis.europa.eu/project/id/731013>).\n    Some functions have been created to be used in conjunction with the R \n    package 'asreml' for the 'ASReml' software, which can be obtained upon \n    purchase from 'VSN' international (<https://vsni.co.uk/software/asreml-r/>).  "
  },
  {
    "id": 21404,
    "package_name": "statgenIBD",
    "title": "Calculation of IBD Probabilities",
    "description": "For biparental, three and four-way crosses Identity by Descent \n    (IBD) probabilities can be calculated using Hidden Markov Models and \n    inheritance vectors following Lander and Green\n    (<https://www.jstor.org/stable/29713>) and Huang\n    (<doi:10.1073/pnas.1100465108>). One of a series of statistical genetic \n    packages for streamlining the analysis of typical plant breeding experiments\n    developed by Biometris.",
    "version": "1.0.9",
    "maintainer": "Bart-Jan van Rossum <bart-jan.vanrossum@wur.nl>",
    "author": "Martin Boer [aut] (ORCID: <https://orcid.org/0000-0002-1879-4588>),\n  Bart-Jan van Rossum [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8673-2514>),\n  Wenhao Li [ctb] (ORCID: <https://orcid.org/0000-0001-5719-5775>),\n  Johannes Kruisselbrink [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1673-5725>)",
    "url": "https://biometris.github.io/statgenIBD/index.html,\nhttps://github.com/Biometris/statgenIBD/",
    "bug_reports": "https://github.com/Biometris/statgenIBD/issues",
    "repository": "https://cran.r-project.org/package=statgenIBD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statgenIBD Calculation of IBD Probabilities For biparental, three and four-way crosses Identity by Descent \n    (IBD) probabilities can be calculated using Hidden Markov Models and \n    inheritance vectors following Lander and Green\n    (<https://www.jstor.org/stable/29713>) and Huang\n    (<doi:10.1073/pnas.1100465108>). One of a series of statistical genetic \n    packages for streamlining the analysis of typical plant breeding experiments\n    developed by Biometris.  "
  },
  {
    "id": 21409,
    "package_name": "statioVAR",
    "title": "Trend Removal for Vector Autoregressive Workflows",
    "description": "Detrending multivariate time-series to approximate stationarity when dealing with intensive longitudinal data, prior to Vector Autoregressive (VAR) or multilevel-VAR estimation. Classical VAR assumes weak stationarity (constant first two moments), and deterministic trends inflate spurious autocorrelation, biasing Granger-causality and impulse-response analyses. All functions operate on raw panel data and write detrended columns back to the data set, but differ in the level at which the trend is estimated. See, for instance, Wang & Maxwell (2015) <doi:10.1037/met0000030>; Burger et al. (2022) <doi:10.4324/9781003111238-13>; Epskamp et al. (2018) <doi:10.1177/2167702617744325>.  ",
    "version": "0.1.3",
    "maintainer": "Giuseppe Corbelli <giuseppe.corbelli@uniroma1.it>",
    "author": "Giuseppe Corbelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2864-3548>)",
    "url": "https://github.com/g-corbelli/statioVAR",
    "bug_reports": "https://github.com/g-corbelli/statioVAR/issues",
    "repository": "https://cran.r-project.org/package=statioVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statioVAR Trend Removal for Vector Autoregressive Workflows Detrending multivariate time-series to approximate stationarity when dealing with intensive longitudinal data, prior to Vector Autoregressive (VAR) or multilevel-VAR estimation. Classical VAR assumes weak stationarity (constant first two moments), and deterministic trends inflate spurious autocorrelation, biasing Granger-causality and impulse-response analyses. All functions operate on raw panel data and write detrended columns back to the data set, but differ in the level at which the trend is estimated. See, for instance, Wang & Maxwell (2015) <doi:10.1037/met0000030>; Burger et al. (2022) <doi:10.4324/9781003111238-13>; Epskamp et al. (2018) <doi:10.1177/2167702617744325>.    "
  },
  {
    "id": 21435,
    "package_name": "stdvectors",
    "title": "C++ Standard Library Vectors in R",
    "description": "Allows the creation and manipulation of C++ std::vector's in R.",
    "version": "0.0.5",
    "maintainer": "Marco Giuliano <mgiuliano.mail@gmail.com>",
    "author": "Marco Giuliano",
    "url": "https://github.com/digEmAll/stdvectors",
    "bug_reports": "https://github.com/digEmAll/stdvectors/issues",
    "repository": "https://cran.r-project.org/package=stdvectors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stdvectors C++ Standard Library Vectors in R Allows the creation and manipulation of C++ std::vector's in R.  "
  },
  {
    "id": 21467,
    "package_name": "stilt",
    "title": "Separable Gaussian Process Interpolation (Emulation)",
    "description": "Functions for simplified emulation of time series computer model output in model parameter space using Gaussian processes. Stilt can be used more generally for Kriging of spatio-temporal fields. There are functions to predict at new parameter settings, to test the emulator using cross-validation (which includes information on 95% confidence interval empirical coverage), and to produce contour plots over 2D slices in model parameter space.",
    "version": "1.3.1",
    "maintainer": "Kelsey Ruckert <datamgmt@scrim.psu.edu>",
    "author": "Roman Olson [aut],\n  Won Chang [aut],\n  Klaus Keller [aut],\n  Murali Haran [aut],\n  Kelsey Ruckert [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stilt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stilt Separable Gaussian Process Interpolation (Emulation) Functions for simplified emulation of time series computer model output in model parameter space using Gaussian processes. Stilt can be used more generally for Kriging of spatio-temporal fields. There are functions to predict at new parameter settings, to test the emulator using cross-validation (which includes information on 95% confidence interval empirical coverage), and to produce contour plots over 2D slices in model parameter space.  "
  },
  {
    "id": 21505,
    "package_name": "str2str",
    "title": "Convert R Objects from One Structure to Another",
    "description": "Offers a suite of functions for converting to and from\n    (atomic) vectors, matrices, data.frames, and (3D+) arrays as well\n    as lists of these objects. It is an alternative to the base R\n    as.<str>.<method>() functions (e.g., as.data.frame.array()) that\n    provides more useful and/or flexible restructuring of R objects.\n    To do so, it only works with common structuring of R objects (e.g.,\n    data.frames with only atomic vector columns).",
    "version": "1.0.0",
    "maintainer": "David Disabato <ddisab01@gmail.com>",
    "author": "David Disabato [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=str2str",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "str2str Convert R Objects from One Structure to Another Offers a suite of functions for converting to and from\n    (atomic) vectors, matrices, data.frames, and (3D+) arrays as well\n    as lists of these objects. It is an alternative to the base R\n    as.<str>.<method>() functions (e.g., as.data.frame.array()) that\n    provides more useful and/or flexible restructuring of R objects.\n    To do so, it only works with common structuring of R objects (e.g.,\n    data.frames with only atomic vector columns).  "
  },
  {
    "id": 21513,
    "package_name": "stratallo",
    "title": "Optimum Sample Allocation in Stratified Sampling",
    "description": "Functions in this package provide solution to classical problem in\n  survey methodology - an optimum sample allocation in stratified sampling. In\n  this context, the optimum allocation is in the classical Tschuprow-Neyman's\n  sense and it satisfies additional lower or upper bounds restrictions imposed\n  on sample sizes in strata. There are few different algorithms available to\n  use, and one them is based on popular sample allocation method that applies\n  Neyman allocation to recursively reduced set of strata.\n  This package also provides the function that computes a solution to the\n  minimum cost allocation problem, which is a minor modification of the\n  classical optimum sample allocation. This problem lies in the determination\n  of a vector of strata sample sizes that minimizes total cost of the survey,\n  under assumed fixed level of the stratified estimator's variance. As in the\n  case of the classical optimum allocation, the problem of minimum cost\n  allocation can be complemented by imposing upper-bounds constraints on sample\n  sizes in strata.",
    "version": "2.2.1",
    "maintainer": "Wojciech W\u00f3jciak <wojciech.wojciak@gmail.com>",
    "author": "Wojciech W\u00f3jciak [aut, cre],\n  Jacek Weso\u0142owski [sad],\n  Robert Wieczorkowski [ctb]",
    "url": "https://github.com/wwojciech/stratallo",
    "bug_reports": "https://github.com/wwojciech/stratallo/issues",
    "repository": "https://cran.r-project.org/package=stratallo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stratallo Optimum Sample Allocation in Stratified Sampling Functions in this package provide solution to classical problem in\n  survey methodology - an optimum sample allocation in stratified sampling. In\n  this context, the optimum allocation is in the classical Tschuprow-Neyman's\n  sense and it satisfies additional lower or upper bounds restrictions imposed\n  on sample sizes in strata. There are few different algorithms available to\n  use, and one them is based on popular sample allocation method that applies\n  Neyman allocation to recursively reduced set of strata.\n  This package also provides the function that computes a solution to the\n  minimum cost allocation problem, which is a minor modification of the\n  classical optimum sample allocation. This problem lies in the determination\n  of a vector of strata sample sizes that minimizes total cost of the survey,\n  under assumed fixed level of the stratified estimator's variance. As in the\n  case of the classical optimum allocation, the problem of minimum cost\n  allocation can be complemented by imposing upper-bounds constraints on sample\n  sizes in strata.  "
  },
  {
    "id": 21522,
    "package_name": "strawr",
    "title": "Fast Implementation of Reading/Dump for .hic Files",
    "description": "API for fast data extraction for .hic files that provides programmatic access to the matrices. It doesn't store the pointer data for all the matrices, only the one queried, and currently we are only supporting matrices (not vectors).",
    "version": "0.0.92",
    "maintainer": "Neva Cherniavsky Durand <neva@broadinstitute.org>",
    "author": "Neva Cherniavsky Durand [aut, cre],\n  Muhammad Saad Shamim [aut],\n  Aiden Lab [cph]",
    "url": "https://github.com/aidenlab/straw/tree/master/R",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=strawr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strawr Fast Implementation of Reading/Dump for .hic Files API for fast data extraction for .hic files that provides programmatic access to the matrices. It doesn't store the pointer data for all the matrices, only the one queried, and currently we are only supporting matrices (not vectors).  "
  },
  {
    "id": 21524,
    "package_name": "stream",
    "title": "Infrastructure for Data Stream Mining",
    "description": "A framework for data stream modeling and associated data\n    mining tasks such as clustering and classification. The development of\n    this package was supported in part by NSF IIS-0948893, NSF CMMI\n    1728612, and NIH R21HG005912. Hahsler et al (2017)\n    <doi:10.18637/jss.v076.i14>.",
    "version": "2.0-3",
    "maintainer": "Michael Hahsler <mhahsler@lyle.smu.edu>",
    "author": "Michael Hahsler [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2716-1405>),\n  Matthew Bola\u00f1os [ctb],\n  John Forrest [ctb],\n  Matthias Carnein [ctb],\n  Dennis Assenmacher [ctb],\n  Dalibor Krle\u017ea [ctb]",
    "url": "https://github.com/mhahsler/stream",
    "bug_reports": "https://github.com/mhahsler/stream/issues",
    "repository": "https://cran.r-project.org/package=stream",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stream Infrastructure for Data Stream Mining A framework for data stream modeling and associated data\n    mining tasks such as clustering and classification. The development of\n    this package was supported in part by NSF IIS-0948893, NSF CMMI\n    1728612, and NIH R21HG005912. Hahsler et al (2017)\n    <doi:10.18637/jss.v076.i14>.  "
  },
  {
    "id": 21539,
    "package_name": "stringdist",
    "title": "Approximate String Matching, Fuzzy Text Search, and String\nDistance Functions",
    "description": "Implements an approximate string matching version of R's native\n    'match' function. Also offers fuzzy text search based on various string\n     distance measures. Can calculate various string distances based on edits\n    (Damerau-Levenshtein, Hamming, Levenshtein, optimal sting alignment), qgrams (q-\n    gram, cosine, jaccard distance) or heuristic metrics (Jaro, Jaro-Winkler). An\n    implementation of soundex is provided as well. Distances can be computed between\n    character vectors while taking proper care of encoding or between integer\n    vectors representing generic sequences. This package is built for speed and\n    runs in parallel by using 'openMP'. An API for C or C++ is exposed as well.\n    Reference: MPJ van der Loo (2014) <doi:10.32614/RJ-2014-011>.",
    "version": "0.9.15",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9807-4686>),\n  Jan van der Laan [ctb],\n  R Core Team [ctb],\n  Nick Logan [ctb],\n  Chris Muir [ctb],\n  Johannes Gruber [ctb],\n  Brian Ripley [ctb]",
    "url": "https://github.com/markvanderloo/stringdist",
    "bug_reports": "https://github.com/markvanderloo/stringdist/issues",
    "repository": "https://cran.r-project.org/package=stringdist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stringdist Approximate String Matching, Fuzzy Text Search, and String\nDistance Functions Implements an approximate string matching version of R's native\n    'match' function. Also offers fuzzy text search based on various string\n     distance measures. Can calculate various string distances based on edits\n    (Damerau-Levenshtein, Hamming, Levenshtein, optimal sting alignment), qgrams (q-\n    gram, cosine, jaccard distance) or heuristic metrics (Jaro, Jaro-Winkler). An\n    implementation of soundex is provided as well. Distances can be computed between\n    character vectors while taking proper care of encoding or between integer\n    vectors representing generic sequences. This package is built for speed and\n    runs in parallel by using 'openMP'. An API for C or C++ is exposed as well.\n    Reference: MPJ van der Loo (2014) <doi:10.32614/RJ-2014-011>.  "
  },
  {
    "id": 21540,
    "package_name": "stringfish",
    "title": "Alt String Implementation",
    "description": "Provides an extendable, performant and multithreaded 'alt-string' implementation backed by 'C++' vectors and strings.",
    "version": "0.17.0",
    "maintainer": "Travers Ching <traversc@gmail.com>",
    "author": "Travers Ching [aut, cre, cph],\n  Phillip Hazel [ctb] (Bundled PCRE2 code),\n  Zoltan Herczeg [ctb, cph] (Bundled PCRE2 code),\n  University of Cambridge [cph] (Bundled PCRE2 code),\n  Tilera Corporation [cph] (Stack-less Just-In-Time compiler bundled with\n    PCRE2),\n  Yann Collet [ctb, cph] (Yann Collet is the author of the bundled xxHash\n    code)",
    "url": "https://github.com/traversc/stringfish",
    "bug_reports": "https://github.com/traversc/stringfish/issues",
    "repository": "https://cran.r-project.org/package=stringfish",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stringfish Alt String Implementation Provides an extendable, performant and multithreaded 'alt-string' implementation backed by 'C++' vectors and strings.  "
  },
  {
    "id": 21541,
    "package_name": "stringformattr",
    "title": "Dynamic String Formatting",
    "description": "Pass named and unnamed character vectors into specified positions\n    in strings. This represents an attempt to replicate some of python's string\n    formatting.",
    "version": "0.1.2",
    "maintainer": "Alexander Hoyle <alexander@alexanderhoyle.com>",
    "author": "Alexander Hoyle",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stringformattr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stringformattr Dynamic String Formatting Pass named and unnamed character vectors into specified positions\n    in strings. This represents an attempt to replicate some of python's string\n    formatting.  "
  },
  {
    "id": 21583,
    "package_name": "sufficientForecasting",
    "title": "Sufficient Forecasting using Factor Models",
    "description": "The sufficient forecasting (SF) method is implemented by this package for a single time series forecasting using many predictors and a possibly nonlinear forecasting function. Assuming that the predictors are driven by some latent factors, the SF first conducts factor analysis and then performs sufficient dimension reduction on the estimated factors to derive predictive indices for forecasting. The package implements several dimension reduction approaches, including principal components (PC), sliced inverse regression (SIR), and directional regression (DR). Methods for dimension reduction are as described in: Fan, J., Xue, L. and Yao, J. (2017) <doi:10.1016/j.jeconom.2017.08.009>, Luo, W., Xue, L., Yao, J. and Yu, X. (2022) <doi:10.1093/biomet/asab037> and Yu, X., Yao, J. and Xue, L. (2022) <doi:10.1080/07350015.2020.1813589>.",
    "version": "0.1.0",
    "maintainer": "Jing Fu <jingfu991224@outlook.com>",
    "author": "Jianqing Fan [aut],\n  Jing Fu [aut, cre],\n  Wei Luo [aut],\n  Lingzhou Xue [aut],\n  Jiawei Yao [aut],\n  Xiufan Yu [aut]",
    "url": "https://github.com/JingFu1224/sufficientForecasting",
    "bug_reports": "https://github.com/JingFu1224/sufficientForecasting/issues",
    "repository": "https://cran.r-project.org/package=sufficientForecasting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sufficientForecasting Sufficient Forecasting using Factor Models The sufficient forecasting (SF) method is implemented by this package for a single time series forecasting using many predictors and a possibly nonlinear forecasting function. Assuming that the predictors are driven by some latent factors, the SF first conducts factor analysis and then performs sufficient dimension reduction on the estimated factors to derive predictive indices for forecasting. The package implements several dimension reduction approaches, including principal components (PC), sliced inverse regression (SIR), and directional regression (DR). Methods for dimension reduction are as described in: Fan, J., Xue, L. and Yao, J. (2017) <doi:10.1016/j.jeconom.2017.08.009>, Luo, W., Xue, L., Yao, J. and Yu, X. (2022) <doi:10.1093/biomet/asab037> and Yu, X., Yao, J. and Xue, L. (2022) <doi:10.1080/07350015.2020.1813589>.  "
  },
  {
    "id": 21605,
    "package_name": "supercells",
    "title": "Superpixels of Spatial Data",
    "description": "Creates superpixels based on input spatial data. \n  This package works on spatial data with one variable (e.g., continuous raster), many variables (e.g., RGB rasters), and spatial patterns (e.g., areas in categorical rasters).\n  It is based on the SLIC algorithm (Achanta et al. (2012) <doi:10.1109/TPAMI.2012.120>), and readapts it to work with arbitrary dissimilarity measures. ",
    "version": "1.0.0",
    "maintainer": "Jakub Nowosad <nowosad.jakub@gmail.com>",
    "author": "Jakub Nowosad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1057-3721>),\n  Pascal Mettes [ctb] (Author of the initial C++ implementation of the\n    SLIC Superpixel algorithm for image data),\n  Charles Jekel [ctb] (Author of underlying C++ code for dtw)",
    "url": "https://jakubnowosad.com/supercells/",
    "bug_reports": "https://github.com/Nowosad/supercells/issues",
    "repository": "https://cran.r-project.org/package=supercells",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "supercells Superpixels of Spatial Data Creates superpixels based on input spatial data. \n  This package works on spatial data with one variable (e.g., continuous raster), many variables (e.g., RGB rasters), and spatial patterns (e.g., areas in categorical rasters).\n  It is based on the SLIC algorithm (Achanta et al. (2012) <doi:10.1109/TPAMI.2012.120>), and readapts it to work with arbitrary dissimilarity measures.   "
  },
  {
    "id": 21612,
    "package_name": "support",
    "title": "Support Points",
    "description": "The functions sp() and sp_seq() compute the support points in Mak and Joseph (2018) <DOI:10.1214/17-AOS1629>. Support points can be used as a representative sample of a desired distribution, or a representative reduction of a big dataset (e.g., an \"optimal\" thinning of Markov-chain Monte Carlo sample chains). This work was supported by USARO grant W911NF-14-1-0024 and NSF DMS grant 1712642.",
    "version": "0.1.7",
    "maintainer": "Simon Mak <sm769@duke.edu>",
    "author": "Simon Mak [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=support",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "support Support Points The functions sp() and sp_seq() compute the support points in Mak and Joseph (2018) <DOI:10.1214/17-AOS1629>. Support points can be used as a representative sample of a desired distribution, or a representative reduction of a big dataset (e.g., an \"optimal\" thinning of Markov-chain Monte Carlo sample chains). This work was supported by USARO grant W911NF-14-1-0024 and NSF DMS grant 1712642.  "
  },
  {
    "id": 21681,
    "package_name": "survivalsvm",
    "title": "Survival Support Vector Analysis",
    "description": "Performs support vectors analysis for data sets with survival\n    outcome. Three approaches are available in the package: The regression approach\n    takes censoring into account when formulating the inequality constraints of\n    the support vector problem. In the ranking approach, the inequality constraints\n    set the objective to maximize the concordance index for comparable pairs\n    of observations. The hybrid approach combines the regression and ranking\n    constraints in the same model.",
    "version": "0.0.6",
    "maintainer": "Cesaire J. K. Fouodo <cesaire.kuetefouodo@uni-luebeck.de>",
    "author": "Cesaire J. K. Fouodo [aut, cre]",
    "url": "https://github.com/imbs-hl/survivalsvm",
    "bug_reports": "https://github.com/imbs-hl/survivalsvm/issues",
    "repository": "https://cran.r-project.org/package=survivalsvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survivalsvm Survival Support Vector Analysis Performs support vectors analysis for data sets with survival\n    outcome. Three approaches are available in the package: The regression approach\n    takes censoring into account when formulating the inequality constraints of\n    the support vector problem. In the ranking approach, the inequality constraints\n    set the objective to maximize the concordance index for comparable pairs\n    of observations. The hybrid approach combines the regression and ranking\n    constraints in the same model.  "
  },
  {
    "id": 21704,
    "package_name": "svars",
    "title": "Data-Driven Identification of SVAR Models",
    "description": "Implements data-driven identification methods for structural vector autoregressive (SVAR) models as described in Lange et al. (2021) <doi:10.18637/jss.v097.i05>. \n             Based on an existing VAR model object (provided by e.g. VAR() from the 'vars' package), the structural \n             impact matrix is obtained via data-driven identification techniques (i.e. changes in volatility (Rigobon, R. (2003) <doi:10.1162/003465303772815727>),  patterns of GARCH (Normadin, M., Phaneuf, L. (2004) <doi:10.1016/j.jmoneco.2003.11.002>),\n             independent component analysis (Matteson, D. S, Tsay, R. S., (2013) <doi:10.1080/01621459.2016.1150851>), least dependent innovations (Herwartz, H., Ploedt, M., (2016) <doi:10.1016/j.jimonfin.2015.11.001>), \n             smooth transition in variances (Luetkepohl, H., Netsunajev, A. (2017) <doi:10.1016/j.jedc.2017.09.001>) or non-Gaussian maximum likelihood (Lanne, M., Meitz, M., Saikkonen, P. (2017) <doi:10.1016/j.jeconom.2016.06.002>)).",
    "version": "1.3.12",
    "maintainer": "Alexander Lange <alexander.lange@uni-goettingen.de>",
    "author": "Alexander Lange [aut, cre],\n  Bernhard Dalheimer [aut],\n  Helmut Herwartz [aut],\n  Simone Maxand [aut],\n  Hannes Riebl [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svars Data-Driven Identification of SVAR Models Implements data-driven identification methods for structural vector autoregressive (SVAR) models as described in Lange et al. (2021) <doi:10.18637/jss.v097.i05>. \n             Based on an existing VAR model object (provided by e.g. VAR() from the 'vars' package), the structural \n             impact matrix is obtained via data-driven identification techniques (i.e. changes in volatility (Rigobon, R. (2003) <doi:10.1162/003465303772815727>),  patterns of GARCH (Normadin, M., Phaneuf, L. (2004) <doi:10.1016/j.jmoneco.2003.11.002>),\n             independent component analysis (Matteson, D. S, Tsay, R. S., (2013) <doi:10.1080/01621459.2016.1150851>), least dependent innovations (Herwartz, H., Ploedt, M., (2016) <doi:10.1016/j.jimonfin.2015.11.001>), \n             smooth transition in variances (Luetkepohl, H., Netsunajev, A. (2017) <doi:10.1016/j.jedc.2017.09.001>) or non-Gaussian maximum likelihood (Lanne, M., Meitz, M., Saikkonen, P. (2017) <doi:10.1016/j.jeconom.2016.06.002>)).  "
  },
  {
    "id": 21709,
    "package_name": "svgtools",
    "title": "Manipulate SVG (Template) Files of Charts",
    "description": "The purpose of this package is to manipulate SVG files that are templates of charts the user wants to produce. \n    In vector graphics one copes with x-/y-coordinates of elements (e.g. lines, rectangles, text). Their scale is often dependent on the program that is used to produce the graphics. \n    In applied statistics one usually has numeric values on a fixed scale (e.g. percentage values between 0 and 100) to show in a chart. \n    Basically, 'svgtools' transforms the statistical values into coordinates and widths/heights of the vector graphics.\n    This is done by stackedBar() for bar charts, by linesSymbols() for charts with lines and/or symbols (dot markers) and scatterSymbols() for scatterplots.",
    "version": "1.1.3",
    "maintainer": "Christian Wimmer <christian.wimmer@iqs.gv.at>",
    "author": "Christian Wimmer [cre, aut],\n  Konrad Oberwimmer [aut],\n  Michael Bruneforth [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svgtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svgtools Manipulate SVG (Template) Files of Charts The purpose of this package is to manipulate SVG files that are templates of charts the user wants to produce. \n    In vector graphics one copes with x-/y-coordinates of elements (e.g. lines, rectangles, text). Their scale is often dependent on the program that is used to produce the graphics. \n    In applied statistics one usually has numeric values on a fixed scale (e.g. percentage values between 0 and 100) to show in a chart. \n    Basically, 'svgtools' transforms the statistical values into coordinates and widths/heights of the vector graphics.\n    This is done by stackedBar() for bar charts, by linesSymbols() for charts with lines and/or symbols (dot markers) and scatterSymbols() for scatterplots.  "
  },
  {
    "id": 21714,
    "package_name": "svrpath",
    "title": "The SVR Path Algorithm",
    "description": "Computes the entire solution paths for Support Vector Regression(SVR) with respect to the regularization parameter, lambda and epsilon in epsilon-intensive loss function, efficiently. We call each path algorithm svrpath and epspath. See Wang, G. et al (2008) <doi:10.1109/TNN.2008.2002077> for details regarding the method.",
    "version": "0.1.2",
    "maintainer": "Do Hyun Kim <09dohkim@gmail.com>",
    "author": "Do Hyun Kim [aut, cre],\n  Seung Jun Shin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svrpath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svrpath The SVR Path Algorithm Computes the entire solution paths for Support Vector Regression(SVR) with respect to the regularization parameter, lambda and epsilon in epsilon-intensive loss function, efficiently. We call each path algorithm svrpath and epspath. See Wang, G. et al (2008) <doi:10.1109/TNN.2008.2002077> for details regarding the method.  "
  },
  {
    "id": 21715,
    "package_name": "svs",
    "title": "Tools for Semantic Vector Spaces",
    "description": "Various tools for semantic vector spaces, such as\n    correspondence analysis (simple, multiple and discriminant), latent\n    semantic analysis, probabilistic latent semantic analysis, non-negative\n    matrix factorization, latent class analysis, EM clustering, logratio\n\tanalysis and log-multiplicative (association) analysis. Furthermore,\n    there are specialized distance measures, plotting functions and some helper\n    functions.",
    "version": "3.1.1",
    "maintainer": "Koen Plevoets <koen.plevoets@ugent.be>",
    "author": "Koen Plevoets [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3889-1809>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svs Tools for Semantic Vector Spaces Various tools for semantic vector spaces, such as\n    correspondence analysis (simple, multiple and discriminant), latent\n    semantic analysis, probabilistic latent semantic analysis, non-negative\n    matrix factorization, latent class analysis, EM clustering, logratio\n\tanalysis and log-multiplicative (association) analysis. Furthermore,\n    there are specialized distance measures, plotting functions and some helper\n    functions.  "
  },
  {
    "id": 21717,
    "package_name": "svyVGAM",
    "title": "Design-Based Inference in Vector Generalised Linear Models",
    "description": "Provides inference based on the survey package for the wide range of parametric models in the 'VGAM' package.",
    "version": "1.2-17",
    "maintainer": "Thomas Lumley <t.lumley@auckland.ac.nz>",
    "author": "Thomas Lumley [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svyVGAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svyVGAM Design-Based Inference in Vector Generalised Linear Models Provides inference based on the survey package for the wide range of parametric models in the 'VGAM' package.  "
  },
  {
    "id": 21751,
    "package_name": "sylcount",
    "title": "Syllable Counting and Readability Measurements",
    "description": "An English language syllable counter, plus readability score\n    measure-er. For readability, we support 'Flesch' Reading Ease and\n    'Flesch-Kincaid' Grade Level ('Kincaid' 'et al'. 1975)\n    <https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1055&context=istlibrary>,\n    Automated Readability Index ('Senter' and Smith 1967)\n    <https://apps.dtic.mil/sti/citations/AD0667273>,\n    Simple Measure of Gobbledygook (McLaughlin 1969),\n    and 'Coleman-Liau' (Coleman and 'Liau' 1975) <doi:10.1037/h0076540>. The\n    package has been carefully optimized and should be very efficient, both in\n    terms of run time performance and memory consumption. The main methods are\n    'vectorized' by document, and scores for multiple documents are computed in\n    parallel via 'OpenMP'.",
    "version": "0.2-6",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre]",
    "url": "https://github.com/wrathematics/sylcount",
    "bug_reports": "https://github.com/wrathematics/sylcount/issues",
    "repository": "https://cran.r-project.org/package=sylcount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sylcount Syllable Counting and Readability Measurements An English language syllable counter, plus readability score\n    measure-er. For readability, we support 'Flesch' Reading Ease and\n    'Flesch-Kincaid' Grade Level ('Kincaid' 'et al'. 1975)\n    <https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1055&context=istlibrary>,\n    Automated Readability Index ('Senter' and Smith 1967)\n    <https://apps.dtic.mil/sti/citations/AD0667273>,\n    Simple Measure of Gobbledygook (McLaughlin 1969),\n    and 'Coleman-Liau' (Coleman and 'Liau' 1975) <doi:10.1037/h0076540>. The\n    package has been carefully optimized and should be very efficient, both in\n    terms of run time performance and memory consumption. The main methods are\n    'vectorized' by document, and scores for multiple documents are computed in\n    parallel via 'OpenMP'.  "
  },
  {
    "id": 21796,
    "package_name": "tabbycat",
    "title": "Tabulate and Summarise Categorical Data",
    "description": "Functions for tabulating and summarising categorical variables. \n    Most functions are designed to work with dataframes, and use the 'tidyverse' \n    idiom of taking the dataframe as the first argument so they work within \n    pipelines. Equivalent functions that operate directly on vectors are also \n    provided where it makes sense. This package aims to make exploratory data \n    analysis involving categorical variables quicker, simpler and more robust.",
    "version": "0.18.0",
    "maintainer": "Oliver Hawkins <oli@olihawkins.com>",
    "author": "Oliver Hawkins [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tabbycat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabbycat Tabulate and Summarise Categorical Data Functions for tabulating and summarising categorical variables. \n    Most functions are designed to work with dataframes, and use the 'tidyverse' \n    idiom of taking the dataframe as the first argument so they work within \n    pipelines. Equivalent functions that operate directly on vectors are also \n    provided where it makes sense. This package aims to make exploratory data \n    analysis involving categorical variables quicker, simpler and more robust.  "
  },
  {
    "id": 21824,
    "package_name": "tabularaster",
    "title": "Tidy Tools for 'Raster' Data",
    "description": "Facilities to work with vector and raster data in efficient \n repeatable and systematic work flow. Missing functionality in existing packages \n is included here to allow extraction from raster data with 'simple features' and \n 'Spatial' types and to make extraction consistent and straightforward. Extract cell \n numbers from raster data and  return the cells as a data frame \n rather than as lists of matrices or vectors. The functions here allow spatial data \n to be used without special handling for the format currently in use. ",
    "version": "0.7.2",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre]",
    "url": "https://github.com/hypertidy/tabularaster",
    "bug_reports": "https://github.com/hypertidy/tabularaster/issues",
    "repository": "https://cran.r-project.org/package=tabularaster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabularaster Tidy Tools for 'Raster' Data Facilities to work with vector and raster data in efficient \n repeatable and systematic work flow. Missing functionality in existing packages \n is included here to allow extraction from raster data with 'simple features' and \n 'Spatial' types and to make extraction consistent and straightforward. Extract cell \n numbers from raster data and  return the cells as a data frame \n rather than as lists of matrices or vectors. The functions here allow spatial data \n to be used without special handling for the format currently in use.   "
  },
  {
    "id": 21884,
    "package_name": "tdarec",
    "title": "A 'recipes' Extension for Persistent Homology and Its\nVectorizations",
    "description": "Topological data analytic methods in machine learning rely on\n    vectorizations of the persistence diagrams that encode persistent\n    homology, as surveyed by Ali &al (2000)\n    <doi:10.48550/arXiv.2212.09703>.  Persistent homology can be computed\n    using 'TDA' and 'ripserr' and vectorized using 'TDAvec'.  The\n    Tidymodels package collection modularizes machine learning in R for\n    straightforward extensibility; see Kuhn & Silge (2022,\n    ISBN:978-1-4920-9644-3).  These 'recipe' steps and 'dials' tuners make\n    efficient algorithms for computing and vectorizing persistence\n    diagrams available for Tidymodels workflows.",
    "version": "0.2.0",
    "maintainer": "Jason Cory Brunson <cornelioid@gmail.com>",
    "author": "Jason Cory Brunson [cre, aut]",
    "url": "https://github.com/tdaverse/tdarec",
    "bug_reports": "https://github.com/tdaverse/tdarec/issues",
    "repository": "https://cran.r-project.org/package=tdarec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tdarec A 'recipes' Extension for Persistent Homology and Its\nVectorizations Topological data analytic methods in machine learning rely on\n    vectorizations of the persistence diagrams that encode persistent\n    homology, as surveyed by Ali &al (2000)\n    <doi:10.48550/arXiv.2212.09703>.  Persistent homology can be computed\n    using 'TDA' and 'ripserr' and vectorized using 'TDAvec'.  The\n    Tidymodels package collection modularizes machine learning in R for\n    straightforward extensibility; see Kuhn & Silge (2022,\n    ISBN:978-1-4920-9644-3).  These 'recipe' steps and 'dials' tuners make\n    efficient algorithms for computing and vectorizing persistence\n    diagrams available for Tidymodels workflows.  "
  },
  {
    "id": 21897,
    "package_name": "teal.logger",
    "title": "Logging Setup for the 'teal' Family of Packages",
    "description": "Utilizing the 'logger' framework to record events within a\n    package, specific to 'teal' family of packages.  Supports logging\n    namespaces, hierarchical logging, various log destinations,\n    vectorization, and more.",
    "version": "0.4.1",
    "maintainer": "Dawid Kaledkowski <dawid.kaledkowski@roche.com>",
    "author": "Dawid Kaledkowski [aut, cre],\n  Konrad Pagacz [aut],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://insightsengineering.github.io/teal.logger/,\nhttps://github.com/insightsengineering/teal.logger/",
    "bug_reports": "https://github.com/insightsengineering/teal.logger/issues",
    "repository": "https://cran.r-project.org/package=teal.logger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "teal.logger Logging Setup for the 'teal' Family of Packages Utilizing the 'logger' framework to record events within a\n    package, specific to 'teal' family of packages.  Supports logging\n    namespaces, hierarchical logging, various log destinations,\n    vectorization, and more.  "
  },
  {
    "id": 21921,
    "package_name": "tempted",
    "title": "Temporal Tensor Decomposition, a Dimensionality Reduction Tool\nfor Longitudinal Multivariate Data",
    "description": "\n    TEMPoral TEnsor Decomposition (TEMPTED), is a dimension reduction method for multivariate longitudinal data with varying temporal sampling. It formats the data into a temporal tensor and decomposes it into a summation of low-dimensional components, each consisting of a subject loading vector, a feature loading vector, and a continuous temporal loading function. These loadings provide a low-dimensional representation of subjects or samples and can be used to identify features associated with clusters of subjects or samples. TEMPTED provides the flexibility of allowing subjects to have different temporal sampling, so time points do not need to be binned, and missing time points do not need to be imputed.",
    "version": "0.1.1",
    "maintainer": "Pixu Shi <pixu.shi@duke.edu>",
    "author": "Pixu Shi",
    "url": "https://github.com/pixushi/tempted",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tempted",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tempted Temporal Tensor Decomposition, a Dimensionality Reduction Tool\nfor Longitudinal Multivariate Data \n    TEMPoral TEnsor Decomposition (TEMPTED), is a dimension reduction method for multivariate longitudinal data with varying temporal sampling. It formats the data into a temporal tensor and decomposes it into a summation of low-dimensional components, each consisting of a subject loading vector, a feature loading vector, and a continuous temporal loading function. These loadings provide a low-dimensional representation of subjects or samples and can be used to identify features associated with clusters of subjects or samples. TEMPTED provides the flexibility of allowing subjects to have different temporal sampling, so time points do not need to be binned, and missing time points do not need to be imputed.  "
  },
  {
    "id": 21937,
    "package_name": "term",
    "title": "Create, Manipulate and Query Parameter Terms",
    "description": "Creates, manipulates, queries and repairs vectors of\n    parameter terms.  Parameter terms are the labels used to reference\n    values in vectors, matrices and arrays. They represent the names in\n    coefficient tables and the column names in 'mcmc' and 'mcmc.list'\n    objects.",
    "version": "0.3.6",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "author": "Joe Thorley [aut, cre] (ORCID: <https://orcid.org/0000-0002-7683-4592>),\n  Kirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>),\n  Ayla Pearson [ctb] (ORCID: <https://orcid.org/0000-0001-7388-1222>),\n  Evan Amies-Galonski [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1096-2089>),\n  Poisson Consulting [cph, fnd]",
    "url": "https://poissonconsulting.github.io/term/,\nhttps://github.com/poissonconsulting/term",
    "bug_reports": "https://github.com/poissonconsulting/term/issues",
    "repository": "https://cran.r-project.org/package=term",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "term Create, Manipulate and Query Parameter Terms Creates, manipulates, queries and repairs vectors of\n    parameter terms.  Parameter terms are the labels used to reference\n    values in vectors, matrices and arrays. They represent the names in\n    coefficient tables and the column names in 'mcmc' and 'mcmc.list'\n    objects.  "
  },
  {
    "id": 21943,
    "package_name": "terrainmeshr",
    "title": "Triangulate and Simplify 3D Terrain Meshes",
    "description": "Provides triangulations of regular height fields, based on the methods described in \"Fast Polygonal Approximation of Terrains and Height Fields\" Michael Garland and Paul S. Heckbert (1995) <https://www.mgarland.org/files/papers/scape.pdf> using code from the 'hmm' library written by Michael Fogleman <https://github.com/fogleman/hmm>.",
    "version": "1.0.1",
    "maintainer": "Tyler Morgan-Wall <tylermw@gmail.com>",
    "author": "Tyler Morgan-Wall [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3131-3814>),\n  Michael Fogleman [ctb, cph]",
    "url": "https://github.com/tylermorganwall/terrainmeshr",
    "bug_reports": "https://github.com/tylermorganwall/terrainmeshr/issues",
    "repository": "https://cran.r-project.org/package=terrainmeshr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "terrainmeshr Triangulate and Simplify 3D Terrain Meshes Provides triangulations of regular height fields, based on the methods described in \"Fast Polygonal Approximation of Terrains and Height Fields\" Michael Garland and Paul S. Heckbert (1995) <https://www.mgarland.org/files/papers/scape.pdf> using code from the 'hmm' library written by Michael Fogleman <https://github.com/fogleman/hmm>.  "
  },
  {
    "id": 21976,
    "package_name": "text2vec",
    "title": "Modern Text Mining Framework for R",
    "description": "Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines.",
    "version": "0.6.6",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "author": "Dmitriy Selivanov [aut, cre, cph],\n  Manuel Bickel [aut, cph] (Coherence measures for topic models),\n  Qing Wang [aut, cph] (Author of the WaprLDA C++ code)",
    "url": "http://text2vec.org",
    "bug_reports": "https://github.com/dselivanov/text2vec/issues",
    "repository": "https://cran.r-project.org/package=text2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2vec Modern Text Mining Framework for R Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines.  "
  },
  {
    "id": 21979,
    "package_name": "textTinyR",
    "title": "Text Processing for Small or Big Data Files",
    "description": "It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.",
    "version": "1.1.8",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>)",
    "url": "https://github.com/mlampros/textTinyR",
    "bug_reports": "https://github.com/mlampros/textTinyR/issues",
    "repository": "https://cran.r-project.org/package=textTinyR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textTinyR Text Processing for Small or Big Data Files It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.  "
  },
  {
    "id": 21980,
    "package_name": "textTools",
    "title": "Functions for Text Cleansing and Text Analysis",
    "description": "A framework for text cleansing and analysis. Conveniently prepare and process large amounts of text for analysis. \n  Includes various metrics for word counts/frequencies that scale efficiently. Quickly \n  analyze large amounts of text data using a text.table (a data.table created with one word (or unit of text analysis) per row, similar to the tidytext format). \n  Offers flexibility to efficiently work with text data stored in vectors as well as text data formatted as a text.table.",
    "version": "0.1.0",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textTools Functions for Text Cleansing and Text Analysis A framework for text cleansing and analysis. Conveniently prepare and process large amounts of text for analysis. \n  Includes various metrics for word counts/frequencies that scale efficiently. Quickly \n  analyze large amounts of text data using a text.table (a data.table created with one word (or unit of text analysis) per row, similar to the tidytext format). \n  Offers flexibility to efficiently work with text data stored in vectors as well as text data formatted as a text.table.  "
  },
  {
    "id": 21995,
    "package_name": "textreg",
    "title": "n-Gram Text Regression, aka Concise Comparative Summarization",
    "description": "Function for sparse regression on raw text, regressing a labeling\n    vector onto a feature space consisting of all possible phrases.",
    "version": "0.1.5",
    "maintainer": "Luke Miratrix <lmiratrix@stat.harvard.edu>",
    "author": "Luke Miratrix",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textreg n-Gram Text Regression, aka Concise Comparative Summarization Function for sparse regression on raw text, regressing a labeling\n    vector onto a feature space consisting of all possible phrases.  "
  },
  {
    "id": 21998,
    "package_name": "tf",
    "title": "S3 Classes and Methods for Tidy Functional Data",
    "description": "Defines S3 vector data types for vectors of functional data \n   (grid-based, spline-based or functional principal components-based) with all \n   arithmetic and summary methods, derivation, integration and smoothing, \n   plotting, data import and export, and data wrangling, such as re-evaluating, \n   subsetting, sub-assigning, zooming into sub-domains, or extracting functional \n   features like minima/maxima and their locations. \n   The implementation allows including such vectors in data frames for joint \n   analysis of functional and scalar variables.",
    "version": "0.3.4",
    "maintainer": "Fabian Scheipl <fabian.scheipl@googlemail.com>",
    "author": "Fabian Scheipl [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8172-3603>),\n  Jeff Goldsmith [aut],\n  Julia Wrobel [ctb] (ORCID: <https://orcid.org/0000-0001-6783-1421>),\n  Maximilian Muecke [ctb] (ORCID:\n    <https://orcid.org/0009-0000-9432-9795>),\n  Sebastian Fischer [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9609-3197>),\n  Trevor Hastie [ctb] (softImpute author),\n  Rahul Mazumder [ctb] (softImpute author),\n  Chen Meng [ctb] (mogsa author)",
    "url": "https://tidyfun.github.io/tf/, https://github.com/tidyfun/tf/",
    "bug_reports": "https://github.com/tidyfun/tf/issues",
    "repository": "https://cran.r-project.org/package=tf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tf S3 Classes and Methods for Tidy Functional Data Defines S3 vector data types for vectors of functional data \n   (grid-based, spline-based or functional principal components-based) with all \n   arithmetic and summary methods, derivation, integration and smoothing, \n   plotting, data import and export, and data wrangling, such as re-evaluating, \n   subsetting, sub-assigning, zooming into sub-domains, or extracting functional \n   features like minima/maxima and their locations. \n   The implementation allows including such vectors in data frames for joint \n   analysis of functional and scalar variables.  "
  },
  {
    "id": 22043,
    "package_name": "tibblify",
    "title": "Rectangle Nested Lists",
    "description": "A tool to rectangle a nested list, that is to convert it into\n    a tibble. This is done automatically or according to a given\n    specification.  A common use case is for nested lists coming from\n    parsing JSON files or the JSON response of REST APIs. It is supported\n    by the 'vctrs' package and therefore offers a wide support of vector\n    types.",
    "version": "0.3.1",
    "maintainer": "Maximilian Girlich <maximilian.girlich@outlook.com>",
    "author": "Maximilian Girlich [aut, cre, cph],\n  Kirill M\u00fcller [ctb]",
    "url": "https://github.com/mgirlich/tibblify,\nhttps://mgirlich.github.io/tibblify/",
    "bug_reports": "https://github.com/mgirlich/tibblify/issues",
    "repository": "https://cran.r-project.org/package=tibblify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tibblify Rectangle Nested Lists A tool to rectangle a nested list, that is to convert it into\n    a tibble. This is done automatically or according to a given\n    specification.  A common use case is for nested lists coming from\n    parsing JSON files or the JSON response of REST APIs. It is supported\n    by the 'vctrs' package and therefore offers a wide support of vector\n    types.  "
  },
  {
    "id": 22047,
    "package_name": "tictoc",
    "title": "Functions for Timing R Scripts, as Well as Implementations of\n\"Stack\" and \"StackList\" Structures",
    "description": "Code execution timing functions 'tic' and 'toc' that\n    can be nested. One can record all timings while a complex script is\n    running, and examine the values later. It is also possible to instrument\n    the timing calls with custom callbacks. In addition, this package provides\n    class 'Stack', implemented as a vector, and class 'StackList', which is a\n    stack implemented as a\n    list, both of which support operations 'push', 'pop', 'first_element',\n    'last_element' and 'clear'.",
    "version": "1.2.1",
    "maintainer": "Sergei Izrailev <sizrailev@jabiruventures.com>",
    "author": "Sergei Izrailev",
    "url": "https://github.com/jabiru/tictoc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tictoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tictoc Functions for Timing R Scripts, as Well as Implementations of\n\"Stack\" and \"StackList\" Structures Code execution timing functions 'tic' and 'toc' that\n    can be nested. One can record all timings while a complex script is\n    running, and examine the values later. It is also possible to instrument\n    the timing calls with custom callbacks. In addition, this package provides\n    class 'Stack', implemented as a vector, and class 'StackList', which is a\n    stack implemented as a\n    list, both of which support operations 'push', 'pop', 'first_element',\n    'last_element' and 'clear'.  "
  },
  {
    "id": 22067,
    "package_name": "tidycensus",
    "title": "Load US Census Boundary and Attribute Data as 'tidyverse' and\n'sf'-Ready Data Frames",
    "description": "An integrated R interface to several United States Census Bureau \n    APIs (<https://www.census.gov/data/developers/data-sets.html>) and the US Census Bureau's \n    geographic boundary files. Allows R users to return Census and ACS data as \n    tidyverse-ready data frames, and optionally returns a list-column with feature geometry for mapping \n    and spatial analysis. ",
    "version": "1.7.3",
    "maintainer": "Kyle Walker <kyle@walker-data.com>",
    "author": "Kyle Walker [aut, cre],\n  Matt Herman [aut],\n  Kris Eberwein [ctb]",
    "url": "https://walker-data.com/tidycensus/",
    "bug_reports": "https://github.com/walkerke/tidycensus/issues",
    "repository": "https://cran.r-project.org/package=tidycensus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidycensus Load US Census Boundary and Attribute Data as 'tidyverse' and\n'sf'-Ready Data Frames An integrated R interface to several United States Census Bureau \n    APIs (<https://www.census.gov/data/developers/data-sets.html>) and the US Census Bureau's \n    geographic boundary files. Allows R users to return Census and ACS data as \n    tidyverse-ready data frames, and optionally returns a list-column with feature geometry for mapping \n    and spatial analysis.   "
  },
  {
    "id": 22106,
    "package_name": "tidynorm",
    "title": "Tools for Tidy Vowel Normalization",
    "description": "An implementation of tidy speaker vowel normalization.\n    This includes generic functions for defining new normalization methods for\n    points, formant tracks, and Discrete Cosine Transform coefficients, as well\n    as convenience functions implementing established normalization methods.\n    References for the implemented methods are:\n    Johnson, Keith (2020) <doi:10.5334/labphon.196>\n    Lobanov, Boris (1971) <doi:10.1121/1.1912396>\n    Nearey, Terrance M. (1978) <https://sites.ualberta.ca/~tnearey/Nearey1978_compressed.pdf>\n    Syrdal, Ann K., and Gopal, H. S. (1986) <doi:10.1121/1.393381>\n    Watt, Dominic, and Fabricius, Anne (2002) <https://www.latl.leeds.ac.uk/article/evaluation-of-a-technique-for-improving-the-mapping-of-multiple-speakers-vowel-spaces-in-the-f1-f2-plane/>.",
    "version": "0.4.0",
    "maintainer": "Josef Fruehwald <JoFrhwld@gmail.com>",
    "author": "Josef Fruehwald [cre, aut, cph]",
    "url": "https://jofrhwld.github.io/tidynorm/,\nhttps://github.com/JoFrhwld/tidynorm",
    "bug_reports": "https://github.com/JoFrhwld/tidynorm/issues",
    "repository": "https://cran.r-project.org/package=tidynorm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidynorm Tools for Tidy Vowel Normalization An implementation of tidy speaker vowel normalization.\n    This includes generic functions for defining new normalization methods for\n    points, formant tracks, and Discrete Cosine Transform coefficients, as well\n    as convenience functions implementing established normalization methods.\n    References for the implemented methods are:\n    Johnson, Keith (2020) <doi:10.5334/labphon.196>\n    Lobanov, Boris (1971) <doi:10.1121/1.1912396>\n    Nearey, Terrance M. (1978) <https://sites.ualberta.ca/~tnearey/Nearey1978_compressed.pdf>\n    Syrdal, Ann K., and Gopal, H. S. (1986) <doi:10.1121/1.393381>\n    Watt, Dominic, and Fabricius, Anne (2002) <https://www.latl.leeds.ac.uk/article/evaluation-of-a-technique-for-improving-the-mapping-of-multiple-speakers-vowel-spaces-in-the-f1-f2-plane/>.  "
  },
  {
    "id": 22129,
    "package_name": "tidyterra",
    "title": "'tidyverse' Methods and 'ggplot2' Helpers for 'terra' Objects",
    "description": "Extension of the 'tidyverse' for 'SpatRaster' and\n    'SpatVector' objects of the 'terra' package. It includes also new\n    'geom_' functions that provide a convenient way of visualizing 'terra'\n    objects with 'ggplot2'.",
    "version": "0.7.2",
    "maintainer": "Diego Hernang\u00f3mez <diego.hernangomezherrero@gmail.com>",
    "author": "Diego Hernang\u00f3mez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>),\n  Dewey Dunnington [ctb] (ORCID: <https://orcid.org/0000-0002-9415-4582>,\n    for ggspatial code),\n  ggplot2 authors [cph] (for contour code),\n  Andrea Manica [ctb]",
    "url": "https://dieghernan.github.io/tidyterra/,\nhttps://github.com/dieghernan/tidyterra",
    "bug_reports": "https://github.com/dieghernan/tidyterra/issues",
    "repository": "https://cran.r-project.org/package=tidyterra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyterra 'tidyverse' Methods and 'ggplot2' Helpers for 'terra' Objects Extension of the 'tidyverse' for 'SpatRaster' and\n    'SpatVector' objects of the 'terra' package. It includes also new\n    'geom_' functions that provide a convenient way of visualizing 'terra'\n    objects with 'ggplot2'.  "
  },
  {
    "id": 22133,
    "package_name": "tidytransit",
    "title": "Read, Validate, Analyze, and Map GTFS Feeds",
    "description": "Read General Transit Feed Specification (GTFS) zipfiles into a list of R dataframes. Perform validation of the data structure against the specification. Analyze the headways and frequencies at routes and stops. Create maps and perform spatial analysis on the routes and stops. Please see the GTFS documentation here for more detail: <https://gtfs.org/>.",
    "version": "1.7.1",
    "maintainer": "Flavio Poletti <flavio.poletti@hotmail.ch>",
    "author": "Flavio Poletti [aut, cre],\n  Daniel Herszenhut [aut] (ORCID:\n    <https://orcid.org/0000-0001-8066-1105>),\n  Mark Padgham [aut],\n  Tom Buckley [aut],\n  Danton Noriega-Goodwin [aut],\n  Angela Li [ctb],\n  Elaine McVey [ctb],\n  Charles Hans Thompson [ctb],\n  Michael Sumner [ctb],\n  Patrick Hausmann [ctb],\n  Bob Rudis [ctb],\n  James Lamb [ctb],\n  Alexandra Kapp [ctb],\n  Kearey Smith [ctb],\n  Dave Vautin [ctb],\n  Kyle Walker [ctb],\n  Davis Vaughan [ctb],\n  Ryan Rymarczyk [ctb],\n  Kirill M\u00fcller [ctb]",
    "url": "https://github.com/r-transit/tidytransit,\nhttps://r-transit.github.io/tidytransit/",
    "bug_reports": "https://github.com/r-transit/tidytransit/issues",
    "repository": "https://cran.r-project.org/package=tidytransit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidytransit Read, Validate, Analyze, and Map GTFS Feeds Read General Transit Feed Specification (GTFS) zipfiles into a list of R dataframes. Perform validation of the data structure against the specification. Analyze the headways and frequencies at routes and stops. Create maps and perform spatial analysis on the routes and stops. Please see the GTFS documentation here for more detail: <https://gtfs.org/>.  "
  },
  {
    "id": 22142,
    "package_name": "tiff",
    "title": "Read and Write TIFF Images",
    "description": "Functions to read, write and display bitmap images stored in the TIFF format. It can read and write both files and in-memory raw vectors, including native image representation.",
    "version": "0.1-12",
    "maintainer": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "author": "Simon Urbanek <Simon.Urbanek@r-project.org> [aut, cre],\n\tKent Johnson <kjohnson@akoyabio.com> [ctb]",
    "url": "https://www.rforge.net/tiff/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tiff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tiff Read and Write TIFF Images Functions to read, write and display bitmap images stored in the TIFF format. It can read and write both files and in-memory raw vectors, including native image representation.  "
  },
  {
    "id": 22150,
    "package_name": "tikatuwq",
    "title": "Water Quality Assessment and Environmental Compliance in Brazil",
    "description": "Tools to import, clean, validate, and analyze freshwater quality data in Brazil. Implements water quality indices including the Water Quality Index (WQI/IQA), the Trophic State Index (TSI/IET) after Carlson (1977) <doi:10.4319/lo.1977.22.2.0361> and Lamparelli (2004) <https://www.teses.usp.br/teses/disponiveis/41/41134/tde-20032006-075813/publico/TeseLamparelli2004.pdf>, and the National Sanitation Foundation Water Quality Index (NSF WQI) <doi:10.1007/s11157-023-09650-7>. The package also checks compliance with Brazilian standard CONAMA Resolution 357/2005 <https://conama.mma.gov.br/?id=450&option=com_sisconama&task=arquivo.download> and generates reproducible reports for routine monitoring workflows.\n    The example dataset (`wq_demo`) is now a real subset from monitoring data (BURANHEM river, 2020-2024, 4 points, 20 rows, 14 columns including extra `rio`, `lat`, `lon`). All core examples and vignettes use this realistic sample, improving reproducibility and documentation value for users.",
    "version": "0.8.0",
    "maintainer": "Vinicius Saraiva Santos <vinisaraiva@gmail.com>",
    "author": "Vinicius Saraiva Santos [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-1387-7927>)",
    "url": "https://github.com/tikatuwq/tikatuwq,\nhttps://tikatuwq.github.io/tikatuwq/",
    "bug_reports": "https://github.com/tikatuwq/tikatuwq/issues",
    "repository": "https://cran.r-project.org/package=tikatuwq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tikatuwq Water Quality Assessment and Environmental Compliance in Brazil Tools to import, clean, validate, and analyze freshwater quality data in Brazil. Implements water quality indices including the Water Quality Index (WQI/IQA), the Trophic State Index (TSI/IET) after Carlson (1977) <doi:10.4319/lo.1977.22.2.0361> and Lamparelli (2004) <https://www.teses.usp.br/teses/disponiveis/41/41134/tde-20032006-075813/publico/TeseLamparelli2004.pdf>, and the National Sanitation Foundation Water Quality Index (NSF WQI) <doi:10.1007/s11157-023-09650-7>. The package also checks compliance with Brazilian standard CONAMA Resolution 357/2005 <https://conama.mma.gov.br/?id=450&option=com_sisconama&task=arquivo.download> and generates reproducible reports for routine monitoring workflows.\n    The example dataset (`wq_demo`) is now a real subset from monitoring data (BURANHEM river, 2020-2024, 4 points, 20 rows, 14 columns including extra `rio`, `lat`, `lon`). All core examples and vignettes use this realistic sample, improving reproducibility and documentation value for users.  "
  },
  {
    "id": 22172,
    "package_name": "timeordered",
    "title": "Time-Ordered and Time-Aggregated Network Analyses",
    "description": "Approaches for incorporating time into network analysis. Methods include: construction of time-ordered networks (temporal graphs); shortest-time and shortest-path-length analyses; resource spread calculations; data resampling and rarefaction for null model construction; reduction to time-aggregated networks with variable window sizes; application of common descriptive statistics to these networks; vector clock latencies; and plotting functionalities. The package supports <doi:10.1371/journal.pone.0020298>. ",
    "version": "1.0.3",
    "maintainer": "Benjamin Wong Blonder <benjamin.blonder@berkeley.edu>",
    "author": "Benjamin Wong Blonder [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timeordered",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeordered Time-Ordered and Time-Aggregated Network Analyses Approaches for incorporating time into network analysis. Methods include: construction of time-ordered networks (temporal graphs); shortest-time and shortest-path-length analyses; resource spread calculations; data resampling and rarefaction for null model construction; reduction to time-aggregated networks with variable window sizes; application of common descriptive statistics to these networks; vector clock latencies; and plotting functionalities. The package supports <doi:10.1371/journal.pone.0020298>.   "
  },
  {
    "id": 22185,
    "package_name": "tinyVAST",
    "title": "Multivariate Spatio-Temporal Models using Structural Equations",
    "description": "Fits a wide variety of multivariate spatio-temporal models\n    with simultaneous and lagged interactions among variables (including\n    vector autoregressive spatio-temporal ('VAST') dynamics)\n    for areal, continuous, or network spatial domains.  \n    It includes time-variable, space-variable, and space-time-variable \n    interactions using dynamic structural equation models ('DSEM') \n    as expressive interface, and the 'mgcv' package to specify splines \n    via the formula interface.  See Thorson et al. (2025)\n    <doi:10.1111/geb.70035> for more details.",
    "version": "1.3.0",
    "maintainer": "James T. Thorson <James.Thorson@noaa.gov>",
    "author": "James T. Thorson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7415-1010>),\n  Sean C. Anderson [aut] (ORCID: <https://orcid.org/0000-0001-9563-1937>)",
    "url": "https://vast-lib.github.io/tinyVAST/",
    "bug_reports": "https://github.com/vast-lib/tinyVAST/issues",
    "repository": "https://cran.r-project.org/package=tinyVAST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinyVAST Multivariate Spatio-Temporal Models using Structural Equations Fits a wide variety of multivariate spatio-temporal models\n    with simultaneous and lagged interactions among variables (including\n    vector autoregressive spatio-temporal ('VAST') dynamics)\n    for areal, continuous, or network spatial domains.  \n    It includes time-variable, space-variable, and space-time-variable \n    interactions using dynamic structural equation models ('DSEM') \n    as expressive interface, and the 'mgcv' package to specify splines \n    via the formula interface.  See Thorson et al. (2025)\n    <doi:10.1111/geb.70035> for more details.  "
  },
  {
    "id": 22188,
    "package_name": "tinylabels",
    "title": "Lightweight Variable Labels",
    "description": "Assign, extract, or remove variable labels from R vectors.\n  Lightweight and dependency-free.",
    "version": "0.2.5",
    "maintainer": "Marius Barth <marius.barth.uni.koeln@gmail.com>",
    "author": "Marius Barth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3421-6665>)",
    "url": "https://github.com/mariusbarth/tinylabels",
    "bug_reports": "https://github.com/mariusbarth/tinylabels/issues",
    "repository": "https://cran.r-project.org/package=tinylabels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinylabels Lightweight Variable Labels Assign, extract, or remove variable labels from R vectors.\n  Lightweight and dependency-free.  "
  },
  {
    "id": 22197,
    "package_name": "tip",
    "title": "Bayesian Clustering Using the Table Invitation Prior (TIP)",
    "description": "Cluster data without specifying the number of clusters using the Table Invitation Prior (TIP) introduced in the paper \"Clustering Gene Expression Using the Table Invitation Prior\" by Charles W. Harrison, Qing He, and Hsin-Hsiung Huang (2022) <doi:10.3390/genes13112036>. TIP is a Bayesian prior that uses pairwise distance and similarity information to cluster vectors, matrices, or tensors.",
    "version": "0.1.0",
    "maintainer": "Charles W. Harrison <charleswharrison@knights.ucf.edu>",
    "author": "Charles W. Harrison [cre, aut, cph],\n  Qing He [aut, cph],\n  Hsin-Hsiung Huang [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tip",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tip Bayesian Clustering Using the Table Invitation Prior (TIP) Cluster data without specifying the number of clusters using the Table Invitation Prior (TIP) introduced in the paper \"Clustering Gene Expression Using the Table Invitation Prior\" by Charles W. Harrison, Qing He, and Hsin-Hsiung Huang (2022) <doi:10.3390/genes13112036>. TIP is a Bayesian prior that uses pairwise distance and similarity information to cluster vectors, matrices, or tensors.  "
  },
  {
    "id": 22230,
    "package_name": "tmap.mapgl",
    "title": "Extensions to 'tmap' with Two New Modes: 'mapbox' and 'maplibre'",
    "description": "The 'tmap' package provides two plotting modes for static and interactive thematic maps. This package extends 'tmap' with two additional modes based on 'Mapbox GL JS' and 'MapLibre GL JS'. These modes feature interactive vector tiles, globe views, and other modern web-mapping capabilities, while maintaining a consistent 'tmap' interface across all plotting modes.",
    "version": "0.1.0",
    "maintainer": "Martijn Tennekes <mtennekes@gmail.com>",
    "author": "Martijn Tennekes [aut, cre]",
    "url": "https://github.com/r-tmap/tmap.mapgl,\nhttps://r-tmap.github.io/tmap.mapgl/",
    "bug_reports": "https://github.com/r-tmap/tmap.mapgl/issues",
    "repository": "https://cran.r-project.org/package=tmap.mapgl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmap.mapgl Extensions to 'tmap' with Two New Modes: 'mapbox' and 'maplibre' The 'tmap' package provides two plotting modes for static and interactive thematic maps. This package extends 'tmap' with two additional modes based on 'Mapbox GL JS' and 'MapLibre GL JS'. These modes feature interactive vector tiles, globe views, and other modern web-mapping capabilities, while maintaining a consistent 'tmap' interface across all plotting modes.  "
  },
  {
    "id": 22233,
    "package_name": "tmapverse",
    "title": "Meta-Package for Thematic Mapping with 'tmap'",
    "description": "Attaches a set of packages commonly used for spatial plotting with 'tmap'. It includes 'tmap' and its extensions ('tmap.glyphs', 'tmap.networks', 'tmap.cartogram', 'tmap.mapgl'), as well as supporting spatial data packages ('sf', 'stars', 'terra') and 'cols4all' for exploring color palettes. The collection is designed for thematic mapping workflows and does not include the full set of packages from the R-spatial ecosystem.",
    "version": "0.1.0",
    "maintainer": "Martijn Tennekes <mtennekes@gmail.com>",
    "author": "Martijn Tennekes [aut, cre],\n  Robin Lovelace [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmapverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmapverse Meta-Package for Thematic Mapping with 'tmap' Attaches a set of packages commonly used for spatial plotting with 'tmap'. It includes 'tmap' and its extensions ('tmap.glyphs', 'tmap.networks', 'tmap.cartogram', 'tmap.mapgl'), as well as supporting spatial data packages ('sf', 'stars', 'terra') and 'cols4all' for exploring color palettes. The collection is designed for thematic mapping workflows and does not include the full set of packages from the R-spatial ecosystem.  "
  },
  {
    "id": 22245,
    "package_name": "tmvtnsim",
    "title": "Truncated Multivariate Normal and t Distribution Simulation",
    "description": "Simulation of random vectors from truncated multivariate normal \n  and t distributions based on the algorithms proposed by Yifang Li and Sujit K. Ghosh (2015) <doi:10.1080/15598608.2014.996690>.",
    "version": "0.1.4",
    "maintainer": "Kaifeng Lu <kaifenglu@gmail.com>",
    "author": "Kaifeng Lu [aut, cre] (ORCID: <https://orcid.org/0000-0002-6160-7119>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmvtnsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmvtnsim Truncated Multivariate Normal and t Distribution Simulation Simulation of random vectors from truncated multivariate normal \n  and t distributions based on the algorithms proposed by Yifang Li and Sujit K. Ghosh (2015) <doi:10.1080/15598608.2014.996690>.  "
  },
  {
    "id": 22257,
    "package_name": "tomledit",
    "title": "Parse, Read, and Edit 'TOML'",
    "description": "A toolkit for working with 'TOML' files in R while preserving\n    formatting, comments, and structure. 'tomledit' enables serialization of R\n    objects such as lists, data.frames, numeric, logical, and date vectors.",
    "version": "0.1.1",
    "maintainer": "Josiah Parry <josiah.parry@gmail.com>",
    "author": "Josiah Parry [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9910-865X>)",
    "url": "https://extendr.github.io/tomledit/,\nhttps://github.com/extendr/tomledit",
    "bug_reports": "https://github.com/extendr/tomledit/issues",
    "repository": "https://cran.r-project.org/package=tomledit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tomledit Parse, Read, and Edit 'TOML' A toolkit for working with 'TOML' files in R while preserving\n    formatting, comments, and structure. 'tomledit' enables serialization of R\n    objects such as lists, data.frames, numeric, logical, and date vectors.  "
  },
  {
    "id": 22274,
    "package_name": "topolow",
    "title": "Force-Directed Euclidean Embedding of Dissimilarity Data",
    "description": "A robust implementation of Topolow algorithm. It embeds objects into a low-dimensional Euclidean space from a matrix of pairwise dissimilarities, even when the data do not satisfy metric or Euclidean axioms. The package is particularly well-suited for sparse, incomplete, and censored (thresholded) datasets such as antigenic relationships. The core is a physics-inspired, gradient-free optimization framework that models objects as particles in a physical system, where observed dissimilarities define spring rest lengths and unobserved pairs exert repulsive forces. The package also provides functions specific to antigenic mapping to transform cross-reactivity and binding affinity measurements into accurate spatial representations in a phenotype space.\n    Key features include:\n    * Robust Embedding from Sparse Data: Effectively creates complete and consistent maps (in optimal dimensions) even with high proportions of missing data (e.g., >95%).\n    * Physics-Inspired Optimization: Models objects (e.g., antigens, landmarks) as particles connected by springs (for measured dissimilarities) and subject to repulsive forces (for missing dissimilarities), and simulates the physical system using laws of mechanics, reducing the need for complex gradient computations.\n    * Automatic Dimensionality Detection: Employs a likelihood-based approach to determine the optimal number of dimensions for the embedding/map, avoiding distortions common in methods with fixed low dimensions.\n    * Noise and Bias Reduction: Naturally mitigates experimental noise and bias through its network-based, error-dampening mechanism.\n    * Antigenic Velocity Calculation (for antigenic data): Introduces and quantifies \"antigenic velocity,\" a vector that describes the rate and direction of antigenic drift for each pathogen isolate. This can help identify cluster transitions and potential lineage replacements.\n    * Broad Applicability: Analyzes data from various objects that their dissimilarity may be of interest, ranging from complex biological measurements such as continuous and relational phenotypes, antibody-antigen interactions, and protein folding to abstract concepts, such as customer perception of different brands.\n    Methods are described in the context of bioinformatics applications in Arhami and Rohani (2025a) <doi:10.1093/bioinformatics/btaf372>, and mathematical proofs and Euclidean embedding details are in Arhami and Rohani (2025b) <doi:10.48550/arXiv.2508.01733>.",
    "version": "2.0.1",
    "maintainer": "Omid Arhami <omid.arhami@uga.edu>",
    "author": "Omid Arhami [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0005-2681-6598>)",
    "url": "https://github.com/omid-arhami/topolow",
    "bug_reports": "https://github.com/omid-arhami/topolow/issues",
    "repository": "https://cran.r-project.org/package=topolow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topolow Force-Directed Euclidean Embedding of Dissimilarity Data A robust implementation of Topolow algorithm. It embeds objects into a low-dimensional Euclidean space from a matrix of pairwise dissimilarities, even when the data do not satisfy metric or Euclidean axioms. The package is particularly well-suited for sparse, incomplete, and censored (thresholded) datasets such as antigenic relationships. The core is a physics-inspired, gradient-free optimization framework that models objects as particles in a physical system, where observed dissimilarities define spring rest lengths and unobserved pairs exert repulsive forces. The package also provides functions specific to antigenic mapping to transform cross-reactivity and binding affinity measurements into accurate spatial representations in a phenotype space.\n    Key features include:\n    * Robust Embedding from Sparse Data: Effectively creates complete and consistent maps (in optimal dimensions) even with high proportions of missing data (e.g., >95%).\n    * Physics-Inspired Optimization: Models objects (e.g., antigens, landmarks) as particles connected by springs (for measured dissimilarities) and subject to repulsive forces (for missing dissimilarities), and simulates the physical system using laws of mechanics, reducing the need for complex gradient computations.\n    * Automatic Dimensionality Detection: Employs a likelihood-based approach to determine the optimal number of dimensions for the embedding/map, avoiding distortions common in methods with fixed low dimensions.\n    * Noise and Bias Reduction: Naturally mitigates experimental noise and bias through its network-based, error-dampening mechanism.\n    * Antigenic Velocity Calculation (for antigenic data): Introduces and quantifies \"antigenic velocity,\" a vector that describes the rate and direction of antigenic drift for each pathogen isolate. This can help identify cluster transitions and potential lineage replacements.\n    * Broad Applicability: Analyzes data from various objects that their dissimilarity may be of interest, ranging from complex biological measurements such as continuous and relational phenotypes, antibody-antigen interactions, and protein folding to abstract concepts, such as customer perception of different brands.\n    Methods are described in the context of bioinformatics applications in Arhami and Rohani (2025a) <doi:10.1093/bioinformatics/btaf372>, and mathematical proofs and Euclidean embedding details are in Arhami and Rohani (2025b) <doi:10.48550/arXiv.2508.01733>.  "
  },
  {
    "id": 22310,
    "package_name": "tr.iatgen",
    "title": "Translate 'iatgen' Generated QSF Files",
    "description": "Automates translating the instructions of 'iatgen' generated qsf\n             (Qualtrics survey files) to other languages using either officially\n             supported or user-supplied translations (for tutorial see Santos\n             et al., 2023 <doi:10.17504/protocols.io.kxygx34jdg8j/v1>).",
    "version": "1.1.0",
    "maintainer": "Michal Kouril <Michal.Kouril@cchmc.org>",
    "author": "Michal Kouril [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4786-7934>),\n  Jo\u00e3o O. Santos [aut] (ORCID: <https://orcid.org/0000-0001-6640-126X>)",
    "url": "https://github.com/iatgen/tr.iatgen",
    "bug_reports": "https://github.com/iatgen/tr.iatgen/issues",
    "repository": "https://cran.r-project.org/package=tr.iatgen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tr.iatgen Translate 'iatgen' Generated QSF Files Automates translating the instructions of 'iatgen' generated qsf\n             (Qualtrics survey files) to other languages using either officially\n             supported or user-supplied translations (for tutorial see Santos\n             et al., 2023 <doi:10.17504/protocols.io.kxygx34jdg8j/v1>).  "
  },
  {
    "id": 22344,
    "package_name": "transform.hazards",
    "title": "Transforms Cumulative Hazards to Parameter Specified by ODE\nSystem",
    "description": "Targets parameters that solve Ordinary Differential\n    Equations (ODEs) driven by a vector of cumulative hazard functions.\n    The package provides a method for estimating these parameters using\n    an estimator defined by a corresponding Stochastic Differential Equation\n    (SDE) system driven by cumulative hazard estimates. By providing cumulative\n    hazard estimates as input, the package gives estimates of the parameter as\n    output, along with pointwise (co)variances derived from an asymptotic\n    expression. Examples of parameters that can be targeted in this way include\n    the survival function, the restricted mean survival function, cumulative\n    incidence functions, among others; see Ryalen, Stensrud, and R\u00f8ysland (2018)\n    <doi:10.1093/biomet/asy035>, and further applications in\n    Stensrud, R\u00f8ysland, and Ryalen (2019) <doi:10.1111/biom.13102>\n    and Ryalen et al. (2021) <doi:10.1093/biostatistics/kxab009>.",
    "version": "0.1.1",
    "maintainer": "P\u00e5l Christie Ryalen <p.c.ryalen@medisin.uio.no>",
    "author": "P\u00e5l Christie Ryalen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3236-6782>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=transform.hazards",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transform.hazards Transforms Cumulative Hazards to Parameter Specified by ODE\nSystem Targets parameters that solve Ordinary Differential\n    Equations (ODEs) driven by a vector of cumulative hazard functions.\n    The package provides a method for estimating these parameters using\n    an estimator defined by a corresponding Stochastic Differential Equation\n    (SDE) system driven by cumulative hazard estimates. By providing cumulative\n    hazard estimates as input, the package gives estimates of the parameter as\n    output, along with pointwise (co)variances derived from an asymptotic\n    expression. Examples of parameters that can be targeted in this way include\n    the survival function, the restricted mean survival function, cumulative\n    incidence functions, among others; see Ryalen, Stensrud, and R\u00f8ysland (2018)\n    <doi:10.1093/biomet/asy035>, and further applications in\n    Stensrud, R\u00f8ysland, and Ryalen (2019) <doi:10.1111/biom.13102>\n    and Ryalen et al. (2021) <doi:10.1093/biostatistics/kxab009>.  "
  },
  {
    "id": 22352,
    "package_name": "transmdl",
    "title": "Semiparametric Transformation Models",
    "description": "To make the semiparametric transformation models easier to apply in real studies, \n    we introduce this R package, in which the MLE in transformation models via \n    an EM algorithm proposed by Zeng D, Lin DY(2007) <doi:10.1111/j.1369-7412.2007.00606.x> \n    and adaptive lasso method in transformation models proposed by Liu XX, Zeng \n    D(2013) <doi:10.1093/biomet/ast029> are implemented.  \n    C++ functions are used to compute complex loops. The coefficient vector and \n    cumulative baseline hazard function can be estimated, along with the \n    corresponding standard errors and P values.",
    "version": "0.1.0",
    "maintainer": "Fengyu Wen <Wenfy1207@qq.com>",
    "author": "Fengyu Wen [aut, cre],\n  Baosheng Liang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=transmdl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transmdl Semiparametric Transformation Models To make the semiparametric transformation models easier to apply in real studies, \n    we introduce this R package, in which the MLE in transformation models via \n    an EM algorithm proposed by Zeng D, Lin DY(2007) <doi:10.1111/j.1369-7412.2007.00606.x> \n    and adaptive lasso method in transformation models proposed by Liu XX, Zeng \n    D(2013) <doi:10.1093/biomet/ast029> are implemented.  \n    C++ functions are used to compute complex loops. The coefficient vector and \n    cumulative baseline hazard function can be estimated, along with the \n    corresponding standard errors and P values.  "
  },
  {
    "id": 22354,
    "package_name": "transplantr",
    "title": "Audit and Research Functions for Transplantation",
    "description": "A set of vectorised functions to calculate medical equations used in transplantation, \n    focused mainly on transplantation of abdominal organs. These functions include donor and recipient\n    risk indices as used by NHS Blood & Transplant, OPTN/UNOS and Eurotransplant, tools for \n    quantifying HLA mismatches, functions for calculating estimated glomerular filtration rate (eGFR), \n    a function to calculate the APRI (AST to platelet ratio) score used in initial screening of suitability to receive a \n    transplant from a hepatitis C seropositive donor and some biochemical unit converter functions. \n    All functions are designed to work with either US or international units.\n    References for the equations are provided in the vignettes and function documentation.",
    "version": "0.2.0",
    "maintainer": "John Asher <john.asher@outlook.com>",
    "author": "John Asher [aut, cre] (ORCID: <https://orcid.org/0000-0001-8735-6453>)",
    "url": "https://transplantr.txtools.net,\nhttps://github.com/johnasher/transplantr",
    "bug_reports": "https://github.com/johnasher/transplantr/issues",
    "repository": "https://cran.r-project.org/package=transplantr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transplantr Audit and Research Functions for Transplantation A set of vectorised functions to calculate medical equations used in transplantation, \n    focused mainly on transplantation of abdominal organs. These functions include donor and recipient\n    risk indices as used by NHS Blood & Transplant, OPTN/UNOS and Eurotransplant, tools for \n    quantifying HLA mismatches, functions for calculating estimated glomerular filtration rate (eGFR), \n    a function to calculate the APRI (AST to platelet ratio) score used in initial screening of suitability to receive a \n    transplant from a hepatitis C seropositive donor and some biochemical unit converter functions. \n    All functions are designed to work with either US or international units.\n    References for the equations are provided in the vignettes and function documentation.  "
  },
  {
    "id": 22355,
    "package_name": "transport",
    "title": "Computation of Optimal Transport Plans and Wasserstein Distances",
    "description": "Solve optimal transport problems. Compute Wasserstein distances (a.k.a. Kantorovitch, Fortet--Mourier, Mallows, Earth Mover's, or minimal L_p distances), return the corresponding transference plans, and display them graphically. Objects that can be compared include grey-scale images, (weighted) point patterns, and mass vectors.",
    "version": "0.15-4",
    "maintainer": "Dominic Schuhmacher <dominic.schuhmacher@mathematik.uni-goettingen.de>",
    "author": "Dominic Schuhmacher [aut, cre],\n  Bj\u00f6rn B\u00e4hre [aut] (aha and power diagrams),\n  Nicolas Bonneel [aut] (networkflow),\n  Carsten Gottschlich [aut] (simplex and shortlist),\n  Valentin Hartmann [aut] (semidiscrete1),\n  Florian Heinemann [aut] (transport_track and networkflow integration),\n  Bernhard Schmitzer [aut] (shielding),\n  J\u00f6rn Schrieber [aut] (subsampling),\n  Timo Wilm [ctb] (wpp)",
    "url": "https://dschuhm1.pages.gwdg.de/software",
    "bug_reports": "https://github.com/dschuhmacher/transport/issues",
    "repository": "https://cran.r-project.org/package=transport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transport Computation of Optimal Transport Plans and Wasserstein Distances Solve optimal transport problems. Compute Wasserstein distances (a.k.a. Kantorovitch, Fortet--Mourier, Mallows, Earth Mover's, or minimal L_p distances), return the corresponding transference plans, and display them graphically. Objects that can be compared include grey-scale images, (weighted) point patterns, and mass vectors.  "
  },
  {
    "id": 22360,
    "package_name": "traudem",
    "title": "Use TauDEM",
    "description": "Simple trustworthy utility functions to use TauDEM \n    (Terrain Analysis Using Digital Elevation Models <https://hydrology.usu.edu/taudem/taudem5/>) command-line interface.\n    This package provides a guide to installation of TauDEM and its dependencies GDAL (Geopatial Data Abstraction Library) \n    and MPI (Message Passing Interface) for different operating systems.\n    Moreover, it checks that TauDEM and its dependencies are correctly installed and included to the PATH, \n    and it provides wrapper commands for calling TauDEM methods from R.",
    "version": "1.0.3",
    "maintainer": "Luca Carraro <Luca.Carraro@eawag.ch>",
    "author": "Luca Carraro [cre, aut],\n  University of Zurich [cph, fnd],\n  Ma\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\n  Wael Sadek [aut],\n  Kirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>)",
    "url": "https://lucarraro.github.io/traudem/,\nhttps://github.com/lucarraro/traudem",
    "bug_reports": "https://github.com/lucarraro/traudem/issues",
    "repository": "https://cran.r-project.org/package=traudem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "traudem Use TauDEM Simple trustworthy utility functions to use TauDEM \n    (Terrain Analysis Using Digital Elevation Models <https://hydrology.usu.edu/taudem/taudem5/>) command-line interface.\n    This package provides a guide to installation of TauDEM and its dependencies GDAL (Geopatial Data Abstraction Library) \n    and MPI (Message Passing Interface) for different operating systems.\n    Moreover, it checks that TauDEM and its dependencies are correctly installed and included to the PATH, \n    and it provides wrapper commands for calling TauDEM methods from R.  "
  },
  {
    "id": 22363,
    "package_name": "trawl",
    "title": "Estimation and Simulation of Trawl Processes",
    "description": "Contains R functions for simulating and estimating integer-valued trawl processes as \n    described in the article Veraart (2019),\"Modeling, simulation and inference for\n    multivariate time series of counts using trawl processes\", Journal of Multivariate Analysis,\n    169, pages 110-129,\n    <doi:10.1016/j.jmva.2018.08.012> and for \n    simulating random vectors from the bivariate negative binomial and the bi- and trivariate \n    logarithmic series distributions.",
    "version": "0.2.2",
    "maintainer": "Almut E. D. Veraart <a.veraart@imperial.ac.uk>",
    "author": "Almut E. D. Veraart",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=trawl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trawl Estimation and Simulation of Trawl Processes Contains R functions for simulating and estimating integer-valued trawl processes as \n    described in the article Veraart (2019),\"Modeling, simulation and inference for\n    multivariate time series of counts using trawl processes\", Journal of Multivariate Analysis,\n    169, pages 110-129,\n    <doi:10.1016/j.jmva.2018.08.012> and for \n    simulating random vectors from the bivariate negative binomial and the bi- and trivariate \n    logarithmic series distributions.  "
  },
  {
    "id": 22421,
    "package_name": "tropAlgebra",
    "title": "Tropical Algebraic Functions",
    "description": "It includes functions like tropical addition, tropical multiplication for vectors and matrices. In tropical algebra, the tropical sum of two numbers is their minimum and the tropical product of two numbers is their ordinary sum. For more information see also I. Simon (1988) Recognizable sets with multiplicities in the tropical semi ring: Volume 324 Lecture Notes I Computer Science, pages 107-120 <doi: 10.1007/BFb0017135>.",
    "version": "0.1.1",
    "maintainer": "Muhammad Kashif Hanif <mkashifhanif@gcuf.edu.pk>",
    "author": "Muhammad Kashif Hanif [cre, aut],\n  Rehman Ahmad [aut],\n  Karl-Heinz Zimmermann [aut],\n  Zia ul Qayyum [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tropAlgebra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tropAlgebra Tropical Algebraic Functions It includes functions like tropical addition, tropical multiplication for vectors and matrices. In tropical algebra, the tropical sum of two numbers is their minimum and the tropical product of two numbers is their ordinary sum. For more information see also I. Simon (1988) Recognizable sets with multiplicities in the tropical semi ring: Volume 324 Lecture Notes I Computer Science, pages 107-120 <doi: 10.1007/BFb0017135>.  "
  },
  {
    "id": 22422,
    "package_name": "trotter",
    "title": "Pseudo-Vectors Containing All Permutations, Combinations and\nSubsets of Objects Taken from a Vector",
    "description": "Class definitions and constructors for pseudo-vectors containing\n    all permutations, combinations and subsets of objects taken from a vector.\n    Simplifies working with structures commonly encountered in combinatorics.",
    "version": "0.6",
    "maintainer": "Richard Ambler <rambler@ibwya.net>",
    "author": "Richard Ambler",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=trotter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trotter Pseudo-Vectors Containing All Permutations, Combinations and\nSubsets of Objects Taken from a Vector Class definitions and constructors for pseudo-vectors containing\n    all permutations, combinations and subsets of objects taken from a vector.\n    Simplifies working with structures commonly encountered in combinatorics.  "
  },
  {
    "id": 22481,
    "package_name": "tsnet",
    "title": "Fitting, Comparing, and Visualizing Networks Based on Time\nSeries Data",
    "description": "Fit, compare, and visualize Bayesian graphical vector autoregressive (GVAR) network models using 'Stan'. These models are commonly used in psychology to represent temporal and contemporaneous relationships between multiple variables in intensive longitudinal data. Fitted models can be compared with a test based on matrix norm differences of posterior point estimates to quantify the differences between two estimated networks. See also Siepe, Kloft & Heck (2024) <doi:10.31234/osf.io/uwfjc>.",
    "version": "0.2.0",
    "maintainer": "Bj\u00f6rn S. Siepe <bjoernsiepe@gmail.com>",
    "author": "Bj\u00f6rn S. Siepe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9558-4648>),\n  Matthias Kloft [aut] (ORCID: <https://orcid.org/0000-0003-1845-6957>),\n  Daniel W. Heck [ctb] (ORCID: <https://orcid.org/0000-0002-6302-9252>)",
    "url": "https://github.com/bsiepe/tsnet",
    "bug_reports": "https://github.com/bsiepe/tsnet/issues",
    "repository": "https://cran.r-project.org/package=tsnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsnet Fitting, Comparing, and Visualizing Networks Based on Time\nSeries Data Fit, compare, and visualize Bayesian graphical vector autoregressive (GVAR) network models using 'Stan'. These models are commonly used in psychology to represent temporal and contemporaneous relationships between multiple variables in intensive longitudinal data. Fitted models can be compared with a test based on matrix norm differences of posterior point estimates to quantify the differences between two estimated networks. See also Siepe, Kloft & Heck (2024) <doi:10.31234/osf.io/uwfjc>.  "
  },
  {
    "id": 22520,
    "package_name": "turner",
    "title": "Turn Vectors and Lists of Vectors into Indexed Structures",
    "description": "Package designed for working with vectors and lists of vectors,\n    mainly for turning them into other indexed data structures.",
    "version": "0.2.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Frederic Bertrand [cre] (ORCID:\n    <https://orcid.org/0000-0002-0837-8281>),\n  Gaston Sanchez [aut]",
    "url": "https://fbertran.github.io/turner/,\nhttps://github.com/fbertran/turner/",
    "bug_reports": "https://github.com/fbertran/turner/issues/",
    "repository": "https://cran.r-project.org/package=turner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "turner Turn Vectors and Lists of Vectors into Indexed Structures Package designed for working with vectors and lists of vectors,\n    mainly for turning them into other indexed data structures.  "
  },
  {
    "id": 22541,
    "package_name": "tweenr",
    "title": "Interpolate Data for Smooth Animations",
    "description": "In order to create smooth animation between states of data,\n    tweening is necessary. This package provides a range of functions for\n    creating tweened data that can be used as basis for animation. Furthermore \n    it adds a number of vectorized interpolaters for common R data \n    types such as numeric, date and colour.",
    "version": "2.0.3",
    "maintainer": "Thomas Lin Pedersen <thomasp85@gmail.com>",
    "author": "Thomas Lin Pedersen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5147-4711>)",
    "url": "https://github.com/thomasp85/tweenr",
    "bug_reports": "https://github.com/thomasp85/tweenr/issues",
    "repository": "https://cran.r-project.org/package=tweenr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tweenr Interpolate Data for Smooth Animations In order to create smooth animation between states of data,\n    tweening is necessary. This package provides a range of functions for\n    creating tweened data that can be used as basis for animation. Furthermore \n    it adds a number of vectorized interpolaters for common R data \n    types such as numeric, date and colour.  "
  },
  {
    "id": 22546,
    "package_name": "twig",
    "title": "For Streamlining Decision and Economic Evaluation Models using\nGrammar of Modeling",
    "description": "Provides tools for building decision and cost-effectiveness analysis models. It enables users to write these models concisely, simulate outcomes\u2014including probabilistic analyses\u2014efficiently using optimized vectorized processes and parallel computing, and produce results. The package employs a Grammar of Modeling approach, inspired by the Grammar of Graphics, to streamline model construction. For an interactive graphical user interface, see 'DecisionTwig' at <https://www.dashlab.ca/projects/decision_twig/>. Comprehensive tutorials and vignettes are available at <https://hjalal.github.io/twig/>.",
    "version": "1.0.0.0",
    "maintainer": "Hawre Jalal <hjalal@uottawa.ca>",
    "author": "Hawre Jalal [aut, cre] (ORCID: <https://orcid.org/0000-0002-8224-6834>)",
    "url": "https://www.dashlab.ca/, https://hjalal.github.io/twig/,\nhttps://www.dashlab.ca/projects/decision_twig/",
    "bug_reports": "https://github.com/hjalal/twig/issues",
    "repository": "https://cran.r-project.org/package=twig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twig For Streamlining Decision and Economic Evaluation Models using\nGrammar of Modeling Provides tools for building decision and cost-effectiveness analysis models. It enables users to write these models concisely, simulate outcomes\u2014including probabilistic analyses\u2014efficiently using optimized vectorized processes and parallel computing, and produce results. The package employs a Grammar of Modeling approach, inspired by the Grammar of Graphics, to streamline model construction. For an interactive graphical user interface, see 'DecisionTwig' at <https://www.dashlab.ca/projects/decision_twig/>. Comprehensive tutorials and vignettes are available at <https://hjalal.github.io/twig/>.  "
  },
  {
    "id": 22548,
    "package_name": "twingp",
    "title": "A Fast Global-Local Gaussian Process Approximation",
    "description": "A global-local approximation framework for large-scale Gaussian process modeling. \n                Please see Vakayil and Joseph (2024) <doi:10.1080/00401706.2023.2296451> for details.\n                This work is supported by U.S. NSF grants CMMI-1921646 and DMREF-1921873. ",
    "version": "1.0.0",
    "maintainer": "Akhil Vakayil <akhilv@gatech.edu>",
    "author": "Akhil Vakayil [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3684-6617>),\n  V. Roshan Joseph [aut, ths] (ORCID:\n    <https://orcid.org/0000-0002-9430-5301>),\n  Jose L. Blanco [ctb] (nanoflann author)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twingp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twingp A Fast Global-Local Gaussian Process Approximation A global-local approximation framework for large-scale Gaussian process modeling. \n                Please see Vakayil and Joseph (2024) <doi:10.1080/00401706.2023.2296451> for details.\n                This work is supported by U.S. NSF grants CMMI-1921646 and DMREF-1921873.   "
  },
  {
    "id": 22618,
    "package_name": "unexcel",
    "title": "Revert Excel Serial Dates Back to Intended Day.Month Numerics",
    "description": "Detects values imported from spreadsheets that were auto-converted \n    to Excel date serials and reconstructs the originally intended day.month\n    decimals (for example, '30.3' that Excel displayed as '30/03/2025').\n    The functions work in a vectorized manner, preserve non-serial values, and \n    support both the 1900 and 1904 date systems.",
    "version": "0.1.0",
    "maintainer": "Hercules Freitas <hercules.freitas@uerj.br>",
    "author": "Hercules Freitas [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1584-9157>)",
    "url": "https://github.com/drhrf/unexcel",
    "bug_reports": "https://github.com/drhrf/unexcel/issues",
    "repository": "https://cran.r-project.org/package=unexcel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unexcel Revert Excel Serial Dates Back to Intended Day.Month Numerics Detects values imported from spreadsheets that were auto-converted \n    to Excel date serials and reconstructs the originally intended day.month\n    decimals (for example, '30.3' that Excel displayed as '30/03/2025').\n    The functions work in a vectorized manner, preserve non-serial values, and \n    support both the 1900 and 1904 date systems.  "
  },
  {
    "id": 22643,
    "package_name": "units",
    "title": "Measurement Units for R Vectors",
    "description": "Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details.",
    "version": "1.0-0",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8049-7069>),\n  Thomas Mailund [aut],\n  Tomasz Kalinowski [aut],\n  James Hiebert [ctb],\n  I\u00f1aki Ucar [aut] (ORCID: <https://orcid.org/0000-0001-6403-5550>),\n  Thomas Lin Pedersen [ctb]",
    "url": "https://r-quantities.github.io/units/,\nhttps://github.com/r-quantities/units",
    "bug_reports": "https://github.com/r-quantities/units/issues",
    "repository": "https://cran.r-project.org/package=units",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "units Measurement Units for R Vectors Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details.  "
  },
  {
    "id": 22679,
    "package_name": "urltools",
    "title": "Vectorised Tools for URL Handling and Parsing",
    "description": "A toolkit for all URL-handling needs, including encoding and decoding,\n    parsing, parameter extraction and modification. All functions are\n    designed to be both fast and entirely vectorised. It is intended to be\n    useful for people dealing with web-related datasets, such as server-side\n    logs, although may be useful for other situations involving large sets of\n    URLs.",
    "version": "1.7.3.1",
    "maintainer": "Os Keyes <ironholds@gmail.com>",
    "author": "Os Keyes [aut, cre],\n  Jay Jacobs [aut],\n  Drew Schmidt [aut],\n  Mark Greenaway [ctb],\n  Bob Rudis [ctb],\n  Alex Pinto [ctb],\n  Maryam Khezrzadeh [ctb],\n  Peter Meilstrup [ctb],\n  Adam M. Costello [cph],\n  Jeff Bezanson [cph],\n  Peter Meilstrup [ctb],\n  Xueyuan Jiang [ctb]",
    "url": "https://github.com/Ironholds/urltools/",
    "bug_reports": "https://github.com/Ironholds/urltools/issues",
    "repository": "https://cran.r-project.org/package=urltools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "urltools Vectorised Tools for URL Handling and Parsing A toolkit for all URL-handling needs, including encoding and decoding,\n    parsing, parameter extraction and modification. All functions are\n    designed to be both fast and entirely vectorised. It is intended to be\n    useful for people dealing with web-related datasets, such as server-side\n    logs, although may be useful for other situations involving large sets of\n    URLs.  "
  },
  {
    "id": 22681,
    "package_name": "ursa",
    "title": "Non-Interactive Spatial Tools for Raster Processing and\nVisualization",
    "description": "S3 classes and methods for manipulation with georeferenced raster data: reading/writing, processing, multi-panel visualization.",
    "version": "3.11.4",
    "maintainer": "Nikita Platonov <platonov@sev-in.ru>",
    "author": "Nikita Platonov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7196-7882>)",
    "url": "https://github.com/nplatonov/ursa",
    "bug_reports": "https://github.com/nplatonov/ursa/issues",
    "repository": "https://cran.r-project.org/package=ursa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ursa Non-Interactive Spatial Tools for Raster Processing and\nVisualization S3 classes and methods for manipulation with georeferenced raster data: reading/writing, processing, multi-panel visualization.  "
  },
  {
    "id": 22695,
    "package_name": "usefun",
    "title": "A Collection of Useful Functions by John",
    "description": "A set of general functions that I have used in various\n    projects and other R packages. Miscellaneous operations on data\n    frames, matrices and vectors, ROC and PR statistics.",
    "version": "0.5.2",
    "maintainer": "John Zobolas <bblodfon@gmail.com>",
    "author": "John Zobolas [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3609-8674>)",
    "url": "https://github.com/bblodfon/usefun",
    "bug_reports": "https://github.com/bblodfon/usefun/issues",
    "repository": "https://cran.r-project.org/package=usefun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usefun A Collection of Useful Functions by John A set of general functions that I have used in various\n    projects and other R packages. Miscellaneous operations on data\n    frames, matrices and vectors, ROC and PR statistics.  "
  },
  {
    "id": 22746,
    "package_name": "valmetrics",
    "title": "Metrics and Plots for Model Evaluation",
    "description": "Functions for metrics and plots for model evaluation. Based on vectors of observed and predicted values. Method: Kristin Piikki, Johanna Wetterlind, Mats Soderstrom and Bo Stenberg (2021). <doi:10.1111/SUM.12694>. ",
    "version": "1.0.0",
    "maintainer": "Kristin Piikki <kristin.piikki@slu.se>",
    "author": "Kristin Piikki [aut, cre, cph],\n  Johanna Wetterlind [aut, cph],\n  Mats Soderstrom [aut, cph],\n  Bo Stenberg [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=valmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "valmetrics Metrics and Plots for Model Evaluation Functions for metrics and plots for model evaluation. Based on vectors of observed and predicted values. Method: Kristin Piikki, Johanna Wetterlind, Mats Soderstrom and Bo Stenberg (2021). <doi:10.1111/SUM.12694>.   "
  },
  {
    "id": 22752,
    "package_name": "valueprhr",
    "title": "Value-Price Analysis with Bayesian and Panel Data Methods",
    "description": "Provides tools for analyzing the relationship between direct\n    prices (based on labor values) and prices of production using Bayesian\n    generalized linear models, panel data methods, partial least squares\n    regression, canonical correlation analysis, and panel vector\n    autoregression. Includes functions for model comparison, out-of-sample\n    validation, and structural break detection. Here, methods use raw accounting data with explicit temporal structure, following Gomez Julian (2023) <doi:10.17605/OSF.IO/7J8KF>\n    and standard econometric techniques for panel data analysis.",
    "version": "0.1.0",
    "maintainer": "Jose Mauricio Gomez Julian <isadore.nabi@pm.me>",
    "author": "Jose Mauricio Gomez Julian [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-2412-3150>)",
    "url": "https://github.com/isadorenabi/valueprhr",
    "bug_reports": "https://github.com/isadorenabi/valueprhr/issues",
    "repository": "https://cran.r-project.org/package=valueprhr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "valueprhr Value-Price Analysis with Bayesian and Panel Data Methods Provides tools for analyzing the relationship between direct\n    prices (based on labor values) and prices of production using Bayesian\n    generalized linear models, panel data methods, partial least squares\n    regression, canonical correlation analysis, and panel vector\n    autoregression. Includes functions for model comparison, out-of-sample\n    validation, and structural break detection. Here, methods use raw accounting data with explicit temporal structure, following Gomez Julian (2023) <doi:10.17605/OSF.IO/7J8KF>\n    and standard econometric techniques for panel data analysis.  "
  },
  {
    "id": 22757,
    "package_name": "vapour",
    "title": "Access to the 'Geospatial Data Abstraction Library' ('GDAL')",
    "description": "Provides low-level access to 'GDAL' functionality.  \n  'GDAL' is the 'Geospatial Data Abstraction Library' a translator for raster and vector geospatial data formats \n  that presents a single raster abstract data model and single vector abstract data model to the calling application \n  for all supported formats <https://gdal.org/>. This package is focussed on providing exactly and only what GDAL does, to enable\n  developing further tools. ",
    "version": "0.13.0",
    "maintainer": "Michael Sumner <mdsumner@gmail.com>",
    "author": "Michael Sumner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2471-7511>),\n  Simon Wotherspoon [ctb] (RasterIO configuration for resampling options),\n  Mark Padgham [ctb] (helped get started :)),\n  Edzer Pebesma [ctb] (wrote the field-read handling, adapted here from\n    sf),\n  Roger Bivand [ctb] (wrote configure.ac, adapted here from rgdal),\n  Jim Hester [ctb, cph] (wrote CollectorList.h, copied here from fs\n    package),\n  Timothy Keitt [ctb] (wrote GetPointsInternal copied here from rgdal2\n    package),\n  Jeroen Ooms [ctb] (tweaked build process, provided Windows build tools),\n  Dale Maschette [ctb] (created the hex logo),\n  Joseph Stachelek [ctb],\n  Even Rouault [ctb] (primary author of the COG format and its use of the\n    GDALwarp app-library, example code used by the warper function\n    here),\n  Robert Hijmans [ctb] (code in terra package used as\n    example/inspiration),\n  Dewey Dunnington [ctb] (wrote the columnar-access mode streaming Arrow\n    support),\n  Tomas Kalibera [ctb]",
    "url": "https://github.com/hypertidy/vapour,\nhttps://hypertidy.github.io/vapour/",
    "bug_reports": "https://github.com/hypertidy/vapour/issues",
    "repository": "https://cran.r-project.org/package=vapour",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vapour Access to the 'Geospatial Data Abstraction Library' ('GDAL') Provides low-level access to 'GDAL' functionality.  \n  'GDAL' is the 'Geospatial Data Abstraction Library' a translator for raster and vector geospatial data formats \n  that presents a single raster abstract data model and single vector abstract data model to the calling application \n  for all supported formats <https://gdal.org/>. This package is focussed on providing exactly and only what GDAL does, to enable\n  developing further tools.   "
  },
  {
    "id": 22800,
    "package_name": "vec2dtransf",
    "title": "2D Cartesian Coordinate Transformation",
    "description": "Applies affine and similarity transformations on vector spatial data (sp objects). Transformations can be defined from control points or directly from parameters. If redundant control points are provided Least Squares is applied allowing to obtain residuals and RMSE.",
    "version": "1.1.5",
    "maintainer": "German Carrillo <geotux_tuxman@linuxmail.org>",
    "author": "German Carrillo [cre, aut]",
    "url": "https://github.com/gacarrillor/vec2dtransf",
    "bug_reports": "https://github.com/gacarrillor/vec2dtransf/issues",
    "repository": "https://cran.r-project.org/package=vec2dtransf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vec2dtransf 2D Cartesian Coordinate Transformation Applies affine and similarity transformations on vector spatial data (sp objects). Transformations can be defined from control points or directly from parameters. If redundant control points are provided Least Squares is applied allowing to obtain residuals and RMSE.  "
  },
  {
    "id": 22801,
    "package_name": "veccompare",
    "title": "Perform Set Operations on Vectors, Automatically Generating All\nn-Wise Comparisons, and Create Markdown Output",
    "description": "Automates set operations (i.e., comparisons of overlap) between multiple vectors.\n    It also contains a function for automating reporting in 'RMarkdown', by generating markdown output for easy analysis, as well as an 'RMarkdown' template for use with 'RStudio'.",
    "version": "0.1.0",
    "maintainer": "Jacob Gerard Levernier <jlevern@upenn.edu>",
    "author": "Jacob Gerard Levernier [aut, cre] (Designed and authored the package\n    source code and documentation. Roles: author, creator, designer,\n    engineer, programmer),\n  Heather Gaile Wacha [aut] (Provided intellectual overview and\n    consultation during development for use with medieval cartographic\n    datasets. Roles: conceptor, consultant, data contributor)",
    "url": "https://github.com/publicus/r-veccompare",
    "bug_reports": "https://github.com/publicus/r-veccompare/issues",
    "repository": "https://cran.r-project.org/package=veccompare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "veccompare Perform Set Operations on Vectors, Automatically Generating All\nn-Wise Comparisons, and Create Markdown Output Automates set operations (i.e., comparisons of overlap) between multiple vectors.\n    It also contains a function for automating reporting in 'RMarkdown', by generating markdown output for easy analysis, as well as an 'RMarkdown' template for use with 'RStudio'.  "
  },
  {
    "id": 22802,
    "package_name": "vecmatch",
    "title": "Generalized Propensity Score Estimation and Matching for\nMultiple Groups",
    "description": "Implements the Vector Matching algorithm to match multiple\n    treatment groups based on previously estimated generalized propensity\n    scores. The package includes tools for visualizing initial confounder\n    imbalances, estimating treatment assignment probabilities using various\n    methods, defining the common support region, performing matching across\n    multiple groups, and evaluating matching quality. For more details, see \n    Lopez and Gutman (2017) <doi:10.1214/17-STS612>.",
    "version": "1.3.0",
    "maintainer": "Mateusz Kolek <mati.kolek13@gmail.com>",
    "author": "Mateusz Kolek [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6470-4830>)",
    "url": "https://github.com/Polymerase3/vecmatch",
    "bug_reports": "https://github.com/Polymerase3/vecmatch/issues",
    "repository": "https://cran.r-project.org/package=vecmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vecmatch Generalized Propensity Score Estimation and Matching for\nMultiple Groups Implements the Vector Matching algorithm to match multiple\n    treatment groups based on previously estimated generalized propensity\n    scores. The package includes tools for visualizing initial confounder\n    imbalances, estimating treatment assignment probabilities using various\n    methods, defining the common support region, performing matching across\n    multiple groups, and evaluating matching quality. For more details, see \n    Lopez and Gutman (2017) <doi:10.1214/17-STS612>.  "
  },
  {
    "id": 22803,
    "package_name": "vecsets",
    "title": "Like Set Tools in 'Base' Package but Keeps Duplicate Elements",
    "description": "The 'base'  tools  union() intersect(), etc.,  follow the algebraic definition\n that each element of a set must be unique. \n Since it's often helpful to compare all elements of two vectors,\n this toolset treats every element as unique for counting purposes.\n For ease of use, all functions in vecsets have an argument\n 'multiple' which, when set to FALSE,\n reverts them to the base::sets (alias for all the items) tools functionality.",
    "version": "1.4",
    "maintainer": "Carl Witthoft <cellocgw@gmail.com>",
    "author": "Carl Witthoft [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vecsets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vecsets Like Set Tools in 'Base' Package but Keeps Duplicate Elements The 'base'  tools  union() intersect(), etc.,  follow the algebraic definition\n that each element of a set must be unique. \n Since it's often helpful to compare all elements of two vectors,\n this toolset treats every element as unique for counting purposes.\n For ease of use, all functions in vecsets have an argument\n 'multiple' which, when set to FALSE,\n reverts them to the base::sets (alias for all the items) tools functionality.  "
  },
  {
    "id": 22804,
    "package_name": "vectorbitops",
    "title": "Vector Bitwise Operations",
    "description": "A tool for fast, efficient bitwise operations along the\n    elements within a vector. Provides such functionality for AND, OR and\n    XOR, as well as infix operators for all of the binary bitwise\n    operations.",
    "version": "1.1.2",
    "maintainer": "Samuel Sapire <sapires@protonmail.com>",
    "author": "Samuel Sapire [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vectorbitops",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vectorbitops Vector Bitwise Operations A tool for fast, efficient bitwise operations along the\n    elements within a vector. Provides such functionality for AND, OR and\n    XOR, as well as infix operators for all of the binary bitwise\n    operations.  "
  },
  {
    "id": 22805,
    "package_name": "vectorsurvR",
    "title": "Data Access and Analytical Tools for 'VectorSurv' Users",
    "description": "Allows registered 'VectorSurv' <https://vectorsurv.org/> users access to data through the 'VectorSurv API' <https://api.vectorsurv.org/>. Additionally provides functions for analysis and visualization.",
    "version": "1.6.1",
    "maintainer": "Christina De Cesaris <cmdecesaris@ucdavis.edu>",
    "author": "Christina De Cesaris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vectorsurvR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vectorsurvR Data Access and Analytical Tools for 'VectorSurv' Users Allows registered 'VectorSurv' <https://vectorsurv.org/> users access to data through the 'VectorSurv API' <https://api.vectorsurv.org/>. Additionally provides functions for analysis and visualization.  "
  },
  {
    "id": 22806,
    "package_name": "vectorwavelet",
    "title": "Vector Wavelet Coherence for Multiple Time Series",
    "description": "New wavelet methodology (vector wavelet coherence) (Oygur, T., Unal, G, 2020 <doi:10.1007/s40435-020-00706-y>) \n  to handle dynamic co-movements of multivariate time series via extending multiple and quadruple wavelet coherence methodologies. \n  This package can be used to perform multiple wavelet coherence, quadruple wavelet coherence, and n-dimensional vector wavelet coherence analyses.",
    "version": "0.1.0",
    "maintainer": "Tunc Oygur <info@tuncoygur.com.tr>",
    "author": "Tunc Oygur [aut, cre],\n  Gazanfer Unal [aut],\n  Tarik C. Gouhier [ctb],\n  Aslak Grinsted [ctb],\n  Viliam Simko [ctb]",
    "url": "https://github.com/toygur/vectorwavelet",
    "bug_reports": "https://github.com/toygur/vectorwavelet/issues",
    "repository": "https://cran.r-project.org/package=vectorwavelet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vectorwavelet Vector Wavelet Coherence for Multiple Time Series New wavelet methodology (vector wavelet coherence) (Oygur, T., Unal, G, 2020 <doi:10.1007/s40435-020-00706-y>) \n  to handle dynamic co-movements of multivariate time series via extending multiple and quadruple wavelet coherence methodologies. \n  This package can be used to perform multiple wavelet coherence, quadruple wavelet coherence, and n-dimensional vector wavelet coherence analyses.  "
  },
  {
    "id": 22807,
    "package_name": "vecvec",
    "title": "Construct Mixed Type Data Structures with Vectors of Vectors",
    "description": "Mixed type vectors are useful for combining semantically similar classes. Some examples of semantically related classes include time across different granularities (e.g. daily, monthly, annual) and probability distributions (e.g. Normal, Uniform, Poisson). These groups of vector types typically share common statistical operations which vary in results with the attributes of each vector. The 'vecvec' data structure facilitates efficient storage and computation across multiple vectors within the same object.",
    "version": "0.2.0",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "author": "Mitchell O'Hara-Wild [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6729-7695>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vecvec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vecvec Construct Mixed Type Data Structures with Vectors of Vectors Mixed type vectors are useful for combining semantically similar classes. Some examples of semantically related classes include time across different granularities (e.g. daily, monthly, annual) and probability distributions (e.g. Normal, Uniform, Poisson). These groups of vector types typically share common statistical operations which vary in results with the attributes of each vector. The 'vecvec' data structure facilitates efficient storage and computation across multiple vectors within the same object.  "
  },
  {
    "id": 22815,
    "package_name": "vegdata",
    "title": "Access Vegetation Databases and Treat Taxonomy",
    "description": "Handling of vegetation data from different sources ( Turboveg\n  2.0 <https://www.synbiosys.alterra.nl/turboveg/>; the German national\n  repository <https://www.vegetweb.de> and others. Taxonomic\n  harmonization (given appropriate taxonomic lists, e.g. the Euro+Med\n  list <https://eurosl.infinitenature.org>).",
    "version": "1.9.15",
    "maintainer": "Florian Jansen <florian.jansen@uni-rostock.de>",
    "author": "Florian Jansen [aut, cre]",
    "url": "https://git.loe.auf.uni-rostock.de/jansen/vegdata.git",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vegdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vegdata Access Vegetation Databases and Treat Taxonomy Handling of vegetation data from different sources ( Turboveg\n  2.0 <https://www.synbiosys.alterra.nl/turboveg/>; the German national\n  repository <https://www.vegetweb.de> and others. Taxonomic\n  harmonization (given appropriate taxonomic lists, e.g. the Euro+Med\n  list <https://eurosl.infinitenature.org>).  "
  },
  {
    "id": 22817,
    "package_name": "vegtable",
    "title": "Handling Vegetation Data Sets",
    "description": "Import and handling data from vegetation-plot databases, especially\n    data stored in 'Turboveg 2' (<https://www.synbiosys.alterra.nl/turboveg/>).\n    Also import/export routines for exchange of data with 'Juice'\n    (<https://www.sci.muni.cz/botany/juice/>) are implemented.",
    "version": "0.1.10",
    "maintainer": "Miguel Alvarez <kamapu78@gmail.com>",
    "author": "Miguel Alvarez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1500-1834>)",
    "url": "https://github.com/kamapu/vegtable,\nhttp://kamapu.github.io/vegtable/",
    "bug_reports": "https://github.com/kamapu/vegtable/issues",
    "repository": "https://cran.r-project.org/package=vegtable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vegtable Handling Vegetation Data Sets Import and handling data from vegetation-plot databases, especially\n    data stored in 'Turboveg 2' (<https://www.synbiosys.alterra.nl/turboveg/>).\n    Also import/export routines for exchange of data with 'Juice'\n    (<https://www.sci.muni.cz/botany/juice/>) are implemented.  "
  },
  {
    "id": 22819,
    "package_name": "vek",
    "title": "Predicate Helper Functions for Testing Simple Atomic Vectors",
    "description": "\n  Predicate helper functions for testing atomic vectors in R. All functions take\n  a single argument 'x' and check whether it's of the target type of base-R\n  atomic vector (i.e. no class extensions nor attributes other than 'names'),\n  returning TRUE or FALSE. Some additionally check for value (e.g. absence of\n  missing values, infinities, blank characters, or 'names' attribute; or having\n  length 1).",
    "version": "1.0.0",
    "maintainer": "Sam Semegne <sam.ahoi@hotmail.com>",
    "author": "Sam Semegne [aut, cre]",
    "url": "https://github.com/samsemegne/vek",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vek",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vek Predicate Helper Functions for Testing Simple Atomic Vectors \n  Predicate helper functions for testing atomic vectors in R. All functions take\n  a single argument 'x' and check whether it's of the target type of base-R\n  atomic vector (i.e. no class extensions nor attributes other than 'names'),\n  returning TRUE or FALSE. Some additionally check for value (e.g. absence of\n  missing values, infinities, blank characters, or 'names' attribute; or having\n  length 1).  "
  },
  {
    "id": 22825,
    "package_name": "verbaliseR",
    "title": "Make your Text Mighty Fine",
    "description": "Turn R analysis outputs into full sentences, by writing vectors into in-sentence lists, pluralising words conditionally, spelling out numbers if they are at the start of sentences, writing out dates in full following US or UK style, and managing capitalisations in tidy data.",
    "version": "0.1",
    "maintainer": "Cara Thompson <cara.r.thompson@gmail.com>",
    "author": "Cara Thompson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6472-5073>)",
    "url": "https://github.com/cararthompson/verbaliseR",
    "bug_reports": "https://github.com/cararthompson/verbaliseR/issues",
    "repository": "https://cran.r-project.org/package=verbaliseR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "verbaliseR Make your Text Mighty Fine Turn R analysis outputs into full sentences, by writing vectors into in-sentence lists, pluralising words conditionally, spelling out numbers if they are at the start of sentences, writing out dates in full following US or UK style, and managing capitalisations in tidy data.  "
  },
  {
    "id": 22844,
    "package_name": "via",
    "title": "Virtual Arrays",
    "description": "The base class 'VirtualArray' is defined, which acts as a wrapper around lists allowing users to fold arbitrary sequential data into n-dimensional, R-style virtual arrays. The derived 'XArray' class is defined to be used for homogeneous lists that contain a single class of objects. The 'RasterArray' and 'SfArray' classes enable the use of stacked spatial data instead of lists.",
    "version": "0.2.0",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "author": "Adam T. Kocsis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9028-665X>),\n  Deutsche Forschungsgemeinschaft [fnd],\n  FAU GeoZentrum Nordbayern [fnd]",
    "url": "",
    "bug_reports": "https://github.com/adamkocsis/via",
    "repository": "https://cran.r-project.org/package=via",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "via Virtual Arrays The base class 'VirtualArray' is defined, which acts as a wrapper around lists allowing users to fold arbitrary sequential data into n-dimensional, R-style virtual arrays. The derived 'XArray' class is defined to be used for homogeneous lists that contain a single class of objects. The 'RasterArray' and 'SfArray' classes enable the use of stacked spatial data instead of lists.  "
  },
  {
    "id": 22873,
    "package_name": "visStatistics",
    "title": "Automated Selection and Visualisation of Statistical Hypothesis\nTests",
    "description": "Automatically selects\n    and visualises statistical hypothesis tests between two vectors,\n    based on their class, distribution, sample size, and a user-defined\n    confidence level (conf.level). Visual outputs - including box plots, bar charts,\n    regression lines with confidence bands, mosaic plots,\n    residual plots, and Q-Q plots - are annotated with relevant test statistics,\n    assumption checks, and post-hoc analyses where applicable. \n    The algorithmic workflow helps the user focus on the interpretation of test\n    results rather than test selection. It is particularly suited for quick data\n    analysis, e.g., in statistical consulting projects or educational settings. \n    The test selection algorithm proceeds as follows:   \n    Input vectors of class numeric or integer are\n    considered numerical; those of class factor are considered categorical.\n    Assumptions of residual normality and homogeneity of variances are \n    considered met if the corresponding test yields a p-value greater than the \n    significance level alpha = 1 - conf.level. \n    (1) When the response vector is numerical and the\n    predictor vector is categorical, a test of central tendencies is selected.\n    If the categorical predictor has exactly two levels, \n    t.test() is applied when group sizes exceed 30\n    (Lumley et al. (2002) <doi:10.1146/annurev.publhealth.23.100901.140546>). \n    For smaller samples, normality of residuals is tested using shapiro.test(); \n    if met, t.test() is used; otherwise, wilcox.test(). \n    If the predictor is categorical with more than two levels, an\n    aov() is initially fitted. Residual normality is evaluated using both\n    shapiro.test() and ad.test(); residuals are considered approximately normal\n    if at least one test yields a p-value above alpha. If this assumption is met, \n    bartlett.test() assesses variance homogeneity. \n    If variances are homogeneous, aov() is used; otherwise oneway.test(). \n    Both tests are followed by TukeyHSD().\n    If residual normality cannot be assumed, kruskal.test() is followed by \n    pairwise.wilcox.test().\n    (2) When both the response and predictor vectors are numerical, \n    a simple linear regression model is fitted using lm(). \n    (3) When both vectors are categorical, Cochran's rule \n    (Cochran (1954) <doi:10.2307/3001666>)\n    is applied to test independence either by chisq.test() or fisher.test(). ",
    "version": "0.1.7",
    "maintainer": "Sabine Schilling <sabineschilling@gmx.ch>",
    "author": "Sabine Schilling [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-8318-9421>, year: 2025),\n  Peter Kauf [ctb]",
    "url": "https://github.com/shhschilling/visStatistics,\nhttps://shhschilling.github.io/visStatistics/",
    "bug_reports": "https://github.com/shhschilling/visStatistics/issues",
    "repository": "https://cran.r-project.org/package=visStatistics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "visStatistics Automated Selection and Visualisation of Statistical Hypothesis\nTests Automatically selects\n    and visualises statistical hypothesis tests between two vectors,\n    based on their class, distribution, sample size, and a user-defined\n    confidence level (conf.level). Visual outputs - including box plots, bar charts,\n    regression lines with confidence bands, mosaic plots,\n    residual plots, and Q-Q plots - are annotated with relevant test statistics,\n    assumption checks, and post-hoc analyses where applicable. \n    The algorithmic workflow helps the user focus on the interpretation of test\n    results rather than test selection. It is particularly suited for quick data\n    analysis, e.g., in statistical consulting projects or educational settings. \n    The test selection algorithm proceeds as follows:   \n    Input vectors of class numeric or integer are\n    considered numerical; those of class factor are considered categorical.\n    Assumptions of residual normality and homogeneity of variances are \n    considered met if the corresponding test yields a p-value greater than the \n    significance level alpha = 1 - conf.level. \n    (1) When the response vector is numerical and the\n    predictor vector is categorical, a test of central tendencies is selected.\n    If the categorical predictor has exactly two levels, \n    t.test() is applied when group sizes exceed 30\n    (Lumley et al. (2002) <doi:10.1146/annurev.publhealth.23.100901.140546>). \n    For smaller samples, normality of residuals is tested using shapiro.test(); \n    if met, t.test() is used; otherwise, wilcox.test(). \n    If the predictor is categorical with more than two levels, an\n    aov() is initially fitted. Residual normality is evaluated using both\n    shapiro.test() and ad.test(); residuals are considered approximately normal\n    if at least one test yields a p-value above alpha. If this assumption is met, \n    bartlett.test() assesses variance homogeneity. \n    If variances are homogeneous, aov() is used; otherwise oneway.test(). \n    Both tests are followed by TukeyHSD().\n    If residual normality cannot be assumed, kruskal.test() is followed by \n    pairwise.wilcox.test().\n    (2) When both the response and predictor vectors are numerical, \n    a simple linear regression model is fitted using lm(). \n    (3) When both vectors are categorical, Cochran's rule \n    (Cochran (1954) <doi:10.2307/3001666>)\n    is applied to test independence either by chisq.test() or fisher.test().   "
  },
  {
    "id": 22920,
    "package_name": "voluModel",
    "title": "Modeling Species Distributions in Three Dimensions",
    "description": "Facilitates modeling species' ecological niches and \n  geographic distributions based on occurrences and environments that \n  have a vertical as well as horizontal component, and projecting models \n  into three-dimensional geographic space. Working in three dimensions is \n  useful in an aquatic context when the organisms one wishes to model can \n  be found across a wide range of depths in the water column. The package\n  also contains functions to automatically generate marine training\n  model training regions using machine learning, and interpolate and smooth\n  patchily sampled environmental rasters using thin plate splines.\n  Davis Rabosky AR, Cox CL, Rabosky DL, Title PO, Holmes IA, Feldman A, McGuire JA (2016) <doi:10.1038/ncomms11484>.\n  Nychka D, Furrer R, Paige J, Sain S (2021) <doi:10.5065/D6W957CT>.\n  Pateiro-Lopez B, Rodriguez-Casal A (2022) <https://CRAN.R-project.org/package=alphahull>.",
    "version": "0.2.3",
    "maintainer": "Hannah L. Owens <hannah.owens@gmail.com>",
    "author": "Hannah L. Owens [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0071-1745>),\n  Emmaline Sheahan [aut] (ORCID: <https://orcid.org/0000-0003-3358-9758>),\n  Carsten Rahbek [aut] (ORCID: <https://orcid.org/0000-0003-4585-0300>)",
    "url": "https://hannahlowens.github.io/voluModel/",
    "bug_reports": "https://github.com/hannahlowens/voluModel/issues",
    "repository": "https://cran.r-project.org/package=voluModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "voluModel Modeling Species Distributions in Three Dimensions Facilitates modeling species' ecological niches and \n  geographic distributions based on occurrences and environments that \n  have a vertical as well as horizontal component, and projecting models \n  into three-dimensional geographic space. Working in three dimensions is \n  useful in an aquatic context when the organisms one wishes to model can \n  be found across a wide range of depths in the water column. The package\n  also contains functions to automatically generate marine training\n  model training regions using machine learning, and interpolate and smooth\n  patchily sampled environmental rasters using thin plate splines.\n  Davis Rabosky AR, Cox CL, Rabosky DL, Title PO, Holmes IA, Feldman A, McGuire JA (2016) <doi:10.1038/ncomms11484>.\n  Nychka D, Furrer R, Paige J, Sain S (2021) <doi:10.5065/D6W957CT>.\n  Pateiro-Lopez B, Rodriguez-Casal A (2022) <https://CRAN.R-project.org/package=alphahull>.  "
  },
  {
    "id": 22943,
    "package_name": "vstdct",
    "title": "Nonparametric Estimation of Toeplitz Covariance Matrices",
    "description": "A nonparametric method to estimate Toeplitz covariance matrices from a sample of n independently and identically distributed p-dimensional vectors with mean zero. The data is preprocessed with the discrete cosine matrix and a variance stabilization transformation to obtain an approximate Gaussian regression setting for the log-spectral density function. Estimates of the spectral density function and the inverse of the covariance matrix are provided as well. Functions for simulating data and a protein data example are included. For details see (Klockmann, Krivobokova; 2023), <arXiv:2303.10018>.",
    "version": "0.2",
    "maintainer": "Karolina Klockmann <karolina.klockmann@gmx.de>",
    "author": "Karolina Klockmann [aut, cre],\n  Tatyana Krivobokova [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vstdct",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vstdct Nonparametric Estimation of Toeplitz Covariance Matrices A nonparametric method to estimate Toeplitz covariance matrices from a sample of n independently and identically distributed p-dimensional vectors with mean zero. The data is preprocessed with the discrete cosine matrix and a variance stabilization transformation to obtain an approximate Gaussian regression setting for the log-spectral density function. Estimates of the spectral density function and the inverse of the covariance matrix are provided as well. Functions for simulating data and a protein data example are included. For details see (Klockmann, Krivobokova; 2023), <arXiv:2303.10018>.  "
  },
  {
    "id": 22968,
    "package_name": "wTO",
    "title": "Computing Weighted Topological Overlaps (wTO) & Consensus wTO\nNetwork",
    "description": "Computes the Weighted Topological Overlap with positive and negative signs (wTO) networks given a data frame containing the mRNA count/ expression/ abundance per sample, and a vector containing the interested nodes of interaction (a subset of the elements of the full data frame). It also computes the cut-off threshold or p-value based on the individuals bootstrap or the values reshuffle per individual. It also allows the construction of a consensus network, based on multiple wTO networks. The package includes a visualization tool for the networks.  More about the methodology can be found at <doi:10.1186/s12859-018-2351-7>.",
    "version": "2.1",
    "maintainer": "Deisy Morselli Gysi <deisy.ccnr@gmail.com>",
    "author": "Deisy Morselli Gysi, Andre Voigt, Tiago Miranda Fragoso, Eivind Almaas and Katja Nowick.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wTO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wTO Computing Weighted Topological Overlaps (wTO) & Consensus wTO\nNetwork Computes the Weighted Topological Overlap with positive and negative signs (wTO) networks given a data frame containing the mRNA count/ expression/ abundance per sample, and a vector containing the interested nodes of interaction (a subset of the elements of the full data frame). It also computes the cut-off threshold or p-value based on the individuals bootstrap or the values reshuffle per individual. It also allows the construction of a consensus network, based on multiple wTO networks. The package includes a visualization tool for the networks.  More about the methodology can be found at <doi:10.1186/s12859-018-2351-7>.  "
  },
  {
    "id": 22970,
    "package_name": "wactor",
    "title": "Word Factor Vectors",
    "description": "A user-friendly factor-like interface for converting strings of\n    text into numeric vectors and rectangular data structures.",
    "version": "0.0.1",
    "maintainer": "Michael W. Kearney <kearneymw@missouri.edu>",
    "author": "Michael W. Kearney [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0730-4694>),\n  Lingshu Hu [ctb] (ORCID: <https://orcid.org/0000-0003-0304-882X>)",
    "url": "https://github.com/mkearney/wactor",
    "bug_reports": "https://github.com/mkearney/wactor/issues",
    "repository": "https://cran.r-project.org/package=wactor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wactor Word Factor Vectors A user-friendly factor-like interface for converting strings of\n    text into numeric vectors and rectangular data structures.  "
  },
  {
    "id": 22973,
    "package_name": "wakefield",
    "title": "Generate Random Data Sets",
    "description": "Generates random data sets including: data.frames, lists,\n        and vectors.",
    "version": "0.3.6",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker [aut, cre],\n  Josh O'Brien [ctb],\n  Ananda Mahto [ctb],\n  Matthew Sigal [ctb],\n  Jonathan Carroll [ctb],\n  Scott Westenberger [ctb]",
    "url": "https://github.com/trinker/wakefield",
    "bug_reports": "https://github.com/trinker/wakefield/issues",
    "repository": "https://cran.r-project.org/package=wakefield",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wakefield Generate Random Data Sets Generates random data sets including: data.frames, lists,\n        and vectors.  "
  },
  {
    "id": 22991,
    "package_name": "washeR",
    "title": "Time Series Outlier Detection",
    "description": "Time series outlier detection with non parametric test. This is a new outlier detection methodology (washer): efficient for time saving elaboration and implementation procedures, adaptable for general assumptions and for needing very short time series, reliable and effective as involving robust non parametric test. You can find two approaches: single time series (a vector) and grouped time series (a data frame). For other informations: Andrea Venturini (2011) Statistica - Universita di Bologna, Vol.71, pp.329-344. For an informal explanation look at R-bloggers on web.",
    "version": "0.1.3",
    "maintainer": "Andrea Venturini <andrea.venturini@bancaditalia.it>",
    "author": "Andrea Venturini",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=washeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "washeR Time Series Outlier Detection Time series outlier detection with non parametric test. This is a new outlier detection methodology (washer): efficient for time saving elaboration and implementation procedures, adaptable for general assumptions and for needing very short time series, reliable and effective as involving robust non parametric test. You can find two approaches: single time series (a vector) and grouped time series (a data frame). For other informations: Andrea Venturini (2011) Statistica - Universita di Bologna, Vol.71, pp.329-344. For an informal explanation look at R-bloggers on web.  "
  },
  {
    "id": 22994,
    "package_name": "waspasR",
    "title": "Tool Kit to Implement a W.A.S.P.A.S. Based Multi-Criteria\nDecision Analysis Solution",
    "description": "Provides a set of functions to implement decision-making systems\n    based on the W.A.S.P.A.S. method (Weighted Aggregated Sum Product Assessment),\n    Chakraborty and Zavadskas (2012) <doi:10.5755/j01.eee.122.6.1810>.\n    So this package offers functions that analyze and validate the\n    raw data, which must be entered in a determined format;\n    extract specific vectors and matrices from this raw database;\n    normalize the input data; calculate rankings by intermediate methods;\n    apply the lambda parameter for the main method; and a function that does\n    everything at once. The package has an example database called choppers,\n    with which the user can see how the input data should be organized so that\n    everything works as recommended by the decision methods based on multiple\n    criteria that this package solves. Basically, the data are composed of a set\n    of alternatives, which will be ranked, a set of choice criteria, a matrix\n    of values for each Alternative-Criterion relationship, a vector of weights\n    associated with the criteria, since certain criteria are considered more\n    important than others, as well as a vector that defines each criterion as\n    cost or benefit, this determines the calculation formula, as there are those\n    criteria that we want the highest possible value (e.g. durability)\n    and others that we want the lowest possible value (e.g. price).",
    "version": "0.1.5",
    "maintainer": "Flavio Barbara <flavio.barbara@gmail.com>",
    "author": "Flavio Barbara [cre, aut],\n  Marcos Santos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=waspasR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "waspasR Tool Kit to Implement a W.A.S.P.A.S. Based Multi-Criteria\nDecision Analysis Solution Provides a set of functions to implement decision-making systems\n    based on the W.A.S.P.A.S. method (Weighted Aggregated Sum Product Assessment),\n    Chakraborty and Zavadskas (2012) <doi:10.5755/j01.eee.122.6.1810>.\n    So this package offers functions that analyze and validate the\n    raw data, which must be entered in a determined format;\n    extract specific vectors and matrices from this raw database;\n    normalize the input data; calculate rankings by intermediate methods;\n    apply the lambda parameter for the main method; and a function that does\n    everything at once. The package has an example database called choppers,\n    with which the user can see how the input data should be organized so that\n    everything works as recommended by the decision methods based on multiple\n    criteria that this package solves. Basically, the data are composed of a set\n    of alternatives, which will be ranked, a set of choice criteria, a matrix\n    of values for each Alternative-Criterion relationship, a vector of weights\n    associated with the criteria, since certain criteria are considered more\n    important than others, as well as a vector that defines each criterion as\n    cost or benefit, this determines the calculation formula, as there are those\n    criteria that we want the highest possible value (e.g. durability)\n    and others that we want the lowest possible value (e.g. price).  "
  },
  {
    "id": 23022,
    "package_name": "wdiEF",
    "title": "Calculation of the Water Deficit Index (WDI) and the Evaporative\nFraction (EF) on Rasters",
    "description": "Provides functions to calculate the Water Deficit Index (WDI) and \n    the Evaporative Fraction (EF) using geospatial raster data such as fractional \n    vegetation cover (FVC) and surface-air temperature difference (TS-TA). \n    The package automates regression-based edge fitting and produces continuous \n    spatial maps of surface moisture and evaporative dynamics.",
    "version": "1.0.4",
    "maintainer": "Gaelle Hamelin <gaelle.hamelin@institut-agro.fr>",
    "author": "Gaelle Hamelin [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-2148-7937>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wdiEF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wdiEF Calculation of the Water Deficit Index (WDI) and the Evaporative\nFraction (EF) on Rasters Provides functions to calculate the Water Deficit Index (WDI) and \n    the Evaporative Fraction (EF) using geospatial raster data such as fractional \n    vegetation cover (FVC) and surface-air temperature difference (TS-TA). \n    The package automates regression-based edge fitting and produces continuous \n    spatial maps of surface moisture and evaporative dynamics.  "
  },
  {
    "id": 23079,
    "package_name": "where",
    "title": "Vectorised Substitution and Evaluation",
    "description": "Provides a clean syntax for vectorising the use of\n    Non-Standard Evaluation (NSE), for example in 'ggplot2', 'dplyr', or\n    'data.table'.",
    "version": "1.0.0",
    "maintainer": "Matt Hendtlass <m.hendtlass@gmail.com>",
    "author": "Matt Hendtlass [aut, cre]",
    "url": "https://github.com/KiwiMateo/where",
    "bug_reports": "https://github.com/KiwiMateo/where/issues",
    "repository": "https://cran.r-project.org/package=where",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "where Vectorised Substitution and Evaluation Provides a clean syntax for vectorising the use of\n    Non-Standard Evaluation (NSE), for example in 'ggplot2', 'dplyr', or\n    'data.table'.  "
  },
  {
    "id": 23085,
    "package_name": "whitebox",
    "title": "'WhiteboxTools' R Frontend",
    "description": "An R frontend for the 'WhiteboxTools' library, which is an advanced geospatial data analysis platform developed by Prof. John Lindsay at the University of Guelph's Geomorphometry and Hydrogeomatics Research Group. 'WhiteboxTools' can be used to perform common geographical information systems (GIS) analysis operations, such as cost-distance analysis, distance buffering, and raster reclassification. Remote sensing and image processing tasks include image enhancement (e.g. panchromatic sharpening, contrast adjustments), image mosaicing, numerous filtering operations, simple classification (k-means), and common image transformations. 'WhiteboxTools' also contains advanced tooling for spatial hydrological analysis (e.g. flow-accumulation, watershed delineation, stream network analysis, sink removal), terrain analysis (e.g. common terrain indices such as slope, curvatures, wetness index, hillshading; hypsometric analysis; multi-scale topographic position analysis), and LiDAR data processing. Suggested citation: Lindsay (2016) <doi:10.1016/j.cageo.2016.07.003>.",
    "version": "2.4.3",
    "maintainer": "Andrew Brown <brown.andrewg@gmail.com>",
    "author": "Qiusheng Wu [aut],\n  Andrew Brown [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4565-533X>)",
    "url": "https://whiteboxr.gishub.org/,\nhttps://github.com/opengeos/whiteboxR",
    "bug_reports": "https://github.com/opengeos/whiteboxR/issues",
    "repository": "https://cran.r-project.org/package=whitebox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "whitebox 'WhiteboxTools' R Frontend An R frontend for the 'WhiteboxTools' library, which is an advanced geospatial data analysis platform developed by Prof. John Lindsay at the University of Guelph's Geomorphometry and Hydrogeomatics Research Group. 'WhiteboxTools' can be used to perform common geographical information systems (GIS) analysis operations, such as cost-distance analysis, distance buffering, and raster reclassification. Remote sensing and image processing tasks include image enhancement (e.g. panchromatic sharpening, contrast adjustments), image mosaicing, numerous filtering operations, simple classification (k-means), and common image transformations. 'WhiteboxTools' also contains advanced tooling for spatial hydrological analysis (e.g. flow-accumulation, watershed delineation, stream network analysis, sink removal), terrain analysis (e.g. common terrain indices such as slope, curvatures, wetness index, hillshading; hypsometric analysis; multi-scale topographic position analysis), and LiDAR data processing. Suggested citation: Lindsay (2016) <doi:10.1016/j.cageo.2016.07.003>.  "
  },
  {
    "id": 23137,
    "package_name": "word2vec",
    "title": "Distributed Representations of Words",
    "description": "Learn vector representations of words by continuous bag of words and skip-gram implementations of the 'word2vec' algorithm. \n    The techniques are detailed in the paper \"Distributed Representations of Words and Phrases and their Compositionality\" by Mikolov et al. (2013), available at <doi:10.48550/arXiv.1310.4546>.",
    "version": "0.4.1",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  BNOSAC [cph] (R wrapper),\n  Max Fomichev [ctb, cph] (Code in src/word2vec)",
    "url": "https://github.com/bnosac/word2vec",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=word2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "word2vec Distributed Representations of Words Learn vector representations of words by continuous bag of words and skip-gram implementations of the 'word2vec' algorithm. \n    The techniques are detailed in the paper \"Distributed Representations of Words and Phrases and their Compositionality\" by Mikolov et al. (2013), available at <doi:10.48550/arXiv.1310.4546>.  "
  },
  {
    "id": 23148,
    "package_name": "wordsalad",
    "title": "Provide Tools to Extract and Analyze Word Vectors",
    "description": "Provides access to various word embedding methods (GloVe, \n    fasttext and word2vec) to extract word vectors using a unified framework to\n    increase reproducibility and correctness.",
    "version": "0.2.0",
    "maintainer": "Emil Hvitfeldt <emilhhvitfeldt@gmail.com>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0679-1945>)",
    "url": "https://github.com/EmilHvitfeldt/wordsalad",
    "bug_reports": "https://github.com/EmilHvitfeldt/wordsalad/issues",
    "repository": "https://cran.r-project.org/package=wordsalad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordsalad Provide Tools to Extract and Analyze Word Vectors Provides access to various word embedding methods (GloVe, \n    fasttext and word2vec) to extract word vectors using a unified framework to\n    increase reproducibility and correctness.  "
  },
  {
    "id": 23149,
    "package_name": "wordvector",
    "title": "Word and Document Vector Models",
    "description": "Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) <doi:10.48550/arXiv.1310.4546> and Latent Semantic Analysis (Deerwester et al., 1990) <doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9>.",
    "version": "0.6.0",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6519-5265>),\n  Jan Wijffels [aut] (Original R code),\n  BNOSAC [cph] (Original R code),\n  Max Fomichev [ctb, cph] (Original C++ code)",
    "url": "https://github.com/koheiw/wordvector",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordvector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordvector Word and Document Vector Models Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) <doi:10.48550/arXiv.1310.4546> and Latent Semantic Analysis (Deerwester et al., 1990) <doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9>.  "
  },
  {
    "id": 23151,
    "package_name": "workspace",
    "title": "Manage Collections of Datasets and Objects",
    "description": "Create, store, read and manage structured collections of\n    datasets and other objects using a 'workspace', then bundle it into a\n    compressed archive.  Using open and interoperable formats makes it\n    possible to exchange bundled data from 'R' to other languages such as\n    'Python' or 'Julia'.  Multiple formats are supported 'Parquet',\n    'JSON', 'yaml', spatial data and raster data are supported.",
    "version": "0.1.6",
    "maintainer": "Eli Daniels <eli.daniels@ardata.fr>",
    "author": "Eli Daniels [aut, cre],\n  David Gohel [aut],\n  ArData [cph, fnd]",
    "url": "https://github.com/ardata-fr/workspace",
    "bug_reports": "https://github.com/ardata-fr/workspace/issues",
    "repository": "https://cran.r-project.org/package=workspace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "workspace Manage Collections of Datasets and Objects Create, store, read and manage structured collections of\n    datasets and other objects using a 'workspace', then bundle it into a\n    compressed archive.  Using open and interoperable formats makes it\n    possible to exchange bundled data from 'R' to other languages such as\n    'Python' or 'Julia'.  Multiple formats are supported 'Parquet',\n    'JSON', 'yaml', spatial data and raster data are supported.  "
  },
  {
    "id": 23173,
    "package_name": "wrMisc",
    "title": "Analyze Experimental High-Throughput (Omics) Data",
    "description": "The efficient treatment and convenient analysis of experimental high-throughput (omics) data gets facilitated through this collection of diverse functions. \n  Several functions address advanced object-conversions, like manipulating lists of lists or lists of arrays, reorganizing lists to arrays or into separate vectors, merging of multiple entries, etc.  \n  Another set of functions provides speed-optimized calculation of standard deviation (sd), coefficient of variance (CV) or standard error of the mean (SEM)  \n  for data in matrixes or means per line with respect to additional grouping (eg n groups of replicates). \n  A group of functions facilitate dealing with non-redundant information, by indexing unique, adding counters to redundant or eliminating lines with respect redundancy in a given reference-column, etc. \n  Help is provided to identify very closely matching numeric values to generate (partial) distance matrixes for very big data in a memory efficient manner or to reduce the complexity of large data-sets by combining very close values. \n  Other functions help aligning a matrix or data.frame to a reference using partial matching or to mine an experimental setup to extract patterns of replicate samples.\n  Many times large experimental datasets need some additional filtering, adequate functions are provided. \n  Convenient data normalization is supported in various different modes, parameter estimation via permutations or boot-strap as well as flexible testing of multiple pair-wise combinations using the framework of 'limma' is provided, too.\n  Batch reading (or writing) of sets of files and combining data to arrays is supported, too. ",
    "version": "1.15.4",
    "maintainer": "Wolfgang Raffelsberger <w.raffelsberger@gmail.com>",
    "author": "Wolfgang Raffelsberger [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wrMisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wrMisc Analyze Experimental High-Throughput (Omics) Data The efficient treatment and convenient analysis of experimental high-throughput (omics) data gets facilitated through this collection of diverse functions. \n  Several functions address advanced object-conversions, like manipulating lists of lists or lists of arrays, reorganizing lists to arrays or into separate vectors, merging of multiple entries, etc.  \n  Another set of functions provides speed-optimized calculation of standard deviation (sd), coefficient of variance (CV) or standard error of the mean (SEM)  \n  for data in matrixes or means per line with respect to additional grouping (eg n groups of replicates). \n  A group of functions facilitate dealing with non-redundant information, by indexing unique, adding counters to redundant or eliminating lines with respect redundancy in a given reference-column, etc. \n  Help is provided to identify very closely matching numeric values to generate (partial) distance matrixes for very big data in a memory efficient manner or to reduce the complexity of large data-sets by combining very close values. \n  Other functions help aligning a matrix or data.frame to a reference using partial matching or to mine an experimental setup to extract patterns of replicate samples.\n  Many times large experimental datasets need some additional filtering, adequate functions are provided. \n  Convenient data normalization is supported in various different modes, parameter estimation via permutations or boot-strap as well as flexible testing of multiple pair-wise combinations using the framework of 'limma' is provided, too.\n  Batch reading (or writing) of sets of files and combining data to arrays is supported, too.   "
  },
  {
    "id": 23205,
    "package_name": "xactonomial",
    "title": "Inference for Functions of Multinomial Parameters",
    "description": "We consider the problem where we observe k vectors (possibly of different lengths), each representing an independent multinomial random vector. For a given function that takes in the concatenated vector of multinomial probabilities and outputs a real number, this is a Monte Carlo estimation procedure of an exact p-value and confidence interval. The resulting inference is valid even in small samples, when the parameter is on the boundary, and when the function is not differentiable at the parameter value, all situations where asymptotic methods and the bootstrap would fail. For more details see Sachs, Fay, and Gabriel (2025) <doi:10.48550/arXiv.2406.19141>.",
    "version": "1.2.0",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "author": "Michael C Sachs [aut, cre],\n  Michael P Fay [aut],\n  Erin E Gabriel [aut],\n  David B Dahl [ctb] ((rbindings.rs))",
    "url": "https://sachsmc.github.io/xactonomial/",
    "bug_reports": "https://github.com/sachsmc/xactonomial/issues/",
    "repository": "https://cran.r-project.org/package=xactonomial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xactonomial Inference for Functions of Multinomial Parameters We consider the problem where we observe k vectors (possibly of different lengths), each representing an independent multinomial random vector. For a given function that takes in the concatenated vector of multinomial probabilities and outputs a real number, this is a Monte Carlo estimation procedure of an exact p-value and confidence interval. The resulting inference is valid even in small samples, when the parameter is on the boundary, and when the function is not differentiable at the parameter value, all situations where asymptotic methods and the bootstrap would fail. For more details see Sachs, Fay, and Gabriel (2025) <doi:10.48550/arXiv.2406.19141>.  "
  },
  {
    "id": 23215,
    "package_name": "xegaDerivationTrees",
    "title": "Generating and Manipulating Derivation Trees",
    "description": "Derivation tree operations are needed for implementing \n            grammar-based genetic programming and grammatical evolution:  \n            Generating a random derivation trees of a context-free grammar \n            of bounded depth, decoding a derivation tree, \n            choosing a random node in a derivation tree, \n            extracting a tree whose root is a specified node, and \n            inserting a subtree into a derivation tree at a specified node.\n            These operations are necessary for the initialization and \n            for decoders of a random population of programs, \n            as well as for implementing crossover and mutation operators.\n            Depth-bounds are guaranteed by switching to a grammar \n            without recursive production rules. \n            For executing the examples, the package 'BNF' is needed.\n            The basic tree operations for generating, extracting, and \n            inserting derivation trees as well as the conditions \n            for guaranteeing complete derivation trees have been \n            presented in Geyer-Schulz (1997, ISBN:978-3-7908-0830-X).\n            The use of random integer vectors for the generation \n            of derivation trees has been introduced in \n            Ryan, C., Collins, J. J., and  O'Neill, M. (1998)\n            <doi:10.1007/BFb0055930> for grammatical evolution.",
    "version": "1.0.0.6",
    "maintainer": "Andreas Geyer-Schulz <Andreas.Geyer-Schulz@kit.edu>",
    "author": "Andreas Geyer-Schulz [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-5237-3579>)",
    "url": "https://github.com/ageyerschulz/xegaDerivationTrees",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xegaDerivationTrees",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xegaDerivationTrees Generating and Manipulating Derivation Trees Derivation tree operations are needed for implementing \n            grammar-based genetic programming and grammatical evolution:  \n            Generating a random derivation trees of a context-free grammar \n            of bounded depth, decoding a derivation tree, \n            choosing a random node in a derivation tree, \n            extracting a tree whose root is a specified node, and \n            inserting a subtree into a derivation tree at a specified node.\n            These operations are necessary for the initialization and \n            for decoders of a random population of programs, \n            as well as for implementing crossover and mutation operators.\n            Depth-bounds are guaranteed by switching to a grammar \n            without recursive production rules. \n            For executing the examples, the package 'BNF' is needed.\n            The basic tree operations for generating, extracting, and \n            inserting derivation trees as well as the conditions \n            for guaranteeing complete derivation trees have been \n            presented in Geyer-Schulz (1997, ISBN:978-3-7908-0830-X).\n            The use of random integer vectors for the generation \n            of derivation trees has been introduced in \n            Ryan, C., Collins, J. J., and  O'Neill, M. (1998)\n            <doi:10.1007/BFb0055930> for grammatical evolution.  "
  },
  {
    "id": 23265,
    "package_name": "xutils",
    "title": "Utility Functions of Fangzhou Xie",
    "description": "This is a collection of some useful functions when dealing with text data. \n    Currently it only contains a very efficient function of decoding HTML entities\n    in character vectors by 'Rcpp' routine.",
    "version": "0.0.2",
    "maintainer": "Fangzhou Xie <fangzhou.xie@rutgers.edu>",
    "author": "Fangzhou Xie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7702-093X>)",
    "url": "https://github.com/fangzhou-xie/xutils,\nhttps://fangzhou-xie.github.io/xutils/index.html",
    "bug_reports": "https://github.com/fangzhou-xie/xutils/issues",
    "repository": "https://cran.r-project.org/package=xutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xutils Utility Functions of Fangzhou Xie This is a collection of some useful functions when dealing with text data. \n    Currently it only contains a very efficient function of decoding HTML entities\n    in character vectors by 'Rcpp' routine.  "
  },
  {
    "id": 23270,
    "package_name": "xxhashlite",
    "title": "Extremely Fast Hashing of R Objects, Raw Data and Files using\n'xxHash' Algorithms",
    "description": "Extremely fast hashing of R objects using 'xxHash'.  R objects are hashed via\n    the standard serialization mechanism in R.  Raw byte vectors and strings\n    can be handled directly for compatibility with hashes created on \n    other systems.  This implementation is a wrapper around the 'xxHash' 'C'\n    library which is available from <https://github.com/Cyan4973/xxHash>.",
    "version": "0.2.2",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph],\n  Yann Collet [ctb, cph] (Author of the embedded xxhash library)",
    "url": "https://github.com/coolbutuseless/xxhashlite",
    "bug_reports": "https://github.com/coolbutuseless/xxhashlite/issues",
    "repository": "https://cran.r-project.org/package=xxhashlite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xxhashlite Extremely Fast Hashing of R Objects, Raw Data and Files using\n'xxHash' Algorithms Extremely fast hashing of R objects using 'xxHash'.  R objects are hashed via\n    the standard serialization mechanism in R.  Raw byte vectors and strings\n    can be handled directly for compatibility with hashes created on \n    other systems.  This implementation is a wrapper around the 'xxHash' 'C'\n    library which is available from <https://github.com/Cyan4973/xxHash>.  "
  },
  {
    "id": 23297,
    "package_name": "yyjsonr",
    "title": "Fast 'JSON', 'NDJSON' and 'GeoJSON' Parser and Generator",
    "description": "A fast 'JSON' parser, generator and validator which converts 'JSON', \n    'NDJSON' (Newline Delimited 'JSON') and 'GeoJSON' (Geographic 'JSON') data \n    to/from R objects.  The standard R data types are \n    supported (e.g. logical, numeric, integer) with configurable handling of NULL \n    and NA values. Data frames, atomic vectors and lists are all supported as data \n    containers translated to/from 'JSON'.  'GeoJSON' data is read in as\n    'simple features' objects.\n    This implementation wraps the 'yyjson' 'C' library which \n    is available from <https://github.com/ibireme/yyjson>.",
    "version": "0.1.21",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph],\n  Yao Yuan [aut, cph] (Author of bundled yyjson),\n  Murat Tasan [ctb] ('json_verbatim' handling)",
    "url": "https://github.com/coolbutuseless/yyjsonr,\nhttps://coolbutuseless.github.io/package/yyjsonr/",
    "bug_reports": "https://github.com/coolbutuseless/yyjsonr/issues",
    "repository": "https://cran.r-project.org/package=yyjsonr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yyjsonr Fast 'JSON', 'NDJSON' and 'GeoJSON' Parser and Generator A fast 'JSON' parser, generator and validator which converts 'JSON', \n    'NDJSON' (Newline Delimited 'JSON') and 'GeoJSON' (Geographic 'JSON') data \n    to/from R objects.  The standard R data types are \n    supported (e.g. logical, numeric, integer) with configurable handling of NULL \n    and NA values. Data frames, atomic vectors and lists are all supported as data \n    containers translated to/from 'JSON'.  'GeoJSON' data is read in as\n    'simple features' objects.\n    This implementation wraps the 'yyjson' 'C' library which \n    is available from <https://github.com/ibireme/yyjson>.  "
  },
  {
    "id": 23298,
    "package_name": "z22",
    "title": "Official Gridded Data from the German Census 2022",
    "description": "Provides fast and easy access to German census grid data\n    from the 2011 and 2022 censuses <https://www.zensus2022.de/>, including a\n    wide range of socio-economic indicators at multiple spatial resolutions\n    (100m, 1km, 10km). Enables efficient download, processing, and analysis\n    of large census datasets covering population, households, families,\n    dwellings, and buildings. Harmonized data structures allow direct\n    comparison with the 2011 census, supporting temporal and spatial analyses.\n    Facilitates conversion of data into common formats for spatial analysis and\n    mapping ('terra', 'sf', 'ggplot2').",
    "version": "1.1.0",
    "maintainer": "Jonas Lieth <jonas.lieth@gesis.org>",
    "author": "Jonas Lieth [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-3451-3176>)",
    "url": "https://github.com/jslth/z22/, https://jslth.github.io/z22/",
    "bug_reports": "https://github.com/jslth/z22/issues",
    "repository": "https://cran.r-project.org/package=z22",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "z22 Official Gridded Data from the German Census 2022 Provides fast and easy access to German census grid data\n    from the 2011 and 2022 censuses <https://www.zensus2022.de/>, including a\n    wide range of socio-economic indicators at multiple spatial resolutions\n    (100m, 1km, 10km). Enables efficient download, processing, and analysis\n    of large census datasets covering population, households, families,\n    dwellings, and buildings. Harmonized data structures allow direct\n    comparison with the 2011 census, supporting temporal and spatial analyses.\n    Facilitates conversion of data into common formats for spatial analysis and\n    mapping ('terra', 'sf', 'ggplot2').  "
  },
  {
    "id": 23327,
    "package_name": "zmisc",
    "title": "Vector Look-Ups and Safer Sampling",
    "description": "A collection of utility functions that facilitate looking up\n    vector values from a lookup table, annotate values in at table for \n    clearer viewing, and support a safer approach to vector sampling, \n    sequence generation, and aggregation.",
    "version": "0.2.3",
    "maintainer": "Magnus Thor Torfason <m@zulutime.net>",
    "author": "Magnus Thor Torfason",
    "url": "https://github.com/torfason/zmisc/,\nhttps://torfason.github.io/zmisc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=zmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zmisc Vector Look-Ups and Safer Sampling A collection of utility functions that facilitate looking up\n    vector values from a lookup table, annotate values in at table for \n    clearer viewing, and support a safer approach to vector sampling, \n    sequence generation, and aggregation.  "
  },
  {
    "id": 23332,
    "package_name": "zonohedra",
    "title": "Compute and Plot Zonohedra from Vector Generators",
    "description": "Computes a zonohedron from real vector generators.  The package also computes zonogons (2D zonotopes) and zonosegs (1D zonotopes).  An elementary S3 class for matroids is included, which supports matroids with rank 3, 2, and 1.  Optimization methods are taken from Heckbert (1985) <https://www.cs.cmu.edu/~ph/zono.ps.gz>.",
    "version": "0.6-0",
    "maintainer": "Glenn Davis <gdavis@gluonics.com>",
    "author": "Glenn Davis [aut, cre]",
    "url": "https://github.com/glenndavis52/zonohedra",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=zonohedra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zonohedra Compute and Plot Zonohedra from Vector Generators Computes a zonohedron from real vector generators.  The package also computes zonogons (2D zonotopes) and zonosegs (1D zonotopes).  An elementary S3 class for matroids is included, which supports matroids with rank 3, 2, and 1.  Optimization methods are taken from Heckbert (1985) <https://www.cs.cmu.edu/~ph/zono.ps.gz>.  "
  }
]