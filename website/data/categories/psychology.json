[
  {
    "id": 1335,
    "package_name": "thurstonianIRT",
    "title": "Thurstonian IRT Models",
    "description": "Fit Thurstonian Item Response Theory (IRT) models in R.\nThis package supports fitting Thurstonian IRT models and its\nextensions using 'Stan', 'lavaan', or 'Mplus' for the model\nestimation. Functionality for extracting results, making\npredictions, and simulating data is provided as well.\nReferences: Brown & Maydeu-Olivares (2011)\n<doi:10.1177/0013164410375112>; B\u00fcrkner et al. (2019)\n<doi:10.1177/0013164419832063>.",
    "version": "0.12.6",
    "maintainer": "Paul-Christian B\u00fcrkner <paul.buerkner@gmail.com>",
    "author": "Paul-Christian B\u00fcrkner [aut, cre],\nAngus Hughes [ctb],\nTrustees of Columbia University [cph]",
    "url": "https://github.com/paul-buerkner/thurstonianIRT",
    "bug_reports": "https://github.com/paul-buerkner/thurstonianIRT/issues",
    "repository": "",
    "exports": [
      [
        "cor_matrix"
      ],
      [
        "empty_block"
      ],
      [
        "fit_TIRT_lavaan"
      ],
      [
        "fit_TIRT_mplus"
      ],
      [
        "fit_TIRT_stan"
      ],
      [
        "gof"
      ],
      [
        "make_lavaan_code"
      ],
      [
        "make_mplus_code"
      ],
      [
        "make_sem_data"
      ],
      [
        "make_stan_data"
      ],
      [
        "make_TIRT_data"
      ],
      [
        "set_block"
      ],
      [
        "set_blocks_from_df"
      ],
      [
        "sim_TIRT_data"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 6.6866,
    "stars": 36,
    "primary_category": "statistics",
    "source_universe": "paul-buerkner",
    "search_text": "thurstonianIRT Thurstonian IRT Models Fit Thurstonian Item Response Theory (IRT) models in R.\nThis package supports fitting Thurstonian IRT models and its\nextensions using 'Stan', 'lavaan', or 'Mplus' for the model\nestimation. Functionality for extracting results, making\npredictions, and simulating data is provided as well.\nReferences: Brown & Maydeu-Olivares (2011)\n<doi:10.1177/0013164410375112>; B\u00fcrkner et al. (2019)\n<doi:10.1177/0013164419832063>. cor_matrix empty_block fit_TIRT_lavaan fit_TIRT_mplus fit_TIRT_stan gof make_lavaan_code make_mplus_code make_sem_data make_stan_data make_TIRT_data set_block set_blocks_from_df sim_TIRT_data cpp"
  },
  {
    "id": 941,
    "package_name": "pangoling",
    "title": "Access to Large Language Model Predictions",
    "description": "Provides access to word predictability estimates using\nlarge language models (LLMs) based on 'transformer'\narchitectures via integration with the 'Hugging Face' ecosystem\n<https://huggingface.co/>. The package interfaces with\npre-trained neural networks and supports both\ncausal/auto-regressive LLMs (e.g., 'GPT-2') and\nmasked/bidirectional LLMs (e.g., 'BERT') to compute the\nprobability of words, phrases, or tokens given their linguistic\ncontext. For details on GPT-2 and causal models, see Radford et\nal. (2019)\n<https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf>,\nfor details on BERT and masked models, see Devlin et al. (2019)\n<doi:10.48550/arXiv.1810.04805>. By enabling a straightforward\nestimation of word predictability, the package facilitates\nresearch in psycholinguistics, computational linguistics, and\nnatural language processing (NLP).",
    "version": "1.0.3",
    "maintainer": "Bruno Nicenboim <b.nicenboim@tilburguniversity.edu>",
    "author": "Bruno Nicenboim [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5176-3943>),\nChris Emmerly [ctb],\nGiovanni Cassani [ctb],\nLisa Levinson [rev],\nUtku Turk [rev]",
    "url": "https://docs.ropensci.org/pangoling/,\nhttps://github.com/ropensci/pangoling",
    "bug_reports": "https://github.com/ropensci/pangoling/issues",
    "repository": "",
    "exports": [
      [
        "causal_config"
      ],
      [
        "causal_lp"
      ],
      [
        "causal_lp_mats"
      ],
      [
        "causal_next_tokens_pred_tbl"
      ],
      [
        "causal_next_tokens_tbl"
      ],
      [
        "causal_pred_mats"
      ],
      [
        "causal_preload"
      ],
      [
        "causal_targets_pred"
      ],
      [
        "causal_tokens_lp_tbl"
      ],
      [
        "causal_tokens_pred_lst"
      ],
      [
        "causal_words_pred"
      ],
      [
        "install_py_pangoling"
      ],
      [
        "installed_py_pangoling"
      ],
      [
        "masked_config"
      ],
      [
        "masked_lp"
      ],
      [
        "masked_preload"
      ],
      [
        "masked_targets_pred"
      ],
      [
        "masked_tokens_pred_tbl"
      ],
      [
        "masked_tokens_tbl"
      ],
      [
        "ntokens"
      ],
      [
        "perplexity_calc"
      ],
      [
        "set_cache_folder"
      ],
      [
        "tokenize_lst"
      ],
      [
        "transformer_vocab"
      ]
    ],
    "topics": [
      [
        "nlp"
      ],
      [
        "psycholinguistics"
      ],
      [
        "transformers"
      ]
    ],
    "score": 6.2833,
    "stars": 12,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pangoling Access to Large Language Model Predictions Provides access to word predictability estimates using\nlarge language models (LLMs) based on 'transformer'\narchitectures via integration with the 'Hugging Face' ecosystem\n<https://huggingface.co/>. The package interfaces with\npre-trained neural networks and supports both\ncausal/auto-regressive LLMs (e.g., 'GPT-2') and\nmasked/bidirectional LLMs (e.g., 'BERT') to compute the\nprobability of words, phrases, or tokens given their linguistic\ncontext. For details on GPT-2 and causal models, see Radford et\nal. (2019)\n<https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf>,\nfor details on BERT and masked models, see Devlin et al. (2019)\n<doi:10.48550/arXiv.1810.04805>. By enabling a straightforward\nestimation of word predictability, the package facilitates\nresearch in psycholinguistics, computational linguistics, and\nnatural language processing (NLP). causal_config causal_lp causal_lp_mats causal_next_tokens_pred_tbl causal_next_tokens_tbl causal_pred_mats causal_preload causal_targets_pred causal_tokens_lp_tbl causal_tokens_pred_lst causal_words_pred install_py_pangoling installed_py_pangoling masked_config masked_lp masked_preload masked_targets_pred masked_tokens_pred_tbl masked_tokens_tbl ntokens perplexity_calc set_cache_folder tokenize_lst transformer_vocab nlp psycholinguistics transformers"
  },
  {
    "id": 359,
    "package_name": "chlorpromazineR",
    "title": "Convert Antipsychotic Doses to Chlorpromazine Equivalents",
    "description": "As different antipsychotic medications have different\npotencies, the doses of different medications cannot be\ndirectly compared. Various strategies are used to convert doses\ninto a common reference so that comparison is meaningful.\nChlorpromazine (CPZ) has historically been used as a reference\nmedication into which other antipsychotic doses can be\nconverted, as \"chlorpromazine-equivalent doses\". Using\nconversion keys generated from widely-cited scientific papers,\ne.g. Gardner et. al 2010 <doi:10.1176/appi.ajp.2009.09060802>\nand Leucht et al. 2016 <doi:10.1093/schbul/sbv167>,\nantipsychotic doses are converted to CPZ (or any specified\nantipsychotic) equivalents. The use of the package is described\nin the included vignette. Not for clinical use.",
    "version": "0.2.0",
    "maintainer": "Eric Brown <eb@ericebrown.com>",
    "author": "Eric Brown [aut, cre] (ORCID: <https://orcid.org/0000-0002-1575-2606>),\nParita Shah [aut] (ORCID: <https://orcid.org/0000-0002-7302-0411>),\nJulia Kim [aut] (ORCID: <https://orcid.org/0000-0002-0379-1333>),\nFrederick Boehm [rev] (ORCID: <https://orcid.org/0000-0002-1644-5931>)",
    "url": "https://docs.ropensci.org/chlorpromazineR/,\nhttps://github.com/ropensci/chlorpromazineR",
    "bug_reports": "https://github.com/ropensci/chlorpromazineR/issues",
    "repository": "",
    "exports": [
      [
        "add_key"
      ],
      [
        "check_ap"
      ],
      [
        "check_key"
      ],
      [
        "to_ap"
      ],
      [
        "to_cpz"
      ],
      [
        "trim_key"
      ]
    ],
    "topics": [
      [
        "antipsychotic"
      ],
      [
        "pharmacology"
      ],
      [
        "psychiatry"
      ],
      [
        "schizophrenia"
      ]
    ],
    "score": 5.2601,
    "stars": 13,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "chlorpromazineR Convert Antipsychotic Doses to Chlorpromazine Equivalents As different antipsychotic medications have different\npotencies, the doses of different medications cannot be\ndirectly compared. Various strategies are used to convert doses\ninto a common reference so that comparison is meaningful.\nChlorpromazine (CPZ) has historically been used as a reference\nmedication into which other antipsychotic doses can be\nconverted, as \"chlorpromazine-equivalent doses\". Using\nconversion keys generated from widely-cited scientific papers,\ne.g. Gardner et. al 2010 <doi:10.1176/appi.ajp.2009.09060802>\nand Leucht et al. 2016 <doi:10.1093/schbul/sbv167>,\nantipsychotic doses are converted to CPZ (or any specified\nantipsychotic) equivalents. The use of the package is described\nin the included vignette. Not for clinical use. add_key check_ap check_key to_ap to_cpz trim_key antipsychotic pharmacology psychiatry schizophrenia"
  },
  {
    "id": 51,
    "package_name": "ExpAnalysis3d",
    "title": "Pacote Para Analise De Experimentos Com Graficos De Superficie\nResposta",
    "description": "Pacote para a analise de experimentos havendo duas variaveis\n    explicativas quantitativas e uma variavel dependente quantitativa. Os\n    experimentos podem ser sem repeticoes ou com delineamento estatistico.\n    Sao ajustados 12 modelos de regressao multipla e plotados graficos de\n    superficie resposta (Hair JF, 2016) <ISBN:13:978-0138132637>.(Package\n    for the analysis of experiments having two explanatory quantitative\n    variables and one quantitative dependent variable. The experiments can\n    be without repetitions or with a statistical design. Twelve multiple\n    regression models are fitted and response surface graphs are plotted\n    (Hair JF, 2016) <ISBN:13:978-0138132637>).",
    "version": "0.1.3",
    "maintainer": "Alcinei Mistico Azevedo <alcineimistico@hotmail.com>",
    "author": "Alcinei Mistico Azevedo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5196-0851>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ExpAnalysis3d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExpAnalysis3d Pacote Para Analise De Experimentos Com Graficos De Superficie\nResposta Pacote para a analise de experimentos havendo duas variaveis\n    explicativas quantitativas e uma variavel dependente quantitativa. Os\n    experimentos podem ser sem repeticoes ou com delineamento estatistico.\n    Sao ajustados 12 modelos de regressao multipla e plotados graficos de\n    superficie resposta (Hair JF, 2016) <ISBN:13:978-0138132637>.(Package\n    for the analysis of experiments having two explanatory quantitative\n    variables and one quantitative dependent variable. The experiments can\n    be without repetitions or with a statistical design. Twelve multiple\n    regression models are fitted and response surface graphs are plotted\n    (Hair JF, 2016) <ISBN:13:978-0138132637>).  "
  },
  {
    "id": 52,
    "package_name": "FAfA",
    "title": "Factor Analysis for All",
    "description": "Provides a comprehensive Shiny-based graphical user interface\n    for conducting a wide range of factor analysis procedures. 'FAfA'\n    (Factor Analysis for All) guides users through data uploading,\n    assumption checking (descriptives, collinearity, multivariate\n    normality, outliers), data wrangling (variable exclusion, data\n    splitting), factor retention analysis (e.g., Parallel Analysis, Hull\n    method, EGA), Exploratory Factor Analysis (EFA) with various rotation\n    and extraction methods, Confirmatory Factor Analysis (CFA) for model\n    testing, Reliability Analysis (e.g., Cronbach's Alpha, McDonald's\n    Omega), Measurement Invariance testing across groups, and item\n    weighting techniques. The application leverages established R packages\n    such as 'lavaan' and 'psych' to perform these analyses, offering an\n    accessible platform for researchers and students. Results are\n    presented in user-friendly tables and plots, with options for\n    downloading outputs.",
    "version": "0.5",
    "maintainer": "Abdullah Faruk KILIC <afarukkilic@trakya.edu.tr>",
    "author": "Abdullah Faruk KILIC [aut, cre],\n  Ahmet Caliskan [aut]",
    "url": "https://github.com/AFarukKILIC/FAfA",
    "bug_reports": "https://github.com/AFarukKILIC/FAfA/issues",
    "repository": "https://cran.r-project.org/package=FAfA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAfA Factor Analysis for All Provides a comprehensive Shiny-based graphical user interface\n    for conducting a wide range of factor analysis procedures. 'FAfA'\n    (Factor Analysis for All) guides users through data uploading,\n    assumption checking (descriptives, collinearity, multivariate\n    normality, outliers), data wrangling (variable exclusion, data\n    splitting), factor retention analysis (e.g., Parallel Analysis, Hull\n    method, EGA), Exploratory Factor Analysis (EFA) with various rotation\n    and extraction methods, Confirmatory Factor Analysis (CFA) for model\n    testing, Reliability Analysis (e.g., Cronbach's Alpha, McDonald's\n    Omega), Measurement Invariance testing across groups, and item\n    weighting techniques. The application leverages established R packages\n    such as 'lavaan' and 'psych' to perform these analyses, offering an\n    accessible platform for researchers and students. Results are\n    presented in user-friendly tables and plots, with options for\n    downloading outputs.  "
  },
  {
    "id": 55,
    "package_name": "FMAT",
    "title": "The Fill-Mask Association Test",
    "description": "\n    The Fill-Mask Association Test ('FMAT')\n    <doi:10.1037/pspa0000396>\n    is an integrative and probability-based method using\n    Masked Language Models to measure conceptual associations\n    (e.g., attitudes, biases, stereotypes, social norms, cultural values)\n    as propositions in natural language.\n    Supported language models include 'BERT'\n    <doi:10.48550/arXiv.1810.04805> and its variants available at 'Hugging Face'\n    <https://huggingface.co/models?pipeline_tag=fill-mask>.\n    Methodological references and installation guidance are provided at\n    <https://psychbruce.github.io/FMAT/>.",
    "version": "2025.12",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/FMAT/",
    "bug_reports": "https://github.com/psychbruce/FMAT/issues",
    "repository": "https://cran.r-project.org/package=FMAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FMAT The Fill-Mask Association Test \n    The Fill-Mask Association Test ('FMAT')\n    <doi:10.1037/pspa0000396>\n    is an integrative and probability-based method using\n    Masked Language Models to measure conceptual associations\n    (e.g., attitudes, biases, stereotypes, social norms, cultural values)\n    as propositions in natural language.\n    Supported language models include 'BERT'\n    <doi:10.48550/arXiv.1810.04805> and its variants available at 'Hugging Face'\n    <https://huggingface.co/models?pipeline_tag=fill-mask>.\n    Methodological references and installation guidance are provided at\n    <https://psychbruce.github.io/FMAT/>.  "
  },
  {
    "id": 87,
    "package_name": "MOTE",
    "title": "Effect Size and Confidence Interval Calculator",
    "description": "Measure of the Effect ('MOTE') is an effect size calculator, including a \n    wide variety of effect sizes in the mean differences family (all versions of d) and \n    the variance overlap family (eta, omega, epsilon, r). 'MOTE' provides non-central \n    confidence intervals for each effect size, relevant test statistics, and output \n    for reporting in APA Style (American Psychological Association, 2010, \n    <ISBN:1433805618>) with 'LaTeX'. In research, an over-reliance on p-values \n    may conceal the fact that a study is under-powered (Halsey, Curran-Everett, \n    Vowler, & Drummond, 2015 <doi:10.1038/nmeth.3288>). A test may be statistically \n    significant, yet practically inconsequential (Fritz, Scherndl, & K\u00fchberger, 2012 \n    <doi:10.1177/0959354312436870>). Although the American Psychological Association \n    has long advocated for the inclusion of effect sizes (Wilkinson & American \n    Psychological Association Task Force on Statistical Inference, 1999 \n    <doi:10.1037/0003-066X.54.8.594>), the vast majority of peer-reviewed, \n    published academic studies stop short of reporting effect sizes and confidence \n    intervals (Cumming, 2013, <doi:10.1177/0956797613504966>). 'MOTE' simplifies \n    the use and interpretation of effect sizes and confidence intervals. ",
    "version": "1.2.2",
    "maintainer": "Erin M. Buchanan <buchananlab@gmail.com>",
    "author": "Erin M. Buchanan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9689-4189>),\n  Amber M Gillenwaters [aut] (ORCID:\n    <https://orcid.org/0000-0002-7580-3591>),\n  John E. Scofield [aut] (ORCID: <https://orcid.org/0000-0001-6345-1181>),\n  K. D. Valentine [aut] (ORCID: <https://orcid.org/0000-0001-6349-5395>)",
    "url": "https://github.com/doomlab/MOTE",
    "bug_reports": "https://github.com/doomlab/MOTE/issues",
    "repository": "https://cran.r-project.org/package=MOTE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOTE Effect Size and Confidence Interval Calculator Measure of the Effect ('MOTE') is an effect size calculator, including a \n    wide variety of effect sizes in the mean differences family (all versions of d) and \n    the variance overlap family (eta, omega, epsilon, r). 'MOTE' provides non-central \n    confidence intervals for each effect size, relevant test statistics, and output \n    for reporting in APA Style (American Psychological Association, 2010, \n    <ISBN:1433805618>) with 'LaTeX'. In research, an over-reliance on p-values \n    may conceal the fact that a study is under-powered (Halsey, Curran-Everett, \n    Vowler, & Drummond, 2015 <doi:10.1038/nmeth.3288>). A test may be statistically \n    significant, yet practically inconsequential (Fritz, Scherndl, & K\u00fchberger, 2012 \n    <doi:10.1177/0959354312436870>). Although the American Psychological Association \n    has long advocated for the inclusion of effect sizes (Wilkinson & American \n    Psychological Association Task Force on Statistical Inference, 1999 \n    <doi:10.1037/0003-066X.54.8.594>), the vast majority of peer-reviewed, \n    published academic studies stop short of reporting effect sizes and confidence \n    intervals (Cumming, 2013, <doi:10.1177/0956797613504966>). 'MOTE' simplifies \n    the use and interpretation of effect sizes and confidence intervals.   "
  },
  {
    "id": 466,
    "package_name": "dextergui",
    "title": "A Graphical User Interface for Dexter",
    "description": "Classical Test and Item analysis, \n  Item Response analysis and data management for educational and psychological tests.",
    "version": "1.0.3",
    "maintainer": "Jesse Koops <jesse.koops@cito.nl>",
    "author": "Jesse Koops [aut, cre],\n  Eva de Schipper [aut],\n  Ivailo Partchev [aut, ctb],\n  Gunter Maris [aut, ctb],\n  Timo Bechger [aut, ctb],\n  Gareth Watts [cph] (author of jquery.sparkline),\n  Hakim El Hattab [cph] (author of zoom.js)",
    "url": "https://dexter-psychometrics.github.io/dexter/",
    "bug_reports": "https://github.com/dexter-psychometrics/dexter/issues",
    "repository": "https://cran.r-project.org/package=dextergui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dextergui A Graphical User Interface for Dexter Classical Test and Item analysis, \n  Item Response analysis and data management for educational and psychological tests.  "
  },
  {
    "id": 519,
    "package_name": "emreliability",
    "title": "Test Reliability and CSEM in Educational Measurement",
    "description": "Provides functions for computing test reliability and conditional\n    standard error of measurement (CSEM) based on the methods described in\n    the Reliability in Educational Measurement chapter of the 5th edition of \n    \"Educational Measurement\" by Lee and Harris (2025, ISBN:9780197654965).",
    "version": "1.0.0",
    "maintainer": "Huan Liu <liuhuanbnu@gmail.com>",
    "author": "Huan Liu [aut, cre, cph],\n  Won-Chan Lee [aut],\n  Min Liang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=emreliability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "emreliability Test Reliability and CSEM in Educational Measurement Provides functions for computing test reliability and conditional\n    standard error of measurement (CSEM) based on the methods described in\n    the Reliability in Educational Measurement chapter of the 5th edition of \n    \"Educational Measurement\" by Lee and Harris (2025, ISBN:9780197654965).  "
  },
  {
    "id": 595,
    "package_name": "fungible",
    "title": "Psychometric Functions from the Waller Lab",
    "description": "Computes fungible coefficients and Monte Carlo data. Underlying theory for these functions is described in the following publications:\n    Waller, N. (2008). Fungible Weights in Multiple Regression. Psychometrika, 73(4), 691-703, <DOI:10.1007/s11336-008-9066-z>.\n    Waller, N. & Jones, J. (2009). Locating the Extrema of Fungible Regression Weights.\n    Psychometrika, 74(4), 589-602, <DOI:10.1007/s11336-008-9087-7>.\n    Waller, N. G. (2016). Fungible Correlation Matrices:\n    A Method for Generating Nonsingular, Singular, and Improper Correlation Matrices for\n    Monte Carlo Research. Multivariate Behavioral Research, 51(4), 554-568.\n    Jones, J. A. & Waller, N. G. (2015). The normal-theory and asymptotic distribution-free (ADF)\n    covariance matrix of standardized regression coefficients: theoretical extensions\n    and finite sample behavior. Psychometrika, 80, 365-378, <DOI:10.1007/s11336-013-9380-y>.\n    Waller, N. G.  (2018).  Direct Schmid-Leiman transformations and rank-deficient loadings matrices.  Psychometrika, 83, 858-870. <DOI:10.1007/s11336-017-9599-0>.",
    "version": "2.4.4.1",
    "maintainer": "Niels Waller <nwaller@umn.edu>",
    "author": "Niels Waller [aut, cre],\n  Justin Kracht [ctb],\n  Jeff Jones [ctb],\n  Casey Giordano [ctb],\n  Hoang V. Nguyen [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fungible",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fungible Psychometric Functions from the Waller Lab Computes fungible coefficients and Monte Carlo data. Underlying theory for these functions is described in the following publications:\n    Waller, N. (2008). Fungible Weights in Multiple Regression. Psychometrika, 73(4), 691-703, <DOI:10.1007/s11336-008-9066-z>.\n    Waller, N. & Jones, J. (2009). Locating the Extrema of Fungible Regression Weights.\n    Psychometrika, 74(4), 589-602, <DOI:10.1007/s11336-008-9087-7>.\n    Waller, N. G. (2016). Fungible Correlation Matrices:\n    A Method for Generating Nonsingular, Singular, and Improper Correlation Matrices for\n    Monte Carlo Research. Multivariate Behavioral Research, 51(4), 554-568.\n    Jones, J. A. & Waller, N. G. (2015). The normal-theory and asymptotic distribution-free (ADF)\n    covariance matrix of standardized regression coefficients: theoretical extensions\n    and finite sample behavior. Psychometrika, 80, 365-378, <DOI:10.1007/s11336-013-9380-y>.\n    Waller, N. G.  (2018).  Direct Schmid-Leiman transformations and rank-deficient loadings matrices.  Psychometrika, 83, 858-870. <DOI:10.1007/s11336-017-9599-0>.  "
  },
  {
    "id": 788,
    "package_name": "manymome",
    "title": "Mediation, Moderation and Moderated-Mediation After Model\nFitting",
    "description": "Computes indirect effects, conditional effects, and conditional\n  indirect effects in a structural equation model or path model after model\n  fitting, with no need to define any user parameters or label any paths in\n  the model syntax, using the approach presented in Cheung and Cheung\n  (2024) <doi:10.3758/s13428-023-02224-z>. Can also form bootstrap\n  confidence intervals by doing bootstrapping only once and reusing the\n  bootstrap estimates in all subsequent computations. Supports bootstrap\n  confidence intervals for standardized (partially or completely) indirect\n  effects, conditional effects, and conditional indirect effects as described\n  in Cheung (2009) <doi:10.3758/BRM.41.2.425> and Cheung, Cheung, Lau, Hui,\n  and Vong (2022) <doi:10.1037/hea0001188>. Model fitting can be done by\n  structural equation modeling using lavaan() or regression using lm().",
    "version": "0.3.2",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "author": "Shu Fai Cheung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9871-9448>),\n  Sing-Hang Cheung [aut] (ORCID: <https://orcid.org/0000-0001-5182-0752>),\n  Rong Wei Sun [ctb] (ORCID: <https://orcid.org/0000-0003-0034-1422>)",
    "url": "https://sfcheung.github.io/manymome/",
    "bug_reports": "https://github.com/sfcheung/manymome/issues",
    "repository": "https://cran.r-project.org/package=manymome",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "manymome Mediation, Moderation and Moderated-Mediation After Model\nFitting Computes indirect effects, conditional effects, and conditional\n  indirect effects in a structural equation model or path model after model\n  fitting, with no need to define any user parameters or label any paths in\n  the model syntax, using the approach presented in Cheung and Cheung\n  (2024) <doi:10.3758/s13428-023-02224-z>. Can also form bootstrap\n  confidence intervals by doing bootstrapping only once and reusing the\n  bootstrap estimates in all subsequent computations. Supports bootstrap\n  confidence intervals for standardized (partially or completely) indirect\n  effects, conditional effects, and conditional indirect effects as described\n  in Cheung (2009) <doi:10.3758/BRM.41.2.425> and Cheung, Cheung, Lau, Hui,\n  and Vong (2022) <doi:10.1037/hea0001188>. Model fitting can be done by\n  structural equation modeling using lavaan() or regression using lm().  "
  },
  {
    "id": 1453,
    "package_name": "wsMed",
    "title": "Within-Subject Mediation Analysis Using Structural Equation\nModeling",
    "description": "Within-subject mediation analysis using structural equation modeling.\n    Examine how changes in an outcome variable between two conditions are mediated \n    through one or more variables. Supports within-subject mediation analysis using \n    the 'lavaan' package by Rosseel (2012) <doi:10.18637/jss.v048.i02>, \n    and extends Monte Carlo confidence interval estimation to missing data scenarios \n    using the 'semmcci' package by Pesigan and Cheung (2023) <doi:10.3758/s13428-023-02114-4>.",
    "version": "1.0.2",
    "maintainer": "Wendie Yang <1581075494q@gmail.com>",
    "author": "Wendie Yang [aut, cre] (ORCID: <https://orcid.org/0009-0000-8388-6481>),\n  Shu Fai Cheung [aut] (ORCID: <https://orcid.org/0000-0002-9871-9448>)",
    "url": "https://yangzhen1999.github.io/wsMed/",
    "bug_reports": "https://github.com/Yangzhen1999/wsMed/issues",
    "repository": "https://cran.r-project.org/package=wsMed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wsMed Within-Subject Mediation Analysis Using Structural Equation\nModeling Within-subject mediation analysis using structural equation modeling.\n    Examine how changes in an outcome variable between two conditions are mediated \n    through one or more variables. Supports within-subject mediation analysis using \n    the 'lavaan' package by Rosseel (2012) <doi:10.18637/jss.v048.i02>, \n    and extends Monte Carlo confidence interval estimation to missing data scenarios \n    using the 'semmcci' package by Pesigan and Cheung (2023) <doi:10.3758/s13428-023-02114-4>.  "
  },
  {
    "id": 1482,
    "package_name": "ABCDscores",
    "title": "Summary Scores of the Adolescent Brain Cognitive Development\n(ABCD) Study",
    "description": "Provides functions to compute summary scores\n  (besides proprietary ones) reported in the tabulated data resource that is\n  released by the Adolescent Brain Cognitive Development (ABCD) study.",
    "version": "6.1.0",
    "maintainer": "Le Zhang <dairc.service@gmail.com>",
    "author": "Le Zhang [aut, cre] (ORCID: <https://orcid.org/0009-0008-0205-2150>),\n  Janosch Linkersdoerfer [aut] (ORCID:\n    <https://orcid.org/0000-0002-1577-1233>),\n  Olivier Celhay [aut] (ORCID: <https://orcid.org/0000-0002-2971-9110>),\n  Biplabendu Das [aut] (ORCID: <https://orcid.org/0000-0003-0855-262X>),\n  Sammy Berman [aut] (ORCID: <https://orcid.org/0000-0002-9803-019X>),\n  Laura Ziemer [aut] (ORCID: <https://orcid.org/0000-0003-0026-3823>)",
    "url": "https://software.nbdc-datahub.org/ABCDscores/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ABCDscores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ABCDscores Summary Scores of the Adolescent Brain Cognitive Development\n(ABCD) Study Provides functions to compute summary scores\n  (besides proprietary ones) reported in the tabulated data resource that is\n  released by the Adolescent Brain Cognitive Development (ABCD) study.  "
  },
  {
    "id": 1500,
    "package_name": "ACTCD",
    "title": "Asymptotic Classification Theory for Cognitive Diagnosis",
    "description": "Cluster analysis for cognitive diagnosis based on the Asymptotic Classification Theory (Chiu, Douglas & Li, 2009; <doi:10.1007/s11336-009-9125-0>). Given the sample statistic of sum-scores, cluster analysis techniques can be used to classify examinees into latent classes based on their attribute patterns. In addition to the algorithms used to classify data, three labeling approaches are proposed to label clusters so that examinees' attribute profiles can be obtained.",
    "version": "1.3-0",
    "maintainer": "Wenchao Ma <wenchao.ma@ua.edu>",
    "author": "Chia-Yi Chiu (University of Minnesota) and Wenchao Ma (The University of Alabama)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ACTCD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ACTCD Asymptotic Classification Theory for Cognitive Diagnosis Cluster analysis for cognitive diagnosis based on the Asymptotic Classification Theory (Chiu, Douglas & Li, 2009; <doi:10.1007/s11336-009-9125-0>). Given the sample statistic of sum-scores, cluster analysis techniques can be used to classify examinees into latent classes based on their attribute patterns. In addition to the algorithms used to classify data, three labeling approaches are proposed to label clusters so that examinees' attribute profiles can be obtained.  "
  },
  {
    "id": 1593,
    "package_name": "ARPobservation",
    "title": "Tools for Simulating Direct Behavioral Observation Recording\nProcedures Based on Alternating Renewal Processes",
    "description": "Tools for simulating data generated by direct observation\n    recording. Behavior streams are simulated based on an alternating renewal\n    process, given specified distributions of event durations and interim\n    times. Different procedures for recording data can then be applied to the\n    simulated behavior streams. Functions are provided for the following\n    recording methods: continuous duration recording, event counting, momentary\n    time sampling, partial interval recording, whole interval recording, and\n    augmented interval recording.",
    "version": "1.2.2",
    "maintainer": "James E. Pustejovsky <jepusto@gmail.com>",
    "author": "James E. Pustejovsky, with contributions from Daniel M. Swan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ARPobservation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ARPobservation Tools for Simulating Direct Behavioral Observation Recording\nProcedures Based on Alternating Renewal Processes Tools for simulating data generated by direct observation\n    recording. Behavior streams are simulated based on an alternating renewal\n    process, given specified distributions of event durations and interim\n    times. Different procedures for recording data can then be applied to the\n    simulated behavior streams. Functions are provided for the following\n    recording methods: continuous duration recording, event counting, momentary\n    time sampling, partial interval recording, whole interval recording, and\n    augmented interval recording.  "
  },
  {
    "id": 1671,
    "package_name": "AlignLV",
    "title": "Multiple Group Item Response Theory Alignment Helpers for\n'lavaan' and 'mirt'",
    "description": "Allows for multiple group item response theory alignment a la 'Mplus' to be applied to lists of single-group models estimated in 'lavaan' or 'mirt'. Allows item sets that are overlapping but not identical, facilitating alignment in secondary data analysis where not all items may be shared across assessments.",
    "version": "0.1.0.0",
    "maintainer": "Maxwell Mansolf <maxwell.mansolf@northwestern.edu>",
    "author": "Maxwell Mansolf [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6861-8657>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AlignLV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AlignLV Multiple Group Item Response Theory Alignment Helpers for\n'lavaan' and 'mirt' Allows for multiple group item response theory alignment a la 'Mplus' to be applied to lists of single-group models estimated in 'lavaan' or 'mirt'. Allows item sets that are overlapping but not identical, facilitating alignment in secondary data analysis where not all items may be shared across assessments.  "
  },
  {
    "id": 1760,
    "package_name": "AzureCognitive",
    "title": "Interface to Azure Cognitive Services",
    "description": "An interface to Azure Cognitive Services <https://learn.microsoft.com/en-us/azure/cognitive-services/>. Both an 'Azure Resource Manager' interface, for deploying Cognitive Services resources, and a client framework are supplied. While 'AzureCognitive' can be called by the end-user, it is meant to provide a foundation for other packages that will support specific services, like Computer Vision, Custom Vision, language translation, and so on. Part of the 'AzureR' family of packages.",
    "version": "1.0.2",
    "maintainer": "Hong Ooi <hongooi73@gmail.com>",
    "author": "Hong Ooi [aut, cre],\n  Microsoft [cph]",
    "url": "https://github.com/Azure/AzureCognitive\nhttps://github.com/Azure/AzureR",
    "bug_reports": "https://github.com/Azure/AzureCognitive/issues",
    "repository": "https://cran.r-project.org/package=AzureCognitive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AzureCognitive Interface to Azure Cognitive Services An interface to Azure Cognitive Services <https://learn.microsoft.com/en-us/azure/cognitive-services/>. Both an 'Azure Resource Manager' interface, for deploying Cognitive Services resources, and a client framework are supplied. While 'AzureCognitive' can be called by the end-user, it is meant to provide a foundation for other packages that will support specific services, like Computer Vision, Custom Vision, language translation, and so on. Part of the 'AzureR' family of packages.  "
  },
  {
    "id": 1772,
    "package_name": "AzureVision",
    "title": "Interface to Azure Computer Vision Services",
    "description": "An interface to 'Azure Computer Vision' <https://docs.microsoft.com/azure/cognitive-services/Computer-vision/Home> and 'Azure Custom Vision' <https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home>, building on the low-level functionality provided by the 'AzureCognitive' package. These services allow users to leverage the cloud to carry out visual recognition tasks using advanced image processing models, without needing powerful hardware of their own. Part of the 'AzureR' family of packages.",
    "version": "1.0.2",
    "maintainer": "Hong Ooi <hongooi73@gmail.com>",
    "author": "Hong Ooi [aut, cre],\n  Microsoft [cph]",
    "url": "https://github.com/Azure/AzureVision\nhttps://github.com/Azure/AzureR",
    "bug_reports": "https://github.com/Azure/AzureVision/issues",
    "repository": "https://cran.r-project.org/package=AzureVision",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AzureVision Interface to Azure Computer Vision Services An interface to 'Azure Computer Vision' <https://docs.microsoft.com/azure/cognitive-services/Computer-vision/Home> and 'Azure Custom Vision' <https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home>, building on the low-level functionality provided by the 'AzureCognitive' package. These services allow users to leverage the cloud to carry out visual recognition tasks using advanced image processing models, without needing powerful hardware of their own. Part of the 'AzureR' family of packages.  "
  },
  {
    "id": 1828,
    "package_name": "BFpack",
    "title": "Flexible Bayes Factor Testing of Scientific Expectations",
    "description": "Implementation of default Bayes factors\n    for testing statistical hypotheses under various statistical models. The package is\n    intended for applied quantitative researchers in the\n    social and behavioral sciences, medical research,\n    and related fields. The Bayes factor tests can be\n    executed for statistical models such as \n    univariate and multivariate normal linear models,\n    correlation analysis, generalized linear models, special cases of \n    linear mixed models, survival models, relational\n    event models. Parameters that can be tested are\n    location parameters (e.g., group means, regression coefficients),\n    variances (e.g., group variances), and measures of \n    association (e.g,. polychoric/polyserial/biserial/tetrachoric/product\n    moments correlations), among others.\n    The statistical underpinnings are\n    described in\n    O'Hagan (1995) <DOI:10.1111/j.2517-6161.1995.tb02017.x>,\n    De Santis and Spezzaferri (2001) <DOI:10.1016/S0378-3758(00)00240-8>,\n    Mulder and Xin (2022) <DOI:10.1080/00273171.2021.1904809>,\n    Mulder and Gelissen (2019) <DOI:10.1080/02664763.2021.1992360>,\n    Mulder (2016) <DOI:10.1016/j.jmp.2014.09.004>,\n    Mulder and Fox (2019) <DOI:10.1214/18-BA1115>,\n    Mulder and Fox (2013) <DOI:10.1007/s11222-011-9295-3>,\n    Boeing-Messing, van Assen, Hofman, Hoijtink, and Mulder (2017) <DOI:10.1037/met0000116>,\n    Hoijtink, Mulder, van Lissa, and Gu (2018) <DOI:10.1037/met0000201>,\n    Gu, Mulder, and Hoijtink (2018) <DOI:10.1111/bmsp.12110>,\n    Hoijtink, Gu, and Mulder (2018) <DOI:10.1111/bmsp.12145>, and\n    Hoijtink, Gu, Mulder, and Rosseel (2018) <DOI:10.1037/met0000187>. When using the\n    packages, please refer to the package Mulder et al. (2021) <DOI:10.18637/jss.v100.i18>\n    and the relevant methodological papers.",
    "version": "1.5.0",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "author": "Joris Mulder [aut, cre],\n  Caspar van Lissa [aut, ctb],\n  Donald R. Williams [aut, ctb],\n  Xin Gu [aut, ctb],\n  Anton Olsson-Collentine [aut, ctb],\n  Florian Boeing-Messing [aut, ctb],\n  Jean-Paul Fox [aut, ctb],\n  Janosch Menke [ctb],\n  Robbie van Aert [ctb],\n  Barry Brown [ctb],\n  James Lovato [ctb],\n  Kathy Russell [ctb],\n  Lapack 3.8 [ctb],\n  Jack Dongarra [ctb],\n  Jim Bunch [ctb],\n  Cleve Moler [ctb],\n  Gilbert Stewart [ctb],\n  John Burkandt [ctb],\n  Ashwith Rego [ctb],\n  Alexander Godunov [ctb],\n  Alan Miller [ctb],\n  Jean-Pierre Moreau [ctb],\n  The R Core Team [cph]",
    "url": "https://github.com/jomulder/BFpack",
    "bug_reports": "https://github.com/jomulder/BFpack/issues",
    "repository": "https://cran.r-project.org/package=BFpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BFpack Flexible Bayes Factor Testing of Scientific Expectations Implementation of default Bayes factors\n    for testing statistical hypotheses under various statistical models. The package is\n    intended for applied quantitative researchers in the\n    social and behavioral sciences, medical research,\n    and related fields. The Bayes factor tests can be\n    executed for statistical models such as \n    univariate and multivariate normal linear models,\n    correlation analysis, generalized linear models, special cases of \n    linear mixed models, survival models, relational\n    event models. Parameters that can be tested are\n    location parameters (e.g., group means, regression coefficients),\n    variances (e.g., group variances), and measures of \n    association (e.g,. polychoric/polyserial/biserial/tetrachoric/product\n    moments correlations), among others.\n    The statistical underpinnings are\n    described in\n    O'Hagan (1995) <DOI:10.1111/j.2517-6161.1995.tb02017.x>,\n    De Santis and Spezzaferri (2001) <DOI:10.1016/S0378-3758(00)00240-8>,\n    Mulder and Xin (2022) <DOI:10.1080/00273171.2021.1904809>,\n    Mulder and Gelissen (2019) <DOI:10.1080/02664763.2021.1992360>,\n    Mulder (2016) <DOI:10.1016/j.jmp.2014.09.004>,\n    Mulder and Fox (2019) <DOI:10.1214/18-BA1115>,\n    Mulder and Fox (2013) <DOI:10.1007/s11222-011-9295-3>,\n    Boeing-Messing, van Assen, Hofman, Hoijtink, and Mulder (2017) <DOI:10.1037/met0000116>,\n    Hoijtink, Mulder, van Lissa, and Gu (2018) <DOI:10.1037/met0000201>,\n    Gu, Mulder, and Hoijtink (2018) <DOI:10.1111/bmsp.12110>,\n    Hoijtink, Gu, and Mulder (2018) <DOI:10.1111/bmsp.12145>, and\n    Hoijtink, Gu, Mulder, and Rosseel (2018) <DOI:10.1037/met0000187>. When using the\n    packages, please refer to the package Mulder et al. (2021) <DOI:10.18637/jss.v100.i18>\n    and the relevant methodological papers.  "
  },
  {
    "id": 1925,
    "package_name": "BUCSS",
    "title": "Bias and Uncertainty Corrected Sample Size",
    "description": "Bias- and Uncertainty-Corrected Sample Size. BUCSS implements a method of correcting for publication bias and\n    uncertainty when planning sample sizes in a future study from an original study. See Anderson, Kelley, & Maxwell (2017; Psychological Science, 28, 1547-1562). ",
    "version": "1.2.1",
    "maintainer": "Ken Kelley <kkelley@nd.edu>",
    "author": "Samantha F. Anderson <samantha.f.anderson@asu.edu>, Ken Kelley <kkelley@nd.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BUCSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BUCSS Bias and Uncertainty Corrected Sample Size Bias- and Uncertainty-Corrected Sample Size. BUCSS implements a method of correcting for publication bias and\n    uncertainty when planning sample sizes in a future study from an original study. See Anderson, Kelley, & Maxwell (2017; Psychological Science, 28, 1547-1562).   "
  },
  {
    "id": 1932,
    "package_name": "BaM",
    "title": "Functions and Datasets for \"Bayesian Methods: A Social and\nBehavioral Sciences Approach\"",
    "description": "Functions and datasets for Jeff Gill: \"Bayesian Methods: A Social and Behavioral Sciences Approach\". First, Second, and Third Edition. Published by Chapman and Hall/CRC (2002, 2007, 2014) <doi:10.1201/b17888>.",
    "version": "1.0.3",
    "maintainer": "Jeff Gill <jgill5402@mac.com>",
    "author": "Jonathan Homola, Danielle Korman, Jacob Metz, Miguel Pereira, Mauricio Vela, and Jeff Gill <jgill5402@mac.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BaM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BaM Functions and Datasets for \"Bayesian Methods: A Social and\nBehavioral Sciences Approach\" Functions and datasets for Jeff Gill: \"Bayesian Methods: A Social and Behavioral Sciences Approach\". First, Second, and Third Edition. Published by Chapman and Hall/CRC (2002, 2007, 2014) <doi:10.1201/b17888>.  "
  },
  {
    "id": 2222,
    "package_name": "CDM",
    "title": "Cognitive Diagnosis Modeling",
    "description": "\n    Functions for cognitive diagnosis modeling and multidimensional item response modeling \n    for dichotomous and polytomous item responses. This package enables the estimation of \n    the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),\n    the multiple group (polytomous) GDINA model (de la Torre, 2011, \n    <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009, \n    <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008, \n    <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992, \n    <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis \n    (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>). \n    See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02> \n    or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)     \n    for further details on estimation and the package structure.\n    For tutorials on how to use the CDM package see \n    George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as\n    Ravand and Robitzsch (2015).",
    "version": "8.3-14",
    "maintainer": "Alexander Robitzsch <robitzsch@ipn.uni-kiel.de>",
    "author": "Alexander Robitzsch [aut, cre],\n  Thomas Kiefer [aut],\n  Ann Cathrice George [aut],\n  Ali Uenlue [aut]",
    "url": "https://github.com/alexanderrobitzsch/CDM,\nhttps://sites.google.com/view/alexander-robitzsch/software",
    "bug_reports": "https://github.com/alexanderrobitzsch/CDM/issues?state=open",
    "repository": "https://cran.r-project.org/package=CDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CDM Cognitive Diagnosis Modeling \n    Functions for cognitive diagnosis modeling and multidimensional item response modeling \n    for dichotomous and polytomous item responses. This package enables the estimation of \n    the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),\n    the multiple group (polytomous) GDINA model (de la Torre, 2011, \n    <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009, \n    <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008, \n    <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992, \n    <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis \n    (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>). \n    See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02> \n    or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)     \n    for further details on estimation and the package structure.\n    For tutorials on how to use the CDM package see \n    George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as\n    Ravand and Robitzsch (2015).  "
  },
  {
    "id": 2290,
    "package_name": "CMHSU",
    "title": "Mental Health Status, Substance Use Status and their Concurrent\nStatus in North American Healthcare Administrative Databases",
    "description": "Patients' Mental Health (MH) status, Substance Use (SU) status, and concurrent MH/SU status in the American/Canadian Healthcare Administrative Databases can be identified. The detection is based on given parameters of interest by clinicians including the list of  plausible ICD MH/SU codes (3/4/5 characters), the required number of visits of hospital for MH/SU , the required number of visits of service physicians for MH/SU, and the maximum time span within MH visits, within SU visits, and, between MH and SU visits. Methods are described in:  Khan S <https://pubmed.ncbi.nlm.nih.gov/29044442/>, Keen C, et al. (2021) <doi:10.1111/add.15580>,  Lavergne MR, et al. (2022) <doi:10.1186/s12913-022-07759-z>, Casillas, S M, et al. (2022) <doi:10.1016/j.abrep.2022.100464>, CIHI (2022) <https://www.cihi.ca/en>, CDC (2024) <https://www.cdc.gov>, WHO (2019) <https://icd.who.int/en>.",
    "version": "0.0.6.9",
    "maintainer": "Chel Hee Lee <chelhee.lee@ucalgary.ca>",
    "author": "Mohsen Soltanifar [aut] (ORCID:\n    <https://orcid.org/0000-0002-5989-0082>),\n  Chel Hee Lee [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-8209-8176>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CMHSU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CMHSU Mental Health Status, Substance Use Status and their Concurrent\nStatus in North American Healthcare Administrative Databases Patients' Mental Health (MH) status, Substance Use (SU) status, and concurrent MH/SU status in the American/Canadian Healthcare Administrative Databases can be identified. The detection is based on given parameters of interest by clinicians including the list of  plausible ICD MH/SU codes (3/4/5 characters), the required number of visits of hospital for MH/SU , the required number of visits of service physicians for MH/SU, and the maximum time span within MH visits, within SU visits, and, between MH and SU visits. Methods are described in:  Khan S <https://pubmed.ncbi.nlm.nih.gov/29044442/>, Keen C, et al. (2021) <doi:10.1111/add.15580>,  Lavergne MR, et al. (2022) <doi:10.1186/s12913-022-07759-z>, Casillas, S M, et al. (2022) <doi:10.1016/j.abrep.2022.100464>, CIHI (2022) <https://www.cihi.ca/en>, CDC (2024) <https://www.cdc.gov>, WHO (2019) <https://icd.who.int/en>.  "
  },
  {
    "id": 2459,
    "package_name": "ChineseNames",
    "title": "Chinese Name Database 1930-2008",
    "description": "\n    A database of Chinese surnames and given names (1930-2008).\n    This database contains nationwide frequency statistics of\n    1,806 Chinese surnames and 2,614 Chinese characters used in given names,\n    covering about 1.2 billion Han Chinese population\n    (96.8 percent of the Han Chinese household-registered population\n    born from 1930 to 2008 and still alive in 2008).\n    This package also contains a function for computing multiple indices of\n    Chinese surnames and given names for social science research (e.g.,\n    name uniqueness, name gender, name valence, and name warmth/competence).\n    Details are provided at\n    <https://psychbruce.github.io/ChineseNames/>.",
    "version": "2025.8",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/ChineseNames/",
    "bug_reports": "https://github.com/psychbruce/ChineseNames/issues",
    "repository": "https://cran.r-project.org/package=ChineseNames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ChineseNames Chinese Name Database 1930-2008 \n    A database of Chinese surnames and given names (1930-2008).\n    This database contains nationwide frequency statistics of\n    1,806 Chinese surnames and 2,614 Chinese characters used in given names,\n    covering about 1.2 billion Han Chinese population\n    (96.8 percent of the Han Chinese household-registered population\n    born from 1930 to 2008 and still alive in 2008).\n    This package also contains a function for computing multiple indices of\n    Chinese surnames and given names for social science research (e.g.,\n    name uniqueness, name gender, name valence, and name warmth/competence).\n    Details are provided at\n    <https://psychbruce.github.io/ChineseNames/>.  "
  },
  {
    "id": 2534,
    "package_name": "CoTiMA",
    "title": "Continuous Time Meta-Analysis ('CoTiMA')",
    "description": "The 'CoTiMA' package performs meta-analyses of correlation matrices of repeatedly measured variables taken from \n   studies that used different time intervals. Different time intervals between measurement occasions impose problems for \n   meta-analyses because the effects (e.g. cross-lagged effects) cannot be simply aggregated, for example, by means of common \n   fixed or random effects analysis. However, continuous time math, which is applied in 'CoTiMA', can be used to extrapolate or \n   intrapolate the results from all studies to any desired time lag. By this, effects obtained in studies that used different \n   time intervals can be meta-analyzed. 'CoTiMA' fits models to empirical data using the structural equation model (SEM) package \n   'ctsem', the effects specified in a SEM are related to parameters that are not directly included in the model (i.e., \n   continuous time parameters; together, they represent the continuous time structural equation model, CTSEM). Statistical \n   model comparisons and significance tests are then performed on the continuous time parameter estimates. 'CoTiMA' also allows \n   analysis of publication bias (Egger's test, PET-PEESE estimates, zcurve analysis etc.) and analysis of statistical power\n   (post hoc power, required sample sizes).  \n   See Dormann, C., Guthier, C., & Cortina, J. M. (2019) <doi:10.1177/1094428119847277>.\n   and Guthier, C., Dormann, C., & Voelkle, M. C. (2020) <doi:10.1037/bul0000304>.",
    "version": "1.0.2",
    "maintainer": "Markus Homberg <cotima@uni-mainz.de>",
    "author": "Christian Dormann [aut, cph],\n  Markus Homberg [aut, com, cre],\n  Olga Diener [ctb],\n  Christina Guthier [ctb],\n  Manuel Voelkle [ctb]",
    "url": "https://github.com/CoTiMA/CoTiMA",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CoTiMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CoTiMA Continuous Time Meta-Analysis ('CoTiMA') The 'CoTiMA' package performs meta-analyses of correlation matrices of repeatedly measured variables taken from \n   studies that used different time intervals. Different time intervals between measurement occasions impose problems for \n   meta-analyses because the effects (e.g. cross-lagged effects) cannot be simply aggregated, for example, by means of common \n   fixed or random effects analysis. However, continuous time math, which is applied in 'CoTiMA', can be used to extrapolate or \n   intrapolate the results from all studies to any desired time lag. By this, effects obtained in studies that used different \n   time intervals can be meta-analyzed. 'CoTiMA' fits models to empirical data using the structural equation model (SEM) package \n   'ctsem', the effects specified in a SEM are related to parameters that are not directly included in the model (i.e., \n   continuous time parameters; together, they represent the continuous time structural equation model, CTSEM). Statistical \n   model comparisons and significance tests are then performed on the continuous time parameter estimates. 'CoTiMA' also allows \n   analysis of publication bias (Egger's test, PET-PEESE estimates, zcurve analysis etc.) and analysis of statistical power\n   (post hoc power, required sample sizes).  \n   See Dormann, C., Guthier, C., & Cortina, J. M. (2019) <doi:10.1177/1094428119847277>.\n   and Guthier, C., Dormann, C., & Voelkle, M. C. (2020) <doi:10.1037/bul0000304>.  "
  },
  {
    "id": 2617,
    "package_name": "ConversationAlign",
    "title": "Process Text and Compute Linguistic Alignment in Conversation\nTranscripts",
    "description": "Imports conversation transcripts into R, concatenates them into a single dataframe appending event identifiers, cleans and formats the text, then yokes user-specified psycholinguistic database values to each word.  'ConversationAlign' then computes alignment indices between two interlocutors across each transcript for >40 possible semantic, lexical, and affective dimensions. In addition to alignment, 'ConversationAlign' also produces a table of analytics (e.g., token count, type-token-ratio) in a summary table describing your particular text corpus.",
    "version": "0.4.0",
    "maintainer": "Jamie Reilly <jamie_reilly@temple.edu>",
    "author": "Jamie Reilly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0891-438X>),\n  Virginia Ulichney [aut],\n  Ben Sacks [aut],\n  Sarah Weinstein [ctb],\n  Chelsea Helion [ctb],\n  Gus Cooney [ctb]",
    "url": "https://github.com/Reilly-ConceptsCognitionLab/ConversationAlign",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConversationAlign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConversationAlign Process Text and Compute Linguistic Alignment in Conversation\nTranscripts Imports conversation transcripts into R, concatenates them into a single dataframe appending event identifiers, cleans and formats the text, then yokes user-specified psycholinguistic database values to each word.  'ConversationAlign' then computes alignment indices between two interlocutors across each transcript for >40 possible semantic, lexical, and affective dimensions. In addition to alignment, 'ConversationAlign' also produces a table of analytics (e.g., token count, type-token-ratio) in a summary table describing your particular text corpus.  "
  },
  {
    "id": 2636,
    "package_name": "CorrMixed",
    "title": "Estimate Correlations Between Repeatedly Measured Endpoints\n(E.g., Reliability) Based on Linear Mixed-Effects Models",
    "description": "In clinical practice and research settings in medicine and the behavioral sciences, it is often of interest to quantify the correlation of a continuous endpoint that was repeatedly measured (e.g., test-retest correlations, ICC, etc.). This package allows for estimating these correlations based on mixed-effects models. Part of this software has been developed using funding provided from the European Union's 7th Framework Programme for research, technological development and demonstration under Grant Agreement no 602552.",
    "version": "1.1",
    "maintainer": "Wim Van der Elst <Wim.vanderelst@gmail.com>",
    "author": "Wim Van der Elst, Geert Molenberghs, Dieter Hilgers, & Nicole Heussen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CorrMixed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CorrMixed Estimate Correlations Between Repeatedly Measured Endpoints\n(E.g., Reliability) Based on Linear Mixed-Effects Models In clinical practice and research settings in medicine and the behavioral sciences, it is often of interest to quantify the correlation of a continuous endpoint that was repeatedly measured (e.g., test-retest correlations, ICC, etc.). This package allows for estimating these correlations based on mixed-effects models. Part of this software has been developed using funding provided from the European Union's 7th Framework Programme for research, technological development and demonstration under Grant Agreement no 602552.  "
  },
  {
    "id": 2701,
    "package_name": "DAKS",
    "title": "Data Analysis and Knowledge Spaces",
    "description": "Functions and an example dataset for the psychometric theory of\n  knowledge spaces.  This package implements data analysis methods and\n  procedures for simulating data and quasi orders and transforming different\n  formulations in knowledge space theory.  See package?DAKS for an overview.",
    "version": "2.1-3",
    "maintainer": "Ali Uenlue <ali.uenlue@tum.de>",
    "author": "Ali Uenlue [aut, cre],\n  Anatol Sargin [aut]",
    "url": "http://www.meb.edu.tum.de",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DAKS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DAKS Data Analysis and Knowledge Spaces Functions and an example dataset for the psychometric theory of\n  knowledge spaces.  This package implements data analysis methods and\n  procedures for simulating data and quasi orders and transforming different\n  formulations in knowledge space theory.  See package?DAKS for an overview.  "
  },
  {
    "id": 2737,
    "package_name": "DDPM",
    "title": "Data Sets for Discrete Probability Models",
    "description": "A wide collection of univariate discrete data sets from various applied domains related to distribution theory. The functions allow quick, easy, and efficient access to 100 univariate discrete data sets. The data are related to different applied domains, including medical, reliability analysis, engineering, manufacturing, occupational safety, geological sciences, terrorism, psychology, agriculture, environmental sciences, road traffic accidents, demography, actuarial science, law, and justice. The documentation, along with associated references for further details and uses, is presented.     ",
    "version": "0.1.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Christophe Chesneau [aut],\n  Muhammad Imran [aut, cre],\n  M.H Tahir [aut],\n  Farrukh Jamal [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DDPM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DDPM Data Sets for Discrete Probability Models A wide collection of univariate discrete data sets from various applied domains related to distribution theory. The functions allow quick, easy, and efficient access to 100 univariate discrete data sets. The data are related to different applied domains, including medical, reliability analysis, engineering, manufacturing, occupational safety, geological sciences, terrorism, psychology, agriculture, environmental sciences, road traffic accidents, demography, actuarial science, law, and justice. The documentation, along with associated references for further details and uses, is presented.       "
  },
  {
    "id": 2810,
    "package_name": "DMCfun",
    "title": "Diffusion Model of Conflict (DMC) in Reaction Time Tasks",
    "description": "\n  DMC model simulation detailed in Ulrich, R., Schroeter, H., Leuthold, H., & Birngruber, T. (2015).\n  Automatic and controlled stimulus processing in conflict tasks: Superimposed diffusion processes and delta functions.\n  Cognitive Psychology, 78, 148-174. Ulrich et al. (2015) <doi:10.1016/j.cogpsych.2015.02.005>.\n  Decision processes within choice reaction-time (CRT) tasks are often modelled using evidence accumulation models (EAMs),\n  a variation of which is the Diffusion Decision Model (DDM, for a review, see Ratcliff & McKoon, 2008).\n  Ulrich et al. (2015) introduced a Diffusion Model for Conflict tasks (DMC). The DMC model combines common\n  features from within standard diffusion models with the addition of superimposed controlled and automatic activation.\n  The DMC model is used to explain distributional reaction time (and error rate) patterns in common behavioural\n  conflict-like tasks (e.g., Flanker task, Simon task). This R-package implements the DMC model and provides functionality\n  to fit the model to observed data. Further details are provided in the following paper: \n  Mackenzie, I.G., & Dudschig, C. (2021). DMCfun: An R package for fitting Diffusion Model of Conflict (DMC) to reaction \n  time and error rate data. Methods in Psychology, 100074. <doi:10.1016/j.metip.2021.100074>.",
    "version": "4.0.1",
    "maintainer": "Ian G. Mackenzie <ian.mackenzie@uni-tuebingen.de>",
    "author": "Ian G. Mackenzie [cre, aut],\n  Carolin Dudschig [aut]",
    "url": "https://github.com/igmmgi/DMCfun,\nhttps://CRAN.R-project.org/package=DMCfun,\nhttps://www.sciencedirect.com/science/article/pii/S259026012100031X",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DMCfun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DMCfun Diffusion Model of Conflict (DMC) in Reaction Time Tasks \n  DMC model simulation detailed in Ulrich, R., Schroeter, H., Leuthold, H., & Birngruber, T. (2015).\n  Automatic and controlled stimulus processing in conflict tasks: Superimposed diffusion processes and delta functions.\n  Cognitive Psychology, 78, 148-174. Ulrich et al. (2015) <doi:10.1016/j.cogpsych.2015.02.005>.\n  Decision processes within choice reaction-time (CRT) tasks are often modelled using evidence accumulation models (EAMs),\n  a variation of which is the Diffusion Decision Model (DDM, for a review, see Ratcliff & McKoon, 2008).\n  Ulrich et al. (2015) introduced a Diffusion Model for Conflict tasks (DMC). The DMC model combines common\n  features from within standard diffusion models with the addition of superimposed controlled and automatic activation.\n  The DMC model is used to explain distributional reaction time (and error rate) patterns in common behavioural\n  conflict-like tasks (e.g., Flanker task, Simon task). This R-package implements the DMC model and provides functionality\n  to fit the model to observed data. Further details are provided in the following paper: \n  Mackenzie, I.G., & Dudschig, C. (2021). DMCfun: An R package for fitting Diffusion Model of Conflict (DMC) to reaction \n  time and error rate data. Methods in Psychology, 100074. <doi:10.1016/j.metip.2021.100074>.  "
  },
  {
    "id": 2820,
    "package_name": "DNMF",
    "title": "Discriminant Non-Negative Matrix Factorization",
    "description": "Discriminant Non-Negative Matrix Factorization aims to extend the Non-negative Matrix Factorization algorithm in order to extract features that enforce not only the spatial locality, but also the separability between classes in a discriminant manner. It refers to three article, Zafeiriou, Stefanos, et al. \"Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification.\" Neural Networks, IEEE Transactions on 17.3 (2006): 683-695. Kim, Bo-Kyeong, and Soo-Young Lee. \"Spectral Feature Extraction Using dNMF for Emotion Recognition in Vowel Sounds.\" Neural Information Processing. Springer Berlin Heidelberg, 2013. and Lee, Soo-Young, Hyun-Ah Song, and Shun-ichi Amari. \"A new discriminant NMF algorithm and its application to the extraction of subtle emotional differences in speech.\" Cognitive neurodynamics 6.6 (2012): 525-535.",
    "version": "1.4.2",
    "maintainer": "Zhilong Jia <zhilongjia@gmail.com>",
    "author": "Zhilong Jia [aut, cre],\n  Xiang Zhang [aut]",
    "url": "https://github.com/zhilongjia/DNMF",
    "bug_reports": "https://github.com/zhilongjia/DNMF/issues",
    "repository": "https://cran.r-project.org/package=DNMF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DNMF Discriminant Non-Negative Matrix Factorization Discriminant Non-Negative Matrix Factorization aims to extend the Non-negative Matrix Factorization algorithm in order to extract features that enforce not only the spatial locality, but also the separability between classes in a discriminant manner. It refers to three article, Zafeiriou, Stefanos, et al. \"Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification.\" Neural Networks, IEEE Transactions on 17.3 (2006): 683-695. Kim, Bo-Kyeong, and Soo-Young Lee. \"Spectral Feature Extraction Using dNMF for Emotion Recognition in Vowel Sounds.\" Neural Information Processing. Springer Berlin Heidelberg, 2013. and Lee, Soo-Young, Hyun-Ah Song, and Shun-ichi Amari. \"A new discriminant NMF algorithm and its application to the extraction of subtle emotional differences in speech.\" Cognitive neurodynamics 6.6 (2012): 525-535.  "
  },
  {
    "id": 2832,
    "package_name": "DPI",
    "title": "The Directed Prediction Index for Causal Direction Inference\nfrom Observational Data",
    "description": "\n    The Directed Prediction Index ('DPI') is\n    a quasi-causal inference (causal discovery) method for observational data\n    designed to quantify the relative endogeneity (relative dependence)\n    of outcome (Y) versus predictor (X) variables in regression models.\n    By comparing the proportion of variance explained (R-squared)\n    between the Y-as-outcome model and the X-as-outcome model\n    while controlling for a sufficient number of possible confounders,\n    it can suggest a plausible (admissible) direction of influence\n    from a less endogenous variable (X) to a more endogenous variable (Y).\n    Methodological details are provided at\n    <https://psychbruce.github.io/DPI/>.\n    This package also includes functions for data simulation and network\n    analysis (correlation, partial correlation, and Bayesian networks).",
    "version": "2025.11",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/DPI/",
    "bug_reports": "https://github.com/psychbruce/DPI/issues",
    "repository": "https://cran.r-project.org/package=DPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DPI The Directed Prediction Index for Causal Direction Inference\nfrom Observational Data \n    The Directed Prediction Index ('DPI') is\n    a quasi-causal inference (causal discovery) method for observational data\n    designed to quantify the relative endogeneity (relative dependence)\n    of outcome (Y) versus predictor (X) variables in regression models.\n    By comparing the proportion of variance explained (R-squared)\n    between the Y-as-outcome model and the X-as-outcome model\n    while controlling for a sufficient number of possible confounders,\n    it can suggest a plausible (admissible) direction of influence\n    from a less endogenous variable (X) to a more endogenous variable (Y).\n    Methodological details are provided at\n    <https://psychbruce.github.io/DPI/>.\n    This package also includes functions for data simulation and network\n    analysis (correlation, partial correlation, and Bayesian networks).  "
  },
  {
    "id": 3091,
    "package_name": "EFAtools",
    "title": "Fast and Flexible Implementations of Exploratory Factor Analysis\nTools",
    "description": "Provides functions to perform exploratory factor analysis (EFA) procedures and compare their solutions. The goal is to provide state-of-the-art factor retention methods and a high degree of flexibility in the EFA procedures. This way, for example, implementations from R 'psych' and 'SPSS' can be compared. Moreover, functions for Schmid-Leiman transformation and the computation of omegas are provided. To speed up the analyses, some of the iterative procedures, like principal axis factoring (PAF), are implemented in C++.",
    "version": "0.6.1",
    "maintainer": "Markus Steiner <markus.d.steiner@gmail.com>",
    "author": "Markus Steiner [aut, cre],\n  Silvia Grieder [aut],\n  William Revelle [ctb],\n  Max Auerswald [ctb],\n  Morten Moshagen [ctb],\n  John Ruscio [ctb],\n  Brendan Roche [ctb],\n  Urbano Lorenzo-Seva [ctb],\n  David Navarro-Gonzalez [ctb],\n  Johan Braeken [ctb]",
    "url": "https://github.com/mdsteiner/EFAtools",
    "bug_reports": "https://github.com/mdsteiner/EFAtools/issues",
    "repository": "https://cran.r-project.org/package=EFAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EFAtools Fast and Flexible Implementations of Exploratory Factor Analysis\nTools Provides functions to perform exploratory factor analysis (EFA) procedures and compare their solutions. The goal is to provide state-of-the-art factor retention methods and a high degree of flexibility in the EFA procedures. This way, for example, implementations from R 'psych' and 'SPSS' can be compared. Moreover, functions for Schmid-Leiman transformation and the computation of omegas are provided. To speed up the analyses, some of the iterative procedures, like principal axis factoring (PAF), are implemented in C++.  "
  },
  {
    "id": 3095,
    "package_name": "EGAnet",
    "title": "Exploratory Graph Analysis \u2013 a Framework for Estimating the\nNumber of Dimensions in Multivariate Data using Network\nPsychometrics",
    "description": "Implements the Exploratory Graph Analysis (EGA) framework for dimensionality\n             and psychometric assessment. EGA estimates the number of dimensions in\n\t     \t psychological data using network estimation methods and community detection\n             algorithms. A bootstrap method is provided to assess the stability of dimensions\n\t     \t and items. Fit is evaluated using the Entropy Fit family of indices. Unique \n             Variable Analysis evaluates the extent to which items are locally dependent (or\n             redundant). Network loadings provide similar information to factor loadings and\n\t     \t can be used to compute network scores. A bootstrap and permutation approach are\n             available to assess configural and metric invariance. Hierarchical structures\n             can be detected using Hierarchical EGA. Time series and intensive longitudinal \n\t     \t data can be analyzed using Dynamic EGA, supporting individual, group, and \n             population level assessments.",
    "version": "2.4.0",
    "maintainer": "Hudson Golino <hfg9s@virginia.edu>",
    "author": "Hudson Golino [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1601-1447>),\n  Alexander Christensen [aut] (ORCID:\n    <https://orcid.org/0000-0002-9798-7037>),\n  Robert Moulder [ctb] (ORCID: <https://orcid.org/0000-0001-7504-9560>),\n  Luis E. Garrido [ctb] (ORCID: <https://orcid.org/0000-0001-8932-6063>),\n  Laura Jamison [ctb] (ORCID: <https://orcid.org/0000-0002-4656-8684>),\n  Dingjing Shi [ctb] (ORCID: <https://orcid.org/0000-0002-5652-3818>)",
    "url": "https://r-ega.net",
    "bug_reports": "https://github.com/hfgolino/EGAnet/issues",
    "repository": "https://cran.r-project.org/package=EGAnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EGAnet Exploratory Graph Analysis \u2013 a Framework for Estimating the\nNumber of Dimensions in Multivariate Data using Network\nPsychometrics Implements the Exploratory Graph Analysis (EGA) framework for dimensionality\n             and psychometric assessment. EGA estimates the number of dimensions in\n\t     \t psychological data using network estimation methods and community detection\n             algorithms. A bootstrap method is provided to assess the stability of dimensions\n\t     \t and items. Fit is evaluated using the Entropy Fit family of indices. Unique \n             Variable Analysis evaluates the extent to which items are locally dependent (or\n             redundant). Network loadings provide similar information to factor loadings and\n\t     \t can be used to compute network scores. A bootstrap and permutation approach are\n             available to assess configural and metric invariance. Hierarchical structures\n             can be detected using Hierarchical EGA. Time series and intensive longitudinal \n\t     \t data can be analyzed using Dynamic EGA, supporting individual, group, and \n             population level assessments.  "
  },
  {
    "id": 3118,
    "package_name": "EMC2",
    "title": "Bayesian Hierarchical Analysis of Cognitive Models of Choice",
    "description": "Fit Bayesian (hierarchical) cognitive models\n    using a linear modeling language interface using particle Metropolis Markov\n    chain Monte Carlo sampling with Gibbs steps. The diffusion decision model (DDM), \n    linear ballistic accumulator model (LBA), racing diffusion model (RDM), and the lognormal\n    race model (LNR) are supported. Additionally, users can specify their own likelihood\n    function and/or choose for non-hierarchical\n    estimation, as well as for a diagonal, blocked or full multivariate normal\n    group-level distribution to test individual differences. Prior specification \n    is facilitated through methods that visualize the (implied) prior. \n    A wide range of plotting functions assist in assessing model convergence and\n    posterior inference. Models can be easily evaluated using functions\n    that plot posterior predictions or using relative model comparison metrics \n    such as information criteria or Bayes factors.\n    References: Stevenson et al. (2024) <doi:10.31234/osf.io/2e4dq>.",
    "version": "3.3.0",
    "maintainer": "Niek Stevenson <niek.stevenson@gmail.com>",
    "author": "Niek Stevenson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3206-7544>),\n  Michelle Donzallaz [aut],\n  Andrew Heathcote [aut],\n  Steven Mileti\u0107 [ctb],\n  Raphael Hartmann [ctb],\n  Karl C. Klauer [ctb],\n  Steven G. Johnson [ctb],\n  Jean M. Linhart [ctb],\n  Brian Gough [ctb],\n  Gerard Jungman [ctb],\n  Rudolf Schuerer [ctb],\n  Przemyslaw Sliwa [ctb],\n  Jason H. Stover [ctb]",
    "url": "https://ampl-psych.github.io/EMC2/,\nhttps://github.com/ampl-psych/EMC2",
    "bug_reports": "https://github.com/ampl-psych/EMC2/issues",
    "repository": "https://cran.r-project.org/package=EMC2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EMC2 Bayesian Hierarchical Analysis of Cognitive Models of Choice Fit Bayesian (hierarchical) cognitive models\n    using a linear modeling language interface using particle Metropolis Markov\n    chain Monte Carlo sampling with Gibbs steps. The diffusion decision model (DDM), \n    linear ballistic accumulator model (LBA), racing diffusion model (RDM), and the lognormal\n    race model (LNR) are supported. Additionally, users can specify their own likelihood\n    function and/or choose for non-hierarchical\n    estimation, as well as for a diagonal, blocked or full multivariate normal\n    group-level distribution to test individual differences. Prior specification \n    is facilitated through methods that visualize the (implied) prior. \n    A wide range of plotting functions assist in assessing model convergence and\n    posterior inference. Models can be easily evaluated using functions\n    that plot posterior predictions or using relative model comparison metrics \n    such as information criteria or Bayes factors.\n    References: Stevenson et al. (2024) <doi:10.31234/osf.io/2e4dq>.  "
  },
  {
    "id": 3302,
    "package_name": "ExpBites",
    "title": "Analyzing Human Exposure to Mosquito Biting",
    "description": "Tools to analyse human and mosquito behavioral interactions and to compute exposure to mosquito bites estimates.\n    Using behavioral data for human individuals and biting patterns for mosquitoes, you will be able to compute hourly exposure for bed net users and non-users, and summarize (e.g. proportion indoors and outdoors, proportion per time periods, and proportion prevented by bed nets) or visualize these dynamics across a 24-hour cycle.",
    "version": "0.1.3",
    "maintainer": "Nicolas Moiroux <nicolas.moiroux@ird.fr>",
    "author": "Nicolas Moiroux [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6755-6167>)",
    "url": "https://github.com/Nmoiroux/ExpBites",
    "bug_reports": "https://github.com/Nmoiroux/ExpBites/issues",
    "repository": "https://cran.r-project.org/package=ExpBites",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExpBites Analyzing Human Exposure to Mosquito Biting Tools to analyse human and mosquito behavioral interactions and to compute exposure to mosquito bites estimates.\n    Using behavioral data for human individuals and biting patterns for mosquitoes, you will be able to compute hourly exposure for bed net users and non-users, and summarize (e.g. proportion indoors and outdoors, proportion per time periods, and proportion prevented by bed nets) or visualize these dynamics across a 24-hour cycle.  "
  },
  {
    "id": 3539,
    "package_name": "FreqProf",
    "title": "Frequency Profiles Computing and Plotting",
    "description": "Tools for generating an informative type of line graph, the frequency profile, \n    which allows single behaviors, multiple behaviors, or the specific behavioral patterns \n    of individual subjects to be graphed from occurrence/nonoccurrence behavioral data.",
    "version": "0.0.1",
    "maintainer": "Ronald E. Robertson <rrobertson@aibrt.org>",
    "author": "Robert Epstein [aut],\n  Thomas Boulier [aut],\n  Ronald E. Robertson [cre, aut],\n  Jonathan Mejia [ctb],\n  AIBRT [cph]",
    "url": "https://github.com/AIBRT/FreqProf",
    "bug_reports": "https://github.com/AIBRT/FreqProf/issues",
    "repository": "https://cran.r-project.org/package=FreqProf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FreqProf Frequency Profiles Computing and Plotting Tools for generating an informative type of line graph, the frequency profile, \n    which allows single behaviors, multiple behaviors, or the specific behavioral patterns \n    of individual subjects to be graphed from occurrence/nonoccurrence behavioral data.  "
  },
  {
    "id": 3557,
    "package_name": "FuzzyM",
    "title": "Fuzzy Cognitive Maps Operations",
    "description": "Contains functions for operations with fuzzy cognitive maps using t-norm and s-norm operators. T-norms and S-norms are described by Dov M. Gabbay and George Metcalfe (2007) <doi:10.1007/s00153-007-0047-1>. System indicators are described by Cox, Earl D. (1995) <isbn:1886801010>. Executable examples are provided in the \"inst/examples\" folder.",
    "version": "0.1.0",
    "maintainer": "Alina Petukhova <petukhova.alina@gmail.com>",
    "author": "Alina Petukhova",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FuzzyM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FuzzyM Fuzzy Cognitive Maps Operations Contains functions for operations with fuzzy cognitive maps using t-norm and s-norm operators. T-norms and S-norms are described by Dov M. Gabbay and George Metcalfe (2007) <doi:10.1007/s00153-007-0047-1>. System indicators are described by Cox, Earl D. (1995) <isbn:1886801010>. Executable examples are provided in the \"inst/examples\" folder.  "
  },
  {
    "id": 3608,
    "package_name": "GDINA",
    "title": "The Generalized DINA Model Framework",
    "description": "A set of psychometric tools for cognitive diagnosis modeling based on the generalized deterministic inputs, noisy and gate (G-DINA) model by de la Torre (2011) <DOI:10.1007/s11336-011-9207-7> and its extensions, including the sequential G-DINA model by Ma and de la Torre (2016) <DOI:10.1111/bmsp.12070> for polytomous responses, and the polytomous G-DINA model by Chen and de la Torre <DOI:10.1177/0146621613479818> for polytomous attributes. Joint attribute distribution can be independent, saturated, higher-order, loglinear smoothed or structured. Q-matrix validation, item and model fit statistics, model comparison at test and item level and differential item functioning can also be conducted. A graphical user interface is also provided. For tutorials, please check Ma and de la Torre (2020) <DOI:10.18637/jss.v093.i14>, Ma and de la Torre (2019) <DOI:10.1111/emip.12262>, Ma (2019) <DOI:10.1007/978-3-030-05584-4_29> and de la Torre and Akbay (2019). ",
    "version": "2.9.12",
    "maintainer": "Wenchao Ma <wma@umn.edu>",
    "author": "Wenchao Ma [aut, cre, cph],\n  Jimmy de la Torre [aut, cph],\n  Miguel Sorrel [ctb],\n  Zhehan Jiang [ctb],\n  Pablo Najera [ctb]",
    "url": "https://github.com/Wenchao-Ma/GDINA,\nhttps://wenchao-ma.github.io/GDINA/",
    "bug_reports": "https://github.com/Wenchao-Ma/GDINA/issues",
    "repository": "https://cran.r-project.org/package=GDINA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GDINA The Generalized DINA Model Framework A set of psychometric tools for cognitive diagnosis modeling based on the generalized deterministic inputs, noisy and gate (G-DINA) model by de la Torre (2011) <DOI:10.1007/s11336-011-9207-7> and its extensions, including the sequential G-DINA model by Ma and de la Torre (2016) <DOI:10.1111/bmsp.12070> for polytomous responses, and the polytomous G-DINA model by Chen and de la Torre <DOI:10.1177/0146621613479818> for polytomous attributes. Joint attribute distribution can be independent, saturated, higher-order, loglinear smoothed or structured. Q-matrix validation, item and model fit statistics, model comparison at test and item level and differential item functioning can also be conducted. A graphical user interface is also provided. For tutorials, please check Ma and de la Torre (2020) <DOI:10.18637/jss.v093.i14>, Ma and de la Torre (2019) <DOI:10.1111/emip.12262>, Ma (2019) <DOI:10.1007/978-3-030-05584-4_29> and de la Torre and Akbay (2019).   "
  },
  {
    "id": 3639,
    "package_name": "GFM",
    "title": "Generalized Factor Model",
    "description": "Generalized factor model is implemented for ultra-high dimensional data with mixed-type variables.\n    Two algorithms, variational EM and alternate maximization, are designed to implement the generalized factor model,\n    respectively. The factor matrix and loading matrix together with the number of factors can be well estimated. \n    This model can be employed in social and behavioral sciences, economy and finance, and  genomics, \n    to extract interpretable nonlinear factors. More details can be referred to \n    Wei Liu, Huazhen Lin, Shurong Zheng and Jin Liu. (2021) <doi:10.1080/01621459.2021.1999818>. ",
    "version": "1.2.1",
    "maintainer": "Wei Liu <LiuWeideng@gmail.com>",
    "author": "Wei Liu [aut, cre],\n  Huazhen Lin [aut],\n  Shurong Zheng [aut],\n  Jin Liu [aut],\n  Jinyu Nie [aut]",
    "url": "https://github.com/feiyoung/GFM",
    "bug_reports": "https://github.com/feiyoung/GFM/issues",
    "repository": "https://cran.r-project.org/package=GFM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GFM Generalized Factor Model Generalized factor model is implemented for ultra-high dimensional data with mixed-type variables.\n    Two algorithms, variational EM and alternate maximization, are designed to implement the generalized factor model,\n    respectively. The factor matrix and loading matrix together with the number of factors can be well estimated. \n    This model can be employed in social and behavioral sciences, economy and finance, and  genomics, \n    to extract interpretable nonlinear factors. More details can be referred to \n    Wei Liu, Huazhen Lin, Shurong Zheng and Jin Liu. (2021) <doi:10.1080/01621459.2021.1999818>.   "
  },
  {
    "id": 3904,
    "package_name": "GxEprs",
    "title": "Genotype-by-Environment Interaction in Polygenic Score Models",
    "description": "A novel PRS model is introduced to enhance the prediction accuracy by utilising GxE effects. This package performs Genome Wide Association Studies (GWAS) and Genome Wide Environment Interaction Studies (GWEIS) using a discovery dataset. The package has the ability to obtain polygenic risk scores (PRSs) for a target sample. Finally it predicts the risk values of each individual in the target sample. Users have the choice of using existing models (Li et al., 2015) <doi:10.1093/annonc/mdu565>, (Pandis et al., 2013) <doi:10.1093/ejo/cjt054>, (Peyrot et al., 2018) <doi:10.1016/j.biopsych.2017.09.009> and (Song et al., 2022) <doi:10.1038/s41467-022-32407-9>, as well as newly proposed models for genomic risk prediction (refer to the URL for more details).",
    "version": "1.2",
    "maintainer": "Dovini Jayasinghe <dovini.jayasinghe@mymail.unisa.edu.au>",
    "author": "Dovini Jayasinghe [aut, cre, cph],\n  Hong Lee [aut, cph],\n  Moksedul Momin [aut, cph]",
    "url": "https://github.com/DoviniJ/GxEprs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GxEprs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GxEprs Genotype-by-Environment Interaction in Polygenic Score Models A novel PRS model is introduced to enhance the prediction accuracy by utilising GxE effects. This package performs Genome Wide Association Studies (GWAS) and Genome Wide Environment Interaction Studies (GWEIS) using a discovery dataset. The package has the ability to obtain polygenic risk scores (PRSs) for a target sample. Finally it predicts the risk values of each individual in the target sample. Users have the choice of using existing models (Li et al., 2015) <doi:10.1093/annonc/mdu565>, (Pandis et al., 2013) <doi:10.1093/ejo/cjt054>, (Peyrot et al., 2018) <doi:10.1016/j.biopsych.2017.09.009> and (Song et al., 2022) <doi:10.1038/s41467-022-32407-9>, as well as newly proposed models for genomic risk prediction (refer to the URL for more details).  "
  },
  {
    "id": 3963,
    "package_name": "HMDA",
    "title": "Holistic Multimodel Domain Analysis for Exploratory Machine\nLearning",
    "description": "Holistic Multimodel Domain Analysis (HMDA) is a robust and transparent framework designed for exploratory machine learning research, aiming to enhance the process of feature assessment and selection. HMDA addresses key limitations of traditional machine learning methods by evaluating the consistency across multiple high-performing models within a fine-tuned modeling grid, thereby improving the interpretability and reliability of feature importance assessments. Specifically, it computes Weighted Mean SHapley Additive exPlanations (WMSHAP), which aggregate feature contributions from multiple models based on weighted performance metrics. HMDA also provides confidence intervals to demonstrate the stability of these feature importance estimates. This framework is particularly beneficial for analyzing complex, multidimensional datasets common in health research, supporting reliable exploration of mental health outcomes such as suicidal ideation, suicide attempts, and other psychological conditions. Additionally, HMDA includes automated procedures for feature selection based on WMSHAP ratios and performs dimension reduction analyses to identify underlying structures among features. For more details see Haghish (2025) <doi:10.13140/RG.2.2.32473.63846>.",
    "version": "0.1.1",
    "maintainer": "E. F. Haghish <haghish@hotmail.com>",
    "author": "E. F. Haghish [aut, cre, cph]",
    "url": "http://dx.doi.org/10.13140/RG.2.2.32473.63846,\nhttps://github.com/haghish/HMDA",
    "bug_reports": "https://github.com/haghish/HMDA/issues",
    "repository": "https://cran.r-project.org/package=HMDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HMDA Holistic Multimodel Domain Analysis for Exploratory Machine\nLearning Holistic Multimodel Domain Analysis (HMDA) is a robust and transparent framework designed for exploratory machine learning research, aiming to enhance the process of feature assessment and selection. HMDA addresses key limitations of traditional machine learning methods by evaluating the consistency across multiple high-performing models within a fine-tuned modeling grid, thereby improving the interpretability and reliability of feature importance assessments. Specifically, it computes Weighted Mean SHapley Additive exPlanations (WMSHAP), which aggregate feature contributions from multiple models based on weighted performance metrics. HMDA also provides confidence intervals to demonstrate the stability of these feature importance estimates. This framework is particularly beneficial for analyzing complex, multidimensional datasets common in health research, supporting reliable exploration of mental health outcomes such as suicidal ideation, suicide attempts, and other psychological conditions. Additionally, HMDA includes automated procedures for feature selection based on WMSHAP ratios and performs dimension reduction analyses to identify underlying structures among features. For more details see Haghish (2025) <doi:10.13140/RG.2.2.32473.63846>.  "
  },
  {
    "id": 4133,
    "package_name": "IFTPredictor",
    "title": "Predictions Using Item-Focused Tree Models",
    "description": "This function predicts item response probabilities and item \n  responses using the item-focused tree model. The item-focused tree model\n  combines logistic regression with recursive partitioning to detect \n  Differential Item Functioning in dichotomous items. The model applies \n  partitioning rules to the data, splitting it into homogeneous subgroups, and \n  uses logistic regression within each subgroup to explain the data. \n  Differential Item Functioning detection is achieved by examining potential \n  group differences in item response patterns. This method is useful for \n  understanding how different predictors, such as demographic or psychological \n  factors, influence item responses across subgroups.",
    "version": "0.1.0",
    "maintainer": "Muditha L. Bodawatte Gedara <muditha.lakmali.1993@gmail.com>",
    "author": "Muditha L. Bodawatte Gedara [aut, cre],\n  Barret A. Monchka [aut],\n  Lisa M. Lix [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IFTPredictor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IFTPredictor Predictions Using Item-Focused Tree Models This function predicts item response probabilities and item \n  responses using the item-focused tree model. The item-focused tree model\n  combines logistic regression with recursive partitioning to detect \n  Differential Item Functioning in dichotomous items. The model applies \n  partitioning rules to the data, splitting it into homogeneous subgroups, and \n  uses logistic regression within each subgroup to explain the data. \n  Differential Item Functioning detection is achieved by examining potential \n  group differences in item response patterns. This method is useful for \n  understanding how different predictors, such as demographic or psychological \n  factors, influence item responses across subgroups.  "
  },
  {
    "id": 4148,
    "package_name": "IMEC",
    "title": "Ising Model of Explanatory Coherence",
    "description": "\n    Theories are one of the most important tools of science. Although psychologists discussed problems of theory in their discipline for a long time, weak theories are still widespread in most subfields. \n    One possible reason for this is that psychologists lack the tools to systematically assess the quality of their theories. \n    Previously a computational model for formal theory evaluation based on the concept of explanatory coherence was developed (Thagard, 1989, <doi:10.1017/S0140525X00057046>). \n    However, there are possible improvements to this model and it is not available in software that psychologists typically use. \n    Therefore, a new implementation of explanatory coherence based on the Ising model is available in this R-package.",
    "version": "0.2.0",
    "maintainer": "Maximilian Maier <maximilianmaier0401@gmail.com>",
    "author": "Maximilian Maier [aut, cre],\n  Noah van Dongen [ths],\n  Denny Borsboom [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IMEC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IMEC Ising Model of Explanatory Coherence \n    Theories are one of the most important tools of science. Although psychologists discussed problems of theory in their discipline for a long time, weak theories are still widespread in most subfields. \n    One possible reason for this is that psychologists lack the tools to systematically assess the quality of their theories. \n    Previously a computational model for formal theory evaluation based on the concept of explanatory coherence was developed (Thagard, 1989, <doi:10.1017/S0140525X00057046>). \n    However, there are possible improvements to this model and it is not available in software that psychologists typically use. \n    Therefore, a new implementation of explanatory coherence based on the Ising model is available in this R-package.  "
  },
  {
    "id": 4154,
    "package_name": "IMak",
    "title": "Item Maker",
    "description": "This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <DOI:10.3389/fpsyg.2018.01286>.",
    "version": "2.1.2",
    "maintainer": "Diego Blum <blumworx@gmail.com>",
    "author": "Diego Blum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IMak",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IMak Item Maker This is an Automatic Item Generator for Psychological Assessment. Items created with the 'IMak' package should not be used in applied settings as part of the working protocol without ensuring first that the items meet the required psychometric quality standards (see Blum & Holling, 2018) <DOI:10.3389/fpsyg.2018.01286>.  "
  },
  {
    "id": 4176,
    "package_name": "IPV",
    "title": "Item Pool Visualization",
    "description": "Generate plots based on the Item Pool Visualization concept for\n    latent constructs. Item Pool Visualizations are used to display the\n    conceptual structure of a set of items (self-report or psychometric).\n    Dantlgraber, Stieger, & Reips (2019) <doi:10.1177/2059799119884283>.",
    "version": "1.0.0",
    "maintainer": "Nils Petras <nils.petras@mailbox.org>",
    "author": "Nils Petras [aut, cre],\n  Michael Dantlgraber [aut],\n  Ulf-Dietrich Reips [ctb],\n  Matthias Bannert [ctb]",
    "url": "https://github.com/NilsPetras/IPV",
    "bug_reports": "https://github.com/NilsPetras/IPV/issues",
    "repository": "https://cran.r-project.org/package=IPV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IPV Item Pool Visualization Generate plots based on the Item Pool Visualization concept for\n    latent constructs. Item Pool Visualizations are used to display the\n    conceptual structure of a set of items (self-report or psychometric).\n    Dantlgraber, Stieger, & Reips (2019) <doi:10.1177/2059799119884283>.  "
  },
  {
    "id": 4386,
    "package_name": "Keng",
    "title": "Knock Errors Off Nice Guesses",
    "description": "Miscellaneous functions and data used in psychological research and teaching. Keng \n    currently has a built-in dataset depress, and could (1) scale a vector; (2) compute the cut-off \n    values of Pearson's r with known sample size; (3) test the significance and compute the post-hoc\n    power for Pearson's r with known sample size; (4) conduct a priori power analysis and plan the \n    sample size for Pearson's r; (5) compare lm()'s fitted outputs using R-squared, f_squared, \n    post-hoc power, and PRE (Proportional Reduction in Error, also called partial R-squared or \n    partial Eta-squared); (6) calculate PRE from partial correlation, Cohen's f, or f_squared; \n    (7) conduct a priori power analysis and plan the sample size for one or a set of predictors in \n    regression analysis; (8) conduct post-hoc power analysis for one or a set of predictors in \n    regression analysis with known sample size; (9) randomly pick numbers for Chinese Super Lotto\n    and Double Color Balls; (10) assess course objective achievement in Outcome-Based Education.",
    "version": "2025.10.8",
    "maintainer": "Qingyao Zhang <qingyaozhang@outlook.com>",
    "author": "Qingyao Zhang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6891-5982>)",
    "url": "https://github.com/qyaozh/Keng",
    "bug_reports": "https://github.com/qyaozh/Keng/issues",
    "repository": "https://cran.r-project.org/package=Keng",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Keng Knock Errors Off Nice Guesses Miscellaneous functions and data used in psychological research and teaching. Keng \n    currently has a built-in dataset depress, and could (1) scale a vector; (2) compute the cut-off \n    values of Pearson's r with known sample size; (3) test the significance and compute the post-hoc\n    power for Pearson's r with known sample size; (4) conduct a priori power analysis and plan the \n    sample size for Pearson's r; (5) compare lm()'s fitted outputs using R-squared, f_squared, \n    post-hoc power, and PRE (Proportional Reduction in Error, also called partial R-squared or \n    partial Eta-squared); (6) calculate PRE from partial correlation, Cohen's f, or f_squared; \n    (7) conduct a priori power analysis and plan the sample size for one or a set of predictors in \n    regression analysis; (8) conduct post-hoc power analysis for one or a set of predictors in \n    regression analysis with known sample size; (9) randomly pick numbers for Chinese Super Lotto\n    and Double Color Balls; (10) assess course objective achievement in Outcome-Based Education.  "
  },
  {
    "id": 4469,
    "package_name": "LLMing",
    "title": "Large Language Model (LLM) Tools for Psychological Text Analysis",
    "description": "A collection of large language model (LLM) text analysis methods\n  designed with psychological data in mind. Currently, LLMing (aka \"lemming\")\n  includes a text anomaly detection method based on the angle-based subspace\n  approach described by Zhang, Lin, and Karim (2015)\n  <doi:10.1016/j.ress.2015.05.025>.",
    "version": "1.0.0",
    "maintainer": "Lindley Slipetz <ddj6tu@virginia.edu>",
    "author": "Lindley Slipetz [aut, cre],\n  Teague Henry [aut],\n  Siqi Sun [ctb]",
    "url": "https://github.com/sliplr19/LLMing",
    "bug_reports": "https://github.com/sliplr19/LLMing/issues",
    "repository": "https://cran.r-project.org/package=LLMing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMing Large Language Model (LLM) Tools for Psychological Text Analysis A collection of large language model (LLM) text analysis methods\n  designed with psychological data in mind. Currently, LLMing (aka \"lemming\")\n  includes a text anomaly detection method based on the angle-based subspace\n  approach described by Zhang, Lin, and Karim (2015)\n  <doi:10.1016/j.ress.2015.05.025>.  "
  },
  {
    "id": 4525,
    "package_name": "LTCDM",
    "title": "Latent Transition Cognitive Diagnosis Model with Covariates",
    "description": "Implementation of the three-step approach of (latent transition) cognitive diagnosis model (CDM) with covariates. This approach can be used for single time-point situations (cross-sectional data) and multiple time-point situations (longitudinal data) to investigate how the covariates are associated with attribute mastery. For multiple time-point situations, the three-step approach of latent transition CDM with covariates allows researchers to assess changes in attribute mastery status and to evaluate the covariate effects on both the initial states and transition probabilities over time using latent logistic regression. Because stepwise approaches often yield biased estimates, correction for classification error probabilities (CEPs) is considered in this approach. The three-step approach for latent transition CDM with covariates involves the following steps: (1) fitting a CDM to the response data without covariates at each time point separately, (2) assigning examinees to latent states at each time point and computing the associated CEPs, and (3) estimating the latent transition CDM with the known CEPs and computing the regression coefficients. The method was proposed in Liang et al. (2023) <doi:10.3102/10769986231163320> and demonstrated using mental health data in Liang et al. (in press; annotated R code and data utilized in this example are available in Mendeley data) <doi:10.17632/kpjp3gnwbt.1>.",
    "version": "1.1.0",
    "maintainer": "Qianru Liang <liangqr@jnu.edu.cn>",
    "author": "Qianru Liang [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7558-0431>),\n  Jimmy de la Torre [aut] (ORCID:\n    <https://orcid.org/0000-0002-0893-3863>),\n  Jingping Du [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LTCDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LTCDM Latent Transition Cognitive Diagnosis Model with Covariates Implementation of the three-step approach of (latent transition) cognitive diagnosis model (CDM) with covariates. This approach can be used for single time-point situations (cross-sectional data) and multiple time-point situations (longitudinal data) to investigate how the covariates are associated with attribute mastery. For multiple time-point situations, the three-step approach of latent transition CDM with covariates allows researchers to assess changes in attribute mastery status and to evaluate the covariate effects on both the initial states and transition probabilities over time using latent logistic regression. Because stepwise approaches often yield biased estimates, correction for classification error probabilities (CEPs) is considered in this approach. The three-step approach for latent transition CDM with covariates involves the following steps: (1) fitting a CDM to the response data without covariates at each time point separately, (2) assigning examinees to latent states at each time point and computing the associated CEPs, and (3) estimating the latent transition CDM with the known CEPs and computing the regression coefficients. The method was proposed in Liang et al. (2023) <doi:10.3102/10769986231163320> and demonstrated using mental health data in Liang et al. (in press; annotated R code and data utilized in this example are available in Mendeley data) <doi:10.17632/kpjp3gnwbt.1>.  "
  },
  {
    "id": 4526,
    "package_name": "LTFGRS",
    "title": "Implementation of Several Phenotype-Based Family Genetic Risk\nScores",
    "description": "Implementation of several phenotype-based family genetic risk scores \n  with unified input data and data preparation functions to help facilitate\n  the required data preparation and management. The implemented family genetic\n  risk scores are the extended liability threshold model conditional on family history\n  from Pedersen (2022) <doi:10.1016/j.ajhg.2022.01.009> and Pedersen (2023)  <https://www.nature.com/articles/s41467-023-41210-z>,\n  Pearson-Aitken Family Genetic Risk Scores from Krebs (2024) <doi:10.1016/j.ajhg.2024.09.009>,\n  and family genetic risk score from Kendler (2021) <doi:10.1001/jamapsychiatry.2021.0336>.",
    "version": "1.0.1",
    "maintainer": "Emil Michael Pedersen <emp@ph.au.dk>",
    "author": "Emil Michael Pedersen [aut, cre],\n  Jette Steinbach [aut],\n  Lucas Rasmussen [ctb],\n  Morten Dybdahl Krebs [aut]",
    "url": "https://emilmip.github.io/LTFGRS/",
    "bug_reports": "https://github.com/EmilMiP/LTFGRS/issues",
    "repository": "https://cran.r-project.org/package=LTFGRS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LTFGRS Implementation of Several Phenotype-Based Family Genetic Risk\nScores Implementation of several phenotype-based family genetic risk scores \n  with unified input data and data preparation functions to help facilitate\n  the required data preparation and management. The implemented family genetic\n  risk scores are the extended liability threshold model conditional on family history\n  from Pedersen (2022) <doi:10.1016/j.ajhg.2022.01.009> and Pedersen (2023)  <https://www.nature.com/articles/s41467-023-41210-z>,\n  Pearson-Aitken Family Genetic Risk Scores from Krebs (2024) <doi:10.1016/j.ajhg.2024.09.009>,\n  and family genetic risk score from Kendler (2021) <doi:10.1001/jamapsychiatry.2021.0336>.  "
  },
  {
    "id": 4618,
    "package_name": "LogisticRCI",
    "title": "Linear and Logistic Regression-Based Reliable Change Index",
    "description": "Here we provide an implementation of the linear and logistic regression-based Reliable Change Index (RCI), to be used with lm and binomial glm model objects, respectively, following Moral et al. <https://psyarxiv.com/gq7az/>. The RCI function returns a score assumed to be approximately normally distributed, which is helpful to detect patients that may present cognitive decline.",
    "version": "1.1",
    "maintainer": "Rafael de Andrade Moral <rafael.deandrademoral@mu.ie>",
    "author": "Rafael de Andrade Moral [aut, cre],\n  Unai Diaz-Orueta [aut],\n  Javier Oltra-Cucarella [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LogisticRCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LogisticRCI Linear and Logistic Regression-Based Reliable Change Index Here we provide an implementation of the linear and logistic regression-based Reliable Change Index (RCI), to be used with lm and binomial glm model objects, respectively, following Moral et al. <https://psyarxiv.com/gq7az/>. The RCI function returns a score assumed to be approximately normally distributed, which is helpful to detect patients that may present cognitive decline.  "
  },
  {
    "id": 4676,
    "package_name": "MBESS",
    "title": "The MBESS R Package",
    "description": "Implements methods that are useful in designing research studies and analyzing data, with \n\tparticular emphasis on methods that are developed for or used within the behavioral, \n\teducational, and social sciences (broadly defined). That being said, many of the methods \n\timplemented within MBESS are applicable to a wide variety of disciplines. MBESS has a \n\tsuite of functions for a variety of related topics, such as effect sizes, confidence intervals \n\tfor effect sizes (including standardized effect sizes and noncentral effect sizes), sample size\n\tplanning (from the accuracy in parameter estimation [AIPE], power analytic, equivalence, and \n\tminimum-risk point estimation perspectives), mediation analysis, various properties of \n\tdistributions, and a variety of utility functions. MBESS (pronounced 'em-bes') was originally \n\tan acronym for 'Methods for the Behavioral, Educational, and Social Sciences,' but MBESS became\n\tmore general and now contains methods applicable and used in a wide variety of fields and is an \n\torphan acronym, in the sense that what was an acronym is now literally its name. MBESS has \n\tgreatly benefited from others, see <https://www3.nd.edu/~kkelley/r-packages.html> for a detailed \n\tlist of those that have contributed and other details.",
    "version": "4.9.41",
    "maintainer": "Ken Kelley <kkelley@nd.edu>",
    "author": "Ken Kelley [aut, cre]",
    "url": "https://www3.nd.edu/~kkelley/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MBESS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MBESS The MBESS R Package Implements methods that are useful in designing research studies and analyzing data, with \n\tparticular emphasis on methods that are developed for or used within the behavioral, \n\teducational, and social sciences (broadly defined). That being said, many of the methods \n\timplemented within MBESS are applicable to a wide variety of disciplines. MBESS has a \n\tsuite of functions for a variety of related topics, such as effect sizes, confidence intervals \n\tfor effect sizes (including standardized effect sizes and noncentral effect sizes), sample size\n\tplanning (from the accuracy in parameter estimation [AIPE], power analytic, equivalence, and \n\tminimum-risk point estimation perspectives), mediation analysis, various properties of \n\tdistributions, and a variety of utility functions. MBESS (pronounced 'em-bes') was originally \n\tan acronym for 'Methods for the Behavioral, Educational, and Social Sciences,' but MBESS became\n\tmore general and now contains methods applicable and used in a wide variety of fields and is an \n\torphan acronym, in the sense that what was an acronym is now literally its name. MBESS has \n\tgreatly benefited from others, see <https://www3.nd.edu/~kkelley/r-packages.html> for a detailed \n\tlist of those that have contributed and other details.  "
  },
  {
    "id": 4758,
    "package_name": "MHQoL",
    "title": "Mental Health Quality of Life Toolkit",
    "description": "Transforms, calculates, and presents results from the Mental Health Quality of Life Questionnaire (MHQoL), a measure of health-related quality of life for individuals with mental health conditions. Provides scoring functions, summary statistics, and visualization tools to facilitate interpretation. For more details see van Krugten et al.(2022) <doi:10.1007/s11136-021-02935-w>.",
    "version": "0.14.0",
    "maintainer": "Stijn Peeters <s.b.peeters@eshpm.eur.nl>",
    "author": "Stijn Peeters [aut, cre] (ORCID:\n    <https://orcid.org/0009-0004-3684-3584>),\n  Frederick Thielen [aut] (ORCID:\n    <https://orcid.org/0000-0002-0312-5891>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MHQoL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHQoL Mental Health Quality of Life Toolkit Transforms, calculates, and presents results from the Mental Health Quality of Life Questionnaire (MHQoL), a measure of health-related quality of life for individuals with mental health conditions. Provides scoring functions, summary statistics, and visualization tools to facilitate interpretation. For more details see van Krugten et al.(2022) <doi:10.1007/s11136-021-02935-w>.  "
  },
  {
    "id": 4789,
    "package_name": "MLCM",
    "title": "Maximum Likelihood Conjoint Measurement",
    "description": "Conjoint measurement is a psychophysical procedure in which stimulus pairs are presented that vary along 2 or more dimensions and the observer is required to compare the stimuli along one of them.  This package contains functions to estimate the contribution of the n scales to the judgment by a maximum likelihood method under several hypotheses of how the perceptual dimensions interact. Reference: Knoblauch & Maloney (2012) \"Modeling Psychophysical Data in R\". <doi:10.1007/978-1-4614-4475-6>.",
    "version": "0.4.3",
    "maintainer": "Guillermo Aguilar <guillermo.aguilar@mail.tu-berlin.de>",
    "author": "Ken Knoblauch [aut],\n  Laurence T. Maloney [aut],\n  Guillermo Aguilar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MLCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MLCM Maximum Likelihood Conjoint Measurement Conjoint measurement is a psychophysical procedure in which stimulus pairs are presented that vary along 2 or more dimensions and the observer is required to compare the stimuli along one of them.  This package contains functions to estimate the contribution of the n scales to the judgment by a maximum likelihood method under several hypotheses of how the perceptual dimensions interact. Reference: Knoblauch & Maloney (2012) \"Modeling Psychophysical Data in R\". <doi:10.1007/978-1-4614-4475-6>.  "
  },
  {
    "id": 4843,
    "package_name": "MOST",
    "title": "Multiphase Optimization Strategy",
    "description": "Provides functions similar to the 'SAS' macros previously provided\n              to accompany Collins, Dziak, and Li (2009) <DOI:10.1037/a0015826>\n              and Dziak, Nahum-Shani, and Collins (2012) <DOI:10.1037/a0026972>, papers\n              which outline practical benefits and challenges of factorial\n              and fractional factorial experiments for scientists interested\n              in developing biological and/or behavioral interventions, especially\n              in the context of the multiphase optimization strategy\n              (see Collins, Kugler & Gwadz 2016) <DOI:10.1007/s10461-015-1145-4>.  The package\n              currently contains three functions. First, RelativeCosts1() draws a graph\n              of the relative cost of complete and reduced factorial designs versus\n              other alternatives. Second, RandomAssignmentGenerator() returns a dataframe\n              which contains a list of random numbers that can be used to conveniently\n              assign participants to conditions in an experiment with\n              many conditions. Third, FactorialPowerPlan() estimates the power, detectable effect\n              size, or required sample size of a factorial or fractional factorial\n              experiment, for main effects or interactions, given several possible choices\n              of effect size metric, and allowing pretests and clustering.",
    "version": "0.1.2",
    "maintainer": "John Dziak <dziakj1@gmail.com>",
    "author": "Linda Collins [aut],\n  Liying Huang [aut],\n  John Dziak [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MOST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOST Multiphase Optimization Strategy Provides functions similar to the 'SAS' macros previously provided\n              to accompany Collins, Dziak, and Li (2009) <DOI:10.1037/a0015826>\n              and Dziak, Nahum-Shani, and Collins (2012) <DOI:10.1037/a0026972>, papers\n              which outline practical benefits and challenges of factorial\n              and fractional factorial experiments for scientists interested\n              in developing biological and/or behavioral interventions, especially\n              in the context of the multiphase optimization strategy\n              (see Collins, Kugler & Gwadz 2016) <DOI:10.1007/s10461-015-1145-4>.  The package\n              currently contains three functions. First, RelativeCosts1() draws a graph\n              of the relative cost of complete and reduced factorial designs versus\n              other alternatives. Second, RandomAssignmentGenerator() returns a dataframe\n              which contains a list of random numbers that can be used to conveniently\n              assign participants to conditions in an experiment with\n              many conditions. Third, FactorialPowerPlan() estimates the power, detectable effect\n              size, or required sample size of a factorial or fractional factorial\n              experiment, for main effects or interactions, given several possible choices\n              of effect size metric, and allowing pretests and clustering.  "
  },
  {
    "id": 4848,
    "package_name": "MPDiR",
    "title": "Data Sets and Scripts for Modeling Psychophysical Data in R",
    "description": "Data sets and scripts for Modeling Psychophysical Data in R (Springer).",
    "version": "0.3",
    "maintainer": "Kenneth Knoblauch <ken.knoblauch@inserm.fr>",
    "author": "Kenneth Knoblauch [aut, cre],\n  Laurence T. Maloney [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MPDiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MPDiR Data Sets and Scripts for Modeling Psychophysical Data in R Data sets and scripts for Modeling Psychophysical Data in R (Springer).  "
  },
  {
    "id": 4854,
    "package_name": "MPTinR",
    "title": "Analyze Multinomial Processing Tree Models",
    "description": "Provides a user-friendly way for the analysis of multinomial processing tree (MPT) models (e.g.,  Riefer, D. M., and Batchelder, W. H. [1988]. Multinomial modeling and the measurement of cognitive processes. Psychological Review, 95, 318-339) for single and multiple datasets. The main functions perform model fitting and model selection. Model selection can be done using AIC, BIC, or the Fisher Information Approximation (FIA) a measure based on the Minimum Description Length (MDL) framework. The model and restrictions can be specified in external files or within an R script in an intuitive syntax or using the context-free language for MPTs. The 'classical' .EQN file format for model files is also supported. Besides MPTs, this package can fit a wide variety of other cognitive models such as SDT models (see fit.model). It also supports multicore fitting and FIA calculation (using the snowfall package), can generate or bootstrap data for simulations, and plot predicted versus observed data.",
    "version": "1.14.1",
    "maintainer": "Henrik Singmann <singmann@gmail.com>",
    "author": "Henrik Singmann [aut, cre],\n  David Kellen [aut],\n  Quentin Gronau [aut],\n  Christian Mueller [ctb],\n  Akhil S Bhel [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MPTinR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MPTinR Analyze Multinomial Processing Tree Models Provides a user-friendly way for the analysis of multinomial processing tree (MPT) models (e.g.,  Riefer, D. M., and Batchelder, W. H. [1988]. Multinomial modeling and the measurement of cognitive processes. Psychological Review, 95, 318-339) for single and multiple datasets. The main functions perform model fitting and model selection. Model selection can be done using AIC, BIC, or the Fisher Information Approximation (FIA) a measure based on the Minimum Description Length (MDL) framework. The model and restrictions can be specified in external files or within an R script in an intuitive syntax or using the context-free language for MPTs. The 'classical' .EQN file format for model files is also supported. Besides MPTs, this package can fit a wide variety of other cognitive models such as SDT models (see fit.model). It also supports multicore fitting and FIA calculation (using the snowfall package), can generate or bootstrap data for simulations, and plot predicted versus observed data.  "
  },
  {
    "id": 4856,
    "package_name": "MPsychoR",
    "title": "Modern Psychometrics with R",
    "description": "Supplementary materials and datasets for the book \"Modern Psychometrics With R\" (Mair, 2018, Springer useR! series).",
    "version": "0.10-8",
    "maintainer": "Patrick Mair <mair@fas.harvard.edu>",
    "author": "Patrick Mair [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MPsychoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MPsychoR Modern Psychometrics with R Supplementary materials and datasets for the book \"Modern Psychometrics With R\" (Mair, 2018, Springer useR! series).  "
  },
  {
    "id": 4937,
    "package_name": "MacBehaviour",
    "title": "Behavioural Studies of Large Language Models",
    "description": "Efficient way to design and conduct psychological experiments for testing the performance of large language models. It simplifies the process of setting up experiments and data collection via language models\u2019 API, facilitating a smooth workflow for researchers in the field of machine behaviour.",
    "version": "1.2.8",
    "maintainer": "Xufeng Duan <dxfdxfdxf88@gmail.com>",
    "author": "Xufeng Duan [aut, cre],\n  Shixuan Li [aut],\n  Zhenguang Cai [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MacBehaviour",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MacBehaviour Behavioural Studies of Large Language Models Efficient way to design and conduct psychological experiments for testing the performance of large language models. It simplifies the process of setting up experiments and data collection via language models\u2019 API, facilitating a smooth workflow for researchers in the field of machine behaviour.  "
  },
  {
    "id": 5088,
    "package_name": "MixedPsy",
    "title": "Statistical Tools for the Analysis of Psychophysical Data",
    "description": "Tools for the analysis of psychophysical data in R. This package allows to estimate the Point of Subjective Equivalence (PSE) \n    and the Just Noticeable Difference (JND), either from a psychometric function or from a Generalized Linear Mixed Model (GLMM). \n    Additionally, the package allows plotting the fitted models and the response data, simulating psychometric functions of different shapes, and simulating data sets.\n    For a description of the use of GLMMs applied to psychophysical data, refer to Moscatelli et al. (2012).",
    "version": "1.3.0",
    "maintainer": "Alessandro Moscatelli <moskante@gmail.com>",
    "author": "Alessandro Moscatelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6269-4536>),\n  Priscilla Balestrucci [aut] (ORCID:\n    <https://orcid.org/0000-0002-5764-9439>)",
    "url": "https://mixedpsychophysics.wordpress.com",
    "bug_reports": "https://github.com/moskante/MixedPsy/issues",
    "repository": "https://cran.r-project.org/package=MixedPsy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MixedPsy Statistical Tools for the Analysis of Psychophysical Data Tools for the analysis of psychophysical data in R. This package allows to estimate the Point of Subjective Equivalence (PSE) \n    and the Just Noticeable Difference (JND), either from a psychometric function or from a Generalized Linear Mixed Model (GLMM). \n    Additionally, the package allows plotting the fitted models and the response data, simulating psychometric functions of different shapes, and simulating data sets.\n    For a description of the use of GLMMs applied to psychophysical data, refer to Moscatelli et al. (2012).  "
  },
  {
    "id": 5097,
    "package_name": "MoLE",
    "title": "Modeling Language Evolution",
    "description": "Model for simulating language evolution in terms of cultural evolution (Smith & Kirby (2008) <DOI:10.1098/rstb.2008.0145>; Deacon 1997). The focus is on the emergence of argument-marking systems (Dowty (1991) <DOI:10.1353/lan.1991.0021>, Van Valin 1999, Dryer 2002, Lestrade 2015a), i.e. noun marking (Aristar (1997) <DOI:10.1075/sl.21.2.04ari>, Lestrade (2010) <DOI:10.7282/T3ZG6R4S>), person indexing (Ariel 1999, Dahl (2000) <DOI:10.1075/fol.7.1.03dah>, Bhat 2004), and word order (Dryer 2013), but extensions are foreseen. Agents start out with a protolanguage (a language without grammar; Bickerton (1981) <DOI:10.17169/langsci.b91.109>, Jackendoff 2002, Arbib (2015) <DOI:10.1002/9781118346136.ch27>) and interact through language games (Steels 1997). Over time, grammatical constructions emerge that may or may not become obligatory (for which the tolerance principle is assumed; Yang 2016). Throughout the simulation, uniformitarianism of principles is assumed (Hopper (1987) <DOI:10.3765/bls.v13i0.1834>, Givon (1995) <DOI:10.1075/z.74>, Croft (2000), Saffran (2001) <DOI:10.1111/1467-8721.01243>, Heine & Kuteva 2007), in which maximal psychological validity is aimed at (Grice (1975) <DOI:10.1057/9780230005853_5>, Levelt 1989, Gaerdenfors 2000) and language representation is usage based (Tomasello 2003, Bybee 2010). In Lestrade (2015b) <DOI:10.15496/publikation-8640>, Lestrade (2015c) <DOI:10.1075/avt.32.08les>, and Lestrade (2016) <DOI:10.17617/2.2248195>), which reported on the results of preliminary versions, this package was announced as WDWTW (for who does what to whom), but for reasons of pronunciation and generalization the title was changed.",
    "version": "1.0.1",
    "maintainer": "Sander Lestrade <samlestrade@protonmail.com>",
    "author": "Sander Lestrade",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MoLE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MoLE Modeling Language Evolution Model for simulating language evolution in terms of cultural evolution (Smith & Kirby (2008) <DOI:10.1098/rstb.2008.0145>; Deacon 1997). The focus is on the emergence of argument-marking systems (Dowty (1991) <DOI:10.1353/lan.1991.0021>, Van Valin 1999, Dryer 2002, Lestrade 2015a), i.e. noun marking (Aristar (1997) <DOI:10.1075/sl.21.2.04ari>, Lestrade (2010) <DOI:10.7282/T3ZG6R4S>), person indexing (Ariel 1999, Dahl (2000) <DOI:10.1075/fol.7.1.03dah>, Bhat 2004), and word order (Dryer 2013), but extensions are foreseen. Agents start out with a protolanguage (a language without grammar; Bickerton (1981) <DOI:10.17169/langsci.b91.109>, Jackendoff 2002, Arbib (2015) <DOI:10.1002/9781118346136.ch27>) and interact through language games (Steels 1997). Over time, grammatical constructions emerge that may or may not become obligatory (for which the tolerance principle is assumed; Yang 2016). Throughout the simulation, uniformitarianism of principles is assumed (Hopper (1987) <DOI:10.3765/bls.v13i0.1834>, Givon (1995) <DOI:10.1075/z.74>, Croft (2000), Saffran (2001) <DOI:10.1111/1467-8721.01243>, Heine & Kuteva 2007), in which maximal psychological validity is aimed at (Grice (1975) <DOI:10.1057/9780230005853_5>, Levelt 1989, Gaerdenfors 2000) and language representation is usage based (Tomasello 2003, Bybee 2010). In Lestrade (2015b) <DOI:10.15496/publikation-8640>, Lestrade (2015c) <DOI:10.1075/avt.32.08les>, and Lestrade (2016) <DOI:10.17617/2.2248195>), which reported on the results of preliminary versions, this package was announced as WDWTW (for who does what to whom), but for reasons of pronunciation and generalization the title was changed.  "
  },
  {
    "id": 5157,
    "package_name": "MultiLevelOptimalBayes",
    "title": "Regularized Bayesian Estimator for Two-Level Latent Variable\nModels",
    "description": "Implements a regularized Bayesian estimator that optimizes the estimation\n of between-group coefficients for multilevel latent variable models by minimizing\n mean squared error (MSE) and balancing variance and bias. The package provides more reliable\n estimates in scenarios with limited data, offering a robust solution for accurate\n parameter estimation in two-level latent variable models. It is designed for\n researchers in psychology, education, and related fields who face challenges in\n estimating between-group effects under small sample sizes and low intraclass\n correlation coefficients. The package includes comprehensive S3 methods for result\n objects: print(), summary(), coef(), se(), vcov(), confint(), as.data.frame(),\n dim(), length(), names(), and update() for enhanced usability and integration\n with standard R workflows. Dashuk et al. (2025a) <doi:10.1017/psy.2025.10045>\n derived the optimal regularized Bayesian estimator;\n Dashuk et al. (2025b) <doi:10.1007/s41237-025-00264-7> extended it to \n the multivariate case; and Luedtke et al. (2008) <doi:10.1037/a0012869>\n formalized the two-level latent variable framework.",
    "version": "0.0.4.0",
    "maintainer": "Valerii Dashuk <vadashuk@gmail.com>",
    "author": "Valerii Dashuk [aut, cre],\n  Binayak Timilsina [aut],\n  Martin Hecht [aut],\n  Steffen Zitzmann [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiLevelOptimalBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiLevelOptimalBayes Regularized Bayesian Estimator for Two-Level Latent Variable\nModels Implements a regularized Bayesian estimator that optimizes the estimation\n of between-group coefficients for multilevel latent variable models by minimizing\n mean squared error (MSE) and balancing variance and bias. The package provides more reliable\n estimates in scenarios with limited data, offering a robust solution for accurate\n parameter estimation in two-level latent variable models. It is designed for\n researchers in psychology, education, and related fields who face challenges in\n estimating between-group effects under small sample sizes and low intraclass\n correlation coefficients. The package includes comprehensive S3 methods for result\n objects: print(), summary(), coef(), se(), vcov(), confint(), as.data.frame(),\n dim(), length(), names(), and update() for enhanced usability and integration\n with standard R workflows. Dashuk et al. (2025a) <doi:10.1017/psy.2025.10045>\n derived the optimal regularized Bayesian estimator;\n Dashuk et al. (2025b) <doi:10.1007/s41237-025-00264-7> extended it to \n the multivariate case; and Luedtke et al. (2008) <doi:10.1037/a0012869>\n formalized the two-level latent variable framework.  "
  },
  {
    "id": 5172,
    "package_name": "MultipleRegression",
    "title": "Multiple Regression Analysis",
    "description": "Tools to analysis of experiments having two or more quantitative explanatory variables and one quantitative dependent variable. Experiments can be without repetitions or with a statistical design (Hair JF, 2016) <ISBN: 13: 978-0138132637>. Pacote para uma analise de experimentos havendo duas ou mais variaveis explicativas quantitativas e uma variavel dependente quantitativa. Os experimentos podem ser sem repeticoes ou com delineamento estatistico (Hair JF, 2016) <ISBN: 13: 978-0138132637>. ",
    "version": "0.1.0",
    "maintainer": "Alcinei Mistico Azevedo <alcineimistico@hotmail.com>",
    "author": "Alcinei Mistico Azevedo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5196-0851>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultipleRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultipleRegression Multiple Regression Analysis Tools to analysis of experiments having two or more quantitative explanatory variables and one quantitative dependent variable. Experiments can be without repetitions or with a statistical design (Hair JF, 2016) <ISBN: 13: 978-0138132637>. Pacote para uma analise de experimentos havendo duas ou mais variaveis explicativas quantitativas e uma variavel dependente quantitativa. Os experimentos podem ser sem repeticoes ou com delineamento estatistico (Hair JF, 2016) <ISBN: 13: 978-0138132637>.   "
  },
  {
    "id": 5193,
    "package_name": "NAP",
    "title": "Non-Local Alternative Priors in Psychology",
    "description": "Conducts Bayesian Hypothesis tests of a point null hypothesis against a two-sided alternative\n             using Non-local Alternative Prior (NAP) for one- and two-sample z- and t-tests \n             (Pramanik and Johnson, 2022). Under the alternative, the NAP is assumed on the standardized \n             effects size in one-sample tests and on their differences in two-sample tests. The package \n             considers two types of NAP densities: (1) the normal moment prior, and (2) the composite alternative. \n             In fixed design tests, the functions calculate the Bayes factors and the expected weight of evidence\n             for varied effect size and sample size. The package also provides a sequential testing framework using the\n             Sequential Bayes Factor (SBF) design. The functions calculate the operating characteristics (OC)\n             and the average sample number (ASN), and also conducts sequential tests for a sequentially observed data.",
    "version": "1.1",
    "maintainer": "Sandipan Pramanik <sandy@stat.tamu.edu>",
    "author": "Sandipan Pramanik [aut, cre],\n  Valen E. Johnson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NAP Non-Local Alternative Priors in Psychology Conducts Bayesian Hypothesis tests of a point null hypothesis against a two-sided alternative\n             using Non-local Alternative Prior (NAP) for one- and two-sample z- and t-tests \n             (Pramanik and Johnson, 2022). Under the alternative, the NAP is assumed on the standardized \n             effects size in one-sample tests and on their differences in two-sample tests. The package \n             considers two types of NAP densities: (1) the normal moment prior, and (2) the composite alternative. \n             In fixed design tests, the functions calculate the Bayes factors and the expected weight of evidence\n             for varied effect size and sample size. The package also provides a sequential testing framework using the\n             Sequential Bayes Factor (SBF) design. The functions calculate the operating characteristics (OC)\n             and the average sample number (ASN), and also conducts sequential tests for a sequentially observed data.  "
  },
  {
    "id": 5261,
    "package_name": "NPCD",
    "title": "Nonparametric Methods for Cognitive Diagnosis",
    "description": "An array of nonparametric and parametric estimation methods for cognitive diagnostic models, including nonparametric classification of examinee attribute profiles, joint maximum likelihood estimation (JMLE) of examinee attribute profiles and item parameters, and nonparametric refinement of the Q-matrix, as well as conditional maximum likelihood estimation (CMLE) of examinee attribute profiles given item parameters and CMLE of item parameters given examinee attribute profiles. Currently the nonparametric methods in the package support both conjunctive and disjunctive models, and the parametric methods in the package support the DINA model, the DINO model, the NIDA model, the G-NIDA model, and the R-RUM model. ",
    "version": "1.0-11",
    "maintainer": "Yi Zheng <yi.isabel.zheng@asu.edu>",
    "author": "Yi Zheng [aut, cre],\n  Chia-Yi Chiu [aut],\n  Jeffrey A. Douglas [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NPCD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NPCD Nonparametric Methods for Cognitive Diagnosis An array of nonparametric and parametric estimation methods for cognitive diagnostic models, including nonparametric classification of examinee attribute profiles, joint maximum likelihood estimation (JMLE) of examinee attribute profiles and item parameters, and nonparametric refinement of the Q-matrix, as well as conditional maximum likelihood estimation (CMLE) of examinee attribute profiles given item parameters and CMLE of item parameters given examinee attribute profiles. Currently the nonparametric methods in the package support both conjunctive and disjunctive models, and the parametric methods in the package support the DINA model, the DINO model, the NIDA model, the G-NIDA model, and the R-RUM model.   "
  },
  {
    "id": 5262,
    "package_name": "NPCDTools",
    "title": "The Nonparametric Classification Methods for Cognitive Diagnosis",
    "description": "Statistical tools for analyzing cognitive diagnosis (CD) data collected from small settings using the nonparametric classification (NPCD) framework. The core methods of the NPCD framework includes the nonparametric classification (NPC) method developed by Chiu and Douglas (2013) <DOI:10.1007/s00357-013-9132-9> and the general NPC (GNPC) method developed by Chiu, Sun, and Bian (2018) <DOI:10.1007/s11336-017-9595-4> and Chiu and K\u00f6hn (2019) <DOI:10.1007/s11336-019-09660-x>. An extension of the NPCD framework included in the package is the nonparametric method for multiple-choice items (MC-NPC) developed by Wang, Chiu, and Koehn (2023) <DOI:10.3102/10769986221133088>.  Functions associated with various extensions concerning the evaluation, validation, and feasibility of the CD analysis are also provided. These topics include the completeness of Q-matrix, Q-matrix refinement method, as well as Q-matrix estimation. ",
    "version": "1.0",
    "maintainer": "Weixuan Xiao <wx2299@tc.columbia.edu>",
    "author": "Chia-Yi Chiu [aut, cph],\n  Weixuan Xiao [aut, cre],\n  Xiran Wen [aut],\n  Yu Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NPCDTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NPCDTools The Nonparametric Classification Methods for Cognitive Diagnosis Statistical tools for analyzing cognitive diagnosis (CD) data collected from small settings using the nonparametric classification (NPCD) framework. The core methods of the NPCD framework includes the nonparametric classification (NPC) method developed by Chiu and Douglas (2013) <DOI:10.1007/s00357-013-9132-9> and the general NPC (GNPC) method developed by Chiu, Sun, and Bian (2018) <DOI:10.1007/s11336-017-9595-4> and Chiu and K\u00f6hn (2019) <DOI:10.1007/s11336-019-09660-x>. An extension of the NPCD framework included in the package is the nonparametric method for multiple-choice items (MC-NPC) developed by Wang, Chiu, and Koehn (2023) <DOI:10.3102/10769986221133088>.  Functions associated with various extensions concerning the evaluation, validation, and feasibility of the CD analysis are also provided. These topics include the completeness of Q-matrix, Q-matrix refinement method, as well as Q-matrix estimation.   "
  },
  {
    "id": 5327,
    "package_name": "NetworkToolbox",
    "title": "Methods and Measures for Brain, Cognitive, and Psychometric\nNetwork Analysis",
    "description": "Implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. Methods include various filtering methods and approaches such as threshold, dependency (Kenett, Tumminello, Madi, Gur-Gershgoren, Mantegna, & Ben-Jacob, 2010 <doi:10.1371/journal.pone.0015032>), Information Filtering Networks (Barfuss, Massara, Di Matteo, & Aste, 2016 <doi:10.1103/PhysRevE.94.062306>), and Efficiency-Cost Optimization (Fallani, Latora, & Chavez, 2017 <doi:10.1371/journal.pcbi.1005305>). Brain methods include the recently developed Connectome Predictive Modeling (see references in package). Also implements several network measures including local network characteristics (e.g., centrality), community-level network characteristics (e.g., community centrality), global network characteristics (e.g., clustering coefficient), and various other measures associated with the reliability and reproducibility of network analysis. ",
    "version": "1.4.4",
    "maintainer": "Alexander Christensen <alexpaulchristensen@gmail.com>",
    "author": "Alexander Christensen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9798-7037>),\n  Guido Previde Massara [ctb] (ORCID:\n    <https://orcid.org/0000-0003-0502-2789>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NetworkToolbox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetworkToolbox Methods and Measures for Brain, Cognitive, and Psychometric\nNetwork Analysis Implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. Methods include various filtering methods and approaches such as threshold, dependency (Kenett, Tumminello, Madi, Gur-Gershgoren, Mantegna, & Ben-Jacob, 2010 <doi:10.1371/journal.pone.0015032>), Information Filtering Networks (Barfuss, Massara, Di Matteo, & Aste, 2016 <doi:10.1103/PhysRevE.94.062306>), and Efficiency-Cost Optimization (Fallani, Latora, & Chavez, 2017 <doi:10.1371/journal.pcbi.1005305>). Brain methods include the recently developed Connectome Predictive Modeling (see references in package). Also implements several network measures including local network characteristics (e.g., centrality), community-level network characteristics (e.g., community centrality), global network characteristics (e.g., clustering coefficient), and various other measures associated with the reliability and reproducibility of network analysis.   "
  },
  {
    "id": 5331,
    "package_name": "NeuroDataSets",
    "title": "A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets",
    "description": "Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.",
    "version": "0.3.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/neurodatasets,\nhttps://lightbluetitan.github.io/neurodatasets/",
    "bug_reports": "https://github.com/lightbluetitan/neurodatasets/issues",
    "repository": "https://cran.r-project.org/package=NeuroDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NeuroDataSets A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.  "
  },
  {
    "id": 5352,
    "package_name": "NormData",
    "title": "Derivation of Regression-Based Normative Data",
    "description": "Normative data are often used to estimate the relative position of a raw test score in the population. This package allows for deriving regression-based normative data. It includes functions that enable the fitting of regression models for the mean and residual (or variance) structures, test the model assumptions, derive the normative data in the form of normative tables or automatic scoring sheets, and estimate confidence intervals for the norms. This package accompanies the book Van der Elst, W. (2024). Regression-based normative data for psychological assessment. A hands-on approach using R. Springer Nature.  ",
    "version": "1.1",
    "maintainer": "Wim Van der Elst <Wim.vanderelst@gmail.com>",
    "author": "Wim Van der Elst [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4315-7406>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NormData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NormData Derivation of Regression-Based Normative Data Normative data are often used to estimate the relative position of a raw test score in the population. This package allows for deriving regression-based normative data. It includes functions that enable the fitting of regression models for the mean and residual (or variance) structures, test the model assumptions, derive the normative data in the form of normative tables or automatic scoring sheets, and estimate confidence intervals for the norms. This package accompanies the book Van der Elst, W. (2024). Regression-based normative data for psychological assessment. A hands-on approach using R. Springer Nature.    "
  },
  {
    "id": 5354,
    "package_name": "NormPsy",
    "title": "Normalisation of Psychometric Tests",
    "description": "Functions for normalizing psychometric test scores. The normalization aims at correcting the metrological properties of the psychometric tests such as the ceiling and floor effects and the curvilinearity (unequal interval scaling). Functions to compute and plot predictions in the natural scale of the psychometric test from the estimates of a linear mixed model estimated on the normalized scores are also provided. See Philipps et al (2014) <doi:10.1159/000365637> for details.",
    "version": "1.0.8",
    "maintainer": "Cecile Proust-Lima <cecile.proust-lima@inserm.fr>",
    "author": "Cecile Proust-Lima, Viviane Philipps",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NormPsy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NormPsy Normalisation of Psychometric Tests Functions for normalizing psychometric test scores. The normalization aims at correcting the metrological properties of the psychometric tests such as the ceiling and floor effects and the curvilinearity (unequal interval scaling). Functions to compute and plot predictions in the natural scale of the psychometric test from the estimates of a linear mixed model estimated on the normalized scores are also provided. See Philipps et al (2014) <doi:10.1159/000365637> for details.  "
  },
  {
    "id": 5439,
    "package_name": "Omisc",
    "title": "Univariate Bootstrapping and Other Things",
    "description": "Primarily devoted to implementing the Univariate Bootstrap (as well as the Traditional Bootstrap). In addition there are multiple functions for DeFries-Fulker behavioral genetics models. The univariate bootstrapping functions, DeFries-Fulker functions, regression and traditional bootstrapping functions form the original core. Additional features may come online later, however this software is a work in progress. For more information about univariate bootstrapping see: Lee and Rodgers (1998) and Beasley et al (2007) <doi:10.1037/1082-989X.12.4.414>.",
    "version": "0.1.5",
    "maintainer": "Patrick O'Keefe <okeefep@ohsu.edu>",
    "author": "Patrick O'Keefe",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Omisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Omisc Univariate Bootstrapping and Other Things Primarily devoted to implementing the Univariate Bootstrap (as well as the Traditional Bootstrap). In addition there are multiple functions for DeFries-Fulker behavioral genetics models. The univariate bootstrapping functions, DeFries-Fulker functions, regression and traditional bootstrapping functions form the original core. Additional features may come online later, however this software is a work in progress. For more information about univariate bootstrapping see: Lee and Rodgers (1998) and Beasley et al (2007) <doi:10.1037/1082-989X.12.4.414>.  "
  },
  {
    "id": 5471,
    "package_name": "OpenRepGrid",
    "title": "Tools to Analyze Repertory Grid Data",
    "description": "Analyze repertory grids, a qualitative-quantitative \n    data collection technique devised by George A. Kelly in the 1950s. Today, grids are used across \n    various domains ranging from clinical psychology to marketing. The package contains functions \n    to quantitatively analyze and visualize repertory grid data (e.g. 'Fransella', 'Bell', & 'Bannister', \n    2004, ISBN: 978-0-470-09080-0). The package is part of the The package is part of the \n    <https://openrepgrid.org/> project.",
    "version": "0.1.18",
    "maintainer": "Mark Heckmann <heckmann.mark@gmail.com>",
    "author": "Mark Heckmann [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0736-7417>),\n  Alejandro Garc\u00eda Guti\u00e9rrez [ctb],\n  Diego Vitali [ctb]",
    "url": "https://github.com/markheckmann/OpenRepGrid",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OpenRepGrid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpenRepGrid Tools to Analyze Repertory Grid Data Analyze repertory grids, a qualitative-quantitative \n    data collection technique devised by George A. Kelly in the 1950s. Today, grids are used across \n    various domains ranging from clinical psychology to marketing. The package contains functions \n    to quantitatively analyze and visualize repertory grid data (e.g. 'Fransella', 'Bell', & 'Bannister', \n    2004, ISBN: 978-0-470-09080-0). The package is part of the The package is part of the \n    <https://openrepgrid.org/> project.  "
  },
  {
    "id": 5563,
    "package_name": "PCSinR",
    "title": "Parallel Constraint Satisfaction Networks in R",
    "description": "Parallel Constraint Satisfaction (PCS) models are an increasingly\n    common class of models in Psychology, with applications to reading and word\n    recognition (McClelland & Rumelhart, 1981), judgment and decision making\n    (Gl\u00f6ckner & Betsch, 2008; Gl\u00f6ckner, Hilbig, & Jekel, 2014), and several\n    other fields (e.g. Read, Vanman, & Miller, 1997). In each of these fields,\n    they provide a quantitative model of psychological phenomena, with precise\n    predictions regarding choice probabilities, decision times, and often the degree\n    of confidence. This package provides the necessary functions to create and\n    simulate basic Parallel Constraint Satisfaction networks within R.",
    "version": "0.1.0",
    "maintainer": "Felix Henninger <mailbox@felixhenninger.com>",
    "author": "Felix Henninger [aut, cre]",
    "url": "https://github.com/felixhenninger/PCSinR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PCSinR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCSinR Parallel Constraint Satisfaction Networks in R Parallel Constraint Satisfaction (PCS) models are an increasingly\n    common class of models in Psychology, with applications to reading and word\n    recognition (McClelland & Rumelhart, 1981), judgment and decision making\n    (Gl\u00f6ckner & Betsch, 2008; Gl\u00f6ckner, Hilbig, & Jekel, 2014), and several\n    other fields (e.g. Read, Vanman, & Miller, 1997). In each of these fields,\n    they provide a quantitative model of psychological phenomena, with precise\n    predictions regarding choice probabilities, decision times, and often the degree\n    of confidence. This package provides the necessary functions to create and\n    simulate basic Parallel Constraint Satisfaction networks within R.  "
  },
  {
    "id": 5684,
    "package_name": "PROscorer",
    "title": "Functions to Score Commonly-Used Patient-Reported Outcome (PRO)\nMeasures and Other Psychometric Instruments",
    "description": "An extensible repository of accurate, up-to-date functions to\n    score commonly used patient-reported outcome (PRO), quality of life\n    (QOL), and other psychometric and psychological measures.\n    'PROscorer', together with the 'PROscorerTools' package, is a system\n    to facilitate the incorporation of PRO measures into research studies\n    and clinical settings in a scientifically rigorous and reproducible\n    manner.  These packages and their vignettes are intended to help\n    establish and promote best practices for scoring PRO and PRO-like \n    measures in research.  The 'PROscorer' Instrument Descriptions vignette \n    contains descriptions of each instrument scored by 'PROscorer', complete \n    with references.  These instrument descriptions are suitable for inclusion \n    in formal study protocol documents, grant proposals, and manuscript Method\n    sections.  Each 'PROscorer' function is composed of helper functions\n    from the 'PROscorerTools' package, and users are encouraged to\n    contribute new functions to 'PROscorer'.  More scoring functions are\n    currently in development and will be added in future updates.",
    "version": "0.0.4",
    "maintainer": "Ray Baser <ray.stats@gmail.com>",
    "author": "Ray Baser [aut, cre]",
    "url": "https://github.com/raybaser/PROscorer",
    "bug_reports": "https://github.com/raybaser/PROscorer/issues",
    "repository": "https://cran.r-project.org/package=PROscorer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROscorer Functions to Score Commonly-Used Patient-Reported Outcome (PRO)\nMeasures and Other Psychometric Instruments An extensible repository of accurate, up-to-date functions to\n    score commonly used patient-reported outcome (PRO), quality of life\n    (QOL), and other psychometric and psychological measures.\n    'PROscorer', together with the 'PROscorerTools' package, is a system\n    to facilitate the incorporation of PRO measures into research studies\n    and clinical settings in a scientifically rigorous and reproducible\n    manner.  These packages and their vignettes are intended to help\n    establish and promote best practices for scoring PRO and PRO-like \n    measures in research.  The 'PROscorer' Instrument Descriptions vignette \n    contains descriptions of each instrument scored by 'PROscorer', complete \n    with references.  These instrument descriptions are suitable for inclusion \n    in formal study protocol documents, grant proposals, and manuscript Method\n    sections.  Each 'PROscorer' function is composed of helper functions\n    from the 'PROscorerTools' package, and users are encouraged to\n    contribute new functions to 'PROscorer'.  More scoring functions are\n    currently in development and will be added in future updates.  "
  },
  {
    "id": 5685,
    "package_name": "PROscorerTools",
    "title": "Tools to Score Patient-Reported Outcome (PRO) and Other\nPsychometric Measures",
    "description": "Provides a reliable and flexible toolbox to score \n    patient-reported outcome (PRO), Quality of Life (QOL), and other \n    psychometric measures. The guiding philosophy is that scoring errors can \n    be eliminated by using a limited number of well-tested, well-behaved \n    functions to score PRO-like measures. The workhorse of the package is \n    the 'scoreScale' function, which can be used to score most single-scale \n    measures. It can reverse code items that need to be reversed before \n    scoring and pro-rate scores for missing item data. Currently, three \n    different types of scores can be output: summed item scores, mean item \n    scores, and scores scaled to range from 0 to 100. The 'PROscorerTools' \n    functions can be used to write new functions that score more complex \n    measures. In fact, 'PROscorerTools' functions are the building blocks of \n    the scoring functions in the 'PROscorer' package (which is a repository \n    of functions that score specific commonly-used instruments). Users are \n    encouraged to use 'PROscorerTools' to write scoring functions for their \n    favorite PRO-like instruments, and to submit these functions for \n    inclusion in 'PROscorer' (a tutorial vignette will be added soon). The \n    long-term vision for the 'PROscorerTools' and 'PROscorer' packages is to \n    provide an easy-to-use system to facilitate the incorporation of PRO \n    measures into research studies in a scientifically rigorous and \n    reproducible manner. These packages and their vignettes are intended to \n    help establish and promote \"best practices\" for scoring and describing \n    PRO-like measures in research. ",
    "version": "0.0.4",
    "maintainer": "Ray Baser <ray.stats@gmail.com>",
    "author": "Ray Baser [aut, cre]",
    "url": "https://github.com/MSKCC-Epi-Bio/PROscorerTools",
    "bug_reports": "https://github.com/MSKCC-Epi-Bio/PROscorerTools/issues",
    "repository": "https://cran.r-project.org/package=PROscorerTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROscorerTools Tools to Score Patient-Reported Outcome (PRO) and Other\nPsychometric Measures Provides a reliable and flexible toolbox to score \n    patient-reported outcome (PRO), Quality of Life (QOL), and other \n    psychometric measures. The guiding philosophy is that scoring errors can \n    be eliminated by using a limited number of well-tested, well-behaved \n    functions to score PRO-like measures. The workhorse of the package is \n    the 'scoreScale' function, which can be used to score most single-scale \n    measures. It can reverse code items that need to be reversed before \n    scoring and pro-rate scores for missing item data. Currently, three \n    different types of scores can be output: summed item scores, mean item \n    scores, and scores scaled to range from 0 to 100. The 'PROscorerTools' \n    functions can be used to write new functions that score more complex \n    measures. In fact, 'PROscorerTools' functions are the building blocks of \n    the scoring functions in the 'PROscorer' package (which is a repository \n    of functions that score specific commonly-used instruments). Users are \n    encouraged to use 'PROscorerTools' to write scoring functions for their \n    favorite PRO-like instruments, and to submit these functions for \n    inclusion in 'PROscorer' (a tutorial vignette will be added soon). The \n    long-term vision for the 'PROscorerTools' and 'PROscorer' packages is to \n    provide an easy-to-use system to facilitate the incorporation of PRO \n    measures into research studies in a scientifically rigorous and \n    reproducible manner. These packages and their vignettes are intended to \n    help establish and promote \"best practices\" for scoring and describing \n    PRO-like measures in research.   "
  },
  {
    "id": 5771,
    "package_name": "Path.Analysis",
    "title": "Path Coefficient Analysis",
    "description": "Facilitates the performance of several analyses, including simple and sequential path coefficient analysis, correlation estimate, drawing correlogram, Heatmap, and path diagram. When working with raw data, that includes one or more dependent variables along with one or more independent variables are available, the path coefficient analysis can be conducted. It allows for testing direct effects, which can be a vital indicator in path coefficient analysis. The process of preparing the dataset rule is explained in detail in the vignette file \"Path.Analysis_manual.Rmd\". You can find this in the folders labelled \"data\" and \"~/inst/extdata\". Also see: 1)the 'lavaan', 2)a sample of sequential path analysis in 'metan' suggested by Olivoto and L\u00facio (2020) <doi:10.1111/2041-210X.13384>, 3)the simple 'PATHSAS' macro written in 'SAS' by Cramer et al. (1999) <doi:10.1093/jhered/90.1.260>, and 4)the semPlot() function of 'OpenMx' as initial tools for conducting path coefficient analyses and SEM (Structural Equation Modeling). To gain a comprehensive understanding of path coefficient analysis, both in theory and practice, see a 'Minitab' macro developed by Arminian, A. in the paper by Arminian et al. (2008) <doi:10.1080/15427520802043182>.",
    "version": "0.1",
    "maintainer": "Ali Arminian <abeyran@gmail.com>",
    "author": "Ali Arminian [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4749-6085>)",
    "url": "https://github.com/abeyran/Path.Analysis",
    "bug_reports": "https://github.com/abeyran/Path.Analysis/issues",
    "repository": "https://cran.r-project.org/package=Path.Analysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Path.Analysis Path Coefficient Analysis Facilitates the performance of several analyses, including simple and sequential path coefficient analysis, correlation estimate, drawing correlogram, Heatmap, and path diagram. When working with raw data, that includes one or more dependent variables along with one or more independent variables are available, the path coefficient analysis can be conducted. It allows for testing direct effects, which can be a vital indicator in path coefficient analysis. The process of preparing the dataset rule is explained in detail in the vignette file \"Path.Analysis_manual.Rmd\". You can find this in the folders labelled \"data\" and \"~/inst/extdata\". Also see: 1)the 'lavaan', 2)a sample of sequential path analysis in 'metan' suggested by Olivoto and L\u00facio (2020) <doi:10.1111/2041-210X.13384>, 3)the simple 'PATHSAS' macro written in 'SAS' by Cramer et al. (1999) <doi:10.1093/jhered/90.1.260>, and 4)the semPlot() function of 'OpenMx' as initial tools for conducting path coefficient analyses and SEM (Structural Equation Modeling). To gain a comprehensive understanding of path coefficient analysis, both in theory and practice, see a 'Minitab' macro developed by Arminian, A. in the paper by Arminian et al. (2008) <doi:10.1080/15427520802043182>.  "
  },
  {
    "id": 5916,
    "package_name": "PredPsych",
    "title": "Predictive Approaches in Psychology",
    "description": "\n    Recent years have seen an increased interest in novel methods\n    for analyzing quantitative data from experimental psychology. Currently, however, they lack an\n    established and accessible software framework. Many existing implementations provide no guidelines,\n    consisting of small code snippets, or sets of packages. In addition, the use of existing packages\n    often requires advanced programming experience. 'PredPsych' is a user-friendly toolbox based on\n    machine learning predictive algorithms. It comprises of multiple functionalities for multivariate\n    analyses of quantitative behavioral data based on machine learning models.",
    "version": "0.5",
    "maintainer": "Atesh Koul <atesh.koul@gmail.com>",
    "author": "Atesh Koul [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PredPsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PredPsych Predictive Approaches in Psychology \n    Recent years have seen an increased interest in novel methods\n    for analyzing quantitative data from experimental psychology. Currently, however, they lack an\n    established and accessible software framework. Many existing implementations provide no guidelines,\n    consisting of small code snippets, or sets of packages. In addition, the use of existing packages\n    often requires advanced programming experience. 'PredPsych' is a user-friendly toolbox based on\n    machine learning predictive algorithms. It comprises of multiple functionalities for multivariate\n    analyses of quantitative behavioral data based on machine learning models.  "
  },
  {
    "id": 5939,
    "package_name": "ProbitSpatial",
    "title": "Probit with Spatial Dependence, SAR, SEM and SARAR Models",
    "description": "Fast estimation of binomial spatial probit regression models with spatial autocorrelation for big datasets.",
    "version": "1.1",
    "maintainer": "Davide Martinetti <davide.martinetti@inrae.fr>",
    "author": "Davide Martinetti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2047-1793>),\n  Ghislain Geniaux [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ProbitSpatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProbitSpatial Probit with Spatial Dependence, SAR, SEM and SARAR Models Fast estimation of binomial spatial probit regression models with spatial autocorrelation for big datasets.  "
  },
  {
    "id": 5960,
    "package_name": "PsychWordVec",
    "title": "Word Embedding Research Framework for Psychological Science",
    "description": "\n    An integrative toolbox of word embedding research that provides:\n    (1) a collection of 'pre-trained' static word vectors in the '.RData'\n    compressed format <https://psychbruce.github.io/WordVector_RData.pdf>;\n    (2) a group of functions to process, analyze, and visualize word vectors;\n    (3) a range of tests to examine conceptual associations, including\n    the Word Embedding Association Test <doi:10.1126/science.aal4230>\n    and the Relative Norm Distance <doi:10.1073/pnas.1720347115>,\n    with permutation test of significance; and\n    (4) a set of training methods to locally train (static) word vectors\n    from text corpora, including 'Word2Vec' <doi:10.48550/arXiv.1301.3781>,\n    'GloVe' <doi:10.3115/v1/D14-1162>, and 'FastText' <doi:10.48550/arXiv.1607.04606>.",
    "version": "2025.11",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/PsychWordVec/",
    "bug_reports": "https://github.com/psychbruce/PsychWordVec/issues",
    "repository": "https://cran.r-project.org/package=PsychWordVec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PsychWordVec Word Embedding Research Framework for Psychological Science \n    An integrative toolbox of word embedding research that provides:\n    (1) a collection of 'pre-trained' static word vectors in the '.RData'\n    compressed format <https://psychbruce.github.io/WordVector_RData.pdf>;\n    (2) a group of functions to process, analyze, and visualize word vectors;\n    (3) a range of tests to examine conceptual associations, including\n    the Word Embedding Association Test <doi:10.1126/science.aal4230>\n    and the Relative Norm Distance <doi:10.1073/pnas.1720347115>,\n    with permutation test of significance; and\n    (4) a set of training methods to locally train (static) word vectors\n    from text corpora, including 'Word2Vec' <doi:10.48550/arXiv.1301.3781>,\n    'GloVe' <doi:10.3115/v1/D14-1162>, and 'FastText' <doi:10.48550/arXiv.1607.04606>.  "
  },
  {
    "id": 6037,
    "package_name": "QuantPsyc",
    "title": "Quantitative Psychology Tools",
    "description": "Contains tools useful for data screening, testing\n        moderation (Cohen et. al. 2003)<doi:10.4324/9780203774441>, \n        mediation (MacKinnon et. al. 2002)<doi:10.1037/1082-989x.7.1.83> \n        and estimating power (Murphy & Myors 2014)<ISBN:9781315773155>.",
    "version": "1.6",
    "maintainer": "Thomas D. Fletcher <t.d.fletcher05@gmail.com>",
    "author": "Thomas D. Fletcher",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QuantPsyc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QuantPsyc Quantitative Psychology Tools Contains tools useful for data screening, testing\n        moderation (Cohen et. al. 2003)<doi:10.4324/9780203774441>, \n        mediation (MacKinnon et. al. 2002)<doi:10.1037/1082-989x.7.1.83> \n        and estimating power (Murphy & Myors 2014)<ISBN:9781315773155>.  "
  },
  {
    "id": 6047,
    "package_name": "Qval",
    "title": "The Q-Matrix Validation Methods Framework",
    "description": "Provide a variety of Q-matrix validation methods for the generalized\n    cognitive diagnosis models, including the method based on the generalized\n    deterministic input, noisy, and gate model (G-DINA) by de la Torre (2011)\n    <DOI:10.1007/s11336-011-9207-7> discrimination index (the GDI method) by\n    de la Torre and Chiu (2016) <DOI:10.1007/s11336-015-9467-8>, the Hull method\n    by Najera et al. (2021) <DOI:10.1111/bmsp.12228>, the stepwise Wald test method\n    (the Wald method) by Ma and de la Torre (2020) <DOI:10.1111/bmsp.12156>, the\n    multiple logistic regression\u2011based Q\u2011matrix validation method (the MLR-B method)\n    by Tu et al. (2022) <DOI:10.3758/s13428-022-01880-x>, the beta method based on\n    signal detection theory by Li and Chen (2024) <DOI:10.1111/bmsp.12371> and\n    Q-matrix validation based on relative fit index by Chen et al. (2013)\n    <DOI:10.1111/j.1745-3984.2012.00185.x>. Different research methods and iterative\n    procedures during Q-matrix validating are available\n    <DOI:10.3758/s13428-024-02547-5>.",
    "version": "1.2.4",
    "maintainer": "Haijiang Qin <haijiang133@outlook.com>",
    "author": "Haijiang Qin [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-6721-5653>),\n  Lei Guo [aut, cph] (ORCID: <https://orcid.org/0000-0002-8273-3587>)",
    "url": "https://haijiangqin.com/Qval/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Qval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Qval The Q-Matrix Validation Methods Framework Provide a variety of Q-matrix validation methods for the generalized\n    cognitive diagnosis models, including the method based on the generalized\n    deterministic input, noisy, and gate model (G-DINA) by de la Torre (2011)\n    <DOI:10.1007/s11336-011-9207-7> discrimination index (the GDI method) by\n    de la Torre and Chiu (2016) <DOI:10.1007/s11336-015-9467-8>, the Hull method\n    by Najera et al. (2021) <DOI:10.1111/bmsp.12228>, the stepwise Wald test method\n    (the Wald method) by Ma and de la Torre (2020) <DOI:10.1111/bmsp.12156>, the\n    multiple logistic regression\u2011based Q\u2011matrix validation method (the MLR-B method)\n    by Tu et al. (2022) <DOI:10.3758/s13428-022-01880-x>, the beta method based on\n    signal detection theory by Li and Chen (2024) <DOI:10.1111/bmsp.12371> and\n    Q-matrix validation based on relative fit index by Chen et al. (2013)\n    <DOI:10.1111/j.1745-3984.2012.00185.x>. Different research methods and iterative\n    procedures during Q-matrix validating are available\n    <DOI:10.3758/s13428-024-02547-5>.  "
  },
  {
    "id": 6089,
    "package_name": "RALSA",
    "title": "R Analyzer for Large-Scale Assessments",
    "description": "\n    Download, prepare and analyze data from large-scale assessments and\n    surveys with complex sampling and assessment design (see 'Rutkowski',\n    2010 <doi:10.3102/0013189X10363170>). Such studies are, for example,\n    international assessments like 'TIMSS', 'PIRLS' and 'PISA'. A graphical\n    interface is available for the non-technical user.The package includes\n    functions to covert the original data from 'SPSS' into 'R' data sets\n    keeping the user-defined missing values, merge data from different\n    respondents and/or countries, generate variable dictionaries, modify\n    data, produce descriptive statistics (percentages, means, percentiles,\n    benchmarks) and multivariate statistics (correlations, linear\n    regression, binary logistic regression). The number of supported\n    studies and analysis types will increase in future. For a general\n    presentation of the package, see 'Mirazchiyski', 2021a\n    (<doi:10.1186/s40536-021-00114-4>). For detailed technical aspects of\n    the package, see 'Mirazchiyski', 2021b (<doi:10.3390/psych3020018>).",
    "version": "1.6.0",
    "maintainer": "Plamen V. Mirazchiyski <plamen.mirazchiyski@ineri.org>",
    "author": "Plamen V. Mirazchiyski [aut, cre],\n  INERI [aut]",
    "url": "https://ralsa.ineri.org/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RALSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RALSA R Analyzer for Large-Scale Assessments \n    Download, prepare and analyze data from large-scale assessments and\n    surveys with complex sampling and assessment design (see 'Rutkowski',\n    2010 <doi:10.3102/0013189X10363170>). Such studies are, for example,\n    international assessments like 'TIMSS', 'PIRLS' and 'PISA'. A graphical\n    interface is available for the non-technical user.The package includes\n    functions to covert the original data from 'SPSS' into 'R' data sets\n    keeping the user-defined missing values, merge data from different\n    respondents and/or countries, generate variable dictionaries, modify\n    data, produce descriptive statistics (percentages, means, percentiles,\n    benchmarks) and multivariate statistics (correlations, linear\n    regression, binary logistic regression). The number of supported\n    studies and analysis types will increase in future. For a general\n    presentation of the package, see 'Mirazchiyski', 2021a\n    (<doi:10.1186/s40536-021-00114-4>). For detailed technical aspects of\n    the package, see 'Mirazchiyski', 2021b (<doi:10.3390/psych3020018>).  "
  },
  {
    "id": 6091,
    "package_name": "RAMpath",
    "title": "Structural Equation Modeling Using the Reticular Action Model\n(RAM) Notation",
    "description": "We rewrite of RAMpath software developed by John McArdle and Steven Boker as an R package. In addition to performing regular SEM analysis through the R package lavaan, RAMpath has unique features.  First, it can generate path diagrams according to a given model. Second, it can display path tracing rules through path diagrams and decompose total effects into their respective direct and indirect effects as well as decompose variance and covariance into individual bridges. Furthermore, RAMpath can fit dynamic system models automatically based on latent change scores and generate vector field plots based upon results obtained from a bivariate dynamic system. Starting version 0.4, RAMpath can conduct power analysis for both univariate and bivariate latent change score models.",
    "version": "0.5.1",
    "maintainer": "Zhiyong Zhang <zzhang4@nd.edu>",
    "author": "Zhiyong Zhang, Jack McArdle, Aki Hamagami, & Kevin Grimm",
    "url": "https://nd.psychstat.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RAMpath",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAMpath Structural Equation Modeling Using the Reticular Action Model\n(RAM) Notation We rewrite of RAMpath software developed by John McArdle and Steven Boker as an R package. In addition to performing regular SEM analysis through the R package lavaan, RAMpath has unique features.  First, it can generate path diagrams according to a given model. Second, it can display path tracing rules through path diagrams and decompose total effects into their respective direct and indirect effects as well as decompose variance and covariance into individual bridges. Furthermore, RAMpath can fit dynamic system models automatically based on latent change scores and generate vector field plots based upon results obtained from a bivariate dynamic system. Starting version 0.4, RAMpath can conduct power analysis for both univariate and bivariate latent change score models.  "
  },
  {
    "id": 6276,
    "package_name": "RMCLab",
    "title": "Lab for Matrix Completion and Imputation of Discrete Rating Data",
    "description": "Collection of methods for rating matrix completion, which is a statistical framework for recommender systems. Another relevant application is the imputation of rating-scale survey data in the social and behavioral sciences. Note that matrix completion and imputation are synonymous terms used in different streams of the literature. The main functionality implements robust matrix completion for discrete rating-scale data with a low-rank constraint on a latent continuous matrix (Archimbaud, Alfons, and Wilms (2025) <doi:10.48550/arXiv.2412.20802>). In addition, the package provides wrapper functions for 'softImpute' (Mazumder, Hastie, and Tibshirani, 2010, <https://www.jmlr.org/papers/v11/mazumder10a.html>; Hastie, Mazumder, Lee, Zadeh, 2015, <https://www.jmlr.org/papers/v16/hastie15a.html>) for easy tuning of the regularization parameter, as well as benchmark methods such as median imputation and mode imputation.",
    "version": "0.1.0",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "author": "Andreas Alfons [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2513-3788>),\n  Aurore Archimbaud [aut] (ORCID:\n    <https://orcid.org/0000-0002-6511-9091>)",
    "url": "https://github.com/aalfons/RMCLab",
    "bug_reports": "https://github.com/aalfons/RMCLab/issues",
    "repository": "https://cran.r-project.org/package=RMCLab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMCLab Lab for Matrix Completion and Imputation of Discrete Rating Data Collection of methods for rating matrix completion, which is a statistical framework for recommender systems. Another relevant application is the imputation of rating-scale survey data in the social and behavioral sciences. Note that matrix completion and imputation are synonymous terms used in different streams of the literature. The main functionality implements robust matrix completion for discrete rating-scale data with a low-rank constraint on a latent continuous matrix (Archimbaud, Alfons, and Wilms (2025) <doi:10.48550/arXiv.2412.20802>). In addition, the package provides wrapper functions for 'softImpute' (Mazumder, Hastie, and Tibshirani, 2010, <https://www.jmlr.org/papers/v11/mazumder10a.html>; Hastie, Mazumder, Lee, Zadeh, 2015, <https://www.jmlr.org/papers/v16/hastie15a.html>) for easy tuning of the regularization parameter, as well as benchmark methods such as median imputation and mode imputation.  "
  },
  {
    "id": 6293,
    "package_name": "RMX",
    "title": "Rasch Models -- eXtensions",
    "description": "\n Extend Rasch and Item Response Theory (IRT) analyses by providing  \n tools for post-processing the output from five major IRT packages  \n (i.e., 'eRm', 'psychotools', 'ltm', 'mirt', and 'TAM').            \n The current version provides the plotPIccc() function, which       \n extracts from the return object of the originating package all     \n information required to draw an extended Person-Item-Map (PIccc),  \n showing any combination of                                         \n * category characteristic curves (CCCs),                           \n * threshold characteristic curves (TCCs),                          \n * item characteristic curves (ICCs),                               \n * category information functions (CIFs),                           \n * item information functions (IIFs),                               \n * test information function (TIF), and the                         \n * standard error curve (S.E.).                                     \n for uni- and multidimensional models (as far as supported by each  \n package). It allows for selecting dimensions, items, and categories\n to plot and offers numerous options to adapt the output. The return\n object contains all calculated values for further processing.      ",
    "version": "0.1-6",
    "maintainer": "Rainer W. Alexandrowicz <rainer.alexandrowicz@aau.at>",
    "author": "Milica Kabic [aut],\n  Rainer W. Alexandrowicz [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RMX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMX Rasch Models -- eXtensions \n Extend Rasch and Item Response Theory (IRT) analyses by providing  \n tools for post-processing the output from five major IRT packages  \n (i.e., 'eRm', 'psychotools', 'ltm', 'mirt', and 'TAM').            \n The current version provides the plotPIccc() function, which       \n extracts from the return object of the originating package all     \n information required to draw an extended Person-Item-Map (PIccc),  \n showing any combination of                                         \n * category characteristic curves (CCCs),                           \n * threshold characteristic curves (TCCs),                          \n * item characteristic curves (ICCs),                               \n * category information functions (CIFs),                           \n * item information functions (IIFs),                               \n * test information function (TIF), and the                         \n * standard error curve (S.E.).                                     \n for uni- and multidimensional models (as far as supported by each  \n package). It allows for selecting dimensions, items, and categories\n to plot and offers numerous options to adapt the output. The return\n object contains all calculated values for further processing.        "
  },
  {
    "id": 6298,
    "package_name": "RMediation",
    "title": "Mediation Analysis Confidence Intervals",
    "description": "Computes confidence intervals for nonlinear functions of model \n    parameters (e.g., product of k coefficients) in single-level and multilevel \n    structural equation models. Methods include the distribution of the product, \n    Monte Carlo simulation, and bootstrap methods. It also performs the Model-Based \n    Constrained Optimization (MBCO) procedure for hypothesis testing of indirect \n    effects.\n    References:\n    Tofighi, D., and MacKinnon, D. P. (2011). RMediation: An R package for mediation \n    analysis confidence intervals. Behavior Research Methods, 43, 692-700. \n    <doi:10.3758/s13428-011-0076-x>;\n    Tofighi, D., and Kelley, K. (2020). Improved inference in mediation analysis: Introducing the model-based constrained optimization procedure. \n    Psychological Methods, 25(4), 496-515. <doi:10.1037/met0000259>;\n    Tofighi, D. (2020). Bootstrap Model-Based Constrained Optimization Tests of \n    Indirect Effects. Frontiers in Psychology, 10, 2989. \n    <doi:10.3389/fpsyg.2019.02989>.",
    "version": "1.3.0",
    "maintainer": "Davood Tofighi <dtofighi@gmail.com>",
    "author": "Davood Tofighi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8523-7776>)",
    "url": "https://data-wise.github.io/rmediation/,\nhttps://github.com/data-wise/rmediation",
    "bug_reports": "https://github.com/data-wise/rmediation/issues",
    "repository": "https://cran.r-project.org/package=RMediation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMediation Mediation Analysis Confidence Intervals Computes confidence intervals for nonlinear functions of model \n    parameters (e.g., product of k coefficients) in single-level and multilevel \n    structural equation models. Methods include the distribution of the product, \n    Monte Carlo simulation, and bootstrap methods. It also performs the Model-Based \n    Constrained Optimization (MBCO) procedure for hypothesis testing of indirect \n    effects.\n    References:\n    Tofighi, D., and MacKinnon, D. P. (2011). RMediation: An R package for mediation \n    analysis confidence intervals. Behavior Research Methods, 43, 692-700. \n    <doi:10.3758/s13428-011-0076-x>;\n    Tofighi, D., and Kelley, K. (2020). Improved inference in mediation analysis: Introducing the model-based constrained optimization procedure. \n    Psychological Methods, 25(4), 496-515. <doi:10.1037/met0000259>;\n    Tofighi, D. (2020). Bootstrap Model-Based Constrained Optimization Tests of \n    Indirect Effects. Frontiers in Psychology, 10, 2989. \n    <doi:10.3389/fpsyg.2019.02989>.  "
  },
  {
    "id": 6300,
    "package_name": "RMixtComp",
    "title": "Mixture Models with Heterogeneous and (Partially) Missing Data",
    "description": "Mixture Composer (Biernacki (2015) <https://inria.hal.science/hal-01253393v1>) is a project to perform clustering using mixture models with\n    heterogeneous data and partially missing data. Mixture models are fitted using a SEM algorithm.\n    It includes 8 models for real, categorical, counting, functional and ranking data.",
    "version": "4.1.5",
    "maintainer": "Quentin Grimonprez <quentingrim@yahoo.fr>",
    "author": "Vincent Kubicki [aut],\n  Christophe Biernacki [aut],\n  Quentin Grimonprez [aut, cre],\n  Matthieu Marbac-Lourdelle [ctb],\n  \u00c9tienne Goffinet [ctb],\n  Serge Iovleff [ctb],\n  Julien Vandaele [ctb]",
    "url": "https://github.com/modal-inria/MixtComp",
    "bug_reports": "https://github.com/modal-inria/MixtComp/issues",
    "repository": "https://cran.r-project.org/package=RMixtComp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMixtComp Mixture Models with Heterogeneous and (Partially) Missing Data Mixture Composer (Biernacki (2015) <https://inria.hal.science/hal-01253393v1>) is a project to perform clustering using mixture models with\n    heterogeneous data and partially missing data. Mixture models are fitted using a SEM algorithm.\n    It includes 8 models for real, categorical, counting, functional and ranking data.  "
  },
  {
    "id": 6952,
    "package_name": "SEMsensitivity",
    "title": "SEM Sensitivity Analysis",
    "description": "Performs sensitivity analysis for Structural Equation Modeling (SEM). It determines which sample points need to be removed for the sign of a specific path in the SEM model to change, thus assessing the robustness of the model. Methodological manuscript in preparation.",
    "version": "0.1.0",
    "maintainer": "Biying Zhou <biying.zhou@psu.edu>",
    "author": "Biying Zhou [aut, cre] (ORCID: <https://orcid.org/0000-0002-3590-3408>),\n  Feng Ji [aut] (ORCID: <https://orcid.org/0000-0002-2051-5453>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SEMsensitivity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SEMsensitivity SEM Sensitivity Analysis Performs sensitivity analysis for Structural Equation Modeling (SEM). It determines which sample points need to be removed for the sign of a specific path in the SEM model to change, thus assessing the robustness of the model. Methodological manuscript in preparation.  "
  },
  {
    "id": 6976,
    "package_name": "SIAmodules",
    "title": "Modules for 'ShinyItemAnalysis'",
    "description": "Package including additional modules for interactive\n    'ShinyItemAnalysis' application for the psychometric analysis of\n    educational tests, psychological assessments, health-related and other\n    types of multi-item measurements, or ratings from multiple raters.",
    "version": "0.1.2",
    "maintainer": "Patricia Martinkova <martinkova@cs.cas.cz>",
    "author": "Patricia Martinkova [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4754-8543>),\n  Jan Netik [aut] (ORCID: <https://orcid.org/0000-0002-3888-3203>),\n  Adela Hladka [aut] (ORCID: <https://orcid.org/0000-0002-9112-1208>)",
    "url": "https://www.ShinyItemAnalysis.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SIAmodules",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIAmodules Modules for 'ShinyItemAnalysis' Package including additional modules for interactive\n    'ShinyItemAnalysis' application for the psychometric analysis of\n    educational tests, psychological assessments, health-related and other\n    types of multi-item measurements, or ratings from multiple raters.  "
  },
  {
    "id": 6998,
    "package_name": "SIRE",
    "title": "Finding Feedback Effects in SEM and Testing for Their\nSignificance",
    "description": "Provides two main functionalities.\n    1 - Given a system of simultaneous equation,\n    it decomposes  the matrix of coefficients weighting the endogenous variables \n    into three submatrices: one includes the subset of coefficients that have a causal nature\n    in the model, two include the subset of coefficients that have a interdependent nature\n    in the model, either at systematic level or induced by the correlation between error terms.\n    2 - Given a decomposed model,\n    it tests for the significance of the interdependent relationships acting in the system, \n    via Maximum likelihood and Wald test, which can be built starting from the function output.\n    For theoretical reference see Faliva (1992) <doi:10.1007/BF02589085> and \n    Faliva and Zoia (1994) <doi:10.1007/BF02589041>.",
    "version": "1.1.0",
    "maintainer": "Gianmarco Vacca <gianmarco.vacca@unicatt.it>",
    "author": "Gianmarco Vacca [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SIRE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIRE Finding Feedback Effects in SEM and Testing for Their\nSignificance Provides two main functionalities.\n    1 - Given a system of simultaneous equation,\n    it decomposes  the matrix of coefficients weighting the endogenous variables \n    into three submatrices: one includes the subset of coefficients that have a causal nature\n    in the model, two include the subset of coefficients that have a interdependent nature\n    in the model, either at systematic level or induced by the correlation between error terms.\n    2 - Given a decomposed model,\n    it tests for the significance of the interdependent relationships acting in the system, \n    via Maximum likelihood and Wald test, which can be built starting from the function output.\n    For theoretical reference see Faliva (1992) <doi:10.1007/BF02589085> and \n    Faliva and Zoia (1994) <doi:10.1007/BF02589041>.  "
  },
  {
    "id": 7136,
    "package_name": "SSVS",
    "title": "Functions for Stochastic Search Variable Selection (SSVS)",
    "description": "Functions for performing stochastic search variable selection (SSVS) \n    for binary and continuous outcomes and visualizing the results. \n    SSVS is a Bayesian variable selection method used to estimate the probability \n    that individual predictors should be included in a regression model. \n    Using MCMC estimation, the method samples thousands of regression models \n    in order to characterize the model uncertainty regarding both the predictor \n    set and the regression parameters. For details see Bainter, McCauley, Wager, \n    and Losin (2020) Improving practices for selecting a subset of important \n    predictors in psychology: An application to predicting pain, Advances in \n    Methods and Practices in Psychological Science 3(1), 66-80 \n    <DOI:10.1177/2515245919885617>.",
    "version": "2.1.0",
    "maintainer": "Sierra Bainter <sbainter@miami.edu>",
    "author": "Sierra Bainter [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-7461-0803>),\n  Thomas McCauley [aut],\n  Mahmoud Fahmy [aut],\n  Dean Attali [aut] (ORCID: <https://orcid.org/0000-0002-5645-3493>)",
    "url": "https://github.com/sabainter/SSVS",
    "bug_reports": "https://github.com/sabainter/SSVS/issues",
    "repository": "https://cran.r-project.org/package=SSVS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSVS Functions for Stochastic Search Variable Selection (SSVS) Functions for performing stochastic search variable selection (SSVS) \n    for binary and continuous outcomes and visualizing the results. \n    SSVS is a Bayesian variable selection method used to estimate the probability \n    that individual predictors should be included in a regression model. \n    Using MCMC estimation, the method samples thousands of regression models \n    in order to characterize the model uncertainty regarding both the predictor \n    set and the regression parameters. For details see Bainter, McCauley, Wager, \n    and Losin (2020) Improving practices for selecting a subset of important \n    predictors in psychology: An application to predicting pain, Advances in \n    Methods and Practices in Psychological Science 3(1), 66-80 \n    <DOI:10.1177/2515245919885617>.  "
  },
  {
    "id": 7216,
    "package_name": "SemNeT",
    "title": "Methods and Measures for Semantic Network Analysis",
    "description": "Implements several functions for the analysis of semantic networks including different network estimation algorithms, partial node bootstrapping (Kenett, Anaki, & Faust, 2014 <doi:10.3389/fnhum.2014.00407>), random walk simulation (Kenett & Austerweil, 2016 <http://alab.psych.wisc.edu/papers/files/Kenett16CreativityRW.pdf>), and a function to compute global network measures. Significance tests and plotting features are also implemented. ",
    "version": "1.4.5",
    "maintainer": "Alexander P. Christensen <alexpaulchristensen@gmail.com>",
    "author": "Alexander P. Christensen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9798-7037>),\n  Yoed N. Kenett [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-3872-7689>)",
    "url": "https://github.com/AlexChristensen/SemNeT",
    "bug_reports": "https://github.com/AlexChristensen/SemNeT/issues",
    "repository": "https://cran.r-project.org/package=SemNeT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SemNeT Methods and Measures for Semantic Network Analysis Implements several functions for the analysis of semantic networks including different network estimation algorithms, partial node bootstrapping (Kenett, Anaki, & Faust, 2014 <doi:10.3389/fnhum.2014.00407>), random walk simulation (Kenett & Austerweil, 2016 <http://alab.psych.wisc.edu/papers/files/Kenett16CreativityRW.pdf>), and a function to compute global network measures. Significance tests and plotting features are also implemented.   "
  },
  {
    "id": 7260,
    "package_name": "ShinyItemAnalysis",
    "title": "Test and Item Analysis via Shiny",
    "description": "Package including functions and interactive shiny application\n    for the psychometric analysis of educational tests, psychological\n    assessments, health-related and other types of multi-item\n    measurements, or ratings from multiple raters.",
    "version": "1.5.5",
    "maintainer": "Patricia Martinkova <martinkova@cs.cas.cz>",
    "author": "Patricia Martinkova [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4754-8543>),\n  Adela Hladka [aut] (ORCID: <https://orcid.org/0000-0002-9112-1208>),\n  Jan Netik [aut] (ORCID: <https://orcid.org/0000-0002-3888-3203>),\n  Ondrej Leder [ctb],\n  Jakub Houdek [ctb],\n  Lubomir Stepanek [ctb],\n  Tomas Jurica [ctb],\n  Jana Vorlickova [ctb]",
    "url": "https://shinyitemanalysis.org/",
    "bug_reports": "https://github.com/patriciamar/ShinyItemAnalysis/issues",
    "repository": "https://cran.r-project.org/package=ShinyItemAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShinyItemAnalysis Test and Item Analysis via Shiny Package including functions and interactive shiny application\n    for the psychometric analysis of educational tests, psychological\n    assessments, health-related and other types of multi-item\n    measurements, or ratings from multiple raters.  "
  },
  {
    "id": 7594,
    "package_name": "TDCM",
    "title": "The Transition Diagnostic Classification Model Framework",
    "description": "Estimate the transition diagnostic classification model (TDCM) \n    described in Madison & Bradshaw (2018) <doi:10.1007/s11336-018-9638-5>, a \n    longitudinal extension of the log-linear cognitive diagnosis model (LCDM) in \n    Henson, Templin & Willse (2009) <doi:10.1007/s11336-008-9089-5>. As the LCDM \n    subsumes many other diagnostic classification models (DCMs), many other DCMs \n    can be estimated longitudinally via the TDCM. The 'TDCM' package includes \n    functions to estimate the single-group and multigroup TDCM, summarize \n    results of interest including item parameters, growth proportions, \n    transition probabilities, transitional reliability, attribute correlations, \n    model fit, and growth plots.",
    "version": "0.1.0",
    "maintainer": "Michael E. Cotterell <mepcott@uga.edu>",
    "author": "Matthew J. Madison [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-2944-7442>),\n  Sergio Haab [aut],\n  Minjeong Jeon [aut, cph],\n  Michael E. Cotterell [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4302-0891>),\n  University of Georgia [cph] (The content and opinions expressed in this\n    material do not necessarily reflect the views of nor are they\n    endorsed by the University of Georgia or the University System of\n    Georgia.),\n  Institute of Education Sciences [fnd] (This work is supported by the\n    U.S. Department of Education Institute of Education Sciences under\n    IES Award Number R305D220020.),\n  National Science Foundation [fnd] (This work is supported by the\n    National Science Foundation under NSF Award Number 1921373.)",
    "url": "https://github.com/cotterell/tdcm",
    "bug_reports": "https://github.com/cotterell/tdcm/issues",
    "repository": "https://cran.r-project.org/package=TDCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TDCM The Transition Diagnostic Classification Model Framework Estimate the transition diagnostic classification model (TDCM) \n    described in Madison & Bradshaw (2018) <doi:10.1007/s11336-018-9638-5>, a \n    longitudinal extension of the log-linear cognitive diagnosis model (LCDM) in \n    Henson, Templin & Willse (2009) <doi:10.1007/s11336-008-9089-5>. As the LCDM \n    subsumes many other diagnostic classification models (DCMs), many other DCMs \n    can be estimated longitudinally via the TDCM. The 'TDCM' package includes \n    functions to estimate the single-group and multigroup TDCM, summarize \n    results of interest including item parameters, growth proportions, \n    transition probabilities, transitional reliability, attribute correlations, \n    model fit, and growth plots.  "
  },
  {
    "id": 7739,
    "package_name": "TestGardener",
    "title": "Information Analysis for Test and Rating Scale Data",
    "description": "Develop, evaluate, and score multiple choice examinations, \n psychological scales, questionnaires, and similar types of data involving\n sequences of choices among one or more sets of answers.\n This version of the package should be considered as brand new.  Almost all\n of the functions have been changed, including their argument list.\n See the file NEWS.Rd in the Inst folder for more information.\n Using the package does not require any formal statistical knowledge \n beyond what would be provided by a first course in statistics in a \n social science department.  There the user would encounter the concept \n of probability and how it is used to model data and make decisions, \n and would become familiar with basic mathematical and statistical notation.\n Most of the output is in graphical form. ",
    "version": "3.3.6",
    "maintainer": "James Ramsay <james.ramsay@mcgill.ca>",
    "author": "James Ramsay [aut, cre],\n  Juan Li [ctb],\n  Marie Wiberg [ctb],\n  Joakim Wallmark [ctb],\n  Spencer Graves [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TestGardener",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestGardener Information Analysis for Test and Rating Scale Data Develop, evaluate, and score multiple choice examinations, \n psychological scales, questionnaires, and similar types of data involving\n sequences of choices among one or more sets of answers.\n This version of the package should be considered as brand new.  Almost all\n of the functions have been changed, including their argument list.\n See the file NEWS.Rd in the Inst folder for more information.\n Using the package does not require any formal statistical knowledge \n beyond what would be provided by a first course in statistics in a \n social science department.  There the user would encounter the concept \n of probability and how it is used to model data and make decisions, \n and would become familiar with basic mathematical and statistical notation.\n Most of the output is in graphical form.   "
  },
  {
    "id": 7763,
    "package_name": "ThurMod",
    "title": "Thurstonian CFA and Thurstonian IRT Modeling",
    "description": "Fit Thurstonian forced-choice models (CFA (simple and factor) and IRT) in R. This package allows for the analysis of item response modeling (IRT) as well as confirmatory factor analysis (CFA) in the Thurstonian framework. Currently, estimation can be performed by 'Mplus' and 'lavaan'. References:\n  Brown & Maydeu-Olivares (2011) <doi:10.1177/0013164410375112>;\n  Jansen, M. T., & Schulze, R. (in review). The Thurstonian linked block design: Improving Thurstonian modeling for paired comparison and ranking data.;\n  Maydeu-Olivares & B\u00f6ckenholt (2005) <doi:10.1037/1082-989X.10.3.285>.",
    "version": "1.1.11",
    "maintainer": "Markus Thomas Jansen <mjansen@uni-wuppertal.de>",
    "author": "Markus Thomas Jansen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5162-4409>)",
    "url": "https://github.com/MarkusTJansen/ThurMod",
    "bug_reports": "https://github.com/MarkusTJansen/ThurMod/issues",
    "repository": "https://cran.r-project.org/package=ThurMod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ThurMod Thurstonian CFA and Thurstonian IRT Modeling Fit Thurstonian forced-choice models (CFA (simple and factor) and IRT) in R. This package allows for the analysis of item response modeling (IRT) as well as confirmatory factor analysis (CFA) in the Thurstonian framework. Currently, estimation can be performed by 'Mplus' and 'lavaan'. References:\n  Brown & Maydeu-Olivares (2011) <doi:10.1177/0013164410375112>;\n  Jansen, M. T., & Schulze, R. (in review). The Thurstonian linked block design: Improving Thurstonian modeling for paired comparison and ranking data.;\n  Maydeu-Olivares & B\u00f6ckenholt (2005) <doi:10.1037/1082-989X.10.3.285>.  "
  },
  {
    "id": 7807,
    "package_name": "TreeBUGS",
    "title": "Hierarchical Multinomial Processing Tree Modeling",
    "description": "User-friendly analysis of hierarchical multinomial processing tree (MPT) \n    models that are often used in cognitive psychology. Implements the latent-trait \n    MPT approach (Klauer, 2010) <DOI:10.1007/s11336-009-9141-0> and the beta-MPT \n    approach (Smith & Batchelder, 2010) <DOI:10.1016/j.jmp.2009.06.007> to model \n    heterogeneity of participants. MPT models are conveniently specified by an\n    .eqn-file as used by other MPT software and data are provided by a .csv-file \n    or directly in R. Models are either fitted by calling JAGS or by an MPT-tailored \n    Gibbs sampler in C++ (only for nonhierarchical and beta MPT models). Provides \n    tests of heterogeneity and MPT-tailored summaries and plotting functions.\n    A detailed documentation is available in Heck, Arnold, & Arnold (2018) \n    <DOI:10.3758/s13428-017-0869-7> and a tutorial on MPT modeling can be found \n    in Schmidt, Erdfelder, & Heck (2023) <DOI:10.1037/met0000561>.",
    "version": "1.5.3",
    "maintainer": "Daniel W. Heck <daniel.heck@uni-marburg.de>",
    "author": "Daniel W. Heck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6302-9252>),\n  Nina R. Arnold [aut, dtc],\n  Denis Arnold [aut],\n  Alexander Ly [ctb],\n  Marius Barth [ctb] (ORCID: <https://orcid.org/0000-0002-3421-6665>)",
    "url": "https://github.com/danheck/TreeBUGS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TreeBUGS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TreeBUGS Hierarchical Multinomial Processing Tree Modeling User-friendly analysis of hierarchical multinomial processing tree (MPT) \n    models that are often used in cognitive psychology. Implements the latent-trait \n    MPT approach (Klauer, 2010) <DOI:10.1007/s11336-009-9141-0> and the beta-MPT \n    approach (Smith & Batchelder, 2010) <DOI:10.1016/j.jmp.2009.06.007> to model \n    heterogeneity of participants. MPT models are conveniently specified by an\n    .eqn-file as used by other MPT software and data are provided by a .csv-file \n    or directly in R. Models are either fitted by calling JAGS or by an MPT-tailored \n    Gibbs sampler in C++ (only for nonhierarchical and beta MPT models). Provides \n    tests of heterogeneity and MPT-tailored summaries and plotting functions.\n    A detailed documentation is available in Heck, Arnold, & Arnold (2018) \n    <DOI:10.3758/s13428-017-0869-7> and a tutorial on MPT modeling can be found \n    in Schmidt, Erdfelder, & Heck (2023) <DOI:10.1037/met0000561>.  "
  },
  {
    "id": 7976,
    "package_name": "ViSe",
    "title": "Visualizing Sensitivity",
    "description": "Designed to help the user to determine the sensitivity of an proposed causal effect to unconsidered common causes. Users can create visualizations of sensitivity, effect sizes, and determine which pattern of effects would support a causal claim for between group differences. Number needed to treat formula from Kraemer H.C. & Kupfer D.J. (2006) <doi:10.1016/j.biopsych.2005.09.014>.",
    "version": "0.1.3",
    "maintainer": "Erin M. Buchanan <buchananlab@gmail.com>",
    "author": "Erin M. Buchanan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9689-4189>)",
    "url": "http://www.aggieerin.com/ViSe/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ViSe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ViSe Visualizing Sensitivity Designed to help the user to determine the sensitivity of an proposed causal effect to unconsidered common causes. Users can create visualizations of sensitivity, effect sizes, and determine which pattern of effects would support a causal claim for between group differences. Number needed to treat formula from Kraemer H.C. & Kupfer D.J. (2006) <doi:10.1016/j.biopsych.2005.09.014>.  "
  },
  {
    "id": 7977,
    "package_name": "ViSiElse",
    "title": "A Visual Tool for Behavior Analysis over Time",
    "description": "A graphical R package designed to visualize behavioral observations over time. Based on raw time data extracted from video recorded sessions of experimental observations, ViSiElse grants a global overview of a process by combining the visualization of multiple actions timestamps for all participants in a single graph. Individuals and/or group behavior can easily be assessed. Supplementary features allow users to further inspect their data by adding summary statistics (mean, standard deviation, quantile or statistical test) and/or time constraints to assess the accuracy of the realized actions.",
    "version": "1.2.2",
    "maintainer": "Elodie Garnier <e.garnier30@gmail.com>",
    "author": "Nastasia Fouret [aut, cph],\n  Mederic Descoins [aut, cph],\n  Elodie Garnier [aut, cre, cph],\n  CEPOI - EA 7388 [cph]",
    "url": "https://github.com/Re2SimLab/ViSiElse",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ViSiElse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ViSiElse A Visual Tool for Behavior Analysis over Time A graphical R package designed to visualize behavioral observations over time. Based on raw time data extracted from video recorded sessions of experimental observations, ViSiElse grants a global overview of a process by combining the visualization of multiple actions timestamps for all participants in a single graph. Individuals and/or group behavior can easily be assessed. Supplementary features allow users to further inspect their data by adding summary statistics (mean, standard deviation, quantile or statistical test) and/or time constraints to assess the accuracy of the realized actions.  "
  },
  {
    "id": 8056,
    "package_name": "WebPower",
    "title": "Basic and Advanced Statistical Power Analysis",
    "description": "This is a collection of tools for conducting both basic and advanced statistical power analysis including correlation, proportion, t-test, one-way ANOVA, two-way ANOVA, linear regression, logistic regression, Poisson regression, mediation analysis, longitudinal data analysis, structural equation modeling and multilevel modeling. It also serves as the engine for conducting power analysis online at <https://webpower.psychstat.org>.",
    "version": "0.9.4",
    "maintainer": "Zhiyong Zhang <johnnyzhz@gmail.com>",
    "author": "Zhiyong Zhang [aut, cre],\n  Yujiao Mai [aut],\n  Miao Yang [ctb],\n  Ziqian Xu [ctb],\n  Conor McNamara [ctb]",
    "url": "https://webpower.psychstat.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WebPower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WebPower Basic and Advanced Statistical Power Analysis This is a collection of tools for conducting both basic and advanced statistical power analysis including correlation, proportion, t-test, one-way ANOVA, two-way ANOVA, linear regression, logistic regression, Poisson regression, mediation analysis, longitudinal data analysis, structural equation modeling and multilevel modeling. It also serves as the engine for conducting power analysis online at <https://webpower.psychstat.org>.  "
  },
  {
    "id": 8085,
    "package_name": "WordPools",
    "title": "Word Pools Used in Studies of Learning and Memory",
    "description": "Collects several classical word pools used most often\n    to provide lists of words in psychological studies of learning and memory. It\n    provides a simple function, 'pickList' for selecting random samples of words\n    within given ranges.",
    "version": "1.2.0",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>),\n  Matthew Dubins [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WordPools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WordPools Word Pools Used in Studies of Learning and Memory Collects several classical word pools used most often\n    to provide lists of words in psychological studies of learning and memory. It\n    provides a simple function, 'pickList' for selecting random samples of words\n    within given ranges.  "
  },
  {
    "id": 8092,
    "package_name": "WrightMap",
    "title": "IRT Item-Person Map with 'ConQuest' Integration",
    "description": "A powerful yet simple graphical tool available in the field of psychometrics is the Wright Map (also known as item maps or item-person maps), which presents the location of both respondents and items on the same scale. Wright Maps are commonly used to present the results of dichotomous or polytomous item response models. The 'WrightMap' package provides functions to create these plots from item parameters and person estimates stored as R objects. Although the package can be used in conjunction with any software used to estimate the IRT model (e.g. 'TAM', 'mirt', 'eRm' or 'IRToys' in 'R', or 'Stata', 'Mplus', etc.),  'WrightMap' features special integration with 'ConQuest' to facilitate reading and plotting its output directly.The 'wrightMap' function creates Wright Maps based on person estimates and item parameters produced by an item response analysis. The 'CQmodel' function reads output files created using 'ConQuest' software and creates a set of data frames for easy data manipulation, bundled in a 'CQmodel' object. The 'wrightMap' function can take a 'CQmodel' object as input or it can be used to create Wright Maps directly from data frames of person and item parameters.",
    "version": "1.4",
    "maintainer": "David Torres Irribarra <david@torresirribarra.me>",
    "author": "David Torres Irribarra [aut, cre],\n  Rebecca Freund [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WrightMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WrightMap IRT Item-Person Map with 'ConQuest' Integration A powerful yet simple graphical tool available in the field of psychometrics is the Wright Map (also known as item maps or item-person maps), which presents the location of both respondents and items on the same scale. Wright Maps are commonly used to present the results of dichotomous or polytomous item response models. The 'WrightMap' package provides functions to create these plots from item parameters and person estimates stored as R objects. Although the package can be used in conjunction with any software used to estimate the IRT model (e.g. 'TAM', 'mirt', 'eRm' or 'IRToys' in 'R', or 'Stata', 'Mplus', etc.),  'WrightMap' features special integration with 'ConQuest' to facilitate reading and plotting its output directly.The 'wrightMap' function creates Wright Maps based on person estimates and item parameters produced by an item response analysis. The 'CQmodel' function reads output files created using 'ConQuest' software and creates a set of data frames for easy data manipulation, bundled in a 'CQmodel' object. The 'wrightMap' function can take a 'CQmodel' object as input or it can be used to create Wright Maps directly from data frames of person and item parameters.  "
  },
  {
    "id": 8110,
    "package_name": "YEAB",
    "title": "Analyze Data from Analysis of Behavior Experiments",
    "description": "Analyze data from behavioral experiments conducted using 'MED-PC' software developed by Med Associates Inc. \n\tIncludes functions to fit exponential and hyperbolic models for delay discounting tasks, exponential mixtures for \n\tinter-response times, and Gaussian plus ramp models for peak procedure data, among others. For more details, refer to Alcala et al. (2023) <doi:10.31234/osf.io/8aq2j>.",
    "version": "1.0.6",
    "maintainer": "Emmanuel Alcala <jealcalat@gmail.com>",
    "author": "Emmanuel Alcala [aut, cre],\n  Rodrigo Sosa [aut],\n  Victor Reyes [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=YEAB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "YEAB Analyze Data from Analysis of Behavior Experiments Analyze data from behavioral experiments conducted using 'MED-PC' software developed by Med Associates Inc. \n\tIncludes functions to fit exponential and hyperbolic models for delay discounting tasks, exponential mixtures for \n\tinter-response times, and Gaussian plus ramp models for peak procedure data, among others. For more details, refer to Alcala et al. (2023) <doi:10.31234/osf.io/8aq2j>.  "
  },
  {
    "id": 8504,
    "package_name": "animalEKF",
    "title": "Extended Kalman Filters for Animal Movement",
    "description": "Synthetic generation of 1-D and 2-D correlated random walks (CRWs) for animal movement with behavioral switching, and particle filter estimation of movement parameters from observed trajectories using Extended Kalman Filter (EKF) model. See Ackerman (2018) <https://digital.library.temple.edu/digital/collection/p245801coll10/id/499150>.",
    "version": "1.3",
    "maintainer": "Samuel Ackerman <smackrmn@gmail.com>",
    "author": "Samuel Ackerman [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=animalEKF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "animalEKF Extended Kalman Filters for Animal Movement Synthetic generation of 1-D and 2-D correlated random walks (CRWs) for animal movement with behavioral switching, and particle filter estimation of movement parameters from observed trajectories using Extended Kalman Filter (EKF) model. See Ackerman (2018) <https://digital.library.temple.edu/digital/collection/p245801coll10/id/499150>.  "
  },
  {
    "id": 8539,
    "package_name": "apa",
    "title": "Format Outputs of Statistical Tests According to APA Guidelines",
    "description": "Formatter functions in the 'apa' package take the return value of a\n    statistical test function, e.g. a call to chisq.test() and return a string\n    formatted according to the guidelines of the APA (American Psychological\n    Association).",
    "version": "0.3.4",
    "maintainer": "Daniel Gromer <dgromer@mailbox.org>",
    "author": "Daniel Gromer [aut, cre]",
    "url": "https://github.com/dgromer/apa",
    "bug_reports": "https://github.com/dgromer/apa/issues",
    "repository": "https://cran.r-project.org/package=apa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apa Format Outputs of Statistical Tests According to APA Guidelines Formatter functions in the 'apa' package take the return value of a\n    statistical test function, e.g. a call to chisq.test() and return a string\n    formatted according to the guidelines of the APA (American Psychological\n    Association).  "
  },
  {
    "id": 8540,
    "package_name": "apa7",
    "title": "Facilitate Writing Documents in American Psychological\nAssociation Style, Seventh Edition",
    "description": "Create American Psychological Association Style, Seventh\n    Edition documents. Format numbers and text consistent with APA style.\n    Create tables that comply with APA style by extending flextable\n    functions.",
    "version": "0.1.0",
    "maintainer": "W. Joel Schneider <w.joel.schneider@gmail.com>",
    "author": "W. Joel Schneider [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8393-5316>)",
    "url": "https://github.com/wjschne/apa7, https://wjschne.github.io/apa7/",
    "bug_reports": "https://github.com/wjschne/apa7/issues",
    "repository": "https://cran.r-project.org/package=apa7",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apa7 Facilitate Writing Documents in American Psychological\nAssociation Style, Seventh Edition Create American Psychological Association Style, Seventh\n    Edition documents. Format numbers and text consistent with APA style.\n    Create tables that comply with APA style by extending flextable\n    functions.  "
  },
  {
    "id": 8541,
    "package_name": "apaTables",
    "title": "Create American Psychological Association (APA) Style Tables",
    "description": "A common task faced by researchers is the creation of APA style\n    (i.e., American Psychological Association style) tables from statistical\n    output. In R a large number of function calls are often needed to obtain all of\n    the desired information for a single APA style table. As well, the process of\n    manually creating APA style tables in a word processor is prone to transcription\n    errors. This package creates Word files (.doc files) containing APA style tables\n    for several types of analyses. Using this package minimizes transcription errors\n    and reduces the number commands needed by the user.",
    "version": "2.0.8",
    "maintainer": "David Stanley <dstanley@uoguelph.ca>",
    "author": "David Stanley [aut, cre]",
    "url": "https://github.com/dstanley4/apaTables",
    "bug_reports": "https://github.com/dstanley4/apaTables/issues",
    "repository": "https://cran.r-project.org/package=apaTables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apaTables Create American Psychological Association (APA) Style Tables A common task faced by researchers is the creation of APA style\n    (i.e., American Psychological Association style) tables from statistical\n    output. In R a large number of function calls are often needed to obtain all of\n    the desired information for a single APA style table. As well, the process of\n    manually creating APA style tables in a word processor is prone to transcription\n    errors. This package creates Word files (.doc files) containing APA style tables\n    for several types of analyses. Using this package minimizes transcription errors\n    and reduces the number commands needed by the user.  "
  },
  {
    "id": 8542,
    "package_name": "apaText",
    "title": "Create R Markdown Text for Results in the Style of the American\nPsychological Association (APA)",
    "description": "Create APA style text from analyses for use within R Markdown documents. Descriptive statistics, confidence intervals, and cell sizes are reported.",
    "version": "0.1.7",
    "maintainer": "David Stanley <dstanley@uoguelph.ca>",
    "author": "David Stanley [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=apaText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apaText Create R Markdown Text for Results in the Style of the American\nPsychological Association (APA) Create APA style text from analyses for use within R Markdown documents. Descriptive statistics, confidence intervals, and cell sizes are reported.  "
  },
  {
    "id": 8544,
    "package_name": "apathe",
    "title": "American Psychological Association Thesis Templates for R\nMarkdown",
    "description": "Facilitates writing computationally reproducible student theses in PDF format that conform to the American Psychological Association (APA) manuscript guidelines (6th Edition). The package currently provides two R Markdown templates for homework and theses at the Psychology Department of the University of Cologne. The package builds on the package 'papaja' but is tailored to the requirements of student theses and omits features for simplicity.",
    "version": "0.1.0",
    "maintainer": "Frederik Aust <frederik.aust@uni-koeln.de>",
    "author": "Frederik Aust [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4900-788X>)",
    "url": "https://github.com/crsh/apathe",
    "bug_reports": "https://github.com/crsh/apathe/issues",
    "repository": "https://cran.r-project.org/package=apathe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apathe American Psychological Association Thesis Templates for R\nMarkdown Facilitates writing computationally reproducible student theses in PDF format that conform to the American Psychological Association (APA) manuscript guidelines (6th Edition). The package currently provides two R Markdown templates for homework and theses at the Psychology Department of the University of Cologne. The package builds on the package 'papaja' but is tailored to the requirements of student theses and omits features for simplicity.  "
  },
  {
    "id": 8708,
    "package_name": "ata",
    "title": "Automated Test Assembly",
    "description": "Provides a collection of psychometric methods to process item metadata\n and use target assessment and measurement blueprint constraints to assemble a test form. Currently two automatic\n test assembly (ata) approaches are enabled. For example, the weighted (positive) deviations method, wdm(), proposed\n by Swanson and Stocking (1993) <doi:10.1177/014662169301700205> was implemented in its full specification allowing\n for both item selection as well as test form refinement. The linear constraint programming approach, atalp(), uses the \n linear equation solver by Berkelaar et. al (2014) <http://lpsolve.sourceforge.net/5.5/>\n to enable a variety of approaches to select items.",
    "version": "1.1.1",
    "maintainer": "Michael Chajewski <mchajewski@hotmail.com>",
    "author": "Gulsah Gurkan [aut],\n  Michael Chajewski [aut, cre],\n  Sam Buttrey [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ata Automated Test Assembly Provides a collection of psychometric methods to process item metadata\n and use target assessment and measurement blueprint constraints to assemble a test form. Currently two automatic\n test assembly (ata) approaches are enabled. For example, the weighted (positive) deviations method, wdm(), proposed\n by Swanson and Stocking (1993) <doi:10.1177/014662169301700205> was implemented in its full specification allowing\n for both item selection as well as test form refinement. The linear constraint programming approach, atalp(), uses the \n linear equation solver by Berkelaar et. al (2014) <http://lpsolve.sourceforge.net/5.5/>\n to enable a variety of approaches to select items.  "
  },
  {
    "id": 8910,
    "package_name": "bayes4psy",
    "title": "User Friendly Bayesian Data Analysis for Psychology",
    "description": "Contains several Bayesian models for data analysis of psychological tests. A user friendly interface for these models should enable students and researchers to perform professional level Bayesian data analysis without advanced knowledge in programming and Bayesian statistics. This package is based on the Stan platform (Carpenter et el. 2017 <doi:10.18637/jss.v076.i01>).",
    "version": "1.2.13",
    "maintainer": "Jure Dem\u0161ar <jure.demsar@fri.uni-lj.si>",
    "author": "Jure Dem\u0161ar [cre, aut],\n  Grega Repov\u0161 [aut],\n  Erik \u0160trumbelj [aut],\n  Trustees of Columbia University [cph],\n  John Kruschke [cph] (R/shared_functions.R - mcmc_hdi,\n    src/stan_files/ttest.stan),\n  Rasmus Baath [cph] (R/b_bootstrap.R)",
    "url": "https://github.com/bstatcomp/bayes4psy",
    "bug_reports": "https://github.com/bstatcomp/bayes4psy/issues",
    "repository": "https://cran.r-project.org/package=bayes4psy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayes4psy User Friendly Bayesian Data Analysis for Psychology Contains several Bayesian models for data analysis of psychological tests. A user friendly interface for these models should enable students and researchers to perform professional level Bayesian data analysis without advanced knowledge in programming and Bayesian statistics. This package is based on the Stan platform (Carpenter et el. 2017 <doi:10.18637/jss.v076.i01>).  "
  },
  {
    "id": 8987,
    "package_name": "bcpa",
    "title": "Behavioral Change Point Analysis of Animal Movement",
    "description": "The Behavioral Change Point Analysis (BCPA) is a method of\n    identifying hidden shifts in the underlying parameters of a time series,\n    developed specifically to be applied to animal movement data which is\n    irregularly sampled.  The method is based on: E. Gurarie, R. Andrews and \n    K. Laidre A novel method for identifying behavioural changes in animal \n    movement data (2009) Ecology Letters 12:5 395-408. A development version is \n    on <https://github.com/EliGurarie/bcpa>. NOTE: the BCPA method may be useful \n    for any univariate, irregularly sampled Gaussian time-series, but animal \n    movement analysts are encouraged to apply correlated velocity change point \n    analysis as implemented in the smoove package, as of this writing on GitHub \n    at <https://github.com/EliGurarie/smoove>. An example of a univariate analysis\n    is provided in the UnivariateBCPA vignette. ",
    "version": "1.3.2",
    "maintainer": "Eliezer Gurarie <egurarie@esf.edu>",
    "author": "Eliezer Gurarie <egurarie@esf.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bcpa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bcpa Behavioral Change Point Analysis of Animal Movement The Behavioral Change Point Analysis (BCPA) is a method of\n    identifying hidden shifts in the underlying parameters of a time series,\n    developed specifically to be applied to animal movement data which is\n    irregularly sampled.  The method is based on: E. Gurarie, R. Andrews and \n    K. Laidre A novel method for identifying behavioural changes in animal \n    movement data (2009) Ecology Letters 12:5 395-408. A development version is \n    on <https://github.com/EliGurarie/bcpa>. NOTE: the BCPA method may be useful \n    for any univariate, irregularly sampled Gaussian time-series, but animal \n    movement analysts are encouraged to apply correlated velocity change point \n    analysis as implemented in the smoove package, as of this writing on GitHub \n    at <https://github.com/EliGurarie/smoove>. An example of a univariate analysis\n    is provided in the UnivariateBCPA vignette.   "
  },
  {
    "id": 9024,
    "package_name": "beezdemand",
    "title": "Behavioral Economic Easy Demand",
    "description": "Facilitates many of the analyses performed in studies of\n    behavioral economic demand. The package supports commonly-used options for\n\t\tmodeling operant demand including (1) data screening proposed by Stein,\n\t\tKoffarnus, Snider, Quisenberry, & Bickel (2015; <doi:10.1037/pha0000020>),\n\t\t(2) fitting models of demand such as linear (Hursh, Raslear, Bauman,\n\t\t& Black, 1989, <doi:10.1007/978-94-009-2470-3_22>), exponential\t(Hursh & Silberberg, 2008,\n\t\t<doi:10.1037/0033-295X.115.1.186>) and modified exponential (Koffarnus,\n\t\tFranck, Stein, & Bickel, 2015, <doi:10.1037/pha0000045>), and (3) calculating\n\t\tnumerous measures\trelevant to applied behavioral economists (Intensity,\n\t\tPmax, Omax). Also\tsupports plotting and comparing data.",
    "version": "0.1.2",
    "maintainer": "Brent Kaplan <bkaplan.ku@gmail.com>",
    "author": "Brent Kaplan [aut, cre, cph],\n  Shawn Gilroy [ctb]",
    "url": "https://github.com/brentkaplan/beezdemand",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=beezdemand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "beezdemand Behavioral Economic Easy Demand Facilitates many of the analyses performed in studies of\n    behavioral economic demand. The package supports commonly-used options for\n\t\tmodeling operant demand including (1) data screening proposed by Stein,\n\t\tKoffarnus, Snider, Quisenberry, & Bickel (2015; <doi:10.1037/pha0000020>),\n\t\t(2) fitting models of demand such as linear (Hursh, Raslear, Bauman,\n\t\t& Black, 1989, <doi:10.1007/978-94-009-2470-3_22>), exponential\t(Hursh & Silberberg, 2008,\n\t\t<doi:10.1037/0033-295X.115.1.186>) and modified exponential (Koffarnus,\n\t\tFranck, Stein, & Bickel, 2015, <doi:10.1037/pha0000045>), and (3) calculating\n\t\tnumerous measures\trelevant to applied behavioral economists (Intensity,\n\t\tPmax, Omax). Also\tsupports plotting and comparing data.  "
  },
  {
    "id": 9025,
    "package_name": "beezdiscounting",
    "title": "Behavioral Economic Easy Discounting",
    "description": "Facilitates some of the analyses performed in studies of\n    behavioral economic discounting. The package supports scoring of the 27-Item Monetary Choice\n    Questionnaire (see Kaplan et al., 2016; <doi:10.1007/s40614-016-0070-9>), calculating k\n    values (Mazur's simple hyperbolic and exponential) using nonlinear regression, calculating\n    various Area Under the Curve (AUC) measures, plotting regression curves for both fit-to-group and\n    two-stage approaches, checking for unsystematic discounting\n    (Johnson & Bickel, 2008; <doi:10.1037/1064-1297.16.3.264>) and scoring of the\n    minute discounting task (see Koffarnus & Bickel, 2014; <doi:10.1037/a0035973>) using the\n    Qualtrics 5-trial discounting template (see the Qualtrics Minute Discounting User Guide;\n    <doi:10.13140/RG.2.2.26495.79527>), which is also available as a .qsf file in this package.",
    "version": "0.3.2",
    "maintainer": "Brent A. Kaplan <bkaplan.ku@gmail.com>",
    "author": "Brent A. Kaplan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3758-6776>)",
    "url": "https://github.com/brentkaplan/beezdiscounting",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=beezdiscounting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "beezdiscounting Behavioral Economic Easy Discounting Facilitates some of the analyses performed in studies of\n    behavioral economic discounting. The package supports scoring of the 27-Item Monetary Choice\n    Questionnaire (see Kaplan et al., 2016; <doi:10.1007/s40614-016-0070-9>), calculating k\n    values (Mazur's simple hyperbolic and exponential) using nonlinear regression, calculating\n    various Area Under the Curve (AUC) measures, plotting regression curves for both fit-to-group and\n    two-stage approaches, checking for unsystematic discounting\n    (Johnson & Bickel, 2008; <doi:10.1037/1064-1297.16.3.264>) and scoring of the\n    minute discounting task (see Koffarnus & Bickel, 2014; <doi:10.1037/a0035973>) using the\n    Qualtrics 5-trial discounting template (see the Qualtrics Minute Discounting User Guide;\n    <doi:10.13140/RG.2.2.26495.79527>), which is also available as a .qsf file in this package.  "
  },
  {
    "id": 9027,
    "package_name": "behaviorchange",
    "title": "Tools for Behavior Change Researchers and Professionals",
    "description": "Contains specialised analyses and\n    visualisation tools for behavior change science.\n    These facilitate conducting determinant studies\n    (for example, using confidence interval-based\n    estimation of relevance, CIBER, or CIBERlite\n    plots, see Crutzen, Noijen & Peters (2017)\n    <doi:10/ghtfz9>),\n    systematically developing, reporting,\n    and analysing interventions (for example, using\n    Acyclic Behavior Change Diagrams), and reporting\n    about intervention effectiveness (for example, using\n    the Numbers Needed for Change, see Gruijters & Peters\n    (2017) <doi:10/jzkt>), and computing the\n    required sample size (using the Meaningful Change\n    Definition, see Gruijters & Peters (2020)\n    <doi:10/ghpnx8>).\n    This package is especially useful for\n    researchers in the field of behavior change or\n    health psychology and to behavior change\n    professionals such as intervention developers and\n    prevention workers.",
    "version": "25.8.0",
    "maintainer": "Gjalt-Jorn Peters <behaviorchange@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Rik Crutzen [ctb] (ORCID: <https://orcid.org/0000-0002-3731-6610>),\n  Jeroen Bruinsma [ctb] (ORCID: <https://orcid.org/0000-0002-7964-0267>),\n  Stefan Gruijters [ctb] (ORCID: <https://orcid.org/0000-0003-0141-0071>)",
    "url": "https://behaviorchange.opens.science",
    "bug_reports": "https://codeberg.org/R-packages/behaviorchange/issues",
    "repository": "https://cran.r-project.org/package=behaviorchange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "behaviorchange Tools for Behavior Change Researchers and Professionals Contains specialised analyses and\n    visualisation tools for behavior change science.\n    These facilitate conducting determinant studies\n    (for example, using confidence interval-based\n    estimation of relevance, CIBER, or CIBERlite\n    plots, see Crutzen, Noijen & Peters (2017)\n    <doi:10/ghtfz9>),\n    systematically developing, reporting,\n    and analysing interventions (for example, using\n    Acyclic Behavior Change Diagrams), and reporting\n    about intervention effectiveness (for example, using\n    the Numbers Needed for Change, see Gruijters & Peters\n    (2017) <doi:10/jzkt>), and computing the\n    required sample size (using the Meaningful Change\n    Definition, see Gruijters & Peters (2020)\n    <doi:10/ghpnx8>).\n    This package is especially useful for\n    researchers in the field of behavior change or\n    health psychology and to behavior change\n    professionals such as intervention developers and\n    prevention workers.  "
  },
  {
    "id": 9055,
    "package_name": "betafunctions",
    "title": "Functions for Working with Two- And Four-Parameter Beta\nProbability Distributions and Psychometric Analysis of\nClassifications",
    "description": "Package providing a number of functions for working with Two- and \n    Four-parameter Beta and closely related distributions (i.e., the Gamma-\n    Binomial-, and Beta-Binomial distributions).\n        Includes, among other things: \n    - d/p/q/r functions for Four-Parameter Beta distributions and Generalized\n    \"Binomial\" (continuous) distributions, and d/p/r- functions for Beta-\n    Binomial distributions.\n    - d/p/q/r functions for Two- and Four-Parameter Beta distributions\n    parameterized in terms of their means and variances rather than their\n    shape-parameters.\n    - Moment generating functions for Binomial distributions, Beta-Binomial \n    distributions, and observed value distributions.\n    - Functions for estimating classification accuracy and consistency, \n    making use of the Classical Test-Theory based 'Livingston and Lewis' (L&L) \n    and 'Hanson and Brennan' approaches.\n      A shiny app is available, providing a GUI for the L&L approach when used \n    for binary classifications. For url to the app, see documentation for the \n    LL.CA() function.\n    Livingston and Lewis (1995) <doi:10.1111/j.1745-3984.1995.tb00462.x>.\n    Lord (1965) <doi:10.1007/BF02289490>.\n    Hanson (1991) <https://files.eric.ed.gov/fulltext/ED344945.pdf>.",
    "version": "1.9.0",
    "maintainer": "Haakon Eidem Haakstad <h.e.haakstad@gmail.com>",
    "author": "Haakon Eidem Haakstad",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=betafunctions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "betafunctions Functions for Working with Two- And Four-Parameter Beta\nProbability Distributions and Psychometric Analysis of\nClassifications Package providing a number of functions for working with Two- and \n    Four-parameter Beta and closely related distributions (i.e., the Gamma-\n    Binomial-, and Beta-Binomial distributions).\n        Includes, among other things: \n    - d/p/q/r functions for Four-Parameter Beta distributions and Generalized\n    \"Binomial\" (continuous) distributions, and d/p/r- functions for Beta-\n    Binomial distributions.\n    - d/p/q/r functions for Two- and Four-Parameter Beta distributions\n    parameterized in terms of their means and variances rather than their\n    shape-parameters.\n    - Moment generating functions for Binomial distributions, Beta-Binomial \n    distributions, and observed value distributions.\n    - Functions for estimating classification accuracy and consistency, \n    making use of the Classical Test-Theory based 'Livingston and Lewis' (L&L) \n    and 'Hanson and Brennan' approaches.\n      A shiny app is available, providing a GUI for the L&L approach when used \n    for binary classifications. For url to the app, see documentation for the \n    LL.CA() function.\n    Livingston and Lewis (1995) <doi:10.1111/j.1745-3984.1995.tb00462.x>.\n    Lord (1965) <doi:10.1007/BF02289490>.\n    Hanson (1991) <https://files.eric.ed.gov/fulltext/ED344945.pdf>.  "
  },
  {
    "id": 9098,
    "package_name": "bidux",
    "title": "Behavioral Insight Design: A Toolkit for Integrating Behavioral\nScience in UI/UX Design",
    "description": "Provides a framework and toolkit to guide R dashboard developers in\n    implementing the Behavioral Insight Design (BID) framework. The package offers\n    functions for documenting each of the five stages (Interpret, Notice,\n    Anticipate, Structure, and Validate), along with a comprehensive concept\n    dictionary. Works with both 'shiny' applications and 'Quarto' dashboards.",
    "version": "0.3.3",
    "maintainer": "Jeremy Winget <contact@jrwinget.com>",
    "author": "Jeremy Winget [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3783-4354>)",
    "url": "https://jrwinget.github.io/bidux/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bidux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bidux Behavioral Insight Design: A Toolkit for Integrating Behavioral\nScience in UI/UX Design Provides a framework and toolkit to guide R dashboard developers in\n    implementing the Behavioral Insight Design (BID) framework. The package offers\n    functions for documenting each of the five stages (Interpret, Notice,\n    Anticipate, Structure, and Validate), along with a comprehensive concept\n    dictionary. Works with both 'shiny' applications and 'Quarto' dashboards.  "
  },
  {
    "id": 9150,
    "package_name": "binaryRL",
    "title": "Reinforcement Learning Tools for Two-Alternative Forced Choice\nTasks",
    "description": "Tools for building Rescorla-Wagner Models for Two-Alternative \n  Forced Choice tasks, commonly employed in psychological research. \n  Most concepts and ideas within this R package are referenced from \n  Sutton and Barto (2018) <ISBN:9780262039246>. \n  The package allows for the intuitive definition of RL models using simple \n  if-else statements and three basic models built into this R package are \n  referenced from \n  Niv et al. (2012)<doi:10.1523/JNEUROSCI.5498-10.2012>. \n  Our approach to constructing and evaluating these computational models \n  is informed by the guidelines proposed in \n  Wilson & Collins (2019) <doi:10.7554/eLife.49547>. \n  Example datasets included with the package are sourced from the work of\n  Mason et al. (2024) <doi:10.3758/s13423-023-02415-x>.",
    "version": "0.9.8",
    "maintainer": "YuKi <hmz1969a@gmail.com>",
    "author": "YuKi [aut, cre] (ORCID: <https://orcid.org/0009-0000-1378-1318>)",
    "url": "https://yuki-961004.github.io/binaryRL/",
    "bug_reports": "https://github.com/yuki-961004/binaryRL/issues",
    "repository": "https://cran.r-project.org/package=binaryRL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binaryRL Reinforcement Learning Tools for Two-Alternative Forced Choice\nTasks Tools for building Rescorla-Wagner Models for Two-Alternative \n  Forced Choice tasks, commonly employed in psychological research. \n  Most concepts and ideas within this R package are referenced from \n  Sutton and Barto (2018) <ISBN:9780262039246>. \n  The package allows for the intuitive definition of RL models using simple \n  if-else statements and three basic models built into this R package are \n  referenced from \n  Niv et al. (2012)<doi:10.1523/JNEUROSCI.5498-10.2012>. \n  Our approach to constructing and evaluating these computational models \n  is informed by the guidelines proposed in \n  Wilson & Collins (2019) <doi:10.7554/eLife.49547>. \n  Example datasets included with the package are sourced from the work of\n  Mason et al. (2024) <doi:10.3758/s13423-023-02415-x>.  "
  },
  {
    "id": 9246,
    "package_name": "blatent",
    "title": "Bayesian Latent Variable Models",
    "description": "Estimation of latent variable models using Bayesian methods. Currently estimates the loglinear cognitive diagnosis model of Henson, Templin, and Willse (2009) <doi:10.1007/s11336-008-9089-5>.",
    "version": "0.1.2",
    "maintainer": "Jonathan Templin <jonathan-templin@uiowa.edu>",
    "author": "Jonathan Templin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7616-0973>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=blatent",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blatent Bayesian Latent Variable Models Estimation of latent variable models using Bayesian methods. Currently estimates the loglinear cognitive diagnosis model of Henson, Templin, and Willse (2009) <doi:10.1007/s11336-008-9089-5>.  "
  },
  {
    "id": 9293,
    "package_name": "bmscstan",
    "title": "Bayesian Multilevel Single Case Models using 'Stan'",
    "description": "Analyse single case analyses against a control group.\n    Its purpose is to provide a flexible, with good power and\n    low first type error\n    approach that can manage at the same time controls' and patient's data.\n    The use of Bayesian statistics allows to test both the alternative and\n    null hypothesis.\n    Scandola, M., & Romano, D. (2020, August 3). <doi:10.31234/osf.io/sajdq>\n    Scandola, M., & Romano, D. (2021). <doi:10.1016/j.neuropsychologia.2021.107834>.",
    "version": "1.2.1.0",
    "maintainer": "Michele Scandola <michele.scandola@univr.it>",
    "author": "Michele Scandola [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0853-8975>)",
    "url": "https://github.com/michelescandola/bmscstan",
    "bug_reports": "https://github.com/michelescandola/bmscstan",
    "repository": "https://cran.r-project.org/package=bmscstan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bmscstan Bayesian Multilevel Single Case Models using 'Stan' Analyse single case analyses against a control group.\n    Its purpose is to provide a flexible, with good power and\n    low first type error\n    approach that can manage at the same time controls' and patient's data.\n    The use of Bayesian statistics allows to test both the alternative and\n    null hypothesis.\n    Scandola, M., & Romano, D. (2020, August 3). <doi:10.31234/osf.io/sajdq>\n    Scandola, M., & Romano, D. (2021). <doi:10.1016/j.neuropsychologia.2021.107834>.  "
  },
  {
    "id": 9304,
    "package_name": "bnpa",
    "title": "Bayesian Networks & Path Analysis",
    "description": "This project aims to enable the method of Path Analysis to infer causalities \n             from data. For this we propose a hybrid approach, which uses Bayesian network \n             structure learning algorithms from data to create the input file for creation of a \n             PA model. The process is performed in a semi-automatic way by our intermediate \n             algorithm, allowing novice researchers to create and evaluate their own PA models\n             from a data set. The references used for this project are: \n             Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. <doi:10.1017/S0269888910000275>. \n             Nagarajan, R., Scutari, M., & L\u00e8bre, S. (2013). Bayesian networks in r. Springer, 122, 125-127. Scutari, M., & Denis, J. B. <doi:10.1007/978-1-4614-6446-4>.\n             Scutari M (2010). Bayesian networks: with examples in R. Chapman and Hall/CRC. <doi:10.1201/b17065>. \n             Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1 - 36. <doi:10.18637/jss.v048.i02>.",
    "version": "0.3.0",
    "maintainer": "Elias Carvalho <ecacarva@gmail.com>",
    "author": "Elias Carvalho, Joao R N Vissoci, Luciano Andrade, Wagner Machado, Emerson P Cabrera, Julio C Nievola",
    "url": "https://sites.google.com/site/bnparp/.",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bnpa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bnpa Bayesian Networks & Path Analysis This project aims to enable the method of Path Analysis to infer causalities \n             from data. For this we propose a hybrid approach, which uses Bayesian network \n             structure learning algorithms from data to create the input file for creation of a \n             PA model. The process is performed in a semi-automatic way by our intermediate \n             algorithm, allowing novice researchers to create and evaluate their own PA models\n             from a data set. The references used for this project are: \n             Koller, D., & Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. <doi:10.1017/S0269888910000275>. \n             Nagarajan, R., Scutari, M., & L\u00e8bre, S. (2013). Bayesian networks in r. Springer, 122, 125-127. Scutari, M., & Denis, J. B. <doi:10.1007/978-1-4614-6446-4>.\n             Scutari M (2010). Bayesian networks: with examples in R. Chapman and Hall/CRC. <doi:10.1201/b17065>. \n             Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1 - 36. <doi:10.18637/jss.v048.i02>.  "
  },
  {
    "id": 9341,
    "package_name": "bootStateSpace",
    "title": "Bootstrap for State Space Models",
    "description": "Provides a streamlined and user-friendly framework for\n    bootstrapping in state space models, particularly when the number of\n    subjects/units (n) exceeds one, a scenario commonly encountered in\n    social and behavioral sciences. The parametric bootstrap implemented\n    here was developed and applied in Pesigan, Russell, and Chow (2025)\n    <doi:10.1037/met0000779>.",
    "version": "1.0.3",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "author": "Ivan Jacob Agaloos Pesigan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4818-8420>),\n  Michael A. Russell [ctb] (ORCID:\n    <https://orcid.org/0000-0002-3956-604X>),\n  Sy-Miin Chow [ctb] (ORCID: <https://orcid.org/0000-0003-1938-027X>)",
    "url": "https://github.com/jeksterslab/bootStateSpace,\nhttps://jeksterslab.github.io/bootStateSpace/",
    "bug_reports": "https://github.com/jeksterslab/bootStateSpace/issues",
    "repository": "https://cran.r-project.org/package=bootStateSpace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootStateSpace Bootstrap for State Space Models Provides a streamlined and user-friendly framework for\n    bootstrapping in state space models, particularly when the number of\n    subjects/units (n) exceeds one, a scenario commonly encountered in\n    social and behavioral sciences. The parametric bootstrap implemented\n    here was developed and applied in Pesigan, Russell, and Chow (2025)\n    <doi:10.1037/met0000779>.  "
  },
  {
    "id": 9352,
    "package_name": "bor",
    "title": "Transforming Behavioral Observation Records into Data Matrices",
    "description": "Transforms focal observations' data, where different types of social interactions\n  can be recorded by multiple observers, into asymmetric data matrices.\n  Each cell in these matrices provides counts on the number of times\n  a specific type of social interaction was initiated by the row subject and\n  directed to the column subject.",
    "version": "0.1.0",
    "maintainer": "David N Sousa <davidnsousa@gmail.com>",
    "author": "David N Sousa [aut, cre],\n  Joao R Daniel [aut] (ORCID: <https://orcid.org/0000-0001-6609-2014>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bor Transforming Behavioral Observation Records into Data Matrices Transforms focal observations' data, where different types of social interactions\n  can be recorded by multiple observers, into asymmetric data matrices.\n  Each cell in these matrices provides counts on the number of times\n  a specific type of social interaction was initiated by the row subject and\n  directed to the column subject.  "
  },
  {
    "id": 9523,
    "package_name": "cIRT",
    "title": "Choice Item Response Theory",
    "description": "Jointly model the accuracy of cognitive responses and item choices\n    within a Bayesian hierarchical framework as described by Culpepper and\n    Balamuta (2015) <doi:10.1007/s11336-015-9484-7>. In addition, the package\n    contains the datasets used within the analysis of the paper.",
    "version": "1.3.3",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "author": "Steven Andrew Culpepper [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4226-6176>),\n  James Joseph Balamuta [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>)",
    "url": "https://tmsalab.github.io/cIRT/, https://github.com/tmsalab/cIRT",
    "bug_reports": "https://github.com/tmsalab/cIRT/issues",
    "repository": "https://cran.r-project.org/package=cIRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cIRT Choice Item Response Theory Jointly model the accuracy of cognitive responses and item choices\n    within a Bayesian hierarchical framework as described by Culpepper and\n    Balamuta (2015) <doi:10.1007/s11336-015-9484-7>. In addition, the package\n    contains the datasets used within the analysis of the paper.  "
  },
  {
    "id": 9524,
    "package_name": "cNORM",
    "title": "Continuous Norming",
    "description": "A comprehensive toolkit for generating continuous test norms in \n     psychometrics and biometrics, and analyzing model fit. The package offers \n     both distribution-free modeling using Taylor polynomials and parametric \n     modeling using the beta-binomial and the 'Sinh-Arcsinh' distribution. \n     Originally developed for achievement tests, it is applicable to a wide \n     range of mental, physical, or other test scores dependent on continuous or \n     discrete explanatory variables. The package provides several advantages: \n     It minimizes deviations from representativeness in subsamples, interpolates \n     between discrete levels of explanatory variables, and significantly reduces \n     the required sample size compared to conventional norming per age group. \n     cNORM enables graphical and analytical evaluation of model fit, \n     accommodates a wide range of scales including those with negative and \n     descending values, and even supports conventional norming. It generates \n     norm tables including confidence intervals. It also includes methods for \n     addressing representativeness issues through Iterative Proportional Fitting. \n     Based on Lenhard et al. (2016) \n    <doi:10.1177/1073191116656437>, Lenhard et al. (2019) \n    <doi:10.1371/journal.pone.0222279>, Lenhard and Lenhard (2021) \n    <doi:10.1177/0013164420928457> and Gary et al. (2023) \n    <doi:10.1007/s00181-023-02456-0>.",
    "version": "3.5.1",
    "maintainer": "Wolfgang Lenhard <wolfgang.lenhard@uni-wuerzburg.de>",
    "author": "Alexandra Lenhard [aut] (ORCID:\n    <https://orcid.org/0000-0001-8680-4381>),\n  Wolfgang Lenhard [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-8184-6889>),\n  Sebastian Gary [aut],\n  WPS Publisher [fnd] (https://www.wpspublish.com/)",
    "url": "https://www.psychometrica.de/cNorm_en.html,\nhttps://github.com/WLenhard/cNORM",
    "bug_reports": "https://github.com/WLenhard/cNORM/issues",
    "repository": "https://cran.r-project.org/package=cNORM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cNORM Continuous Norming A comprehensive toolkit for generating continuous test norms in \n     psychometrics and biometrics, and analyzing model fit. The package offers \n     both distribution-free modeling using Taylor polynomials and parametric \n     modeling using the beta-binomial and the 'Sinh-Arcsinh' distribution. \n     Originally developed for achievement tests, it is applicable to a wide \n     range of mental, physical, or other test scores dependent on continuous or \n     discrete explanatory variables. The package provides several advantages: \n     It minimizes deviations from representativeness in subsamples, interpolates \n     between discrete levels of explanatory variables, and significantly reduces \n     the required sample size compared to conventional norming per age group. \n     cNORM enables graphical and analytical evaluation of model fit, \n     accommodates a wide range of scales including those with negative and \n     descending values, and even supports conventional norming. It generates \n     norm tables including confidence intervals. It also includes methods for \n     addressing representativeness issues through Iterative Proportional Fitting. \n     Based on Lenhard et al. (2016) \n    <doi:10.1177/1073191116656437>, Lenhard et al. (2019) \n    <doi:10.1371/journal.pone.0222279>, Lenhard and Lenhard (2021) \n    <doi:10.1177/0013164420928457> and Gary et al. (2023) \n    <doi:10.1007/s00181-023-02456-0>.  "
  },
  {
    "id": 9563,
    "package_name": "calms",
    "title": "Comprehensive Analysis of Latent Means",
    "description": "Provides a Shiny application to conduct comprehensive analysis of latent means including the examination of group equivalency, propensity score analysis, \n measurement invariance analysis, and assessment of latent mean differences of \n equivalent groups with invariant data. Group equivalency and propensity score analyses are implemented using the 'MatchIt' package [Ho et al. (2011) <doi:10.18637/jss.v042.i08>], \n ensuring robust control for covariates. Structural equation modeling and invariance testing rely heavily on the 'lavaan' package [Rosseel (2012) <doi:10.18637/jss.v048.i02>], \n providing a flexible and powerful modeling framework. The application also integrates modified functions from Hammack-Brown et al. (2021) <doi:10.1002/hrdq.21452> \n to support factor ratio testing and the list-and-delete procedure. ",
    "version": "1.0-1",
    "maintainer": "Kim Nimon <kim.nimon@gmail.com>",
    "author": "Kim Nimon [aut, cre],\n  Julia Fulmore [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=calms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "calms Comprehensive Analysis of Latent Means Provides a Shiny application to conduct comprehensive analysis of latent means including the examination of group equivalency, propensity score analysis, \n measurement invariance analysis, and assessment of latent mean differences of \n equivalent groups with invariant data. Group equivalency and propensity score analyses are implemented using the 'MatchIt' package [Ho et al. (2011) <doi:10.18637/jss.v042.i08>], \n ensuring robust control for covariates. Structural equation modeling and invariance testing rely heavily on the 'lavaan' package [Rosseel (2012) <doi:10.18637/jss.v048.i02>], \n providing a flexible and powerful modeling framework. The application also integrates modified functions from Hammack-Brown et al. (2021) <doi:10.1002/hrdq.21452> \n to support factor ratio testing and the list-and-delete procedure.   "
  },
  {
    "id": 9610,
    "package_name": "careless",
    "title": "Procedures for Computing Indices of Careless Responding",
    "description": "When taking online surveys, participants sometimes respond to items\n    without regard to their content. These types of responses, referred to as \n    careless or insufficient effort responding, constitute significant problems \n    for data quality, leading to distortions in data analysis and hypothesis \n    testing, such as spurious correlations. The 'R' package 'careless' provides \n    solutions designed to detect such careless / insufficient effort responses \n    by allowing easy calculation of indices proposed in the literature. It \n    currently supports the calculation of longstring, even-odd consistency, \n    psychometric synonyms/antonyms, Mahalanobis distance, and intra-individual \n    response variability (also termed inter-item standard deviation). \n    For a review of these methods, see Curran (2016) <doi:10.1016/j.jesp.2015.07.006>.",
    "version": "1.2.2",
    "maintainer": "Richard Yentes <ryentes@gmail.com>",
    "author": "Richard Yentes [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-6767-8065>),\n  Francisco Wilhelm [aut]",
    "url": "https://github.com/ryentes/careless/",
    "bug_reports": "https://github.com/ryentes/careless/issues",
    "repository": "https://cran.r-project.org/package=careless",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "careless Procedures for Computing Indices of Careless Responding When taking online surveys, participants sometimes respond to items\n    without regard to their content. These types of responses, referred to as \n    careless or insufficient effort responding, constitute significant problems \n    for data quality, leading to distortions in data analysis and hypothesis \n    testing, such as spurious correlations. The 'R' package 'careless' provides \n    solutions designed to detect such careless / insufficient effort responses \n    by allowing easy calculation of indices proposed in the literature. It \n    currently supports the calculation of longstring, even-odd consistency, \n    psychometric synonyms/antonyms, Mahalanobis distance, and intra-individual \n    response variability (also termed inter-item standard deviation). \n    For a review of these methods, see Curran (2016) <doi:10.1016/j.jesp.2015.07.006>.  "
  },
  {
    "id": 9654,
    "package_name": "catlearn",
    "title": "Formal Psychological Models of Categorization and Learning",
    "description": "Formal psychological models of categorization and learning, independently-replicated data sets against which to test them, and simulation archives.",
    "version": "1.1",
    "maintainer": "Andy Wills <andy@willslab.co.uk>",
    "author": "Andy Wills [aut, cre],\n  Lenard Dome [aut],\n  Charlotte Edmunds [aut],\n  Garrett Honke [aut],\n  Angus Inkster [aut],\n  Ren\u00e9 Schlegelmilch [aut],\n  Stuart Spicer [aut]",
    "url": "https://github.com/ajwills72/catlearn",
    "bug_reports": "https://github.com/ajwills72/catlearn/issues",
    "repository": "https://cran.r-project.org/package=catlearn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catlearn Formal Psychological Models of Categorization and Learning Formal psychological models of categorization and learning, independently-replicated data sets against which to test them, and simulation archives.  "
  },
  {
    "id": 9660,
    "package_name": "catseyes",
    "title": "Create Catseye Plots Illustrating the Normal Distribution of the\nMeans",
    "description": "Provides the tools to produce catseye plots, principally\n    by catseyesplot() function which calls R's standard plot() function internally, or alternatively\n    by the catseyes() function to overlay the catseye plot onto an existing\n    R plot window. Catseye plots illustrate the normal distribution of the mean (picture a \n    normal bell curve reflected over its base and rotated 90 degrees), with a shaded confidence\n    interval; they are an intuitive way of illustrating and comparing normally distributed estimates,\n    and are arguably a superior alternative to standard confidence intervals, since they show the full\n    distribution rather than fixed quantile bounds. The catseyesplot and catseyes functions require\n    pre-calculated means and standard errors (or standard deviations), provided as numeric vectors;\n    this allows the flexibility of obtaining this information from a variety of sources, such as\n    direct calculation or prediction from a model.  Catseye plots, as illustrations of the\n    normal distribution of the means, are described in Cumming (2013 & 2014).\n    Cumming, G. (2013). The new statistics: Why and how. Psychological Science, 27, 7-29. <doi:10.1177/0956797613504966> pmid:24220629.",
    "version": "0.2.5",
    "maintainer": "Clark Andersen <crandersen@mdanderson.org>",
    "author": "Clark Andersen [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catseyes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catseyes Create Catseye Plots Illustrating the Normal Distribution of the\nMeans Provides the tools to produce catseye plots, principally\n    by catseyesplot() function which calls R's standard plot() function internally, or alternatively\n    by the catseyes() function to overlay the catseye plot onto an existing\n    R plot window. Catseye plots illustrate the normal distribution of the mean (picture a \n    normal bell curve reflected over its base and rotated 90 degrees), with a shaded confidence\n    interval; they are an intuitive way of illustrating and comparing normally distributed estimates,\n    and are arguably a superior alternative to standard confidence intervals, since they show the full\n    distribution rather than fixed quantile bounds. The catseyesplot and catseyes functions require\n    pre-calculated means and standard errors (or standard deviations), provided as numeric vectors;\n    this allows the flexibility of obtaining this information from a variety of sources, such as\n    direct calculation or prediction from a model.  Catseye plots, as illustrations of the\n    normal distribution of the means, are described in Cumming (2013 & 2014).\n    Cumming, G. (2013). The new statistics: Why and how. Psychological Science, 27, 7-29. <doi:10.1177/0956797613504966> pmid:24220629.  "
  },
  {
    "id": 9717,
    "package_name": "ccpsyc",
    "title": "Methods for Cross-Cultural Psychology",
    "description": "With the development of new cross-cultural methods this package is intended to combine multiple functions automating and simplifying functions providing a unified analysis approach for commonly employed methods. ",
    "version": "0.2.6",
    "maintainer": "Johannes Karl <johannes.a.karl@gmail.com>",
    "author": "Johannes Karl [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ccpsyc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ccpsyc Methods for Cross-Cultural Psychology With the development of new cross-cultural methods this package is intended to combine multiple functions automating and simplifying functions providing a unified analysis approach for commonly employed methods.   "
  },
  {
    "id": 9724,
    "package_name": "cdcatR",
    "title": "Cognitive Diagnostic Computerized Adaptive Testing",
    "description": "Provides a set of functions for conducting cognitive diagnostic computerized adaptive testing applications (Chen, 2009) <DOI:10.1007/s11336-009-9123-2>). It includes different item selection rules such us the global discrimination index (Kaplan, de la Torre, and Barrada (2015) <DOI:10.1177/0146621614554650>) and the nonparametric selection method (Chang, Chiu, and Tsai (2019) <DOI:10.1177/0146621618813113>), as well as several stopping rules. Functions for generating item banks and responses are also provided. To guide item bank calibration, model comparison at the item level can be conducted using the two-step likelihood ratio test statistic by Sorrel, de la Torre, Abad and Olea (2017) <DOI:10.1027/1614-2241/a000131>.",
    "version": "1.0.6",
    "maintainer": "Miguel A. Sorrel <miguel.sorrel@uam.es>",
    "author": "Miguel A. Sorrel [aut, cre, cph],\n  Pablo N\u00e1jera [aut, cph],\n  Francisco J. Abad [aut, cph]",
    "url": "https://github.com/miguel-sorrel/cdcatR",
    "bug_reports": "https://github.com/miguel-sorrel/cdcatR/issues",
    "repository": "https://cran.r-project.org/package=cdcatR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdcatR Cognitive Diagnostic Computerized Adaptive Testing Provides a set of functions for conducting cognitive diagnostic computerized adaptive testing applications (Chen, 2009) <DOI:10.1007/s11336-009-9123-2>). It includes different item selection rules such us the global discrimination index (Kaplan, de la Torre, and Barrada (2015) <DOI:10.1177/0146621614554650>) and the nonparametric selection method (Chang, Chiu, and Tsai (2019) <DOI:10.1177/0146621618813113>), as well as several stopping rules. Functions for generating item banks and responses are also provided. To guide item bank calibration, model comparison at the item level can be conducted using the two-step likelihood ratio test statistic by Sorrel, de la Torre, Abad and Olea (2017) <DOI:10.1027/1614-2241/a000131>.  "
  },
  {
    "id": 9734,
    "package_name": "cdmTools",
    "title": "Useful Tools for Cognitive Diagnosis Modeling",
    "description": "Provides useful tools for cognitive diagnosis modeling (CDM). The package includes functions for empirical Q-matrix estimation and validation, such as the Hull method (N\u00e1jera, Sorrel, de la Torre, & Abad, 2021, <doi:10.1111/bmsp.12228>) and the discrete factor loading method (Wang, Song, & Ding, 2018, <doi:10.1007/978-3-319-77249-3_29>). It also contains dimensionality assessment procedures for CDM, including parallel analysis and automated fit comparison as explored in N\u00e1jera, Abad, and Sorrel (2021, <doi:10.3389/fpsyg.2021.614470>). Other relevant methods and features for CDM applications, such as the restricted DINA model (N\u00e1jera et al., 2023; <doi:10.3102/10769986231158829>), the general nonparametric classification method (Chiu et al., 2018; <doi:10.1007/s11336-017-9595-4>), and corrected estimation of the classification accuracy via multiple imputation (Kreitchmann et al., 2022; <doi:10.3758/s13428-022-01967-5>) are also available. Lastly, the package provides some useful functions for CDM simulation studies, such as random Q-matrix generation and detection of complete/identified Q-matrices.",
    "version": "1.0.6",
    "maintainer": "Pablo N\u00e1jera <p.najeraalvarez@gmail.com>",
    "author": "Pablo N\u00e1jera [aut, cre, cph],\n  Miguel A. Sorrel [aut, cph],\n  Francisco J. Abad [aut, cph],\n  Rodrigo S. Kreitchmann [ctb],\n  Kevin Santos [ctb]",
    "url": "https://github.com/pablo-najera/cdmTools",
    "bug_reports": "https://github.com/pablo-najera/cdmTools/issues",
    "repository": "https://cran.r-project.org/package=cdmTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdmTools Useful Tools for Cognitive Diagnosis Modeling Provides useful tools for cognitive diagnosis modeling (CDM). The package includes functions for empirical Q-matrix estimation and validation, such as the Hull method (N\u00e1jera, Sorrel, de la Torre, & Abad, 2021, <doi:10.1111/bmsp.12228>) and the discrete factor loading method (Wang, Song, & Ding, 2018, <doi:10.1007/978-3-319-77249-3_29>). It also contains dimensionality assessment procedures for CDM, including parallel analysis and automated fit comparison as explored in N\u00e1jera, Abad, and Sorrel (2021, <doi:10.3389/fpsyg.2021.614470>). Other relevant methods and features for CDM applications, such as the restricted DINA model (N\u00e1jera et al., 2023; <doi:10.3102/10769986231158829>), the general nonparametric classification method (Chiu et al., 2018; <doi:10.1007/s11336-017-9595-4>), and corrected estimation of the classification accuracy via multiple imputation (Kreitchmann et al., 2022; <doi:10.3758/s13428-022-01967-5>) are also available. Lastly, the package provides some useful functions for CDM simulation studies, such as random Q-matrix generation and detection of complete/identified Q-matrices.  "
  },
  {
    "id": 9910,
    "package_name": "circhelp",
    "title": "Circular Analyses Helper Functions",
    "description": "Light-weight functions for computing descriptive statistics in different circular spaces (e.g., 2pi, 180, or 360 degrees), to handle angle-dependent biases, pad circular data, and more. Specifically aimed for psychologists and neuroscientists analyzing circular data. Basic methods are based on Jammalamadaka and SenGupta (2001) <doi:10.1142/4031>, removal of cardinal biases is based on the approach introduced in van Bergen, Ma, Pratte, & Jehee (2015) <doi:10.1038/nn.4150> and Chetverikov and Jehee (2023) <doi:10.1038/s41467-023-43251-w>.",
    "version": "1.1",
    "maintainer": "Andrey Chetverikov <andrey.chetverikov@uib.no>",
    "author": "Andrey Chetverikov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2767-6310>),\n  Eline Van Geert [ctb] (ORCID: <https://orcid.org/0000-0002-7848-5998>)",
    "url": "https://achetverikov.github.io/circhelp/index.html,\nhttps://github.com/achetverikov/circhelp",
    "bug_reports": "https://github.com/achetverikov/circhelp/issues",
    "repository": "https://cran.r-project.org/package=circhelp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "circhelp Circular Analyses Helper Functions Light-weight functions for computing descriptive statistics in different circular spaces (e.g., 2pi, 180, or 360 degrees), to handle angle-dependent biases, pad circular data, and more. Specifically aimed for psychologists and neuroscientists analyzing circular data. Basic methods are based on Jammalamadaka and SenGupta (2001) <doi:10.1142/4031>, removal of cardinal biases is based on the approach introduced in van Bergen, Ma, Pratte, & Jehee (2015) <doi:10.1038/nn.4150> and Chetverikov and Jehee (2023) <doi:10.1038/s41467-023-43251-w>.  "
  },
  {
    "id": 10027,
    "package_name": "clust.bin.pair",
    "title": "Statistical Methods for Analyzing Clustered Matched Pair Data",
    "description": "Tests, utilities, and case studies for analyzing significance in\n  clustered binary matched-pair data. The central function clust.bin.pair uses\n  one of several tests to calculate a Chi-square statistic. Implemented are the\n  tests Eliasziw (1991) <doi:10.1002/sim.4780101211>, Obuchowski (1998)\n  <doi:10.1002/(SICI)1097-0258(19980715)17:13%3C1495::AID-SIM863%3E3.0.CO;2-I>,\n  Durkalski (2003) <doi:10.1002/sim.1438>, and Yang (2010)\n  <doi:10.1002/bimj.201000035> with McNemar (1947) <doi:10.1007/BF02295996>\n  included for comparison. The utility functions nested.to.contingency and\n  paired.to.contingency convert data between various useful formats. Thyroids\n  and psychiatry are the canonical datasets from Obuchowski and Petryshen (1989)\n  <doi:10.1016/0165-1781(89)90196-0> respectively.",
    "version": "0.1.2",
    "maintainer": "Dan Gopstein <dan@gopstein.com>",
    "author": "Dan Gopstein [aut, cre]",
    "url": "https://github.com/dgopstein/clust.bin.pair",
    "bug_reports": "https://github.com/dgopstein/clust.bin.pair/issues",
    "repository": "https://cran.r-project.org/package=clust.bin.pair",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clust.bin.pair Statistical Methods for Analyzing Clustered Matched Pair Data Tests, utilities, and case studies for analyzing significance in\n  clustered binary matched-pair data. The central function clust.bin.pair uses\n  one of several tests to calculate a Chi-square statistic. Implemented are the\n  tests Eliasziw (1991) <doi:10.1002/sim.4780101211>, Obuchowski (1998)\n  <doi:10.1002/(SICI)1097-0258(19980715)17:13%3C1495::AID-SIM863%3E3.0.CO;2-I>,\n  Durkalski (2003) <doi:10.1002/sim.1438>, and Yang (2010)\n  <doi:10.1002/bimj.201000035> with McNemar (1947) <doi:10.1007/BF02295996>\n  included for comparison. The utility functions nested.to.contingency and\n  paired.to.contingency convert data between various useful formats. Thyroids\n  and psychiatry are the canonical datasets from Obuchowski and Petryshen (1989)\n  <doi:10.1016/0165-1781(89)90196-0> respectively.  "
  },
  {
    "id": 10142,
    "package_name": "coefa",
    "title": "Meta Analysis of Factor Analysis Based on CO-Occurrence Matrices",
    "description": "Provide a series of functions to conduct a meta analysis of \n    factor analysis based on co-occurrence matrices. The tool can be used to \n    solve the factor structure (i.e. inner structure of a construct, or scale) \n    debate in several disciplines, such as psychology, psychiatry, management, \n    education so on. References: Shafer (2005) <doi:10.1037/1040-3590.17.3.324>; \n    Shafer (2006) <doi:10.1002/jclp.20213>; Loeber and Schmaling (1985) <doi:10.1007/BF00910652>.",
    "version": "1.0.3",
    "maintainer": "Xijian Zheng <psydreammer@foxmail.com>",
    "author": "Xijian Zheng [aut, cre],\n  Huiyong Fan [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=coefa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "coefa Meta Analysis of Factor Analysis Based on CO-Occurrence Matrices Provide a series of functions to conduct a meta analysis of \n    factor analysis based on co-occurrence matrices. The tool can be used to \n    solve the factor structure (i.e. inner structure of a construct, or scale) \n    debate in several disciplines, such as psychology, psychiatry, management, \n    education so on. References: Shafer (2005) <doi:10.1037/1040-3590.17.3.324>; \n    Shafer (2006) <doi:10.1002/jclp.20213>; Loeber and Schmaling (1985) <doi:10.1007/BF00910652>.  "
  },
  {
    "id": 10143,
    "package_name": "coefficientalpha",
    "title": "Robust Coefficient Alpha and Omega with Missing and Non-Normal\nData",
    "description": "Cronbach's alpha and McDonald's omega are widely used reliability or internal consistency measures in social, behavioral and education sciences. Alpha is reported in nearly every study that involves measuring a construct through multiple test items. The package 'coefficientalpha' calculates coefficient alpha and coefficient omega with missing data and non-normal data. Robust standard errors and confidence intervals are also provided. A test is also available to test the tau-equivalent and homogeneous assumptions. Since Version 0.5, the bootstrap confidence intervals were added.",
    "version": "0.7.2",
    "maintainer": "Zhiyong Zhang <johnnyzhz@gmail.com>",
    "author": "Zhiyong Zhang and Ke-Hai Yuan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=coefficientalpha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "coefficientalpha Robust Coefficient Alpha and Omega with Missing and Non-Normal\nData Cronbach's alpha and McDonald's omega are widely used reliability or internal consistency measures in social, behavioral and education sciences. Alpha is reported in nearly every study that involves measuring a construct through multiple test items. The package 'coefficientalpha' calculates coefficient alpha and coefficient omega with missing data and non-normal data. Robust standard errors and confidence intervals are also provided. A test is also available to test the tau-equivalent and homogeneous assumptions. Since Version 0.5, the bootstrap confidence intervals were added.  "
  },
  {
    "id": 10152,
    "package_name": "cogirt",
    "title": "Cognitive Testing Using Item Response Theory",
    "description": "Psychometrically analyze latent individual differences related to tasks, interventions, or maturational/aging effects in the context of experimental or longitudinal cognitive research using methods first described by Thomas et al. (2020) <doi:10.1177/0013164420919898>.",
    "version": "1.0.0",
    "maintainer": "Michael Thomas <michael.l.thomas@colostate.edu>",
    "author": "Michael Thomas [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cogirt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cogirt Cognitive Testing Using Item Response Theory Psychometrically analyze latent individual differences related to tasks, interventions, or maturational/aging effects in the context of experimental or longitudinal cognitive research using methods first described by Thomas et al. (2020) <doi:10.1177/0013164420919898>.  "
  },
  {
    "id": 10155,
    "package_name": "cogmapr",
    "title": "Cognitive Mapping Tools Based on Coding of Textual Sources",
    "description": "Functions for building cognitive maps based on qualitative data. Inputs are textual sources (articles, transcription of qualitative interviews of agents,...). These sources have been coded using relations and are linked to (i) a table describing the variables (or concepts) used for the coding and (ii) a table describing the sources (typology of agents, ...). Main outputs are Individual Cognitive Maps (ICM), Social Cognitive Maps (all sources or group of sources) and a list of quotes linked to relations. This package is linked to the work done during the PhD of Frederic M. Vanwindekens (CRA-W / UCL) hold the 13 of May 2014 at University of Louvain in collaboration with the Walloon Agricultural Research Centre (project MIMOSA, MOERMAN fund).",
    "version": "0.9.3",
    "maintainer": "Fr\u00e9d\u00e9ric M. Vanwindekens <f.vanwindekens@cra.wallonie.be>",
    "author": "Fr\u00e9d\u00e9ric M. Vanwindekens [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9117-7543>),\n  Didier Stilmant [aut, ths],\n  Philippe V. Baret [aut, ths]",
    "url": "https://frdvnw.gitlab.io/cogmapr/",
    "bug_reports": "https://gitlab.com/FrdVnW/cogmapr/-/issues",
    "repository": "https://cran.r-project.org/package=cogmapr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cogmapr Cognitive Mapping Tools Based on Coding of Textual Sources Functions for building cognitive maps based on qualitative data. Inputs are textual sources (articles, transcription of qualitative interviews of agents,...). These sources have been coded using relations and are linked to (i) a table describing the variables (or concepts) used for the coding and (ii) a table describing the sources (typology of agents, ...). Main outputs are Individual Cognitive Maps (ICM), Social Cognitive Maps (all sources or group of sources) and a list of quotes linked to relations. This package is linked to the work done during the PhD of Frederic M. Vanwindekens (CRA-W / UCL) hold the 13 of May 2014 at University of Louvain in collaboration with the Walloon Agricultural Research Centre (project MIMOSA, MOERMAN fund).  "
  },
  {
    "id": 10313,
    "package_name": "confreq",
    "title": "Configural Frequencies Analysis Using Log-Linear Modeling",
    "description": "Offers several functions for Configural Frequencies Analysis (CFA), which is a useful statistical tool for the analysis of multiway contingency tables. CFA was introduced by G. A. Lienert as 'Konfigurations Frequenz Analyse - KFA'. Lienert, G. A. (1971). Die Konfigurationsfrequenzanalyse: I. Ein neuer Weg zu Typen und Syndromen. Zeitschrift f\u00fcr Klinische Psychologie und Psychotherapie, 19(2), 99\u2013115.",
    "version": "1.6.1-3",
    "maintainer": "Joerg-Henrik Heine <jhheine@googlemail.com>",
    "author": "Joerg-Henrik Heine [aut, cre, com],\n  Rainer W. Alexandrowicz [aut, com],\n  Mark Stemmler [rev, com]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=confreq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "confreq Configural Frequencies Analysis Using Log-Linear Modeling Offers several functions for Configural Frequencies Analysis (CFA), which is a useful statistical tool for the analysis of multiway contingency tables. CFA was introduced by G. A. Lienert as 'Konfigurations Frequenz Analyse - KFA'. Lienert, G. A. (1971). Die Konfigurationsfrequenzanalyse: I. Ein neuer Weg zu Typen und Syndromen. Zeitschrift f\u00fcr Klinische Psychologie und Psychotherapie, 19(2), 99\u2013115.  "
  },
  {
    "id": 10319,
    "package_name": "conmet",
    "title": "Construct Measurement Evaluation Tool",
    "description": "With this package you can run 'ConMET' locally in R. 'ConMET' is an R-shiny application that facilitates performing and evaluating confirmatory factor analyses (CFAs) and is useful for running and reporting typical measurement models in applied psychology and management journals. 'ConMET' automatically creates, compares and summarizes CFA models. Most common fit indices (E.g., CFI and SRMR) are put in an overview table. ConMET also allows to test for common method variance. The application is particularly useful for teaching and instruction of measurement issues in survey research. The application uses the 'lavaan' package (Rosseel, 2012) to run CFAs.",
    "version": "0.1.0",
    "maintainer": "Leander De Schutter <deschutter@rsm.nl>",
    "author": "Leander De Schutter [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9826-4896>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=conmet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conmet Construct Measurement Evaluation Tool With this package you can run 'ConMET' locally in R. 'ConMET' is an R-shiny application that facilitates performing and evaluating confirmatory factor analyses (CFAs) and is useful for running and reporting typical measurement models in applied psychology and management journals. 'ConMET' automatically creates, compares and summarizes CFA models. Most common fit indices (E.g., CFI and SRMR) are put in an overview table. ConMET also allows to test for common method variance. The application is particularly useful for teaching and instruction of measurement issues in survey research. The application uses the 'lavaan' package (Rosseel, 2012) to run CFAs.  "
  },
  {
    "id": 10326,
    "package_name": "conogive",
    "title": "Congeneric Normal-Ogive Model",
    "description": "The congeneric normal-ogive model is a popular model for \n   psychometric data (McDonald, R. P. (1997) <doi:10.1007/978-1-4757-2691-6_15>).\n   This model estimates the model, calculates theoretical and concrete \n   reliability coefficients, and predicts the latent variable of the model. \n   This is the companion package to Moss (2020) <doi:10.31234/osf.io/nvg5d>.",
    "version": "1.0.0",
    "maintainer": "Jonas Moss <jonas.gjertsen@gmail.com>",
    "author": "Jonas Moss [aut, cre] (ORCID: <https://orcid.org/0000-0002-6876-6964>)",
    "url": "https://github.com/JonasMoss/conogive",
    "bug_reports": "https://github.com/JonasMoss/conogive/issues",
    "repository": "https://cran.r-project.org/package=conogive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conogive Congeneric Normal-Ogive Model The congeneric normal-ogive model is a popular model for \n   psychometric data (McDonald, R. P. (1997) <doi:10.1007/978-1-4757-2691-6_15>).\n   This model estimates the model, calculates theoretical and concrete \n   reliability coefficients, and predicts the latent variable of the model. \n   This is the companion package to Moss (2020) <doi:10.31234/osf.io/nvg5d>.  "
  },
  {
    "id": 10331,
    "package_name": "conrad",
    "title": "Client for the Microsoft's 'Cognitive Services Text to Speech\nREST' API",
    "description": "Convert text into synthesized speech and get a list of supported voices for a region. \n    Microsoft's 'Cognitive Services Text to Speech REST' API <https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-text-to-speech?tabs=streaming>  \n    supports neural text to speech voices, which support specific languages and dialects that are identified by locale. ",
    "version": "1.0.0.1",
    "maintainer": "Howard Baek <howardbaek.fh@gmail.com>",
    "author": "Howard Baek [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-8942-1618>),\n  John Muschelli [aut]",
    "url": "https://github.com/fhdsl/conrad",
    "bug_reports": "https://github.com/fhdsl/conrad/issues",
    "repository": "https://cran.r-project.org/package=conrad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conrad Client for the Microsoft's 'Cognitive Services Text to Speech\nREST' API Convert text into synthesized speech and get a list of supported voices for a region. \n    Microsoft's 'Cognitive Services Text to Speech REST' API <https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-text-to-speech?tabs=streaming>  \n    supports neural text to speech voices, which support specific languages and dialects that are identified by locale.   "
  },
  {
    "id": 10643,
    "package_name": "cssTools",
    "title": "Cognitive Social Structure Tools",
    "description": "A collection of tools for estimating a network from a random sample of cognitive social structure (CSS) slices. Also contains functions for evaluating a CSS in terms of various error types observed in each slice.",
    "version": "1.0",
    "maintainer": "Deniz Yenigun <deniz.yenigun@bilgi.edu.tr>",
    "author": "Deniz Yenigun, Gunes Ertan, Michael Siciliano",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cssTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cssTools Cognitive Social Structure Tools A collection of tools for estimating a network from a random sample of cognitive social structure (CSS) slices. Also contains functions for evaluating a CSS in terms of various error types observed in each slice.  "
  },
  {
    "id": 10656,
    "package_name": "ctgdist",
    "title": "Likert Category Distance Calculator",
    "description": "It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments.",
    "version": "0.1.0",
    "maintainer": "Huseyin Yildiz <huseyinyildiz35@gmail.com>",
    "author": "Huseyin Yildiz [aut, cre],\n  Alperen Yandi [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ctgdist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctgdist Likert Category Distance Calculator It is assumed that psychological distances between the categories are equal for the measurement instruments consisted of polytomously scored items. According to Muraki, this assumption must be tested. In the examination process of this assumption, the fit indexes are obtained and evaluated. This package provides that this assumption is removed. By with this package, the converted scale values of all items in a measurement instrument can be calculated by estimating a category parameter set for each item. Thus, the calculations can be made without any need to usage of the common category parameter set. Through this package, the psychological distances of the items are scaled. The scaling of a category parameter set for each item cause differentiation of score of the categories will be got from items. Also, the total measurement instrument score of an individual can be calculated according to the scaling of item score categories by with this package.This package provides that the place of individuals related to the structure to be measured with a measurement instrument consisted of polytomously scored items can be reveal more accurately. In this way, it is thought that the results obtained about individuals can be made more sensitive, and the differences between individuals can be revealed more accurately. On the other hand, it can be argued that more accurate evidences can be obtained regarding the psychometric properties of the measurement instruments.  "
  },
  {
    "id": 10671,
    "package_name": "ctsemOMX",
    "title": "Continuous Time SEM - 'OpenMx' Based Functions",
    "description": "Original 'ctsem' (continuous time structural equation modelling)\n    functionality, based on the 'OpenMx' software, as described in \n    Driver, Oud, Voelkle (2017) <doi:10.18637/jss.v077.i05>, with updated details in vignette. \n    Combines stochastic differential equations representing latent processes with \n    structural equation measurement models. These functions were split off from\n    the main package of 'ctsem', as the main package uses the 'rstan' package as a backend now --\n    offering estimation options from max likelihood to Bayesian.\n    There are nevertheless use cases for the wide format SEM style approach as offered here, \n    particularly when there are no individual differences in observation timing and the\n    number of individuals is large. For the main 'ctsem' package, see <https://cran.r-project.org/package=ctsem>.",
    "version": "1.0.7",
    "maintainer": "Charles Driver <charles.driver2@uzh.ch>",
    "author": "Charles Driver [aut, cre, cph],\n  Manuel Voelkle [aut, cph],\n  Han Oud [aut, cph]",
    "url": "https://github.com/cdriveraus/ctsemOMX",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ctsemOMX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctsemOMX Continuous Time SEM - 'OpenMx' Based Functions Original 'ctsem' (continuous time structural equation modelling)\n    functionality, based on the 'OpenMx' software, as described in \n    Driver, Oud, Voelkle (2017) <doi:10.18637/jss.v077.i05>, with updated details in vignette. \n    Combines stochastic differential equations representing latent processes with \n    structural equation measurement models. These functions were split off from\n    the main package of 'ctsem', as the main package uses the 'rstan' package as a backend now --\n    offering estimation options from max likelihood to Bayesian.\n    There are nevertheless use cases for the wide format SEM style approach as offered here, \n    particularly when there are no individual differences in observation timing and the\n    number of individuals is large. For the main 'ctsem' package, see <https://cran.r-project.org/package=ctsem>.  "
  },
  {
    "id": 10709,
    "package_name": "cusp",
    "title": "Cusp-Catastrophe Model Fitting Using Maximum Likelihood",
    "description": "Cobb's maximum likelihood method for cusp-catastrophe modeling\n        (Grasman, van der Maas, and Wagenmakers (2009) <doi:10.18637/jss.v032.i08>;\n        Cobb (1981), Behavioral Science, 26(1), 75-78). Includes a cusp() function for model \n        fitting, and several utility functions for plotting, and for comparing the\n        model to linear regression and logistic curve models.",
    "version": "2.3.8",
    "maintainer": "Raoul P. P. P. Grasman <rgrasman@uva.nl>",
    "author": "Raoul P. P. P. Grasman [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7458-1272>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cusp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cusp Cusp-Catastrophe Model Fitting Using Maximum Likelihood Cobb's maximum likelihood method for cusp-catastrophe modeling\n        (Grasman, van der Maas, and Wagenmakers (2009) <doi:10.18637/jss.v032.i08>;\n        Cobb (1981), Behavioral Science, 26(1), 75-78). Includes a cusp() function for model \n        fitting, and several utility functions for plotting, and for comparing the\n        model to linear regression and logistic curve models.  "
  },
  {
    "id": 10733,
    "package_name": "cvsem",
    "title": "SEM Model Comparison with K-Fold Cross-Validation",
    "description": "The goal of 'cvsem' is to provide functions that allow for comparing Structural Equation Models (SEM) using cross-validation. Users can specify multiple SEMs using 'lavaan' syntax. 'cvsem' computes the Kullback Leibler (KL) Divergence between 1) the model implied covariance matrix estimated from the training data and 2) the sample covariance matrix estimated from the test data described in Cudeck, Robert & Browne (1983) <doi:10.18637/jss.v048.i02>. The KL Divergence is computed for each of the specified SEMs allowing for the models to be compared based on their prediction errors. ",
    "version": "1.0.0",
    "maintainer": "Anna Wysocki <awysocki@ucdavis.edu>",
    "author": "Anna Wysocki [aut, cre],\n  Danielle Siegel [aut],\n  Cameron allen [aut],\n  Philippe Rast [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cvsem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvsem SEM Model Comparison with K-Fold Cross-Validation The goal of 'cvsem' is to provide functions that allow for comparing Structural Equation Models (SEM) using cross-validation. Users can specify multiple SEMs using 'lavaan' syntax. 'cvsem' computes the Kullback Leibler (KL) Divergence between 1) the model implied covariance matrix estimated from the training data and 2) the sample covariance matrix estimated from the test data described in Cudeck, Robert & Browne (1983) <doi:10.18637/jss.v048.i02>. The KL Divergence is computed for each of the specified SEMs allowing for the models to be compared based on their prediction errors.   "
  },
  {
    "id": 10768,
    "package_name": "dRiftDM",
    "title": "Estimating (Time-Dependent) Drift Diffusion Models",
    "description": "Fit and explore Drift Diffusion Models (DDMs),\n    a common tool in psychology for describing decision processes in simple\n    tasks. It can handle both time-independent and time-dependent DDMs. You\n    either choose prebuilt models or create your own, and the package takes\n    care of model predictions and parameter estimation. Model predictions\n    are derived via the numerical solutions provided by Richter, Ulrich, and\n    Janczyk (2023, <doi:10.1016/j.jmp.2023.102756>).",
    "version": "0.3.1",
    "maintainer": "Valentin Koob <v.koob@web.de>",
    "author": "Valentin Koob [cre, aut, cph],\n  Thomas Richter [aut, cph],\n  Markus Janczyk [aut]",
    "url": "https://github.com/bucky2177/dRiftDM,\nhttps://bucky2177.github.io/dRiftDM/",
    "bug_reports": "https://github.com/bucky2177/dRiftDM/issues",
    "repository": "https://cran.r-project.org/package=dRiftDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dRiftDM Estimating (Time-Dependent) Drift Diffusion Models Fit and explore Drift Diffusion Models (DDMs),\n    a common tool in psychology for describing decision processes in simple\n    tasks. It can handle both time-independent and time-dependent DDMs. You\n    either choose prebuilt models or create your own, and the package takes\n    care of model predictions and parameter estimation. Model predictions\n    are derived via the numerical solutions provided by Richter, Ulrich, and\n    Janczyk (2023, <doi:10.1016/j.jmp.2023.102756>).  "
  },
  {
    "id": 10910,
    "package_name": "dcmstan",
    "title": "Generate 'Stan' Code for Diagnostic Classification Models",
    "description": "Diagnostic classification models are psychometric models used\n    to categorically estimate respondents mastery, or proficiency, on a\n    set of predefined skills (Bradshaw, 2016,\n    <doi:10.1002/9781118956588.ch13>).  Diagnostic models can be estimated\n    with 'Stan'; however, the necessary scripts can be long and\n    complicated. This package automates the creation of 'Stan' scripts for\n    diagnostic classification models. Specify different types of\n    diagnostic models, define prior distributions, and automatically\n    generate the necessary 'Stan' code for estimating the model.",
    "version": "0.1.0",
    "maintainer": "W. Jake Thompson <wjakethompson@gmail.com>",
    "author": "W. Jake Thompson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7339-0300>),\n  Jeffrey Hoover [aut] (ORCID: <https://orcid.org/0000-0002-0276-0308>),\n  Auburn Jimenez [aut] (ORCID: <https://orcid.org/0000-0002-7072-2960>),\n  University of Kansas [cph],\n  Institute of Education Sciences [fnd]",
    "url": "https://dcmstan.r-dcm.org, https://github.com/r-dcm/dcmstan",
    "bug_reports": "https://github.com/r-dcm/dcmstan/issues",
    "repository": "https://cran.r-project.org/package=dcmstan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dcmstan Generate 'Stan' Code for Diagnostic Classification Models Diagnostic classification models are psychometric models used\n    to categorically estimate respondents mastery, or proficiency, on a\n    set of predefined skills (Bradshaw, 2016,\n    <doi:10.1002/9781118956588.ch13>).  Diagnostic models can be estimated\n    with 'Stan'; however, the necessary scripts can be long and\n    complicated. This package automates the creation of 'Stan' scripts for\n    diagnostic classification models. Specify different types of\n    diagnostic models, define prior distributions, and automatically\n    generate the necessary 'Stan' code for estimating the model.  "
  },
  {
    "id": 10917,
    "package_name": "ddModel",
    "title": "The Decision Diffusion Model",
    "description": "Provides functions for computing the density, distribution, and random generation of the Decision Diffusion model (DDM), a widely used cognitive model for analysing choice and response time data. The package allows model specification, including the ability to fix, constrain, or vary parameters across experimental conditions. While it does not include a built-in optimiser, it supports likelihood evaluation and can be integrated with external tools for parameter estimation. Functions for simulating synthetic datasets are also provided. This package is intended for researchers modelling speeded decision-making in behavioural and cognitive experiments. For more information, see Voss, Rothermund, and Voss (2004) <doi:10.3758/BF03196893>, Voss and Voss (2007) <doi:10.3758/BF03192967>, and Ratcliff and McKoon (2008) <doi:10.1162/neco.2008.12-06-420>.",
    "version": "0.2.9.0",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "author": "Yi-Shin Lin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ddModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ddModel The Decision Diffusion Model Provides functions for computing the density, distribution, and random generation of the Decision Diffusion model (DDM), a widely used cognitive model for analysing choice and response time data. The package allows model specification, including the ability to fix, constrain, or vary parameters across experimental conditions. While it does not include a built-in optimiser, it supports likelihood evaluation and can be integrated with external tools for parameter estimation. Functions for simulating synthetic datasets are also provided. This package is intended for researchers modelling speeded decision-making in behavioural and cognitive experiments. For more information, see Voss, Rothermund, and Voss (2004) <doi:10.3758/BF03196893>, Voss and Voss (2007) <doi:10.3758/BF03192967>, and Ratcliff and McKoon (2008) <doi:10.1162/neco.2008.12-06-420>.  "
  },
  {
    "id": 11080,
    "package_name": "dexter",
    "title": "Data Management and Analysis of Tests",
    "description": "A system for the management, assessment, and psychometric analysis of data from educational and psychological tests. ",
    "version": "1.7.0",
    "maintainer": "Jesse Koops <jesse.koops@cito.nl>",
    "author": "Gunter Maris [aut],\n  Timo Bechger [aut],\n  Jesse Koops [aut, cre],\n  Ivailo Partchev [aut]",
    "url": "https://dexter-psychometrics.github.io/dexter/",
    "bug_reports": "https://github.com/dexter-psychometrics/dexter/issues",
    "repository": "https://cran.r-project.org/package=dexter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dexter Data Management and Analysis of Tests A system for the management, assessment, and psychometric analysis of data from educational and psychological tests.   "
  },
  {
    "id": 11162,
    "package_name": "dina",
    "title": "Bayesian Estimation of DINA Model",
    "description": "Estimate the Deterministic Input, Noisy \"And\" Gate (DINA)\n    cognitive diagnostic model parameters using the Gibbs sampler described\n    by Culpepper (2015) <doi:10.3102/1076998615595403>.",
    "version": "2.0.2",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "author": "Steven Andrew Culpepper [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4226-6176>),\n  James Joseph Balamuta [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>)",
    "url": "https://github.com/tmsalab/dina, https://tmsalab.github.io/dina/",
    "bug_reports": "https://github.com/tmsalab/dina/issues",
    "repository": "https://cran.r-project.org/package=dina",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dina Bayesian Estimation of DINA Model Estimate the Deterministic Input, Noisy \"And\" Gate (DINA)\n    cognitive diagnostic model parameters using the Gibbs sampler described\n    by Culpepper (2015) <doi:10.3102/1076998615595403>.  "
  },
  {
    "id": 11249,
    "package_name": "diversityArch",
    "title": "Computes Diversity Indices with Archaeological Data",
    "description": "Companion package of Arnaud Barat, Andreu Sans\u00f3, Maite Arilla-Osuna, \n    Ruth Blasco, I\u00f1aki P\u00e9rez-Fern\u00e1ndez, Gabriel Cifuentes-Alcobenda, \n    Rub\u00e9n Llorente, Daniel Vivar-R\u00edos, Ella Assaf, Ran Barkai, Avi Gopher, &\n    Jordi Rosell-Ard\u00e8vol (2025), \"Quantifying Diversity through Entropy \n    Decomposition. Insights into Hominin Occupation and Carcass Processing \n    at Qesem cave\".",
    "version": "0.1.0",
    "maintainer": "Andreu Sans\u00f3 <andreu.sanso@uib.eu>",
    "author": "Andreu Sans\u00f3 [aut, cre],\n  Arnaud Barat [aut],\n  Jordi Rosell [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=diversityArch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diversityArch Computes Diversity Indices with Archaeological Data Companion package of Arnaud Barat, Andreu Sans\u00f3, Maite Arilla-Osuna, \n    Ruth Blasco, I\u00f1aki P\u00e9rez-Fern\u00e1ndez, Gabriel Cifuentes-Alcobenda, \n    Rub\u00e9n Llorente, Daniel Vivar-R\u00edos, Ella Assaf, Ran Barkai, Avi Gopher, &\n    Jordi Rosell-Ard\u00e8vol (2025), \"Quantifying Diversity through Entropy \n    Decomposition. Insights into Hominin Occupation and Carcass Processing \n    at Qesem cave\".  "
  },
  {
    "id": 11388,
    "package_name": "drhutools",
    "title": "Political Science Academic Research Gears",
    "description": "Using these tools to simplify the research process of political science and other social sciences. The current version can create folder system for academic project in political science, calculate psychological trait scores, visualize experimental and spatial data, and set up color-blind palette, functions used in academic research of political psychology or political science in general.",
    "version": "1.0.1",
    "maintainer": "Yue Hu <yuehu@tsinghua.edu.cn>",
    "author": "Yue Hu [aut, cre],\n  Qian Qiu [ctb],\n  Wen Deng [ctb]",
    "url": "https://www.drhuyue.site/software/drhutools/",
    "bug_reports": "https://github.com/sammo3182/drhutools/issues",
    "repository": "https://cran.r-project.org/package=drhutools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drhutools Political Science Academic Research Gears Using these tools to simplify the research process of political science and other social sciences. The current version can create folder system for academic project in political science, calculate psychological trait scores, visualize experimental and spatial data, and set up color-blind palette, functions used in academic research of political psychology or political science in general.  "
  },
  {
    "id": 11406,
    "package_name": "ds4psy",
    "title": "Data Science for Psychologists",
    "description": "All datasets and functions required for the examples and exercises of the book \"Data Science for Psychologists\" (by Hansjoerg Neth, Konstanz University, 2025, <doi:10.5281/zenodo.7229812>), freely available at <https://bookdown.org/hneth/ds4psy/>. The book and corresponding courses introduce principles and methods of data science to students of psychology and other biological or social sciences. The 'ds4psy' package primarily provides datasets, but also functions for data generation and manipulation (e.g., of text and time data) and graphics that are used in the book and its exercises. All functions included in 'ds4psy' are designed to be explicit and instructive, rather than efficient or elegant. ",
    "version": "1.2.0",
    "maintainer": "Hansjoerg Neth <h.neth@uni.kn>",
    "author": "Hansjoerg Neth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5427-3141>)",
    "url": "https://bookdown.org/hneth/ds4psy/,\nhttps://github.com/hneth/ds4psy/",
    "bug_reports": "https://github.com/hneth/ds4psy/issues",
    "repository": "https://cran.r-project.org/package=ds4psy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ds4psy Data Science for Psychologists All datasets and functions required for the examples and exercises of the book \"Data Science for Psychologists\" (by Hansjoerg Neth, Konstanz University, 2025, <doi:10.5281/zenodo.7229812>), freely available at <https://bookdown.org/hneth/ds4psy/>. The book and corresponding courses introduce principles and methods of data science to students of psychology and other biological or social sciences. The 'ds4psy' package primarily provides datasets, but also functions for data generation and manipulation (e.g., of text and time data) and graphics that are used in the book and its exercises. All functions included in 'ds4psy' are designed to be explicit and instructive, rather than efficient or elegant.   "
  },
  {
    "id": 11472,
    "package_name": "dySEM",
    "title": "Dyadic Structural Equation Modeling",
    "description": "Scripting of structural equation models via 'lavaan' for\n    Dyadic Data Analysis, and helper functions for supplemental\n    calculations, tabling, and model visualization.  Current models\n    supported include Dyadic Confirmatory Factor Analysis, the Actor\u2013Partner \n    Interdependence Model (observed and latent), the Common Fate Model\n    (observed and latent), Mutual Influence Model (latent), and the Bifactor\n    Dyadic Model (latent).",
    "version": "1.1.1",
    "maintainer": "John Sakaluk <jksakaluk@gmail.com>",
    "author": "John Sakaluk [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2515-9822>),\n  Omar Camanto [aut],\n  Robyn Kilshaw [ctb],\n  Alexandra Fisher [ctb]",
    "url": "https://github.com/jsakaluk/dySEM,\nhttps://jsakaluk.github.io/dySEM/",
    "bug_reports": "https://github.com/jsakaluk/dySEM/issues",
    "repository": "https://cran.r-project.org/package=dySEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dySEM Dyadic Structural Equation Modeling Scripting of structural equation models via 'lavaan' for\n    Dyadic Data Analysis, and helper functions for supplemental\n    calculations, tabling, and model visualization.  Current models\n    supported include Dyadic Confirmatory Factor Analysis, the Actor\u2013Partner \n    Interdependence Model (observed and latent), the Common Fate Model\n    (observed and latent), Mutual Influence Model (latent), and the Bifactor\n    Dyadic Model (latent).  "
  },
  {
    "id": 11538,
    "package_name": "easy.glmnet",
    "title": "Functions to Simplify the Use of 'glmnet' for Machine Learning",
    "description": "Provides several functions to simplify using the 'glmnet' package: converting data frames into matrices ready for 'glmnet'; b) imputing missing variables multiple times; c) fitting and applying prediction models straightforwardly; d) assigning observations to folds in a balanced way; e) cross-validate the models; f) selecting the most representative model across imputations and folds; and g) getting the relevance of the model regressors; as described in several publications: Solanes et al. (2022) <doi:10.1038/s41537-022-00309-w>, Palau et al. (2023) <doi:10.1016/j.rpsm.2023.01.001>, Sobregrau et al. (2024) <doi:10.1016/j.jpsychores.2024.111656>.",
    "version": "1.0",
    "maintainer": "Joaquim Radua <quimradua@gmail.com>",
    "author": "Joaquim Radua [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1240-5438>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=easy.glmnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easy.glmnet Functions to Simplify the Use of 'glmnet' for Machine Learning Provides several functions to simplify using the 'glmnet' package: converting data frames into matrices ready for 'glmnet'; b) imputing missing variables multiple times; c) fitting and applying prediction models straightforwardly; d) assigning observations to folds in a balanced way; e) cross-validate the models; f) selecting the most representative model across imputations and folds; and g) getting the relevance of the model regressors; as described in several publications: Solanes et al. (2022) <doi:10.1038/s41537-022-00309-w>, Palau et al. (2023) <doi:10.1016/j.rpsm.2023.01.001>, Sobregrau et al. (2024) <doi:10.1016/j.jpsychores.2024.111656>.  "
  },
  {
    "id": 11555,
    "package_name": "easybgm",
    "title": "Extracting and Visualizing Bayesian Graphical Models",
    "description": "Fit and visualize the results of a Bayesian analysis of networks commonly found in psychology. \n    The package supports fitting cross-sectional network models fitted using the packages 'BDgraph', 'bgms' and 'BGGM', \n    as well as network comparison fitted using the 'bgms' and 'BBGM'. \n    The package provides the parameter estimates, posterior inclusion probabilities, inclusion Bayes factor, and the \n    posterior density of the parameters. In addition, for 'BDgraph' and 'bgms' it allows to assess the posterior \n    structure space. Furthermore, the package comes with an extensive suite for visualizing results.",
    "version": "0.3.1",
    "maintainer": "Karoline Huth <k.huth@uva.nl>",
    "author": "Karoline Huth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0662-1591>),\n  Sara Keetelaar [ctb],\n  Nikola Sekulovski [ctb],\n  Gali Geller [ctb]",
    "url": "https://github.com/KarolineHuth/easybgm",
    "bug_reports": "https://github.com/KarolineHuth/easybgm/issues",
    "repository": "https://cran.r-project.org/package=easybgm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easybgm Extracting and Visualizing Bayesian Graphical Models Fit and visualize the results of a Bayesian analysis of networks commonly found in psychology. \n    The package supports fitting cross-sectional network models fitted using the packages 'BDgraph', 'bgms' and 'BGGM', \n    as well as network comparison fitted using the 'bgms' and 'BBGM'. \n    The package provides the parameter estimates, posterior inclusion probabilities, inclusion Bayes factor, and the \n    posterior density of the parameters. In addition, for 'BDgraph' and 'bgms' it allows to assess the posterior \n    structure space. Furthermore, the package comes with an extensive suite for visualizing results.  "
  },
  {
    "id": 11669,
    "package_name": "edina",
    "title": "Bayesian Estimation of an Exploratory Deterministic Input, Noisy\nand Gate Model",
    "description": "Perform a Bayesian estimation of the exploratory \n    deterministic input, noisy and gate (EDINA)\n    cognitive diagnostic model described by Chen et al. (2018)\n    <doi:10.1007/s11336-017-9579-4>.",
    "version": "0.1.2",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "author": "James Joseph Balamuta [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>),\n  Steven Andrew Culpepper [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4226-6176>),\n  Jeffrey A. Douglas [aut]",
    "url": "https://github.com/tmsalab/edina, https://tmsalab.github.io/edina/",
    "bug_reports": "https://github.com/tmsalab/edina/issues",
    "repository": "https://cran.r-project.org/package=edina",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edina Bayesian Estimation of an Exploratory Deterministic Input, Noisy\nand Gate Model Perform a Bayesian estimation of the exploratory \n    deterministic input, noisy and gate (EDINA)\n    cognitive diagnostic model described by Chen et al. (2018)\n    <doi:10.1007/s11336-017-9579-4>.  "
  },
  {
    "id": 11675,
    "package_name": "edmdata",
    "title": "Data Sets for Psychometric Modeling",
    "description": "Collection of data sets from various assessments that can be used to \n    evaluate psychometric models. These data sets have been analyzed in the\n    following papers that introduced new methodology as part of the application section:\n    Jimenez, A., Balamuta, J. J., & Culpepper, S. A. (2023) <doi:10.1111/bmsp.12307>,\n    Culpepper, S. A., & Balamuta, J. J. (2021) <doi:10.1080/00273171.2021.1985949>,\n    Yinghan Chen et al. (2021) <doi:10.1007/s11336-021-09750-9>,\n    Yinyin Chen et al. (2020) <doi:10.1007/s11336-019-09693-2>,\n    Culpepper, S. A. (2019a) <doi:10.1007/s11336-019-09683-4>,\n    Culpepper, S. A. (2019b) <doi:10.1007/s11336-018-9643-8>,\n    Culpepper, S. A., & Chen, Y. (2019) <doi:10.3102/1076998618791306>,\n    Culpepper, S. A., & Balamuta, J. J. (2017) <doi:10.1007/s11336-015-9484-7>,\n    and Culpepper, S. A. (2015) <doi:10.3102/1076998615595403>.",
    "version": "1.3.0",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "author": "James Joseph Balamuta [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>),\n  Steven Andrew Culpepper [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4226-6176>),\n  Jeffrey Alan Douglas [aut, cph],\n  Auburn Jimenez [ctb, cph]",
    "url": "https://tmsalab.github.io/edmdata/,\nhttps://github.com/tmsalab/edmdata/",
    "bug_reports": "https://github.com/tmsalab/edmdata/issues",
    "repository": "https://cran.r-project.org/package=edmdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edmdata Data Sets for Psychometric Modeling Collection of data sets from various assessments that can be used to \n    evaluate psychometric models. These data sets have been analyzed in the\n    following papers that introduced new methodology as part of the application section:\n    Jimenez, A., Balamuta, J. J., & Culpepper, S. A. (2023) <doi:10.1111/bmsp.12307>,\n    Culpepper, S. A., & Balamuta, J. J. (2021) <doi:10.1080/00273171.2021.1985949>,\n    Yinghan Chen et al. (2021) <doi:10.1007/s11336-021-09750-9>,\n    Yinyin Chen et al. (2020) <doi:10.1007/s11336-019-09693-2>,\n    Culpepper, S. A. (2019a) <doi:10.1007/s11336-019-09683-4>,\n    Culpepper, S. A. (2019b) <doi:10.1007/s11336-018-9643-8>,\n    Culpepper, S. A., & Chen, Y. (2019) <doi:10.3102/1076998618791306>,\n    Culpepper, S. A., & Balamuta, J. J. (2017) <doi:10.1007/s11336-015-9484-7>,\n    and Culpepper, S. A. (2015) <doi:10.3102/1076998615595403>.  "
  },
  {
    "id": 11688,
    "package_name": "eesim",
    "title": "Simulate and Evaluate Time Series for Environmental Epidemiology",
    "description": "Provides functions to create simulated time series of environmental\n    exposures (e.g., temperature, air pollution) and health outcomes for use in\n    power analysis and simulation studies in environmental epidemiology. This\n    package also provides functions to evaluate the results of simulation studies\n    based on these simulated time series. This work was supported by a grant\n    from the National Institute of Environmental Health Sciences (R00ES022631) and\n    a fellowship from the Colorado State University Programs for Research and\n    Scholarly Excellence.",
    "version": "0.1.0",
    "maintainer": "Brooke Anderson <brooke.anderson@colostate.edu>",
    "author": "Sarah Koehler [aut],\n  Brooke Anderson [aut, cre]",
    "url": "http://github.com/sakoehler7/eesim",
    "bug_reports": "http://github.com/sakoehler7/eesim/issues",
    "repository": "https://cran.r-project.org/package=eesim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eesim Simulate and Evaluate Time Series for Environmental Epidemiology Provides functions to create simulated time series of environmental\n    exposures (e.g., temperature, air pollution) and health outcomes for use in\n    power analysis and simulation studies in environmental epidemiology. This\n    package also provides functions to evaluate the results of simulation studies\n    based on these simulated time series. This work was supported by a grant\n    from the National Institute of Environmental Health Sciences (R00ES022631) and\n    a fellowship from the Colorado State University Programs for Research and\n    Scholarly Excellence.  "
  },
  {
    "id": 11691,
    "package_name": "eff2",
    "title": "Efficient Least Squares for Total Causal Effects",
    "description": "Estimate a total causal effect from observational data under \n    linearity and causal sufficiency. The observational data is supposed to \n    be generated from a linear structural equation model (SEM) with independent \n    and additive noise. The underlying causal DAG associated the SEM is required\n    to be known up to a maximally oriented partially directed graph (MPDAG), \n    which is a general class of graphs consisting of both directed and \n    undirected edges, including CPDAGs (i.e., essential graphs) and DAGs. Such\n    graphs are usually obtained with structure learning algorithms with added \n    background knowledge. The program is able to estimate every identified \n    effect, including single and multiple treatment variables. Moreover, the \n    resulting estimate has the minimal asymptotic covariance (and hence \n    shortest confidence intervals) among all estimators that are based on the \n    sample covariance. ",
    "version": "1.0.2",
    "maintainer": "Richard Guo <ricguo@uw.edu>",
    "author": "Richard Guo [aut, cre] (ORCID: <https://orcid.org/0000-0002-2081-7398>)",
    "url": "https://github.com/richardkwo/eff2",
    "bug_reports": "https://github.com/richardkwo/eff2/issues",
    "repository": "https://cran.r-project.org/package=eff2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eff2 Efficient Least Squares for Total Causal Effects Estimate a total causal effect from observational data under \n    linearity and causal sufficiency. The observational data is supposed to \n    be generated from a linear structural equation model (SEM) with independent \n    and additive noise. The underlying causal DAG associated the SEM is required\n    to be known up to a maximally oriented partially directed graph (MPDAG), \n    which is a general class of graphs consisting of both directed and \n    undirected edges, including CPDAGs (i.e., essential graphs) and DAGs. Such\n    graphs are usually obtained with structure learning algorithms with added \n    background knowledge. The program is able to estimate every identified \n    effect, including single and multiple treatment variables. Moreover, the \n    resulting estimate has the minimal asymptotic covariance (and hence \n    shortest confidence intervals) among all estimators that are based on the \n    sample covariance.   "
  },
  {
    "id": 11721,
    "package_name": "eirm",
    "title": "Explanatory Item Response Modeling for Dichotomous and\nPolytomous Items",
    "description": "Analysis of dichotomous and polytomous response data using the explanatory item response modeling framework, as described in Bulut, Gorgun, & Yildirim-Erbasli (2021) <doi:10.3390/psych3030023>, Stanke & Bulut (2019) <doi:10.21449/ijate.515085>, and De Boeck & Wilson (2004) <doi:10.1007/978-1-4757-3990-9>. Generalized linear mixed modeling is used for estimating the effects of item-related and person-related variables on dichotomous and polytomous item responses. ",
    "version": "0.5",
    "maintainer": "Okan Bulut <bulut@ualberta.ca>",
    "author": "Okan Bulut [aut, cre] (ORCID: <https://orcid.org/0000-0001-5853-1267>)",
    "url": "https://github.com/okanbulut/eirm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eirm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eirm Explanatory Item Response Modeling for Dichotomous and\nPolytomous Items Analysis of dichotomous and polytomous response data using the explanatory item response modeling framework, as described in Bulut, Gorgun, & Yildirim-Erbasli (2021) <doi:10.3390/psych3030023>, Stanke & Bulut (2019) <doi:10.21449/ijate.515085>, and De Boeck & Wilson (2004) <doi:10.1007/978-1-4757-3990-9>. Generalized linear mixed modeling is used for estimating the effects of item-related and person-related variables on dichotomous and polytomous item responses.   "
  },
  {
    "id": 11872,
    "package_name": "epos",
    "title": "Epilepsy Ontologies' Similarities",
    "description": "Analysis and visualization of similarities between epilepsy ontologies based on text mining results by comparing ranked lists of co-occurring drug terms in the BioASQ corpus. The ranked result lists of neurological drug terms co-occurring with terms from the epilepsy ontologies EpSO, ESSO, EPILONT, EPISEM and FENICS undergo further analysis. The source data to create the ranked lists of drug names is produced using the text mining workflows described in Mueller, Bernd and Hagelstein, Alexandra (2016) <doi:10.4126/FRL01-006408558>, Mueller, Bernd et al. (2017) <doi:10.1007/978-3-319-58694-6_22>, Mueller, Bernd and Rebholz-Schuhmann, Dietrich (2020) <doi:10.1007/978-3-030-43887-6_52>, and Mueller, Bernd et al. (2022) <doi:10.1186/s13326-021-00258-w>.",
    "version": "1.2",
    "maintainer": "Bernd Mueller <bernd.mueller@zbmed.de>",
    "author": "Bernd Mueller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3062-8192>)",
    "url": "https://github.com/bernd-mueller/epos",
    "bug_reports": "https://github.com/bernd-mueller/epos/issues",
    "repository": "https://cran.r-project.org/package=epos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epos Epilepsy Ontologies' Similarities Analysis and visualization of similarities between epilepsy ontologies based on text mining results by comparing ranked lists of co-occurring drug terms in the BioASQ corpus. The ranked result lists of neurological drug terms co-occurring with terms from the epilepsy ontologies EpSO, ESSO, EPILONT, EPISEM and FENICS undergo further analysis. The source data to create the ranked lists of drug names is produced using the text mining workflows described in Mueller, Bernd and Hagelstein, Alexandra (2016) <doi:10.4126/FRL01-006408558>, Mueller, Bernd et al. (2017) <doi:10.1007/978-3-319-58694-6_22>, Mueller, Bernd and Rebholz-Schuhmann, Dietrich (2020) <doi:10.1007/978-3-030-43887-6_52>, and Mueller, Bernd et al. (2022) <doi:10.1186/s13326-021-00258-w>.  "
  },
  {
    "id": 11922,
    "package_name": "esci",
    "title": "Estimation Statistics with Confidence Intervals",
    "description": "A collection of functions and 'jamovi' module for the estimation approach to inferential statistics, the approach which emphasizes effect sizes, interval estimates, and meta-analysis. Nearly all functions are based on 'statpsych' and 'metafor'.  This package is still under active development, and breaking changes are likely, especially with the plot and hypothesis test functions.  Data sets are included for all examples from Cumming & Calin-Jageman (2024) <ISBN:9780367531508>.",
    "version": "1.0.7",
    "maintainer": "Robert Calin-Jageman <rcalinjageman@dom.edu>",
    "author": "Robert Calin-Jageman [aut, cre, cph]",
    "url": "https://github.com/rcalinjageman/esci/,\nhttps://rcalinjageman.github.io/esci/",
    "bug_reports": "https://github.com/rcalinjageman/esci/issues/",
    "repository": "https://cran.r-project.org/package=esci",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "esci Estimation Statistics with Confidence Intervals A collection of functions and 'jamovi' module for the estimation approach to inferential statistics, the approach which emphasizes effect sizes, interval estimates, and meta-analysis. Nearly all functions are based on 'statpsych' and 'metafor'.  This package is still under active development, and breaking changes are likely, especially with the plot and hypothesis test functions.  Data sets are included for all examples from Cumming & Calin-Jageman (2024) <ISBN:9780367531508>.  "
  },
  {
    "id": 11925,
    "package_name": "esem",
    "title": "Exploratory Structural Equation Modeling ESEM",
    "description": "A collection of functions developed to support the tutorial on using Exploratory Structural Equiation Modeling (ESEM) (Asparouhov & Muth\u00e9n, 2009) <https://www.statmodel.com/download/EFACFA810.pdf>) with Longitudinal Study of Australian Children (LSAC) dataset (Mohal et al., 2023) <doi:10.26193/QR4L6Q>. \n    The package uses 'tidyverse','psych', 'lavaan','semPlot' and provides additional functions to conduct ESEM. \n    The package provides general functions to complete ESEM, including esem_c(), creation of target matrix (if it is used) make_target(), generation of the Confirmatory Factor Analysis (CFA) model syntax esem_cfa_syntax(). \n    A sample data is provided - the package includes a sample data of the Strengths and Difficulties Questionnaire of the Longitudinal Study of Australian Children (SDQ LSAC) in sdq_lsac().  \n    'ESEM' package vignette presents the tutorial demonstrating the use of ESEM on SDQ LSAC data. ",
    "version": "2.0.0",
    "maintainer": "Maria Prokofieva <maria.prokofieva@gmail.com>",
    "author": "Maria Prokofieva [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1974-3827>),\n  Vasileios Stavropoulos [aut],\n  Daniel Zarate [aut]",
    "url": "https://github.com/maria-pro/esem",
    "bug_reports": "https://github.com/maria-pro/esem/issues",
    "repository": "https://cran.r-project.org/package=esem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "esem Exploratory Structural Equation Modeling ESEM A collection of functions developed to support the tutorial on using Exploratory Structural Equiation Modeling (ESEM) (Asparouhov & Muth\u00e9n, 2009) <https://www.statmodel.com/download/EFACFA810.pdf>) with Longitudinal Study of Australian Children (LSAC) dataset (Mohal et al., 2023) <doi:10.26193/QR4L6Q>. \n    The package uses 'tidyverse','psych', 'lavaan','semPlot' and provides additional functions to conduct ESEM. \n    The package provides general functions to complete ESEM, including esem_c(), creation of target matrix (if it is used) make_target(), generation of the Confirmatory Factor Analysis (CFA) model syntax esem_cfa_syntax(). \n    A sample data is provided - the package includes a sample data of the Strengths and Difficulties Questionnaire of the Longitudinal Study of Australian Children (SDQ LSAC) in sdq_lsac().  \n    'ESEM' package vignette presents the tutorial demonstrating the use of ESEM on SDQ LSAC data.   "
  },
  {
    "id": 12026,
    "package_name": "exametrika",
    "title": "Test Theory Analysis and Biclustering",
    "description": "Implements comprehensive test data engineering methods as described in \n    Shojima (2022, ISBN:978-9811699856). Provides statistical techniques for \n    engineering and processing test data: Classical Test Theory (CTT) with \n    reliability coefficients for continuous ability assessment; Item Response \n    Theory (IRT) including Rasch, 2PL, and 3PL models with item/test information \n    functions; Latent Class Analysis (LCA) for nominal clustering; Latent Rank \n    Analysis (LRA) for ordinal clustering with automatic determination of cluster \n    numbers; Biclustering methods including infinite relational models for \n    simultaneous clustering of examinees and items without predefined cluster \n    numbers; and Bayesian Network Models (BNM) for visualizing inter-item \n    dependencies. Features local dependence analysis through LRA and biclustering, \n    parameter estimation, dimensionality assessment, and network structure \n    visualization for educational, psychological, and social science research.",
    "version": "1.8.0",
    "maintainer": "Koji Kosugi <kosugitti@gmail.com>",
    "author": "Koji Kosugi [aut, cre] (ORCID: <https://orcid.org/0000-0001-5816-0099>)",
    "url": "https://kosugitti.github.io/exametrika/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=exametrika",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exametrika Test Theory Analysis and Biclustering Implements comprehensive test data engineering methods as described in \n    Shojima (2022, ISBN:978-9811699856). Provides statistical techniques for \n    engineering and processing test data: Classical Test Theory (CTT) with \n    reliability coefficients for continuous ability assessment; Item Response \n    Theory (IRT) including Rasch, 2PL, and 3PL models with item/test information \n    functions; Latent Class Analysis (LCA) for nominal clustering; Latent Rank \n    Analysis (LRA) for ordinal clustering with automatic determination of cluster \n    numbers; Biclustering methods including infinite relational models for \n    simultaneous clustering of examinees and items without predefined cluster \n    numbers; and Bayesian Network Models (BNM) for visualizing inter-item \n    dependencies. Features local dependence analysis through LRA and biclustering, \n    parameter estimation, dimensionality assessment, and network structure \n    visualization for educational, psychological, and social science research.  "
  },
  {
    "id": 12044,
    "package_name": "exhaustiveRasch",
    "title": "Item Selection and Exhaustive Search for Rasch Models",
    "description": "Automation of the item selection processes for Rasch scales by means of exhaustive search for suitable Rasch models (dichotomous, partial credit, rating-scale) in a list of item-combinations. The item-combinations to test can be either all possible combinations or item-combinations can be defined by several rules (forced inclusion of specific items, exclusion of combinations, minimum/maximum items of a subset of items). Tests for model fit and item fit include ordering of the thresholds, item fit-indices, likelihood ratio test, Martin-L\u00f6f test, Wald-like test, person-item distribution, person separation index, principal components of Rasch residuals, empirical representation of all raw scores or Rasch trees for detecting differential item functioning. The tests, their ordering and their parameters can be defined by the user. For parameter estimation and model tests, functions of the packages 'eRm', 'psychotools' or 'pairwise' can be used.",
    "version": "0.3.7",
    "maintainer": "Christian Grebe <grebe@hepcos.com>",
    "author": "Christian Grebe [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-1944-7422>),\n  Mirko Sch\u00fcrmann [aut] (ORCID: <https://orcid.org/0000-0003-2646-085X>),\n  Joerg-Henrik Heine [ctb],\n  Patrick Mair [ctb],\n  Thomas Rusch [ctb],\n  Reinhold Hatzinger [ctb],\n  Marco J. Maier [ctb],\n  Rudolf Debelak [ctb]",
    "url": "https://github.com/chrgrebe/exhaustiveRasch",
    "bug_reports": "https://github.com/chrgrebe/exhaustiveRasch/issues",
    "repository": "https://cran.r-project.org/package=exhaustiveRasch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exhaustiveRasch Item Selection and Exhaustive Search for Rasch Models Automation of the item selection processes for Rasch scales by means of exhaustive search for suitable Rasch models (dichotomous, partial credit, rating-scale) in a list of item-combinations. The item-combinations to test can be either all possible combinations or item-combinations can be defined by several rules (forced inclusion of specific items, exclusion of combinations, minimum/maximum items of a subset of items). Tests for model fit and item fit include ordering of the thresholds, item fit-indices, likelihood ratio test, Martin-L\u00f6f test, Wald-like test, person-item distribution, person separation index, principal components of Rasch residuals, empirical representation of all raw scores or Rasch trees for detecting differential item functioning. The tests, their ordering and their parameters can be defined by the user. For parameter estimation and model tests, functions of the packages 'eRm', 'psychotools' or 'pairwise' can be used.  "
  },
  {
    "id": 12101,
    "package_name": "eyeris",
    "title": "Flexible, Extensible, & Reproducible Pupillometry Preprocessing",
    "description": "Pupillometry offers a non-invasive window into the mind and has been used extensively as a psychophysiological readout of arousal signals linked with cognitive processes like attention, stress, and emotional states [Clewett et al. (2020) <doi:10.1038/s41467-020-17851-9>; Kret & Sjak-Shie (2018) <doi:10.3758/s13428-018-1075-y>; Strauch (2024) <doi:10.1016/j.tins.2024.06.002>]. Yet, despite decades of pupillometry research, many established packages and workflows to date lack design patterns based on Findability, Accessibility, Interoperability, and Reusability (FAIR) principles [see Wilkinson et al. (2016) <doi:10.1038/sdata.2016.18>]. 'eyeris' provides a modular, performant, and extensible preprocessing framework for pupillometry data with BIDS-like organization and interactive output reports [Esteban et al. (2019) <doi:10.1038/s41592-018-0235-4>; Gorgolewski et al. (2016) <doi:10.1038/sdata.2016.44>]. Development was supported, in part, by the Stanford Wu Tsai Human Performance Alliance, Stanford Ric Weiland Graduate Fellowship, Stanford Center for Mind, Brain, Computation and Technology, NIH National Institute on Aging Grants (R01-AG065255, R01-AG079345), NSF GRFP (DGE-2146755), McKnight Brain Research Foundation Clinical Translational Research Scholarship in Cognitive Aging and Age-Related Memory Loss, American Brain Foundation, and the American Academy of Neurology.",
    "version": "3.0.1",
    "maintainer": "Shawn Schwartz <shawn.t.schwartz@gmail.com>",
    "author": "Shawn Schwartz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6444-8451>),\n  Mingjian He [ctb],\n  Haopei Yang [ctb],\n  Alice Xue [ctb],\n  Gustavo Santiago-Reyes [ctb]",
    "url": "https://shawnschwartz.com/eyeris/,\nhttps://github.com/shawntz/eyeris/",
    "bug_reports": "https://github.com/shawntz/eyeris/issues",
    "repository": "https://cran.r-project.org/package=eyeris",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eyeris Flexible, Extensible, & Reproducible Pupillometry Preprocessing Pupillometry offers a non-invasive window into the mind and has been used extensively as a psychophysiological readout of arousal signals linked with cognitive processes like attention, stress, and emotional states [Clewett et al. (2020) <doi:10.1038/s41467-020-17851-9>; Kret & Sjak-Shie (2018) <doi:10.3758/s13428-018-1075-y>; Strauch (2024) <doi:10.1016/j.tins.2024.06.002>]. Yet, despite decades of pupillometry research, many established packages and workflows to date lack design patterns based on Findability, Accessibility, Interoperability, and Reusability (FAIR) principles [see Wilkinson et al. (2016) <doi:10.1038/sdata.2016.18>]. 'eyeris' provides a modular, performant, and extensible preprocessing framework for pupillometry data with BIDS-like organization and interactive output reports [Esteban et al. (2019) <doi:10.1038/s41592-018-0235-4>; Gorgolewski et al. (2016) <doi:10.1038/sdata.2016.44>]. Development was supported, in part, by the Stanford Wu Tsai Human Performance Alliance, Stanford Ric Weiland Graduate Fellowship, Stanford Center for Mind, Brain, Computation and Technology, NIH National Institute on Aging Grants (R01-AG065255, R01-AG079345), NSF GRFP (DGE-2146755), McKnight Brain Research Foundation Clinical Translational Research Scholarship in Cognitive Aging and Age-Related Memory Loss, American Brain Foundation, and the American Academy of Neurology.  "
  },
  {
    "id": 12106,
    "package_name": "ezCutoffs",
    "title": "Fit Measure Cutoffs in SEM",
    "description": "Calculate cutoff values for model fit measures used in structural equation modeling (SEM) by simulating and testing data sets (cf. Hu & Bentler, 1999 <doi:10.1080/10705519909540118>) with the same parameters (population model, number of observations, etc.) as the model under consideration.",
    "version": "1.0.2",
    "maintainer": "Bjarne Schmalbach <bjarne.schmalbach@gmail.com>",
    "author": "Bjarne Schmalbach [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6853-412X>),\n  Julien Patrick Irmer [aut],\n  Martin Schultze [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ezCutoffs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ezCutoffs Fit Measure Cutoffs in SEM Calculate cutoff values for model fit measures used in structural equation modeling (SEM) by simulating and testing data sets (cf. Hu & Bentler, 1999 <doi:10.1080/10705519909540118>) with the same parameters (population model, number of observations, etc.) as the model under consideration.  "
  },
  {
    "id": 12160,
    "package_name": "factorH",
    "title": "Multifactor Nonparametric Rank-Based ANOVA with Post Hoc Tests",
    "description": "Multifactor nonparametric analysis of variance based on ranks.\n    Builds on the Kruskal-Wallis H test and its 2x2 Scheirer-Ray-Hare\n    extension to handle any factorial designs. Provides effect sizes,\n    Dunn-Bonferroni pairwise-comparison matrices, and simple-effects\n    analyses. Tailored for psychology and the social sciences, with\n    beginner-friendly R syntax and outputs that can be dropped into\n    journal reports. Includes helpers to export tab-separated results and\n    compact tables of descriptive statistics (to APA-style reports).",
    "version": "0.5.0",
    "maintainer": "Tomasz Rak <tomasz.rak@upjp2.edu.pl>",
    "author": "Tomasz Rak [aut, cre],\n  Szymon Wrzesniowski [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=factorH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "factorH Multifactor Nonparametric Rank-Based ANOVA with Post Hoc Tests Multifactor nonparametric analysis of variance based on ranks.\n    Builds on the Kruskal-Wallis H test and its 2x2 Scheirer-Ray-Hare\n    extension to handle any factorial designs. Provides effect sizes,\n    Dunn-Bonferroni pairwise-comparison matrices, and simple-effects\n    analyses. Tailored for psychology and the social sciences, with\n    beginner-friendly R syntax and outputs that can be dropped into\n    journal reports. Includes helpers to export tab-separated results and\n    compact tables of descriptive statistics (to APA-style reports).  "
  },
  {
    "id": 12171,
    "package_name": "fairGATE",
    "title": "Fair Gated Algorithm for Targeted Equity",
    "description": "Tools for training and analysing fairness-aware gated neural \n    networks for subgroup-aware prediction and interpretation in clinical datasets. \n    Methods draw on prior work in mixture-of-experts neural networks by\n    Jordan and Jacobs (1994) <doi:10.1007/978-1-4471-2097-1_113>,\n    fairness-aware learning by Hardt, Price, and Srebro (2016) <doi:10.48550/arXiv.1610.02413>,\n    and personalised treatment prediction for depression by Iniesta, Stahl, and McGuffin (2016) \n    <doi:10.1016/j.jpsychires.2016.03.016>.",
    "version": "0.1.1",
    "maintainer": "Rhys Holland <rhys.holland@icloud.com>",
    "author": "Rhys Holland [aut, cre],\n  Raquel Iniesta [aut]",
    "url": "https://github.com/rhysholland/FairGATE",
    "bug_reports": "https://github.com/rhysholland/FairGate/issues",
    "repository": "https://cran.r-project.org/package=fairGATE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fairGATE Fair Gated Algorithm for Targeted Equity Tools for training and analysing fairness-aware gated neural \n    networks for subgroup-aware prediction and interpretation in clinical datasets. \n    Methods draw on prior work in mixture-of-experts neural networks by\n    Jordan and Jacobs (1994) <doi:10.1007/978-1-4471-2097-1_113>,\n    fairness-aware learning by Hardt, Price, and Srebro (2016) <doi:10.48550/arXiv.1610.02413>,\n    and personalised treatment prediction for depression by Iniesta, Stahl, and McGuffin (2016) \n    <doi:10.1016/j.jpsychires.2016.03.016>.  "
  },
  {
    "id": 12172,
    "package_name": "fairGNN",
    "title": "Fairness-Aware Gated Neural Networks",
    "description": "Tools for training and analysing fairness-aware gated neural \n    networks for subgroup-aware prediction and interpretation in clinical datasets. \n    Methods draw on prior work in mixture-of-experts neural networks by\n    Jordan and Jacobs (1994) <doi:10.1007/978-1-4471-2097-1_113>,\n    fairness-aware learning by Hardt, Price, and Srebro (2016) <doi:10.48550/arXiv.1610.02413>,\n    and personalised treatment prediction for depression by Iniesta, Stahl, and McGuffin (2016) \n    <doi:10.1016/j.jpsychires.2016.03.016>.",
    "version": "0.1.0",
    "maintainer": "Rhys Holland <rhys.holland@icloud.com>",
    "author": "Rhys Holland [aut, cre]",
    "url": "https://github.com/rhysholland/fairGNN",
    "bug_reports": "https://github.com/rhysholland/fairGNN/issues",
    "repository": "https://cran.r-project.org/package=fairGNN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fairGNN Fairness-Aware Gated Neural Networks Tools for training and analysing fairness-aware gated neural \n    networks for subgroup-aware prediction and interpretation in clinical datasets. \n    Methods draw on prior work in mixture-of-experts neural networks by\n    Jordan and Jacobs (1994) <doi:10.1007/978-1-4471-2097-1_113>,\n    fairness-aware learning by Hardt, Price, and Srebro (2016) <doi:10.48550/arXiv.1610.02413>,\n    and personalised treatment prediction for depression by Iniesta, Stahl, and McGuffin (2016) \n    <doi:10.1016/j.jpsychires.2016.03.016>.  "
  },
  {
    "id": 12293,
    "package_name": "fcm",
    "title": "Inference of Fuzzy Cognitive Maps (FCMs)",
    "description": "Provides a selection of 3 different inference rules (including additionally the clamped types of the referred inference rules) and 4 threshold functions in order to obtain the inference of the FCM (Fuzzy Cognitive Map). Moreover, the 'fcm' package returns a data frame of the concepts' values of each state after the inference procedure. Fuzzy cognitive maps were introduced by Kosko (1986) <doi:10.1002/int.4550010405> providing ideal causal cognition tools for modeling and simulating dynamic systems.",
    "version": "0.1.3",
    "maintainer": "Zoumpoulia Dikopoulou <dikopoulia@gmail.com>",
    "author": "Zoumpoulia Dikopoulou <dikopoulia@gmail.com> <zoumpolia.dikopoulou@uhasselt.be>, Elpiniki Papageorgiou <epapageorgiou@teiste.gr>, <e.i.papageorgiou75@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fcm Inference of Fuzzy Cognitive Maps (FCMs) Provides a selection of 3 different inference rules (including additionally the clamped types of the referred inference rules) and 4 threshold functions in order to obtain the inference of the FCM (Fuzzy Cognitive Map). Moreover, the 'fcm' package returns a data frame of the concepts' values of each state after the inference procedure. Fuzzy cognitive maps were introduced by Kosko (1986) <doi:10.1002/int.4550010405> providing ideal causal cognition tools for modeling and simulating dynamic systems.  "
  },
  {
    "id": 12300,
    "package_name": "fda",
    "title": "Functional Data Analysis",
    "description": "These functions were developed to support functional data\n analysis as described in Ramsay, J. O. and Silverman, B. W.\n (2005) Functional Data Analysis. New York: Springer and in \n Ramsay, J. O., Hooker, Giles, and Graves, Spencer (2009). \n Functional Data Analysis with R and Matlab (Springer). \n The package includes data sets and script files working many examples \n including all but one of the 76 figures in this latter book.  Matlab versions \n are available by ftp from \n <https://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/>.",
    "version": "6.3.0",
    "maintainer": "James Ramsay <ramsay@psych.mcgill.ca>",
    "author": "James Ramsay [aut, cre],\n  Giles Hooker [ctb],\n  Spencer Graves [ctb]",
    "url": "http://www.functionaldata.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fda Functional Data Analysis These functions were developed to support functional data\n analysis as described in Ramsay, J. O. and Silverman, B. W.\n (2005) Functional Data Analysis. New York: Springer and in \n Ramsay, J. O., Hooker, Giles, and Graves, Spencer (2009). \n Functional Data Analysis with R and Matlab (Springer). \n The package includes data sets and script files working many examples \n including all but one of the 76 figures in this latter book.  Matlab versions \n are available by ftp from \n <https://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/>.  "
  },
  {
    "id": 12476,
    "package_name": "flankr",
    "title": "Implementing Computational Models of Attentional Selectivity",
    "description": "A set of methods to simulate from and fit computational models of\n    attentional selectivity. The package implements the dual-stage two-phase\n    (DSTP) model of H\u00fcbner et al. (2010) <doi:10.1037/a0019471>, and the\n    shrinking spotlight (SSP) model of White et al. (2011)\n    <doi:10.1016/j.cogpsych.2011.08.001>.",
    "version": "1.2.0",
    "maintainer": "Jim Grange <grange.jim@gmail.com>",
    "author": "Jim Grange [aut, cre] (ORCID: <https://orcid.org/0000-0002-8352-8390>)",
    "url": "https://github.com/JimGrange/flankr",
    "bug_reports": "https://github.com/JimGrange/flankr/issues",
    "repository": "https://cran.r-project.org/package=flankr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flankr Implementing Computational Models of Attentional Selectivity A set of methods to simulate from and fit computational models of\n    attentional selectivity. The package implements the dual-stage two-phase\n    (DSTP) model of H\u00fcbner et al. (2010) <doi:10.1037/a0019471>, and the\n    shrinking spotlight (SSP) model of White et al. (2011)\n    <doi:10.1016/j.cogpsych.2011.08.001>.  "
  },
  {
    "id": 12742,
    "package_name": "fuel",
    "title": "Framework for Unified Estimation in Lognormal Models",
    "description": "Lognormal models have broad applications in various research areas such as economics, actuarial science, biology, environmental science and psychology. The estimation problem in lognormal models has been extensively studied. This R package 'fuel' implements thirty-nine existing and newly proposed estimators. See Zhang, F., and Gou, J. (2020), A unified framework for estimation in lognormal models, Technical report. ",
    "version": "1.2.0",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou and Fengqing (Zoe) Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fuel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fuel Framework for Unified Estimation in Lognormal Models Lognormal models have broad applications in various research areas such as economics, actuarial science, biology, environmental science and psychology. The estimation problem in lognormal models has been extensively studied. This R package 'fuel' implements thirty-nine existing and newly proposed estimators. See Zhang, F., and Gou, J. (2020), A unified framework for estimation in lognormal models, Technical report.   "
  },
  {
    "id": 12746,
    "package_name": "fullROC",
    "title": "Plot Full ROC Curves using Eyewitness Lineup Data",
    "description": "Enable researchers to adjust identification rates using the 1/(lineup size) method, generate the full receiver operating characteristic (ROC) curves, and statistically compare the area under the curves (AUC). \n  References: Yueran Yang & Andrew Smith. (2020). \"fullROC: An R package for generating and analyzing eyewitness-lineup ROC curves\". <doi:10.13140/RG.2.2.20415.94885/1>  ,\n              Andrew Smith, Yueran Yang, & Gary Wells. (2020). \"Distinguishing between investigator discriminability and eyewitness discriminability: A method for creating full receiver operating characteristic curves of lineup identification performance\". Perspectives on Psychological Science, 15(3), 589-607. <doi:10.1177/1745691620902426>.",
    "version": "0.1.0",
    "maintainer": "Yueran Yang <yuerany@unr.edu>",
    "author": "Yueran Yang [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/yuerany/fullROC/issues",
    "repository": "https://cran.r-project.org/package=fullROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fullROC Plot Full ROC Curves using Eyewitness Lineup Data Enable researchers to adjust identification rates using the 1/(lineup size) method, generate the full receiver operating characteristic (ROC) curves, and statistically compare the area under the curves (AUC). \n  References: Yueran Yang & Andrew Smith. (2020). \"fullROC: An R package for generating and analyzing eyewitness-lineup ROC curves\". <doi:10.13140/RG.2.2.20415.94885/1>  ,\n              Andrew Smith, Yueran Yang, & Gary Wells. (2020). \"Distinguishing between investigator discriminability and eyewitness discriminability: A method for creating full receiver operating characteristic curves of lineup identification performance\". Perspectives on Psychological Science, 15(3), 589-607. <doi:10.1177/1745691620902426>.  "
  },
  {
    "id": 12767,
    "package_name": "funmediation",
    "title": "Functional Mediation for a Distal Outcome",
    "description": "Fits a functional mediation model with a scalar distal outcome. The method is described in detail by Coffman, Dziak, Litson, Chakraborti, Piper & Li (2021) <arXiv:2112.03960>. The model is similar to that of Lindquist (2012) <doi:10.1080/01621459.2012.695640> although allowing a binary outcome as an alternative to a numerical outcome.  The current version is a minor bug fix in the vignette. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.",
    "version": "1.0.2",
    "maintainer": "John J. Dziak <dziakj1@gmail.com>",
    "author": "John J. Dziak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0762-5495>),\n  Donna L. Coffman [aut] (ORCID: <https://orcid.org/0000-0001-6305-6579>),\n  Kaylee Litson [aut],\n  Yajnaseni Chakraborti [aut],\n  Runze Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=funmediation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "funmediation Functional Mediation for a Distal Outcome Fits a functional mediation model with a scalar distal outcome. The method is described in detail by Coffman, Dziak, Litson, Chakraborti, Piper & Li (2021) <arXiv:2112.03960>. The model is similar to that of Lindquist (2012) <doi:10.1080/01621459.2012.695640> although allowing a binary outcome as an alternative to a numerical outcome.  The current version is a minor bug fix in the vignette. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.  "
  },
  {
    "id": 13089,
    "package_name": "geopsych",
    "title": "Methods of Applied Psychology and Psychometrics in Geographical\nAnalysis",
    "description": "Integrating applied psychological and psychometric methods into geographical analysis. With the emergence of geo-referenced questionnaires, spatially explicit psychological and psychometric methods can offer a geographically contextualised approach that reflects latent traits and processes at a more local scale, leading to more tailored research and decision-making processes. The implemented methods include Geographically Weighted Cronbach's alpha and its bandwidth selection. See Zhang & Li (2025) <doi:10.1111/gean.70021>.",
    "version": "0.1.0",
    "maintainer": "Sui Zhang <sui.zhang@manchester.ac.uk>",
    "author": "Sui Zhang [aut, cre] (ORCID: <https://orcid.org/0000-0002-4143-2331>)",
    "url": "https://github.com/ZhangSui921/geopsych",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geopsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geopsych Methods of Applied Psychology and Psychometrics in Geographical\nAnalysis Integrating applied psychological and psychometric methods into geographical analysis. With the emergence of geo-referenced questionnaires, spatially explicit psychological and psychometric methods can offer a geographically contextualised approach that reflects latent traits and processes at a more local scale, leading to more tailored research and decision-making processes. The implemented methods include Geographically Weighted Cronbach's alpha and its bandwidth selection. See Zhang & Li (2025) <doi:10.1111/gean.70021>.  "
  },
  {
    "id": 13196,
    "package_name": "ggdmc",
    "title": "Cognitive Models",
    "description": "Hierarchical Bayesian models. The package provides tools to fit two response time models, using the population-based Markov Chain Monte Carlo. ",
    "version": "0.2.6.2",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "author": "Yi-Shin Lin [aut, cre],\n  Andrew Heathcote [aut]",
    "url": "https://github.com/yxlin/ggdmc",
    "bug_reports": "https://github.com/yxlin/ggdmc/issues",
    "repository": "https://cran.r-project.org/package=ggdmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggdmc Cognitive Models Hierarchical Bayesian models. The package provides tools to fit two response time models, using the population-based Markov Chain Monte Carlo.   "
  },
  {
    "id": 13197,
    "package_name": "ggdmcHeaders",
    "title": "'C++' Headers for 'ggdmc' Package",
    "description": "A fast 'C++' implementation of the design-based, Diffusion Decision Model (DDM) and the Linear Ballistic Accumulation (LBA) model. It enables the user to optimise the choice response time model by connecting with the Differential Evolution Markov Chain Monte Carlo (DE-MCMC) sampler implemented in the 'ggdmc' package. The package fuses the hierarchical modelling, Bayesian inference, choice response time models and factorial designs, allowing users to build their own design-based models. For more information on the underlying models, see the works by Voss, Rothermund, and Voss (2004) <doi:10.3758/BF03196893>, Ratcliff and McKoon (2008) <doi:10.1162/neco.2008.12-06-420>, and Brown and Heathcote (2008) <doi:10.1016/j.cogpsych.2007.12.002>.",
    "version": "0.2.9.1",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "author": "Yi-Shin Lin [aut, cre]",
    "url": "https://github.com/yxlin/ggdmcHeaders",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggdmcHeaders",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggdmcHeaders 'C++' Headers for 'ggdmc' Package A fast 'C++' implementation of the design-based, Diffusion Decision Model (DDM) and the Linear Ballistic Accumulation (LBA) model. It enables the user to optimise the choice response time model by connecting with the Differential Evolution Markov Chain Monte Carlo (DE-MCMC) sampler implemented in the 'ggdmc' package. The package fuses the hierarchical modelling, Bayesian inference, choice response time models and factorial designs, allowing users to build their own design-based models. For more information on the underlying models, see the works by Voss, Rothermund, and Voss (2004) <doi:10.3758/BF03196893>, Ratcliff and McKoon (2008) <doi:10.1162/neco.2008.12-06-420>, and Brown and Heathcote (2008) <doi:10.1016/j.cogpsych.2007.12.002>.  "
  },
  {
    "id": 13198,
    "package_name": "ggdmcLikelihood",
    "title": "Likelihood Computation for 'ggdmc' Package",
    "description": "Efficient computation of likelihoods in design-based choice response time models, including the Decision Diffusion Model, is supported. The package enables rapid evaluation of likelihood functions for both single- and multi-subject models across trial-level data. It also offers fast initialisation of starting parameters for genetic sampling with many Markov chains, facilitating estimation in complex models typically found in experimental psychology and behavioural science. These optimisations help reduce computational overhead in large-scale model fitting tasks.",
    "version": "0.2.9.0",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "author": "Yi-Shin Lin [aut, cre]",
    "url": "https://github.com/yxlin/ggdmcLikelihood",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggdmcLikelihood",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggdmcLikelihood Likelihood Computation for 'ggdmc' Package Efficient computation of likelihoods in design-based choice response time models, including the Decision Diffusion Model, is supported. The package enables rapid evaluation of likelihood functions for both single- and multi-subject models across trial-level data. It also offers fast initialisation of starting parameters for genetic sampling with many Markov chains, facilitating estimation in complex models typically found in experimental psychology and behavioural science. These optimisations help reduce computational overhead in large-scale model fitting tasks.  "
  },
  {
    "id": 13405,
    "package_name": "glba",
    "title": "General Linear Ballistic Accumulator Models",
    "description": "Analyses response times and accuracies from psychological experiments with the linear ballistic accumulator (LBA) model from Brown and Heathcote (2008). The LBA model is optionally fitted with explanatory variables on the parameters such as the drift rate, the boundary and the starting point parameters. A log-link function on the linear predictors can be used to ensure that parameters remain positive when needed.  ",
    "version": "0.2.1",
    "maintainer": "Ingmar Visser <i.visser@uva.nl>",
    "author": "Ingmar Visser",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glba General Linear Ballistic Accumulator Models Analyses response times and accuracies from psychological experiments with the linear ballistic accumulator (LBA) model from Brown and Heathcote (2008). The LBA model is optionally fitted with explanatory variables on the parameters such as the drift rate, the boundary and the starting point parameters. A log-link function on the linear predictors can be used to ensure that parameters remain positive when needed.    "
  },
  {
    "id": 13655,
    "package_name": "grmsem",
    "title": "Genetic-Relationship-Matrix Structural Equation Modelling\n(GRMSEM)",
    "description": "Quantitative genetics tool supporting the modelling of multivariate \n genetic variance structures in quantitative data. It allows fitting different \n models through multivariate genetic-relationship-matrix (GRM) \n structural equation modelling (SEM) in unrelated individuals, \n using a maximum likelihood approach. Specifically, \n it combines genome-wide genotyping information, as captured by GRMs, \n with twin-research-based SEM techniques, \n St Pourcain et al. (2017) <doi:10.1016/j.biopsych.2017.09.020>, \n Shapland et al. (2020) <doi:10.1101/2020.08.14.251199>.",
    "version": "1.1.0",
    "maintainer": "Beate StPourcain <Beate.StPourcain@mpi.nl>",
    "author": "Beate StPourcain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4680-3517>),\n  Alexander Klassmann [ctb]",
    "url": "https://CRAN.R-project.org/package=grmsem,\nhttps://gitlab.gwdg.de/beate.stpourcain/grmsem",
    "bug_reports": "https://gitlab.gwdg.de/beate.stpourcain/grmsem/-/issues",
    "repository": "https://cran.r-project.org/package=grmsem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grmsem Genetic-Relationship-Matrix Structural Equation Modelling\n(GRMSEM) Quantitative genetics tool supporting the modelling of multivariate \n genetic variance structures in quantitative data. It allows fitting different \n models through multivariate genetic-relationship-matrix (GRM) \n structural equation modelling (SEM) in unrelated individuals, \n using a maximum likelihood approach. Specifically, \n it combines genome-wide genotyping information, as captured by GRMs, \n with twin-research-based SEM techniques, \n St Pourcain et al. (2017) <doi:10.1016/j.biopsych.2017.09.020>, \n Shapland et al. (2020) <doi:10.1101/2020.08.14.251199>.  "
  },
  {
    "id": 13691,
    "package_name": "grt",
    "title": "General Recognition Theory",
    "description": "Functions to generate and analyze data for psychology experiments based on the General Recognition Theory.",
    "version": "0.2.1",
    "maintainer": "Andy Wills <andy@willslab.co.uk>",
    "author": "Kazunaga Matsuki",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=grt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grt General Recognition Theory Functions to generate and analyze data for psychology experiments based on the General Recognition Theory.  "
  },
  {
    "id": 13885,
    "package_name": "heatindex",
    "title": "Calculating Heat Stress",
    "description": "Implements the simpler and faster heat index, which matches the values of the original 1979 heat index and its 2022 extension for air temperatures above 300 K (27 C, 80 F) and with only minor differences at lower temperatures. Also implements an algorithm for calculating the thermodynamic (and psychrometric) wet-bulb (and ice-bulb) temperature.",
    "version": "0.0.2",
    "maintainer": "David M. Romps <romps@berkeley.edu>",
    "author": "Yi-Chuan Lu [aut] (ORCID: <https://orcid.org/0000-0003-3659-1474>),\n  David M. Romps [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7649-5175>)",
    "url": "https://heatindex.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=heatindex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "heatindex Calculating Heat Stress Implements the simpler and faster heat index, which matches the values of the original 1979 heat index and its 2022 extension for air temperatures above 300 K (27 C, 80 F) and with only minor differences at lower temperatures. Also implements an algorithm for calculating the thermodynamic (and psychrometric) wet-bulb (and ice-bulb) temperature.  "
  },
  {
    "id": 13980,
    "package_name": "higlasso",
    "title": "Hierarchical Integrative Group LASSO",
    "description": "\n    Environmental health studies are increasingly measuring multiple pollutants\n    to characterize the joint health effects attributable to exposure mixtures.\n    However, the underlying dose-response relationship between toxicants and\n    health outcomes of interest may be highly nonlinear, with possible nonlinear\n    interaction effects. Hierarchical integrative group least absolute shrinkage\n    and selection operator (HiGLASSO), developed by Boss et al (2020)\n    <arXiv:2003.12844>, is a general framework to identify noteworthy nonlinear \n    main and interaction effects in the presence of group structures among a set\n    of exposures.",
    "version": "0.9.0",
    "maintainer": "Alexander Rix <alexrix@umich.edu>",
    "author": "Alexander Rix [aut, cre],\n  Jonathan Boss [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=higlasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "higlasso Hierarchical Integrative Group LASSO \n    Environmental health studies are increasingly measuring multiple pollutants\n    to characterize the joint health effects attributable to exposure mixtures.\n    However, the underlying dose-response relationship between toxicants and\n    health outcomes of interest may be highly nonlinear, with possible nonlinear\n    interaction effects. Hierarchical integrative group least absolute shrinkage\n    and selection operator (HiGLASSO), developed by Boss et al (2020)\n    <arXiv:2003.12844>, is a general framework to identify noteworthy nonlinear \n    main and interaction effects in the presence of group structures among a set\n    of exposures.  "
  },
  {
    "id": 14009,
    "package_name": "hmcdm",
    "title": "Hidden Markov Cognitive Diagnosis Models for Learning",
    "description": "Fitting hidden Markov models of learning under the cognitive diagnosis framework.\n  The estimation of the hidden Markov diagnostic classification model,\n  the first order hidden Markov model, the reduced-reparameterized unified learning model,\n  and the joint learning model for responses and response times.",
    "version": "2.1.2",
    "maintainer": "Sunbeom Kwon <sunbeom2@illinois.edu>",
    "author": "Susu Zhang [aut],\n  Shiyu Wang [aut],\n  Yinghan Chen [aut],\n  Sunbeom Kwon [aut, cre]",
    "url": "https://github.com/tmsalab/hmcdm",
    "bug_reports": "https://github.com/tmsalab/hmcdm/issues",
    "repository": "https://cran.r-project.org/package=hmcdm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hmcdm Hidden Markov Cognitive Diagnosis Models for Learning Fitting hidden Markov models of learning under the cognitive diagnosis framework.\n  The estimation of the hidden Markov diagnostic classification model,\n  the first order hidden Markov model, the reduced-reparameterized unified learning model,\n  and the joint learning model for responses and response times.  "
  },
  {
    "id": 14026,
    "package_name": "holland",
    "title": "Statistics for Holland's Theory of Vocational Choice",
    "description": "Offers a convenient way to compute parameters in the framework of the theory of vocational choice introduced by J.L. Holland, (1997). A comprehensive summary to this theory  of vocational choice is given in Holland, J.L. (1997). Making vocational choices. A theory of vocational personalities and work environments.  Lutz, FL: Psychological Assessment.",
    "version": "0.1.2-4",
    "maintainer": "Joerg-Henrik Heine <jhheine@googlemail.com>",
    "author": "Joerg-Henrik Heine [aut, cre, com],\n  Florian Hartmann [aut, com],\n  Bernhard Ertl [rev, com],\n  Christian Tarnai [com, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=holland",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "holland Statistics for Holland's Theory of Vocational Choice Offers a convenient way to compute parameters in the framework of the theory of vocational choice introduced by J.L. Holland, (1997). A comprehensive summary to this theory  of vocational choice is given in Holland, J.L. (1997). Making vocational choices. A theory of vocational personalities and work environments.  Lutz, FL: Psychological Assessment.  "
  },
  {
    "id": 14039,
    "package_name": "hornpa",
    "title": "Horn's (1965) Test to Determine the Number of Components/Factors",
    "description": "A stand-alone function that generates a user specified number of random datasets and computes eigenvalues using the random datasets (i.e., implements Horn's [1965, Psychometrika] parallel analysis <doi:10.1007/BF02289447>). Users then compare the resulting eigenvalues (the mean or the specified percentile) from the random datasets (i.e., eigenvalues resulting from noise) to the eigenvalues generated with the user's data. Can be used for both principal components analysis (PCA) and common/exploratory factor analysis (EFA). The output table shows how large eigenvalues can be as a result of merely using randomly generated datasets. If the user's own dataset has  actual eigenvalues greater than the corresponding eigenvalues, that lends support to retain that factor/component. In other words, if the i(th) eigenvalue from the actual data was larger than the percentile of the (i)th eigenvalue generated using randomly generated data, empirical support is provided to retain that factor/component. Horn, J. (1965). A rationale and test for the number of factors in factor analysis. Psychometrika, 32, 179-185.",
    "version": "1.1.1",
    "maintainer": "Francis Huang <huangf@missouri.edu>",
    "author": "Francis Huang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hornpa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hornpa Horn's (1965) Test to Determine the Number of Components/Factors A stand-alone function that generates a user specified number of random datasets and computes eigenvalues using the random datasets (i.e., implements Horn's [1965, Psychometrika] parallel analysis <doi:10.1007/BF02289447>). Users then compare the resulting eigenvalues (the mean or the specified percentile) from the random datasets (i.e., eigenvalues resulting from noise) to the eigenvalues generated with the user's data. Can be used for both principal components analysis (PCA) and common/exploratory factor analysis (EFA). The output table shows how large eigenvalues can be as a result of merely using randomly generated datasets. If the user's own dataset has  actual eigenvalues greater than the corresponding eigenvalues, that lends support to retain that factor/component. In other words, if the i(th) eigenvalue from the actual data was larger than the percentile of the (i)th eigenvalue generated using randomly generated data, empirical support is provided to retain that factor/component. Horn, J. (1965). A rationale and test for the number of factors in factor analysis. Psychometrika, 32, 179-185.  "
  },
  {
    "id": 14107,
    "package_name": "hurricaneexposure",
    "title": "Explore and Map County-Level Hurricane Exposure in the United\nStates",
    "description": "Allows users to create time series of tropical storm\n    exposure histories for chosen counties for a number of hazard metrics\n    (wind, rain, distance from the storm, etc.). This package interacts\n    with data available through the 'hurricaneexposuredata' package, which\n    is available in a 'drat' repository. To access this data package, see the \n    instructions at <https://github.com/geanders/hurricaneexposure>. \n    The size of the 'hurricaneexposuredata' package is\n    approximately 20 MB. This work was supported in part by grants from the National\n    Institute of Environmental Health Sciences (R00ES022631), the National Science\n    Foundation (1331399), and a NASA Applied Sciences Program/Public Health Program\n    Grant (NNX09AV81G).",
    "version": "0.1.1",
    "maintainer": "Brooke Anderson <brooke.anderson@colostate.edu>",
    "author": "Brooke Anderson [aut, cre],\n  Meilin Yan [aut],\n  Joshua Ferreri [aut],\n  William Crosson [ctb],\n  Mohammad Al-Hamdan [ctb],\n  Andrea Schumacher [ctb],\n  Dirk Eddelbuettel [ctb]",
    "url": "https://github.com/geanders/hurricaneexposure",
    "bug_reports": "https://github.com/geanders/hurricaneexposure/issues",
    "repository": "https://cran.r-project.org/package=hurricaneexposure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hurricaneexposure Explore and Map County-Level Hurricane Exposure in the United\nStates Allows users to create time series of tropical storm\n    exposure histories for chosen counties for a number of hazard metrics\n    (wind, rain, distance from the storm, etc.). This package interacts\n    with data available through the 'hurricaneexposuredata' package, which\n    is available in a 'drat' repository. To access this data package, see the \n    instructions at <https://github.com/geanders/hurricaneexposure>. \n    The size of the 'hurricaneexposuredata' package is\n    approximately 20 MB. This work was supported in part by grants from the National\n    Institute of Environmental Health Sciences (R00ES022631), the National Science\n    Foundation (1331399), and a NASA Applied Sciences Program/Public Health Program\n    Grant (NNX09AV81G).  "
  },
  {
    "id": 14332,
    "package_name": "imagefluency",
    "title": "Image Statistics Based on Processing Fluency",
    "description": "Get image statistics based on processing fluency theory. The\n    functions provide scores for several basic aesthetic principles that\n    facilitate fluent cognitive processing of images: contrast,\n    complexity / simplicity, self-similarity, symmetry, and typicality.\n    See Mayer & Landwehr (2018) <doi:10.1037/aca0000187> and Mayer & Landwehr\n    (2018) <doi:10.31219/osf.io/gtbhw> for the theoretical background of the methods.",
    "version": "0.2.5",
    "maintainer": "Stefan Mayer <stefan@mayer-de.com>",
    "author": "Stefan Mayer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0034-7090>)",
    "url": "https://imagefluency.com, https://github.com/stm/imagefluency/,\nhttps://doi.org/10.5281/zenodo.5614665",
    "bug_reports": "https://github.com/stm/imagefluency/issues/",
    "repository": "https://cran.r-project.org/package=imagefluency",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imagefluency Image Statistics Based on Processing Fluency Get image statistics based on processing fluency theory. The\n    functions provide scores for several basic aesthetic principles that\n    facilitate fluent cognitive processing of images: contrast,\n    complexity / simplicity, self-similarity, symmetry, and typicality.\n    See Mayer & Landwehr (2018) <doi:10.1037/aca0000187> and Mayer & Landwehr\n    (2018) <doi:10.31219/osf.io/gtbhw> for the theoretical background of the methods.  "
  },
  {
    "id": 14420,
    "package_name": "infinitefactor",
    "title": "Bayesian Infinite Factor Models",
    "description": "Sampler and post-processing functions for semi-parametric Bayesian infinite factor models, motivated by the Multiplicative Gamma Shrinkage Prior of Bhattacharya and Dunson (2011) <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3419391/>. Contains component C++ functions for building samplers for linear and 2-way interaction factor models using the multiplicative gamma and Dirichlet-Laplace shrinkage priors. The package also contains post processing functions to return matrices that display rotational ambiguity to identifiability through successive application of orthogonalization procedures and resolution of column label and sign switching. This package was developed with the support of the National Institute of Environmental Health Sciences grant 1R01ES028804-01.",
    "version": "1.0",
    "maintainer": "Evan Poworoznek <infinitefactorpackage@gmail.com>",
    "author": "Evan Poworoznek",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=infinitefactor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "infinitefactor Bayesian Infinite Factor Models Sampler and post-processing functions for semi-parametric Bayesian infinite factor models, motivated by the Multiplicative Gamma Shrinkage Prior of Bhattacharya and Dunson (2011) <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3419391/>. Contains component C++ functions for building samplers for linear and 2-way interaction factor models using the multiplicative gamma and Dirichlet-Laplace shrinkage priors. The package also contains post processing functions to return matrices that display rotational ambiguity to identifiability through successive application of orthogonalization procedures and resolution of column label and sign switching. This package was developed with the support of the National Institute of Environmental Health Sciences grant 1R01ES028804-01.  "
  },
  {
    "id": 14504,
    "package_name": "intervalpsych",
    "title": "Analyzing Interval Data in Psychometrics",
    "description": "Implements the Interval Consensus Model (ICM) for analyzing continuous bounded interval-valued responses in psychometrics using 'Stan' for 'Bayesian' estimation. Provides functions for transforming interval data to simplex representations, fitting item response theory (IRT) models with isometric log-ratio (ILR) and sum log-ratio (SLR) link functions, and visualizing results. The package enables aggregation and analysis of interval-valued response data commonly found in psychological measurement and related disciplines. Based on Kloft et al. (2024) <doi:10.31234/osf.io/dzvw2>.",
    "version": "0.1.0",
    "maintainer": "Matthias Kloft <kloft.dev+intervalpsych@gmail.com>",
    "author": "Matthias Kloft [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1845-6957>),\n  Bj\u00f6rn S. Siepe [aut] (ORCID: <https://orcid.org/0000-0002-9558-4648>)",
    "url": "https://matthiaskloft.github.io/intervalpsych/,\nhttps://github.com/matthiaskloft/intervalpsych",
    "bug_reports": "https://github.com/matthiaskloft/intervalpsych/issues",
    "repository": "https://cran.r-project.org/package=intervalpsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intervalpsych Analyzing Interval Data in Psychometrics Implements the Interval Consensus Model (ICM) for analyzing continuous bounded interval-valued responses in psychometrics using 'Stan' for 'Bayesian' estimation. Provides functions for transforming interval data to simplex representations, fitting item response theory (IRT) models with isometric log-ratio (ILR) and sum log-ratio (SLR) link functions, and visualizing results. The package enables aggregation and analysis of interval-valued response data commonly found in psychological measurement and related disciplines. Based on Kloft et al. (2024) <doi:10.31234/osf.io/dzvw2>.  "
  },
  {
    "id": 14535,
    "package_name": "iopsych",
    "title": "Methods for Industrial/Organizational Psychology",
    "description": "Collection of functions for IO Psychologists.",
    "version": "0.90.1",
    "maintainer": "Allen Goebl <goebl005@umn.edu>",
    "author": "Allen Goebl <goebl005@umn.edu>, Jeff Jones <jone1087@umn.ed>, and Adam Beatty <abeatty@humrro.org>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iopsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iopsych Methods for Industrial/Organizational Psychology Collection of functions for IO Psychologists.  "
  },
  {
    "id": 14588,
    "package_name": "irt",
    "title": "Item Response Theory and Computerized Adaptive Testing Functions",
    "description": "A collection of Item Response Theory (IRT) and Computerized \n     Adaptive Testing (CAT) functions that are used in psychometrics. ",
    "version": "0.2.9",
    "maintainer": "Emre Gonulates <egonulates@gmail.com>",
    "author": "Emre Gonulates [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3834-3266>)",
    "url": "https://github.com/egonulates/irt",
    "bug_reports": "https://github.com/egonulates/irt/issues",
    "repository": "https://cran.r-project.org/package=irt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "irt Item Response Theory and Computerized Adaptive Testing Functions A collection of Item Response Theory (IRT) and Computerized \n     Adaptive Testing (CAT) functions that are used in psychometrics.   "
  },
  {
    "id": 14723,
    "package_name": "jmetrik",
    "title": "Tools for Interacting with 'jMetrik'",
    "description": "The main purpose of this package is to make it easy for userR's to interact with 'jMetrik' an open source application for psychometric analysis. For example it allows useR's to write data frames to file in a format that can be used by 'jMetrik'. It also allows useR's to read *.jmetrik files (e.g. output from an analysis) for follow-up analysis in R. The *.jmetrik format is a flat file that includes a multiline header and the data as comma separated values. The header includes metadata about the file and one row per variable with the following information in each row: variable name, data type, item scoring, special data codes, and variable label. ",
    "version": "1.1",
    "maintainer": "J. Patrick Meyer <meyerjp3@gmail.com>",
    "author": "J. Patrick Meyer <support@itemanalysis.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jmetrik",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jmetrik Tools for Interacting with 'jMetrik' The main purpose of this package is to make it easy for userR's to interact with 'jMetrik' an open source application for psychometric analysis. For example it allows useR's to write data frames to file in a format that can be used by 'jMetrik'. It also allows useR's to read *.jmetrik files (e.g. output from an analysis) for follow-up analysis in R. The *.jmetrik format is a flat file that includes a multiline header and the data as comma separated values. The header includes metadata about the file and one row per variable with the following information in each row: variable name, data type, item scoring, special data codes, and variable label.   "
  },
  {
    "id": 14765,
    "package_name": "jrt",
    "title": "Item Response Theory Modeling and Scoring for Judgment Data",
    "description": "Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225> and Myszkowski (2021) <doi:10.1037/aca0000287>. A function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions.",
    "version": "1.1.3",
    "maintainer": "Nils Myszkowski <nilsmyszkowskiscience@gmail.com>",
    "author": "Nils Myszkowski [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jrt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jrt Item Response Theory Modeling and Scoring for Judgment Data Psychometric analysis and scoring of judgment data using polytomous Item-Response Theory (IRT) models, as described in Myszkowski and Storme (2019) <doi:10.1037/aca0000225> and Myszkowski (2021) <doi:10.1037/aca0000287>. A function is used to automatically compare and select models, as well as to present a variety of model-based statistics. Plotting functions are used to present category curves, as well as information, reliability and standard error functions.  "
  },
  {
    "id": 14873,
    "package_name": "kgraph",
    "title": "Knowledge Graphs Constructions and Visualizations",
    "description": "Knowledge graphs enable to efficiently visualize and gain insights into large-scale data analysis results, as p-values from multiple studies or embedding data matrices. The usual workflow is a user providing a data frame of association studies results and specifying target nodes, e.g. phenotypes, to visualize. The knowledge graph then shows all the features which are significantly associated with the phenotype, with the edges being proportional to the association scores. As the user adds several target nodes and grouping information about the nodes such as biological pathways, the construction of such graphs soon becomes complex. The 'kgraph' package aims to enable users to easily build such knowledge graphs, and provides two main features: first, to enable building a knowledge graph based on a data frame of concepts relationships, be it p-values or cosine similarities; second, to enable determining an appropriate cut-off on cosine similarities from a complete embedding matrix, to enable the building of a knowledge graph directly from an embedding matrix. The 'kgraph' package provides several display, layout and cut-off options, and has already proven useful to researchers to enable them to visualize large sets of p-value associations with various phenotypes, and to quickly be able to visualize embedding results. Two example datasets are provided to demonstrate these behaviors, and several live 'shiny' applications are hosted by the CELEHS laboratory and Parse Health, as the KESER Mental Health application <https://keser-mental-health.parse-health.org/> based on Hong C. (2021) <doi:10.1038/s41746-021-00519-z>.",
    "version": "1.2.0",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "author": "Thomas Charlon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7497-0470>),\n  Hongyi Yuan [ctb] (ORCID: <https://orcid.org/0000-0003-2597-1973>),\n  CELEHS [aut] (<https://celehs.hms.harvard.edu>),\n  PARSE Health [aut] (<https://parse-health.org>)",
    "url": "https://gitlab.com/thomaschln/kgraph",
    "bug_reports": "https://gitlab.com/thomaschln/kgraph/-/issues",
    "repository": "https://cran.r-project.org/package=kgraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kgraph Knowledge Graphs Constructions and Visualizations Knowledge graphs enable to efficiently visualize and gain insights into large-scale data analysis results, as p-values from multiple studies or embedding data matrices. The usual workflow is a user providing a data frame of association studies results and specifying target nodes, e.g. phenotypes, to visualize. The knowledge graph then shows all the features which are significantly associated with the phenotype, with the edges being proportional to the association scores. As the user adds several target nodes and grouping information about the nodes such as biological pathways, the construction of such graphs soon becomes complex. The 'kgraph' package aims to enable users to easily build such knowledge graphs, and provides two main features: first, to enable building a knowledge graph based on a data frame of concepts relationships, be it p-values or cosine similarities; second, to enable determining an appropriate cut-off on cosine similarities from a complete embedding matrix, to enable the building of a knowledge graph directly from an embedding matrix. The 'kgraph' package provides several display, layout and cut-off options, and has already proven useful to researchers to enable them to visualize large sets of p-value associations with various phenotypes, and to quickly be able to visualize embedding results. Two example datasets are provided to demonstrate these behaviors, and several live 'shiny' applications are hosted by the CELEHS laboratory and Parse Health, as the KESER Mental Health application <https://keser-mental-health.parse-health.org/> based on Hong C. (2021) <doi:10.1038/s41746-021-00519-z>.  "
  },
  {
    "id": 14879,
    "package_name": "kim",
    "title": "A Toolkit for Behavioral Scientists",
    "description": "A collection of functions for analyzing data typically collected \n    or used by behavioral scientists. Examples of the functions include\n    a function that compares groups in a factorial experimental design,\n    a function that conducts two-way analysis of variance (ANOVA),\n    and a function that cleans a data set generated by Qualtrics surveys.\n    Some of the functions will require installing additional package(s).\n    Such packages and other references are cited within the section\n    describing the relevant functions. Many functions in this package\n    rely heavily on these two popular R packages:\n    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.\n    Wickham et al. (2021) <https://CRAN.R-project.org/package=ggplot2>.",
    "version": "0.6.1",
    "maintainer": "Jin Kim <jinkim@aya.yale.edu>",
    "author": "Jin Kim [aut, cre] (ORCID: <https://orcid.org/0000-0002-5013-3958>)",
    "url": "https://github.com/jinkim3/kim, https://jinkim.science",
    "bug_reports": "https://github.com/jinkim3/kim/issues",
    "repository": "https://cran.r-project.org/package=kim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kim A Toolkit for Behavioral Scientists A collection of functions for analyzing data typically collected \n    or used by behavioral scientists. Examples of the functions include\n    a function that compares groups in a factorial experimental design,\n    a function that conducts two-way analysis of variance (ANOVA),\n    and a function that cleans a data set generated by Qualtrics surveys.\n    Some of the functions will require installing additional package(s).\n    Such packages and other references are cited within the section\n    describing the relevant functions. Many functions in this package\n    rely heavily on these two popular R packages:\n    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.\n    Wickham et al. (2021) <https://CRAN.R-project.org/package=ggplot2>.  "
  },
  {
    "id": 15053,
    "package_name": "lavaan.mi",
    "title": "Fit Structural Equation Models to Multiply Imputed Data",
    "description": "The primary purpose of 'lavaan.mi' is to extend the functionality \n     of the R package 'lavaan', which implements structural equation modeling\n     (SEM).  When incomplete data have been multiply imputed, the imputed data\n     sets can be analyzed by 'lavaan' using complete-data estimation methods,\n     but results must be pooled across imputations (Rubin, 1987, <doi:10.1002/9780470316696>).\n     The 'lavaan.mi' package automates the pooling of point and standard-error\n     estimates, as well as a variety of test statistics, using a familiar interface\n     that allows users to fit an SEM to multiple imputations as they would to a\n     single data set using the 'lavaan' package.",
    "version": "0.1-0",
    "maintainer": "Terrence D. Jorgensen <TJorgensen314@gmail.com>",
    "author": "Terrence D. Jorgensen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5111-6773>),\n  Yves Rosseel [ctb] (ORCID: <https://orcid.org/0000-0002-4129-4477>)",
    "url": "https://github.com/TDJorgensen/lavaan.mi",
    "bug_reports": "https://github.com/TDJorgensen/lavaan.mi/issues",
    "repository": "https://cran.r-project.org/package=lavaan.mi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaan.mi Fit Structural Equation Models to Multiply Imputed Data The primary purpose of 'lavaan.mi' is to extend the functionality \n     of the R package 'lavaan', which implements structural equation modeling\n     (SEM).  When incomplete data have been multiply imputed, the imputed data\n     sets can be analyzed by 'lavaan' using complete-data estimation methods,\n     but results must be pooled across imputations (Rubin, 1987, <doi:10.1002/9780470316696>).\n     The 'lavaan.mi' package automates the pooling of point and standard-error\n     estimates, as well as a variety of test statistics, using a familiar interface\n     that allows users to fit an SEM to multiple imputations as they would to a\n     single data set using the 'lavaan' package.  "
  },
  {
    "id": 15054,
    "package_name": "lavaan.printer",
    "title": "Helper Functions for Printing 'lavaan' Outputs",
    "description": "Helpers for customizing selected outputs from\n  'lavaan' by Rosseel (2012) <doi:10.18637/jss.v048.i02>\n  and print them. The functions are intended to be used\n  by package developers in their packages and so are not\n  designed to be user-friendly. They are designed to be\n  let developers customize the tables by other\n  functions. Currently the parameter estimates tables of a\n  fitted object are supported.",
    "version": "0.1.0",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "author": "Shu Fai Cheung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9871-9448>)",
    "url": "https://sfcheung.github.io/lavaan.printer/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lavaan.printer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaan.printer Helper Functions for Printing 'lavaan' Outputs Helpers for customizing selected outputs from\n  'lavaan' by Rosseel (2012) <doi:10.18637/jss.v048.i02>\n  and print them. The functions are intended to be used\n  by package developers in their packages and so are not\n  designed to be user-friendly. They are designed to be\n  let developers customize the tables by other\n  functions. Currently the parameter estimates tables of a\n  fitted object are supported.  "
  },
  {
    "id": 15055,
    "package_name": "lavaan.shiny",
    "title": "Latent Variable Analysis with Shiny",
    "description": "Interactive shiny application for working with different kinds of\n    latent variable analysis, with the 'lavaan' package. Graphical output for models\n    are provided and different estimators are supported.",
    "version": "1.2",
    "maintainer": "W. Kyle Hamilton <kyle.hamilton@gmail.com>",
    "author": "W. Kyle Hamilton <kyle.hamilton@gmail.com>",
    "url": "https://github.com/kylehamilton/lavaan.shiny",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lavaan.shiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaan.shiny Latent Variable Analysis with Shiny Interactive shiny application for working with different kinds of\n    latent variable analysis, with the 'lavaan' package. Graphical output for models\n    are provided and different estimators are supported.  "
  },
  {
    "id": 15056,
    "package_name": "lavaanExtra",
    "title": "Convenience Functions for Package 'lavaan'",
    "description": "Affords an alternative, vector-based syntax to 'lavaan', as well as other \n             convenience functions such as naming paths and defining indirect\n             links automatically, in addition to convenience formatting optimized\n             for a publication and script sharing workflow.",
    "version": "0.2.2",
    "maintainer": "R\u00e9mi Th\u00e9riault <remi.theriault@mail.mcgill.ca>",
    "author": "R\u00e9mi Th\u00e9riault [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4315-6788>)",
    "url": "https://lavaanExtra.remi-theriault.com",
    "bug_reports": "https://github.com/rempsyc/lavaanExtra/issues",
    "repository": "https://cran.r-project.org/package=lavaanExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaanExtra Convenience Functions for Package 'lavaan' Affords an alternative, vector-based syntax to 'lavaan', as well as other \n             convenience functions such as naming paths and defining indirect\n             links automatically, in addition to convenience formatting optimized\n             for a publication and script sharing workflow.  "
  },
  {
    "id": 15057,
    "package_name": "lavaanPlot",
    "title": "Path Diagrams for 'Lavaan' Models via 'DiagrammeR'",
    "description": "Plots path diagrams from models in 'lavaan' using the plotting\n    functionality from the 'DiagrammeR' package. 'DiagrammeR' provides nice path diagrams \n    via 'Graphviz', and these functions make it easy to generate these diagrams from a\n    'lavaan' path model without having to write the DOT language graph specification.",
    "version": "0.8.1",
    "maintainer": "Alex Lishinski <alexlishinski@gmail.com>",
    "author": "Alex Lishinski",
    "url": "https://github.com/alishinski/lavaanPlot,\nhttps://lavaanplot.alexlishinski.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lavaanPlot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaanPlot Path Diagrams for 'Lavaan' Models via 'DiagrammeR' Plots path diagrams from models in 'lavaan' using the plotting\n    functionality from the 'DiagrammeR' package. 'DiagrammeR' provides nice path diagrams \n    via 'Graphviz', and these functions make it easy to generate these diagrams from a\n    'lavaan' path model without having to write the DOT language graph specification.  "
  },
  {
    "id": 15058,
    "package_name": "lavaangui",
    "title": "Graphical User Interface with Integrated 'Diagrammer' for\n'Lavaan'",
    "description": "Provides a graphical user interface with an \n    integrated diagrammer for latent variable models from the 'lavaan' package. \n    It offers two core functions:  first, lavaangui() launches a web application\n    that allows users to specify models by drawing path diagrams, fitting them, \n    assessing model fit, and more; second, plot_lavaan() creates \n    interactive path diagrams from models specified in 'lavaan'. \n    Karch (2024) <doi: 10.1080/10705511.2024.2420678> contains a tutorial.",
    "version": "0.3.2",
    "maintainer": "Julian D. Karch <j.d.karch@fsw.leidenuniv.nl>",
    "author": "Julian D. Karch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1625-2822>)",
    "url": "https://lavaangui.org/, https://github.com/karchjd/lavaangui",
    "bug_reports": "https://github.com/karchjd/lavaangui/issues",
    "repository": "https://cran.r-project.org/package=lavaangui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavaangui Graphical User Interface with Integrated 'Diagrammer' for\n'Lavaan' Provides a graphical user interface with an \n    integrated diagrammer for latent variable models from the 'lavaan' package. \n    It offers two core functions:  first, lavaangui() launches a web application\n    that allows users to specify models by drawing path diagrams, fitting them, \n    assessing model fit, and more; second, plot_lavaan() creates \n    interactive path diagrams from models specified in 'lavaan'. \n    Karch (2024) <doi: 10.1080/10705511.2024.2420678> contains a tutorial.  "
  },
  {
    "id": 15060,
    "package_name": "lavinteract",
    "title": "Post-Estimation Utilities for 'lavaan' Fitted Models",
    "description": "Companion toolbox for structural equation models fitted with 'lavaan'. Provides post-estimation diagnostics and graphics that operate directly on a fitted object using its estimates and covariance, and refits auxiliary models when needed. The package relies on 'lavaan' (Rosseel, 2012) <doi:10.18637/jss.v048.i02>.",
    "version": "0.3.4",
    "maintainer": "Giuseppe Corbelli <giuseppe.corbelli@uninettunouniversity.net>",
    "author": "Giuseppe Corbelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2864-3548>)",
    "url": "https://github.com/g-corbelli/lavinteract",
    "bug_reports": "https://github.com/g-corbelli/lavinteract/issues",
    "repository": "https://cran.r-project.org/package=lavinteract",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lavinteract Post-Estimation Utilities for 'lavaan' Fitted Models Companion toolbox for structural equation models fitted with 'lavaan'. Provides post-estimation diagnostics and graphics that operate directly on a fitted object using its estimates and covariance, and refits auxiliary models when needed. The package relies on 'lavaan' (Rosseel, 2012) <doi:10.18637/jss.v048.i02>.  "
  },
  {
    "id": 15073,
    "package_name": "lbaModel",
    "title": "The Linear Ballistic Accumulation Model",
    "description": "Provides density, distribution and random generation functions for the Linear Ballistic Accumulation (LBA) model, a widely used choice response time model in cognitive psychology. The package supports model specifications, parameter estimation, and likelihood computation, facilitating simulation and statistical inference for LBA-based experiments. For details on the LBA model, see Brown and Heathcote (2008) <doi:10.1016/j.cogpsych.2007.12.002>.",
    "version": "0.2.9.2",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "author": "Yi-Shin Lin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lbaModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lbaModel The Linear Ballistic Accumulation Model Provides density, distribution and random generation functions for the Linear Ballistic Accumulation (LBA) model, a widely used choice response time model in cognitive psychology. The package supports model specifications, parameter estimation, and likelihood computation, facilitating simulation and statistical inference for LBA-based experiments. For details on the LBA model, see Brown and Heathcote (2008) <doi:10.1016/j.cogpsych.2007.12.002>.  "
  },
  {
    "id": 15088,
    "package_name": "lcsm",
    "title": "Univariate and Bivariate Latent Change Score Modelling",
    "description": "Helper functions to implement univariate and bivariate latent change score models in R using the 'lavaan' package.\n  For details about Latent Change Score Modeling (LCSM) see McArdle (2009) <doi:10.1146/annurev.psych.60.110707.163612> and Grimm, An, McArdle, Zonderman and Resnick (2012) <doi:10.1080/10705511.2012.659627>.\n  The package automatically generates 'lavaan' syntax for different model specifications and varying timepoints.\n  The 'lavaan' syntax generated by this package can be returned and further specifications can be added manually.\n  Longitudinal plots as well as simplified path diagrams can be created to visualise data and model specifications.\n  Estimated model parameters and fit statistics can be extracted as data frames.\n  Data for different univariate and bivariate LCSM can be simulated by specifying estimates for model parameters to explore their effects.\n  This package combines the strengths of other R packages like 'lavaan', 'broom', and 'semPlot' by generating 'lavaan' syntax that helps these packages work together.",
    "version": "0.3.2",
    "maintainer": "Milan Wiedemann <milan.wiedemann@gmail.com>",
    "author": "Milan Wiedemann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1991-282X>),\n  Graham M Thew [ctb] (ORCID: <https://orcid.org/0000-0003-2851-1315>),\n  Ur\u0161ka Ko\u0161ir [ctb] (ORCID: <https://orcid.org/0000-0003-2132-4090>),\n  Anke Ehlers [ths] (ORCID: <https://orcid.org/0000-0002-8742-0192>),\n  Mental Health Research UK [fnd]",
    "url": "https://milanwiedemann.github.io/lcsm/",
    "bug_reports": "https://github.com/milanwiedemann/lcsm/issues",
    "repository": "https://cran.r-project.org/package=lcsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lcsm Univariate and Bivariate Latent Change Score Modelling Helper functions to implement univariate and bivariate latent change score models in R using the 'lavaan' package.\n  For details about Latent Change Score Modeling (LCSM) see McArdle (2009) <doi:10.1146/annurev.psych.60.110707.163612> and Grimm, An, McArdle, Zonderman and Resnick (2012) <doi:10.1080/10705511.2012.659627>.\n  The package automatically generates 'lavaan' syntax for different model specifications and varying timepoints.\n  The 'lavaan' syntax generated by this package can be returned and further specifications can be added manually.\n  Longitudinal plots as well as simplified path diagrams can be created to visualise data and model specifications.\n  Estimated model parameters and fit statistics can be extracted as data frames.\n  Data for different univariate and bivariate LCSM can be simulated by specifying estimates for model parameters to explore their effects.\n  This package combines the strengths of other R packages like 'lavaan', 'broom', and 'semPlot' by generating 'lavaan' syntax that helps these packages work together.  "
  },
  {
    "id": 15145,
    "package_name": "lessSEM",
    "title": "Non-Smooth Regularization for Structural Equation Models",
    "description": "Provides regularized structural equation modeling \n  (regularized SEM) with non-smooth penalty functions (e.g., lasso) building \n  on 'lavaan'. The package is heavily inspired by the \n  ['regsem'](<https://github.com/Rjacobucci/regsem>) and \n  ['lslx'](<https://github.com/psyphh/lslx>) packages.",
    "version": "1.5.7",
    "maintainer": "Jannik H. Orzek <jannik.orzek@mailbox.org>",
    "author": "Jannik H. Orzek [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3123-2248>)",
    "url": "https://github.com/jhorzek/lessSEM,\nhttps://jhorzek.github.io/lessSEM/",
    "bug_reports": "https://github.com/jhorzek/lessSEM/issues",
    "repository": "https://cran.r-project.org/package=lessSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lessSEM Non-Smooth Regularization for Structural Equation Models Provides regularized structural equation modeling \n  (regularized SEM) with non-smooth penalty functions (e.g., lasso) building \n  on 'lavaan'. The package is heavily inspired by the \n  ['regsem'](<https://github.com/Rjacobucci/regsem>) and \n  ['lslx'](<https://github.com/psyphh/lslx>) packages.  "
  },
  {
    "id": 15202,
    "package_name": "likelihoodR",
    "title": "Likelihood Analyses for Common Statistical Tests",
    "description": "A collection of functions that calculate the log likelihood \n    (support) for a range of statistical tests. Where possible the likelihood \n    function and likelihood interval for the observed data are displayed. The \n    evidential approach used here is based on the book \"Likelihood\" by A.W.F. \n    Edwards (1992, ISBN-13 : 978-0801844430), \"Statistical Evidence\" by R. \n    Royall (1997, ISBN-13 : 978-0412044113), S.N. Goodman & R. Royall \n    (2011) <doi:10.2105/AJPH.78.12.1568>, \"Understanding \n    Psychology as a Science\" by Z. Dienes (2008, ISBN-13 : 978-0230542310), \n    S. Glover & P. Dixon <doi:10.3758/BF03196706> \n    and others. This package accompanies \"Evidence-Based Statistics\" by \n    P. Cahusac (2020, ISBN-13 : 978-1119549802) \n    <doi:10.1002/9781119549833>.",
    "version": "1.1.5",
    "maintainer": "Peter Cahusac <peteqsac@gmail.com>",
    "author": "Peter Cahusac [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4976-2834>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=likelihoodR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "likelihoodR Likelihood Analyses for Common Statistical Tests A collection of functions that calculate the log likelihood \n    (support) for a range of statistical tests. Where possible the likelihood \n    function and likelihood interval for the observed data are displayed. The \n    evidential approach used here is based on the book \"Likelihood\" by A.W.F. \n    Edwards (1992, ISBN-13 : 978-0801844430), \"Statistical Evidence\" by R. \n    Royall (1997, ISBN-13 : 978-0412044113), S.N. Goodman & R. Royall \n    (2011) <doi:10.2105/AJPH.78.12.1568>, \"Understanding \n    Psychology as a Science\" by Z. Dienes (2008, ISBN-13 : 978-0230542310), \n    S. Glover & P. Dixon <doi:10.3758/BF03196706> \n    and others. This package accompanies \"Evidence-Based Statistics\" by \n    P. Cahusac (2020, ISBN-13 : 978-1119549802) \n    <doi:10.1002/9781119549833>.  "
  },
  {
    "id": 15355,
    "package_name": "logistic4p",
    "title": "Logistic Regression with Misclassification in Dependent\nVariables",
    "description": "Error in a binary dependent variable, also known as misclassification, has not drawn much attention in psychology. Ignoring misclassification in logistic regression can result in misleading parameter estimates and statistical inference. This package conducts logistic regression analysis with misspecification in outcome variables. ",
    "version": "1.6",
    "maintainer": "Zhiyong Zhang <johnnyzhz@gmail.com>",
    "author": "Haiyan Liu and Zhiyong Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=logistic4p",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "logistic4p Logistic Regression with Misclassification in Dependent\nVariables Error in a binary dependent variable, also known as misclassification, has not drawn much attention in psychology. Ignoring misclassification in logistic regression can result in misleading parameter estimates and statistical inference. This package conducts logistic regression analysis with misspecification in outcome variables.   "
  },
  {
    "id": 15430,
    "package_name": "lsasim",
    "title": "Functions to Facilitate the Simulation of Large Scale Assessment\nData",
    "description": "Provides functions to simulate data from large-scale educational\n  assessments, including background questionnaire data and cognitive item\n  responses that adhere to a multiple-matrix sampled design. The theoretical\n  foundation can be found on\n  Matta, T.H., Rutkowski, L., Rutkowski, D. et al. (2018)\n  <doi:10.1186/s40536-018-0068-8>.",
    "version": "2.1.6",
    "maintainer": "Waldir Leoncio <w.l.netto@medisin.uio.no>",
    "author": "Tyler Matta [aut],\n  Leslie Rutkowski [aut],\n  David Rutkowski [aut],\n  Yuan-Ling Linda Liaw [aut],\n  Kondwani Kajera Mughogho [ctb],\n  Waldir Leoncio [aut, cre],\n  Sinan Yavuz [ctb],\n  Paul Bailey [ctb]",
    "url": "",
    "bug_reports": "https://github.com/tmatta/lsasim/issues",
    "repository": "https://cran.r-project.org/package=lsasim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lsasim Functions to Facilitate the Simulation of Large Scale Assessment\nData Provides functions to simulate data from large-scale educational\n  assessments, including background questionnaire data and cognitive item\n  responses that adhere to a multiple-matrix sampled design. The theoretical\n  foundation can be found on\n  Matta, T.H., Rutkowski, L., Rutkowski, D. et al. (2018)\n  <doi:10.1186/s40536-018-0068-8>.  "
  },
  {
    "id": 15443,
    "package_name": "lsr",
    "title": "Companion to \"Learning Statistics with R\"",
    "description": "A collection of tools intended to make introductory \n    statistics easier to teach, including wrappers for common \n    hypothesis tests and basic data manipulation. It accompanies \n    Navarro, D. J. (2015). Learning Statistics with R: A Tutorial \n    for Psychology Students and Other Beginners, Version 0.6. ",
    "version": "0.5.2",
    "maintainer": "Danielle Navarro <djnavarro@protonmail.com>",
    "author": "Danielle Navarro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7648-6578>)",
    "url": "https://github.com/djnavarro/lsr",
    "bug_reports": "https://github.com/djnavarro/lsr/issues",
    "repository": "https://cran.r-project.org/package=lsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lsr Companion to \"Learning Statistics with R\" A collection of tools intended to make introductory \n    statistics easier to teach, including wrappers for common \n    hypothesis tests and basic data manipulation. It accompanies \n    Navarro, D. J. (2015). Learning Statistics with R: A Tutorial \n    for Psychology Students and Other Beginners, Version 0.6.   "
  },
  {
    "id": 15470,
    "package_name": "lvmcomp",
    "title": "Stochastic EM Algorithms for Latent Variable Models with a\nHigh-Dimensional Latent Space",
    "description": "Provides stochastic EM algorithms for latent variable models\n    with a high-dimensional latent space. So far, we provide functions for confirmatory item\n    factor analysis based on the multidimensional two parameter logistic (M2PL) model and the \n    generalized multidimensional partial credit model. These functions scale well for problems\n    with many latent traits (e.g., thirty or even more) and are virtually tuning-free.\n    The computation is facilitated by multiprocessing 'OpenMP' API.\n    For more information, please refer to:\n    Zhang, S., Chen, Y., & Liu, Y. (2018). An Improved Stochastic EM Algorithm for Large-scale\n    Full-information Item Factor Analysis. British Journal of Mathematical and Statistical\n    Psychology. <doi:10.1111/bmsp.12153>.",
    "version": "1.2",
    "maintainer": "Siliang Zhang <zhangsiliang123@gmail.com>",
    "author": "Siliang Zhang [aut, cre],\n  Yunxiao Chen [aut],\n  Jorge Nocedal [cph],\n  Naoaki Okazaki [cph]",
    "url": "https://github.com/slzhang-fd/lvmcomp",
    "bug_reports": "https://github.com/slzhang-fd/lvmcomp/issues",
    "repository": "https://cran.r-project.org/package=lvmcomp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lvmcomp Stochastic EM Algorithms for Latent Variable Models with a\nHigh-Dimensional Latent Space Provides stochastic EM algorithms for latent variable models\n    with a high-dimensional latent space. So far, we provide functions for confirmatory item\n    factor analysis based on the multidimensional two parameter logistic (M2PL) model and the \n    generalized multidimensional partial credit model. These functions scale well for problems\n    with many latent traits (e.g., thirty or even more) and are virtually tuning-free.\n    The computation is facilitated by multiprocessing 'OpenMP' API.\n    For more information, please refer to:\n    Zhang, S., Chen, Y., & Liu, Y. (2018). An Improved Stochastic EM Algorithm for Large-scale\n    Full-information Item Factor Analysis. British Journal of Mathematical and Statistical\n    Psychology. <doi:10.1111/bmsp.12153>.  "
  },
  {
    "id": 15474,
    "package_name": "lwc2022",
    "title": "Langa-Weir Classification of Cognitive Function for 2022 HRS\nData",
    "description": "Generates the Langa-Weir classification of cognitive function for \n    the 2022 Health and Retirement Study (HRS) cognition data. It is \n    particularly useful for researchers studying cognitive aging who wish to \n    work with the most recent release of HRS data. The package provides \n    user-friendly functions for data preprocessing, scoring, and classification\n    allowing users to easily apply the Langa-Weir classification system. \n    For details regarding the;\n    HRS <https://hrsdata.isr.umich.edu/> and\n    Langa-Weir classifications <https://hrsdata.isr.umich.edu/data-products/langa-weir-classification-cognitive-function-1995-2020>.",
    "version": "1.0.0",
    "maintainer": "Cormac Monaghan <cormacmonaghan@protonmail.com>",
    "author": "Cormac Monaghan [cph, aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9012-3060>),\n  Rafael de Andrade Moral [aut] (ORCID:\n    <https://orcid.org/0000-0002-0875-3563>),\n  Joanna McHugh Power [aut] (ORCID:\n    <https://orcid.org/0000-0002-7387-3107>)",
    "url": "https://github.com/C-Monaghan/lwc2022,\nhttps://c-monaghan.github.io/lwc2022/",
    "bug_reports": "https://github.com/C-Monaghan/lwc2022/issues",
    "repository": "https://cran.r-project.org/package=lwc2022",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lwc2022 Langa-Weir Classification of Cognitive Function for 2022 HRS\nData Generates the Langa-Weir classification of cognitive function for \n    the 2022 Health and Retirement Study (HRS) cognition data. It is \n    particularly useful for researchers studying cognitive aging who wish to \n    work with the most recent release of HRS data. The package provides \n    user-friendly functions for data preprocessing, scoring, and classification\n    allowing users to easily apply the Langa-Weir classification system. \n    For details regarding the;\n    HRS <https://hrsdata.isr.umich.edu/> and\n    Langa-Weir classifications <https://hrsdata.isr.umich.edu/data-products/langa-weir-classification-cognitive-function-1995-2020>.  "
  },
  {
    "id": 15786,
    "package_name": "mde",
    "title": "Missing Data Explorer",
    "description": "Correct identification and handling of missing data is one of the most important steps in any analysis. To aid this process, 'mde' provides a very easy to use yet robust framework to quickly get an idea of where the missing data\n             lies and therefore find the most appropriate action to take.\n             Graham WJ (2009) <doi:10.1146/annurev.psych.58.110405.085530>. ",
    "version": "0.3.2",
    "maintainer": "Nelson Gonzabato <gonzabato@hotmail.com>",
    "author": "Nelson Gonzabato [aut, cre]",
    "url": "https://github.com/Nelson-Gon/mde",
    "bug_reports": "https://github.com/Nelson-Gon/mde/issues",
    "repository": "https://cran.r-project.org/package=mde",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mde Missing Data Explorer Correct identification and handling of missing data is one of the most important steps in any analysis. To aid this process, 'mde' provides a very easy to use yet robust framework to quickly get an idea of where the missing data\n             lies and therefore find the most appropriate action to take.\n             Graham WJ (2009) <doi:10.1146/annurev.psych.58.110405.085530>.   "
  },
  {
    "id": 15803,
    "package_name": "measr",
    "title": "Bayesian Psychometric Measurement Using 'Stan'",
    "description": "Estimate diagnostic classification models (also called cognitive\n    diagnostic models) with 'Stan'. Diagnostic classification models are \n    confirmatory latent class models, as described by Rupp et al.\n    (2010, ISBN: 978-1-60623-527-0). Automatically generate 'Stan' code for the\n    general loglinear cognitive diagnostic diagnostic model proposed by\n    Henson et al. (2009) <doi:10.1007/s11336-008-9089-5> and other subtypes that\n    introduce additional model constraints. Using the generated 'Stan' code,\n    estimate the model evaluate the model's performance using model fit indices,\n    information criteria, and reliability metrics.",
    "version": "1.0.0",
    "maintainer": "W. Jake Thompson <wjakethompson@gmail.com>",
    "author": "W. Jake Thompson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7339-0300>),\n  Nathan Jones [ctb] (ORCID: <https://orcid.org/0000-0001-6177-7161>),\n  Matthew Johnson [cph] (Provided code adapted for\n    reliability.measrdcm()),\n  Paul-Christian B\u00fcrkner [cph] (Author of eval_silent()),\n  University of Kansas [cph],\n  Institute of Education Sciences [fnd]",
    "url": "https://measr.info, https://github.com/wjakethompson/measr",
    "bug_reports": "https://github.com/wjakethompson/measr/issues",
    "repository": "https://cran.r-project.org/package=measr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "measr Bayesian Psychometric Measurement Using 'Stan' Estimate diagnostic classification models (also called cognitive\n    diagnostic models) with 'Stan'. Diagnostic classification models are \n    confirmatory latent class models, as described by Rupp et al.\n    (2010, ISBN: 978-1-60623-527-0). Automatically generate 'Stan' code for the\n    general loglinear cognitive diagnostic diagnostic model proposed by\n    Henson et al. (2009) <doi:10.1007/s11336-008-9089-5> and other subtypes that\n    introduce additional model constraints. Using the generated 'Stan' code,\n    estimate the model evaluate the model's performance using model fit indices,\n    information criteria, and reliability metrics.  "
  },
  {
    "id": 15874,
    "package_name": "metaHelper",
    "title": "Transforms Statistical Measures Commonly Used for Meta-Analysis",
    "description": "Helps calculate statistical values commonly used in meta-analysis. It provides several methods to compute different forms of standardized mean differences, as well as other values such as standard errors and standard deviations.\n  The methods used in this package are described in the following references:\n  Altman D G, Bland J M. (2011) <doi:10.1136/bmj.d2090>\n  Borenstein, M., Hedges, L.V., Higgins, J.P.T. and Rothstein, H.R. (2009) <doi:10.1002/9780470743386.ch4>\n  Chinn S. (2000) <doi:10.1002/1097-0258(20001130)19:22%3C3127::aid-sim784%3E3.0.co;2-m>\n  Cochrane Handbook (2011) <https://handbook-5-1.cochrane.org/front_page.htm>\n  Cooper, H., Hedges, L. V., & Valentine, J. C. (2009) <https://psycnet.apa.org/record/2009-05060-000>\n  Cohen, J. (1977) <https://psycnet.apa.org/record/1987-98267-000>\n  Ellis, P.D. (2009) <https://www.psychometrica.de/effect_size.html>\n  Goulet-Pelletier, J.-C., & Cousineau, D. (2018) <doi:10.20982/tqmp.14.4.p242>\n  Hedges, L. V. (1981) <doi:10.2307/1164588>\n  Hedges L. V., Olkin I. (1985) <doi:10.1016/C2009-0-03396-0>\n  Murad M H, Wang Z, Zhu Y, Saadi S, Chu H, Lin L et al. (2023) <doi:10.1136/bmj-2022-073141>\n  Mayer M (2023) <https://search.r-project.org/CRAN/refmans/confintr/html/ci_proportion.html>\n  Stackoverflow (2014) <https://stats.stackexchange.com/questions/82720/confidence-interval-around-binomial-estimate-of-0-or-1>\n  Stackoverflow (2018) <https://stats.stackexchange.com/q/338043>.",
    "version": "1.0.0",
    "maintainer": "Robert Emprechtinger <emprechtinger@stateofhealth.at>",
    "author": "Robert Emprechtinger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3114-9812>),\n  Guido Schwarzer [aut],\n  Ulf T\u00f6lch [aut],\n  G\u00fcnther Schreder [aut],\n  Gerald Gartlehner [aut]",
    "url": "https://github.com/RobertEmprechtinger/metaHelper",
    "bug_reports": "https://github.com/RobertEmprechtinger/metaHelper/issues",
    "repository": "https://cran.r-project.org/package=metaHelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaHelper Transforms Statistical Measures Commonly Used for Meta-Analysis Helps calculate statistical values commonly used in meta-analysis. It provides several methods to compute different forms of standardized mean differences, as well as other values such as standard errors and standard deviations.\n  The methods used in this package are described in the following references:\n  Altman D G, Bland J M. (2011) <doi:10.1136/bmj.d2090>\n  Borenstein, M., Hedges, L.V., Higgins, J.P.T. and Rothstein, H.R. (2009) <doi:10.1002/9780470743386.ch4>\n  Chinn S. (2000) <doi:10.1002/1097-0258(20001130)19:22%3C3127::aid-sim784%3E3.0.co;2-m>\n  Cochrane Handbook (2011) <https://handbook-5-1.cochrane.org/front_page.htm>\n  Cooper, H., Hedges, L. V., & Valentine, J. C. (2009) <https://psycnet.apa.org/record/2009-05060-000>\n  Cohen, J. (1977) <https://psycnet.apa.org/record/1987-98267-000>\n  Ellis, P.D. (2009) <https://www.psychometrica.de/effect_size.html>\n  Goulet-Pelletier, J.-C., & Cousineau, D. (2018) <doi:10.20982/tqmp.14.4.p242>\n  Hedges, L. V. (1981) <doi:10.2307/1164588>\n  Hedges L. V., Olkin I. (1985) <doi:10.1016/C2009-0-03396-0>\n  Murad M H, Wang Z, Zhu Y, Saadi S, Chu H, Lin L et al. (2023) <doi:10.1136/bmj-2022-073141>\n  Mayer M (2023) <https://search.r-project.org/CRAN/refmans/confintr/html/ci_proportion.html>\n  Stackoverflow (2014) <https://stats.stackexchange.com/questions/82720/confidence-interval-around-binomial-estimate-of-0-or-1>\n  Stackoverflow (2018) <https://stats.stackexchange.com/q/338043>.  "
  },
  {
    "id": 15879,
    "package_name": "metaSDTreg",
    "title": "Regression Models for Meta Signal Detection Theory",
    "description": "Regression methods for the meta-SDT model. The package implements methods for cognitive experiments of metacognition as described in Kristensen, S. B., Sandberg, K., & Bibby, B. M. (2020). Regression methods for metacognitive sensitivity. Journal of Mathematical Psychology, 94. <doi:10.1016/j.jmp.2019.102297>.",
    "version": "0.2.2",
    "maintainer": "Simon Bang Kristensen <simonbk@ph.au.dk>",
    "author": "Simon Bang Kristensen [aut, cre],\n  Kristian Sandberg [aut],\n  Bo Martin Bibby [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metaSDTreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaSDTreg Regression Models for Meta Signal Detection Theory Regression methods for the meta-SDT model. The package implements methods for cognitive experiments of metacognition as described in Kristensen, S. B., Sandberg, K., & Bibby, B. M. (2020). Regression methods for metacognitive sensitivity. Journal of Mathematical Psychology, 94. <doi:10.1016/j.jmp.2019.102297>.  "
  },
  {
    "id": 15880,
    "package_name": "metaSEM",
    "title": "Meta-Analysis using Structural Equation Modeling",
    "description": "A collection of functions for conducting meta-analysis using a\n             structural equation modeling (SEM) approach via the 'OpenMx' and\n             'lavaan' packages. It also implements various procedures to\n             perform meta-analytic structural equation modeling on the\n             correlation and covariance matrices, see Cheung (2015)\n             <doi:10.3389/fpsyg.2014.01521>.",
    "version": "1.5.0",
    "maintainer": "Mike Cheung <mikewlcheung@nus.edu.sg>",
    "author": "Mike Cheung [aut, cre] (ORCID: <https://orcid.org/0000-0003-0113-0758>)",
    "url": "https://github.com/mikewlcheung/metasem",
    "bug_reports": "https://github.com/mikewlcheung/metasem/issues",
    "repository": "https://cran.r-project.org/package=metaSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaSEM Meta-Analysis using Structural Equation Modeling A collection of functions for conducting meta-analysis using a\n             structural equation modeling (SEM) approach via the 'OpenMx' and\n             'lavaan' packages. It also implements various procedures to\n             perform meta-analytic structural equation modeling on the\n             correlation and covariance matrices, see Cheung (2015)\n             <doi:10.3389/fpsyg.2014.01521>.  "
  },
  {
    "id": 16067,
    "package_name": "minb",
    "title": "Multiple-Inflated Negative Binomial Model",
    "description": "Count data is prevalent and informative, with widespread\n    application in many fields such as social psychology, personality, and\n    public health. Classical statistical methods for the analysis of count\n    outcomes are commonly variants of the log-linear model, including\n    Poisson regression and Negative Binomial regression. However, a\n    typical problem with count data modeling is inflation, in the sense\n    that the counts are evidently accumulated on some integers. Such an\n    inflation problem could distort the distribution of the observed\n    counts, further bias estimation and increase error, making the classic\n    methods infeasible. Traditional inflated value selection methods based\n    on histogram inspection are easy to neglect true points and\n    computationally expensive in addition. Therefore, we propose a\n    multiple-inflated negative binomial model to handle count data\n    modeling with multiple inflated values, achieving data-driven inflated\n    value selection. The proposed approach provides simultaneous\n    identification of important regression predictors on the target count\n    response as well. More details about the proposed method are described in \n    Li, Y., Wu, M., Wu, M., & Ma, S. (2023) <arXiv:2309.15585>.",
    "version": "0.1.0",
    "maintainer": "Mingcong Wu <wumingcong@ruc.edu.cn>",
    "author": "Yang Li [aut],\n  Mingcong Wu [aut, cre],\n  Mengyun Wu [aut],\n  Shuangge Ma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minb Multiple-Inflated Negative Binomial Model Count data is prevalent and informative, with widespread\n    application in many fields such as social psychology, personality, and\n    public health. Classical statistical methods for the analysis of count\n    outcomes are commonly variants of the log-linear model, including\n    Poisson regression and Negative Binomial regression. However, a\n    typical problem with count data modeling is inflation, in the sense\n    that the counts are evidently accumulated on some integers. Such an\n    inflation problem could distort the distribution of the observed\n    counts, further bias estimation and increase error, making the classic\n    methods infeasible. Traditional inflated value selection methods based\n    on histogram inspection are easy to neglect true points and\n    computationally expensive in addition. Therefore, we propose a\n    multiple-inflated negative binomial model to handle count data\n    modeling with multiple inflated values, achieving data-driven inflated\n    value selection. The proposed approach provides simultaneous\n    identification of important regression predictors on the target count\n    response as well. More details about the proposed method are described in \n    Li, Y., Wu, M., Wu, M., & Ma, S. (2023) <arXiv:2309.15585>.  "
  },
  {
    "id": 16104,
    "package_name": "mirtjml",
    "title": "Joint Maximum Likelihood Estimation for High-Dimensional Item\nFactor Analysis",
    "description": "Provides constrained joint maximum likelihood estimation\n    algorithms for item factor analysis (IFA) based on multidimensional item response theory\n    models. So far, we provide functions for exploratory and confirmatory IFA based on the \n    multidimensional two parameter logistic (M2PL) model for binary response data. Comparing \n    with traditional estimation methods for IFA, the methods implemented in this package scale\n    better to data with large numbers of respondents, items, and latent factors. The computation\n    is facilitated by multiprocessing 'OpenMP' API. For more information, please refer to:\n    1. Chen, Y., Li, X., & Zhang, S. (2018). Joint Maximum Likelihood Estimation for \n    High-Dimensional Exploratory Item Factor Analysis. Psychometrika, 1-23. \n    <doi:10.1007/s11336-018-9646-5>;\n    2. Chen, Y., Li, X., & Zhang, S. (2019). Structured Latent Factor Analysis for Large-scale Data: \n    Identifiability, Estimability, and Their Implications. Journal of the American Statistical \n    Association, <doi: 10.1080/01621459.2019.1635485>.",
    "version": "1.4.0",
    "maintainer": "Siliang Zhang <zhangsiliang123@gmail.com>",
    "author": "Siliang Zhang [aut, cre],\n  Yunxiao Chen [aut],\n  Xiaoou Li [aut]",
    "url": "https://github.com/slzhang-fd/mirtjml",
    "bug_reports": "https://github.com/slzhang-fd/mirtjml/issues",
    "repository": "https://cran.r-project.org/package=mirtjml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mirtjml Joint Maximum Likelihood Estimation for High-Dimensional Item\nFactor Analysis Provides constrained joint maximum likelihood estimation\n    algorithms for item factor analysis (IFA) based on multidimensional item response theory\n    models. So far, we provide functions for exploratory and confirmatory IFA based on the \n    multidimensional two parameter logistic (M2PL) model for binary response data. Comparing \n    with traditional estimation methods for IFA, the methods implemented in this package scale\n    better to data with large numbers of respondents, items, and latent factors. The computation\n    is facilitated by multiprocessing 'OpenMP' API. For more information, please refer to:\n    1. Chen, Y., Li, X., & Zhang, S. (2018). Joint Maximum Likelihood Estimation for \n    High-Dimensional Exploratory Item Factor Analysis. Psychometrika, 1-23. \n    <doi:10.1007/s11336-018-9646-5>;\n    2. Chen, Y., Li, X., & Zhang, S. (2019). Structured Latent Factor Analysis for Large-scale Data: \n    Identifiability, Estimability, and Their Implications. Journal of the American Statistical \n    Association, <doi: 10.1080/01621459.2019.1635485>.  "
  },
  {
    "id": 16105,
    "package_name": "mirtsvd",
    "title": "SVD-Based Estimation for Exploratory Item Factor Analysis",
    "description": "Provides singular value decomposition based estimation algorithms for exploratory item factor analysis (IFA) \n    based on multidimensional item response theory models. For more information, please refer to:\n    Zhang, H., Chen, Y., & Li, X. (2020). A note on exploratory item factor analysis by \n    singular value decomposition. Psychometrika, 1-15, <DOI:10.1007/s11336-020-09704-7>.",
    "version": "1.0.1",
    "maintainer": "Haoran Zhang <hrzhang16@gmail.com>",
    "author": "Haoran Zhang [aut, cre],\n  Yunxiao Chen [aut],\n  Xiaoou Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mirtsvd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mirtsvd SVD-Based Estimation for Exploratory Item Factor Analysis Provides singular value decomposition based estimation algorithms for exploratory item factor analysis (IFA) \n    based on multidimensional item response theory models. For more information, please refer to:\n    Zhang, H., Chen, Y., & Li, X. (2020). A note on exploratory item factor analysis by \n    singular value decomposition. Psychometrika, 1-15, <DOI:10.1007/s11336-020-09704-7>.  "
  },
  {
    "id": 16128,
    "package_name": "missalpha",
    "title": "Find Range of Cronbach Alpha with a Dataset Including Missing\nData",
    "description": "Provides functions to calculate the minimum and maximum possible \n    values of Cronbach's alpha when item-level missing data are present. \n    Cronbach's alpha (Cronbach, 1951 <doi:10.1007/BF02310555>) is one of the most widely used \n    measures of internal consistency in the social, behavioral, and medical sciences \n    (Bland & Altman, 1997 <doi:10.1136/bmj.314.7080.572>; Tavakol & Dennick, 2011 \n    <doi:10.5116/ijme.4dfb.8dfd>). However, conventional implementations assume \n    complete data, and listwise deletion is often applied when missingness occurs, \n    which can lead to biased or overly optimistic reliability estimates (Enders, 2003 \n    <doi:10.1037/1082-989X.8.3.322>). This package implements computational strategies \n    including enumeration, Monte Carlo sampling, and optimization algorithms \n    (e.g., Genetic Algorithm, Differential Evolution, Sequential Least Squares \n    Programming) to obtain sharp lower and upper bounds of Cronbach's alpha under \n    arbitrary missing data patterns. The approach is motivated by Manski's partial \n    identification framework and pessimistic bounding ideas from optimization literature.",
    "version": "0.2.0",
    "maintainer": "Biying Zhou <biying.zhou@psu.edu>",
    "author": "Feng Ji [aut],\n  Biying Zhou [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=missalpha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "missalpha Find Range of Cronbach Alpha with a Dataset Including Missing\nData Provides functions to calculate the minimum and maximum possible \n    values of Cronbach's alpha when item-level missing data are present. \n    Cronbach's alpha (Cronbach, 1951 <doi:10.1007/BF02310555>) is one of the most widely used \n    measures of internal consistency in the social, behavioral, and medical sciences \n    (Bland & Altman, 1997 <doi:10.1136/bmj.314.7080.572>; Tavakol & Dennick, 2011 \n    <doi:10.5116/ijme.4dfb.8dfd>). However, conventional implementations assume \n    complete data, and listwise deletion is often applied when missingness occurs, \n    which can lead to biased or overly optimistic reliability estimates (Enders, 2003 \n    <doi:10.1037/1082-989X.8.3.322>). This package implements computational strategies \n    including enumeration, Monte Carlo sampling, and optimization algorithms \n    (e.g., Genetic Algorithm, Differential Evolution, Sequential Least Squares \n    Programming) to obtain sharp lower and upper bounds of Cronbach's alpha under \n    arbitrary missing data patterns. The approach is motivated by Manski's partial \n    identification framework and pessimistic bounding ideas from optimization literature.  "
  },
  {
    "id": 16322,
    "package_name": "modelfree",
    "title": "Model-Free Estimation of a Psychometric Function",
    "description": "Local linear estimation of psychometric functions. Provides functions for nonparametric estimation of a psychometric function and for estimation of a derived threshold and slope, and their standard deviations and confidence intervals.For more details see Zychaluk and Foster (2009) <doi:10.3758/APP.71.6.1414> and Foster and Zychaluk (2007) <doi:10.1109/MSP.2007.4286564>.",
    "version": "1.2.1",
    "maintainer": "Kamila Zychaluk <Kamila.Zychaluk@liverpool.ac.uk>",
    "author": "Ivan Marin-Franch [aut] (ORCID:\n    <https://orcid.org/0000-0001-7079-6874>),\n  Kamila Zychaluk [aut, cre, cph],\n  David H. Foster [aut] (ORCID: <https://orcid.org/0000-0003-2428-715X>)",
    "url": "https://personalpages.manchester.ac.uk/staff/d.h.foster/software-modelfree/latest/home.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=modelfree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modelfree Model-Free Estimation of a Psychometric Function Local linear estimation of psychometric functions. Provides functions for nonparametric estimation of a psychometric function and for estimation of a derived threshold and slope, and their standard deviations and confidence intervals.For more details see Zychaluk and Foster (2009) <doi:10.3758/APP.71.6.1414> and Foster and Zychaluk (2007) <doi:10.1109/MSP.2007.4286564>.  "
  },
  {
    "id": 16342,
    "package_name": "modsem",
    "title": "Latent Interaction (and Moderation) Analysis in Structural\nEquation Models (SEM)",
    "description": "\n    Estimation of interaction (i.e., moderation) effects between latent variables\n    in structural equation models (SEM). \n    The supported methods are:\n      The constrained approach (Algina & Moulder, 2001).\n      The unconstrained approach (Marsh et al., 2004).\n      The residual centering approach (Little et al., 2006).\n      The double centering approach (Lin et al., 2010).\n      The latent moderated structural equations (LMS) approach (Klein & Moosbrugger, 2000).\n      The quasi-maximum likelihood (QML) approach (Klein & Muth\u00e9n, 2007)\n    The constrained- unconstrained, residual- and double centering- approaches\n    are estimated via 'lavaan' (Rosseel, 2012), whilst the LMS- and QML- approaches\n    are estimated via 'modsem' it self. Alternatively model can be\n    estimated via 'Mplus' (Muth\u00e9n & Muth\u00e9n, 1998-2017).\n    References:\n    Algina, J., & Moulder, B. C. (2001). \n      <doi:10.1207/S15328007SEM0801_3>.\n      \"A note on estimating the J\u00f6reskog-Yang model for latent variable interaction using 'LISREL' 8.3.\"\n    Klein, A., & Moosbrugger, H. (2000). \n      <doi:10.1007/BF02296338>.\n      \"Maximum likelihood estimation of latent interaction effects with the LMS method.\"\n    Klein, A. G., & Muth\u00e9n, B. O. (2007). \n      <doi:10.1080/00273170701710205>.\n      \"Quasi-maximum likelihood estimation of structural equation models with multiple interaction and quadratic effects.\"\n    Lin, G. C., Wen, Z., Marsh, H. W., & Lin, H. S. (2010). \n      <doi:10.1080/10705511.2010.488999>.\n      \"Structural equation models of latent interactions: Clarification of orthogonalizing and double-mean-centering strategies.\"\n    Little, T. D., Bovaird, J. A., & Widaman, K. F. (2006). \n      <doi:10.1207/s15328007sem1304_1>.\n      \"On the merits of orthogonalizing powered and product terms: Implications for modeling interactions among latent variables.\"\n    Marsh, H. W., Wen, Z., & Hau, K. T. (2004). \n      <doi:10.1037/1082-989X.9.3.275>.\n      \"Structural equation models of latent interactions: evaluation of alternative estimation strategies and indicator construction.\"\n    Muth\u00e9n, L.K. and Muth\u00e9n, B.O. (1998-2017).  \n      \"'Mplus' User\u2019s Guide. Eighth Edition.\"\n      <https://www.statmodel.com/>.\n    Rosseel Y (2012). \n      <doi:10.18637/jss.v048.i02>.\n      \"'lavaan': An R Package for Structural Equation Modeling.\" ",
    "version": "1.0.14",
    "maintainer": "Kjell Solem Slupphaug <slupphaugkjell@gmail.com>",
    "author": "Kjell Solem Slupphaug [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-8324-2834>),\n  Mehmet Mehmetoglu [ctb] (ORCID:\n    <https://orcid.org/0000-0002-6092-8551>),\n  Matthias Mittner [ctb] (ORCID: <https://orcid.org/0000-0003-0205-7353>)",
    "url": "https://modsem.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=modsem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modsem Latent Interaction (and Moderation) Analysis in Structural\nEquation Models (SEM) \n    Estimation of interaction (i.e., moderation) effects between latent variables\n    in structural equation models (SEM). \n    The supported methods are:\n      The constrained approach (Algina & Moulder, 2001).\n      The unconstrained approach (Marsh et al., 2004).\n      The residual centering approach (Little et al., 2006).\n      The double centering approach (Lin et al., 2010).\n      The latent moderated structural equations (LMS) approach (Klein & Moosbrugger, 2000).\n      The quasi-maximum likelihood (QML) approach (Klein & Muth\u00e9n, 2007)\n    The constrained- unconstrained, residual- and double centering- approaches\n    are estimated via 'lavaan' (Rosseel, 2012), whilst the LMS- and QML- approaches\n    are estimated via 'modsem' it self. Alternatively model can be\n    estimated via 'Mplus' (Muth\u00e9n & Muth\u00e9n, 1998-2017).\n    References:\n    Algina, J., & Moulder, B. C. (2001). \n      <doi:10.1207/S15328007SEM0801_3>.\n      \"A note on estimating the J\u00f6reskog-Yang model for latent variable interaction using 'LISREL' 8.3.\"\n    Klein, A., & Moosbrugger, H. (2000). \n      <doi:10.1007/BF02296338>.\n      \"Maximum likelihood estimation of latent interaction effects with the LMS method.\"\n    Klein, A. G., & Muth\u00e9n, B. O. (2007). \n      <doi:10.1080/00273170701710205>.\n      \"Quasi-maximum likelihood estimation of structural equation models with multiple interaction and quadratic effects.\"\n    Lin, G. C., Wen, Z., Marsh, H. W., & Lin, H. S. (2010). \n      <doi:10.1080/10705511.2010.488999>.\n      \"Structural equation models of latent interactions: Clarification of orthogonalizing and double-mean-centering strategies.\"\n    Little, T. D., Bovaird, J. A., & Widaman, K. F. (2006). \n      <doi:10.1207/s15328007sem1304_1>.\n      \"On the merits of orthogonalizing powered and product terms: Implications for modeling interactions among latent variables.\"\n    Marsh, H. W., Wen, Z., & Hau, K. T. (2004). \n      <doi:10.1037/1082-989X.9.3.275>.\n      \"Structural equation models of latent interactions: evaluation of alternative estimation strategies and indicator construction.\"\n    Muth\u00e9n, L.K. and Muth\u00e9n, B.O. (1998-2017).  \n      \"'Mplus' User\u2019s Guide. Eighth Edition.\"\n      <https://www.statmodel.com/>.\n    Rosseel Y (2012). \n      <doi:10.18637/jss.v048.i02>.\n      \"'lavaan': An R Package for Structural Equation Modeling.\"   "
  },
  {
    "id": 16414,
    "package_name": "mousetRajectory",
    "title": "Mouse Trajectory Analyses for Behavioural Scientists",
    "description": "Helping psychologists and other behavioural scientists\n    to analyze mouse movement (and other 2-D trajectory) data. Bundles\n    together several functions that compute spatial measures (e.g., maximum\n    absolute deviation, area under the curve, sample entropy) or provide a\n    shorthand for procedures that are frequently used (e.g., time \n    normalization, linear interpolation, extracting initiation and movement \n    times). For more information on these dependent measures, see Wirth et al. \n    (2020) <doi:10.3758/s13428-020-01409-0>.",
    "version": "0.2.1",
    "maintainer": "Roland Pfister <mail@roland-pfister.net>",
    "author": "Roland Pfister [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4429-1052>),\n  Solveig Tonn [aut] (ORCID: <https://orcid.org/0000-0001-5254-8391>),\n  Moritz Schaaf [aut] (ORCID: <https://orcid.org/0000-0002-9959-2928>),\n  Robert Wirth [aut] (ORCID: <https://orcid.org/0000-0001-8446-1880>)",
    "url": "https://github.com/mc-schaaf/mousetRajectory,\nhttps://mc-schaaf.github.io/mousetRajectory/",
    "bug_reports": "https://github.com/mc-schaaf/mousetRajectory/issues",
    "repository": "https://cran.r-project.org/package=mousetRajectory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mousetRajectory Mouse Trajectory Analyses for Behavioural Scientists Helping psychologists and other behavioural scientists\n    to analyze mouse movement (and other 2-D trajectory) data. Bundles\n    together several functions that compute spatial measures (e.g., maximum\n    absolute deviation, area under the curve, sample entropy) or provide a\n    shorthand for procedures that are frequently used (e.g., time \n    normalization, linear interpolation, extracting initiation and movement \n    times). For more information on these dependent measures, see Wirth et al. \n    (2020) <doi:10.3758/s13428-020-01409-0>.  "
  },
  {
    "id": 16415,
    "package_name": "mousetrap",
    "title": "Process and Analyze Mouse-Tracking Data",
    "description": "Mouse-tracking, the analysis of mouse movements in computerized\n    experiments, is a method that is becoming increasingly popular in the\n    cognitive sciences. The mousetrap package offers functions for importing,\n    preprocessing, analyzing, aggregating, and visualizing mouse-tracking data.\n    An introduction into mouse-tracking analyses using mousetrap can be found\n    in Wulff, Kieslich, Henninger, Haslbeck, & Schulte-Mecklenbeck (2023)\n    <doi:10.31234/osf.io/v685r> (preprint: \n    <https://osf.io/preprints/psyarxiv/v685r>).",
    "version": "3.2.3",
    "maintainer": "Pascal J. Kieslich <pascal.kieslich@gmail.com>",
    "author": "Pascal J. Kieslich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0853-9364>),\n  Dirk U. Wulff [aut] (ORCID: <https://orcid.org/0000-0002-4008-8022>),\n  Felix Henninger [aut] (ORCID: <https://orcid.org/0000-0002-7730-9511>),\n  Jonas M. B. Haslbeck [aut],\n  Sarah Brockhaus [ctb]",
    "url": "https://pascalkieslich.github.io/mousetrap/,\nhttps://github.com/pascalkieslich/mousetrap",
    "bug_reports": "https://github.com/pascalkieslich/mousetrap/issues",
    "repository": "https://cran.r-project.org/package=mousetrap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mousetrap Process and Analyze Mouse-Tracking Data Mouse-tracking, the analysis of mouse movements in computerized\n    experiments, is a method that is becoming increasingly popular in the\n    cognitive sciences. The mousetrap package offers functions for importing,\n    preprocessing, analyzing, aggregating, and visualizing mouse-tracking data.\n    An introduction into mouse-tracking analyses using mousetrap can be found\n    in Wulff, Kieslich, Henninger, Haslbeck, & Schulte-Mecklenbeck (2023)\n    <doi:10.31234/osf.io/v685r> (preprint: \n    <https://osf.io/preprints/psyarxiv/v685r>).  "
  },
  {
    "id": 16444,
    "package_name": "mpt",
    "title": "Multinomial Processing Tree Models",
    "description": "Fitting and testing multinomial processing tree (MPT) models, a\n  class of nonlinear models for categorical data.  The parameters are the\n  link probabilities of a tree-like graph and represent the latent cognitive\n  processing steps executed to arrive at observable response categories\n  (Batchelder & Riefer, 1999 <doi:10.3758/bf03210812>; Erdfelder et al., 2009\n  <doi:10.1027/0044-3409.217.3.108>; Riefer & Batchelder, 1988\n  <doi:10.1037/0033-295x.95.3.318>).",
    "version": "1.0-0",
    "maintainer": "Florian Wickelmaier <wickelmaier@web.de>",
    "author": "Florian Wickelmaier [aut, cre],\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "https://www.mathpsy.uni-tuebingen.de/wickelmaier/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mpt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mpt Multinomial Processing Tree Models Fitting and testing multinomial processing tree (MPT) models, a\n  class of nonlinear models for categorical data.  The parameters are the\n  link probabilities of a tree-like graph and represent the latent cognitive\n  processing steps executed to arrive at observable response categories\n  (Batchelder & Riefer, 1999 <doi:10.3758/bf03210812>; Erdfelder et al., 2009\n  <doi:10.1027/0044-3409.217.3.108>; Riefer & Batchelder, 1988\n  <doi:10.1037/0033-295x.95.3.318>).  "
  },
  {
    "id": 16476,
    "package_name": "mscstexta4r",
    "title": "R Client for the Microsoft Cognitive Services Text Analytics\nREST API",
    "description": "R Client for the Microsoft Cognitive Services Text Analytics\n    REST API, including Sentiment Analysis, Topic Detection, Language Detection,\n    and Key Phrase Extraction. An account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly.",
    "version": "0.1.2",
    "maintainer": "Phil Ferriere <pferriere@hotmail.com>",
    "author": "Phil Ferriere [aut, cre]",
    "url": "https://github.com/philferriere/mscstexta4r",
    "bug_reports": "http://www.github.com/philferriere/mscstexta4r/issues",
    "repository": "https://cran.r-project.org/package=mscstexta4r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mscstexta4r R Client for the Microsoft Cognitive Services Text Analytics\nREST API R Client for the Microsoft Cognitive Services Text Analytics\n    REST API, including Sentiment Analysis, Topic Detection, Language Detection,\n    and Key Phrase Extraction. An account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly.  "
  },
  {
    "id": 16477,
    "package_name": "mscstts",
    "title": "R Client for the Microsoft Cognitive Services 'Text-to-Speech'\nREST API",
    "description": "R Client for the Microsoft Cognitive Services \n  'Text-to-Speech' REST API, including voice synthesis. A valid account \n  must be registered at the Microsoft Cognitive Services website \n  <https://azure.microsoft.com/en-us/products/ai-services/> in order to \n  obtain a (free) API key. Without an API key, this package will not \n  work properly.",
    "version": "0.6.4",
    "maintainer": "John Muschelli <muschellij2@gmail.com>",
    "author": "John Muschelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6469-1750>)",
    "url": "https://github.com/jhudsl/mscstts",
    "bug_reports": "https://github.com/jhudsl/mscstts/issues",
    "repository": "https://cran.r-project.org/package=mscstts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mscstts R Client for the Microsoft Cognitive Services 'Text-to-Speech'\nREST API R Client for the Microsoft Cognitive Services \n  'Text-to-Speech' REST API, including voice synthesis. A valid account \n  must be registered at the Microsoft Cognitive Services website \n  <https://azure.microsoft.com/en-us/products/ai-services/> in order to \n  obtain a (free) API key. Without an API key, this package will not \n  work properly.  "
  },
  {
    "id": 16478,
    "package_name": "mscsweblm4r",
    "title": "R Client for the Microsoft Cognitive Services Web Language Model\nREST API",
    "description": "R Client for the Microsoft Cognitive Services Web Language Model\n    REST API, including Break Into Words, Calculate Conditional\n    Probability, Calculate Joint Probability, Generate Next Words, and List\n    Available Models. A valid account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly.",
    "version": "0.1.2",
    "maintainer": "Phil Ferriere <pferriere@hotmail.com>",
    "author": "Phil Ferriere [aut, cre]",
    "url": "https://github.com/philferriere/mscsweblm4r",
    "bug_reports": "http://www.github.com/philferriere/mscsweblm4r/issues",
    "repository": "https://cran.r-project.org/package=mscsweblm4r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mscsweblm4r R Client for the Microsoft Cognitive Services Web Language Model\nREST API R Client for the Microsoft Cognitive Services Web Language Model\n    REST API, including Break Into Words, Calculate Conditional\n    Probability, Calculate Joint Probability, Generate Next Words, and List\n    Available Models. A valid account MUST be registered at the Microsoft\n    Cognitive Services website <https://www.microsoft.com/cognitive-services/>\n    in order to obtain a (free) API key. Without an API key, this package will\n    not work properly.  "
  },
  {
    "id": 16591,
    "package_name": "multinomineq",
    "title": "Bayesian Inference for Multinomial Models with Inequality\nConstraints",
    "description": "\n    Implements Gibbs sampling and Bayes factors for multinomial models with\n    linear inequality constraints on the vector of probability parameters. As\n    special cases, the model class includes models that predict a linear order \n    of binomial probabilities (e.g., p[1] < p[2] < p[3] < .50) and mixture models \n    assuming that the parameter vector p must be inside the convex hull of a \n    finite number of predicted patterns (i.e., vertices). A formal definition of \n    inequality-constrained multinomial models and the implemented computational\n    methods is provided in: Heck, D.W., & Davis-Stober, C.P. (2019). \n    Multinomial models with linear inequality constraints: Overview and improvements \n    of computational methods for Bayesian inference. Journal of Mathematical \n    Psychology, 91, 70-87. <doi:10.1016/j.jmp.2019.03.004>.\n    Inequality-constrained multinomial models have applications in the area of \n    judgment and decision making to fit and test random utility models  \n    (Regenwetter, M., Dana, J., & Davis-Stober, C.P. (2011). Transitivity of \n    preferences. Psychological Review, 118, 42\u201356, <doi:10.1037/a0021150>) or to \n    perform outcome-based strategy classification to select the decision strategy \n    that provides the best account for a vector of observed choice frequencies \n    (Heck, D.W., Hilbig, B.E., & Moshagen, M. (2017). From information \n    processing to decisions: Formalizing and comparing probabilistic choice models. \n    Cognitive Psychology, 96, 26\u201340. <doi:10.1016/j.cogpsych.2017.05.003>).",
    "version": "0.2.6",
    "maintainer": "Daniel W. Heck <daniel.heck@uni-marburg.de>",
    "author": "Daniel W. Heck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6302-9252>)",
    "url": "https://github.com/danheck/multinomineq",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multinomineq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multinomineq Bayesian Inference for Multinomial Models with Inequality\nConstraints \n    Implements Gibbs sampling and Bayes factors for multinomial models with\n    linear inequality constraints on the vector of probability parameters. As\n    special cases, the model class includes models that predict a linear order \n    of binomial probabilities (e.g., p[1] < p[2] < p[3] < .50) and mixture models \n    assuming that the parameter vector p must be inside the convex hull of a \n    finite number of predicted patterns (i.e., vertices). A formal definition of \n    inequality-constrained multinomial models and the implemented computational\n    methods is provided in: Heck, D.W., & Davis-Stober, C.P. (2019). \n    Multinomial models with linear inequality constraints: Overview and improvements \n    of computational methods for Bayesian inference. Journal of Mathematical \n    Psychology, 91, 70-87. <doi:10.1016/j.jmp.2019.03.004>.\n    Inequality-constrained multinomial models have applications in the area of \n    judgment and decision making to fit and test random utility models  \n    (Regenwetter, M., Dana, J., & Davis-Stober, C.P. (2011). Transitivity of \n    preferences. Psychological Review, 118, 42\u201356, <doi:10.1037/a0021150>) or to \n    perform outcome-based strategy classification to select the decision strategy \n    that provides the best account for a vector of observed choice frequencies \n    (Heck, D.W., Hilbig, B.E., & Moshagen, M. (2017). From information \n    processing to decisions: Formalizing and comparing probabilistic choice models. \n    Cognitive Psychology, 96, 26\u201340. <doi:10.1016/j.cogpsych.2017.05.003>).  "
  },
  {
    "id": 16604,
    "package_name": "multisite.accuracy",
    "title": "Estimation of Accuracy in Multisite Machine-Learning Models",
    "description": "The effects of the site may severely bias the accuracy of a multisite machine-learning model, even if the analysts removed them when fitting the model in the 'training set' and applying the model in the 'test set' (Solanes et al., Neuroimage 2023, 265:119800). This simple R package estimates the accuracy of a multisite machine-learning model unbiasedly, as described in (Solanes et al., Psychiatry Research: Neuroimaging 2021, 314:111313). It currently supports the estimation of sensitivity, specificity, balanced accuracy (for binary or multinomial variables), the area under the curve, correlation, mean squarer error, and hazard ratio for binomial, multinomial, gaussian, and survival (time-to-event) outcomes.",
    "version": "1.3",
    "maintainer": "Joaquim Radua <quimradua@gmail.com>",
    "author": "Joaquim Radua [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1240-5438>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multisite.accuracy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multisite.accuracy Estimation of Accuracy in Multisite Machine-Learning Models The effects of the site may severely bias the accuracy of a multisite machine-learning model, even if the analysts removed them when fitting the model in the 'training set' and applying the model in the 'test set' (Solanes et al., Neuroimage 2023, 265:119800). This simple R package estimates the accuracy of a multisite machine-learning model unbiasedly, as described in (Solanes et al., Psychiatry Research: Neuroimaging 2021, 314:111313). It currently supports the estimation of sensitivity, specificity, balanced accuracy (for binary or multinomial variables), the area under the curve, correlation, mean squarer error, and hazard ratio for binomial, multinomial, gaussian, and survival (time-to-event) outcomes.  "
  },
  {
    "id": 16695,
    "package_name": "mxsem",
    "title": "Specify 'OpenMx' Models with a 'lavaan'-Style Syntax",
    "description": "Provides a 'lavaan'-like syntax for 'OpenMx' models. The syntax supports \n  definition variables, bounds, and parameter transformations. This allows for\n  latent growth curve models with person-specific measurement occasions, moderated\n  nonlinear factor analysis and much more.",
    "version": "0.1.0",
    "maintainer": "Jannik H. Orzek <jannik.orzek@mailbox.org>",
    "author": "Jannik H. Orzek [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3123-2248>)",
    "url": "https://jhorzek.github.io/mxsem/,\nhttps://github.com/jhorzek/mxsem/",
    "bug_reports": "https://github.com/jhorzek/mxsem/issues",
    "repository": "https://cran.r-project.org/package=mxsem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mxsem Specify 'OpenMx' Models with a 'lavaan'-Style Syntax Provides a 'lavaan'-like syntax for 'OpenMx' models. The syntax supports \n  definition variables, bounds, and parameter transformations. This allows for\n  latent growth curve models with person-specific measurement occasions, moderated\n  nonlinear factor analysis and much more.  "
  },
  {
    "id": 16699,
    "package_name": "mycaas",
    "title": "My Computerized Adaptive Assessment",
    "description": "Implementation of adaptive assessment procedures based\n    on Knowledge Space Theory (KST, Doignon & Falmagne, 1999 <ISBN:9783540645016>) and Formal Psychological Assessment\n    (FPA, Spoto, Stefanutti & Vidotto, 2010 <doi:10.3758/BRM.42.1.342>) frameworks. An adaptive assessment is a type of evaluation that\n    adjusts the difficulty and nature of subsequent questions based on the\n    test taker's responses to previous ones. The package contains functions\n    to perform and simulate an adaptive assessment. Moreover, it is\n    integrated with two 'Shiny' interfaces, making it both accessible and\n    user-friendly.  The package has been partially funded by the European Union -\n    NextGenerationEU and by the Ministry of University and Research (MUR),\n    National Recovery and Resilience Plan (NRRP), Mission 4, Component 2,\n    Investment 1.5, project \u201cRAISE - Robotics and AI for Socio-economic\n    Empowerment\u201d (ECS00000035).",
    "version": "0.0.1",
    "maintainer": "Andrea Brancaccio <andreabrancaccio01@gmail.com>",
    "author": "Andrea Brancaccio [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-5919-6990>),\n  Umberto Granziol [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-6286-6569>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mycaas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mycaas My Computerized Adaptive Assessment Implementation of adaptive assessment procedures based\n    on Knowledge Space Theory (KST, Doignon & Falmagne, 1999 <ISBN:9783540645016>) and Formal Psychological Assessment\n    (FPA, Spoto, Stefanutti & Vidotto, 2010 <doi:10.3758/BRM.42.1.342>) frameworks. An adaptive assessment is a type of evaluation that\n    adjusts the difficulty and nature of subsequent questions based on the\n    test taker's responses to previous ones. The package contains functions\n    to perform and simulate an adaptive assessment. Moreover, it is\n    integrated with two 'Shiny' interfaces, making it both accessible and\n    user-friendly.  The package has been partially funded by the European Union -\n    NextGenerationEU and by the Ministry of University and Research (MUR),\n    National Recovery and Resilience Plan (NRRP), Mission 4, Component 2,\n    Investment 1.5, project \u201cRAISE - Robotics and AI for Socio-economic\n    Empowerment\u201d (ECS00000035).  "
  },
  {
    "id": 16784,
    "package_name": "neatStats",
    "title": "Neat and Painless Statistical Reporting",
    "description": "User-friendly, clear and simple statistics, primarily for\n  publication in psychological science. The main functions are wrappers for\n  other packages, but there are various additions as well. Every relevant step\n  from data aggregation to reportable printed statistics is covered for basic\n  experimental designs.",
    "version": "1.13.5",
    "maintainer": "G\u00e1sp\u00e1r Luk\u00e1cs <lkcsgaspar@gmail.com>",
    "author": "G\u00e1sp\u00e1r Luk\u00e1cs [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9401-4830>),\n  Bennett Kleinberg [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1658-9086>),\n  Johnny van Doorn [ctb] (ORCID: <https://orcid.org/0000-0003-0270-096X>)",
    "url": "https://github.com/gasparl/neatstats",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=neatStats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neatStats Neat and Painless Statistical Reporting User-friendly, clear and simple statistics, primarily for\n  publication in psychological science. The main functions are wrappers for\n  other packages, but there are various additions as well. Every relevant step\n  from data aggregation to reportable printed statistics is covered for basic\n  experimental designs.  "
  },
  {
    "id": 16851,
    "package_name": "networksem",
    "title": "Network Structural Equation Modeling",
    "description": "Several methods have been developed to integrate structural equation modeling techniques with network data analysis to examine the relationship between network and non-network data. Both node-based and edge-based information can be extracted from the network data to be used as observed variables in structural equation modeling. To facilitate the application of these methods, model specification can be performed in the familiar syntax of the 'lavaan' package, ensuring ease of use for researchers. Technical details and examples can be found at <https://bigsem.psychstat.org>. ",
    "version": "0.4",
    "maintainer": "Zhiyong Zhang <johnnyzhz@gmail.com>",
    "author": "Zhiyong Zhang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0590-2196>),\n  Ziqian Xu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=networksem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "networksem Network Structural Equation Modeling Several methods have been developed to integrate structural equation modeling techniques with network data analysis to examine the relationship between network and non-network data. Both node-based and edge-based information can be extracted from the network data to be used as observed variables in structural equation modeling. To facilitate the application of these methods, model specification can be performed in the familiar syntax of the 'lavaan' package, ensuring ease of use for researchers. Technical details and examples can be found at <https://bigsem.psychstat.org>.   "
  },
  {
    "id": 16853,
    "package_name": "networktree",
    "title": "Recursive Partitioning of Network Models",
    "description": "Network trees recursively partition the data with respect to covariates. Two network tree algorithms are available: model-based trees based on a multivariate normal model and nonparametric trees based on covariance structures. After partitioning, correlation-based networks (psychometric networks) can be fit on the partitioned data. For details see Jones, Mair, Simon, & Zeileis (2020) <doi:10.1007/s11336-020-09731-4>. ",
    "version": "1.0.1",
    "maintainer": "Payton Jones <paytonjjones@gmail.com>",
    "author": "Payton Jones [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6513-8498>),\n  Thorsten Simon [aut] (ORCID: <https://orcid.org/0000-0002-3778-7738>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "https://paytonjjones.github.io/networktree/",
    "bug_reports": "https://github.com/paytonjjones/networktree/issues",
    "repository": "https://cran.r-project.org/package=networktree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "networktree Recursive Partitioning of Network Models Network trees recursively partition the data with respect to covariates. Two network tree algorithms are available: model-based trees based on a multivariate normal model and nonparametric trees based on covariance structures. After partitioning, correlation-based networks (psychometric networks) can be fit on the partitioned data. For details see Jones, Mair, Simon, & Zeileis (2020) <doi:10.1007/s11336-020-09731-4>.   "
  },
  {
    "id": 17050,
    "package_name": "normref",
    "title": "Continuous Norming",
    "description": "A toolbox for calculating continuous norms for psychological tests, where the norms can be age-dependent. The norms are based Generalized Additive Models for Location, Scale, and Shape (GAMLSS) for the test scores in the normative sample. The package includes functions for model selection, reliability estimation, and calculating norms, including confidence intervals. For more details, see Timmerman et al. (2021) <doi:10.1037/met0000348>. ",
    "version": "0.0.0.1",
    "maintainer": "Marieke Timmerman <m.e.timmerman@rug.nl>",
    "author": "Klazien de Vries [aut] (ORCID: <https://orcid.org/0009-0007-9302-1562>),\n  Hannah Heister [aut] (ORCID: <https://orcid.org/0009-0001-1512-5549>),\n  Julian Urban [aut] (ORCID: <https://orcid.org/0000-0001-8886-4724>),\n  Lieke Voncken [ctb] (ORCID: <https://orcid.org/0000-0002-6710-271X>),\n  Marieke Timmerman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3480-5918>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=normref",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "normref Continuous Norming A toolbox for calculating continuous norms for psychological tests, where the norms can be age-dependent. The norms are based Generalized Additive Models for Location, Scale, and Shape (GAMLSS) for the test scores in the normative sample. The package includes functions for model selection, reliability estimation, and calculating norms, including confidence intervals. For more details, see Timmerman et al. (2021) <doi:10.1037/met0000348>.   "
  },
  {
    "id": 17135,
    "package_name": "numGen",
    "title": "Number Series Generator",
    "description": "A number series generator that creates number series items based on cognitive models.",
    "version": "0.1.1",
    "maintainer": "Bao Sheng Loe (Aiden) <bsl28@cam.ac.uk>",
    "author": "Bao Sheng Loe (Aiden) [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=numGen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "numGen Number Series Generator A number series generator that creates number series items based on cognitive models.  "
  },
  {
    "id": 17207,
    "package_name": "odr",
    "title": "Optimal Design and Statistical Power for Experimental Studies\nInvestigating Main, Mediation, and Moderation Effects",
    "description": "Calculate the optimal sample size allocation that \n    uses the minimum resources to achieve targeted statistical \n    power in experiments.\n    Perform power analyses with and without accommodating \n    costs and budget. The designs cover single-level \n    and multilevel experiments detecting main, mediation, \n    and moderation effects (and some combinations). \n    The references for the proposed methods include: \n    (1) Shen, Z., & Kelcey, B. (2020). \n    Optimal sample allocation under unequal costs in \n    cluster-randomized trials.\n    Journal of Educational and Behavioral Statistics, 45(4): 446-474.\n        <doi:10.3102/1076998620912418>.\n    (2) Shen, Z., & Kelcey, B. (2022b). Optimal sample allocation for\n    three-level multisite cluster-randomized trials. \n    Journal of Research on Educational Effectiveness, 15 (1), 130-150.\n    <doi:10.1080/19345747.2021.1953200>.\n    (3) Shen, Z., & Kelcey, B. (2022a). Optimal sample allocation in\n    multisite randomized trials. The Journal of Experimental Education,\n    90(3), 693-711. <doi:10.1080/00220973.2020.1830361>.\n    (4) Shen, Z., Leite, W., Zhang, H., Quan, J., & Kuang, H. (2025). \n    Using ant colony optimization to identify optimal sample \n    allocations in cluster-randomized trials. \n    The Journal of Experimental Education, 93(1), 167-185.\n    <doi:10.1080/00220973.2024.2306392>.\n    (5) Shen, Z., Li, W., & Leite, W. (in press). Statistical power and \n    optimal design for randomized controlled trials investigating \n    mediation effects. Psychological Methods.\n     <doi:10.1037/met0000698>.\n    (6) Champely, S. (2020). pwr: Basic functions for power analysis \n    (Version 1.3-0) [Software]. Available from \n    <https://CRAN.R-project.org/package=pwr>.",
    "version": "1.7.3",
    "maintainer": "Zuchao Shen <zuchao.shen@gmail.com>",
    "author": "Zuchao Shen [aut, cre] (ORCID: <https://orcid.org/0000-0003-3483-0451>),\n  Benjamin Kelcey [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=odr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "odr Optimal Design and Statistical Power for Experimental Studies\nInvestigating Main, Mediation, and Moderation Effects Calculate the optimal sample size allocation that \n    uses the minimum resources to achieve targeted statistical \n    power in experiments.\n    Perform power analyses with and without accommodating \n    costs and budget. The designs cover single-level \n    and multilevel experiments detecting main, mediation, \n    and moderation effects (and some combinations). \n    The references for the proposed methods include: \n    (1) Shen, Z., & Kelcey, B. (2020). \n    Optimal sample allocation under unequal costs in \n    cluster-randomized trials.\n    Journal of Educational and Behavioral Statistics, 45(4): 446-474.\n        <doi:10.3102/1076998620912418>.\n    (2) Shen, Z., & Kelcey, B. (2022b). Optimal sample allocation for\n    three-level multisite cluster-randomized trials. \n    Journal of Research on Educational Effectiveness, 15 (1), 130-150.\n    <doi:10.1080/19345747.2021.1953200>.\n    (3) Shen, Z., & Kelcey, B. (2022a). Optimal sample allocation in\n    multisite randomized trials. The Journal of Experimental Education,\n    90(3), 693-711. <doi:10.1080/00220973.2020.1830361>.\n    (4) Shen, Z., Leite, W., Zhang, H., Quan, J., & Kuang, H. (2025). \n    Using ant colony optimization to identify optimal sample \n    allocations in cluster-randomized trials. \n    The Journal of Experimental Education, 93(1), 167-185.\n    <doi:10.1080/00220973.2024.2306392>.\n    (5) Shen, Z., Li, W., & Leite, W. (in press). Statistical power and \n    optimal design for randomized controlled trials investigating \n    mediation effects. Psychological Methods.\n     <doi:10.1037/met0000698>.\n    (6) Champely, S. (2020). pwr: Basic functions for power analysis \n    (Version 1.3-0) [Software]. Available from \n    <https://CRAN.R-project.org/package=pwr>.  "
  },
  {
    "id": 17388,
    "package_name": "ordinalTables",
    "title": "Fit Models to Two-Way Tables with Correlated Ordered Response\nCategories",
    "description": "Fit a variety of models to two-way tables with ordered categories.\n  Most of the models are appropriate to apply to tables of that have correlated ordered\n  response categories.  There is a particular interest in rater data and models for rescore\n  tables. Some utility functions (e.g., Cohen's kappa and weighted kappa) support\n  more general work on rater agreement.\n  Because the names of the models are very similar, the functions that implement them are\n  organized by last name of the primary author of the article or book that suggested the model,\n  with the name of the function beginning with that author's name and an underscore.  This\n  may make some models more difficult to locate if one doesn't have the original sources.  The\n  vignettes and tests can help to locate models of interest.  For more dertaiils see the following references:\n  Agresti, A. (1983) <doi:10.1016/0167-7152(83)90051-2> \"A Simple Diagonals-Parameter Symmetry And Quasi-Symmetry Model\",\n  Agrestim A. (1983) <doi:10.2307/2531022> \"Testing Marginal Homogeneity for Ordinal Categorical Variables\",\n  Agresti, A. (1988) <doi:10.2307/2531866> \"A Model For Agreement Between Ratings On An Ordinal Scale\",\n  Agresti, A. (1989) <doi:10.1016/0167-7152(89)90104-1> \"An Agreement Model With Kappa As Parameter\",\n  Agresti, A. (2010 ISBN:978-0470082898) \"Analysis Of Ordinal Categorical Data\",\n  Bhapkar, V. P. (1966) <doi:10.1080/01621459.1966.10502021> \"A Note On The Equivalence Of Two Test Criteria For Hypotheses In Categorical Data\",\n  Bhapkar, V. P. (1979) <doi:10.2307/2530344> \"On Tests Of Marginal Symmetry And Quasi-Symmetry In Two And Three-Dimensional Contingency Tables\",\n  Bowker, A. H. (1948) <doi:10.2307/2280710> \"A Test For Symmetry In Contingency Tables\",\n  Clayton, D. G. (1974) <doi:10.2307/2335638> \"Some Odds Ratio Statistics For The Analysis Of Ordered Categorical Data\",\n  Cliff, N. (1993) <doi:10.1037/0033-2909.114.3.494> \"Dominance Statistics: Ordinal Analyses To Answer Ordinal Questions\",\n  Cliff, N. (1996 ISBN:978-0805813333) \"Ordinal Methods For Behavioral Data Analysis\",\n  Goodman, L. A. (1979) <doi:10.1080/01621459.1979.10481650> \"Simple Models For The Analysis Of Association In Cross-Classifications Having Ordered Categories\",\n  Goodman, L. A. (1979) <doi:10.2307/2335159> \"Multiplicative Models For Square Contingency Tables With Ordered Categories\",\n  Ireland, C. T., Ku, H. H., & Kullback, S. (1969) <doi:10.2307/2286071> \"Symmetry And Marginal Homogeneity Of An r \u00d7 r Contingency Table\",\n  Ishi-kuntz, M. (1994 ISBN:978-0803943766) \"Ordinal Log-linear Models\",\n  McCullah, P. (1977) <doi:10.2307/2345320> \"A Logistic Model For Paired Comparisons With Ordered Categorical Data\",\n  McCullagh, P. (1978) <doi:10.2307/2335224> A Class Of Parametric Models For The Analysis Of Square Contingency Tables With Ordered Categories\",\n  McCullagh, P. (1980) <doi:10.1111/j.2517-6161.1980.tb01109.x> \"Regression Models For Ordinal Data\",\n  Penn State: Eberly College of Science (undated) <https://online.stat.psu.edu/stat504/lesson/11> \"Stat 504: Analysis of Discrete Data, 11. Advanced Topics I\",\n  Schuster, C. (2001) <doi:10.3102/10769986026003331> \"Kappa As A Parameter Of A Symmetry Model For Rater Agreement\",\n  Shoukri, M. M. (2004 ISBN:978-1584883210). \"Measures Of Interobserver Agreement\",\n  Stuart, A. (1953) <doi:10.2307/2333101> \"The Estimation Of And Comparison Of Strengths Of Association In Contingency Tables\",\n  Stuart, A. (1955) <doi:10.2307/2333387> \"A Test For Homogeneity Of The Marginal Distributions In A Two-Way Classification\",\n  von Eye, A., & Mun, E. Y. (2005 ISBN:978-0805849677) \"Analyzing Rater Agreement: Manifest Variable Methods\".",
    "version": "1.0.0.3",
    "maintainer": "John R. Donoghue <jdonoghue0823@gmail.com>",
    "author": "John R. Donoghue [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ordinalTables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ordinalTables Fit Models to Two-Way Tables with Correlated Ordered Response\nCategories Fit a variety of models to two-way tables with ordered categories.\n  Most of the models are appropriate to apply to tables of that have correlated ordered\n  response categories.  There is a particular interest in rater data and models for rescore\n  tables. Some utility functions (e.g., Cohen's kappa and weighted kappa) support\n  more general work on rater agreement.\n  Because the names of the models are very similar, the functions that implement them are\n  organized by last name of the primary author of the article or book that suggested the model,\n  with the name of the function beginning with that author's name and an underscore.  This\n  may make some models more difficult to locate if one doesn't have the original sources.  The\n  vignettes and tests can help to locate models of interest.  For more dertaiils see the following references:\n  Agresti, A. (1983) <doi:10.1016/0167-7152(83)90051-2> \"A Simple Diagonals-Parameter Symmetry And Quasi-Symmetry Model\",\n  Agrestim A. (1983) <doi:10.2307/2531022> \"Testing Marginal Homogeneity for Ordinal Categorical Variables\",\n  Agresti, A. (1988) <doi:10.2307/2531866> \"A Model For Agreement Between Ratings On An Ordinal Scale\",\n  Agresti, A. (1989) <doi:10.1016/0167-7152(89)90104-1> \"An Agreement Model With Kappa As Parameter\",\n  Agresti, A. (2010 ISBN:978-0470082898) \"Analysis Of Ordinal Categorical Data\",\n  Bhapkar, V. P. (1966) <doi:10.1080/01621459.1966.10502021> \"A Note On The Equivalence Of Two Test Criteria For Hypotheses In Categorical Data\",\n  Bhapkar, V. P. (1979) <doi:10.2307/2530344> \"On Tests Of Marginal Symmetry And Quasi-Symmetry In Two And Three-Dimensional Contingency Tables\",\n  Bowker, A. H. (1948) <doi:10.2307/2280710> \"A Test For Symmetry In Contingency Tables\",\n  Clayton, D. G. (1974) <doi:10.2307/2335638> \"Some Odds Ratio Statistics For The Analysis Of Ordered Categorical Data\",\n  Cliff, N. (1993) <doi:10.1037/0033-2909.114.3.494> \"Dominance Statistics: Ordinal Analyses To Answer Ordinal Questions\",\n  Cliff, N. (1996 ISBN:978-0805813333) \"Ordinal Methods For Behavioral Data Analysis\",\n  Goodman, L. A. (1979) <doi:10.1080/01621459.1979.10481650> \"Simple Models For The Analysis Of Association In Cross-Classifications Having Ordered Categories\",\n  Goodman, L. A. (1979) <doi:10.2307/2335159> \"Multiplicative Models For Square Contingency Tables With Ordered Categories\",\n  Ireland, C. T., Ku, H. H., & Kullback, S. (1969) <doi:10.2307/2286071> \"Symmetry And Marginal Homogeneity Of An r \u00d7 r Contingency Table\",\n  Ishi-kuntz, M. (1994 ISBN:978-0803943766) \"Ordinal Log-linear Models\",\n  McCullah, P. (1977) <doi:10.2307/2345320> \"A Logistic Model For Paired Comparisons With Ordered Categorical Data\",\n  McCullagh, P. (1978) <doi:10.2307/2335224> A Class Of Parametric Models For The Analysis Of Square Contingency Tables With Ordered Categories\",\n  McCullagh, P. (1980) <doi:10.1111/j.2517-6161.1980.tb01109.x> \"Regression Models For Ordinal Data\",\n  Penn State: Eberly College of Science (undated) <https://online.stat.psu.edu/stat504/lesson/11> \"Stat 504: Analysis of Discrete Data, 11. Advanced Topics I\",\n  Schuster, C. (2001) <doi:10.3102/10769986026003331> \"Kappa As A Parameter Of A Symmetry Model For Rater Agreement\",\n  Shoukri, M. M. (2004 ISBN:978-1584883210). \"Measures Of Interobserver Agreement\",\n  Stuart, A. (1953) <doi:10.2307/2333101> \"The Estimation Of And Comparison Of Strengths Of Association In Contingency Tables\",\n  Stuart, A. (1955) <doi:10.2307/2333387> \"A Test For Homogeneity Of The Marginal Distributions In A Two-Way Classification\",\n  von Eye, A., & Mun, E. Y. (2005 ISBN:978-0805849677) \"Analyzing Rater Agreement: Manifest Variable Methods\".  "
  },
  {
    "id": 17564,
    "package_name": "papaja",
    "title": "Prepare American Psychological Association Journal Articles with\nR Markdown",
    "description": "Tools to create dynamic, submission-ready manuscripts, which\n  conform to American Psychological Association manuscript guidelines. We\n  provide R Markdown document formats for manuscripts (PDF and Word) and\n  revision letters (PDF). Helper functions facilitate reporting statistical\n  analyses or create publication-ready tables and plots.",
    "version": "0.1.4",
    "maintainer": "Frederik Aust <frederik.aust@uni-koeln.de>",
    "author": "Frederik Aust [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4900-788X>),\n  Marius Barth [aut] (ORCID: <https://orcid.org/0000-0002-3421-6665>),\n  Birk Diedenhofen [ctb],\n  Christoph Stahl [ctb],\n  Joseph V. Casillas [ctb],\n  Rudolf Siegel [ctb]",
    "url": "https://github.com/crsh/papaja",
    "bug_reports": "https://github.com/crsh/papaja/issues",
    "repository": "https://cran.r-project.org/package=papaja",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "papaja Prepare American Psychological Association Journal Articles with\nR Markdown Tools to create dynamic, submission-ready manuscripts, which\n  conform to American Psychological Association manuscript guidelines. We\n  provide R Markdown document formats for manuscripts (PDF and Word) and\n  revision letters (PDF). Helper functions facilitate reporting statistical\n  analyses or create publication-ready tables and plots.  "
  },
  {
    "id": 17572,
    "package_name": "parafac4microbiome",
    "title": "Parallel Factor Analysis Modelling of Longitudinal Microbiome\nData",
    "description": "Creation and selection of PARAllel FACtor Analysis (PARAFAC)\n    models of longitudinal microbiome data. You can import your own data with\n    our import functions or use one of the example datasets to create your own\n    PARAFAC models. Selection of the optimal number of components can be done\n    using assessModelQuality() and assessModelStability(). The selected model can\n    then be plotted using plotPARAFACmodel(). The Parallel Factor\n    Analysis method was originally described by Caroll and Chang (1970)\n    <doi:10.1007/BF02310791> and Harshman (1970)\n    <https://www.psychology.uwo.ca/faculty/harshman/wpppfac0.pdf>.",
    "version": "1.3.2",
    "maintainer": "Geert Roelof van der Ploeg <g.r.ploeg@uva.nl>",
    "author": "Geert Roelof van der Ploeg [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-5204-3386>),\n  Johan Westerhuis [ctb] (ORCID: <https://orcid.org/0000-0002-6747-9779>),\n  Anna Heintz-Buschart [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9780-1933>),\n  Age Smilde [ctb] (ORCID: <https://orcid.org/0000-0002-3052-4644>),\n  University of Amsterdam [cph, fnd]",
    "url": "https://grvanderploeg.com/parafac4microbiome/,\nhttps://github.com/GRvanderPloeg/parafac4microbiome/",
    "bug_reports": "https://github.com/GRvanderPloeg/parafac4microbiome/issues",
    "repository": "https://cran.r-project.org/package=parafac4microbiome",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parafac4microbiome Parallel Factor Analysis Modelling of Longitudinal Microbiome\nData Creation and selection of PARAllel FACtor Analysis (PARAFAC)\n    models of longitudinal microbiome data. You can import your own data with\n    our import functions or use one of the example datasets to create your own\n    PARAFAC models. Selection of the optimal number of components can be done\n    using assessModelQuality() and assessModelStability(). The selected model can\n    then be plotted using plotPARAFACmodel(). The Parallel Factor\n    Analysis method was originally described by Caroll and Chang (1970)\n    <doi:10.1007/BF02310791> and Harshman (1970)\n    <https://www.psychology.uwo.ca/faculty/harshman/wpppfac0.pdf>.  "
  },
  {
    "id": 17621,
    "package_name": "passt",
    "title": "Probability Associator Time (PASS-T)",
    "description": "Simulates judgments of frequency and duration based on\n    the Probability Associator Time (PASS-T) model. PASS-T is a memory\n    model based on a simple competitive artificial neural network. It \n    can imitate human judgments of frequency and duration, which have\n    been extensively studied in cognitive psychology\n    (e.g. Hintzman (1970) <doi:10.1037/h0028865>, Betsch et al. (2010)\n    <https://psycnet.apa.org/record/2010-18204-003>). The PASS-T model\n    is an extension of the PASS model (Sedlmeier, 2002,\n    ISBN:0198508638). The package provides an easy way to run\n    simulations, which can then be compared with empirical data in\n    human judgments of frequency and duration.",
    "version": "0.1.3",
    "maintainer": "Johannes Titz <johannes.titz@gmail.com>",
    "author": "Johannes Titz [aut, cre]",
    "url": "https://github.com/johannes-titz/passt",
    "bug_reports": "https://github.com/johannes-titz/passt/issues",
    "repository": "https://cran.r-project.org/package=passt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "passt Probability Associator Time (PASS-T) Simulates judgments of frequency and duration based on\n    the Probability Associator Time (PASS-T) model. PASS-T is a memory\n    model based on a simple competitive artificial neural network. It \n    can imitate human judgments of frequency and duration, which have\n    been extensively studied in cognitive psychology\n    (e.g. Hintzman (1970) <doi:10.1037/h0028865>, Betsch et al. (2010)\n    <https://psycnet.apa.org/record/2010-18204-003>). The PASS-T model\n    is an extension of the PASS model (Sedlmeier, 2002,\n    ISBN:0198508638). The package provides an easy way to run\n    simulations, which can then be compared with empirical data in\n    human judgments of frequency and duration.  "
  },
  {
    "id": 17624,
    "package_name": "pastboon",
    "title": "Simulation of Parameterized Stochastic Boolean Networks",
    "description": "A Boolean network is a particular kind of discrete dynamical system where the variables are simple binary switches. Despite its simplicity, Boolean network modeling has been a successful method to describe the behavioral pattern of various phenomena. Applying stochastic noise to Boolean networks is a useful approach for representing the effects of various perturbing stimuli on complex systems. A number of methods have been developed to control noise effects on Boolean networks using parameters integrated into the update rules. This package provides functions to examine three such methods: Boolean network with perturbations (BNp), described by Trairatphisan et al. (2013) <doi:10.1186/1478-811X-11-46>, stochastic discrete dynamical systems (SDDS), proposed by Murrugarra et al. (2012) <doi:10.1186/1687-4153-2012-5>, and Boolean network with probabilistic edge weights (PEW), presented by Deritei et al. (2022) <doi:10.1371/journal.pcbi.1010536>. This package includes source code derived from the 'BoolNet' package, which is licensed under the Artistic License 2.0.",
    "version": "0.1.4",
    "maintainer": "Mohammad Taheri-Ledari <mo.taheri@ut.ac.ir>",
    "author": "Mohammad Taheri-Ledari [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0007-9132-077X>),\n  Kaveh Kavousi [ctb] (ORCID: <https://orcid.org/0000-0002-1906-3912>),\n  Sayed-Amir Marashi [ctb] (ORCID:\n    <https://orcid.org/0000-0001-9801-7449>),\n  Authors of BoolNet [ctb] (Original authors of the BoolNet package),\n  Troy D. Hanson [ctb] (Contributed uthash macros)",
    "url": "",
    "bug_reports": "https://github.com/taherimo/pastboon/issues",
    "repository": "https://cran.r-project.org/package=pastboon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pastboon Simulation of Parameterized Stochastic Boolean Networks A Boolean network is a particular kind of discrete dynamical system where the variables are simple binary switches. Despite its simplicity, Boolean network modeling has been a successful method to describe the behavioral pattern of various phenomena. Applying stochastic noise to Boolean networks is a useful approach for representing the effects of various perturbing stimuli on complex systems. A number of methods have been developed to control noise effects on Boolean networks using parameters integrated into the update rules. This package provides functions to examine three such methods: Boolean network with perturbations (BNp), described by Trairatphisan et al. (2013) <doi:10.1186/1478-811X-11-46>, stochastic discrete dynamical systems (SDDS), proposed by Murrugarra et al. (2012) <doi:10.1186/1687-4153-2012-5>, and Boolean network with probabilistic edge weights (PEW), presented by Deritei et al. (2022) <doi:10.1371/journal.pcbi.1010536>. This package includes source code derived from the 'BoolNet' package, which is licensed under the Artistic License 2.0.  "
  },
  {
    "id": 17642,
    "package_name": "pattern.checks",
    "title": "Identifies Patterned Responses in Scales",
    "description": "Identifies the entries with patterned responses for psychometric scales. The patterns included in the package are identical (a, a, a), ascending (a, b, c), descending (c, b, a), alternative (a, b, a, b / a, b, c, a, b, c).",
    "version": "0.1.0",
    "maintainer": "JASEEL C K <jzlckclt@gmail.com>",
    "author": "JASEEL C K [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pattern.checks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pattern.checks Identifies Patterned Responses in Scales Identifies the entries with patterned responses for psychometric scales. The patterns included in the package are identical (a, a, a), ascending (a, b, c), descending (c, b, a), alternative (a, b, a, b / a, b, c, a, b, c).  "
  },
  {
    "id": 17705,
    "package_name": "pcpr",
    "title": "Principal Component Pursuit for Environmental Epidemiology",
    "description": "Implementation of the pattern recognition technique Principal\n    Component Pursuit tailored to environmental health data, as described\n    in Gibson et al (2022) <doi:10.1289/EHP10479>.",
    "version": "1.0.0",
    "maintainer": "Lawrence G. Chillrud <lawrencechillrud@gmail.com>",
    "author": "Lawrence G. Chillrud [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0727-0161>),\n  Jaime Benavides [aut] (ORCID: <https://orcid.org/0000-0002-1851-5155>),\n  Elizabeth A. Gibson [aut] (ORCID:\n    <https://orcid.org/0000-0001-5119-5133>),\n  Junhui Zhang [aut] (ORCID: <https://orcid.org/0009-0008-5922-1058>),\n  Jingkai Yan [aut] (ORCID: <https://orcid.org/0000-0002-2094-2092>),\n  John N. Wright [aut],\n  Jeff Goldsmith [aut],\n  Marianthi-Anna Kioumourtzoglou [aut] (ORCID:\n    <https://orcid.org/0000-0001-5710-4992>),\n  Columbia University [fnd] (ROR: <https://ror.org/00hj8s172>)",
    "url": "https://columbia-prime.github.io/pcpr/,\nhttps://github.com/Columbia-PRIME/pcpr",
    "bug_reports": "https://github.com/Columbia-PRIME/pcpr/issues",
    "repository": "https://cran.r-project.org/package=pcpr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcpr Principal Component Pursuit for Environmental Epidemiology Implementation of the pattern recognition technique Principal\n    Component Pursuit tailored to environmental health data, as described\n    in Gibson et al (2022) <doi:10.1289/EHP10479>.  "
  },
  {
    "id": 17812,
    "package_name": "personr",
    "title": "Test Your Personality",
    "description": "An R-package-version of an open online science-based personality\n    test from <https://openpsychometrics.org/tests/IPIP-BFFM/>,\n    providing a better-designed interface and a more detailed report.\n    The core command launch_test() opens a personality test in your browser,\n    and generates a report after you click \"Submit\". In this report, your results\n    are compared with other people's, to show what these results mean.\n    Other people's data is from <https://openpsychometrics.org/_rawdata/BIG5.zip>.",
    "version": "1.0.0",
    "maintainer": "Renfei Mao <renfeimao@gmail.com>",
    "author": "Renfei Mao",
    "url": "https://github.com/flujoo/personr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=personr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "personr Test Your Personality An R-package-version of an open online science-based personality\n    test from <https://openpsychometrics.org/tests/IPIP-BFFM/>,\n    providing a better-designed interface and a more detailed report.\n    The core command launch_test() opens a personality test in your browser,\n    and generates a report after you click \"Submit\". In this report, your results\n    are compared with other people's, to show what these results mean.\n    Other people's data is from <https://openpsychometrics.org/_rawdata/BIG5.zip>.  "
  },
  {
    "id": 17814,
    "package_name": "persval",
    "title": "Computing Personal Values Scores",
    "description": "Compute personal values scores from various questionnaires based on the theoretical constructs proposed by professor Shalom H. Schwartz. Designed for researchers and practitioners in psychology, sociology, and related fields, the package facilitates the quantification and visualization of different dimensions related to personal values from survey data. It incorporates the recommended statistical adjustment to enhance the accuracy and interpretation of the results.   ",
    "version": "1.1.2",
    "maintainer": "Giuseppe Corbelli <giuseppe.corbelli@uninettunouniversity.net>",
    "author": "Giuseppe Corbelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2864-3548>)",
    "url": "https://github.com/g-corbelli/persval",
    "bug_reports": "https://github.com/g-corbelli/persval/issues",
    "repository": "https://cran.r-project.org/package=persval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "persval Computing Personal Values Scores Compute personal values scores from various questionnaires based on the theoretical constructs proposed by professor Shalom H. Schwartz. Designed for researchers and practitioners in psychology, sociology, and related fields, the package facilitates the quantification and visualization of different dimensions related to personal values from survey data. It incorporates the recommended statistical adjustment to enhance the accuracy and interpretation of the results.     "
  },
  {
    "id": 17819,
    "package_name": "pervasive",
    "title": "Pervasiveness Functions for Correlational Data",
    "description": "Analysis of pervasiveness of effects in correlational data. The Observed Proportion (or Percentage) of Concordant Pairs (OPCP) is Kendall's Tau expressed on a 0 to 1 metric instead of the traditional -1 to 1 metric to facilitate interpretation. As its name implies, it represents the proportion of concordant pairs in a sample (with an adjustment for ties). Pairs are concordant when a participant who has a larger value on a variable than another participant also has a larger value on a second variable. The OPCP is therefore an easily interpretable indicator of monotonicity. The pervasive functions are essentially wrappers for the 'arules' package by Hahsler et al. (2025)<doi:10.32614/CRAN.package.arules> and serve to count individuals who actually display the pattern(s) suggested by a regression. For more details, see the paper \"Considering approaches to pervasiveness in the context of personality psychology\" now accepted at the journal Personality Science.",
    "version": "1.0",
    "maintainer": "Denis Lajoie <denis.lajoie@umoncton.ca>",
    "author": "Denis Lajoie [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pervasive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pervasive Pervasiveness Functions for Correlational Data Analysis of pervasiveness of effects in correlational data. The Observed Proportion (or Percentage) of Concordant Pairs (OPCP) is Kendall's Tau expressed on a 0 to 1 metric instead of the traditional -1 to 1 metric to facilitate interpretation. As its name implies, it represents the proportion of concordant pairs in a sample (with an adjustment for ties). Pairs are concordant when a participant who has a larger value on a variable than another participant also has a larger value on a second variable. The OPCP is therefore an easily interpretable indicator of monotonicity. The pervasive functions are essentially wrappers for the 'arules' package by Hahsler et al. (2025)<doi:10.32614/CRAN.package.arules> and serve to count individuals who actually display the pattern(s) suggested by a regression. For more details, see the paper \"Considering approaches to pervasiveness in the context of personality psychology\" now accepted at the journal Personality Science.  "
  },
  {
    "id": 17822,
    "package_name": "petersenlab",
    "title": "A Collection of R Functions by the Petersen Lab",
    "description": "A collection of R functions that are widely used by the Petersen\n    Lab. Included are functions for various purposes, including evaluating the\n    accuracy of judgments and predictions, performing scoring of assessments,\n    generating correlation matrices, conversion of data between various types,\n    data management, psychometric evaluation, extensions related to latent\n    variable modeling, various plotting capabilities, and other miscellaneous\n    useful functions. By making the package available, we hope to make our\n    methods reproducible and replicable by others and to help others perform\n    their data processing and analysis methods more easily and efficiently. The\n    codebase is provided in Petersen (2025) <doi:10.5281/zenodo.7602890> and on\n    'CRAN': <doi: 10.32614/CRAN.package.petersenlab>. The package is described\n    in \"Principles of Psychological Assessment: With Applied Examples in R\"\n    (Petersen, 2024, 2025a) <doi:10.1201/9781003357421>,\n    <doi:10.25820/work.007199>, <doi:10.5281/zenodo.6466589> and in \"Fantasy\n    Football Analytics: Statistics, Prediction, and Empiricism Using R\"\n    (Petersen, 2025b).",
    "version": "1.2.0",
    "maintainer": "Isaac T. Petersen <isaac-t-petersen@uiowa.edu>",
    "author": "Isaac T. Petersen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3072-6673>),\n  Developmental Psychopathology Lab at the University of Iowa [ctb],\n  Angela D. Staples [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7678-5794>),\n  Johanna Caskey [ctb] (ORCID: <https://orcid.org/0009-0001-8227-6603>),\n  Philipp Doebler [ctb] (ORCID: <https://orcid.org/0000-0002-2946-8526>),\n  Loreen Sabel [ctb] (ORCID: <https://orcid.org/0000-0002-9832-8842>)",
    "url": "https://github.com/DevPsyLab/petersenlab,\nhttps://devpsylab.github.io/petersenlab/",
    "bug_reports": "https://github.com/DevPsyLab/petersenlab/issues",
    "repository": "https://cran.r-project.org/package=petersenlab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "petersenlab A Collection of R Functions by the Petersen Lab A collection of R functions that are widely used by the Petersen\n    Lab. Included are functions for various purposes, including evaluating the\n    accuracy of judgments and predictions, performing scoring of assessments,\n    generating correlation matrices, conversion of data between various types,\n    data management, psychometric evaluation, extensions related to latent\n    variable modeling, various plotting capabilities, and other miscellaneous\n    useful functions. By making the package available, we hope to make our\n    methods reproducible and replicable by others and to help others perform\n    their data processing and analysis methods more easily and efficiently. The\n    codebase is provided in Petersen (2025) <doi:10.5281/zenodo.7602890> and on\n    'CRAN': <doi: 10.32614/CRAN.package.petersenlab>. The package is described\n    in \"Principles of Psychological Assessment: With Applied Examples in R\"\n    (Petersen, 2024, 2025a) <doi:10.1201/9781003357421>,\n    <doi:10.25820/work.007199>, <doi:10.5281/zenodo.6466589> and in \"Fantasy\n    Football Analytics: Statistics, Prediction, and Empiricism Using R\"\n    (Petersen, 2025b).  "
  },
  {
    "id": 17920,
    "package_name": "phylosem",
    "title": "Phylogenetic Structural Equation Model",
    "description": "Applies phylogenetic comparative methods (PCM) and phylogenetic trait imputation using \n    structural equation models (SEM), extending methods from Thorson et al. (2023) <doi:10.1111/2041-210X.14076>.  \n    This implementation includes a minimal set of features, to \n    allow users to easily read all of the documentation and source code.  PCM using SEM \n    includes phylogenetic linear models and structural equation models as nested submodels, \n    but also allows imputation of missing values.  Features and comparison with other packages\n    are described in Thorson and van der Bijl (2023) <doi:10.1111/jeb.14234>. ",
    "version": "1.1.4",
    "maintainer": "James Thorson <James.Thorson@noaa.gov>",
    "author": "James Thorson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7415-1010>),\n  Wouter van der Bijl [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7366-1868>)",
    "url": "https://james-thorson-noaa.github.io/phylosem/",
    "bug_reports": "https://github.com/James-Thorson-NOAA/phylosem/issues",
    "repository": "https://cran.r-project.org/package=phylosem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylosem Phylogenetic Structural Equation Model Applies phylogenetic comparative methods (PCM) and phylogenetic trait imputation using \n    structural equation models (SEM), extending methods from Thorson et al. (2023) <doi:10.1111/2041-210X.14076>.  \n    This implementation includes a minimal set of features, to \n    allow users to easily read all of the documentation and source code.  PCM using SEM \n    includes phylogenetic linear models and structural equation models as nested submodels, \n    but also allows imputation of missing values.  Features and comparison with other packages\n    are described in Thorson and van der Bijl (2023) <doi:10.1111/jeb.14234>.   "
  },
  {
    "id": 17967,
    "package_name": "pisaRT",
    "title": "Small Example Response and Response Time Data from PISA 2018",
    "description": "Scored responses and responses times from the Canadian subsample of the PISA 2018 assessment, accessible as the \"Cognitive items total time/visits data file\" by OECD (2020) <https://www.oecd.org/pisa/data/2018database/>. ",
    "version": "2.0.2",
    "maintainer": "Benjamin Becker <b.becker@iqb.hu-berlin.de>",
    "author": "Benjamin Becker [aut, cre],\n  Esther Ulitzsch [ctb],\n  Christoph Koenig [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pisaRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pisaRT Small Example Response and Response Time Data from PISA 2018 Scored responses and responses times from the Canadian subsample of the PISA 2018 assessment, accessible as the \"Cognitive items total time/visits data file\" by OECD (2020) <https://www.oecd.org/pisa/data/2018database/>.   "
  },
  {
    "id": 18169,
    "package_name": "pompom",
    "title": "Person-Oriented Method and Perturbation on the Model",
    "description": "An implementation of a hybrid method of person-oriented method and perturbation on the model. Pompom is the initials of the two methods. The hybrid method will provide a multivariate intraindividual variability metric (iRAM). The person-oriented method used in this package refers to uSEM (unified structural equation modeling, see Kim et al., 2007, Gates et al., 2010 and Gates et al., 2012 for details). Perturbation on the model was conducted according to impulse response analysis introduced in Lutkepohl (2007). \n    Kim, J., Zhu, W., Chang, L., Bentler, P. M., & Ernst, T. (2007) <doi:10.1002/hbm.20259>. \n    Gates, K. M., Molenaar, P. C. M., Hillary, F. G., Ram, N., & Rovine, M. J. (2010) <doi:10.1016/j.neuroimage.2009.12.117>. \n    Gates, K. M., & Molenaar, P. C. M. (2012) <doi:10.1016/j.neuroimage.2012.06.026>. \n    Lutkepohl, H. (2007, ISBN:3540262393).",
    "version": "0.2.1",
    "maintainer": "Xiao Yang <vwendy@gmail.com>",
    "author": "Xiao Yang [cre, aut],\n  Nilam Ram [aut],\n  Peter Molenaar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pompom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pompom Person-Oriented Method and Perturbation on the Model An implementation of a hybrid method of person-oriented method and perturbation on the model. Pompom is the initials of the two methods. The hybrid method will provide a multivariate intraindividual variability metric (iRAM). The person-oriented method used in this package refers to uSEM (unified structural equation modeling, see Kim et al., 2007, Gates et al., 2010 and Gates et al., 2012 for details). Perturbation on the model was conducted according to impulse response analysis introduced in Lutkepohl (2007). \n    Kim, J., Zhu, W., Chang, L., Bentler, P. M., & Ernst, T. (2007) <doi:10.1002/hbm.20259>. \n    Gates, K. M., Molenaar, P. C. M., Hillary, F. G., Ram, N., & Rovine, M. J. (2010) <doi:10.1016/j.neuroimage.2009.12.117>. \n    Gates, K. M., & Molenaar, P. C. M. (2012) <doi:10.1016/j.neuroimage.2012.06.026>. \n    Lutkepohl, H. (2007, ISBN:3540262393).  "
  },
  {
    "id": 18233,
    "package_name": "power4mome",
    "title": "Power Analysis for Moderation and Mediation",
    "description": "Power analysis and sample size determination\n  for moderation, mediation, and moderated mediation in models\n  fitted by structural equation modelling using the 'lavaan'\n  package by Rosseel (2012) <doi:10.18637/jss.v048.i02> or\n  by multiple regression. The package 'manymome' by\n  Cheung and Cheung (2024) <doi:10.3758/s13428-023-02224-z>\n  is used to specify the indirect paths or conditional\n  indirect paths to be tested.",
    "version": "0.1.1",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "author": "Shu Fai Cheung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9871-9448>),\n  Sing-Hang Cheung [aut] (ORCID: <https://orcid.org/0000-0001-5182-0752>),\n  Wendie Yang [aut] (ORCID: <https://orcid.org/0009-0000-8388-6481>)",
    "url": "https://sfcheung.github.io/power4mome/",
    "bug_reports": "https://github.com/sfcheung/power4mome/issues",
    "repository": "https://cran.r-project.org/package=power4mome",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "power4mome Power Analysis for Moderation and Mediation Power analysis and sample size determination\n  for moderation, mediation, and moderated mediation in models\n  fitted by structural equation modelling using the 'lavaan'\n  package by Rosseel (2012) <doi:10.18637/jss.v048.i02> or\n  by multiple regression. The package 'manymome' by\n  Cheung and Cheung (2024) <doi:10.3758/s13428-023-02224-z>\n  is used to specify the indirect paths or conditional\n  indirect paths to be tested.  "
  },
  {
    "id": 18240,
    "package_name": "powerNLSEM",
    "title": "Simulation-Based Power Estimation (MSPE) for Nonlinear SEM",
    "description": "Model-implied simulation-based power estimation (MSPE) for\n    nonlinear (and linear) SEM, path analysis and regression analysis. A\n    theoretical framework is used to approximate the relation between\n    power and sample size for given type I error rates and effect sizes.\n    The package offers an adaptive search algorithm to find the optimal N for\n    given effect sizes and type I error rates. Plots can be used to visualize\n    the power relation to N for different parameters of interest (POI). \n    Theoretical justifications are given in Irmer et al. \n    (2024a) <doi:10.31219/osf.io/pe5bj> and detailed description\n    are given in Irmer et al. (2024b) <doi:10.3758/s13428-024-02476-3>.",
    "version": "0.1.2",
    "maintainer": "Julien Patrick Irmer <jpirmer@gmail.com>",
    "author": "Julien Patrick Irmer [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7544-6483>)",
    "url": "https://github.com/jpirmer/powerNLSEM",
    "bug_reports": "https://github.com/jpirmer/powerNLSEM/issues",
    "repository": "https://cran.r-project.org/package=powerNLSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerNLSEM Simulation-Based Power Estimation (MSPE) for Nonlinear SEM Model-implied simulation-based power estimation (MSPE) for\n    nonlinear (and linear) SEM, path analysis and regression analysis. A\n    theoretical framework is used to approximate the relation between\n    power and sample size for given type I error rates and effect sizes.\n    The package offers an adaptive search algorithm to find the optimal N for\n    given effect sizes and type I error rates. Plots can be used to visualize\n    the power relation to N for different parameters of interest (POI). \n    Theoretical justifications are given in Irmer et al. \n    (2024a) <doi:10.31219/osf.io/pe5bj> and detailed description\n    are given in Irmer et al. (2024b) <doi:10.3758/s13428-024-02476-3>.  "
  },
  {
    "id": 18249,
    "package_name": "powerly",
    "title": "Sample Size Analysis for Psychological Networks and More",
    "description": "An implementation of the sample size computation method for network\n    models proposed by Constantin et al. (2023) <doi:10.1037/met0000555>.\n    The implementation takes the form of a three-step recursive algorithm\n    designed to find an optimal sample size given a model specification and a\n    performance measure of interest. It starts with a Monte Carlo simulation\n    step for computing the performance measure and a statistic at various sample\n    sizes selected from an initial sample size range. It continues with a\n    monotone curve-fitting step for interpolating the statistic across the entire\n    sample size range. The final step employs stratified bootstrapping to quantify\n    the uncertainty around the fitted curve.",
    "version": "1.10.0",
    "maintainer": "Mihai Constantin <mihai@mihaiconstantin.com>",
    "author": "Mihai Constantin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6460-0107>)",
    "url": "https://powerly.dev",
    "bug_reports": "https://github.com/mihaiconstantin/powerly/issues",
    "repository": "https://cran.r-project.org/package=powerly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerly Sample Size Analysis for Psychological Networks and More An implementation of the sample size computation method for network\n    models proposed by Constantin et al. (2023) <doi:10.1037/met0000555>.\n    The implementation takes the form of a three-step recursive algorithm\n    designed to find an optimal sample size given a model specification and a\n    performance measure of interest. It starts with a Monte Carlo simulation\n    step for computing the performance measure and a statistic at various sample\n    sizes selected from an initial sample size range. It continues with a\n    monotone curve-fitting step for interpolating the statistic across the entire\n    sample size range. The final step employs stratified bootstrapping to quantify\n    the uncertainty around the fitted curve.  "
  },
  {
    "id": 18286,
    "package_name": "practicalSigni",
    "title": "Practical Significance Ranking of Regressors and Exact t Density",
    "description": "Consider a possibly nonlinear nonparametric regression\n   with p regressors. We provide evaluations by 13 methods to rank\n   regressors by their practical significance or importance using \n   various methods, including machine learning tools. Comprehensive\n   methods are as follows. \n   m6=Generalized partial correlation coefficient or\n   GPCC by Vinod (2021)<doi:10.1007/s10614-021-10190-x> and\n   Vinod (2022)<https://www.mdpi.com/1911-8074/15/1/32>.\n   m7= a generalization of psychologists' effect size incorporating \n   nonlinearity and many variables.\n   m8= local linear partial (dy/dxi) using the 'np' package for kernel \n   regressions.\n   m9= partial (dy/dxi) using the 'NNS' package.\n   m10= importance measure using the 'NNS' boost function.\n   m11= Shapley Value measure of importance (cooperative game theory).\n   m12 and m13= two versions of the random forest algorithm.\n   Taraldsen's exact density for sampling distribution of correlations added.",
    "version": "0.1.2",
    "maintainer": "Hrishikesh Vinod <vinod@fordham.edu>",
    "author": "Hrishikesh Vinod [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=practicalSigni",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "practicalSigni Practical Significance Ranking of Regressors and Exact t Density Consider a possibly nonlinear nonparametric regression\n   with p regressors. We provide evaluations by 13 methods to rank\n   regressors by their practical significance or importance using \n   various methods, including machine learning tools. Comprehensive\n   methods are as follows. \n   m6=Generalized partial correlation coefficient or\n   GPCC by Vinod (2021)<doi:10.1007/s10614-021-10190-x> and\n   Vinod (2022)<https://www.mdpi.com/1911-8074/15/1/32>.\n   m7= a generalization of psychologists' effect size incorporating \n   nonlinearity and many variables.\n   m8= local linear partial (dy/dxi) using the 'np' package for kernel \n   regressions.\n   m9= partial (dy/dxi) using the 'NNS' package.\n   m10= importance measure using the 'NNS' boost function.\n   m11= Shapley Value measure of importance (cooperative game theory).\n   m12 and m13= two versions of the random forest algorithm.\n   Taraldsen's exact density for sampling distribution of correlations added.  "
  },
  {
    "id": 18381,
    "package_name": "prmisc",
    "title": "Miscellaneous Printing of Numeric and Statistical Output in R\nMarkdown and Quarto Documents",
    "description": "Miscellaneous printing of numeric or statistical results in R Markdown or Quarto documents according to guidelines of the \"Publication Manual\" of the American Psychological Association (2020, ISBN: 978-1-4338-3215-4). These guidelines are usually referred to as APA style (<https://apastyle.apa.org/>) and include specific rules on the formatting of numbers and statistical test results. APA style has to be implemented when submitting scientific reports in a wide range of research fields, especially in the social sciences. The default output of numbers in the R console or R Markdown and Quarto documents does not meet the APA style requirements, and reformatting results manually can be cumbersome and error-prone. This package covers the automatic conversion of R objects to textual representations that meet the APA style requirements, which can be included in R Markdown or Quarto documents. It covers some basic statistical tests (t-test, ANOVA, correlation, chi-squared test, Wilcoxon test) as well as some basic number printing manipulations (formatting p-values, removing leading zeros for numbers that cannot be greater than one, and others). Other packages exist for formatting numbers and tests according to the APA style guidelines, such as 'papaja' (<https://cran.r-project.org/package=papaja>) and 'apa' (<https://cran.r-project.org/package=apa>), but they do not offer all convenience functionality included in 'prmisc'. The vignette has an overview of most of the functions included in the package.",
    "version": "0.0.3",
    "maintainer": "Martin Papenberg <martin.papenberg@hhu.de>",
    "author": "Martin Papenberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9900-4268>),\n  Juliane V. Nagel [aut] (ORCID: <https://orcid.org/0000-0002-5310-8088>)",
    "url": "https://github.com/m-Py/prmisc",
    "bug_reports": "https://github.com/m-Py/prmisc/issues",
    "repository": "https://cran.r-project.org/package=prmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prmisc Miscellaneous Printing of Numeric and Statistical Output in R\nMarkdown and Quarto Documents Miscellaneous printing of numeric or statistical results in R Markdown or Quarto documents according to guidelines of the \"Publication Manual\" of the American Psychological Association (2020, ISBN: 978-1-4338-3215-4). These guidelines are usually referred to as APA style (<https://apastyle.apa.org/>) and include specific rules on the formatting of numbers and statistical test results. APA style has to be implemented when submitting scientific reports in a wide range of research fields, especially in the social sciences. The default output of numbers in the R console or R Markdown and Quarto documents does not meet the APA style requirements, and reformatting results manually can be cumbersome and error-prone. This package covers the automatic conversion of R objects to textual representations that meet the APA style requirements, which can be included in R Markdown or Quarto documents. It covers some basic statistical tests (t-test, ANOVA, correlation, chi-squared test, Wilcoxon test) as well as some basic number printing manipulations (formatting p-values, removing leading zeros for numbers that cannot be greater than one, and others). Other packages exist for formatting numbers and tests according to the APA style guidelines, such as 'papaja' (<https://cran.r-project.org/package=papaja>) and 'apa' (<https://cran.r-project.org/package=apa>), but they do not offer all convenience functionality included in 'prmisc'. The vignette has an overview of most of the functions included in the package.  "
  },
  {
    "id": 18416,
    "package_name": "projectLSA",
    "title": "Latent Structure Analysis Toolkit",
    "description": "Provides an interactive Shiny-based toolkit for conducting latent structure analyses, including Latent Profile Analysis (LPA), Latent Class Analysis (LCA), Latent Trait Analysis (LTA/IRT), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM). The implementation is grounded in established methodological frameworks: LPA is supported through 'tidyLPA' (Rosenberg et al., 2018) <doi:10.21105/joss.00978>, LCA through 'poLCA' (Linzer & Lewis, 2011), LTA/IRT via 'mirt' (Chalmers, 2012) <doi:10.18637/jss.v048.i06>, and EFA via 'psych' (Revelle, 2025). SEM and CFA functionalities build upon the 'lavaan' framework (Rosseel, 2012) <doi:10.18637/jss.v048.i02>. Users can upload datasets or use built-in examples, fit models, compare fit indices, visualize results, and export outputs without programming.",
    "version": "0.0.3",
    "maintainer": "Hasan Djidu <hasandjidu@gmail.com>",
    "author": "Hasan Djidu [aut, cre] (ORCID: <https://orcid.org/0000-0003-1110-6815>),\n  Heri Retnawati [ctb] (ORCID: <https://orcid.org/0000-0002-1792-5873>),\n  Samsul Hadi [ctb] (ORCID: <https://orcid.org/0000-0003-3437-2542>),\n  Haryanto [ctb] (ORCID: <https://orcid.org/0000-0003-3322-904X>)",
    "url": "https://github.com/hasandjidu/projectLSA",
    "bug_reports": "https://github.com/hasandjidu/projectLSA/issues",
    "repository": "https://cran.r-project.org/package=projectLSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "projectLSA Latent Structure Analysis Toolkit Provides an interactive Shiny-based toolkit for conducting latent structure analyses, including Latent Profile Analysis (LPA), Latent Class Analysis (LCA), Latent Trait Analysis (LTA/IRT), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM). The implementation is grounded in established methodological frameworks: LPA is supported through 'tidyLPA' (Rosenberg et al., 2018) <doi:10.21105/joss.00978>, LCA through 'poLCA' (Linzer & Lewis, 2011), LTA/IRT via 'mirt' (Chalmers, 2012) <doi:10.18637/jss.v048.i06>, and EFA via 'psych' (Revelle, 2025). SEM and CFA functionalities build upon the 'lavaan' framework (Rosseel, 2012) <doi:10.18637/jss.v048.i02>. Users can upload datasets or use built-in examples, fit models, compare fit indices, visualize results, and export outputs without programming.  "
  },
  {
    "id": 18472,
    "package_name": "psHarmonize",
    "title": "Creates a Harmonized Dataset Based on a Set of Instructions",
    "description": "Functions which facilitate harmonization of data from multiple\n    different datasets. Data harmonization involves taking data sources with\n    differing values, creating coding instructions to create a harmonized\n    set of values, then making those data modifications. 'psHarmonize' will\n    assist with data modification once the harmonization instructions are\n    written. Coding instructions are written by the user to create a\n    \"harmonization sheet\". This sheet catalogs variable names, domains\n    (e.g. clinical, behavioral, outcomes), provides R code instructions for\n    mapping or conversion of data, specifies the variable name in the\n    harmonized data set, and tracks notes. The package will then harmonize\n    the source datasets according to the harmonization sheet to create a\n    harmonized dataset. Once harmonization is finished, the package also has\n    functions that will create descriptive statistics using 'RMarkdown'. Data\n    Harmonization guidelines have been described by Fortier I, Raina P,\n    Van den Heuvel ER, et al. (2017) <doi:10.1093/ije/dyw075>. Additional\n    details of our R package have been described by Stephen JJ, Carolan P,\n    Krefman AE, et al. (2024) <doi:10.1016/j.patter.2024.101003>.",
    "version": "0.3.6",
    "maintainer": "John Stephen <John.Stephen@northwestern.edu>",
    "author": "John Stephen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7309-9193>),\n  Maxwell Mansolf [ctb] (ORCID: <https://orcid.org/0000-0001-6861-8657>)",
    "url": "https://github.com/NUDACC/psHarmonize",
    "bug_reports": "https://github.com/NUDACC/psHarmonize/issues",
    "repository": "https://cran.r-project.org/package=psHarmonize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psHarmonize Creates a Harmonized Dataset Based on a Set of Instructions Functions which facilitate harmonization of data from multiple\n    different datasets. Data harmonization involves taking data sources with\n    differing values, creating coding instructions to create a harmonized\n    set of values, then making those data modifications. 'psHarmonize' will\n    assist with data modification once the harmonization instructions are\n    written. Coding instructions are written by the user to create a\n    \"harmonization sheet\". This sheet catalogs variable names, domains\n    (e.g. clinical, behavioral, outcomes), provides R code instructions for\n    mapping or conversion of data, specifies the variable name in the\n    harmonized data set, and tracks notes. The package will then harmonize\n    the source datasets according to the harmonization sheet to create a\n    harmonized dataset. Once harmonization is finished, the package also has\n    functions that will create descriptive statistics using 'RMarkdown'. Data\n    Harmonization guidelines have been described by Fortier I, Raina P,\n    Van den Heuvel ER, et al. (2017) <doi:10.1093/ije/dyw075>. Additional\n    details of our R package have been described by Stephen JJ, Carolan P,\n    Krefman AE, et al. (2024) <doi:10.1016/j.patter.2024.101003>.  "
  },
  {
    "id": 18509,
    "package_name": "psy",
    "title": "Various Procedures Used in Psychometrics",
    "description": "Kappa, ICC, reliability coefficient, parallel analysis, \n     multi-traits multi-methods, spherical representation of a correlation matrix.",
    "version": "1.2",
    "maintainer": "Bruno Falissard <falissard_b@wanadoo.fr>",
    "author": "Bruno Falissard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psy Various Procedures Used in Psychometrics Kappa, ICC, reliability coefficient, parallel analysis, \n     multi-traits multi-methods, spherical representation of a correlation matrix.  "
  },
  {
    "id": 18510,
    "package_name": "psycCleaning",
    "title": "Data Cleaning for Psychological Analyses",
    "description": "Useful for preparing and cleaning data. It includes functions to center data, reverse coding, dummy code and effect code data, and more.",
    "version": "0.1.1",
    "maintainer": "Jason Moy <jason.moyhj@gmail.com>",
    "author": "Jason Moy [aut, cre] (ORCID: <https://orcid.org/0000-0001-8795-3311>)",
    "url": "https://jasonmoy28.github.io/psycCleaning/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psycCleaning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psycCleaning Data Cleaning for Psychological Analyses Useful for preparing and cleaning data. It includes functions to center data, reverse coding, dummy code and effect code data, and more.  "
  },
  {
    "id": 18511,
    "package_name": "psych",
    "title": "Procedures for Psychological, Psychometric, and Personality\nResearch",
    "description": "A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <https://personality-project.org/r/> web page.",
    "version": "2.5.6",
    "maintainer": "William Revelle <revelle@northwestern.edu>",
    "author": "William Revelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4880-9610>)",
    "url": "https://personality-project.org/r/psych/\nhttps://personality-project.org/r/psych-manual.pdf",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psych Procedures for Psychological, Psychometric, and Personality\nResearch A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <https://personality-project.org/r/> web page.  "
  },
  {
    "id": 18512,
    "package_name": "psychReport",
    "title": "Reproducible Reports in Psychology",
    "description": "Helper functions for producing reports in Psychology (Reproducible Research). Provides required formatted strings (APA style) for use in 'Knitr'/'Latex' integration within *.Rnw files.",
    "version": "3.0.2",
    "maintainer": "Ian G Mackenzie <ian.mackenzie@uni-tuebingen.de>",
    "author": "Ian G Mackenzie [cre, aut],\n  Carolin Dudschig [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychReport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychReport Reproducible Reports in Psychology Helper functions for producing reports in Psychology (Reproducible Research). Provides required formatted strings (APA style) for use in 'Knitr'/'Latex' integration within *.Rnw files.  "
  },
  {
    "id": 18513,
    "package_name": "psychTools",
    "title": "Tools to Accompany the 'psych' Package for Psychological\nResearch",
    "description": "Support functions,  data sets, and vignettes for the 'psych' package. Contains several of the biggest data sets for the 'psych' package as well as four vignettes. A few helper functions for file manipulation are included as well. For more information, see the <https://personality-project.org/r/> web page.",
    "version": "2.5.7.22",
    "maintainer": "William Revelle <revelle@northwestern.edu>",
    "author": "William Revelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4880-9610>)",
    "url": "https://personality-project.org/r/psych/\nhttps://personality-project.org/r/psych-manual.pdf",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychTools Tools to Accompany the 'psych' Package for Psychological\nResearch Support functions,  data sets, and vignettes for the 'psych' package. Contains several of the biggest data sets for the 'psych' package as well as four vignettes. A few helper functions for file manipulation are included as well. For more information, see the <https://personality-project.org/r/> web page.  "
  },
  {
    "id": 18514,
    "package_name": "psychmeta",
    "title": "Psychometric Meta-Analysis Toolkit",
    "description": "Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.",
    "version": "2.7.0",
    "maintainer": "Jeffrey A. Dahlke <jeff.dahlke.phd@gmail.com>",
    "author": "Jeffrey A. Dahlke [aut, cre],\n  Brenton M. Wiernik [aut],\n  Wesley Gardiner [ctb] (Unit tests),\n  Michael T. Brannick [ctb] (Testing),\n  Jack Kostal [ctb] (Code for reshape_mat2dat function),\n  Sean Potter [ctb] (Testing; Code for cumulative and leave1out plots),\n  John Sakaluk [ctb] (Code for funnel and forest plots),\n  Yuejia (Mandy) Teng [ctb] (Testing)",
    "url": "",
    "bug_reports": "https://github.com/psychmeta/psychmeta/issues",
    "repository": "https://cran.r-project.org/package=psychmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychmeta Psychometric Meta-Analysis Toolkit Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.  "
  },
  {
    "id": 18515,
    "package_name": "psycho",
    "title": "Efficient and Publishing-Oriented Workflow for Psychological\nScience",
    "description": "The main goal of the psycho package is to provide tools for psychologists, neuropsychologists and neuroscientists, \n   to facilitate and speed up the time spent on data analysis. It aims at supporting best practices and tools to format the output \n   of statistical methods to directly paste them into a manuscript, ensuring statistical reporting standardization and conformity.",
    "version": "0.6.1",
    "maintainer": "Dominique Makowski <dom.makowski@gmail.com>",
    "author": "Dominique Makowski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5375-9967>),\n  Hugo Najberg [ctb],\n  Viliam Simko [ctb],\n  Sasha Epskamp [rev] (Sasha reviewed the package for JOSS, see\n    https://github.com/openjournals/joss-reviews/issues/470)",
    "url": "https://github.com/neuropsychology/psycho.R",
    "bug_reports": "https://github.com/neuropsychology/psycho.R/issues",
    "repository": "https://cran.r-project.org/package=psycho",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psycho Efficient and Publishing-Oriented Workflow for Psychological\nScience The main goal of the psycho package is to provide tools for psychologists, neuropsychologists and neuroscientists, \n   to facilitate and speed up the time spent on data analysis. It aims at supporting best practices and tools to format the output \n   of statistical methods to directly paste them into a manuscript, ensuring statistical reporting standardization and conformity.  "
  },
  {
    "id": 18516,
    "package_name": "psychometric",
    "title": "Applied Psychometric Theory",
    "description": "Contains functions useful for correlation theory, \n    meta-analysis (validity-generalization), reliability, \n    item analysis, inter-rater reliability, and classical utility.",
    "version": "2.4",
    "maintainer": "Thomas D. Fletcher <t.d.fletcher05@gmail.com>",
    "author": "Thomas D. Fletcher",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychometric",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychometric Applied Psychometric Theory Contains functions useful for correlation theory, \n    meta-analysis (validity-generalization), reliability, \n    item analysis, inter-rater reliability, and classical utility.  "
  },
  {
    "id": 18517,
    "package_name": "psychomix",
    "title": "Psychometric Mixture Models",
    "description": "Psychometric mixture models based on 'flexmix' infrastructure. At the moment Rasch mixture models\n  with different parameterizations of the score distribution (saturated vs. mean/variance specification),\n  Bradley-Terry mixture models, and MPT mixture models are implemented. These mixture models can be estimated\n  with or without concomitant variables. See Frick et al. (2012) <doi:10.18637/jss.v048.i07> and\n  Frick et al. (2015) <doi:10.1177/0013164414536183> for details on the Rasch mixture models.",
    "version": "1.1-9",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Hannah Frick [aut] (ORCID: <https://orcid.org/0000-0002-6049-5258>),\n  Friedrich Leisch [aut] (ORCID: <https://orcid.org/0000-0001-7278-1983>),\n  Carolin Strobl [aut] (ORCID: <https://orcid.org/0000-0003-0952-3230>),\n  Florian Wickelmaier [aut],\n  Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychomix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychomix Psychometric Mixture Models Psychometric mixture models based on 'flexmix' infrastructure. At the moment Rasch mixture models\n  with different parameterizations of the score distribution (saturated vs. mean/variance specification),\n  Bradley-Terry mixture models, and MPT mixture models are implemented. These mixture models can be estimated\n  with or without concomitant variables. See Frick et al. (2012) <doi:10.18637/jss.v048.i07> and\n  Frick et al. (2015) <doi:10.1177/0013164414536183> for details on the Rasch mixture models.  "
  },
  {
    "id": 18519,
    "package_name": "psychotools",
    "title": "Psychometric Modeling Infrastructure",
    "description": "Infrastructure for psychometric modeling such as data classes (for\n  item response data and paired comparisons), basic model fitting functions (for\n  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,\n  rating scale, multinomial processing tree models), extractor functions for\n  different types of parameters (item, person, threshold, discrimination,\n  guessing, upper asymptotes), unified inference and visualizations, and various\n  datasets for illustration.  Intended as a common lightweight and efficient\n  toolbox for psychometric modeling and a common building block for fitting\n  psychometric mixture models in package \"psychomix\" and trees based on\n  psychometric models in package \"psychotree\".",
    "version": "0.7-5",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Carolin Strobl [aut] (ORCID: <https://orcid.org/0000-0003-0952-3230>),\n  Florian Wickelmaier [aut],\n  Basil Komboz [aut],\n  Julia Kopf [aut],\n  Lennart Schneider [aut] (ORCID:\n    <https://orcid.org/0000-0003-4152-5308>),\n  Rudolf Debelak [aut] (ORCID: <https://orcid.org/0000-0001-8900-2106>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychotools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychotools Psychometric Modeling Infrastructure Infrastructure for psychometric modeling such as data classes (for\n  item response data and paired comparisons), basic model fitting functions (for\n  Bradley-Terry, Rasch, parametric logistic IRT, generalized partial credit,\n  rating scale, multinomial processing tree models), extractor functions for\n  different types of parameters (item, person, threshold, discrimination,\n  guessing, upper asymptotes), unified inference and visualizations, and various\n  datasets for illustration.  Intended as a common lightweight and efficient\n  toolbox for psychometric modeling and a common building block for fitting\n  psychometric mixture models in package \"psychomix\" and trees based on\n  psychometric models in package \"psychotree\".  "
  },
  {
    "id": 18520,
    "package_name": "psychotree",
    "title": "Recursive Partitioning Based on Psychometric Models",
    "description": "Recursive partitioning based on psychometric models,\n  employing the general MOB algorithm (from package partykit) to obtain\n  Bradley-Terry trees, Rasch trees, rating scale and partial credit trees, and\n  MPT trees, trees for 1PL, 2PL, 3PL and 4PL models and generalized partial \n  credit models.",
    "version": "0.16-2",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Carolin Strobl [aut] (ORCID: <https://orcid.org/0000-0003-0952-3230>),\n  Florian Wickelmaier [aut],\n  Basil Komboz [aut],\n  Julia Kopf [aut],\n  Lennart Schneider [aut] (ORCID:\n    <https://orcid.org/0000-0003-4152-5308>),\n  David Dreifuss [aut],\n  Rudolf Debelak [aut] (ORCID: <https://orcid.org/0000-0001-8900-2106>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychotree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychotree Recursive Partitioning Based on Psychometric Models Recursive partitioning based on psychometric models,\n  employing the general MOB algorithm (from package partykit) to obtain\n  Bradley-Terry trees, Rasch trees, rating scale and partial credit trees, and\n  MPT trees, trees for 1PL, 2PL, 3PL and 4PL models and generalized partial \n  credit models.  "
  },
  {
    "id": 18521,
    "package_name": "psychrolib",
    "title": "Psychrometric Properties of Moist and Dry Air",
    "description": "\n    Implementation of 'PsychroLib'\n    <https://github.com/psychrometrics/psychrolib> library which contains\n    functions to enable the calculation properties of moist and dry air in both\n    metric (SI) and imperial (IP) systems of units. References: Meyer, D. and\n    Thevenard, D (2019) <doi:10.21105/joss.01137>.",
    "version": "2.5.2",
    "maintainer": "Hongyuan Jia <hongyuanjia@outlook.com>",
    "author": "Hongyuan Jia [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0075-8183>),\n  The PsychroLib Contributors [ctb, cph] (Authors listed in\n    inst/PSYCHROLIB_AUTHORS.txt),\n  ASHRAE [cph] (Copyright 2017 ASHRAE Handbook Fundamentals\n    (https://www.ashrae.org) for equations and coefficients published\n    ASHRAE Handbook Fundamentals Chapter 1.)",
    "url": "https://github.com/psychrometrics/psychrolib",
    "bug_reports": "https://github.com/psychrometrics/psychrolib/issues",
    "repository": "https://cran.r-project.org/package=psychrolib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychrolib Psychrometric Properties of Moist and Dry Air \n    Implementation of 'PsychroLib'\n    <https://github.com/psychrometrics/psychrolib> library which contains\n    functions to enable the calculation properties of moist and dry air in both\n    metric (SI) and imperial (IP) systems of units. References: Meyer, D. and\n    Thevenard, D (2019) <doi:10.21105/joss.01137>.  "
  },
  {
    "id": 18522,
    "package_name": "psymetadata",
    "title": "Open Datasets from Meta-Analyses in Psychology Research",
    "description": "Data and examples from meta-analyses in psychology research. ",
    "version": "1.0.1",
    "maintainer": "Josue E. Rodriguez <josue.rodriguez594@gmail.com>",
    "author": "Josue E. Rodriguez [aut, cre],\n  Donald Williams [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psymetadata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psymetadata Open Datasets from Meta-Analyses in Psychology Research Data and examples from meta-analyses in psychology research.   "
  },
  {
    "id": 18523,
    "package_name": "psyntur",
    "title": "Helper Tools for Teaching Statistical Data Analysis",
    "description": "Provides functions and data-sets that are helpful\n  for teaching statistics and data analysis. It was originally designed for use\n  when teaching students in the Psychology Department at Nottingham Trent University.",
    "version": "0.1.1",
    "maintainer": "Mark Andrews <mark.andrews@ntu.ac.uk>",
    "author": "Mark Andrews [aut, cre],\n  Jens Roeser [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psyntur",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psyntur Helper Tools for Teaching Statistical Data Analysis Provides functions and data-sets that are helpful\n  for teaching statistics and data analysis. It was originally designed for use\n  when teaching students in the Psychology Department at Nottingham Trent University.  "
  },
  {
    "id": 18524,
    "package_name": "psyphy",
    "title": "Functions for Analyzing Psychophysical Data in R",
    "description": "An assortment of functions that could be useful in analyzing data from psychophysical experiments. It includes functions for calculating d' from several different experimental designs, links for m-alternative forced-choice (mafc) data to be used with the binomial family in glm (and possibly other contexts) and self-Start functions for estimating gamma values for CRT screen calibrations.",
    "version": "0.3",
    "maintainer": "Ken Knoblauch <ken.knoblauch@inserm.fr>",
    "author": "Kenneth Knoblauch",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psyphy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psyphy Functions for Analyzing Psychophysical Data in R An assortment of functions that could be useful in analyzing data from psychophysical experiments. It includes functions for calculating d' from several different experimental designs, links for m-alternative forced-choice (mafc) data to be used with the binomial family in glm (and possibly other contexts) and self-Start functions for estimating gamma values for CRT screen calibrations.  "
  },
  {
    "id": 18525,
    "package_name": "psyverse",
    "title": "Decentralized Unequivocality in Psychological Science",
    "description": "The constructs used to study the human psychology have\n    many definitions and corresponding instructions for eliciting\n    and coding qualitative data pertaining to constructs' content and\n    for measuring the constructs. This plethora of definitions and\n    instructions necessitates unequivocal reference to specific\n    definitions and instructions in empirical and secondary research.\n    This package implements a human- and machine-readable standard for\n    specifying construct definitions and instructions for measurement\n    and qualitative research based on 'YAML'. This standard facilitates\n    systematic unequivocal reference to specific construct definitions\n    and corresponding instructions in a decentralized manner (i.e.\n    without requiring central curation; Peters (2020)\n    <doi:10.31234/osf.io/xebhn>).",
    "version": "0.2.6",
    "maintainer": "Gjalt-Jorn Peters <psyverse@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Rik Crutzen [ctb] (ORCID: <https://orcid.org/0000-0002-3731-6610>)",
    "url": "https://psyverse.one",
    "bug_reports": "https://gitlab.com/r-packages/psyverse/-/issues",
    "repository": "https://cran.r-project.org/package=psyverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psyverse Decentralized Unequivocality in Psychological Science The constructs used to study the human psychology have\n    many definitions and corresponding instructions for eliciting\n    and coding qualitative data pertaining to constructs' content and\n    for measuring the constructs. This plethora of definitions and\n    instructions necessitates unequivocal reference to specific\n    definitions and instructions in empirical and secondary research.\n    This package implements a human- and machine-readable standard for\n    specifying construct definitions and instructions for measurement\n    and qualitative research based on 'YAML'. This standard facilitates\n    systematic unequivocal reference to specific construct definitions\n    and corresponding instructions in a decentralized manner (i.e.\n    without requiring central curation; Peters (2020)\n    <doi:10.31234/osf.io/xebhn>).  "
  },
  {
    "id": 18583,
    "package_name": "pwranova",
    "title": "Power Analysis of Flexible ANOVA Designs and Related Tests",
    "description": "Provides functions for conducting power analysis in ANOVA designs, including between-, within-, and mixed-factor designs, with full support for both main effects and interactions. The package allows calculation of statistical power, required total sample size, significance level, and minimal detectable effect sizes expressed as partial eta squared or Cohen's f for ANOVA terms and planned contrasts. In addition, complementary functions are included for common related tests such as t-tests and correlation tests, making the package a convenient toolkit for power analysis in experimental psychology and related fields.",
    "version": "1.0.2",
    "maintainer": "Hiroyuki Muto <mutopsy@omu.ac.jp>",
    "author": "Hiroyuki Muto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0007-6019>)",
    "url": "https://github.com/mutopsy/pwranova,\nhttps://mutopsy.github.io/pwranova/",
    "bug_reports": "https://github.com/mutopsy/pwranova/issues",
    "repository": "https://cran.r-project.org/package=pwranova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pwranova Power Analysis of Flexible ANOVA Designs and Related Tests Provides functions for conducting power analysis in ANOVA designs, including between-, within-, and mixed-factor designs, with full support for both main effects and interactions. The package allows calculation of statistical power, required total sample size, significance level, and minimal detectable effect sizes expressed as partial eta squared or Cohen's f for ANOVA terms and planned contrasts. In addition, complementary functions are included for common related tests such as t-tests and correlation tests, making the package a convenient toolkit for power analysis in experimental psychology and related fields.  "
  },
  {
    "id": 18584,
    "package_name": "pwrss",
    "title": "Statistical Power and Sample Size Calculation Tools",
    "description": "\n  The 'pwrss' R package provides flexible and comprehensive functions for\n  statistical power and minimum required sample size calculations across a wide\n  range of commonly used hypothesis tests in psychological, biomedical, and\n  social sciences.",
    "version": "1.0.0",
    "maintainer": "Metin Bulus <bulusmetin@gmail.com>",
    "author": "Metin Bulus [aut, cre],\n  Sebastian Jentschke [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pwrss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pwrss Statistical Power and Sample Size Calculation Tools \n  The 'pwrss' R package provides flexible and comprehensive functions for\n  statistical power and minimum required sample size calculations across a wide\n  range of commonly used hypothesis tests in psychological, biomedical, and\n  social sciences.  "
  },
  {
    "id": 18639,
    "package_name": "qgraph",
    "title": "Graph Plotting Methods, Psychometric Data Visualization and\nGraphical Model Estimation",
    "description": "Fork of qgraph - Weighted network visualization and analysis, as well as Gaussian graphical model computation. See Epskamp et al. (2012) <doi:10.18637/jss.v048.i04>.",
    "version": "1.9.8",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "author": "Sacha Epskamp [aut, cre],\n  Giulio Costantini [aut],\n  Jonas Haslbeck [aut],\n  Adela Isvoranu [aut],\n  Angelique O. J. Cramer [ctb],\n  Lourens J. Waldorp [ctb],\n  Verena D. Schmittmann [ctb],\n  Denny Borsboom [ctb]",
    "url": "",
    "bug_reports": "https://github.com/SachaEpskamp/qgraph",
    "repository": "https://cran.r-project.org/package=qgraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qgraph Graph Plotting Methods, Psychometric Data Visualization and\nGraphical Model Estimation Fork of qgraph - Weighted network visualization and analysis, as well as Gaussian graphical model computation. See Epskamp et al. (2012) <doi:10.18637/jss.v048.i04>.  "
  },
  {
    "id": 18765,
    "package_name": "quickpsy",
    "title": "Fits Psychometric Functions for Multiple Groups",
    "description": "Quickly fits and plots psychometric functions (normal, logistic,\n    Weibull or any or any function defined by the user) for multiple groups.",
    "version": "0.1.5.1",
    "maintainer": "Linares Daniel <danilinares@gmail.com>",
    "author": "Linares Daniel [aut, cre],\n  L<U+00F3>pez-Moliner Joan [aut]",
    "url": "http://dlinares.org/quickpsy.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quickpsy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quickpsy Fits Psychometric Functions for Multiple Groups Quickly fits and plots psychometric functions (normal, logistic,\n    Weibull or any or any function defined by the user) for multiple groups.  "
  },
  {
    "id": 18811,
    "package_name": "r4lineups",
    "title": "Statistical Inference on Lineup Fairness",
    "description": "Since the early 1970s eyewitness testimony researchers have recognised the \n    importance of estimating properties such as lineup bias (is the lineup biased against \n    the suspect, leading to a rate of choosing higher than one would expect by chance?), \n    and lineup size (how many reasonable choices are in fact available to the witness? \n    A lineup is supposed to consist of a suspect and a number of additional members, \n    or foils, whom a poor-quality witness might mistake for the perpetrator). Lineup \n    measures are descriptive, in the first instance, but since the earliest articles in \n    the literature researchers have recognised the importance of reasoning inferentially \n    about them. This package contains functions to compute various properties of \n    laboratory or police lineups, and is intended for use by researchers in forensic \n    psychology and/or eyewitness testimony research. Among others, the r4lineups package \n    includes functions for calculating lineup proportion, functional size, various \n    estimates of effective size, diagnosticity ratio, homogeneity of the diagnosticity \n    ratio, ROC curves for confidence x accuracy data and the degree of similarity of \n    faces in a lineup. ",
    "version": "0.1.1",
    "maintainer": "Colin Tredoux <colin.tredoux@uct.ac.za>",
    "author": "Colin Tredoux [aut, cre],\n  Tamsyn Naylor [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=r4lineups",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r4lineups Statistical Inference on Lineup Fairness Since the early 1970s eyewitness testimony researchers have recognised the \n    importance of estimating properties such as lineup bias (is the lineup biased against \n    the suspect, leading to a rate of choosing higher than one would expect by chance?), \n    and lineup size (how many reasonable choices are in fact available to the witness? \n    A lineup is supposed to consist of a suspect and a number of additional members, \n    or foils, whom a poor-quality witness might mistake for the perpetrator). Lineup \n    measures are descriptive, in the first instance, but since the earliest articles in \n    the literature researchers have recognised the importance of reasoning inferentially \n    about them. This package contains functions to compute various properties of \n    laboratory or police lineups, and is intended for use by researchers in forensic \n    psychology and/or eyewitness testimony research. Among others, the r4lineups package \n    includes functions for calculating lineup proportion, functional size, various \n    estimates of effective size, diagnosticity ratio, homogeneity of the diagnosticity \n    ratio, ROC curves for confidence x accuracy data and the degree of similarity of \n    faces in a lineup.   "
  },
  {
    "id": 18867,
    "package_name": "rMEA",
    "title": "Synchrony in Motion Energy Analysis (MEA) Time-Series",
    "description": "A suite of tools useful to read, visualize and export bivariate motion energy time-series. Lagged synchrony between subjects can be analyzed through windowed cross-correlation. Surrogate data generation allows an estimation of pseudosynchrony that helps to estimate the effect size of the observed synchronization. Kleinbub, J. R., & Ramseyer, F. T. (2020). rMEA: An R package to assess nonverbal synchronization in motion energy analysis time-series. Psychotherapy research, 1-14. <doi:10.1080/10503307.2020.1844334>.",
    "version": "1.2.2",
    "maintainer": "Johann R. Kleinbub <johann.kleinbub@gmail.com>",
    "author": "Johann R. Kleinbub, Fabian Ramseyer",
    "url": "https://github.com/kleinbub/rMEA https://psync.ch",
    "bug_reports": "https://github.com/kleinbub/rMEA/issues",
    "repository": "https://cran.r-project.org/package=rMEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rMEA Synchrony in Motion Energy Analysis (MEA) Time-Series A suite of tools useful to read, visualize and export bivariate motion energy time-series. Lagged synchrony between subjects can be analyzed through windowed cross-correlation. Surrogate data generation allows an estimation of pseudosynchrony that helps to estimate the effect size of the observed synchronization. Kleinbub, J. R., & Ramseyer, F. T. (2020). rMEA: An R package to assess nonverbal synchronization in motion energy analysis time-series. Psychotherapy research, 1-14. <doi:10.1080/10503307.2020.1844334>.  "
  },
  {
    "id": 18943,
    "package_name": "ramchoice",
    "title": "Revealed Preference and Attention Analysis in Random Limited\nAttention Models",
    "description": "It is widely documented in psychology, economics and other disciplines that socio-economic agent may not pay full attention to all available alternatives, rendering standard revealed preference theory invalid. This package implements the estimation and inference procedures of Cattaneo, Ma, Masatlioglu and Suleymanov (2020) <arXiv:1712.03448> and Cattaneo, Cheung, Ma, and Masatlioglu (2022) <arXiv:2110.10650>, which utilizes standard choice data to partially identify and estimate a decision maker's preference and attention. For inference, several simulation-based critical values are provided. ",
    "version": "2.2",
    "maintainer": "Xinwei Ma <x1ma@ucsd.edu>",
    "author": "Matias D. Cattaneo, Paul Cheung, Xinwei Ma, Yusufcan Masatlioglu, Elchin Suleymanov",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ramchoice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ramchoice Revealed Preference and Attention Analysis in Random Limited\nAttention Models It is widely documented in psychology, economics and other disciplines that socio-economic agent may not pay full attention to all available alternatives, rendering standard revealed preference theory invalid. This package implements the estimation and inference procedures of Cattaneo, Ma, Masatlioglu and Suleymanov (2020) <arXiv:1712.03448> and Cattaneo, Cheung, Ma, and Masatlioglu (2022) <arXiv:2110.10650>, which utilizes standard choice data to partially identify and estimate a decision maker's preference and attention. For inference, several simulation-based critical values are provided.   "
  },
  {
    "id": 18998,
    "package_name": "rapidsplithalf",
    "title": "A Fast Permutation-Based Split-Half Reliability Algorithm",
    "description": "Accurately estimates the reliability of cognitive tasks using a fast and flexible permutation-based split-half reliability algorithm that supports stratified splitting while maintaining equal split sizes. See Kahveci, Bathke, and Blechert (2025) <doi:10.3758/s13423-024-02597-y> for details.",
    "version": "0.6",
    "maintainer": "Sercan Kahveci <sercan.kahveci@plus.ac.at>",
    "author": "Sercan Kahveci [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4139-5710>)",
    "url": "",
    "bug_reports": "https://github.com/Spiritspeak/rapidsplit/issues/",
    "repository": "https://cran.r-project.org/package=rapidsplithalf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rapidsplithalf A Fast Permutation-Based Split-Half Reliability Algorithm Accurately estimates the reliability of cognitive tasks using a fast and flexible permutation-based split-half reliability algorithm that supports stratified splitting while maintaining equal split sizes. See Kahveci, Bathke, and Blechert (2025) <doi:10.3758/s13423-024-02597-y> for details.  "
  },
  {
    "id": 19032,
    "package_name": "raters",
    "title": "A Modification of Fleiss' Kappa in Case of Nominal and Ordinal\nVariables",
    "description": "The kappa statistic implemented by Fleiss is a very popular index for assessing the reliability of agreement among multiple observers. It is used both in the psychological and in the psychiatric field. Other fields of application are typically medicine, biology and engineering. Unfortunately,the kappa statistic may behave inconsistently in case of strong agreement between raters, since this index assumes lower values than it would have been expected. We propose a modification kappa implemented by Fleiss in case of nominal and ordinal variables. Monte Carlo simulations are used both to testing statistical hypotheses and to calculating percentile bootstrap confidence intervals based on proposed statistic in case of nominal and ordinal data.",
    "version": "2.1.1",
    "maintainer": "Daniele Giardiello <daniele.giardiello1@gmail.com>",
    "author": "Daniele Giardiello [cre],\n  Piero Quatto [aut],\n  Enrico Ripamonti [aut],\n  Stefano Vigliani [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=raters",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raters A Modification of Fleiss' Kappa in Case of Nominal and Ordinal\nVariables The kappa statistic implemented by Fleiss is a very popular index for assessing the reliability of agreement among multiple observers. It is used both in the psychological and in the psychiatric field. Other fields of application are typically medicine, biology and engineering. Unfortunately,the kappa statistic may behave inconsistently in case of strong agreement between raters, since this index assumes lower values than it would have been expected. We propose a modification kappa implemented by Fleiss in case of nominal and ordinal variables. Monte Carlo simulations are used both to testing statistical hypotheses and to calculating percentile bootstrap confidence intervals based on proposed statistic in case of nominal and ordinal data.  "
  },
  {
    "id": 19069,
    "package_name": "rblt",
    "title": "Bio-Logging Toolbox",
    "description": "An R-shiny application to visualize bio-loggers time series at a microsecond precision as Acceleration, Temperature, Pressure, Light intensity. It is possible to link behavioral labels extracted\n  from 'BORIS' software <http://www.boris.unito.it> or manually written in a csv file.",
    "version": "0.2.4.7",
    "maintainer": "Sebastien Geiger <sebastien.geiger@iphc.cnrs.fr>",
    "author": "Sebastien Geiger [aut, cre]",
    "url": "https://github.com/sg4r/rblt",
    "bug_reports": "https://github.com/sg4r/rblt/issues",
    "repository": "https://cran.r-project.org/package=rblt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rblt Bio-Logging Toolbox An R-shiny application to visualize bio-loggers time series at a microsecond precision as Acceleration, Temperature, Pressure, Light intensity. It is possible to link behavioral labels extracted\n  from 'BORIS' software <http://www.boris.unito.it> or manually written in a csv file.  "
  },
  {
    "id": 19153,
    "package_name": "rdrobust",
    "title": "Robust Data-Driven Statistical Inference in\nRegression-Discontinuity Designs",
    "description": "Regression-discontinuity (RD) designs are quasi-experimental research designs popular in social, behavioral and natural sciences. The RD design is usually employed to study the (local) causal effect of a treatment, intervention or policy. This package provides tools for data-driven graphical and analytical statistical inference in RD\tdesigns: rdrobust() to construct local-polynomial point estimators and robust confidence intervals for average treatment effects at the cutoff in Sharp, Fuzzy and Kink RD settings, rdbwselect() to perform bandwidth selection for the different procedures implemented, and rdplot() to conduct exploratory data analysis (RD plots).",
    "version": "3.0.0",
    "maintainer": "Sebastian Calonico <scalonico@ucdavis.edu>",
    "author": "Sebastian Calonico [aut, cre],\n  Matias D. Cattaneo [aut],\n  Max H. Farrell [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rdrobust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdrobust Robust Data-Driven Statistical Inference in\nRegression-Discontinuity Designs Regression-discontinuity (RD) designs are quasi-experimental research designs popular in social, behavioral and natural sciences. The RD design is usually employed to study the (local) causal effect of a treatment, intervention or policy. This package provides tools for data-driven graphical and analytical statistical inference in RD\tdesigns: rdrobust() to construct local-polynomial point estimators and robust confidence intervals for average treatment effects at the cutoff in Sharp, Fuzzy and Kink RD settings, rdbwselect() to perform bandwidth selection for the different procedures implemented, and rdplot() to conduct exploratory data analysis (RD plots).  "
  },
  {
    "id": 19223,
    "package_name": "recmetrics",
    "title": "Psychometric Evaluation Using Relative Excess Correlations",
    "description": "Modern results of psychometric theory are implemented to provide users with a way of evaluating the internal structure of a set of items guided by theory. These methods are discussed in detail in VanderWeele and Padgett (2024) <doi:10.31234/osf.io/rnbk5>. The relative excess correlation matrices will, generally, have numerous negative entries even if all of the raw correlations between each pair of indicators are positive. The positive deviations of the relative excess correlation matrix entries help identify clusters of indicators that are more strongly related to one another, providing insights somewhat analogous to factor analysis, but without the need for rotations or decisions concerning the number of factors. A goal similar to exploratory/confirmatory factor analysis, but 'recmetrics' uses novel methods that do not rely on assumptions of latent variables or latent variable structures. ",
    "version": "0.1.0",
    "maintainer": "R. Noah Padgett <npadgett@hsph.harvard.edu>",
    "author": "R. Noah Padgett [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9114-3896>)",
    "url": "https://noah-padgett.github.io/recmetrics/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=recmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recmetrics Psychometric Evaluation Using Relative Excess Correlations Modern results of psychometric theory are implemented to provide users with a way of evaluating the internal structure of a set of items guided by theory. These methods are discussed in detail in VanderWeele and Padgett (2024) <doi:10.31234/osf.io/rnbk5>. The relative excess correlation matrices will, generally, have numerous negative entries even if all of the raw correlations between each pair of indicators are positive. The positive deviations of the relative excess correlation matrix entries help identify clusters of indicators that are more strongly related to one another, providing insights somewhat analogous to factor analysis, but without the need for rotations or decisions concerning the number of factors. A goal similar to exploratory/confirmatory factor analysis, but 'recmetrics' uses novel methods that do not rely on assumptions of latent variables or latent variable structures.   "
  },
  {
    "id": 19262,
    "package_name": "reflectR",
    "title": "Automatic Scoring of the Cognitive Reflection Test",
    "description": "Automatic coding of open-ended responses to the Cognitive Reflection Test (CRT), a widely used class of tests in cognitive science and psychology that assess the tendency to override an initial intuitive (but incorrect) answer and engage in reflection to reach a correct solution. The package standardizes CRT response coding across datasets in cognitive psychology, decision-making, and related fields. Automated coding reduces manual effort and improves reproducibility by limiting variability from subjective interpretation of open-ended responses. The package supports automatic coding and machine scoring for the original English-language CRT (Frederick, 2005) <doi:10.1257/089533005775196732>, CRT4 and CRT7 (Toplak et al., 2014) <doi:10.1080/13546783.2013.844729>, CRT-long (Primi et al., 2016) <doi:10.1002/bdm.1883>, and CRT-2 (Thomson & Oppenheimer, 2016) <doi:10.1017/s1930297500007622>.  ",
    "version": "2.1.4",
    "maintainer": "Giuseppe Corbelli <giuseppe.corbelli@uninettunouniversity.net>",
    "author": "Giuseppe Corbelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2864-3548>)",
    "url": "https://github.com/g-corbelli/reflectR",
    "bug_reports": "https://github.com/g-corbelli/reflectR/issues",
    "repository": "https://cran.r-project.org/package=reflectR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reflectR Automatic Scoring of the Cognitive Reflection Test Automatic coding of open-ended responses to the Cognitive Reflection Test (CRT), a widely used class of tests in cognitive science and psychology that assess the tendency to override an initial intuitive (but incorrect) answer and engage in reflection to reach a correct solution. The package standardizes CRT response coding across datasets in cognitive psychology, decision-making, and related fields. Automated coding reduces manual effort and improves reproducibility by limiting variability from subjective interpretation of open-ended responses. The package supports automatic coding and machine scoring for the original English-language CRT (Frederick, 2005) <doi:10.1257/089533005775196732>, CRT4 and CRT7 (Toplak et al., 2014) <doi:10.1080/13546783.2013.844729>, CRT-long (Primi et al., 2016) <doi:10.1002/bdm.1883>, and CRT-2 (Thomson & Oppenheimer, 2016) <doi:10.1017/s1930297500007622>.    "
  },
  {
    "id": 19335,
    "package_name": "rempsyc",
    "title": "Convenience Functions for Psychology",
    "description": "Make your workflow faster and easier. Easily customizable\n    plots (via 'ggplot2'), nice APA tables (following the style of the\n    *American Psychological Association*) exportable to Word (via\n    'flextable'), easily run statistical tests or check assumptions, and\n    automatize various other tasks.",
    "version": "0.2.0",
    "maintainer": "R\u00e9mi Th\u00e9riault <remi.theriault@mail.mcgill.ca>",
    "author": "R\u00e9mi Th\u00e9riault [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4315-6788>)",
    "url": "https://rempsyc.remi-theriault.com",
    "bug_reports": "https://github.com/rempsyc/rempsyc/issues",
    "repository": "https://cran.r-project.org/package=rempsyc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rempsyc Convenience Functions for Psychology Make your workflow faster and easier. Easily customizable\n    plots (via 'ggplot2'), nice APA tables (following the style of the\n    *American Psychological Association*) exportable to Word (via\n    'flextable'), easily run statistical tests or check assumptions, and\n    automatize various other tasks.  "
  },
  {
    "id": 19796,
    "package_name": "rprime",
    "title": "Functions for Working with 'Eprime' Text Files",
    "description": "'Eprime' is a set of programs for administering\n    psychological experiments by computer. This package provides functions\n    for loading, parsing, filtering and exporting data in the text files\n    produced by 'Eprime' experiments.",
    "version": "0.1.3",
    "maintainer": "Tristan Mahr <tristan.mahr@wisc.edu>",
    "author": "Tristan Mahr [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8890-5116>)",
    "url": "https://github.com/tjmahr/rprime",
    "bug_reports": "https://github.com/tjmahr/rprime/issues",
    "repository": "https://cran.r-project.org/package=rprime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rprime Functions for Working with 'Eprime' Text Files 'Eprime' is a set of programs for administering\n    psychological experiments by computer. This package provides functions\n    for loading, parsing, filtering and exporting data in the text files\n    produced by 'Eprime' experiments.  "
  },
  {
    "id": 19899,
    "package_name": "rtdists",
    "title": "Response Time Distributions",
    "description": "Provides response time distributions (density/PDF,\n       distribution function/CDF, quantile function, and random\n       generation): (a) Ratcliff diffusion model (Ratcliff &\n       McKoon, 2008, <doi:10.1162/neco.2008.12-06-420>) based on C\n       code by Andreas and Jochen Voss and (b) linear ballistic\n       accumulator (LBA; Brown & Heathcote, 2008,\n       <doi:10.1016/j.cogpsych.2007.12.002>) with different\n       distributions underlying the drift rate.",
    "version": "0.11-5",
    "maintainer": "Henrik Singmann <singmann@gmail.com>",
    "author": "Henrik Singmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4842-3657>),\n  Scott Brown [aut],\n  Matthew Gretton [aut],\n  Andrew Heathcote [aut],\n  Andreas Voss [ctb],\n  Jochen Voss [ctb],\n  Andrew Terry [ctb]",
    "url": "https://github.com/rtdists/rtdists/",
    "bug_reports": "https://github.com/rtdists/rtdists/issues",
    "repository": "https://cran.r-project.org/package=rtdists",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtdists Response Time Distributions Provides response time distributions (density/PDF,\n       distribution function/CDF, quantile function, and random\n       generation): (a) Ratcliff diffusion model (Ratcliff &\n       McKoon, 2008, <doi:10.1162/neco.2008.12-06-420>) based on C\n       code by Andreas and Jochen Voss and (b) linear ballistic\n       accumulator (LBA; Brown & Heathcote, 2008,\n       <doi:10.1016/j.cogpsych.2007.12.002>) with different\n       distributions underlying the drift rate.  "
  },
  {
    "id": 20212,
    "package_name": "schoRsch",
    "title": "Tools for Analyzing Factorial Experiments",
    "description": "Offers a helping hand to psychologists and other behavioral scientists who routinely deal with experimental data from factorial experiments. It includes several functions to format output from other R functions according to the style guidelines of the APA (American Psychological Association). This formatted output can be copied directly into manuscripts to facilitate data reporting. These features are backed up by a toolkit of several small helper functions, e.g., offering out-of-the-box outlier removal. The package lends its name to Georg \"Schorsch\" Schuessler, ingenious technician at the Department of Psychology III, University of Wuerzburg. For details on the implemented methods, see Roland Pfister and Markus Janczyk (2016) <doi: 10.20982/tqmp.12.2.p147>.",
    "version": "1.11",
    "maintainer": "Roland Pfister <mail@roland-pfister.net>",
    "author": "Roland Pfister [aut, cre],\n  Markus Janczyk [aut]",
    "url": "https://www.tqmp.org/RegularArticles/vol12-2/p147/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=schoRsch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "schoRsch Tools for Analyzing Factorial Experiments Offers a helping hand to psychologists and other behavioral scientists who routinely deal with experimental data from factorial experiments. It includes several functions to format output from other R functions according to the style guidelines of the APA (American Psychological Association). This formatted output can be copied directly into manuscripts to facilitate data reporting. These features are backed up by a toolkit of several small helper functions, e.g., offering out-of-the-box outlier removal. The package lends its name to Georg \"Schorsch\" Schuessler, ingenious technician at the Department of Psychology III, University of Wuerzburg. For details on the implemented methods, see Roland Pfister and Markus Janczyk (2016) <doi: 10.20982/tqmp.12.2.p147>.  "
  },
  {
    "id": 20232,
    "package_name": "score",
    "title": "A Package to Score Behavioral Questionnaires",
    "description": "Provides routines for scoring behavioral questionnaires. Includes scoring procedures for the 'International Physical Activity Questionnaire (IPAQ)' <http://www.ipaq.ki.se>. Compares physical functional performance to the age- and gender-specific normal ranges. ",
    "version": "1.0.2",
    "maintainer": "Jaejoon Song <jjsong2@mdanderson.org>",
    "author": "Jaejoon Song ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=score",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "score A Package to Score Behavioral Questionnaires Provides routines for scoring behavioral questionnaires. Includes scoring procedures for the 'International Physical Activity Questionnaire (IPAQ)' <http://www.ipaq.ki.se>. Compares physical functional performance to the age- and gender-specific normal ranges.   "
  },
  {
    "id": 20362,
    "package_name": "semEffect",
    "title": "Structural Equation Model Effect Analysis and Visualization",
    "description": "Provides standardized effect decomposition (direct, indirect, and total effects) for three major structural equation modeling frameworks: \n\t'lavaan', 'piecewiseSEM', and 'plspm'. Automatically handles zero-effect variables, generates publication-ready 'ggplot2' visualizations, and returns \n\tboth wide-format and long-format effect tables. Supports effect filtering, multi-model object inputs, and customizable visualization parameters.\n\tFor a general overview of the methods used in this package, see Rosseel (2012) <doi:10.18637/jss.v048.i02> and Lefcheck (2016) <doi:10.1111/2041-210X.12512>.",
    "version": "1.2.3",
    "maintainer": "Weiping Mei <meiweipingg@163.com>",
    "author": "Weiping Mei [aut, cre] (ORCID: <https://orcid.org/0000-0001-6400-9862>)",
    "url": "https://github.com/PhDMeiwp/semEffect/",
    "bug_reports": "https://github.com/PhDMeiwp/semEffect/issues",
    "repository": "https://cran.r-project.org/package=semEffect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semEffect Structural Equation Model Effect Analysis and Visualization Provides standardized effect decomposition (direct, indirect, and total effects) for three major structural equation modeling frameworks: \n\t'lavaan', 'piecewiseSEM', and 'plspm'. Automatically handles zero-effect variables, generates publication-ready 'ggplot2' visualizations, and returns \n\tboth wide-format and long-format effect tables. Supports effect filtering, multi-model object inputs, and customizable visualization parameters.\n\tFor a general overview of the methods used in this package, see Rosseel (2012) <doi:10.18637/jss.v048.i02> and Lefcheck (2016) <doi:10.1111/2041-210X.12512>.  "
  },
  {
    "id": 20363,
    "package_name": "semPlot",
    "title": "Path Diagrams and Visual Analysis of Various SEM Packages'\nOutput",
    "description": "Path diagrams and visual analysis of various SEM packages' output.",
    "version": "1.1.7",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "author": "Sacha Epskamp [aut, cre],\n  Simon Stuber [ctb],\n  Jason Nak [ctb],\n  Myrthe Veenman [ctb],\n  Terrence D. Jorgensen [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5111-6773>)",
    "url": "https://github.com/SachaEpskamp/semPlot",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=semPlot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semPlot Path Diagrams and Visual Analysis of Various SEM Packages'\nOutput Path diagrams and visual analysis of various SEM packages' output.  "
  },
  {
    "id": 20364,
    "package_name": "semPower",
    "title": "Power Analyses for SEM",
    "description": "Provides a-priori, post-hoc, and compromise power-analyses\n    for structural equation models (SEM).",
    "version": "2.1.3",
    "maintainer": "Morten Moshagen <morten.moshagen@uni-ulm.de>",
    "author": "Morten Moshagen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2929-7288>),\n  Martina Bader [aut] (ORCID: <https://orcid.org/0000-0002-5706-8933>)",
    "url": "https://github.com/moshagen/semPower",
    "bug_reports": "https://github.com/moshagen/semPower/issues",
    "repository": "https://cran.r-project.org/package=semPower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semPower Power Analyses for SEM Provides a-priori, post-hoc, and compromise power-analyses\n    for structural equation models (SEM).  "
  },
  {
    "id": 20366,
    "package_name": "semTools",
    "title": "Useful Tools for Structural Equation Modeling",
    "description": "Provides miscellaneous tools for structural equation modeling, \n             many of which extend the 'lavaan' package. For example, latent\n             interactions can be estimated using product indicators (Lin et al.,\n             2010, <doi:10.1080/10705511.2010.488999>) and simple effects probed;\n             analytical power analyses can be conducted (Jak et al., 2021,\n             <doi:10.3758/s13428-020-01479-0>); and scale reliability\n             can be estimated based on estimated factor-model parameters.",
    "version": "0.5-7",
    "maintainer": "Terrence D. Jorgensen <TJorgensen314@gmail.com>",
    "author": "Terrence D. Jorgensen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5111-6773>),\n  Sunthud Pornprasertmanit [aut],\n  Alexander M. Schoemann [aut] (ORCID:\n    <https://orcid.org/0000-0002-8479-8798>),\n  Yves Rosseel [aut] (ORCID: <https://orcid.org/0000-0002-4129-4477>),\n  Patrick Miller [ctb],\n  Corbin Quick [ctb],\n  Mauricio Garnier-Villarreal [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2951-6647>),\n  James Selig [ctb],\n  Aaron Boulton [ctb],\n  Kristopher Preacher [ctb],\n  Donna Coffman [ctb],\n  Mijke Rhemtulla [ctb] (ORCID: <https://orcid.org/0000-0003-2572-2424>),\n  Alexander Robitzsch [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8226-3132>),\n  Craig Enders [ctb],\n  Ruben Arslan [ctb] (ORCID: <https://orcid.org/0000-0002-6670-5658>),\n  Bell Clinton [ctb],\n  Pavel Panko [ctb],\n  Edgar Merkle [ctb] (ORCID: <https://orcid.org/0000-0001-7158-0653>),\n  Steven Chesnut [ctb],\n  Jarrett Byrnes [ctb],\n  Jason D. Rights [ctb],\n  Ylenio Longo [ctb],\n  Maxwell Mansolf [ctb] (ORCID: <https://orcid.org/0000-0001-6861-8657>),\n  Mattan S. Ben-Shachar [ctb] (ORCID:\n    <https://orcid.org/0000-0002-4287-4801>),\n  Mikko R\u00f6nkk\u00f6 [ctb] (ORCID: <https://orcid.org/0000-0001-7988-7609>),\n  Andrew R. Johnson [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7000-8065>),\n  Leonard Vanbrabant [ctb]",
    "url": "https://github.com/simsem/semTools/wiki",
    "bug_reports": "https://github.com/simsem/semTools/issues",
    "repository": "https://cran.r-project.org/package=semTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semTools Useful Tools for Structural Equation Modeling Provides miscellaneous tools for structural equation modeling, \n             many of which extend the 'lavaan' package. For example, latent\n             interactions can be estimated using product indicators (Lin et al.,\n             2010, <doi:10.1080/10705511.2010.488999>) and simple effects probed;\n             analytical power analyses can be conducted (Jak et al., 2021,\n             <doi:10.3758/s13428-020-01479-0>); and scale reliability\n             can be estimated based on estimated factor-model parameters.  "
  },
  {
    "id": 20370,
    "package_name": "semboottools",
    "title": "Bootstrapping Helpers for Structural Equation Modelling",
    "description": "A collection of helper functions for forming\n  bootstrapping confidence intervals and examining bootstrap\n  estimates in structural equation modelling. Currently\n  supports models fitted by the 'lavaan' package by\n  Rosseel (2012) <doi: 10.18637/jss.v048.i02>.",
    "version": "0.1.1",
    "maintainer": "Wendie Yang <1581075494q@gmail.com>",
    "author": "Wendie Yang [aut, cre] (ORCID: <https://orcid.org/0009-0000-8388-6481>),\n  Shu Fai Cheung [aut] (ORCID: <https://orcid.org/0000-0002-9871-9448>)",
    "url": "https://Yangzhen1999.github.io/semboottools/",
    "bug_reports": "https://github.com/Yangzhen1999/semboottools/issues",
    "repository": "https://cran.r-project.org/package=semboottools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semboottools Bootstrapping Helpers for Structural Equation Modelling A collection of helper functions for forming\n  bootstrapping confidence intervals and examining bootstrap\n  estimates in structural equation modelling. Currently\n  supports models fitted by the 'lavaan' package by\n  Rosseel (2012) <doi: 10.18637/jss.v048.i02>.  "
  },
  {
    "id": 20371,
    "package_name": "semdrw",
    "title": "'SEM Shiny'",
    "description": "Interactive 'shiny' application for working with Structural Equation Modelling technique. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/semwebappk/> .",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=semdrw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semdrw 'SEM Shiny' Interactive 'shiny' application for working with Structural Equation Modelling technique. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/semwebappk/> .  "
  },
  {
    "id": 20372,
    "package_name": "semds",
    "title": "Structural Equation Multidimensional Scaling",
    "description": "Fits a structural equation multidimensional scaling (SEMDS) model for asymmetric and three-way input dissimilarities. It assumes that the dissimilarities are measured with errors. The latent dissimilarities are estimated as factor scores within an SEM framework while the objects are represented in a low-dimensional space as in MDS. ",
    "version": "0.9-7",
    "maintainer": "Patrick Mair <mair@fas.harvard.edu>",
    "author": "Patrick Mair [aut, cre],\n  Jose Fernando Vera [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=semds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semds Structural Equation Multidimensional Scaling Fits a structural equation multidimensional scaling (SEMDS) model for asymmetric and three-way input dissimilarities. It assumes that the dissimilarities are measured with errors. The latent dissimilarities are estimated as factor scores within an SEM framework while the objects are represented in a low-dimensional space as in MDS.   "
  },
  {
    "id": 20375,
    "package_name": "semhelpinghands",
    "title": "Helper Functions for Structural Equation Modeling",
    "description": "An assortment of helper functions for doing structural equation\n  modeling, mainly by 'lavaan' for now. Most of them are time-saving functions\n  for common tasks in doing structural equation modeling and reading the\n  output. This package is not for functions that implement advanced statistical\n  procedures. It is a light-weight package for simple functions that do simple\n  tasks conveniently, with as few dependencies as possible.",
    "version": "0.1.12",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "author": "Shu Fai Cheung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9871-9448>)",
    "url": "https://sfcheung.github.io/semhelpinghands/",
    "bug_reports": "https://github.com/sfcheung/semhelpinghands/issues",
    "repository": "https://cran.r-project.org/package=semhelpinghands",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semhelpinghands Helper Functions for Structural Equation Modeling An assortment of helper functions for doing structural equation\n  modeling, mainly by 'lavaan' for now. Most of them are time-saving functions\n  for common tasks in doing structural equation modeling and reading the\n  output. This package is not for functions that implement advanced statistical\n  procedures. It is a light-weight package for simple functions that do simple\n  tasks conveniently, with as few dependencies as possible.  "
  },
  {
    "id": 20383,
    "package_name": "semlrtp",
    "title": "Likelihood Ratio Test P-Values for Structural Equation Models",
    "description": "Computes likelihood ratio test (LRT) p-values\n  for free parameters in a structural equation model.\n  Currently supports models fitted by the 'lavaan' package by\n  Rosseel (2012) <doi:10.18637/jss.v048.i02>.",
    "version": "0.1.1",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "author": "Shu Fai Cheung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9871-9448>),\n  Mark Hok Chio Lai [aut] (ORCID:\n    <https://orcid.org/0000-0002-9196-7406>)",
    "url": "https://sfcheung.github.io/semlrtp/",
    "bug_reports": "https://github.com/sfcheung/semlrtp/issues",
    "repository": "https://cran.r-project.org/package=semlrtp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semlrtp Likelihood Ratio Test P-Values for Structural Equation Models Computes likelihood ratio test (LRT) p-values\n  for free parameters in a structural equation model.\n  Currently supports models fitted by the 'lavaan' package by\n  Rosseel (2012) <doi:10.18637/jss.v048.i02>.  "
  },
  {
    "id": 20384,
    "package_name": "semmcci",
    "title": "Monte Carlo Confidence Intervals in Structural Equation Modeling",
    "description": "Monte Carlo confidence intervals for free and defined parameters\n    in models fitted in the structural equation modeling package 'lavaan'\n    can be generated using the 'semmcci' package.\n    'semmcci' has three main functions, namely, MC(), MCMI(), and MCStd().\n    The output of 'lavaan' is passed as the first argument\n    to the MC() function or the MCMI() function to generate Monte Carlo confidence intervals.\n    Monte Carlo confidence intervals for the standardized estimates\n    can also be generated by passing the output of the MC() function or the MCMI() function\n    to the MCStd() function.\n    A description of the package and code examples\n    are presented in Pesigan and Cheung (2024) <doi:10.3758/s13428-023-02114-4>.",
    "version": "1.1.5",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "author": "Ivan Jacob Agaloos Pesigan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4818-8420>),\n  Shu Fai Cheung [ctb] (ORCID: <https://orcid.org/0000-0002-9871-9448>)",
    "url": "https://github.com/jeksterslab/semmcci,\nhttps://jeksterslab.github.io/semmcci/",
    "bug_reports": "https://github.com/jeksterslab/semmcci/issues",
    "repository": "https://cran.r-project.org/package=semmcci",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semmcci Monte Carlo Confidence Intervals in Structural Equation Modeling Monte Carlo confidence intervals for free and defined parameters\n    in models fitted in the structural equation modeling package 'lavaan'\n    can be generated using the 'semmcci' package.\n    'semmcci' has three main functions, namely, MC(), MCMI(), and MCStd().\n    The output of 'lavaan' is passed as the first argument\n    to the MC() function or the MCMI() function to generate Monte Carlo confidence intervals.\n    Monte Carlo confidence intervals for the standardized estimates\n    can also be generated by passing the output of the MC() function or the MCMI() function\n    to the MCStd() function.\n    A description of the package and code examples\n    are presented in Pesigan and Cheung (2024) <doi:10.3758/s13428-023-02114-4>.  "
  },
  {
    "id": 20390,
    "package_name": "semtree",
    "title": "Recursive Partitioning for Structural Equation Models",
    "description": "SEM Trees and SEM Forests -- an extension of model-based decision\n    trees and forests to Structural Equation Models (SEM). SEM trees hierarchically\n    split empirical data into homogeneous groups each sharing similar data patterns\n    with respect to a SEM by recursively selecting optimal predictors of these\n    differences. SEM forests are an extension of SEM trees. They are ensembles of\n    SEM trees each built on a random sample of the original data. By aggregating\n    over a forest, we obtain measures of variable importance that are more robust\n    than measures from single trees. A description of the method was published by\n    Brandmaier, von Oertzen, McArdle, & Lindenberger (2013) <doi:10.1037/a0030001> \n    and Arnold, Voelkle, & Brandmaier (2020) <doi:10.3389/fpsyg.2020.564403>.",
    "version": "0.9.23",
    "maintainer": "Andreas M. Brandmaier <andy@brandmaier.de>",
    "author": "Andreas M. Brandmaier [aut, cre],\n  John J. Prindle [aut],\n  Manuel Arnold [aut],\n  Caspar J. Van Lissa [aut],\n  Moritz John [ctb]",
    "url": "https://github.com/brandmaier/semtree",
    "bug_reports": "https://github.com/brandmaier/semtree/issues",
    "repository": "https://cran.r-project.org/package=semtree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semtree Recursive Partitioning for Structural Equation Models SEM Trees and SEM Forests -- an extension of model-based decision\n    trees and forests to Structural Equation Models (SEM). SEM trees hierarchically\n    split empirical data into homogeneous groups each sharing similar data patterns\n    with respect to a SEM by recursively selecting optimal predictors of these\n    differences. SEM forests are an extension of SEM trees. They are ensembles of\n    SEM trees each built on a random sample of the original data. By aggregating\n    over a forest, we obtain measures of variable importance that are more robust\n    than measures from single trees. A description of the method was published by\n    Brandmaier, von Oertzen, McArdle, & Lindenberger (2013) <doi:10.1037/a0030001> \n    and Arnold, Voelkle, & Brandmaier (2020) <doi:10.3389/fpsyg.2020.564403>.  "
  },
  {
    "id": 20649,
    "package_name": "shortr",
    "title": "Optimal Subset Identification in Undirected Weighted Network\nModels",
    "description": "Identifies what optimal subset of a desired number of items should be retained in a short version of a psychometric instrument to assess the \u201cbroadest\u201d proportion of the construct-level content of the set of items included in the original version of the said psychometric instrument. Expects a symmetric adjacency matrix as input (undirected weighted network model). Supports brute force and simulated annealing combinatorial search algorithms.",
    "version": "1.0.1",
    "maintainer": "Lo\u00efs Fournier <lois.fournier@unil.ch>",
    "author": "Lo\u00efs Fournier [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6743-6456>),\n  Alexandre Heeren [aut] (ORCID: <https://orcid.org/0000-0003-0553-6149>),\n  St\u00e9phanie Baggio [aut] (ORCID: <https://orcid.org/0000-0002-5347-5937>),\n  Luke Clark [aut] (ORCID: <https://orcid.org/0000-0003-1103-2422>),\n  Antonio Verdejo-Garc\u00eda [aut] (ORCID:\n    <https://orcid.org/0000-0001-8874-9339>),\n  Jos\u00e9 C. Perales [aut] (ORCID: <https://orcid.org/0000-0001-5163-8811>),\n  Jo\u00ebl Billieux [aut] (ORCID: <https://orcid.org/0000-0002-7388-6194>)",
    "url": "https://doi.org/10.32614/CRAN.package.shortr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shortr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shortr Optimal Subset Identification in Undirected Weighted Network\nModels Identifies what optimal subset of a desired number of items should be retained in a short version of a psychometric instrument to assess the \u201cbroadest\u201d proportion of the construct-level content of the set of items included in the original version of the said psychometric instrument. Expects a symmetric adjacency matrix as input (undirected weighted network model). Supports brute force and simulated annealing combinatorial search algorithms.  "
  },
  {
    "id": 20694,
    "package_name": "silp",
    "title": "Conditional Process Analysis (CPA) via SEM Approach",
    "description": "Utilizes the Reliability-Adjusted Product Indicator (RAPI) method to \n    estimate effects among latent variables, thus allowing for more precise definition and analysis of \n    mediation and moderation models. Our simulation studies reveal that while 'silp' may exhibit \n    instability with smaller sample sizes and lower reliability scores (e.g., N = 100, 'omega' = 0.7), \n    implementing nearest positive definite matrix correction and bootstrap confidence interval \n    estimation can significantly ameliorate this volatility. When these adjustments are applied, \n    'silp' achieves estimations akin in quality to those derived from LMS. In conclusion, the 'silp' \n    package is a valuable tool for researchers seeking to explore complex relational structures between \n    variables without resorting to commercial software.\n    Cheung et al.(2021)<doi:10.1007/s10869-020-09717-0>\n    Hsiao et al.(2018)<doi:10.1177/0013164416679877>.",
    "version": "1.0.3",
    "maintainer": "Yi-Hsuan Tseng <r12227115@g.ntu.edu.tw>",
    "author": "Yi-Hsuan Tseng [aut, cre],\n  Po-Hsien Huang [aut]",
    "url": "https://github.com/TomBJJJ/silp",
    "bug_reports": "https://github.com/TomBJJJ/silp/issues",
    "repository": "https://cran.r-project.org/package=silp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "silp Conditional Process Analysis (CPA) via SEM Approach Utilizes the Reliability-Adjusted Product Indicator (RAPI) method to \n    estimate effects among latent variables, thus allowing for more precise definition and analysis of \n    mediation and moderation models. Our simulation studies reveal that while 'silp' may exhibit \n    instability with smaller sample sizes and lower reliability scores (e.g., N = 100, 'omega' = 0.7), \n    implementing nearest positive definite matrix correction and bootstrap confidence interval \n    estimation can significantly ameliorate this volatility. When these adjustments are applied, \n    'silp' achieves estimations akin in quality to those derived from LMS. In conclusion, the 'silp' \n    package is a valuable tool for researchers seeking to explore complex relational structures between \n    variables without resorting to commercial software.\n    Cheung et al.(2021)<doi:10.1007/s10869-020-09717-0>\n    Hsiao et al.(2018)<doi:10.1177/0013164416679877>.  "
  },
  {
    "id": 20698,
    "package_name": "simBKMRdata",
    "title": "Helper Functions for Bayesian Kernel Machine Regression",
    "description": "Provides a suite of helper functions to support Bayesian Kernel \n    Machine Regression (BKMR) analyses in environmental health research. It \n    enables the simulation of realistic multivariate exposure data using \n    Multivariate Skewed Gamma distributions, estimation of distributional \n    parameters by subgroup, and application of adaptive, data-driven thresholds \n    for feature selection via Posterior Inclusion Probabilities (PIPs). It is \n    especially suited for handling skewed exposure data and enhancing the \n    interpretability of BKMR results through principled variable selection. The\n    methodology is shown in Hasan et. al. (2025) <doi:10.1101/2025.04.14.25325822>.",
    "version": "0.2.1",
    "maintainer": "Kazi Tanvir Hasan <khasa006@fiu.edu>",
    "author": "Kazi Tanvir Hasan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7723-4177>),\n  Ibrahimou Boubakari [aut] (ORCID:\n    <https://orcid.org/0000-0003-3173-4317>),\n  Guerini Cristian [aut] (ORCID: <https://orcid.org/0009-0004-4480-5182>),\n  Bursac Zoran [aut] (ORCID: <https://orcid.org/0000-0001-9306-0907>),\n  Roberto Lucchini [aut] (ORCID: <https://orcid.org/0000-0002-9723-0237>),\n  Gabriel Odom [aut] (ORCID: <https://orcid.org/0000-0003-1341-4555>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simBKMRdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simBKMRdata Helper Functions for Bayesian Kernel Machine Regression Provides a suite of helper functions to support Bayesian Kernel \n    Machine Regression (BKMR) analyses in environmental health research. It \n    enables the simulation of realistic multivariate exposure data using \n    Multivariate Skewed Gamma distributions, estimation of distributional \n    parameters by subgroup, and application of adaptive, data-driven thresholds \n    for feature selection via Posterior Inclusion Probabilities (PIPs). It is \n    especially suited for handling skewed exposure data and enhancing the \n    interpretability of BKMR results through principled variable selection. The\n    methodology is shown in Hasan et. al. (2025) <doi:10.1101/2025.04.14.25325822>.  "
  },
  {
    "id": 20715,
    "package_name": "simStateSpace",
    "title": "Simulate Data from State Space Models",
    "description": "Provides a streamlined and user-friendly framework\n    for simulating data in state space models,\n    particularly when the number of subjects/units (n) exceeds one,\n    a scenario commonly encountered in social and behavioral sciences.\n    This package was designed to generate data for the simulations\n    performed in Pesigan, Russell, and Chow (2025) <doi:10.1037/met0000779>.    ",
    "version": "1.2.12",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "author": "Ivan Jacob Agaloos Pesigan [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4818-8420>),\n  Michael A. Russell [ctb] (ORCID:\n    <https://orcid.org/0000-0002-3956-604X>),\n  Sy-Miin Chow [ctb] (ORCID: <https://orcid.org/0000-0003-1938-027X>)",
    "url": "https://github.com/jeksterslab/simStateSpace,\nhttps://jeksterslab.github.io/simStateSpace/",
    "bug_reports": "https://github.com/jeksterslab/simStateSpace/issues",
    "repository": "https://cran.r-project.org/package=simStateSpace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simStateSpace Simulate Data from State Space Models Provides a streamlined and user-friendly framework\n    for simulating data in state space models,\n    particularly when the number of subjects/units (n) exceeds one,\n    a scenario commonly encountered in social and behavioral sciences.\n    This package was designed to generate data for the simulations\n    performed in Pesigan, Russell, and Chow (2025) <doi:10.1037/met0000779>.      "
  },
  {
    "id": 20721,
    "package_name": "simcdm",
    "title": "Simulate Cognitive Diagnostic Model ('CDM') Data",
    "description": "Provides efficient R and 'C++' routines to simulate cognitive diagnostic\n    model data for Deterministic Input, Noisy \"And\" Gate ('DINA') and\n    reduced Reparameterized Unified Model ('rRUM') from \n    Culpepper and Hudson (2017) <doi: 10.1177/0146621617707511>,\n    Culpepper (2015) <doi:10.3102/1076998615595403>, and\n    de la Torre (2009) <doi:10.3102/1076998607309474>.",
    "version": "0.1.2",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "author": "James Joseph Balamuta [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>),\n  Steven Andrew Culpepper [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4226-6176>),\n  Aaron Hudson [ctb, cph] (ORCID:\n    <https://orcid.org/0000-0002-9731-2224>)",
    "url": "https://tmsalab.github.io/simcdm/,\nhttps://github.com/tmsalab/simcdm",
    "bug_reports": "https://github.com/tmsalab/simcdm/issues",
    "repository": "https://cran.r-project.org/package=simcdm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simcdm Simulate Cognitive Diagnostic Model ('CDM') Data Provides efficient R and 'C++' routines to simulate cognitive diagnostic\n    model data for Deterministic Input, Noisy \"And\" Gate ('DINA') and\n    reduced Reparameterized Unified Model ('rRUM') from \n    Culpepper and Hudson (2017) <doi: 10.1177/0146621617707511>,\n    Culpepper (2015) <doi:10.3102/1076998615595403>, and\n    de la Torre (2009) <doi:10.3102/1076998607309474>.  "
  },
  {
    "id": 20745,
    "package_name": "simphony",
    "title": "Simulating Large-Scale, Rhythmic Data",
    "description": "A tool for simulating rhythmic data: transcriptome data using\n  Gaussian or negative binomial distributions, and behavioral activity data\n  using Bernoulli or Poisson distributions. See Singer et al. (2019)\n  <doi:10.7717/peerj.6985>.",
    "version": "1.0.3",
    "maintainer": "Jake Hughey <jakejhughey@gmail.com>",
    "author": "Jake Hughey [aut, cre],\n  Jordan Singer [aut],\n  Darwin Fu [ctb]",
    "url": "https://simphony.hugheylab.org,\nhttps://github.com/hugheylab/simphony",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simphony",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simphony Simulating Large-Scale, Rhythmic Data A tool for simulating rhythmic data: transcriptome data using\n  Gaussian or negative binomial distributions, and behavioral activity data\n  using Bernoulli or Poisson distributions. See Singer et al. (2019)\n  <doi:10.7717/peerj.6985>.  "
  },
  {
    "id": 20789,
    "package_name": "sindyr",
    "title": "Sparse Identification of Nonlinear Dynamics",
    "description": "\n    This implements the Brunton et al (2016; PNAS <doi:10.1073/pnas.1517384113>) sparse\n    identification algorithm for finding ordinary differential equations for a measured \n    system from raw data (SINDy). The package includes a set of additional tools for \n    working with raw data, with an emphasis on cognitive science applications (Dale \n    and Bhat, 2018 <doi:10.1016/j.cogsys.2018.06.020>). See \n    <https://github.com/racdale/sindyr> for examples and updates.",
    "version": "0.2.4",
    "maintainer": "Rick Dale <racdale@gmail.com>",
    "author": "Rick Dale and Harish S. Bhat",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sindyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sindyr Sparse Identification of Nonlinear Dynamics \n    This implements the Brunton et al (2016; PNAS <doi:10.1073/pnas.1517384113>) sparse\n    identification algorithm for finding ordinary differential equations for a measured \n    system from raw data (SINDy). The package includes a set of additional tools for \n    working with raw data, with an emphasis on cognitive science applications (Dale \n    and Bhat, 2018 <doi:10.1016/j.cogsys.2018.06.020>). See \n    <https://github.com/racdale/sindyr> for examples and updates.  "
  },
  {
    "id": 20851,
    "package_name": "slcm",
    "title": "Sparse Latent Class Model for Cognitive Diagnosis",
    "description": "Perform a Bayesian estimation of the exploratory \n    Sparse Latent Class Model for Binary Data \n    described by Chen, Y., Culpepper, S. A., and Liang, F. (2020) \n    <doi:10.1007/s11336-019-09693-2>.",
    "version": "0.1.1",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "author": "James Joseph Balamuta [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>),\n  Steven Andrew Culpepper [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4226-6176>)",
    "url": "https://tmsalab.github.io/slcm/, https://github.com/tmsalab/slcm",
    "bug_reports": "https://github.com/tmsalab/slcm/issues",
    "repository": "https://cran.r-project.org/package=slcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "slcm Sparse Latent Class Model for Cognitive Diagnosis Perform a Bayesian estimation of the exploratory \n    Sparse Latent Class Model for Binary Data \n    described by Chen, Y., Culpepper, S. A., and Liang, F. (2020) \n    <doi:10.1007/s11336-019-09693-2>.  "
  },
  {
    "id": 21090,
    "package_name": "sparseSEM",
    "title": "Elastic Net Penalized Maximum Likelihood for Structural Equation\nModels with Network GPT Framework",
    "description": "Provides elastic net penalized maximum likelihood estimator for structural equation models (SEM). The package implements `lasso` and `elastic net` (l1/l2) penalized SEM and estimates the model parameters with an efficient block coordinate ascent algorithm that maximizes the penalized likelihood of the SEM.  Hyperparameters are inferred from cross-validation (CV).  A Stability Selection (STS) function is also available to provide accurate causal effect selection. The software achieves high accuracy performance through a `Network Generative Pre-trained Transformer` (Network GPT) Framework with two steps: 1) pre-trains the model to generate a complete (fully connected) graph; and 2) uses the complete graph as the initial state to fit the `elastic net` penalized SEM.",
    "version": "4.1",
    "maintainer": "Anhui Huang <anhuihuang@gmail.com>",
    "author": "Anhui Huang [aut, ctb, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sparseSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseSEM Elastic Net Penalized Maximum Likelihood for Structural Equation\nModels with Network GPT Framework Provides elastic net penalized maximum likelihood estimator for structural equation models (SEM). The package implements `lasso` and `elastic net` (l1/l2) penalized SEM and estimates the model parameters with an efficient block coordinate ascent algorithm that maximizes the penalized likelihood of the SEM.  Hyperparameters are inferred from cross-validation (CV).  A Stability Selection (STS) function is also available to provide accurate causal effect selection. The software achieves high accuracy performance through a `Network Generative Pre-trained Transformer` (Network GPT) Framework with two steps: 1) pre-trains the model to generate a complete (fully connected) graph; and 2) uses the complete graph as the initial state to fit the `elastic net` penalized SEM.  "
  },
  {
    "id": 21231,
    "package_name": "splithalfr",
    "title": "Estimate Split-Half Reliabilities",
    "description": "Estimates split-half reliabilities for scoring algorithms of cognitive tasks and questionnaires. The 'splithalfr' supports researcher-provided scoring algorithms, with six vignettes illustrating how on included datasets. The package provides four splitting methods (first-second, odd-even, permutated, Monte Carlo), the option to stratify splits by task design, a number of reliability coefficients, the option to sub-sample data, and bootstrapped confidence intervals.",
    "version": "3.0.0",
    "maintainer": "Thomas Pronk <pronkthomas@gmail.com>",
    "author": "Thomas Pronk [aut, cre]",
    "url": "https://github.com/tpronk/splithalfr",
    "bug_reports": "https://github.com/tpronk/splithalfr/issues",
    "repository": "https://cran.r-project.org/package=splithalfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "splithalfr Estimate Split-Half Reliabilities Estimates split-half reliabilities for scoring algorithms of cognitive tasks and questionnaires. The 'splithalfr' supports researcher-provided scoring algorithms, with six vignettes illustrating how on included datasets. The package provides four splitting methods (first-second, odd-even, permutated, Monte Carlo), the option to stratify splits by task design, a number of reliability coefficients, the option to sub-sample data, and bootstrapped confidence intervals.  "
  },
  {
    "id": 21390,
    "package_name": "statConfR",
    "title": "Models of Decision Confidence and Measures of Metacognition",
    "description": "Provides fitting functions and other tools for decision confidence \n    and metacognition researchers, including meta-d'/d', often considered to be \n    the gold standard to measure metacognitive efficiency, and information-theoretic measures of metacognition. \n    Also allows to fit and compare several static models of decision making and confidence.",
    "version": "0.2.1",
    "maintainer": "Manuel Rausch <manuel.rausch@ku.de>",
    "author": "Manuel Rausch [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5805-5544>),\n  Sascha Meyen [aut] (ORCID: <https://orcid.org/0000-0001-6928-4126>),\n  Sebastian Hellmann [aut] (ORCID:\n    <https://orcid.org/0000-0002-3621-6343>)",
    "url": "https://github.com/ManuelRausch/StatConfR",
    "bug_reports": "https://github.com/ManuelRausch/StatConfR/issues",
    "repository": "https://cran.r-project.org/package=statConfR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statConfR Models of Decision Confidence and Measures of Metacognition Provides fitting functions and other tools for decision confidence \n    and metacognition researchers, including meta-d'/d', often considered to be \n    the gold standard to measure metacognitive efficiency, and information-theoretic measures of metacognition. \n    Also allows to fit and compare several static models of decision making and confidence.  "
  },
  {
    "id": 21420,
    "package_name": "statpsych",
    "title": "Statistical Methods for Psychologists",
    "description": "Implements confidence interval and sample size methods that \n    are especially useful in psychological research. The methods can be \n    applied in 1-group, 2-group, paired-samples, and multiple-group designs\n    and to a variety of parameters including means, medians, proportions, \n    slopes, standardized mean differences, standardized linear contrasts \n    of means, plus several measures of correlation and association. \n    Confidence interval and sample size functions are given for single \n    parameters as well as differences, ratios, and linear contrasts of\n    parameters. The sample size functions can be used to approximate the \n    sample size needed to estimate a parameter or function of parameters \n    with desired confidence interval precision or to perform a variety of\n    hypothesis tests (directional two-sided, equivalence, superiority, \n    noninferiority) with desired power. For details see: Statistical Methods\n    for Psychologists, Volumes 1 \u2013 4, <https://dgbonett.sites.ucsc.edu/>.   ",
    "version": "1.8.0",
    "maintainer": "Douglas G. Bonett <dgbonett@ucsc.edu>",
    "author": "Douglas G. Bonett [aut, cre],\n  Robert J. Calin-Jageman [ctb]",
    "url": "https://github.com/dgbonett/statpsych/,\nhttps://dgbonett.github.io/statpsych/",
    "bug_reports": "https://github.com/dgbonett/statpsych/issues",
    "repository": "https://cran.r-project.org/package=statpsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statpsych Statistical Methods for Psychologists Implements confidence interval and sample size methods that \n    are especially useful in psychological research. The methods can be \n    applied in 1-group, 2-group, paired-samples, and multiple-group designs\n    and to a variety of parameters including means, medians, proportions, \n    slopes, standardized mean differences, standardized linear contrasts \n    of means, plus several measures of correlation and association. \n    Confidence interval and sample size functions are given for single \n    parameters as well as differences, ratios, and linear contrasts of\n    parameters. The sample size functions can be used to approximate the \n    sample size needed to estimate a parameter or function of parameters \n    with desired confidence interval precision or to perform a variety of\n    hypothesis tests (directional two-sided, equivalence, superiority, \n    noninferiority) with desired power. For details see: Statistical Methods\n    for Psychologists, Volumes 1 \u2013 4, <https://dgbonett.sites.ucsc.edu/>.     "
  },
  {
    "id": 21558,
    "package_name": "studentlife",
    "title": "Tidy Handling and Navigation of the Student-Life Dataset",
    "description": "Download, navigate and analyse the Student-Life dataset. \n    The Student-Life dataset contains passive and automatic sensing data \n    from the phones of a class of 48 Dartmouth college students. \n    It was collected over a 10 week term. Additionally, the dataset contains ecological \n    momentary assessment results along with pre-study and post-study mental  \n    health surveys. The intended use is to assess \n    mental health, academic performance and behavioral trends. \n    The raw dataset and additional information is \n    available at <https://studentlife.cs.dartmouth.edu/>.",
    "version": "1.1.0",
    "maintainer": "Daniel Fryer <d.fryer@latrobe.edu.au>",
    "author": "Daniel Fryer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6032-0522>),\n  Hien Nguyen [aut] (ORCID: <https://orcid.org/0000-0002-9958-432X>),\n  Pierre Orban [aut]",
    "url": "https://github.com/Frycast/studentlife",
    "bug_reports": "https://github.com/Frycast/studentlife/issues",
    "repository": "https://cran.r-project.org/package=studentlife",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "studentlife Tidy Handling and Navigation of the Student-Life Dataset Download, navigate and analyse the Student-Life dataset. \n    The Student-Life dataset contains passive and automatic sensing data \n    from the phones of a class of 48 Dartmouth college students. \n    It was collected over a 10 week term. Additionally, the dataset contains ecological \n    momentary assessment results along with pre-study and post-study mental  \n    health surveys. The intended use is to assess \n    mental health, academic performance and behavioral trends. \n    The raw dataset and additional information is \n    available at <https://studentlife.cs.dartmouth.edu/>.  "
  },
  {
    "id": 21945,
    "package_name": "test2norm",
    "title": "Normative Standards for Cognitive Tests",
    "description": "Package test2norm contains functions to generate formulas for \n    normative standards applied to cognitive tests. It takes raw test scores \n    (e.g., number of correct responses) and converts them to scaled scores and \n    demographically adjusted scores, using methods described in Heaton et al. \n    (2003) <doi:10.1016/B978-012703570-3/50010-9> & Heaton et al. (2009, \n    ISBN:9780199702800). The scaled scores are calculated as quantiles of the \n    raw test scores, scaled to have the mean of 10 and standard deviation of 3, \n    such that higher values always correspond to better performance on the test. \n    The demographically adjusted scores are calculated from the residuals of a \n    model that regresses scaled scores on demographic predictors (e.g., age). \n    The norming procedure makes use of the mfp2() function from the 'mfp2' \n    package to explore nonlinear associations between cognition and demographic \n    variables.",
    "version": "0.3.0.1",
    "maintainer": "Anya Umlauf <aumlauf@health.ucsd.edu>",
    "author": "Anya Umlauf [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=test2norm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "test2norm Normative Standards for Cognitive Tests Package test2norm contains functions to generate formulas for \n    normative standards applied to cognitive tests. It takes raw test scores \n    (e.g., number of correct responses) and converts them to scaled scores and \n    demographically adjusted scores, using methods described in Heaton et al. \n    (2003) <doi:10.1016/B978-012703570-3/50010-9> & Heaton et al. (2009, \n    ISBN:9780199702800). The scaled scores are calculated as quantiles of the \n    raw test scores, scaled to have the mean of 10 and standard deviation of 3, \n    such that higher values always correspond to better performance on the test. \n    The demographically adjusted scores are calculated from the residuals of a \n    model that regresses scaled scores on demographic predictors (e.g., age). \n    The norming procedure makes use of the mfp2() function from the 'mfp2' \n    package to explore nonlinear associations between cognition and demographic \n    variables.  "
  },
  {
    "id": 21975,
    "package_name": "text2speech",
    "title": "Text to Speech Conversion",
    "description": "Converts text into speech using various text-to-speech (TTS) engines and provides an unified interface for accessing their functionality.\n  With this package, users can easily generate audio files of spoken words, phrases, or sentences from plain text data. The package supports multiple TTS engines, \n  including Google's 'Cloud Text-to-Speech API', 'Amazon Polly', Microsoft's 'Cognitive Services Text to Speech REST API',  and a free TTS engine called 'Coqui TTS'.",
    "version": "1.0.0",
    "maintainer": "Howard Baek <howardbaek.fh@gmail.com>",
    "author": "Howard Baek [cre] (ORCID: <https://orcid.org/0009-0000-8942-1618>),\n  John Muschelli [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-6469-1750>)",
    "url": "https://github.com/jhudsl/text2speech",
    "bug_reports": "https://github.com/jhudsl/text2speech/issues",
    "repository": "https://cran.r-project.org/package=text2speech",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2speech Text to Speech Conversion Converts text into speech using various text-to-speech (TTS) engines and provides an unified interface for accessing their functionality.\n  With this package, users can easily generate audio files of spoken words, phrases, or sentences from plain text data. The package supports multiple TTS engines, \n  including Google's 'Cloud Text-to-Speech API', 'Amazon Polly', Microsoft's 'Cognitive Services Text to Speech REST API',  and a free TTS engine called 'Coqui TTS'.  "
  },
  {
    "id": 22061,
    "package_name": "tidySEM",
    "title": "Tidy Structural Equation Modeling",
    "description": "A tidy workflow for generating, estimating, reporting,\n    and plotting structural equation models using 'lavaan', 'OpenMx', or\n    'Mplus'. Throughout this workflow, elements of syntax, results, and graphs\n    are represented as 'tidy' data, making them easy to customize.\n    Includes functionality to estimate latent class analyses, and to plot\n    'dagitty' and 'igraph' objects.",
    "version": "0.2.9",
    "maintainer": "Caspar J. van Lissa <c.j.vanlissa@tilburguniversity.edu>",
    "author": "Caspar J. van Lissa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0808-5024>),\n  Mauricio Garnier-Villarreal [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2951-6647>),\n  Frank C Gootjes [ctb] (ORCID: <https://orcid.org/0000-0002-0639-1001>)",
    "url": "https://cjvanlissa.github.io/tidySEM/",
    "bug_reports": "https://github.com/cjvanlissa/tidySEM/issues",
    "repository": "https://cran.r-project.org/package=tidySEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidySEM Tidy Structural Equation Modeling A tidy workflow for generating, estimating, reporting,\n    and plotting structural equation models using 'lavaan', 'OpenMx', or\n    'Mplus'. Throughout this workflow, elements of syntax, results, and graphs\n    are represented as 'tidy' data, making them easy to customize.\n    Includes functionality to estimate latent class analyses, and to plot\n    'dagitty' and 'igraph' objects.  "
  },
  {
    "id": 22481,
    "package_name": "tsnet",
    "title": "Fitting, Comparing, and Visualizing Networks Based on Time\nSeries Data",
    "description": "Fit, compare, and visualize Bayesian graphical vector autoregressive (GVAR) network models using 'Stan'. These models are commonly used in psychology to represent temporal and contemporaneous relationships between multiple variables in intensive longitudinal data. Fitted models can be compared with a test based on matrix norm differences of posterior point estimates to quantify the differences between two estimated networks. See also Siepe, Kloft & Heck (2024) <doi:10.31234/osf.io/uwfjc>.",
    "version": "0.2.0",
    "maintainer": "Bj\u00f6rn S. Siepe <bjoernsiepe@gmail.com>",
    "author": "Bj\u00f6rn S. Siepe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9558-4648>),\n  Matthias Kloft [aut] (ORCID: <https://orcid.org/0000-0003-1845-6957>),\n  Daniel W. Heck [ctb] (ORCID: <https://orcid.org/0000-0002-6302-9252>)",
    "url": "https://github.com/bsiepe/tsnet",
    "bug_reports": "https://github.com/bsiepe/tsnet/issues",
    "repository": "https://cran.r-project.org/package=tsnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsnet Fitting, Comparing, and Visualizing Networks Based on Time\nSeries Data Fit, compare, and visualize Bayesian graphical vector autoregressive (GVAR) network models using 'Stan'. These models are commonly used in psychology to represent temporal and contemporaneous relationships between multiple variables in intensive longitudinal data. Fitted models can be compared with a test based on matrix norm differences of posterior point estimates to quantify the differences between two estimated networks. See also Siepe, Kloft & Heck (2024) <doi:10.31234/osf.io/uwfjc>.  "
  },
  {
    "id": 22528,
    "package_name": "tvem",
    "title": "Time-Varying Effect Models",
    "description": "Fits time-varying effect models (TVEM). These are a kind of application of varying-coefficient models in the context of longitudinal data, allowing the strength of linear, logistic, or Poisson regression relationships to change over time.  These models are described further in Tan, Shiyko, Li, Li & Dierker (2012) <doi:10.1037/a0025814>.  We thank Kaylee Litson, Patricia Berglund, Yajnaseni Chakraborti, and Hanjoo Kim for their valuable help with testing the package and the documentation. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.",
    "version": "1.4.1",
    "maintainer": "John J. Dziak <dziakj1@gmail.com>",
    "author": "John J. Dziak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0762-5495>),\n  Donna L. Coffman [aut] (ORCID: <https://orcid.org/0000-0001-6305-6579>),\n  Runze Li [aut] (ORCID: <https://orcid.org/0000-0002-0154-2202>),\n  Kaylee Litson [aut] (ORCID: <https://orcid.org/0000-0003-1296-4811>),\n  Yajnaseni Chakraborti [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tvem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tvem Time-Varying Effect Models Fits time-varying effect models (TVEM). These are a kind of application of varying-coefficient models in the context of longitudinal data, allowing the strength of linear, logistic, or Poisson regression relationships to change over time.  These models are described further in Tan, Shiyko, Li, Li & Dierker (2012) <doi:10.1037/a0025814>.  We thank Kaylee Litson, Patricia Berglund, Yajnaseni Chakraborti, and Hanjoo Kim for their valuable help with testing the package and the documentation. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.  "
  },
  {
    "id": 22772,
    "package_name": "variationalDCM",
    "title": "Variational Bayesian Estimation for Diagnostic Classification\nModels",
    "description": "Enables computationally efficient parameters-estimation by variational Bayesian methods for various diagnostic classification models (DCMs). DCMs are a class of discrete latent variable models for classifying respondents into latent classes that typically represent distinct combinations of skills they possess. Recently, to meet the growing need of large-scale diagnostic measurement in the field of educational, psychological, and psychiatric measurements, variational Bayesian inference has been developed as a computationally efficient alternative to the Markov chain Monte Carlo methods, e.g., Yamaguchi and Okada (2020a) <doi:10.1007/s11336-020-09739-w>, Yamaguchi and Okada (2020b) <doi:10.3102/1076998620911934>, Yamaguchi (2020) <doi:10.1007/s41237-020-00104-w>, Oka and Okada (2023) <doi:10.1007/s11336-022-09884-4>, and Yamaguchi and Martinez (2023) <doi:10.1111/bmsp.12308>. To facilitate their applications, 'variationalDCM' is developed to provide a collection of recently-proposed variational Bayesian estimation methods for various DCMs.",
    "version": "2.0.1",
    "maintainer": "Keiichiro Hijikata <k.hijikata.1120@outlook.jp>",
    "author": "Keiichiro Hijikata [aut, cre],\n  Motonori Oka [aut] (ORCID: <https://orcid.org/0000-0002-9867-8922>),\n  Kazuhiro Yamaguchi [aut] (ORCID:\n    <https://orcid.org/0000-0001-8011-8575>),\n  Kensuke Okada [aut] (ORCID: <https://orcid.org/0000-0003-1663-5812>)",
    "url": "https://github.com/khijikata/variationalDCM",
    "bug_reports": "https://github.com/khijikata/variationalDCM/issues",
    "repository": "https://cran.r-project.org/package=variationalDCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "variationalDCM Variational Bayesian Estimation for Diagnostic Classification\nModels Enables computationally efficient parameters-estimation by variational Bayesian methods for various diagnostic classification models (DCMs). DCMs are a class of discrete latent variable models for classifying respondents into latent classes that typically represent distinct combinations of skills they possess. Recently, to meet the growing need of large-scale diagnostic measurement in the field of educational, psychological, and psychiatric measurements, variational Bayesian inference has been developed as a computationally efficient alternative to the Markov chain Monte Carlo methods, e.g., Yamaguchi and Okada (2020a) <doi:10.1007/s11336-020-09739-w>, Yamaguchi and Okada (2020b) <doi:10.3102/1076998620911934>, Yamaguchi (2020) <doi:10.1007/s41237-020-00104-w>, Oka and Okada (2023) <doi:10.1007/s11336-022-09884-4>, and Yamaguchi and Martinez (2023) <doi:10.1111/bmsp.12308>. To facilitate their applications, 'variationalDCM' is developed to provide a collection of recently-proposed variational Bayesian estimation methods for various DCMs.  "
  },
  {
    "id": 22793,
    "package_name": "vcmeta",
    "title": "Varying Coefficient Meta-Analysis",
    "description": "Implements functions for varying coefficient meta-analysis methods. \n  These methods do not assume effect size homogeneity. Subgroup effect size \n  comparisons, general linear effect size contrasts, and linear models of \n  effect sizes based on varying coefficient methods can be used to describe \n  effect size heterogeneity. Varying coefficient meta-analysis methods do not \n  require the unrealistic assumptions of the traditional fixed-effect and \n  random-effects meta-analysis methods. For details see: \n  Statistical Methods for Psychologists, Volume 5, <https://dgbonett.sites.ucsc.edu/>.",
    "version": "1.5.0",
    "maintainer": "Douglas G. Bonett <dgbonett@ucsc.edu>",
    "author": "Douglas G. Bonett [aut, cre],\n  Robert J. Calin-Jageman [ctb]",
    "url": "https://github.com/dgbonett/vcmeta/,\nhttps://dgbonett.github.io/vcmeta/",
    "bug_reports": "https://github.com/dgbonett/vcmeta/issues",
    "repository": "https://cran.r-project.org/package=vcmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vcmeta Varying Coefficient Meta-Analysis Implements functions for varying coefficient meta-analysis methods. \n  These methods do not assume effect size homogeneity. Subgroup effect size \n  comparisons, general linear effect size contrasts, and linear models of \n  effect sizes based on varying coefficient methods can be used to describe \n  effect size heterogeneity. Varying coefficient meta-analysis methods do not \n  require the unrealistic assumptions of the traditional fixed-effect and \n  random-effects meta-analysis methods. For details see: \n  Statistical Methods for Psychologists, Volume 5, <https://dgbonett.sites.ucsc.edu/>.  "
  },
  {
    "id": 22909,
    "package_name": "vocaldia",
    "title": "Create and Manipulate Vocalisation Diagrams",
    "description": "Create adjacency matrices of vocalisation graphs from\n  dataframes containing sequences of speech and silence intervals,\n  transforming these matrices into Markov diagrams, and generating\n  datasets for classification of these diagrams by 'flattening' them\n  and adding global properties (functionals) etc.  Vocalisation\n  diagrams date back to early work in psychiatry (Jaffe and Feldstein,\n  1970) and social psychology (Dabbs and Ruback, 1987) but have only\n  recently been employed as a data representation method for machine\n  learning tasks including meeting segmentation (Luz, 2012)\n  <doi:10.1145/2328967.2328970> and classification (Luz,\n  2013) <doi:10.1145/2522848.2533788>.",
    "version": "0.8.4",
    "maintainer": "Saturnino Luz <luzs@acm.org>",
    "author": "Saturnino Luz [aut, cre]",
    "url": "https://git.ecdf.ed.ac.uk/sluzfil/vocaldia",
    "bug_reports": "https://git.ecdf.ed.ac.uk/sluzfil/vocaldia/-issues",
    "repository": "https://cran.r-project.org/package=vocaldia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vocaldia Create and Manipulate Vocalisation Diagrams Create adjacency matrices of vocalisation graphs from\n  dataframes containing sequences of speech and silence intervals,\n  transforming these matrices into Markov diagrams, and generating\n  datasets for classification of these diagrams by 'flattening' them\n  and adding global properties (functionals) etc.  Vocalisation\n  diagrams date back to early work in psychiatry (Jaffe and Feldstein,\n  1970) and social psychology (Dabbs and Ruback, 1987) but have only\n  recently been employed as a data representation method for machine\n  learning tasks including meeting segmentation (Luz, 2012)\n  <doi:10.1145/2328967.2328970> and classification (Luz,\n  2013) <doi:10.1145/2522848.2533788>.  "
  },
  {
    "id": 22912,
    "package_name": "voiceR",
    "title": "Voice Analytics for Social Scientists",
    "description": "Simplifies and largely automates practical voice analytics for social science research. This package offers an accessible and easy-to-use interface, including an interactive Shiny app, that simplifies the processing, extraction, analysis, and reporting of voice recording data in the behavioral and social sciences. The package includes batch processing capabilities to read and analyze multiple voice files in parallel, automates the extraction of key vocal features for further analysis, and automatically generates APA formatted reports for typical between-group comparisons in experimental social science research. A more extensive methodological introduction that inspired the development of the 'voiceR' package is provided in Hildebrand et al. 2020 <doi:10.1016/j.jbusres.2020.09.020>.  ",
    "version": "0.1.0",
    "maintainer": "Francesc Busquet <francesc.busquet@unisg.ch>",
    "author": "Francesc Busquet [aut, cre],\n  Christian Hildebrand [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=voiceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "voiceR Voice Analytics for Social Scientists Simplifies and largely automates practical voice analytics for social science research. This package offers an accessible and easy-to-use interface, including an interactive Shiny app, that simplifies the processing, extraction, analysis, and reporting of voice recording data in the behavioral and social sciences. The package includes batch processing capabilities to read and analyze multiple voice files in parallel, automates the extraction of key vocal features for further analysis, and automatically generates APA formatted reports for typical between-group comparisons in experimental social science research. A more extensive methodological introduction that inspired the development of the 'voiceR' package is provided in Hildebrand et al. 2020 <doi:10.1016/j.jbusres.2020.09.020>.    "
  },
  {
    "id": 23143,
    "package_name": "wordnet",
    "title": "WordNet Interface",
    "description": "An interface to WordNet using the Jawbone Java API to WordNet.\n  WordNet (<https://wordnet.princeton.edu/>) is a large lexical database of\n  English.  Nouns, verbs, adjectives and adverbs are grouped into sets of\n  cognitive synonyms (synsets), each expressing a distinct concept.  Synsets\n  are interlinked by means of conceptual-semantic and lexical relations.\n  Please note that WordNet(R) is a registered tradename.  Princeton\n  University makes WordNet available to research and commercial users\n  free of charge provided the terms of their license\n  (<https://wordnet.princeton.edu/license-and-commercial-use>) are followed,\n  and proper reference is made to the project using an appropriate\n  citation (<https://wordnet.princeton.edu/citing-wordnet>).\n  The WordNet database files need to be made available separately,\n  either via package 'wordnetDicts' from <https://datacube.wu.ac.at>,\n  installing system packages where available, or direct download from\n  <https://wordnetcode.princeton.edu/3.0/WNdb-3.0.tar.gz>.",
    "version": "0.1-17",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Ingo Feinerer [aut],\n  Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  Mike Wallace [ctb, cph] (Jawbone Java WordNet API library)",
    "url": "https://wordnet.princeton.edu/,\nhttps://sites.google.com/site/mfwallace/jawbone",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordnet WordNet Interface An interface to WordNet using the Jawbone Java API to WordNet.\n  WordNet (<https://wordnet.princeton.edu/>) is a large lexical database of\n  English.  Nouns, verbs, adjectives and adverbs are grouped into sets of\n  cognitive synonyms (synsets), each expressing a distinct concept.  Synsets\n  are interlinked by means of conceptual-semantic and lexical relations.\n  Please note that WordNet(R) is a registered tradename.  Princeton\n  University makes WordNet available to research and commercial users\n  free of charge provided the terms of their license\n  (<https://wordnet.princeton.edu/license-and-commercial-use>) are followed,\n  and proper reference is made to the project using an appropriate\n  citation (<https://wordnet.princeton.edu/citing-wordnet>).\n  The WordNet database files need to be made available separately,\n  either via package 'wordnetDicts' from <https://datacube.wu.ac.at>,\n  installing system packages where available, or direct download from\n  <https://wordnetcode.princeton.edu/3.0/WNdb-3.0.tar.gz>.  "
  },
  {
    "id": 23268,
    "package_name": "xxIRT",
    "title": "Item Response Theory and Computer-Based Testing in R",
    "description": "A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>.",
    "version": "2.1.2",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/xxIRT",
    "bug_reports": "https://github.com/xluo11/xxIRT/issues",
    "repository": "https://cran.r-project.org/package=xxIRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xxIRT Item Response Theory and Computer-Based Testing in R A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>.  "
  }
]